{"seq_id": "1b7d6394-801d-4944-bad8-1a39bbe568de", "title": "This Time of Dying", "text": "【0】This Time of Dying\nA must-read for those who think flu pandemic preparedness is much ado about nothing, this first novel by Reina James brings home the severity and plausibility of a flu disaster. And without data, statistics, or comment, it makes a compelling case for progress on the matter of influenza, to ease suffering, prevent spread, and possibly eliminate the scourge.\n\n【1】A series of journal entries beginning October 14 and ending November 7, 1918, this story of humans under stress unfolds in London against the savage backdrop of World War I and amidst the oft-recorded pestilence of Spanish flu. Without speculation, the author constructs population drama, one patient and one family at a time. The reader is drawn into it instantly and stays absorbed ’til the end.\n\n【2】Characters from all walks of life make up the cast of the unfolding tragedy. They chase about in their masks obsessing over work, social and self-imposed restrictions, unrelenting class barriers, and daily trivia until the schools close, the servants die or flee the ailing households, businesses fail, or until they succumb, filling the streets, mortuaries, graveyards, churches. We are allowed into their disinfected homes, to watch the banality, madness, or sheer horror of their lives.\n\n【3】Some are stars. The heroic physician is back from the war with heightened awareness of the health emergency. “A plague is now among us which may well leave the earth to the animals,” he writes in a note intended to alert the health authorities. “You must stop the movement of troops, close our ports and warn others to follow suit.” He dies, “a blue man in the road,” before he can deliver the message. “His lips and ears were purple-blue, like a plum, and the skin on his face was mottled and pale but still tinged with blue, as if he’d been wiped with an inky rag.”\n\n【4】The undertaker, who finds the warning note crumbled in the dead physician’s palm, is a man who plays the piano in between constructing coffins and practicing the family trade. He becomes increasingly unsettled by the note as the dead overwhelm his business, his city, his life. “The world’s body can hardly draw breath; it is sick and brought to its bed,” the physician had written. “Its wounds are open. The young spill out . Death is crossing every sea.” He confirms that indeed the dying “were unusually young,” tries in vain to inform the authorities, and concludes with disbelief that “They had no plan.” The knowledge imparted by the note changes the undertaker’s life, which is already complicated by his universally disapproved affair with a woman “a little bit above his station.”\n\n【5】His friend dismissed her, “She’s like a bloke.” Her friend, in a moment of weakness, thought her “a foolish ageing woman with a weak, offended face. Her lips merged unpleasantly into the skin surrounding them; her cheeks were dragged flat by failing muscle. There was too much eyelid and too little eyebrow. There was even a suggestion of fluff above the mouth, of the type that would inevitably grow into a moustache.” Their attraction, slowed by awkwardness and saved by spunk, adds an endearing almost hopeful quality to a tale rife with decomposing bodies.\n\n【6】Another physician, a retired practitioner, is exhausted by the torrent of patients and his inability to improve their situation. Asked his opinion about the epidemic, he retorted angrily, “I haven’t got an _opinion_ . I’m too busy.” Exasperated by people reporting “cyanosis,” he admitted that few things irritated him “more than patients using medical terminology.” In the absence of medical interventions, he suggested “fresh air in the bedroom, aspirin for headache, plenty of fluids and light food only, if tolerated.” His unloved wife and assistant by default props him up in bed to sign a few more death certificates before he collapses.\n\n【7】Dedicated by the author to her maternal grandparents who died of it, this account of Spanish flu in London rings true for its hold on human behavior and of course for the flu, whose specter looms on our horizon. Along with the science, along with the vaccine, a dose of past history in human terms warns against underestimating a new pandemic. As James put it, “if every one of the newly bereaved were to hold a lantern in the sky, the man in the moon would think the world to be on fire.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7ed4918b-edfe-4243-885e-335355b675a0", "title": "Malaria and Global Warming in Perspective?", "text": "【0】Malaria and Global Warming in Perspective?\n**To the Editor:** I read with great interest the article \"From Shakespeare to Defoe: malaria in England in the Little Ice Age\"  . Unfortunately, the article is not as balanced as a presentation last year by Paul Reiter, which clearly illustrated that, although climate is important in the transmission of malaria, the influence of other factors (e.g. access to medical care and improved housing) is likely to be of more importance in Europe.\n\n【1】Malaria indeed was quite common in Europe, even in the Roman Empire and in Medieval Europe, and until a few decades ago, it was still present in parts of Europe, Australia, and North America. In fact, the failure of the 1806 British invasion of Zeeland in the Netherlands may be attributable to infection of the British forces with malaria. However, the authors referenced by Reiter have never made the claim that in the coming years warmer \"temperatures will result in malaria transmission in Europe and North America.\" On the contrary, the reports of the Intergovernmental Panel on Climate Change Reiter quotes conclude that \"Although climate change could increase the potential transmission of malaria \\[in Europe and North America\\], existing public health resources--disease surveillance, surface water management, and treatment of cases--would make reemergent malaria unlikely\" .\n\n【2】Reiter's argument that some scientists attribute the recent observed increase in malaria risk to climate trends is also not accurate. While acknowledging the sensitivity of the malaria mosquito and parasite to climate, these researchers examine insect and incidence data to explore multiple factors underlying malaria emergence. Another group of scientists uses mathematical simulation models to estimate changes in malaria risk over the next few decades. These models, which are heuristic tools not meant to predict future worlds, assess how potential risk for malaria may by affected by changes in climate  . The goals of both types of research are to improve knowledge of the complex malaria transmission cycle, define epidemic-prone areas, identify the reasons for increased malaria risk, and develop solutions to protect vulnerable communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9b231f17-db4a-4d37-910f-fa9aa3e9edfc", "title": "Serologic Evidence of Scrub Typhus in the Peruvian Amazon", "text": "【0】Serologic Evidence of Scrub Typhus in the Peruvian Amazon\nInfections with scrub typhus group orientiae (STGO) are a common and widespread cause of fever in Asia, the western Pacific, and northern Australia (the tsutsugamushi triangle). The causative pathogen, _Orientia tsutsugamushi_ , is transmitted by the larval stage of trombiculid mites of the genus _Leptobromidium_ (chiggers). Clinical manifestations can be mild and unspecific, but complications include jaundice, meningoencephalitis, myocarditis, and interstitial pneumonia leading to acute respiratory distress syndrome and renal failure .\n\n【1】Epidemiologic studies in Southeast Asia have identified rickettsial illness (especially scrub typhus and murine typhus) as leading causes of treatable undifferentiated febrile illness, although they are often misdiagnosed as malaria, dengue, or typhoid fever . The causative agent of scrub typhus was not believed to exist outside the tsutsugamushi triangle until _Orientia_ spp. were identified by serologic and molecular methods in febrile patients from the Middle East  and Chile . Recently, more autochthonous cases of scrub typhus were reported from Chiloé Island in southern Chile, suggesting endemicity in the area . Therefore, we conducted a study to obtain serologic evidence of scrub typhus in the Peruvian Amazon.\n\n【2】### The Study\n\n【3】This study was conducted in Iquitos, which is located in the Amazon forest in the Department of Loreto in northeastern Peru, where ongoing epidemiologic studies on febrile illness and rickettsial disease  have been conducted. STGO testing was nested in an ongoing febrile surveillance study (NMRCD.2010.0010), which was approved by the US Naval Medical Research Unit No. 6 Institutional Review Board (Lima, Peru). The current study protocol was approved by the US Naval Medical Research Unit No. 6 Institutional Review Board in compliance with all applicable federal regulations governing the protection of human subjects.\n\n【4】Febrile surveillance was conducted in 12 health facilities (hospitals and 9 outpatient clinics; 2 of the 12 were military facilities) distributed across 4 districts of Iquitos. Febrile patients who fulfilled the inclusion criteria (axillary temperature \\> 37.5°C, duration of illness < 5 days, and age \\> 5 years) were asked to participate in the study. An acute-phase blood sample was collected at the time of enrollment, and a convalescent-phase blood sample was obtained 10–30 days later. These samples had already been used for detection of other pathogens (mainly dengue virus and other arboviruses), and results and testing methods have been reported .\n\n【5】We retrospectively screened convalescent-phase blood samples obtained from participants enrolled during 2013 by using an STGO-specific IgG ELISA. We screened convalescent-phase samples for IgG against a mixture of whole-cell antigen preparations from Karp, Kato, and Gilliam strains of _O. tsutsugamushi_ in an STGO-specific IgG ELISA as described . For convalescent-phase samples with net absorbance \\> 0.500 at a 1:100 serum dilution, we subsequently assessed STGO IgG ELISA titer (range 1:100–1:6,400) with their paired acute-phase samples side-by-side to determine their endpoint titers. We considered samples with a net total absorbance \\> 1.000 for serum dilutions 1:100, 1:400, 1:1,600, and 1:6,400 titer positive. We classified samples with a seroconversion or a \\> 4-fold increase in IgG titer between acute-phase and convalescent-phase blood samples and minimum titer of 1:400 in the convalescent-phase sample as indicative of active rickettsial infection .\n\n【6】Samples with evidence of active infection were then confirmed by immunofluorescence assay (IFA) with the _Orientia_ MIF IgG Kit (Fuller Laboratories, Fullerton, CA, USA) and Karp, Kato, Gilliam, and Boryong strains of _O. tsutsugamushi_ according to the manufacturer’s instructions. We further tested acute-phase samples with evidence for active infection by PCR to identify the causative pathogen.\n\n【7】We extracted DNA from whole blood samples by using QIAmp DNA Mini Kits (QIAGEN, Valencia, CA, USA) following the manufacturer’s instructions except that a final elution volume of 100 μL was used. Samples were stored at −80°C. We assessed DNA samples from persons with evidence for recent active disease by using a quantitative PCR specific for the _O. tsutsugamuchi_ 47-kD antigen gene as reported .\n\n【8】During 2013, we enrolled 1,497 participants in the main study. Of these participants, 1,124 had paired serum samples. Results of the STGO-specific IgG were positive for 60 (.3%) of 1,124 convalescent-phase serum samples with a titer \\> 1:400. One participant had a >4-fold increase in titer (acute-phase sample titer 1:400, convalescent-phase sample titer \\> 6,400) and was confirmed by IFA as showing a \\> 4-fold increase in titer for 3 of 4 strains of _O. tsutsugamushi_ (Karp, Gilliam, and Boryong) and a 2-fold increase in titer for the Kato strain. A test result of the acute-phase sample for _Orientia_ sp. DNA was negative. This case-patient was a 22-year-old soldier stationed in a rural military camp at the time of illness. He had fever, chills, malaise, muscle pain, nausea, and vomiting but no rash or jaundice. The patient had not traveled in the 2 weeks before illness and therefore could not have contracted the infection elsewhere.\n\n【9】### Conclusions\n\n【10】We performed a retrospective systematic analysis for the presence of scrub typhus in Iquitos, Peru, by testing blood samples collected prospectively during unspecific acute febrile illness. Although definitive molecular evidence of _Orientia_ spp. infection was not found, our serologic evidence strongly suggests the presence of this pathogen in tropical areas of Peru.\n\n【11】Seroprevalence among febrile illness patients was low, especially because most patients tested came from urban rather than rural areas. In addition, our study design could further underestimate scrub typhus because we only included patients with fever for <5 days, whereas the mean time to signs of scrub typhus has been reported as 8.2 days . Although the 1 patient with ELISA and IFA evidence of seroconversion showed a negative result by PCR, a negative PCR result during the acute phase of scrub typhus is not uncommon, even when testing is performed on whole blood or buffy coat samples . In addition, whether the causative agents of scrub typhus in South America are recognized by the primer set used in the quantitative PCR is unknown .\n\n【12】Cross-reactivity with other _Rickettsia_ spp. does not seem to be a concern. The _Orientia_ spp. ELISA using the immunodominant genus-specific, 56-kDa, type-specific antigen (outer membrane protein \\[Omp\\]) does not react with antibodies produced during typhus group rickettsiae (TGR) and spotted fever group rickettsiae (SFGR) infections because rickettsiae lack the 56-kDa type-specific antigen. Similarly, antibodies produced against _Orientia_ spp. infection do not react against SFGR and TGR ELISA antigens (lipopolysaccharides OmpA and OmpB), because _Orientia_ spp. do not contain these antigens . As expected, screening results for SFGR and TGR  were negative for the case-patient we report.\n\n【13】Dengue virus and other arboviruses were excluded (by PCR or virus isolation) as alternative causes of fever. Furthermore, study participants had probably already received over-the-counter medications, including antimicrobial drugs, which would account for false-negative PCR results. According to the prospectively collected clinical data, a typical rash was not documented for the case-patient we report. However, because this surveillance study was not directed toward scrub typhus features, a small eschar could have been easily missed.\n\n【14】This study raises many new issues, such as possible local reservoirs and vectors. The presence of trombiculide mites in Peru has been reported in other areas , but this species has not been studied as a local vector for pathogens. The case-patient we report was in the military and stationed in a rural camp. Therefore, he was regularly exposed to typical vegetation that supports the presence of mites. Our results and those from studies in Chile  indicate a need for a more expansive survey for evidence of scrub typhus, not only in Peru, but throughout South America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ef6c6f26-a27d-455a-a38a-a9a35ea35601", "title": "Annual Mycobacterium tuberculosis Infection Risk and Interpretation of Clustering Statistics", "text": "【0】Annual Mycobacterium tuberculosis Infection Risk and Interpretation of Clustering Statistics\nStudies are increasingly using levels of clustering of isolates from tuberculosis cases (proportion sharing identical DNA fingerprint patterns) to estimate the extent of disease attributable to recent transmission. To date, few studies have been conducted in unindustrialized countries, where the impact of tuberculosis and the proportion of disease attributable to recent transmission are greatest. Whether or not the properties and interpretation of clustering statistics in such settings are similar to those in industrialized populations is unclear.\n\n【1】Studies in industrialized countries have found relatively low overall levels of clustering (e.g. 30% to 40% during a 3-year period ) but much higher levels among younger versus older patients. This age differential probably reflects past trends in the annual risk for infection, which was high in the early 20th century (e.g. _\\>_ 2% per year before 1940 in the Netherlands ) and is currently very low. Thus, a large proportion of disease in older patients is attributable to reactivation of infections acquired many years ago, and, given the short half-life of DNA fingerprint patterns , only a small proportion of old patients share identical isolates with other patients **.** In some unindustrialized countries, on the other hand, where the annual risk for infection may not have changed much over time, the age differential in clustering might be small, given that a large proportion of disease even among older persons may be attributable to recent (re)infection. Understanding the effect of the magnitude of the annual risk for tuberculous infection on clustering frequency helps determine how molecular epidemiologic data can be best applied to estimate the extent of ongoing transmission of _Mycobacterium tuberculosis_ , and hence to identify optimal control strategies.\n\n【2】We explored how the magnitude and trend in the annual risk for infection influence the age-specific proportion of clustered cases and its relationship to the extent of disease attributable to recent transmission. We use a model of the transmission dynamics of _M. tuberculosis_ previously calibrated to data from the Netherlands , where isolates from all tuberculosis cases with onset since 1993 have been routinely DNA fingerprinted . We describe the general epidemiologic assumptions in the model and how it distinguishes between cases according to the DNA fingerprint pattern of the strain causing the disease episode, which is needed to calculate clustering statistics.\n\n【3】### Methods\n\n【4】Our analysis is based on a model developed recently to interpret data on clustering of DNA fingerprint patterns in the Netherlands . Equations describing the model’s formulation are provided in the Appendix.\n\n【5】##### Epidemiologic Assumptions in the Model\n\n【6】The model’s structure, parameters, and assumptions have been published . Persons are assumed to be born uninfected. Infected persons are divided into those in whom primary disease has not yet developed (defined by convention as disease within 5 years of initial infection ), and those in the “latent” class, who are at risk for endogenous reactivation or for reinfection, which can be followed by exogenous disease. Exogenous disease is here defined as the first disease episode within 5 years of the most recent reinfection; endogenous disease includes disease occurring >5 years after the most recent (re)infection event, and second or subsequent disease episodes occurring <5 years after the most recent (re)infection event. (These definitions differ slightly from those of Sutherland et al. to include the assumption that once persons have recovered from disease during the first 5 years after initial infection or reinfection, their risk of developing disease becomes the same as that of developing disease through reactivation, until they are newly reinfected.)\n\n【7】The infection and reinfection risks are assumed to be identical, but reinfection is less likely to lead to disease than is initial infection, due to some immunity induced by the prior infection . We explored the implications of four assumptions for the magnitude (and trend) in the annual risk for infection, namely, that the risk for infection 1) declined over time, as estimated for the Netherlands (from approximately 2% in 1940 to approximately 5/10,000 by 1979 ); 2) remained unchanged over time at a very low level (.1%); 3) remained unchanged at 1%; or 4) remained unchanged at 3%. Infection risks of 1% have been found in several populations (e.g. Malawi ). Infection risks of 3% are uncommon today but have been reported in parts of South Africa . For simplicity, we assumed that persons cannot be reinfected during the period between initial infection (or reinfection) and onset of the first primary episode (or exogenous disease).\n\n【8】The risks of developing disease depend on age and sex ; they are based on previous analyses, in which we fitted predictions of disease incidence to observed notifications in the U.K . The risks of developing either a first primary episode or disease following exogenous reinfection also depend on the time since infection and reinfection, respectively . The probability that a disease episode is infectious (sputum smear/culture-positive) is age dependent  . The demography of the population described in the model is assumed to be that for the Netherlands. Analyses are restricted to respiratory (pulmonary) forms of tuberculosis, since these are far more likely than extrapulmonary forms to lead to transmission. Although additional factors such as immigration and HIV can influence the extent of clustering in complicated ways , these factors are not considered here, where the focus is upon the effect of the magnitude and trend in the annual risk for infection on clustering.\n\n【9】##### Derivation of Clustering Statistics\n\n【10】Recent studies suggest that the half-life of DNA fingerprint patterns based on IS _6110_ restriction fragment length polymorphism (RFLP, which has been used for the DNA fingerprinting conducted to date in most studies) is 2–5 years . If the molecular clock speed for IS _6110_ RFLP patterns of strains involved in latent infection (currently unknown) were to be similar, this relatively short half-life implies that most of the fingerprint patterns of the strains causing disease today differ from those that caused disease many years ago. Similarly, this short half-life implies that the _M. tuberculosis_ fingerprint types and cluster distributions in tuberculosis cases today depend only loosely upon those that existed 50 years ago. Based on this assumption, to derive clustering estimates for a given population for recent years, we designed the model to simulate the introduction and subsequent transmission of strains with new DNA fingerprint patterns from a sufficiently distant time in the past (taken to be 1950), so that a) all cases with onset in recent years involved a strain whose DNA fingerprint pattern had first appeared since then and b) no assumptions would be required about the distributions of strains that existed before 1950. The general steps in the calculations are outlined briefly below.\n\n【11】The numbers of persons of each age in each of the epidemiologic categories for 1950 were calculated by using the model, based on described equations . From 1950, each of these age-sex classes was stratified to distinguish between those who had, versus those who had not, been (re)infected since 1950. Those who had been (re)infected since 1950 were subdivided further according to the time of infection or reinfection. The transmission dynamics were tracked simultaneously for all persons with the equations described in the Appendix and elsewhere , by using time steps of 6 months and 1 year for calendar year and age, respectively.\n\n【12】In each interval, disease was assumed to develop in a proportion of infected persons, and a proportion of these disease episodes was attributed to a strain for which the DNA fingerprint pattern differed from that of the strain with which the persons were originally infected. This latter proportion depended on the time since infection , and each of the new DNA fingerprint patterns was assigned a unique identity number. Each infectious patient with onset at a given time was assumed to contact a different number of persons . The frequency distributions of the number of persons contacted by each patient were used to derive the total number of persons who were newly (re)infected at this time. The corresponding equations were then applied to this number to determine the total number of persons in whom disease developed at a later time, _T,_ among those who had been infected at time _t_ . The DNA fingerprint patterns of the strains in these diseased persons were then determined by using the frequency distribution of the number of persons contacted by each case-patient at time _t_ . These calculations are described further in the Appendix.\n\n【13】##### Estimating the Effect of the Annual Risk for Infection on Clustering as an Indicator of Recent Transmission\n\n【14】Our model was used to calculate the age-specific proportion of disease attributable to primary and exogenous disease from 1993 to 1997 for the Netherlands and for settings in which the annual risk for infection is assumed to have remained unchanged over time at 0.1%, 1% _,_ and 3% _._ Primary and exogenous disease involve disease occurring during the first 5 years after the most recent (re)infection event, although the majority of persons in whom primary or exogenous (reinfection) disease develops acquire the disease within 2–3 years . The clustering by sex and age for cases with onset in different periods between 1993 and 1997 for the Netherlands, and for settings in which the annual risk for infection is assumed to have remained unchanged over time at 0.1%, 1%, and 3%,was also calculated by using the age and sex distribution of the cases with onset in that period . For simplicity, we present age-specific levels of clustering for male patients only. Model predictions for male patients generally compared better against the observed data in the Netherlands than did those for female patients .\n\n【15】The predictive values of clustering for the identification of recent transmission were calculated as follows. The positive predictive value of clustering for identifying recent transmission in different age groups in different periods was calculated as the proportion of case-patients who were in a cluster in a given period who had been infected or reinfected <5 years before disease onset. The negative predictive value of clustering for identifying recent transmission in different age groups was calculated as the proportion of case-patients who were not in a cluster in a given period who had been infected or last reinfected >5 years before disease onset.\n\n【16】### Results\n\n【17】##### Model Predictions of the Extent of Clustering and Disease Attributable to Recent Transmission\n\n【18】As shown in Figure 3 , very different age patterns in the proportion of disease attributable to recent transmission were predicted for the Netherlands and for settings in which the annual risk for infection has remained unchanged over time. In the Netherlands, the proportion of disease attributed to recent infection decreased dramatically with age, e.g. from 100% in the young to approximately 50% and 10% for 45- to 54-year-old patients and persons >65 years of age, respectively. The proportion of disease attributed to recent reinfection was very low for all age groups (<3%). For constant infection risk settings, the predicted proportion of disease attributable to recent transmission (i.e. recent infection or reinfection) was very similar, falling from 100% in the young to 85%, 88% _,_ and 90% in the oldest age groups for the 0.1%, 1%, and 3% infection risk scenarios, respectively. On the other hand, large differences between settings were predicted in the proportion of disease attributed to initial infection or to reinfection. In all instances, the proportion attributed to reinfection was zero in the youngest age groups, but this proportion increased with age to 3%, 35%, and 80% for the 0.1%, 1%, and 3% annual infection risk assumptions, respectively. The proportion attributed to recent initial infection in these settings decreased from 100% in the young to 80%, 50%, and 15%, respectively, in old patients.\n\n【19】As shown in Figure 4A , for each setting, the overall clustering (i.e. that seen among all age groups) was predicted to increase with study duration, e.g. from 15% for the Netherlands for a 1-year period to approximately 25% for a 5-year period. The clustering predicted for all the constant infection risk scenarios was similar in magnitude for each study period and increased from 60% to 70% for a 1-year period to 75% to 85% for a 5-year period. Since the overall clustering was not predicted to increase much for study periods of more than 3 years, clustering is defined using a 3-year period in the remainder of these analyses (represented by 1993–1995). As shown in Figure 4B , the clustering predicted for each age group was similar for each of the settings in which the annual risk for infection remained unchanged over time, and declined only slightly with age, e.g. from 83% for the youngest age group to approximately 75% for the oldest age category. In contrast, for the Netherlands, the clustering was predicted to decrease dramatically with age, from approximately 75% among young case-patients to approximately 15% in very old patients. This prediction is consistent with observed data .\n\n【20】##### Reliability of Clustering as a Measure of the Extent of Recent Transmission\n\n【21】For settings in which the annual risk for infection remained unchanged over time at 0.1%, 1%, and 3%, the predicted clustering in each age group underestimated the proportion who had been recently infected or reinfected . In settings with an annual risk for infection of 0.1%, at least 90% of cases in each age group were predicted to have been recently (re)infected, whereas the proportion clustered decreased from about 85% in the youngest age group to approximately 70% for the oldest persons. For the Netherlands (described elsewhere ), clustering underestimated the proportion of disease attributable to recent transmission in the young (by up to 43%) and overestimated that for older patients (by up to 50%).\n\n【22】The positive and negative predictive values of being in a cluster, as an indicator of recent transmission, depended both on age and the study setting . For settings with a high annual risk for infection that had remained unchanged over time, model predictions suggested that most patients clustered in each age group were likely to have been recently (re)infected, corresponding to a positive predictive value of clustering for recent transmission of almost 100% in each age group . The positive predictive value was estimated to decrease with age in the Netherlands from 100% in the very young to about 20% for the oldest patients.\n\n【23】When unclustered cases were considered, the proportion of clinical case-patients who were estimated to have been infected >5 years previously was low (<5%) for young patients and increased with age for all settings, approaching 100% for patients ages >55 years in the Netherlands . Almost all case-patients of ages >55 years who were not in a cluster in the Netherlands were therefore estimated to have been infected >5 years previously and thus owed their disease to reactivation of latent foci. Of the adult case-patients who were not in a cluster in the other settings, the proportion who had been infected >5 years previously was <45%, 35%, and 20% if the annual risk for infection was 0.1%, 1%, and 3%, respectively.\n\n【24】### Discussion\n\n【25】The availability of DNA fingerprinting techniques has led to a large number of studies that measure clustering of isolates from tuberculosis cases . Most of these studies have been conducted in industrialized settings and have found relatively low levels of clustering (% to 40%) and decreases in clustering with age. Our analyses indicate that those findings have been influenced strongly by the large secular decline in the annual risk for infection that occurred in industrialized settings during the 20th century and that very different findings are expected in settings where the annual risk for infection has changed little over time. The clustering predicted is high (>60% for 2-year periods) in such settings, similar for all age groups, and may nevertheless still underestimate the extent of disease that is due to recent transmission.\n\n【26】Our conclusions are based on a model of the transmission dynamics of _M. tuberculosis_ that includes several simplifications. The most obvious is our assumption that the risks for disease, given infection in settings in which the infection risk is high, are the same as those estimated for industrialized populations. HIV influences these risks , although its effect on clustering is not yet understood . Another simplification is our assumption that the half-life of DNA fingerprint patterns is identical for strains involved in active disease and in latent infection. If latent infections are associated with a slow rate of genetic change of the bacilli, our assumption would have led to an underestimate of clustering but would not have affected our conclusions for settings in which the annual risk for infection has remained unchanged over time, where only a small proportion of disease is attributed to reactivation of a latent infection . The effect of this assumption on clustering estimates for the Netherlands is discussed elsewhere .\n\n【27】Our finding that the overall azmount of clustering in populations with a low (constant) annual infection risk should be similar to that observed in populations with a high (constant) infection risk may appear paradoxical. Our finding follows from the fact that in such populations any decline in the proportion of disease attributable to recent primary infection with age is compensated by increases in the proportion attributable to recent reinfection with age . As a result, both the overall and age-specific predicted proportions of disease attributable to recent transmission in these populations are very similar; this finding leads to predictions that the overall and age-specific levels of clustering in these settings would also be similar.\n\n【28】Previous model-based analyses  have indicated that in industrialized settings such as the Netherlands clustering among young case-patients will underestimate the extent of disease attributable to recent transmission (because some sources of infection have onset outside the study period and because DNA fingerprint patterns can change between infection and disease onset), and clustering among old case-patients may overestimate recent transmission (because clustering among older case-patients is more likely to be attributable to their being sources of infection rather than their being recently reinfected). These analyses extend those findings and indicate that in settings in which the annual risk for infection has not changed much over time, the overall level of clustering in any given age group is likely to underestimate the extent of recent transmission . This underestimate follows from the fact that in these settings, most disease in all age groups is attributable to recent transmission, and some patients will have been infected or reinfected immediately before the study started and thus may not be in a cluster.\n\n【29】These analyses provide the first estimates of the positive and negative predictive values of clustering. Overall, these analyses highlight the fact that in settings in which the annual risk for infection has not changed greatly over time, most clustered case-patients are likely to have been recently infected or reinfected (i.e. the positive predictive value of clustering is high) . This finding suggests that in such settings, application of the “n-1” rule , which assumes that each cluster comprises an index case attributable to reactivation and the other cases result (in)directly from that case, will lead to even more unreliable estimates of the extent of recent transmission than those based on the “n” rule. Similarly, estimates of the proportion of disease attributable to reactivation will be unreliable if they are based on the proportion of patients who fail to be in a cluster in a given period.\n\n【30】Our analyses demonstrate that the properties and interpretation of clustering statistics depend strongly on the trend and magnitude in the annual risk for infection and thus will vary between settings. For example, in settings in which the annual risk for infection has remained unchanged at either a high or a low level, the age differential in clustering is likely to be small, in contrast with that in industrialized settings, and clustering is likely to underestimate the extent of recent transmission in all age groups. Given the growing importance of clustering studies, which, to date have been conducted in populations in which the annual risk for infection declined dramatically over time and is currently very low, these insights are important for an improved understanding of the natural history of tuberculosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ea732667-10c6-4b3a-a93e-0f2d4430ecb3", "title": "Prion Disease in Dromedary Camels, Algeria", "text": "【0】Prion Disease in Dromedary Camels, Algeria\nPrions are responsible for a group of fatal and transmissible neurodegenerative diseases named prion diseases. A misfolded and aggregated isoform of a cell-surface protein termed cellular prion protein (PrP Sc  ) is the main, if not the sole, component of prions . Creutzfeldt-Jakob disease in humans and scrapie in small ruminants are the longest known diseases in this group, but prion diseases entered the public spotlight with the massive bovine spongiform encephalopathy (BSE) epidemic started in 1986 in the United Kingdom, revealing the zoonotic potential of animal prions.\n\n【1】Since the BSE epidemic begin, interest in these diseases has increased, and the prion universe has continued to expand . Several new prion diseases—including variant Creutzfeldt-Jakob disease, atypical/Nor98 scrapie of sheep, and atypical L- and H-type BSE—have been identified in the past 20 years, and chronic wasting disease (CWD) is spreading dramatically across cervid populations in North America and recently was discovered in Norway .\n\n【2】Public health concern increased markedly after variant Creutzfeldt-Jakob disease was demonstrated to be caused by the same prion strain responsible for the BSE epidemics . Unprecedented efforts were made to control the epidemics in cattle and to contain the exposure of humans to potentially infected cattle-derived materials.\n\n【3】In addition to having fatal consequences for infected animals, scrapie and BSE have a serious economic effect on the livestock industry. Scrapie brings economic damages through production loss, export loss, and increased cost for carcass disposal, which account for $10–$20 million annually in the United States . In the United Kingdom, where BSE was diagnosed in >180,000 cattle and up to 3 million were likely to have been affected, the cost to the public was >£5 billion (≈$7.1 billion US) .\n\n【4】Prion diseases can manifest as sporadic (putatively spontaneous), genetic, or infectious disorders . In animals, disorders resembling sporadic or genetic human prion diseases have been reported only recently, with the discovery of atypical/Nor98 scrapie in small ruminants  and L- and H-type BSE in cattle . Infectious prion diseases have been known for much longer and have been described in several animal species. Some diseases derived from accidental transmission, as is the case with BSE, which affected millions of cattle but also involved goats, domestic cats, nonhuman primates, and wild bovid and felid species, most likely fed with material contaminated by the BSE agent . Even the outbreaks of transmissible mink encephalopathy reported in the United States and various European countries in ranch-raised mink most likely originated from feedstuff accidentally contaminated by prions .\n\n【5】Despite the long list of susceptible animal species, prion diseases behave as infectious and naturally occurring conditions only in ruminants. Scrapie affects sheep and goats, and CWD affects different species of the _Cervidae_ family: mule deer (Odocoileus hemionus_ ), white-tailed deer (O. virginianus_ ), elk (Cervus canadensis_ ), and moose (Alces alces_ ) . Furthermore, CWD has been recently diagnosed in reindeer (Rangifer tarandus_ )  and moose  from Norway.\n\n【6】We report prion disease in dromedary camels (Camelus dromedarius_ ) from a Saharian population in Ouargla in southeastern Algeria, where the disease was observed in animals brought for slaughter at the Ouargla abattoir. Dromedaries are widespread throughout northern and eastern Africa, the Middle East, and part of Asia, where they are the means of subsistence for millions of families who live in the most hostile ecosystems on the planet. Since ancient times, camels have been exploited as beasts of burden and sources of milk and meat and for riding; today, they are tremendously important as a sustainable livestock species. During the past 10 years, the camel farming system has evolved rapidly and improved substantially . The emergence of a prion disease in a farmed animal species of such importance requires a thorough risk assessment for implementing evidence-based policies to control the disease in animals and minimize human exposure.\n\n【7】### Materials and Methods\n\n【8】##### Animals and Tissue Samples\n\n【9】The Ouargla abattoir is one of the largest slaughterhouses in slaughtered volume for cattle, camels, and small ruminants in Algeria. In the past 5 years, neurologic symptoms have been observed more often in adult dromedaries at antemortem examination. The signs include weight loss; behavioral abnormalities; and neurologic signs, such as tremors, aggressiveness, hyperreactivity, typical down and upward movements of the head, hesitant and uncertain gait, ataxia of the hind limbs, occasional falls, and difficulty getting up .\n\n【10】According to breeders’ descriptions, the early stage of the disease was mainly characterized by behavioral signs, such as loss of appetite and irritability. Separation from the herd at pastures along with aggressiveness and tendency to kick and bite when handled were usually observed. With disease progression, neurologic signs became obvious; animals showed ataxia that eventually led to recumbency and death.\n\n【11】Breeders reported that signs progressed slowly and that the duration of disease varied from 3 to 8 months. Although it has not been possible to date back the first cases of illness, information gathered from breeders and slaughterhouse personnel suggests the illness has been present since the 1980s.\n\n【12】Prion disease was suspected in dromedaries brought to the abattoir on the basis of clinical signs.\n\n【13】We collected brain samples from 3 dromedaries (nos. 3, 4, and 8) showing neurologic symptoms and from 1 clinically healthy animal , as well as cervical, prescapular, and lumbar aortic lymph nodes from 1 animal . The animals were all females, belonging to the Sahraoui population, 10, 11, 13, and 14 years of age, respectively.\n\n【14】We fixed samples in formalin for histologic and immunohistochemical examination. We also collected frozen brain samples from animals 4 and 8 for Western blot and genetic analysis and sampled formalin-fixed brain tissue from a clinically healthy animal . We obtained brain samples from BSE-infected cattle and from ARQ/ARQ sheep, either naturally affected by scrapie or experimentally infected with BSE, from the surveillance system in Italy or from previous studies .\n\n【15】##### Neuropathologic, Immunohistochemical, and Paraffin-Embedded Tissue Blot Analyses\n\n【16】We embedded brain and lymph node samples in paraffin wax, sectioned at 5 μm, and stained with hematoxylin and eosin or subjected to immunohistochemical or paraffin-embedded tissue blot analysis. We pretreated sections for immunohistochemistry with 98% formic acid for 5 min, followed by autoclaving in citrate buffer for 5 min at 121°C. We then treated sections with 6% normal goat serum (Vector Laboratories, Burlingame, CA, USA) in phosphate-buffered saline for 60 min. We performed immunohistochemical detection of PrP Sc  with L42 monoclonal antibody (mAb) (R-Biopharm, Darmstadt, Germany) at 0.01 μg/mL in phosphate-buffered saline overnight at 4°C. We treated sections with secondary biotinylated mouse antibody (Vector Laboratories), ABC Complex (Vector Laboratories) for 45 min, and diaminobenzidine (Sigma-Aldrich, St. Louis, MO, USA) for 3 min. We used Mayer’s hematoxylin for counterstaining. Each run comprised positive- and negative-control sections. We analyzed 3 sections from each lymph node sample.\n\n【17】We collected sections for paraffin-embedded blot on prewetted 0.45-μm–pore nitrocellulose membranes (Schleicher & Schuell, Dassel, Germany) and dried membranes for 24 h at 55°C. We performed membrane treatments, proteinase K (PK) (Sigma-Aldrich) digestion (μg/mL), and immunodetection as described . We used mAb L42 (.01 μg/mL) as the primary antibody.\n\n【18】##### Western Blot Analysis\n\n【19】We performed Western blot analysis of PrP Sc  from brain homogenates as previously described  and performed preliminary diagnosis with a final concentration of PK at 50 μg/mL. To compare dromedary PrP Sc  with PrP Sc  from sheep and cattle prion diseases, we performed molecular typing of their protease-resistant cores (PrP res  ) by discriminatory immunoblotting, conducted according to the ISS (Istituto Superiore di Sanità) discriminatory Western blot method  with minor modifications. The principle of discrimination is based on the differential N terminal cleavage by PK (μg/mL), revealed by using N terminal mAb with an epitope that is partially lost after PK digestion of BSE samples . As an additional discriminatory parameter, we measured the relative proportions of diglycosylated, monoglycosylated, and unglycosylated PrP fragments in L42 blots.\n\n【20】We performed deglycosylation by adding 18 μL of 0.2 M sodium phosphate buffer (pH 7.4) containing 0.8% Nonidet P40 (Roche. Penzberg, Germany) and 2 μL (U/mL) di N-Glycosidase F (Roche) to 5 μL of denaturated samples and incubating overnight at 37°C with gentle shaking. The mAbs used and their epitope on ovine PrP were as follows: L42 , 12B2 , SAF32 (octarepeat).\n\n【21】##### PrP Gene Sequence Analysis\n\n【22】We extracted DNA from 100 mg of frozen brain tissue with DNeasy Blood and Tissue Kit (QIAGEN, Hilden, Germany) following the manufacturer’s instructions. We amplified the PrP gene (PRNP_ ) coding sequence in a 50-μL final volume using 5 μL of extracted DNA, 1× AmpliTaq Gold 360 PCR Buffer (Applied Biosystems, Foster City, CA, USA), 2.5 mmol/L MgCl2, 1× 360 GC Enhancer, 200 μmol/L dNTPs, 0.25 μmol/L of forward (′-GCTGACACCCTCTTTATTTTGCAG-3′) and reverse (′-GATTAAGAAGATAATGAAAACAGGAAG-3′) primers , and 0.5 μL of AmpliTaq Gold 360 (Applied Biosystems), according to the following amplification protocol: 5 min at 96°C; 30 s at 96°C, 15 s at 57°C, 90 s at 72°C for 40 cycles, and 4 min at 72°C.\n\n【23】We purified amplicons by using an Illustra ExoProStar 1-Step clean-up kit (GE Healthcare Life Sciences, Little Chalfont, UK). We conducted sequencing reactions by using the BigDye Terminator v1.1 Cycle Sequencing Kit, purified using BigDye XTerminator Purification Kit, and detected with the ABI PRISM 3130 apparatus (all Applied Biosystems). We analyzed sequences by using Seq Scape version 2.5 (Applied Biosystems).\n\n【24】### Results\n\n【25】Histopathologic examination showed spongiform change, gliosis, and neuronal loss in several brain areas of the 3 symptomatic animals  but not in the asymptomatic dromedary. We observed vacuoles preferentially in the neuropil  but also frequently involving the neuronal bodies . Confluent vacuoles were rarely observed. These neurodegenerative changes consistently occurred in gray matter of subcortical brain areas, such as striatum, thalamus , midbrain, and pons  of all 3 animals; white matter was rarely affected. We observed moderate vacuolation in medulla oblongata, particularly in the vestibular and the olivary nucleus; nucleus of solitary tract and hypoglossal nucleus were less often affected. Cervical medulla, available only for animal 8, showed no spongiform changes. Cortical brain areas were variably involved. Animals 3 and 8 showed dispersed vacuolation in cingulate, piriform, and frontal cortices. In contrast, cerebral cortices were more heavily affected in animal 4. Cerebellum was collected from animals 4 and 8, and vacuoles were observed only in the molecular layer of animal 4.\n\n【26】By immunohistochemical analysis, we detected PrP Sc  in the brain of all symptomatic dromedaries. Overall, PrP Sc  deposition was invariably observed in brain areas with spongiform degeneration . In addition, PrP Sc  deposits also involved areas less often affected or not affected by spongiosis, such as the nucleus of the solitary tract ; the hypoglossal nucleus; pyramidal cells of hippocampus; the granular layer of cerebellum, including Purkinje cells ; and several white matter areas.\n\n【27】PrP Sc  deposition patterns involving neuropil, neurons, and glia differed. Patterns included synaptic/punctate , intraneuronal , perineuronal and linear , intraglial , and perivascular . In pons and medulla oblongata, we frequently observed an atypical intracellular pattern  in which PrP Sc  filled the whole cytoplasm. PrP Sc  was absent in the brain of the asymptomatic dromedary . Prominent protease-resistant PrP Sc  deposition was easily detected by paraffin-embedded blot in the same brain areas found positive by immunohistochemical analysis, such as the deep layers of cortices , the pyramidal layer and fimbria of hippocampus , the granular layer of cerebellum and the associated white matter , and the gray matter of pons .\n\n【28】We detected PrP Sc  deposits in cervical, prescapular, and lumbar aortic lymph nodes from animal 8  that involved >80% of primary and secondary follicles in the 3 sections analyzed. PrP Sc  deposits consisted of a reticular network at the center of the lymphoid follicles, which varied in staining intensity, accompanied by fine to coarse granules of PrP Sc  in the cytoplasm of nonlymphoid cells within the follicle. We also observed additional granular or intracellular PrP Sc  immunolabeling in the interfollicular areas.\n\n【29】Western blot analysis of brain homogenates from dromedaries 4 and 8 revealed PrP Sc  with a PrP res  showing the classical electrophoretic profile, characterized by 3 main bands representing diglycosylated, monoglycosylated, and unglycosylated PrP res  . Accordingly, the 3 bands were resolved in a single band of ≈18 kDa after enzymatic deglycosylation .\n\n【30】The apparent molecular weight of PrP res  from both animals was slightly higher than classical scrapie and clearly higher than BSE and sheep passaged BSE . This finding prompted us to investigate the N terminal PK cleavage under stringent PK conditions by discriminatory immunoblotting, which enables the molecular discrimination of the most common ruminant TSE strains from classical BSE . Epitope mapping of PrP res  showed that the higher apparent molecular weight in dromedary PrP res  reflects a more N terminal cleavage site than with BSEs and scrapie samples. Indeed, upon treatment with PK, dromedary PrP res  preserved the N terminal 12B2 and SAF32 mAb epitopes, whereas classical scrapie lost the SAF32 mAb epitope while preserving the 12B2 mAb epitope, and BSE samples lost both epitopes, being negative with SAF32 and 12B2 mAbs . We have previously shown that, with the ISS discriminatory Western blot, BSE and scrapie are both characterized by a diglycosylated dominant PrP res  pattern, although BSE is more heavily glycosylated than scrapie . Our data confirm this difference and show that PrP res  from dromedary camels is further less glycosylated than classical scrapie, being characterized by a monoglycosylated dominant PrP res  . Sequencing revealed the same _PRNP_ sequence in animals 4 and 8 (GenBank accession nos. MF990558–9), which, in turn, showed 100% nt identity with the _PRNP_ sequence already reported for dromedary camels .\n\n【31】In parallel to the laboratory analyses, we undertook a retrospective investigation of neurologic signs in dromedaries at the Ouargla slaughterhouse. Twenty of 937 animals in 2015 and 51 of 1,322 in 2016 showed the previously described neurologic signs ; the overall prevalence was 3.1% in dromedaries brought for slaughter. All slaughtered animals derived from the area surrounding Ouargla, and the disease was observed only in animals >8 years of age.\n\n【32】### Discussion\n\n【33】We describe a prion disease in dromedary camels, designated as camel prion disease (CPD), that we detected during routine antemortem inspection at the Ouargla slaughterhouse in Algeria. Retrospective analysis indicated a 3.1% prevalence of animals with neurologic signs suggestive of the disease in dromedaries brought for slaughter. That figure appears to be reliable given that clinical suspicion was confirmed in all 3 animals undergoing laboratory analysis. However, because prion diseases are characterized by long incubation periods and the age at which the disease becomes apparent (>8 years) is more advanced than the age at which most dromedaries are slaughtered (<5 years), the prevalence found in the older animals is probably higher than the actual prevalence (excluding younger animals).\n\n【34】The spectrum of animal species susceptible to prion disease is large. However, only in ruminants belonging to the _Bovidae_ and _Cervidae_ families do prion diseases behave as infectious and naturally occurring conditions. Dromedaries are not ruminants (suborder _Ruminantia_ ) but rather are _Tylopoda_ , a suborder of _Artiodactyla_ , which also includes the 2-humped camel (Camelus bactrianus_ ), wild Bactrian camel (C. ferus_ ), llamas (Lama glama_ ), alpacas (Vicugna pacos_ ), and vicuñas (V. vicugna_ ) . The presence of a prion disease in dromedaries extends the spectrum of animal species naturally susceptible to prion diseases to taxa different from those already known and opens up new research areas on the ecology and the host–pathogen relationship of prion diseases.\n\n【35】Whether CPD is an infectious disease in natural conditions is a key question. In scrapie and CWD, in which lymphoid tissues are extensively involved, the horizontal transmission in natural conditions is efficient. In contrast, when the peripheral lymphoid tissues are not substantially involved, as in cattle BSE, atypical/Nor98 scrapie, and most human prion diseases, the horizontal transmission appears to be inefficient. This inefficiency usually is explained by assuming the in vivo dissemination of PrP Sc  to the periphery as a prerequisite to facilitate prion shedding into the environment . Although we obtained samples from a single animal, our detection of PrP Sc  in all lymph nodes available suggests an abundant extraneural pathogenesis and, along with the notable prevalence of clinical cases at the slaughterhouse, concurs to suggest the infectious nature of CPD. These observations also suggest that the disease has an acquired rather than spontaneous onset.\n\n【36】The origin of CPD is unknown. It might be a disease unique to dromedaries or a malady deriving from transmission of a prion disease from another species. It is worth noting that meat and bone meal has been exported from the United Kingdom worldwide, and after the ban on feeding animals with ruminant protein in 1988, export to the Third World had soared to 30,000 tons  in 1991. Thus, the possibility that BSE-infected feed could have reached North Africa cannot be ruled out. However, even if the risk for BSE has not been formally assessed in Algeria and an official surveillance system for animal prion diseases is lacking, BSE is unlikely to appear in dromedaries without evidence in cattle populations. Moreover, dromedaries are mostly raised with no use of feedstuff. Lastly, the PrP Sc  biochemical signature in CPD clearly differs from that of BSE or sheep-passaged BSE. Although host factors are known to be able to alter the PrP Sc  signature during interspecies transmission, the BSE profile generally has been preserved in species accidentally or experimentally affected. In principle, CPD also might have derived from scrapie. Dromedaries often are raised along with sheep and goats, sharing common pastures. However, although the absence of an effective surveillance system prevents drawing any conclusions, scrapie has never been reported in Algeria, and a field survey in northeastern Algeria could not provide evidence of the disease . Moreover, the PrP Sc  signature of CPD differed from the classical scrapie case used for comparison . To help clarify the origin and nature of CPD, bioassays in a panel of rodent models are ongoing for a thorough prion strain characterization.\n\n【37】Future investigations of the geographic distribution of CPD will help clarify its origin. If the disease is confined to the dromedary populations of the Ouargla region, a localized event of transmission could be hypothesized. Common-source scrapie epidemics in sheep and goats occurred in the United Kingdom and Italy as a consequence of the use of accidentally contaminated vaccines . However, in the Ouargla region, no vaccination program has been implemented for infectious disease prophylaxis in dromedaries. Intriguingly, dromedary breeders indicate that the only food source other than pasture available to dromedaries in the Ouargla region are the waste dumps widespread in the desert near the oil extraction plants, where dromedaries and small ruminants gather and scavenge . The possibility that dromedaries acquired the disease from eating prion-contaminated waste needs to be considered.\n\n【38】Tracing the origin of prion diseases is challenging. In the case of CPD, the traditional extensive and nomadic herding practices of dromedaries represent a formidable factor for accelerating the spread of the disease at long distances, making the path of its diffusion difficult to determine. Finally, the major import flows of live animals to Algeria from Niger, Mali, and Mauritania  should be investigated to trace the possible origin of CPD from other countries.\n\n【39】Camels are a vital animal species for millions of persons globally. The world camel population has a yearly growth rate of 2.1% . In 2014, the population was estimated at ≈28 million animals, but this number is probably underestimated. Approximately 88% of camels are found in Africa, especially eastern Africa, and 12% are found in Asia. Official data reported 350,000 dromedaries in Algeria in 2014 .\n\n【40】On the basis of phenotypic traits and sociogeographic criteria, several dromedary populations have been suggested to exist in Algeria . However, recent genetic studies in Algeria and Egypt point to a weak differentiation of the dromedary population as a consequence of historical use as a cross-continental beast of burden along trans-Saharan caravan routes, coupled with traditional extensive/nomadic herding practices .\n\n【41】Such genetic homogeneity also might be reflected in _PRNP_ . Studies on _PRNP_ variability in camels are therefore warranted to explore the existence of genotypes resistant to CPD, which could represent an important tool for CPD management as it was for breeding programs for scrapie eradication in sheep.\n\n【42】In the past 10 years, the camel farming system has changed rapidly, with increasing setup of periurban dairy farms and dairy plants and diversification of camel products and market penetration . This evolution requires improved health standards for infectious diseases and, in light of CPD, for prion diseases.\n\n【43】The emergence of another prion disease in an animal species of crucial importance for millions of persons worldwide makes it necessary to assess the risk for humans and develop evidence-based policies to control and limit the spread of the disease in animals and minimize human exposure. The implementation of a surveillance system for prion diseases would be a first step to enable disease control and minimize human and animal exposure. Finally, the diagnostic capacity of prion diseases needs to be improved in all countries in Africa where dromedaries are part of the domestic livestock.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a276cd72-1df0-4581-9177-f456b97cc343", "title": "Novel Picornavirus in Turkey Poults with Hepatitis, California, USA", "text": "【0】Novel Picornavirus in Turkey Poults with Hepatitis, California, USA\nTurkey viral hepatitis (TVH) is a highly infectious disease affecting young turkey poults. The disease is often subclinical, causing minor histologic lesions, and becomes overt when the animals are stressed, resulting in varying rates of illness and death . Mortality rates of up to 25% have been reported . Diagnosis is based on characteristic lesions in the liver, which include multifocal necrosis and mononuclear inflammatory cell infiltrates . Similar lesions may be found in the pancreas. Clinical signs include anorexia, depression, diarrhea, and weight loss compatible with a diagnosis of enteritis, the second most common diagnosis made in turkey poults throughout the United States. Although we cannot with confidence estimate the specific burden of TVH, its economic effects are likely substantial; in the United States, turkey production was valued at $3.71 billion in 2007. The identification of a pathogen and development of specific diagnostics will lead to better understanding of the economic consequences and other effects of TVH.\n\n【1】The disease has been experimentally reproduced in turkey poults by inoculation with material derived from affected animals . A viral basis for TVH has been presumed since its initial description in 1959 because the causative agent passed through 100-nm membranes, was acid stable, was not affected by antimicrobial drugs, and could be propagated in the yolk sac of embryonated chicken eggs . Icosahedral particles of 24 to 30 nm have been found by electron microscopy (EM) in liver lesions of birds  as well as in embryonated turkey eggs  that have been inoculated with material derived from affected birds; however, no agent has been consistently implicated .\n\n【2】### Materials and Methods\n\n【3】##### Animals\n\n【4】Samples from healthy and diseased turkey poults were collected from February 2008 through January 2010 from 8 commercial flocks in California, USA. Flock sizes ranged from 22,500 to 40,000 birds. Clinical signs in diseased poults included anorexia, lethargy, diarrhea, and increased mortality rates. Postmortem analyses revealed livers with white foci and, occasionally, pale patchy areas in the pancreas. Histopathologic examination showed necrosis of hepatocytes and acinar cells of the pancreas and inflammation.\n\n【5】##### High-throughput Pyrosequencing\n\n【6】Total RNA was extracted from 4 sick 25-day-old turkey poults by using TRI Reagent (Molecular Research Center, Inc. Cincinnati, OH, USA) and treated with DNaseI (Ambion, Austin, TX, USA). Two micrograms of DNaseI-treated RNA was reverse transcribed by using Superscript II (Invitrogen, Carlsbad, CA, USA) and random octamer primers with an arbitrary specific anchor sequence as described previously . cDNA was RNase H-treated before random amplification by PCR. The resulting products were purified by using MinElute (QIAGEN, Hilden, Germany) and ligated to linkers for sequencing on a GSL FLX Sequencer (Life Sciences, Branford, CT, USA). Sequences were clustered and assembled into contiguous fragments (contigs) after trimming of primer sequences, and BLAST analysis was applied to compare contigs (or single reads) at the nucleotide and amino acid levels to the GenBank database .\n\n【7】##### PCR and Genome Sequencing\n\n【8】Primers were designed that bridged contigs identified by high-throughput sequencing. Additional primers were selected to resequence the genome with special attention to junctions of the P1 and P2 regions. The PCRs were conducted by using HotStar polymerase (QIAGEN) primers at 0.2 µmol/L each and 1 µL of random hexamer-primed cDNA. The draft genome sequence was confirmed by selecting additional primers to generate ≈1 kb products across the entire sequence for direct dideoxy sequencing (Genewiz, South Plainfield, NJ, USA), applying TaKaRa LA Taq polymerase with GC buffer (TaKaRa Bio, Otsu, Japan) to obtain products in areas that proved difficult to amplify because of potential secondary structures or elevated GC content.\n\n【9】##### Quantitative TaqMan Real-time PCR\n\n【10】Primers and probes for quantitative real-time PCR were selected within the 5′ untranslated region (UTR) of the turkey hepatitis virus (THV) genome by using Primer Express 1.0 software (Applied Biosystems, Foster City, CA). The primer/probe set THVforward1 5′-CACCCTCTAYGGGCAATGT-3′, THVreverse1 5′-TCAGCCAGTCTATGGCCAGG-3′, and THVprobe1 6FAM-5′-TGGATTCCCATCTCACGCGTCCAC-3′-TMR used in assay 1  was chosen on the basis of the initial THV strains sequenced. Primer THVforward2 5′-CACCCTYYAYGGGCAAATGT-3′ and probe THVprobe2 6FAM-5′-ATTCCCATCTCACGCGTCCAC-3′-TMR were later selected to address sequence variation of additional strains and used with THVreverse1 primer in assay 2 . A calibration standard for both assays was generated from strain 2993A by cloning a 571-nt genomic fragment into the pGEM-T Easy vector (Promega Corp. Madison, WI, USA). PCRs were pursued in triplicate by using a StepOnePlus Real-time PCR system (Applied Biosystems), and a standard cycling profile of 45 cycles in a volume of 25 µL containing random hexamer-primed cDNA, 300 nmol/L primer (each), and 200 nmol/L probe. Results were expressed as mean copy number per 300 ng total RNA.\n\n【11】##### Phylogenetic Analysis\n\n【12】Phylogenetic analyses were performed based on THV P1, 2C/3C/3D, and full polyprotein sequence excluding divergent aa 799–1199. Sequences were aligned to selected members of the _Picornaviridae_ family by ClustalW and trees were constructed by using the Molecular Evolutionary Genetics Analysis (MEGA) software version 4.0.2 . A Jukes-Cantor model was applied to calculate distance, and statistical significance was assessed by bootstrap resampling of 1,000 pseudoreplicate datasets.\n\n【13】##### In Situ Hybridization\n\n【14】Viral probes and β-actin control probes were designed and applied according to the QuantiGene ViewRNA protocol by using branched DNA technology (Panomics, Fremont, CA, USA). Twenty viral probes, 17–28 nt long, were selected that cover 500 nt of target sequence in 2B/2C. Forty β-actin probes, 17–26 nt long, were selected covering 873 nt of mRNA sequence. Five-micrometer–thick paraffin-embedded tissue sections were fixed, permeabilized with protease, and hybridized with oligonucleotides conjugated to alkaline phosphatase. After incubation with FastRed substrate, the slides were counterstained with hematoxylin and mounted with coverslips by using Permount (Fisher Scientific, Pittsburgh, PA, USA). Images were acquired by using a Zeiss AX10 Scope AI, ProgRes digital microscope camera and Mac Capture Pro 2.6.0 software (Jenoptik, Jena, Germany).\n\n【15】##### Immunohistochemical Analysis\n\n【16】Glass slides with embedded tissues were heated at 56°C for 10 min and washed in citrus clearing agent (Fisher Scientific) to remove paraffin. The sections were rehydrated through graded alcohol solutions. Endogenous peroxidase activity was blocked by incubation in 0.3% H 2  O 2  diluted in methanol for 30 min. Nonspecific binding was blocked by incubating sections in 10% goat serum in phosphate-buffered saline (PBS) for 1 h at 37°C. Serum specimens from poults with or without disease were added to the sections at 1:1,000 dilutions in PBS for overnight incubation at 4°C. After being washed in PBS, horseradish peroxidase–labeled goat anti-turkey immunoglobulin G (KPL, Gaithersburg, MD, USA) was added at a 1:250 dilution in PBS for 1 h at 37°C. After PBS washes, the Vectastain Elite ABC kit (Vector Laboratories, Burlingame, CA, USA) was used, with 3, 3′-diaminobenzidine tetrahydrochloride as substrate (ImmPACT DAB kit; Vector Laboratories). The sections were counterstained with hematoxylin and dehydrated through graded alcohols. Finally, the slides were mounted with coverslips by using Permount (Fisher Scientific) and examined under a Zeiss AX10 Scope AI light microscope at ×40 magnification (Jenoptik).\n\n【17】### Results\n\n【18】##### Identification of THV\n\n【19】Unbiased high-throughput pyrosequencing of RNA extracted from livers of 4 poults with TVH (animals 2993A, 2993B, 2993C, and 2993D) yielded ≈63,100 sequence reads with a mean length of 285 nt. Seven contigs (average length of 674 nt, comprising 105 sequence reads) and 2 singletons (read lengths of 486 nt and 498 nt) that together yielded 3,182 nt of sequence were identified after primer trimming and assembly. Analysis at the nucleotide level was not informative; however, BLASTx analysis revealed significant similarity to picornavirus sequences at the amino acid level. The remainder of the genome was determined from RNA of poult 2993D by RT-PCR with primers linking the individual contigs and singletons . Applying additional primers in various genome regions, a second 9-kb genomic sequence was generated from another diseased poult (animal 0091.1). Sequence analysis showed that the second strain had the same genome organization as strain 2993D and 96.8% aa and 89.9% nt sequence identity (GenBank accession nos. HM751199 and HQ189775).\n\n【20】##### THV Genome Organization\n\n【21】The THV genome, comprising >9,040 nt and 2,813 aa, is larger than that of equine rhinitis B virus (genus _Erbovirus_ ), the largest known picornavirus genome . The length chiefly reflects the presence of a 1.2-kb sequence at the junction of the P1 and P2 regions in an otherwise typical picornavirus genome . The incomplete 461-nt 5′ UTR includes a 30-nt motif (nt 270–300 of THV) that is identical to duck hepatitis A virus 5′ UTR (Avihepatovirus_ genus), which has a type IV internal ribosome entry site. However, Mfold-modeling of the available sequence did not allow identification of the internal ribosome entry site type present in THV. At the 3′ end, 140 nt of the UTR were recovered (without reaching a poly-A tail) that showed no sequence similarity to other picornavirus 3′ UTRs.\n\n【22】With the exception of a highly conserved Gxxx\\[T/S\\] motif  that in some picornaviruses permits myristoylation, little sequence conservation occurs in the VP0 of THV with respect to other picornaviruses. Cleavage at an upstream Q 73  /A would not generate an N terminal G, cleavage at this site is not supported by NetPicoRNA prediction, and the presence of a leader sequence is unclear . Therefore, as in parechoviruses, hepatitis A virus, and avian encephalomyelitis virus, this site is probably not functional for myristoylation in THV . NetPicoRNA analysis did not indicate a VP2/4 maturation cleavage as found in avihepatoviruses, kobuviruses, and parechoviruses. Sequence comparisons of the P1 region, mainly driven by recognizable sequence conservation in VP3 and a few regions in VP1, indicate the highest amino acid identity (%) with turdiviruses, unclassified picornaviruses recently identified from wild birds . Homology in P1 to pfam sequence cluster cd00205 “picornavirus capsid protein domain like”  was observed between amino acid residues 109–267, 391–539, and 622–768 . Potential cleavage sites within P1 are predicted after Q 388  (VP0/3) and Q 554  (VP3/1).\n\n【23】Protein 2A motifs in THV are conserved with respect to kobuviruses after a predicted cleavage site Q 1199  . However, the sequence lacks the trypsin-like protease motifs that allow autocatalytic cleavage at the N-termini of enteroviruses and sapeloviruses, as well as the NPGP motif that facilitates C-terminal cleavage in aphthoviruses, avihepatoviruses, cardioviruses, erboviruses, senecaviruses, and teschoviruses. The predicted 2A sequence of THV resembles Hbox-NC motifs in hepatoviruses, kobuviruses, parechoviruses, and tremoviruses (H 1250  WGI, N 1310  C followed by a hydrophobic region L 1332  \\-V 1350  ). The THV 2A protein may be generated by cleavage at conserved protease sites; however, multiple cleavage sites between P1 and P2 are predicted by NetPicoRNA . Cleavage after Q 798  and Q 1199  appears likely because it would generate VP1 and 2A products that align with other picornavirus proteins. As a result, 1 or 2 additional proteins (depending on cleavage at Q 880  ) may be produced from this genome region that have no homology to any viral product recorded in GenBank. Multiple 2A1, 2A2, or 2A3 protein products with undefined function are described in Ljungan virus, seal picornavirus, and duck hepatitis virus genomes . Although we predict, based on alignment analyses, that the C-terminal cleavage of 2A occurs at Q 1398  , this prediction is poorly supported by NetPicoRNA. 2B and 2C have sequence homology to kobuvirus sequences, particularly in a conserved 2C helicase domain G 1724  SPGVGKS that aligns to the PF00910 RNA helicase domain.\n\n【24】Although no sequence homology to any picornavirus record in GenBank was found for THV 3A, 3B displays a conserved tyrosine in position 3 as well as a conserved glycine in position 5. The THV 3C protease contains the active site motif G 2298  MCGA, which is consistent with 3C proteases of other picornaviruses, and shows highest homology to cosaviral 3C (PF00548 3C cysteine protease \\[picornain\\] aligning to aa 2153–2321). The identity of a 472-aa sequence at the 5′ end of the genome as THV 3D is supported by homology of aa 2355–2809 to PF00680 RNA-dependent RNA polymerases, and the conservation of positive-strand viral RNA-dependent RNA polymerase motifs A–E  .\n\n【25】The assembled THV genome sequences were used to reanalyze the initial read library generated by unbiased high-throughput pyrosequencing. This analysis confirmed the presence, and overlap with, the adjacent sequences of divergent 2A and 3A region reads in the initial dataset.\n\n【26】Phylogenetic analyses based on amino acid sequence of the most informative genome regions 2C/3C/3D, the P1 region, and the full polyprotein sequence (excluding the nonconserved aa 799–1199) showed THV as a distinct species separate from classified genera . The 2C/3C/3D analysis indicates THV in an ancestral position to kobuviruses, klasseviruses, and turdiviruses, the viruses most related to THV.\n\n【27】##### THV RNA Load in Liver, Bile, Serum, and Cloacal Swab Specimens\n\n【28】Two TaqMan real-time PCR assays, both targeting the 5′ UTR, were developed to quantitate viral RNA load in affected animals. Assay 1  was replaced by assay 2  after the characterization of additional THV strains indicated divergent sequences.\n\n【29】In the liver samples from turkeys with TVH, viral RNA typically exceeded 10 5  copies/300 ng of total RNA; only 1 animal had a lower load (animal 0690). No viral RNA was detected in livers from non-diseased control animals. Analysis of animals 1813.3 and 1999 indicated presence of the virus in the intestine and pancreas as well as in the liver. Cloacal swab samples, bile, and serum from 28-, 29- and 39-day-old poults with disease were also analyzed. Five of 9 cloacal swab specimens from TVH-affected animals were positive for viral RNA . Viral RNA >10 4  copies/300 ng total RNA was detected in the bile of all 5 poults with TVH tested. Viral RNA was also found in serum samples from 8 of 18 poults with TVH.\n\n【30】Cloacal swabs were analyzed from 32- and 39-day-old turkey poults that did not have TVH according to histopathologic findings. Viral RNA was detected in 2 of 10 animals tested .\n\n【31】##### Localization of THV RNA and Protein in Liver\n\n【32】The distribution of THV RNA was examined in liver sections of affected and unaffected poults by in situ hybridization. THV signal was found in the cytoplasm of hepatocytes of affected poults. No signal was observed in healthy poults . A hybridization signal with a control β-actin probe was present in both affected and healthy poults; however, β-actin signal was less pronounced in affected poults. In situ hybridization also indicated viral RNA in 1 intestinal sample from the 0091 animals (not shown), in line with the real-time PCR data obtained for animals 1813.3 and 1999 .\n\n【33】Serum from TVH-affected poult 394.9, which was positive by PCR for THV RNA , was used on paraffin-embedded tissues for immunohistochemical analysis. The serum showed reactivity when tested in livers from affected poults. No reactivity was observed with healthy poults .\n\n【34】### Discussion\n\n【35】The _Picornaviridae_ , a family of small, nonenveloped viruses with a positive-sense, single-strand RNA genome, currently consist of 12 genera. Recent additions to the family include salivirus NG-J1 and human klasseviruses identified in pediatric stool samples , cosaviruses , and an unclassified seal picornavirus 1 . Our phylogenetic analyses indicate that THV, although distantly related to viruses of the _Kobuvirus_ genus as well as to unclassified klasseviruses/saliviruses and recently identified turdiviruses, is distinct from known picornaviruses.\n\n【36】Most peculiar is the unique 2A region of THV. THV appears to encode multiple 2A products. Although multiple 2A products have also been described for Ljungan virus, seal picornavirus, and duck hepatitis virus , the 2A region of THV shows no sequence homology to these products or to any other picornavirus sequence currently in GenBank. Also remarkable is the lack of sequence homology of THV 3A to picorna- or other virus sequences. Whereas VP3 (representing P1) is closest to turdivirus 1 (% aa identity), 2B/C (representing P2) is closer to Aichi virus (%) and 3C/D (representing P3) is closer to turdivirus 3 (%). Thus, THV shows classic features of known picornaviruses but also unique features that do not support inclusion of THV into existing taxa of the _Picornaviridae_ family.\n\n【37】Turkey poults with TVH may have diarrhea and pancreatitis as well as hepatitis. Picornavirus particles in the feces of animals with TVH have been described . Accordingly, we detected THV RNA in the intestine, pancreas, bile, and cloaca samples as well as the liver. These findings are consistent with a fecal–oral route for transmission. We also detected THV RNA in cloacal swab samples from 2 of 10 asymptomatic poults. Because these animals were housed on a farm with history of TVH, this finding is consistent with reports suggesting subclinical infections . The advent of a noninvasive screening test for THV may aid in disease containment.\n\n【38】We molecularly characterized a picornavirus in turkey viral hepatitis and linked it to disease through measurements of load and tissue distribution, viremia and a humoral immune response to the agent. On the basis of the data presented here, we suggest that THV represents a new species in the order Picornavirales and a likely candidate for causing TVH.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aacbf14a-5589-4802-9507-12a8ff1aa849", "title": "Strategies Implemented by 20 Local Tobacco Control Agencies to Promote Smoke-Free Recreation Areas, California, 2004-2007", "text": "【0】Strategies Implemented by 20 Local Tobacco Control Agencies to Promote Smoke-Free Recreation Areas, California, 2004-2007\nAbstract\n--------\n\n【1】**Introduction**Since 2000, local jurisdictions in California have enacted hundreds of policies and ordinances in an effort to protect their citizens from the harmful effects of secondhand smoke. We evaluated strategies used by state-funded local tobacco control programs to enact local smoke-free policies involving outdoor recreational spaces. **Methods**The Tobacco Control Evaluation Center analyzed 23 final evaluation reports that discussed adopting local smoke-free policies in outdoor recreational facilities in California. These reports were submitted for the 2004 through 2007 funding period by local tobacco control organizations to the California Department of Public Health, Tobacco Control Program. We used a comparative technique whereby we coded passages and compared them by locale and case, focusing on strategies that led to the enactment of smoke-free policies. **Results**Our analysis found the following 6 strategies to be the most effective: 1) having a “champion” who helps to carry an objective forward, 2) tapping into a pool of potential youth volunteers, 3) collecting and using local data as a persuasive tool, 4) educating the community in smoke-free policy efforts, 5) working strategically in the local political climate, and 6) framing the policy appropriately. **Conclusion**These strategies proved effective regardless of whether policies were voluntary, administrative, or legislative. Successful policy enactment required a strong foundation of agency funding and an experienced and committed staff. These results should be relevant to other tobacco control organizations that are attempting to secure local smoke-free policy.  \n\n【2】Introduction\n------------\n\n【3】In the past 2 decades, a growing body of scientific evidence has demonstrated the dangers of secondhand smoke . New evidence showing that secondhand smoke in outdoor areas also presents health risks  has elevated efforts to control tobacco use in outdoor spaces and recreational areas to protect people from exposure to secondhand smoke. Furthermore, prohibiting smoking in parks, on beaches, and in other outdoor recreation areas reduces tobacco litter (ie, cigarette butts) and its resulting toxins, which can be harmful , and discourages adults from modeling smoking, which may influence youth . Finally, most US citizens support smoke-free policy in many outdoor recreation areas . For all these reasons, securing outdoor smoke-free policy has become a recommended strategy by the Centers for Disease Control and Prevention (CDC) for states and local jurisdictions , and California’s tobacco control program likewise supports community policies to restrict outdoor area smoking . Policy theorists and tobacco control researchers provide some guidance in their analysis of factors that lead to successful local policy change . Many variables may explain adoption of policy, including the support of key decision makers, emulation of comparable jurisdictions , and alignment of the policy with the local political climate . Affluent communities appear more likely to support costly local policies . There is also evidence that the presence of “political entrepreneurs” and strong advocacy organizations facilitate policy adoption . Finally, the likelihood of local policy adoption of smoking-related laws is higher when the local health community promotes the efforts . Since 2000, local jurisdictions in California have enacted hundreds of policies and ordinances in an effort to protect their citizens from the harmful effects of secondhand smoke. The California Tobacco Control Program (CTCP) funds tobacco control programs of county health departments statewide, a small number of metropolitan areas, and selected community-based organizations, and supports the Tobacco Control Evaluation Center (TCEC) of the University of California, Davis. CTCP coordinates a comprehensive approach of multifaceted strategies; its goal is “to change the broad social norms around the use of tobacco,” and by doing so “creating a social environment and legal climate in which tobacco use becomes less desirable, less acceptable, and less accessible” . Local projects focus on 1 or more programmatic objectives, such as implementing tobacco retail licensing, limited smoking in apartment complexes, and increasing the number of smoke-free outdoor spaces and recreational facilities. At the conclusion of their work, they are required by CTCP to submit final evaluation reports (FERs), which TCEC analyzes and scores. Despite the evidence of the many external factors that are related to adoption of tobacco control policies, little is known about the strategies local organizations use to change local policies. Our objective was to analyze local policy initiatives to identify common strategies used to adopt smoking policies in outdoor spaces and recreational areas at the local level in California.  \n\n【4】Methods\n-------\n\n【5】FERs followed a standard format that included an abstract, introduction, intervention activities, evaluation methods and results, and conclusions and recommendations. Each main section included subsections of required information. TCEC scored FERs to ensure accountability and to offer feedback to the local projects for future tobacco control evaluations. Of the 400 FERs submitted, we selected only those with an objective related to adopting smoke-free policy in outdoor spaces and recreational areas for analysis. Outdoor spaces and recreational areas included parks, beaches, piers, sport stadiums, and places where other local community events are held, including fairs, rodeos, and parades. Local projects submitted FERs in June and July of 2007, and we analyzed them between May and September of 2010. We used a qualitative data analysis with an evolving coding scheme  to identify strategies employed in promoting smoking policies in outdoor spaces and recreational areas. Local FERs were analyzed, and variables emerged during the coding process. The data used for this study were collected from 23 FERs that 20 local tobacco control projects submitted to CTCP for the 2004 through 2007 funding period. After identifying the relevant FERs, a reviewer used open coding, whereby “similar events/actions/interactions are grouped together to form categories and subcategories” . As in grounded theory field work , categories emerged from the reading of the first papers through “questioning and constant comparison” and were modified and expanded with the reading of subsequent papers. This quasi-inductive, pattern-level analysis considered the frequency, weight, and contextual factors of items across FER data . The reviewer performed several readings of all reports. After the reviewer had coded all reports, she created summary tables and narratives. Then, a second reviewer read all reports to confirm or dismiss any categorization, to look for trends that were missed, and to seek evidence that conflicted with the first reviewer’s findings . Two other members of the research team then compared the results to findings in existing literature.  \n\n【6】Results\n-------\n\n【7】Nineteen of the 20 projects passed some form of policy to promote smoke-free recreational facilities . The 1 project unable to secure passage of an official policy did succeed in having its county board of supervisors approve a preliminary plan. Some projects advocated for voluntary or administrative changes that could be made without a formal vote, such as making a music festival or a ski resort smoke-free. Other projects sought policy changes with local fair boards to add smoke-free days to county fairs or rodeos, which typically required a formal vote of the respective boards. Policies pursued by projects working with city councils ranged from seeking a single smoke-free park to enacting smoke-free policies that affected all parks and recreational areas in the city’s jurisdiction. In 2 cases, policies legislated at the county level provided the impetus to municipalities in the county to create and enact their own citywide ordinances. Projects unable to secure stringent legislative policies sometimes decided to pursue more “doable” administrative or voluntary policies in their place. Securing passage of policy was often difficult. The FERs described various challenges, including polarization related to perceived economic and individual rights issues, difficulties in gaining access to policy makers, internal organizational barriers such as staff turnover, and local political orientation that centered around conservative versus liberal viewpoints of the proposed policies . Six strategies for success emerged from our analysis: 1) having a “champion” who helped to carry an objective forward, 2) tapping into a pool of potential youth volunteers, 3) collecting and using local data as a persuasive tool, 4) educating the community in smoke-free policy efforts, 5) working strategically in the local political climate, and 6) framing the policy appropriately. \n\n【8】### Having a champion\n\n【9】A crucial factor in a project’s success was identifying a “champion,” someone who was a member of, or was respected by, the targeted decision makers. FERs repeatedly mentioned examples of champions helping to carry a policy forward. An FER from a coastal county noted, “For us, the most crucial component of a successful smoke-free beach/pier policy effort has proven to be a strong champion.” Champions typically had inside knowledge of the decision-making body and had the power to influence the outcome of policy adoption or provided access to policy makers, a critical step in adopting policy . A rural county wrote, “The fair director was crucial for us. He . supported the concept of a smoke-free fair and was key to developing the good working relationships with the fair.” \n\n【10】### Tapping potential youth volunteers\n\n【11】Projects recruited youth volunteers from local universities, colleges, and high school groups already engaged in antidrug work. These students lent their enthusiastic presence to all phases of the campaign: planning, collecting data, and making presentations to the policy makers. Youth were often seen as the primary benefactors of such laws in places such as parks, beaches, and stadiums and for events such as fairs and carnivals, so successful projects often framed their policy arguments around the safety of children. Having the youth make these presentations was a strategic coup. One county’s FER stated \n\n【12】> Project staff showed great political acumen in collecting data and having youth present the data to the city councils. They operated on the assumption that it would be difficult for public officials and role models to refuse to act in what was demonstrated to be clearly in the best interest of both youth and the public at large. \n\n【13】### Using local data as a persuasive tool\n\n【14】Before meeting with policy makers, most local agencies gathered national and state fact sheets and polling results from tobacco-related networks. This information was augmented with local data such as public opinion surveys to demonstrate community support and key informant interview data that provided insights into policy makers’ positions on the same issues. FERs made it clear that local decision makers found data gathered from the community itself persuasive. For example, the effect on policy makers of 30 student volunteers gathering more than 22,000 cigarette butts in 2 hours was profound and dramatically underscored the need for smoke-free policy. This and other forms of local data also helped establish a baseline for later comparisons. \n\n【15】### Educating the community\n\n【16】Community education was essential in securing support for local outdoor smoke-free policies. Local projects set up information booths at public events and made presentations to community organizations, vividly describing the harmful effects of tobacco litter and secondhand smoke. Many projects took full advantage of local media (ie, newspapers, radio, and television) by using inventive approaches to bring the need for tobacco control directly to the community and simultaneously building local support and placing pressure on decision makers. One report from a rural county described this strategy as follows: \n\n【17】> The strategic timing of ads, articles, letters to the editor, and op-eds helped to propel smoke-free parks onto the community’s and elected officials’ radar leading up to critical policy discussions and votes. \n\n【18】### Working strategically in the local political climate\n\n【19】Each community faced unique issues related to local policy makers and the political climate. Key informant interviews with policy makers, influential constituents, and other community insiders were crucial in developing and maintaining an effective campaign, as one rural countys FER explained: \n\n【20】> Key informant interviews at the prepolicy adoption phase provided us with some of the challenges facing our adoption of smoke-free outdoor area policies. Respondents also gave us . their opinion on what strategies might work to counter these challenges. For instance, after conducting these interviews we were able to obtain specific information about some of the city officials who we would need to focus our efforts on because they were seen as opposing smoke-free policies due to special vested interests. \n\n【21】Several FERs noted that constituencies in the same county aligned along different, often opposing, viewpoints, requiring carefully adapted approaches to accomplish the desired results. Recognizing and respecting the political climate that shaped the thinking of each decision maker, rather than attempting to use a generic, one-size-fits-all strategy, was necessary. Several FERs described recasting the argument for smoke-free parks and beaches to frame the issue less around the dangers of secondhand smoke and more around environmental issues caused by cigarette litter. Particularly in coastal regions and among more affluent populations, the environment “held more weight,” as one FER put it. Successful strategic shifts often resulted from a deeper understanding of the local political climate. \n\n【22】### Framing the policy appropriately\n\n【23】Three tactics emerged as particularly successful framing devices in persuading decision makers to support the proposed policy: 1) demonstrating constituent support for the proposed policy, 2) conveying how the proposed policy protected children and the environment, and 3) demonstrating that precedent existed. The most successful presentations that the FERs described tended to incorporate these 3 ideas. Presentations, generally led by project directors or coordinators, frequently involved the active support of coalition members and community volunteers bolstered by the legitimizing presence of representatives from local chapters of national organizations such as the American Cancer Society and the American Lung Association. A strong emotional appeal for the protection of ecological values and young children from tobacco litter, backed by consistent data showing that the community favored tobacco control, frequently succeeded in eliminating opposition to the proposed policy. One county FER reported \n\n【24】> Public presentations afforded us the ability to publicly deliver our data regarding exposure to secondhand smoke in outdoor areas, and our coalition members who helped us were able to gain a measure of public accountability from the officials and staff to address the issue. \n\n【25】Local projects also found that decision makers were swayed by legitimizing precedents, such as evidence of public acceptance for the same policy in neighboring communities or past tobacco-related successes in their own jurisdiction. One project director from a county with an urban populace wrote in the countys FER \n\n【26】> In every case, mention was made of other communities that had implemented similar ordinances and measures. This effect cannot be overstated: no local action would have been likely without the evidence of reasonableness afforded by precedent. The presence of pending state-level legislation provided a similar legitimacy to local control efforts. \n\n【27】Discussion\n----------\n\n【28】For the most part, our findings confirmed the results from other studies on policy change. For instance, several communities benefited from being near cities or counties that had already passed outdoor smoking bans, normalizing the issue for policy makers . The policy literature concludes that less affluent rural areas may face resistance to tobacco control regulations , which may explain why projects located in rural, less affluent counties were more likely to pursue weaker voluntary and administrative policy changes. Our findings that local health departments tailored policies to the local political climate and that they enlisted an influential champion to take up the cause follow the conclusions of existing literature , as does the influence of a health agency in the advocacy process . Also consistent with findings from other studies on tobacco control policy at the local level , we found no reports of tobacco industry lobbying. This study identified 2 effective strategies used in local policy change not previously mentioned in other studies: 1) using local data to advance a policy position to decision makers, particularly displaying cigarette litter as a visual representation of the environmental problems that smoking causes, and 2) training youth advocates to collect cigarette litter and to present their findings to policy makers. These strategies are relevant for environmental policy change, which tended to appeal to constituents and decision makers across the political spectrum. In essence, these strategies enabled local projects to frame the issue around the more intuitively compelling issue of environmental protection as opposed to the less persuasive issue of secondhand smoke, which is perceived by the public and by policy makers as an indoor, not an outdoor, health problem. In many cases, policy makers who opposed the latter point on the basis of economic or individual-rights arguments  could still agree with a proposed outdoor smoke-free policy in the context of more wide-ranging local environmental concerns. Likewise, using youth volunteers to present the policy argument to decision makers tended to fortify the value of these policy positions. This study also offers a more nuanced view of local tobacco control policy change, where policies are enacted not only by city councils or county boards of supervisors, but also by rodeo and fair boards, organizers of outdoor fairs and concerts, and administrators of parks and recreation districts. Policies can be legislative changes passed by elected officials, voluntary changes by concert organizers, or administrative changes by a host of different local agencies and organizations. The range of voluntary to legislative policies is in keeping with the “continuum of tobacco control policy” concept of Francis et al that progresses over time from voluntary to enforceable legislative policy . One unexpected finding was the similarity of many of the policy strategies used in outdoor spaces and recreation areas to policy strategies used in other tobacco control policy campaigns. Blaine et al reached similar conclusions from their evaluation of 7 communities that successfully used similar methods to reduce youth access to tobacco. The authors concluded that a general set of strategies could be applied successfully to almost any “narrowly focused public health problem” . We also did not anticipate that 19 of the 20 cases we studied would successfully enact smoke-free policies. This finding could be interpreted to mean that policy to create smoke-free outdoor areas is simple to enact; alternatively, we would argue that the projects in this study correctly assessed their local political climates and tailored their approaches accordingly. They were sufficiently sophisticated to propose voluntary or administrative policy change when they anticipated resistance to outdoor smoke-free policies, recognizing the need to postpone plans for adopting more stringent legislative policy. One limitation of this study is that the findings are not necessarily generalizable, in part because of the unique support the local projects in this study received from the state tobacco control program. CTCP guarantees minimum funding to local health departments, encourages local policy change to reduce exposure to secondhand smoke, and hosts frequent training sessions for staff on community organizing and assessment, factors that give these projects certain advantages. Another limitation is that the number of projects for this comparative analysis was small. However, these 20 projects covered more than 200 communities in California, so the effect of their enacted policies is potentially large. Smoke-free outdoor policies are a relevant new area in tobacco control efforts and are consistent with CDC’s emphasis on eliminating tobacco smoke from public areas . To our knowledge, this study is the first to investigate the internal strategies that advance local smoke-free outdoor policies. More research on this topic is needed to determine whether these strategies are equally effective when used by nongovernmental agencies that are funded for a limited time and to isolate the factors that explain the diffusion of new outdoor smoke-free policies across jurisdictions. A careful examination of the effect of outdoor policies on protection from secondhand smoke, reduction of negative modeling for children, and reduction of litter and toxins from cigarette butts would add public health legitimacy to this policy approach.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9ef8fa02-42c4-4f9f-945c-a430e1d591b7", "title": "HIV Transmission in Correctional Facility", "text": "【0】HIV Transmission in Correctional Facility\nCorrectional facilities house a disproportionate number of HIV-infected inmates  and are a setting for unprotected sexual intercourse . Although symptoms of acute retroviral syndrome develop in up to 89% of persons newly infected with HIV , the timely recognition and diagnosis of primary HIV infection and initiation of antiretroviral treatment before HIV seroconversion have rarely been reported from a correctional facility.\n\n【1】### The Case\n\n【2】In October 2003, a man with a history of noninjection multiple-drug abuse (including methamphetamine) was admitted to a detention center (regional jail). His pre-incarceration history included unprotected sex with men in the community, most recently in April 2003; however, results of multiple serologic assays for HIV performed in the community had been negative, most recently in June 2003 and December 2002. He had an unremarkable past medical history.\n\n【3】On December 31, 2003, this patient came to the correctional facility's medical clinic with perianal and rectal discomfort; perianal condylomata were present. He reported that during December he had consensual, unprotected, receptive anal intercourse with 2 male inmates at the correctional facility; both of these inmates had chronic HIV infection. One, who was not receiving antiretroviral treatment, had a plasma HIV RNA level of 53,000 copies/mL; the second, who was receiving antiretroviral treatment, had a plasma HIV RNA level of 92 copies/mL. On January 6, enzyme immunoassay (EIA) testing of the index patient for HIV was negative.\n\n【4】On January 9, the patient came to the medical clinic with fever, sore throat, myalgia, headache, vertigo, nausea, and vomiting. His oral temperature was 40.7°C, and he was profusely diaphoretic. Posterior pharyngeal erythema was present, as well as tender, minimally enlarged, anterior cervical lymphadenopathy. Laboratory testing showed plasma HIV RNA of 436,000 copies/mL; CD4+ T-lymphocyte count of 616 cells/μL (%); serum alkaline phosphatase 183 IU/L; and negative serologic test results for hepatitis A, B, and C viruses. He denied participating in tattooing or injection drug use.\n\n【5】On January 13, he reported vertigo and urinary retention; his temperature was 39.4°C, and he was ataxic. Acute urinary retention required urethral catheterization. On January 14, physical examination showed bilateral horizontal nystagmus and perianal ulcerations; plasma HIV RNA was >750,000 copies/mL. On January 15, a second EIA for HIV was negative; however, a Western blot of that serum sample showed an equivocal p24 band.\n\n【6】On January 16, he reported difficulty defecating and urinating. Physical examination showed a scattered macular exanthem of discrete erythematous macules on the trunk and extremities with involvement of the palms; oral mucositis; a tender prostate; and a friable, mildly inflamed anal mucosa with some ulcerations and excoriations. His persistent urinary retention required short-term urethral catheterization. Swab cultures of the rectum for herpesvirus, _Chlamydia_ , and _Neisseria gonorrhoeae_ were negative. Serologic testing for syphilis was negative.\n\n【7】On January 20, a CD4+ T-lymphocyte count drawn on January 15 showed 338 cells/μL (%); plasma HIV RNA level was 234,000 copies/mL. His exanthem had become maculopapular, diffuse, and pruritic. Antiretroviral therapy with efavirenz, zidovudine, and lamivudine was initiated at the correctional facility to treat his primary HIV infection. A genotype of his pretreatment HIV isolate collected on January 14 showed sensitivity to all antiretroviral agents.\n\n【8】On January 21, he complained of paresthesias involving the tips of his fingers. On January 22, he was able to urinate spontaneously. On January 30, he reported a poor appetite; his weight had decreased 1.4 kg since January 16; physical examination showed a resolving, salmon-colored macular exanthem across the trunk and upper and lower extremities and resolution of his oral and anal lesions.\n\n【9】On February 4, his plasma HIV RNA was 2,463 copies/mL, and his CD4+ T-lymphocyte count was 1,575 cells/μL. An EIA for HIV was positive, and a Western blot was positive with bands present for p24, gp40, p55, and gpl60. On February 9, he reported that his appetite had recovered and his paresthesias had resolved; his weight had increased 0.5 kg since January 30. On March 19, his plasma HIV RNA was <50 copies/mL, and his CD4+ T-lymphocyte count was 1,056 cells/μL. On April 1, the court ordered that he be released, and he moved to another state.\n\n【10】### Conclusions\n\n【11】In April 2005, Lambert et al. reported concerns that a resurgence of HIV/AIDS may be imminent, fueled in part by increasing indicators of high-risk behavior in the gay and bisexual population. The March 2005 report by Markowitz et al. regarding men who have sex with men, use of methamphetamine, and transmission of HIV underscores these concerns. The high prevalence of HIV infection in overcrowded and understaffed correctional facilities further accentuates these concerns and poses a public health challenge.\n\n【12】On December 31, 2002, 2.0% of state prison inmates were positive for HIV ; among interviewed jail inmates, 1.3% disclosed they were HIV positive. Estimates of the proportion of inmates who indulge in homosexual intercourse while in prison range from 2% to 65%, and most of this sexual contact is likely unsafe because few correctional facilities address the issue of intraprison sex or distribute condoms . Nevertheless, inmate-to-inmate transmission of HIV has rarely been documented. Taylor et al. proposed that the paucity of evidence for transmission of HIV infection within correctional facilities is probably accounted for by the difficulties in determining the time of HIV seroconversion in relation to the period of incarceration, rather than by the rarity of the event.\n\n【13】Krebs and Simmons  used surveillance data from a 22-year period (January 1, 1978–January 1, 2000) to identify inmates who contracted HIV while incarcerated in the Florida state prison system. They reported that a minimum of 33 inmates contracted HIV while in prison, compared to 238 who contracted HIV after leaving prison; inmates were more likely to have contracted HIV in prison by having sex with other men than through injection drug use.\n\n【14】Additional reports of HIV transmission in correctional facilities have been published from Illinois (HIV seroconversions) , Nevada (seroconversions) , Maryland (seroconversions) , Australia (seroconversion) , and Scotland . Yirrell et al. determined that 13 inmates had acquired HIV infection by sharing needles during their incarceration.\n\n【15】Acute retroviral syndrome and primary HIV infection may be frequently unsuspected by the evaluating clinician because the signs and symptoms are relatively nonspecific. However, within correctional facilities, the diagnosis of primary HIV infection should be considered in the differential diagnosis of any inmate with an acute febrile illness associated with pharyngitis and mucocutaneous lesions. Our report is limited in that virus was not sequenced to document transmission between inmates.\n\n【16】Early diagnosis of primary HIV infection can lead to successful antiretroviral intervention  and prevention of secondary transmission. Whether antiretroviral treatment of acute HIV infection results in long-term virologic, immunologic, or clinical benefit is unknown. In October 2005, the US Department of Health and Human Services Clinical Practices Panel noted that antiretroviral treatment of acute HIV infection is optional. If the clinician and patient elect to treat acute HIV infection with antiretroviral therapy, treatment should be implemented with the goal of suppressing plasma HIV RNA to below detectable levels; resistance testing at baseline will likely optimize virologic response .\n\n【17】We urge correctional facilities to address the issue of unprotected sex among inmates and the associated transmission of sexually transmitted diseases within institutions . In 2001, Wolfe et al. reported that from 1991 to 1999, >5 outbreaks of syphilis occurred in Alabama prisons; multiple concurrent sex networks involving 4, 7, and 10 inmates were identified in the 1999 outbreak. Wolfe et al. recommended that condom distribution should be used to control sexually transmitted disease in correctional facilities. Nevertheless, in 2006, <1% of US correctional facilities provide inmates with condoms. Reasons for not providing condoms include the conflict with policies forbidding sexual intercourse (or sodomy) and the potential for condoms to be used as weapons or to smuggle contraband . In contrast, condoms are available to inmates in all Canadian federal prisons and some provincial prisons; few problems related to condom distribution have been reported from those systems . Wolfe et al. proposed that providing condoms to prisoners may yield additional public health advantages beyond the prison walls if exposure to and experience with condoms in this setting translate into increased use after release.\n\n【18】Correctional staff and inmates should be educated about the consequences of unprotected sex and the signs and symptoms of acute retroviral syndrome. Because many correctional systems contract for medical care, and because staff turnover rates are high, annual education should be implemented. Education for staff who screen sick inmates is critical , and all inmates should have access to HIV counseling and testing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c45aa478-c189-442b-a203-786b9f3ceb2e", "title": "Achieving Operational Hydrologic Monitoring of Mosquitoborne Disease", "text": "【0】Achieving Operational Hydrologic Monitoring of Mosquitoborne Disease\nOperational systems that can accurately monitor and predict mosquitoborne disease transmission continue to be needed. Because mosquitoes and the pathogens they transmit are sensitive to environmental conditions, 1 approach has been to use our ability to monitor and predict environmental variability and our understanding of mosquito and mosquitoborne pathogen response to that variability to monitor and predict mosquitoborne disease transmission. This reasoning assumes that we can monitor and predict environmental variability over large areas at the scale at which it affects mosquitoborne disease transmission and that the relationships between environmental variability and mosquitoborne transmission response are clear and stationary. Here we review the recent developments of operational systems that use hydrologic variability to monitor mosquitoborne disease transmission.\n\n【1】### Rainfall and Mosquitoborne Disease\n\n【2】Many mosquito species depend on the availability of water. The first 3 stages of the mosquito life cycle (egg, larvae, and pupae) are aquatic. Consequently, mosquito abundance and the transmission of many mosquitoborne pathogens can be affected by hydrologic variability, in particular, fluctuations in the water cycle that alter the availability of suitable aquatic habitats. To explore these effects, researchers have long looked for associations between rainfall variability and mosquito abundance  and mosquitoborne disease incidence . Although using rainfall as an explanatory hydrologic variable is convenient, the physical effects of precipitation on surface conditions are multiple, and the responses of different mosquitoes and mosquitoborne pathogens to these effects are varied. As a result, establishing statistically significant and stationary relationships between precipitation and mosquito abundance or mosquitoborne disease transmission is difficult.\n\n【3】Rainfall has 2 principal influences on the mosquito life cycle: 1) the increased near-surface humidity associated with rainfall enhances mosquito flight activity and host-seeking behavior, and 2) rainfall can alter the abundance and type of aquatic habitats available to the mosquito for oviposition. The first influence can increase mosquito abundance by accelerating the reproductive cycle, which requires mating, host-seeking, and blood-feeding flights. The second influence, however, has less certain consequences. Rainfall increases the wetness of soil near the surface and can expand saturated lowland areas. As a result, the moist, humid habitats preferred by many mosquito species for oviposition, such as swamps and floodwaters (e.g. puddles, water-filled divots), may increase in abundance. This change may favor an increase of mosquito species abundance in these habitats. Such changes in mosquito species composition, abundance, and age structure may then lead to an increase in local disease transmission.\n\n【4】However, the availability of suitable mosquito habitats is not a simple linear function of rainfall. Surface wetness depends on a number of environmental conditions other than precipitation, including antecedent wetness, soil type, and rates of evapotranspiration (i.e. combined evaporation and transpiration). Furthermore, excessive rainfall can decimate some mosquito populations by flushing larval habitats. Other mosquito species can benefit from drought conditions such as when streams dry up and pools more suitable for oviposition form in riverbeds, or when standing waters become eutrophic with increased organic content, which provides additional food for mosquito larvae.\n\n【5】Further complications arise when attempts are made to associate rainfall with mosquitoborne disease incidence. Mosquitoborne disease transmission is related most directly to the number of infected mosquitoes able to transmit disease and not to the total number of biting mosquitoes present in a population . As a consequence, increased mosquito abundance does not necessarily increase mosquitoborne disease transmission. As mosquito abundance increases, mosquito infection rates must also increase if disease transmission risk is to increase substantially; this requires that newly emerged mosquitoes acquire pathogens and become infective.\n\n【6】### Monitoring Mosquito Breeding Habitats\n\n【7】The response of mosquito populations to changes in precipitation and the effects of such changes on mosquitoborne disease transmission are quite complex and variable. However, these responses might be better elucidated if mosquito-breeding habitat availability, the variable for which precipitation is principally a proxy, could be monitored directly.\n\n【8】Several studies have examined how water management practices (e.g. irrigation, damming) affect anopheline density and malaria incidence. Surface waters on rice-cultivated land were associated with _Anopheles gambiae_ density in the Ivory Coast ; anopheline densities in Thailand were associated with rice paddy fields ; and in Peru, irrigation around villages and houses played a role in determining human malaria risk . In Tanzania, _An. arabiensis_ densities were 4 times higher in villages with rice cultivation, but malaria exposure was lower because of greatly decreased sporozoite rates among this mosquito population . This last study again illustrates some of the complexity underlying the relationship between mosquito abundance and disease transmission risk.\n\n【9】Although the effects of irrigation and water control are clearly important, for many disease systems natural surface water variability is likely an even greater determinant of vector density and mosquitoborne disease transmission rates. However, because monitoring surface water has traditionally been difficult, relatively few studies have explored these relationships. In Uganda, malaria incidence among children was associated with the proximity of their homes to swamps and streams that served as mosquito breeding sites . In Sri Lanka, the abundance of the primary malaria vector, _An. culicifacies_ , has been linked to the drying of riverbeds .\n\n【10】Because ground observations of surface water prevalence are time-consuming and difficult to carry out over large areas, an attractive alternative has been to use remote sensing measurements of land surface wetness. Many such top-down studies have associated the abundance of vectors or vectorborne disease incidence by using satellite imaging . Such investigations have generally used vegetation classification or the Normalized Differential Vegetation Index (NDVI), which measures vegetation greenness, as proxies for soil moisture and land surface wetness.\n\n【11】A more recent approach has been to simulate land surface wetness conditions by using a hydrology model. Such models can represent the hydrologic cycle at the land-atmosphere interface and track the movement of water and energy between the soil, vegetation, and atmosphere. By accounting for soil type, vegetation type, topography, evapotranspiration rates, and precipitation, one may continuously simulate representation of surface pooling in space and time.\n\n【12】In some sense, the use of a hydrology model is a hybridization of bottom-up (ground observation) and top-down (satellite imaging) approaches and can be developed in conjunction with both through data assimilation. This approach has several advantages: 1) it models the actual aquatic environment used by the mosquitoes, not a filtered proxy; 2) it offers continuous real-time prediction of hydrologic conditions, that is unconstrained by orbital patterns, cloud cover, or vegetation; 3) it resolves the whereabouts of the potential breeding habitats at a very fine scale (areas as small as 10-m cells); and 4) hydrologic models are readily coupled to global climate models, allowing additional medium- and long-range forecast of hydrologic conditions.\n\n【13】Several studies have employed such models. Patz et al. used a water balance model to hindcast weekly soil moisture levels in the Lake Victoria basin. These soil moisture levels were then associated with local human biting rates and entomologic inoculation rates. Shaman et al. used a more detailed hydrology model to predict flood and swamp water mosquito abundances in New Jersey. Mosquito species were found to respond differently to changing local wetness conditions, and these differences were consistent with known breeding behavior and habitat preferences. For example, swamp and flood water mosquito abundance increased during wet conditions, while mosquitoes that preferred eutrophic breeding habitats increased during dry periods. Thus, hydrologic variability was able to differentially predict mosquito species abundances. In a separate study in Florida, amplification and transmission of St. Louis encephalitis virus (SLEV) were associated with changing modeled land-surface wetness conditions .\n\n【14】### Developing Early Warning Systems\n\n【15】Irrespective of the hydrologic variable measured (e.g. measured rainfall, water management, or irrigation effects; satellite measurements of land-surface wetness; or modeled hydrologic conditions), if the variable is associated with mosquitoborne disease transmission in a stationary and robust manner, it may be used to monitor that disease. A few such monitoring systems have recently been developed in which climatic conditions (monthly rainfall and temperature) appropriate for mosquitoborne disease transmission are used to develop risk maps of the geographic distribution of the disease. In Africa this risk assessment has been applied on a large scale and used to develop maps of malaria risk and distribution . Where precipitation is found to precede malaria transmission and the data are readily available, these maps could be used as part of a malaria early warning system. Such maps are an important step toward achieving operational monitoring and forecasting of malaria transmission.\n\n【16】### Issues of Scale\n\n【17】Before hydrologic factors are used to monitor and forecast mosquitoborne disease transmission, the scales at which the disease system responds to hydrologic variability, as well as the scales at which hydrologic variability can be monitored, must be considered. For instance, empirical findings may demonstrate that a particular disease vector responds to local variations in precipitation (i.e. hydrologic changes in its immediate environment). To monitor this vector population, one would like to keep informed of local rates of rainfall, preferably over the entire geographic range of the vector. However, in the tropics, precipitation rates can differ greatly between locations just a few kilometers apart, but meteorologic stations are much more sparsely distributed. The mismatch between the scales at which a disease vector responds to hydrologic variability and the scales at which hydrologic variability can actually be monitored limits operational application of such empirical findings and underscores the need to develop systems that monitor and forecast hydrologic variability at scales corresponding to disease system ecologies.\n\n【18】Operational monitoring and forecasting of any vectorborne disease also requires consideration of how information is to be used. Empirical relationships derived from either microenvironment- or macroenvironment-level scales must be relevant at the scales at which vector control interventions are applied. For example, modeling and mapping of individual mosquito oviposition sites (e.g. herbivore hoof prints) may prove too detailed, heterogeneous, and computationally expensive to be of practical use as an aid for vector control efforts. However, if the flooding and drying patterns of such hoof prints are highly covariable over a large spatial scale, for example, several square kilometers, then modeling flooding and drying at these sites might prove feasible. That is, rather than attempt to monitor and control oviposition sites individually, the herbivore hoof prints would instead be monitored collectively on a larger spatial scale. This information could then be used to help make vector-control intervention decisions at that larger spatial scale by focusing the application of larvicide to selected areas wet enough to support many hoof print pools. This approach would reduce unneeded control efforts in regions too dry to support such pools.\n\n【19】Similarly, seasonal climate model predictions may lack the temporal and spatial resolution needed to discern local disease transmission patterns. Resources may be wasted if control efforts are blanketed over a large region of which only small portions are \"hot zones\" of vector and disease activity. Yet if stable relationships exist between grid-scale and subgrid scale variability, useful information for intervention might be gleaned from such models. For instance, climate models often have a resolution (i.e. grid scale) of 2.5°×2.5°, about 75,000 km 2  . This is a very large area over which to adopt a single vector-control intervention strategy. Clearly, variable levels of vector and disease activity will exist within such an area. Local understanding of subgrid scale (i.e. scales <75,000 km 2  ) variability and how it relates to grid-scale variability is needed to focus control efforts. This understanding could be as simple as knowing that during drought the rivers within a given 2.5°×2.5° area pool and serve as mosquito-breeding sites and that control efforts should be focused in the pools that form along these rivers.\n\n【20】Thus, the issue of scale has to be considered from both scientific and operational vantages. Not only must empirical relationships between mosquitoborne disease systems and hydrology be robust, they must also be capable of being monitored and used with vectorborne disease intervention programs. Monitoring hydrologic variability must be possible both at the scale at which it affects mosquitoes and mosquitoborne disease transmission as well as at a scale at which interventions can feasibly and effectively be applied. Such considerations, of course, are inextricably linked to an understanding of the biology and ecology of vectorborne disease systems.\n\n【21】### Beginning Operational Application: Florida\n\n【22】We now present an example from our own efforts to establish, real-time operational hydrologic monitoring of West Nile virus (WNV) in Florida. We begin with the scientific basis for this system then describe operational application of the system in 2004. Further development of the monitoring system is then discussed, and issues of scale are considered.\n\n【23】Since its appearance in New York City in 1999, WNV has spread to every state in the continental United States and has emerged as an important infectious disease threat to the general public. WNV was first detected in Florida in 2001; since then human cases of WNV illness have occurred sporadically throughout the state. WNV is closely related to SLEV. Both WNV and SLEV are maintained in the environment through enzootic transmission between avian amplifying hosts and vector mosquitoes . In a series of articles, we have demonstrated that a specific sequence of hydrologic conditions, spring drought followed by continued summer rainfall, is critical for the amplification and transmission of both SLEV and WNV in south Florida . Amplification involves a cascade of enzootic virus transmission between vector mosquitoes and wild avian hosts and results in a rapid increase in the number of infected and infective vector mosquitoes. Drought enhances amplification by restricting vector mosquito activity to selected habitats in south Florida, specifically densely vegetated hammocks where mosquitoes rest and wild birds nest and roost. Increased contact between vector mosquitoes and avian amplification hosts in these hammocks facilitates early season viral amplification by forcing the interaction of mosquitoes and birds.\n\n【24】When drought-induced, early season viral amplification in the initial transmission foci is followed by high amounts of summer rainfall that increases near surface humidity levels and the availability of oviposition sites, infective mosquitoes are able to disperse and initiate secondary transmission foci away from the original amplification site. This dispersal and secondary amplification in urban and suburban habitats dramatically increase the number of infective mosquitoes in the environment and place them into greater contact with humans during the late summer months, when WNV and SLEV epidemics typically occur in south Florida. In addition, the mosquito population can increase dramatically in response to the increased water resources. Because of the increased viremic rate among the wild bird population, these newly emergent mosquitoes are themselves more likely to acquire infection.\n\n【25】Our analyses cited above demonstrate that high transmission rates of SLEV and WNV to humans are more likely to occur after a spring drought that is followed by continuous summer rainfall. Public health and vector control personnel are greatly concerned about the likelihood of a major WNV epidemic in Florida. A widespread spring drought followed by continued summer wetting, conducive for epidemic transmission, last occurred in Florida in 1990. During that year, 226 human cases of SLE were reported throughout the southern half of the state. Since the first appearance of WNV in Florida during 2001, the hydrologic pattern of spring drought followed by summer wetting has not occurred. Because of the similarity of SLEV and WNV transmission cycles , such a hydrologic pattern will likely result in a major WNV epidemic in the human population of south Florida.\n\n【26】To combat this threat, during 2004 we established real-time monitoring of hydrologic conditions in south Florida. Simulations using the topographically based hydrology (TBH) model  were made with National Climate Data Center meteorologic data from 49 station sites in south Florida through the end of 2003 . These simulations were then extended in real time during 2004 by using real-time, hourly data of land surface meteorologic conditions distributed at 0.125° resolution available from the National Oceanic and Atmospheric Administration (NOAA) through its Global Energy and Water Cycle Experiment (GEWEX) Continental-Scale International Project Land Data Assimilation System (LDAS) Project. Using these data and the TBH model, we produced maps of land-surface wetness conditions . These maps were made available to personnel at the Florida Medical Entomological Laboratory, as well public health and mosquito control officials throughout Florida. These data were used to evaluate ongoing WNV transmission and the threat of virus transmission to humans. The land surface wetness maps were especially helpful in evaluating WNV transmission in Miami, where sporadic human WNV disease cases were reported from June through September 2004.\n\n【27】During 2004, Florida did experience intense, widespread drought; however, the drought persisted through much of the summer. Exceedingly dry conditions existed throughout most of south Florida during late June, July, and early August . This severe drought prevented the infectious mosquito population, which had developed during amplification in May and early June, from dispersing and initiating secondary transmission and amplification foci. This situation resulted in limited WNV transmission to humans in south Florida, where cases were reported in Hillsborough (cases), Brevard (cases), Broward (cases), and Dade (cases) Counties. Only with the arrival of Hurricanes Charley and Frances in mid-August and early September  did land surface conditions in south Florida become considerably wetter. This occurrence, however, was too late to allow the dispersal of infective mosquitoes and the establishment of secondary amplification and transmission foci in time to produce infected mosquitoes on a level that would result in epidemic transmission of WNV.\n\n【28】### Future Prospects and Issues of Scale in Florida\n\n【29】In 2005, we plan to turn operation of this real-time monitoring over to personnel at the Florida Medical Entomological Laboratory. Sustained real-time use of this information will require continual evaluation of TBH model performance, as well as the empiric relationship between modeled land surface wetness conditions and WNV transmission. Our analyses of WNV and SLEV transmission over the last 25 years in south Florida indicate that this empiric relationship is robust and stationary. The spring maps of hydrologic conditions indicate where drought is occurring and WNV amplification is most likely to occur. The summer maps demonstrate where mosquito dispersal and the establishment of secondary transmission foci are likely to occur. The late summer maps indicate where the risk for WNV transmission to humans is greatest in south Florida.\n\n【30】A geographically large-scale WNV epidemic is of primary concern to public health workers in Florida. Large-scale hydrologic events are easily monitored by using records from the current network of meteorologic stations to model land-surface wetness. However, a denser network of stations may be necessary to comprehensively monitor and predict small-scale focal and sporadic WNV transmission in south Florida. To first order, hydrologic conditions in subtropical Florida covary at the spatial scales at which storms organize. With the exception of irrigation, precipitation is the sole source of water to near-surface soils. Because much of the rainfall in Florida is convective, land-surface wetness can vary over short spatial scales (<10 km). The network of meteorologic stations in south Florida is too sparse to detect this small-scale variability. In the future, we plan to use Doppler radar measurements of rainfall and GEWEX LDAS data to run the TBH model at 0.125° resolution throughout south Florida. Such higher resolution monitoring of drought conditions will enable more comprehensive depiction of the small-scale hydrologic variability associated with focal and sporadic WNV amplification and transmission and may enable such events to be detected. Interventions, such as public health warnings or more intense mosquito control efforts, could then be effected at this scale (.125° resolution).\n\n【31】In addition to monitoring hydrologic conditions, quantitative maps of WNV transmission risk can be constructed at a scale that is not presently available. Risk maps are generated by combining the empirical relationships derived for the dependence of WNV transmission to sentinel chickens and humans on TBH-modeled wetness conditions in south Florida  with the real-time TBH-model-simulated hydrologic conditions. These predictions provide likelihoods of future WNV transmission to sentinel chickens and to humans. As such, these forecasts do not provide all-or-none predictions, which are right or wrong. Rather, the information allows vector-control experts and public health officials to examine shifts in the likelihood of WNV transmission in both space and time. A cost-benefit analysis might show that a small increase in the likelihood of WNV transmission in an area warrants radical changes in local vector-control efforts. Such an analysis could determine the optimal allocation of public health monies in response to these probabilities. Further investigation of such issues is needed for scientists and public health officials to determine fully the utility and limitations of hydrologic predictions.\n\n【32】Our hope is that an operational WNV and SLEV monitoring system will anticipate the next large arbovirus epidemic in Florida. Results will be even better if timely vector control measures are initiated in response to the real-time modeled hydrologic conditions and avert an epidemic. Whether vector control efforts can mitigate a major WN epidemic remains to be seen; however, vector-control efforts at the front end of an epidemic, during amplification, are far more valuable than control efforts attempted after the epidemic has peaked. The key will be to focus the intervention accurately in space and time to the scale of the problem.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3458fe7c-1338-4a62-812b-cd3f8d2fd087", "title": "Increase in Serotype 6C Pneumococcal Carriage, United Kingdom", "text": "【0】Increase in Serotype 6C Pneumococcal Carriage, United Kingdom\n**To the Editor:** _Streptococcus pneumoniae_ is a major human pathogen. In 2007, Park et al. identified a novel serotype, 6C , which emerged from serotype 6A. A study of children in the Netherlands who had not previously received a pneumococcal vaccine found low prevalence of this newly identified serotype before the heptavalent pneumococcal conjugate vaccine Prevnar/Prevenar (PCV7) (Wyeth, Taplow, UK) was introduced . Studies have shown cross-protection between vaccine serotype 6B and vaccine-related serotype 6A. However, PCV7 elicits no cross-protection against serotype 6C.\n\n【1】The potential exists for the emergence of nonvaccine serotypes or novel clones. These serotypes and clones may be better adapted to colonize the nasopharynx, evade the human immune response, and cause disease. A recent study showed an increase in prevalence of serotype 6C pneumococci in children and a corresponding decrease in serotype 6A after introduction of PCV7 . We studied the underlying genetic basis for expansion of serotype 6C. Initial data from an ongoing study of pneumococcal carriage are presented.\n\n【2】This study was reviewed and approved by the Southampton and South West Hampshire Research Ethics Committee (B) (reference 06/Q1704/105). A total of 697 nasopharyngeal swab specimens were collected from unselected (not selected by a method) children < 4 years of age in the pediatric outpatient department of a large teaching hospital in the United Kingdom. Samples were obtained during October 2006–March 2007, during implementation of PCV7 in the infant immunization schedule of the United Kingdom. During October 2007–March 2008, a total of 202 pneumococci were isolated. All pneumococci were characterized by serotype and genotype.\n\n【3】In the first year of this study, we identified 3 (.1%) serotype 6C pneumococci belonging to 3 sequence types (STs): ST65, ST1714, and ST1692 . ST1714 and ST 1692 shared a common clonal complex. Only ST 65 was shared between serotype 6C and serotype 6A. In the second year, we identified 14 (.6%) serotype 6C pneumococci belonging to 6 STs . Two of these STs, of the same ST, were from siblings. Three of them (ST1692 \\[n = 8\\], ST1714 \\[n = 2\\], and ST395 \\[n = 1\\]) were members of a common clonal complex with a predicted founder of ST395. Each of the remaining 3 STs (ST398, ST1862, and ST3460) was isolated only once. One serotype 6A isolate of ST1692 was also observed.\n\n【4】No serotype 6C ST65 was observed in the second year. We isolated more serotype 6C pneumococci in year 2 than in year 1 (p < 0.01), which was explained mostly by a large increase in ST1692 (p < 0.03) . A recent study by Nunes et al. reported serotype 6C ST1692 within a clonal complex that also included ST395 and ST1714 , and we identified the same clonal complex in year 2 of our study.\n\n【5】Our study showed a large increase in ST1692 in serotype 6C pneumococci during the implementation of PCV7 and an increase in serotype 6C. Depending on the extent of cross-protection between vaccine-related serotypes, introduction of conjugate vaccines could induce clearance or emergence of vaccine-related serotypes. This introduction could also contribute to their substitution with novel or existing serotypes that are better adapted to the ecologic niche. However, our data may only be relevant to carried pneumococci and not reflected in pneumococcal disease epidemiology. Nevertheless, the increase in serotype 6C pneumococci in the United Kingdom, which is supported by a similar observation in the United States , highlights the potential for emergence of serotypes not included in the current study and newly developed pneumococcal conjugate vaccines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7870d3ce-75ce-4e63-ad1b-4030b4723dc0", "title": "Etymologia: Ehrlichia", "text": "【0】Etymologia: Ehrlichia\n### Ehrlichia \\[ār-lik′e-ə\\]\n\n【1】Named in honor of German scientist Paul Ehrlich, _Ehrlichia_ is a genus of gram-negative bacteria of the family _Anaplasmataceae_ . Although Ehrlich was not a bacteriologist and was primarily known for his work in hematology, immunology, and chemotherapy, the species _Ehrlichia kurlovi_ was proposed in 1937 by Russian rickettsiologist Sh. D. Moshkovsky.\n\n【2】In 1889, Mikhail Georgiyevich Kurloff, perhaps working with Ehrlich, published a description of atypical granules in guinea pig leukocytes. These granules were assumed to be normal; rickettsiae would not be discovered for several decades. In his 1937 paper, Moshkovsky recognized the guinea pig granules as rickettsial inclusion bodies and named the genus “in honor of Paul Ehrlich, since it was in his laboratory that the first representatives of this group were discovered, and because he has contributed so much to the study of the morphology of the blood and of the agents of infectious diseases.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6d8dc187-4155-4293-8a8d-0d221faa5fc9", "title": "Timing the Origin of Cryptococcus gattii sensu stricto, Southeastern United States", "text": "【0】Timing the Origin of Cryptococcus gattii sensu stricto, Southeastern United States\nThe _Cryptococcus gattii_ species complex consists of \\> 4 major subtypes (VGI–VGIV), which are now considered to be different species . _C. gattii_ species complex is often described as native to tropical and subtropical regions, but recent reports have shown that it is more cosmopolitan than previously thought . In North America, the emergence of _C. deuterogattii_ (VGII) in the Pacific Northwest of Canada and the United States generated a great deal of interest in the study of _C. gattii_ species complex in this area . Although this emergence was the impetus for the study of _C. gattii_ species complex in the United States, this pathogen has actually been known and documented in the United States for multiple decades, especially in southern California . Despite its occurrence in the Pacific Southwest, the emergence in the Pacific Northwest is thought to be quite recent. Recent work using Bayesian evolutionary analysis by sampling trees generated using BEAST software showed that the emergence of _C. deuterogattii_ in the Pacific Northwest was a recent event, occurring within the last 60–100 years .\n\n【1】Historical reports have described the presence of _C. gattii_ species complex in the southeastern United States, where documented clinical cases are rare. However, these cases are often unacknowledged by literature reviews because they occurred when _C. gattii_ could only be detected as a unique serotype of _C. neoformans_ , before it became known as a separate species . Although published reports have been rare, recent _C. gattii_ cases have been reported in the southeastern United States . We recently described the population structure of 10 _C. gattii_ sensu stricto (VGI) patient isolates from the southeastern United States . Here we describe selecting 8 of those same isolates and using BEAST software  to predict the timing of the emergence of _C. gattii_ in the southeastern United States.\n\n【2】### The Study\n\n【3】We identified single-nucleotide polymorphisms (SNPs) and conducted phylogenetic analyses as previously described . We identified 43,731 total SNPs among 8 isolates, based on a genome quality breadth of 16,991,136 bases. The maximum-likelihood tree  had a consistency index of 1.0, and all branching bootstrap values equaled 100 for 100 replicates. Because all isolates in the southeastern United States group contain only the α mating type and the phylogenies have a perfect consistency index (i.e. demonstrating no homoplasy), we assume a clonal expansion for this population (i.e. lack of multiple mating types limits opportunity for recombination), although cryptic recombination cannot be ruled out. We therefore applied the clonal mutation rate obtained from the prior Bayesian molecular clock analysis (using BEAST software ) of the Pacific Northwest _C. gattii_ species complex, specifically for VGIIa and VGIIc (.59 × 10 −8  SNPs/base/y). We determined the best-fitting clock and demographic model combination by implementing path and stepping stone sampling marginal-likelihood estimators as described previously . We implemented a general time-reversible nucleotide substitution model and an uncorrelated log-normal molecular clock with a Bayesian skyline model across 4 chains with 3 billion iterations and achieved across- and within-chain convergence.\n\n【4】Employing these previously described methods with a log-normal Bayesian skyline model on the genomes from the 8 isolates from the southeastern United States, BEAST provided an estimated range of time to most recent common ancestor (tMRCA) of approximately 9,000–19,000 years. A general recombining population mutation rate for _Cryptococcus_ was previously estimated at 2.0 × 10 −9  , which is an order of magnitude slower than the estimated clonal rate; therefore, applying this rate provides for a tMRCA that is nearly an order of magnitude greater (years).\n\n【5】### Conclusions\n\n【6】The clonal mutation rate appears to be more appropriate than the general mutation rate for recombining populations for estimating tMRCA, showing that the timing of _C. gattii_ emergence and subsequent divergence in the southeastern United States is clearly much older than the Pacific Northwest emergence. The sharp contrast in the time of arrival compared with _C. deuterogattii_ in the Pacific Northwest is considerable. Although _C. gattii_ has apparently been in the southeastern United States for thousands more years than _C. deuterogattii_ has been in the Pacific Northwest, the number of cases detected in the southeastern United States is far fewer. Infections in the South are probably not regularly missed, given that _C. gattii_ in the southeastern United States causes primarily a devastating meningitis or meningoencephalitis . Questions of whether subacute cases might be going undetected, whether the distribution of the fungus in the environment might be lower, or whether the niche is not readily accessible by humans remain unanswered. We clearly know less about the _Cryptococcus_ species that has been in the United States for thousands of years (C. gattii_ sensu stricto) than we know about the one that has only recently arrived (C. deuterogattii_ ).\n\n【7】The Pacific Northwest emergence has been hypothesized to be related to the opening of the Panama Canal, enabling more shipping from eastern ports of South America to the West Coast of North America . The estimated timing of emergence of _C. gattii_ in the southeastern United States occurred long before industrial shipping. Although humans might have populated the southeastern United States by the time of the dispersal, most of the human movement was north to south rather than the reverse . This relatively recent emergence during the Pleistocene epoch is different from what has been hypothesized for other endemic mycoses, a much older fungal species dispersal resulting from mass migration of animal populations between continents . The idea of recent emergence also departs sharply from what has been hypothesized as a speciation attributable to prehistoric land movements, as has been proposed for larger _Cryptococcus_ species separations . _C. gattii_ sensu stricto has been found in and likely originates from South American locales . Besides anthropogenic means, possible dispersal mechanisms out of South America might have included animal, bird, or insect species migration or even ocean detritus brought by Caribbean currents or hurricanes. However, given the lack of population structure, dispersal probably occurred through a discrete event or through limited events from the same originating population. No matter the mechanism of arrival, _C. gattii_ has been hiding, mostly undetected, in the southeastern United States for millennia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0acc8c9b-ebee-442a-8b14-5a1dd46f1966", "title": "Virulence Potential of Fusogenic Orthoreoviruses", "text": "【0】Virulence Potential of Fusogenic Orthoreoviruses\nThe virus family _Reoviridae_ comprises a diverse group of nonenveloped, segmented, double-stranded RNA viruses classified into 10 genera. Members of the genus _Orthoreovirus_ are classified as fusogenic or nonfusogenic, depending on their ability to cause syncytium formation in cell culture. Genetically, orthoreoviruses contain 10 segments: 3 large, 3 medium, and 4 small (S1, S2, S3, and S4). The characteristic of the segmented genome facilitates virus reassortment, which contributes to virus diversity. Sequencing studies indicate that reassortment in nature is a major determinant of reovirus evolution .\n\n【1】In 2007, Chua et al. reported the isolation of Melaka virus, a novel fusogenic reovirus of bat origin, which has been associated with acute respiratory illness in humans . Subsequently, other related strains of bat-associated orthoreoviruses have been reported, including Kampar virus from Malaysia and Xi River virus from the People’s Republic of China . The role of bats as reservoirs of various zoonotic viruses, such as Nipah, Hendra, coronaviruses, and lyssavirus, has brought attention to the role of bats in virus evolution and emerging infections in humans .\n\n【2】We isolated and characterized 3 fusogenic orthoreoviruses from 3 travelers who had returned from Indonesia to Hong Kong during 2007–2010. Using a plaque-reduction neutralization test (PRNT), we confirmed infection in 2 of these patients and showed cross-reactivity of antibodies among the 3 virus strains. To track virus evolution and provide evidence of genetic reassortment, we used PCR sequencing and phylogenetic analysis to compare their genetic relatedness.\n\n【3】### Methods\n\n【4】##### Case Descriptions\n\n【5】Patient 1 was a 51-year-old male tourist from Bali, Indonesia, who became ill in April 2007. This case has been reported elsewhere . The patient left Hong Kong before a serum sample could be obtained.\n\n【6】Patient 2 was a 26-year-old woman who sought care at a designated fever clinic during the outbreak of pandemic influenza in July 2009. She had a 2-day history of influenza-like illness (ILI), and oseltamivir was prescribed. Throat and nasal swab specimens were taken 3 days after onset of illness. Serum was collected 59 days after symptom onset. The patient recovered uneventfully. She reported having traveled to Bali, Indonesia, for 5 days and noticing ILI symptoms on the last day of her visit. While in Indonesia, she visited a safari park and entered bat caves on the second day. Her travel companions remained asymptomatic.\n\n【7】Patient 3 was a 29-year-old woman who sought care for ILI at the General Outpatient Clinic in Hong Kong in June 2010 after a recent visit to Indonesia. As part of sentinel surveillance for ILI, throat and nasal swab specimens were taken for routine isolation of respiratory viruses . Serum was collected 8 days after symptom onset.\n\n【8】##### Virus Isolation\n\n【9】As part of routine surveillance to monitor influenza activity, throat and nasal swab specimens were inoculated into cells. The cell lines used for culture were MDCK, rhabdomyosarcoma, human laryngeal carcinoma, and rhesus monkey kidney .\n\n【10】##### Virus Identification and Characterization\n\n【11】##### Immunofluorescence Testing\n\n【12】For samples that exhibited syncytial cytopathic effect (CPE) on cell culture, we used immunofluorescence testing with monoclonal antibodies to preliminarily identify viruses . We tested for viruses commonly associated with syncytial CPE formation on cell culture: respiratory syncytial virus, measles virus, and mumps virus. As part of routine investigation to exclude infection with influenza A(H1N1)pdm09 virus during the time of the pandemic influenza outbreak, a sample from patient 2 was also tested for influenza A.\n\n【13】##### Electron Microscopy\n\n【14】For viruses that had negative immunofluorescence results for respiratory syncytial, mumps, and measles virus monoclonal antibodies, we processed supernatant of tissue culture fluid for examination of virus morphologic appearance by electron microscopy. For negative staining, 1 drop of culture supernatant was adsorbed on Formvar carbon coated grid (min), stained with 3% phosphotungstic acid (pH 6.3) (min), and inactivated with ultraviolet irradiation before examination at 80 KV with a Philips transmission electron microscope (Eindhoven, the Netherlands).\n\n【15】##### PCR, Nucleotide Sequencing, and Phylogenetic Analysis\n\n【16】To further identify the virus and its phylogeny, we next designed primers based on published sequences of Melaka virus, selective for the S1 to S4 segments of orthoreovirus . In brief, by using a QIAamp Viral RNA Mini Kit (QIAGEN, Valencia, CA, USA) according to manufacturer’s instructions, we extracted viral RNA from 200 μL of culture supernatant of the 3 orthoreoviruses. Virus segments S1–S4 were amplified by using a QIAGEN One-Step RT-PCR Kit with primers shown in the Table . Reverse transcription PCR was performed as described . The amplified products were analyzed by 2% agarose gel electrophoresis. Sanger sequencing was conducted by using an ABI Prism 3130XL DNA Sequencer (Applied Biosystems, Foster City, CA, USA) and an ABI Prism Big Dye Terminator Cycle Sequencing Kit, version 3.1 (Applied Biosystems). Sequence alignment by using Simmonic and phylogenetic analysis of the partial sequences of segments S1 (nt), S2 (nt), S3 (nt), and S4 (nt) was performed by using MEGA4 software  with the neighbor-joining method. Sequences were submitted to GenBank and compared with other orthoreovirus sequences for genetic relatedness and evolution .\n\n【17】##### PRNT\n\n【18】For PRNT, we first prepared a 1:5 diluted serum sample by using 2% minimal essential medium (MEM) and heat inactivation at 56°C for 30 min. We then performed serial double dilutions of serum samples from 1:5 to 1:2,560. Equal volumes of serum and virus mixture were incubated at 37°C for 2 h. Then 200 μL of serum and virus mixture with final dilutions of 1:10 to 1:5,120 was added to each of the 6-well plates containing a monolayer of confluent Vero cells (× 10 5  cells/mL in 2% MEM). Serum, virus, and cell controls were included in the assay. After 1 h of virus absorption, the inoculum was discarded, plates were washed with 2% MEM, 4 mL of overlay LE agar was added, and the plates were incubated at 37°C in a CO 2  incubator for 4 days. The end-point neutralizing antibody titer was determined by serum with the highest dilution that reduced plaques by 70% more than the virus control .\n\n【19】### Results\n\n【20】##### Virus Identification\n\n【21】All 3 orthoreoviruses (designated HK23629/07, HK46886/09, and HK50842/10) showed syncytial CPE in culture tubes after 3 to 4 days of incubation in each of the 4 cell lines. (Orthoreoviruses also cause CPE in MRC5 \\[human diploid lung\\] and Vero cells.) Immunofluorescent testing indicated that the samples were negative for influenza, respiratory syncytial, measles, and mumps viruses. Electron microscopy revealed virus morphologic appearance consistent with that of a reovirus .\n\n【22】##### Neutralizing Antibody Titers\n\n【23】According to PRNT results, serum of patient 2 had a neutralizing antibody titer of 1,280 against HK46886/09 virus and HK23629/07 virus and a titer of 640 against HK50842/10 virus. Serum of patient 3 had a neutralizing antibody titer of 20 against all 3 viruses. The control serum had a neutralizing antibody titer <10.\n\n【24】##### Nucleotide Sequences and Phylogeny\n\n【25】For the S1 virus cell attachment protein, the HK46886/09 and HK50842/10 viruses were clustered with the Miyazaki virus and related closely to the Kampar virus isolated in Malaysia; the S1 virus cell attachment protein of the HK23629/07 virus has been reported as similar but distinct from Melaka virus  . For the S2 major inner capsid protein and the S4 major outer capsid protein, all 3 viruses were clustered together and closely related to Miyazaki virus . For the S3 nonstructural protein, the HK46886/09 and HK50842/10 viruses were clustered together and similar to Kampar and Paula viruses, whereas the HK23629/07 virus was clustered with Miyazaki virus . Sequence analysis of the S1–S4 segments showed that HK46886/09 and HK50842/10 viruses were similar strains and different from the HK23629/07 virus. The difference in sequence homology between segments of the virus strains suggested reassortment events.\n\n【26】### Discussion\n\n【27】The isolation and epidemiologic studies of Melaka and Kampar virus in Malaysia provide evidence that a novel group of fusogenic orthoreovirus of bat origin is associated with human respiratory tract infection. The role of bats as reservoirs and sources of potential emerging viral infections has been extensively studied with regard to Nipah, Hendra, lyssavirus, and coronaviruses . Although we were unable to demonstrate rises in antibody titers because of a lack of paired acute-phase and convalescent-phase serum samples, we believe that the isolation of fusogenic orthoreoviruses, the presence of antibody to the viruses, and epidemiologic information suggest that orthoreoviruses were possibly associated with the patients’ upper respiratory tract infections.\n\n【28】Similar to infection caused by Melaka virus, the infection in the patients reported here was a mild upper respiratory infection followed by full recovery. Also, similar to the 7-day incubation period of the patient infected with Melaka virus, incubation periods for the patients reported here ranged from 4 to 7 days. The cross-reactivity of the 3 virus strains, demonstrated by PRNT, suggests that they were antigenically similar. Neutralizing antibodies appeared within 8 days of disease onset, and titers were still high at 2 months. Although all persons who had been in contact with the patients during their travel and at home were asymptomatic, no serum was available for further workup to determine subclinical infection or human-to-human transmission. Nevertheless, subclinical infection and transmission among close contacts have been demonstrated .\n\n【29】Phylogenetic analysis of the partial S1–S4 gene segments demonstrated genetic diversity of the viruses. Although the S2 major inner capsid proteins and S4 major outer capsid proteins of all 3 viruses were clustered together, they differed in the S1 virus cell attachment protein and S3 nonstructural protein. The S1 segments of HK46886/09 and HK50842/10 viruses were clustered together with the Miyazaki virus and related to the Kampar virus, but that of the HK23629/07 virus was similar to but distinct from the Melaka virus. The S3 segments of HK46886/09 and HK50842/10 viruses were clustered together and similar to those of the Kampar and Paula viruses, whereas the HK23629/07 virus shared similar homology with the Miyazaki virus. The difference in clustering based on the S1–S4 segments alone provides evidence for virus reassortment.\n\n【30】Epidemiologic information indicated that all 3 orthoreoviruses isolated were related to travel to Indonesia. Isolation of closely related orthoreoviruses within a short time in the region illustrates the genetic diversity of the virus and that reassortment events are not uncommon. The association of orthoreovirus infection with bat exposure showed that these abundant mammals are natural reservoirs and play a contributing role in virus evolution. In addition, the segmented nature of the virus genome also facilitates virus reassortment, which leads to virus diversity.\n\n【31】Outbreaks of emerging infections in the past decade, such as those caused by avian influenza (H5N1) virus, severe acute respiratory syndrome coronavirus, Nipah virus, and Hendra virus, have been associated with animal origins. Infections occurred in humans when the pathogens crossed the species barrier to expand their host range. Changes in human activities also bring humans in close contact with animals . Epidemiologic and molecular studies provide evidence that the pathogens isolated from humans shared a high degree of homology with viruses isolated from their animal reservoirs . The ability of a pathogen to emerge and cause outbreaks depends on the efficiency of its human-to-human spread. Recombination and reassortment events facilitate virus adaptation in the new host. Further study is warranted to unravel the possible virus spillover and adaptation of orthoreoviruses as a cause of human infection.\n\n【32】Although symptoms of orthoreovirus infection are mild and self-limiting, continued close monitoring of virus evolution and further study are essential. Changes in virulence would signal the need for prevention and control strategies.\n\n【33】Dr A.H. Wong is a medical virologist at the Public Health Laboratory, Centre for Health Protection, Hong Kong. Her research interests include diagnosis and laboratory surveillance of viral diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "325d684a-4aff-41f3-927a-dfa59fea6734", "title": "Pegivirus Infection in Domestic Pigs, Germany", "text": "【0】Pegivirus Infection in Domestic Pigs, Germany\n**To the Editor:** The family _Flaviviridae_ includes many human and animal virus pathogens. Recently, in addition to the genera _Flavivirus_ , _Hepacivirus_ , and _Pestivirus_ , a fourth genus, _Pegivirus_ , has been identified . In addition to human pegiviruses, a range of phylogenetic, highly divergent pegiviral sequences have been identified in various animal species, including primates, bats, rodents, and horses . We report the detection of a porcine pegivirus (PPgV) in serum samples from pigs.\n\n【1】Initially, we investigated pooled serum samples by using high-throughput sequencing methods and isolated RNA from individual porcine serum samples by using the QIAmp Viral RNA Mini Kit (QIAGEN, Hilden, Germany). We prepared libraries compatible with Illumina (San Diego, CA, USA) sequencing from pooled samples and individual serum samples by using the ScriptSeq version 2 RNA-Seq Library Preparation Kit (Epicenter, Madison, WI, USA) and sequenced them by using a HiSeq 2500 (× 150 cycles paired-end; Illumina) for pooled samples and MiSeq (× 250 cycles paired-end; Illumina) for individual samples .\n\n【2】We conducted quantitative reverse transcription PCR (RT-PCR) by using a Quantitect-SYBR Green Assay (QIAGEN) and primers PPgV\\_fwd: 5′-CTGTCTATGCTGGTCACGGA-3′ and PPgV\\_rev: 5′-GCCATAGAACGGGAAGTCGC-3′. By using high-throughput sequencing of the pooled serum sample library (reads), we identified 1 contig (bp) that had distant nucleotide sequence simi-larity to bat pegivirus (% and 4% sequence coverage) and 2 contigs (bp and 665 bp) that had 73% sequence coverage, thereby covering 8% and 37% of the identified sequence. RT-PCR with primers designed on basis of recovered sequences identified the sample containing pegivirus sequences. Subsequent MiSeq analysis (reads) of an RNA library prepared from a sample from 1 animal identified 1 contig (nt) with sequence similarity to pe-givirus sequences.\n\n【3】We performed 3′ end completion of the viral genome by rapid amplification of cDNA ends and identified the entire open reading frame of PPgV\\_903 encoding 2,972 aa . Analysis of the pegivirus 5′-untranslated region identified a highly structured internal ribosome entry site motif , which was similar in structure to previously described 5′ untranslated region structures of other pegiviruses .\n\n【4】Pegiviruses do not encode a protein homologous to the capsid protein of other viruses of the family _Flaviviridae_ , another common feature of pegiviruses . The presence of cleavage sites for cellular signal peptidases and viral proteases indicates that, similar to polyproteins of other pegiviruses and members of the genus _Hepacivirus_ , the pegivirus polyprotein NH 2  \\-E1- E2-Px-NS2-NS3-NS4A-NS4B-NS5A-NS5B-COOH (E \\[envelope\\], NS \\[nonstructural\\], and Px \\[protein X\\]) is cleaved co-translationally and posttranslationally.\n\n【5】We tested 3 additional animals from the same breeding cohort for virus RNA at irregular intervals for 22 months. One animal was positive for pegivirus RNA for 7 months, and the other 2 animals had pegivirus RNA in serum for 16 and 22 months. None of these animals showed obvious clinical signs attributable to virus infection. Follow-up investigation of 455 serum samples from 37 swine holdings from Germany identified 10 (.2%) samples from 6 pig holdings that contained pegivirus RNA. We obtained 2 additional near full-length genomic sequences (PPgV\\_80F and PPgV\\_S8-7) from 2 animals in different herds by high-throughput sequencing, RT-PCR, and Sanger sequencing (GenBank accession nos. KU351670 and KU351671).\n\n【6】Phylogenetic analyses of complete coding regions showed the close relationship of the 3 pegivirus sequences from Germany. These 3 sequences formed a separate clade within the genus _Pegivirus_ . Pairwise comparison between PPgV\\_903 and the other 2 pegivirus sequences showed strong nucleotide identities (.0%–98.4%). A distance scan over the entire polyprotein showed genetic distance to other pegiviruses and demonstrated that NS3 and NS5B contain the most conserved regions among pegivirus polyproteins .\n\n【7】In horses, 2 distinct pegiviruses that had different potentials to cause clinical disease in infected animals have been described . No obvious clinical effects were observed in pegivirus-infected animals during our study. However, potential consequences of viral infection for animal health and food production need to be explored more closely under field and experimental conditions. Pegiviruses can interact with the immune system of the host. Co-infection with human pegivirus and HIV can have beneficial effects, which result in decreased retroviral loads and delayed disease progression .\n\n【8】It will be useful to investigate whether co-infections with pegiviruses can influence clinical manifestations of infectious diseases of swine, including multifactorial diseases such as postweaning multisystemic wasting syndrome, in which unknown immune modulating virus infections have been suggested to influence the degree of clinical illness . RNA viruses have considerable potential to adapt to new environmental conditions and to overcome host restrictions . Until now, the host tropism of PPgV has not been investigated in detail. Therefore, additional studies will be required to elucidate whether the spectrum of potential hosts might include other farm or companion animals, and whether the virus might be able to infect humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d2490726-4330-42cf-9907-926c5e63eacd", "title": "Correction: Vol. 18 No. 2", "text": "【0】Correction: Vol. 18 No. 2\nAuthor Richard Njouom’s surname was misspelled in High Seroprevalence of Enterovirus Infections in Apes and Old World Monkeys (H. Harvala et al.). The article has been corrected online .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a6b04518-6137-4d62-84ca-fa045da280c2", "title": "Q Fever Outbreak in Homeless Shelter", "text": "【0】Q Fever Outbreak in Homeless Shelter\nHomelessness, a problem that has been increasing since the mid-1980s, raises substantial public health concerns . We have worked with the homeless population of Marseilles since 1993 in ongoing studies on louse-transmitted diseases .\n\n【1】Q fever is a zoonosis caused by _Coxiella burnetii_ , an intracellular bacterium transmitted by aerosols from contaminated soil or animal waste or by drinking contaminated milk . Domestic ungulates (cows, sheep, and goats) are the main reservoir for _C_ . _burnetii_ , but other mammals, including dogs, cats, and wild rabbits, have been implicated . Q fever usually occurs in rural areas when people are incidentally exposed to aerosols or infected milk or milk products , although urban outbreaks have been reported after occupational exposure to slaughterhouses .\n\n【2】While investigating louse-borne diseases in two homeless shelters in Marseilles, we systematically tested persons for antibodies to _C_ . _burnetii_ and found a significantly higher seroprevalence in the homeless population from the northern shelter  than in control blood donors. This shelter (A) is located 2 kilometers south of an abandoned slaughterhouse that is used 1 day each year by the Muslim population of Marseilles for the traditional sheep feast, “Aid El Khebir,” during which sheep are ritually killed. We hypothesized that the northern wind (the mistral) that blows over the slaughterhouse and the shelter was involved in spreading _C. burnetii_ , as has been reported previously in another outbreak near Marseilles . We consequently investigated and followed-up the homeless population from the two shelters for 4 consecutive years and report here the first outbreak of Q fever in this population. We propose that the wind played a critical role in the outbreak.\n\n【3】### The Study\n\n【4】The protocol was reviewed and approved by an institutional review board (Comités Consultatifs de Protection des Personnes dans la Recherche Biomédicale 99/76), and all participants gave informed consent. A medical team of 27 persons, comprising 9 nurses, 6 infectious diseases residents or fellows, and 12 infectious diseases specialists, visited the two shelters once yearly for 4 consecutive years. Each shelter can accommodate 300 persons each night, and each offers showers, food, washing machines, and clean clothes. Shelter B is located downtown, while shelter A is located in the northern part of the town . Homeless persons completed a standardized questionnaire, and a physical examination was performed. Nurses collected blood samples for laboratory investigation. Control subjects were sex- and age-matched blood donors enrolled during the same period and living in Marseilles. Serologic analysis was carried out at the French National Reference Center for Rickettsial Diseases. The antigen used was a phase II and phase I _C. burnetii_ Nine Mile strain (ATCC VR 615) grown in our laboratory in L929 mouse fibroblasts. Phase I was obtained by injection in mice. Samples were assessed by microimmunofluorescence (MIF) as described elsewhere . Immunoglobulin (Ig) G phase II antibody titer \\> 1:50 indicated _C_ . _burnetii_ exposure in the past 6 months to 5 years.\n\n【5】Meteorologic data were obtained from Meteo-France departmental weather stations . Maximum wind speeds and directions were measured three times each hour, which led to >1,400 data entries for the month observed. We asked for wind information during the month which followed the Aid El Khebir in each year: March 27–April 27, 1999; March 16–April 16, 2001, 2000; March 6–April 6, 2001; and February 23–March 23, 2002. Epidemiologic, clinical, and laboratory data were entered into SPSS Data Entry Builder 3.0 (SPSS Inc. Chicago, IL) and then analyzed with SPSS 10.0 (SPSS Inc.). Wind data were captured in an Excel database (Microsoft). Qualitative variables were compared with Fisher or χ 2  tests. A logistic regression model was used for multivariate analysis. The regression was carried out stepwise, and the model included all variables present in the univariate analysis for which p < 0.20.\n\n【6】A total of 930 homeless persons were recruited, 261 in 2000, 171 in 2001, 296 in 2002, and 202 in 2003. Mean age was 43 years (range 18–83), with 44 females and 886 males. Country of origin was France for 36.7%, northern Africa for 37%, eastern Europe for 15.4%, western Europe for 6.2%, sub-Saharan Africa for 2.4%, and Asia for 0.7%. Among 467 controls, 217 completed the questionnaire.\n\n【7】_C. burnetii_ IgG phase II antibodies were found in 17 (.8%) of 157 persons in shelter A in 2000, compared to 14 (%) of 460 controls (p < 0.001). In this shelter, the number of patients with a positive test result was significantly higher in 2000 than in 2001 (/96, 0%) and 2002 (/182, 0.54%) (p < 0.001). This difference was not significant when compared with results in 2003 (/129, 5.4%). The number of homeless persons with positive test results for _C. burnetii_ was not significantly different between shelter B residents and controls .\n\n【8】When exposure to cats, kittens, or dogs; Muslim religion; and living in shelter A were considered, only contact with a kitten (p = 0.031) was associated with _C. burnetii_ positivity in the univariate analysis. However, multivariate analysis using a stepwise linear regression model with all variables included in the univariate analysis showed that living in shelter A was the only factor independently associated with a positive test result for _C. burnetii_ . Moreover, one person in 2002 and seven in 2003 were found positive in shelter A, compared to none in shelter B in those years. Acute Q fever was diagnosed in three homeless persons with IgM anti–phase II antibodies >1:50, one person in 2000 and two in 2003. Two were asymptomatic, and one showed symptoms of high-grade fever, arthralgia, myalgia, and dyspnea. He was hospitalized, and a chest x-ray noted interstitial bilateral pneumonitis. No cardiac murmur was detected. The serologic tests showed IgG, IgM, and IgA antibody titers of 1:800, 1:50, and 1:200 to phase II antigen and 1:400, 1:25, and 1:200 to phase I antigen, respectively. He was treated with 200 mg oral doxycycline each day for 15 days, and he recovered.\n\n【9】Weather records showed that the cumulative number of windy days with the wind blowing from the north (N), north-northwest (NNW), and northwest (NW) was significantly higher in the month that followed the Aid El Khebir in 1999 compared to 2000 (/32, p = 0.002) and 2001 (/32, p = 0.0006)  but not to 2002. The strength of the mistral measured as a mean of the daily recorded maximum speed was not significantly different among the investigated years .\n\n【10】### Conclusions\n\n【11】Q fever is a disease caused by _C. burnetii_ , a strict intracellular bacterium that can survive in the environment for up to 10 months at 15°–20°C, for >1 month on meat in cold storage, and for >40 months in skim milk at room temperature . Two distinct sets of symptoms of Q fever are prevalent. In the acute phase, patients may have fever, granulomatous hepatitis, or interstitial pneumonitis; the chronic phase is primarily characterized by culture-negative endocarditis. The acute phase is asymptomatic in >50% of cases, which explains why an outbreak might be unnoticed .\n\n【12】Q fever is primarily transmitted to humans when aerosolized fluids are inhaled during or after parturition of an infected animal. The organism can stick on wool and dust and be spread by wind. The wind has been shown to spread _C. burnetii_ in other circumstances. In a small town in southern France, wind blew through a steppe where sheep were gathered after lambing, and persons whose homes were exposed to the wind were more often infected with Q fever than their neighbors . In cities, the role of slaughterhouses in the spread of Q fever is well-known . The last reported slaughterhouse-related outbreak of Q fever in France was related to contaminated waste from sheep sacrificed for a Christian Easter feast. The waste had been left uncovered outside the slaughterhouse, which was near a heliport. Helicopters might have facilitated airborne transmission of the infectious agent . _C_ . _burnetii_ has also been shown to be transmitted by dogs , wild rabbits , and parturient cats , and transmission has been associated with religious practices . In this study, contact with kittens and correlation with wind from the slaughterhouse were the only identified risk factors. We showed here that homeless persons were likely exposed to _C. burnetii_ in shelter A during the month that followed the Aid El Khebir in 1999, with the wind playing a critical role in this outbreak. Some controversy surrounds this feast in France because of the way sheep are ritually sacrificed. Several hundred sheep are maintained for a few days inside and outside the slaughterhouse before having their throats slit and being bled outside. They are then displayed to buyers, as shown in Figure 2 . That sheep are maintained under conditions of poor hygiene, without veterinary counsel, and that the bleeding and sale takes place outside may explain how _C. burnetii_ –infected particles could have contaminated soil, wool, or loose straw, and particles could have blown downwind. Veterinary control of sheep flocks would help avoid such contamination.\n\n【13】Shelter B is further south than shelter A. Since no significant differences in incidence of Q fever were found in shelter B, homelessness itself is not associated with Q fever. Access to health care is problematic for this population, so an outbreak of Q fever could go unnoticed unless Q fever testing were a part of disease surveillance in homeless persons. The risk of an unnoticed outbreak emphasizes the need to systematically survey this population and nearby residents. Persons in other areas surrounding the slaughterhouse were also likely exposed to _C. burnetii_ in 1999.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35eac7b0-9918-4ddb-bd5d-ce75275fc214", "title": "Timing of Influenza A(H5N1) in Poultry and Humans and Seasonal Influenza Activity Worldwide, 2004–2013", "text": "【0】Timing of Influenza A(H5N1) in Poultry and Humans and Seasonal Influenza Activity Worldwide, 2004–2013\nCo-circulation of influenza A viruses in human and animal reservoirs can provide opportunities for these viruses to reassort and acquire genetic material that facilitates sustained human-to-human transmission, a necessary trait of pandemic viruses . One influenza strain at the forefront of pandemic preparedness planning is highly pathogenic avian influenza (HPAI) A(H5N1) virus. Asian-lineage H5N1 viruses emerged in domestic birds in Southeast Asia in 1996 and are now endemic in 5 countries . As of December 2013, H5N1 virus outbreaks have been documented among domestic poultry and wild birds in >60 countries . The spread of H5N1 among the world’s domestic poultry population increases the risk for H5N1 to infect humans.\n\n【1】Humans are at risk for H5N1 infection if they have direct or close contact with infected domestic poultry, such as by handling sick animals or their byproducts, caregiving, slaughtering, and butchering; infection can also occur through some forms of indirect contact (e.g. proximity to live poultry or wet markets) . During 2003–2013, more than 645 human cases of HPAI H5N1 were confirmed; the case-fatality rate was ≈60% . H5N1 viruses have not yet acquired the ability to be transmitted between humans beyond 3 generations, therefore failing to show sustained human-to-human transmission . However, these viruses have wide geographic distribution and the potential to reassort with human seasonal influenza viruses. These characteristics mean that clarifying the timing of H5N1 outbreaks among poultry and infections in humans may be useful for prevention and control activities.\n\n【2】Recently published data suggest a seasonal pattern to H5N1 virus infection among domestic and wild birds . Park et al. noted that H5N1 outbreaks among poultry and infections in humans in Southeast Asia occurred in the cooler months during 1997–2006 . Other studies from Southeast Asia have suggested that decreasing temperatures may correlate with an increase in the number of H5N1 outbreaks among birds . Although ecologic studies suggest that H5N1 virus activity may occur during predictable times of the year, a systematic analysis of global poultry and human H5N1 data has not tested this hypothesis. In this study, we explored whether H5N1 outbreaks among domestic poultry and human H5N1 cases occurred in temporal proximity, occurred during certain climate conditions, or overlapped with human seasonal influenza epidemics.\n\n【3】### Methods\n\n【4】##### Timing of H5N1 Outbreaks among Poultry\n\n【5】To describe the temporal patterns of H5N1 outbreaks among poultry worldwide, we extracted the number of monthly outbreaks reported by 52 countries as immediate notifications to the World Organisation for Animal Health (OIE) during January 2004–December 2013 . We used the reported onset month as the occurrence month for each outbreak. Egypt and Indonesia stopped submitting immediate notifications of new H5N1 outbreaks among poultry after H5N1 in poultry was declared endemic: for Egypt in July 2008, and for Indonesia in September 2006. To obtain the number of monthly outbreaks for these countries, we extracted the number from their OIE biannual disease reports. If only the number of new cases but not the number of outbreaks in which they occurred was reported, we classified the month as having 1 outbreak.\n\n【6】For each country and year, we designated the peak month of H5N1 activity as the month that had the largest number of reported H5N1 outbreaks among poultry. Next, we identified the average peak month of H5N1 activity for each country (e.g. if the peak months in a hypothetical country occurred during January 2010, February 2011, and March 2012, then the average peak month during this period would have been February for this country).\n\n【7】##### Timing of Human H5N1 Cases and Poultry H5N1 Outbreaks\n\n【8】We identified the monthly number of human H5N1 cases reported to the World Health Organization (WHO) during January 2004– June 2013. Initially, we obtained data from all 15 countries that reported human H5N1 cases since January 2004 . We then restricted the analysis to the 8 countries that reported 97% of all human H5N1 cases and that reported recurrent H5N1 activity among humans and poultry. To examine the temporal relationship between human H5N1 cases and H5N1 outbreaks among poultry, we compared the number of human H5N1 cases to the number of poultry H5N1 outbreaks for each month from these 8 countries . We then analyzed the association of the monthly number of human H5N1 cases and poultry H5N1 outbreaks by using the Spearman coefficient (ρ).\n\n【9】##### Timing of Human Seasonal Influenza and H5N1 Cases\n\n【10】To clarify the hypothetical risk for human co-infection with avian and seasonal influenza viruses, we investigated the extent to which human H5N1 cases occurred during months when seasonal influenza viruses were circulating. To determine whether H5N1 and seasonal influenza viruses were identified among humans during the same months, we obtained seasonal influenza surveillance data collected by WHO during 2004–2013 . We estimated periods of seasonal influenza epidemics by assessing the months during which the proportion of respiratory samples that tested positive for influenza viruses was higher than the annual mean . We then identified the number of months during which human H5N1 cases and human seasonal influenza epidemics occurred during the same month.\n\n【11】##### Ambient Temperature, Human H5N1 Cases, and Poultry H5N1 Outbreaks\n\n【12】To determine whether cool ambient temperature was associated with the number of poultry H5N1 outbreaks, we abstracted monthly mean temperature data for each country from the US National Oceanic and Atmospheric Administration or from the Vietnam General Statistics Office . We used linear regression models to determine whether the monthly number of poultry H5N1 outbreaks and number of human H5N1 cases were negatively associated with mean monthly temperature and precipitation (e.g. after accounting for the absolute value of each country’s central latitude  and 2-way interactions) .\n\n【13】### Results\n\n【14】##### Timing of Poultry H5N1 Outbreaks\n\n【15】During January 2004–June 2013, a total of 12,610 poultry H5N1 outbreaks in 52 countries were reported to the OIE. Fifteen of these countries also reported laboratory-confirmed H5N1 cases in humans to WHO. Of these 15 countries, 8 (Bangladesh, Cambodia, China, Egypt, Indonesia, Thailand, Turkey, and Vietnam) accounted for 11,331 (%) of worldwide poultry H5N1 outbreaks. These 8 countries served as the basis for our analyses. Four of the 8 countries (Bangladesh, China, Egypt, and Turkey) are classified as Northern Temperate or Subtropical countries because most of their land mass is north of the Tropic of Cancer (latitude 23.27°N). The other 4 countries (Cambodia, Indonesia, Thailand, and Vietnam) are classified as tropical because they are located between the Tropics of Cancer and Capricorn .\n\n【16】The 8 countries provided a total of 79 whole years (months) of data on outbreaks among poultry; 2013 data from Indonesia were not complete and therefore were not included in the analysis. Outbreaks were reported during 39% (/948) of the study months. Most poultry H5N1 outbreaks (/11,331, 76%) were reported during January–March. February had the highest total number of reported poultry H5N1 outbreaks, 3,118. Figure 2 provides the cumulative number of poultry H5N1 outbreaks reported each month by country and illustrates how these outbreaks clustered during January–March.\n\n【17】##### Poultry H5N1 Outbreaks and Human H5N1 Cases\n\n【18】During 2004–2013, the 8 study countries reported nearly all (/645, 97%) human H5N1 cases ; cases were reported in 227 (%) of the 948 study months. As seen with H5N1 outbreaks among poultry, half of human cases (/625, 50%) occurred during January–March, and January had the highest number of reported H5N1 cases among humans (ρ = 0.8, p = 0.004) . At least 1 human case of H5N1 infection occurred during 168 (%) of the 374 months in which ≥1 outbreak in poultry occurred, compared with 59 (%) of 574 months without outbreaks among poultry (relative risk = 5, p<0.001). After their initial identification, poultry H5N1 outbreaks and human H5N1 cases recurred concurrently during October–March in 24 (%) of 70 study year equivalents.\n\n【19】##### Human H5N1 and Seasonal Influenza Activity\n\n【20】Seasonal influenza was epidemic in 267 (%) of 888 months for which data were available. Human H5N1 cases occurred in 59 (%) of these 267 months .\n\n【21】##### H5N1 Outbreaks in Poultry during Colder Temperatures\n\n【22】In each of the 8 countries, the number of poultry H5N1 outbreaks was highest during months with the lowest average ambient air temperature . In our regression model, colder ambient temperature was also ecologically associated with an increase in the number of poultry H5N1 outbreaks and human cases. For each decrease in temperature by 3.3°C, 1 additional poultry H5N1 outbreak was reported (p = 0.004), and for each decrease in temperature by 0.3°C, 1 additional H5N1 human case was reported (p<0.001). Precipitation was co-linear to temperature, however, and was dropped from the analysis.\n\n【23】### Discussion\n\n【24】Our study reaffirms that, in Southeast Asia, H5N1 outbreaks among poultry and human H5N1 cases often occur seasonally, during months when temperatures are relatively cool. Even when accounting for H5N1-endemic countries outside Southeast Asia, most (>50%) poultry H5N1 outbreaks and human H5N1 cases of H5N1 infection occurred during January–March. Human and animal health officials in affected countries should consider exploring the value of enhanced surveillance, flock biosecurity, and live bird market disinfection and/or rest days during periods when H5N1 viruses are typically detected .\n\n【25】Our analysis of 2004–2013 data from 8 countries also suggests that lower ambient temperatures are associated with H5N1 outbreaks among poultry, even though half of our data came from tropical countries, where annual temperature variations are often small. These results are similar to those described by Park and Glass, who observed poultry H5N1 outbreaks during 1997–2006 in Southeast Asia and China and concluded that these outbreaks most often occurred during colder months . Other studies have found similar associations . A decrease in temperature can make poultry more susceptible to H5N1 because lower ambient temperature can decrease poultry immunity . Moreover, cold weather may enable prolonged viral survival in the secretions and feces of infected poultry, and anticipation of seasonal holidays (e.g. Chinese New Year) often results in increases in population density of domestic poultry and in trafficking of poultry .\n\n【26】Human H5N1 cases were almost 5 times more common in months during which poultry H5N1 outbreaks occurred. These findings reaffirm reports that human H5N1 virus infection is typically preceded by exposure to sick or dead poultry  and suggest that human and animal health officials in affected countries should explore the effectiveness of education and outreach efforts before and postexposure prophylaxis during anticipated H5N1 epidemic periods. These efforts could further enhance surveillance of H5N1 in poultry and lead to more prompt culling of poultry when H5N1 viruses are detected, thus helping to increase flock biosecurity and enable timely education and outreach efforts. For example, affected countries may want to evaluate the cost-effectiveness of targeted risk communication campaigns delivered before peak months of H5N1 infection to promote the avoidance of sick or dead poultry, the importance of hand washing, and the appropriate use of personal protective equipment among subpopulations that frequently are in contact with poultry. These efforts could help mitigate the transmission of H5N1 viruses among poultry and the spread of the virus to human populations .\n\n【27】Our data also suggest that one fifth of human H5N1 cases occurred in months during which seasonal influenza was epidemic. Concurrent H5N1 and human seasonal influenza activity provides opportunities for humans and other animals (e.g. swine) to become co-infected with these co-circulating viruses and for the viruses to reassort. Reassortment may generate novel influenza A virus strains with the ability to cause sustained human-to-human transmission.\n\n【28】Our findings support and demonstrate the feasibility of integrating human and animal surveillance for avian influenza and other zoonotic diseases. Combining the expertise from multiple surveillance systems can increase the detection rate of H5N1 cases in humans from 57% to 93% and of epizootics from 40% to 53% . This recommendation for an integrated H5N1 surveillance approach enables better prediction of disease risk and identification of outbreaks . Such approaches can assist in filling in gaps in human and animal surveillance systems .\n\n【29】This study has several limitations. H5N1-endemic countries are not required to report immediate notifications. H5N1 reports also often are limited because of a lack of resources for surveillance, laboratory capacity to confirm H5N1, and, often, the political will to report outbreaks in poultry that may result in substantial economic losses . No formal case definition for a poultry H5N1 outbreak has been developed. During our study period, surveillance practices for human and poultry H5N1 and human seasonal influenza were not standardized; in fact, outbreaks among poultry might be more likely to be reported during periods when human H5N1 cases occurred. Further standardization of surveillance and reporting of influenza outbreaks in humans and animals might help clarify the epidemiology of these viruses .\n\n【30】Health authorities in H5N1 virus–affected countries should consider enhanced surveillance for H5N1 viruses among domestic poultry and humans in anticipation of cooler months, when H5N1 virus outbreaks are most likely to occur. Although year-round surveillance is important for the identification of novel strains of influenza, laboratorians should emphasize the identification, reporting, and sharing of nonsubtypable influenza A viruses during January–March. Public and animal health authorities may consider enhancing animal market surveillance during this time frame and encourage the use of personal protective equipment, disinfectant, and other resources for containment and mitigation in anticipation of increased H5N1 activity among poultry . These lessons may also be relevant to the management of avian influenza A(H7N9) virus outbreaks in China, which have occurred during similar times of the year.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c126d213-e2b9-4b66-a82b-198e2f844d56", "title": "Malaria Elimination―Not Just a Bunch of Hocus-Pocus", "text": "【0】Malaria Elimination―Not Just a Bunch of Hocus-Pocus\nQuintus Serenus Sammonicus (c. 370 CE–212 CE), Liber Medicinalis. Unknown transcriber, Abbey of St. Augustine, Canterbury, England. Parchment from 13th century. 7 in × 4.7 in/180 mm × 120 mm. British Library, London, UK. Public Domain in most countries other than the United Kingdom.\n\n【1】_Abracadabra_ ! This ubiquitous incantation remains a staple of stage magicians, children’s stories, and purveyors of pseudoscience. The exact origin of the term engenders debate, and pundits have suggested various ancient Aramaic, Hebrew, and Latin terms as the source. What is known is that its first appearance in print is found in the surviving fragments of the third century CE book _Liber Medicinalis_ (sometimes known as _De Medicina Praecepta Saluberrima_ ) written by Quintus Serenus Sammonicus. Though Serenus was the physician to the Roman Emperor Caracalla and considered “the learned man of his age,” few details of his life are known.\n\n【2】Following the practice of his time, Serenus composed his teachings as didactic poetry. The surviving fragment of _Liber Medicinalis_ includes popular treatments, remedies, and antidotes written in verse.\n\n【3】Among those remedies, Serenus proposed a magical procedure based on the wizardly word _Abracadabra_ for treating “semitertian” fever, known today as malaria. That malady devastated ancient Rome and was sometimes also called “rage of the Dog Star” as the ascendance of Sirius presaged the oppressive heat and humidity thought to cause fever and illness. Some wealthy Romans sought to escape this scourge by moving to villas they had built in the hills away from the “bad air” (malum aeris in Latin) emanating from the marshes and wetlands surrounding Rome. With a bit of alchemical panache, Serenus offered another approach documented in chapter 51 of the _Liber Medicinalis:_\n\n【4】_Inscribis chartae, quod dicitur Abracadabra:_\n\n【5】_Saepius et subter repetas, sed detrahe summae,_\n\n【6】_Et magis atque magis desint elementa figuris:_\n\n【7】_Singula quae semper rapies et coetera figes,_\n\n【8】_Donec in angustam redigatur litera conum._\n\n【9】_His lino nexis collum redimire memento_\n\n【10】Various translations of the Latin are available. This one comes from by A. C. Wootton, who, for three decades, served as editor of the trade journal _Chemist and Druggist_ .\n\n【11】“Write several times on a piece of paper the word ‘Abracadabra,’ and repeat the words in the lines below but take away letters from the complete word and let the letters fall away one at a time in each succeeding line. Take these away ever, but keep the rest until the writing is reduced to a narrow cone. Remember to tie these papers with flax and bind them round the neck.”\n\n【12】The idea underscoring this magical thinking was that by making the letters disappear, the illness would likewise vanish. This month’s cover image shows a 13th century transcription of this page from _Liber Medicinalis_ and comes from the Benedictine Abbey of St. Augustine, Canterbury, England. Like a slice of pie resting in the margin of the book, the _ABRACADABRA_ cone is visible near the lower right of the parchment. The original Latin, rendered painstakingly in ornate Gothic and Gothic cursive, explains Serenus’ process for creating the triangular charm inscribed with the enchantment and for wearing it as an amulet. Perhaps to hedge his bets, Serenus also suggested smearing lion’s fat on one’s body or wearing a domestic cat’s skin festooned with jewels to ward off these fevers.\n\n【13】Feline byproducts, bejeweled or otherwise, magic words, and amulets all failed, and malaria continues to be one of the most severe global public health problems. Fortunately, though, the scientists of today have been a bit more effective through core interventions of surveillance, diagnosis, prevention, and treatment. In many countries, using artemisinin-based combination therapy, undertaking vector control measures such as using long-lasting insecticides on bed nets and interior walls of houses, and strengthening public health infrastructure have successfully reduced cases and deaths.\n\n【14】While progress is starting to plateau in many highly malaria-endemic countries, several other countries either have eliminated, or are on the verge of eliminating, malaria. Paraguay and Uzbekistan recently celebrated their initiation into the malaria elimination club, receiving certification by the World Health Organization (WHO). El Salvador and China have recently reached zero cases. Both countries will receive their official WHO certification when they have “proven, beyond reasonable doubt, that the chain of local transmission of all human malaria parasites has been interrupted nationwide for at least the past 3 consecutive years; and that a fully functional surveillance and response system that can prevent re-establishment of indigenous transmission is in place.”\n\n【15】In just the past 10 years, the number of malaria-endemic countries has decreased from 108 (in 2008) to 90 (in 2018). In 2017, about half of all of the remaining malaria-endemic countries had reported fewer than 10,000 cases per year. WHO has outlined a strategy to continue paring down that list of malaria-endemic countries, one by one, with ambitious targets through 2030. Peering into our crystal ball, we hope, one day, to see that final country on the list like Serenus’ ultimate letter A and then _Abracadabra_ ―all gone.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c184a014-b0f2-46aa-8255-9fe5dd319e85", "title": "Malaria on the Move: Human Population Movement and Malaria Transmission", "text": "【0】Malaria on the Move: Human Population Movement and Malaria Transmission\nMalaria, the world's most prevalent vector-borne disease, is endemic in 92 countries, with pockets of transmission in an additional eight countries  . Approximately 41% of the world's population is at risk, and each year 300 million to 500 million clinical cases of malaria, >90% of them in Africa, are reported. Worldwide, approximately 2 million deaths per year can be attributed to malaria, half of these in children under 5 years of age.\n\n【1】Historically, population movement has contributed to the spread of disease  . Failure to consider this factor contributed to failure of malaria eradication campaigns in the 1950s and 1960s  . The movement of infected people from areas where malaria was still endemic to areas where the disease had been eradicated led to resurgence of the disease. However, population movement can precipitate or increase malaria transmission in other ways as well. As people move, they can increase their risk for acquiring the disease through the ways in which they change the environment and through the technology they introduce, for example, through deforestation and irrigation systems  . Such activities can create more favorable habitats for Anopheles mosquitoes; at the same time, workers may have increased exposure to the vector. Furthermore, people can inadvertently transport infectious mosquitoes to malaria-free areas, reintroducing disease. Population movement is also increasingly implicated in the spread of drug resistance in malaria  .\n\n【2】The unprecedented increase in mobility in the last few decades has led to greater concern about the relationship between mobility and malaria. There are a number of reasons for increased mobility. First, sophisticated forms of transport now permit the swift movement of people over huge distances. Air travel has increased by almost 7% a year in the last 20 years and is predicted to increase by >5% a year during the next 20 years  . Second, in the developing world a rapidly increasing population is putting pressure on scarce resources, leading to major population redistribution. This particularly involves the movement from rural to urban areas. Third, natural disasters such as droughts and floods have created approximately 25 million environmental refugees  . Finally, conflict, often a result of population pressures and environmental degradation, displaces vast numbers of people. We examine the impact of population movement on malaria transmission.\n\n【3】### A Typology of Population Movement\n\n【4】The decision-making process leading to population movement can best be understood in the light of \"push and pull\" forces  . When their needs can no longer be met in a particular environment, people move elsewhere. The \"push factor\" could be environmental degradation, population pressure on land, droughts, famines, conflict, or loss or lack of employment. When people are satisfied with their situation but believe that a move elsewhere will provide new and attractive opportunities, a \"pull factor\" is involved. This pull factor could be better political, economic, or social opportunities or improved living conditions. Push and pull factors can operate simultaneously; for example, people can be pushed by environmental deterioration and scarce resources and pulled by the economic opportunities offered by development projects.\n\n【5】Population movements can be differentiated by their temporal and spatial dimensions. Temporal dimensions include circulation and migration. Circulation encompasses a variety of movements, usually short-term and cyclical and involving no longstanding change of residence. Migration involves a permanent change of residence  . Circulation can be subdivided into daily, periodic, seasonal, and long term   . Daily circulation involves leaving a place of residence for up to 24 hours. Periodic circulation may vary from 1 night to 1 year, although it is usually shorter than seasonal circulation. Seasonal circulation is a type of periodic circulation in which the period is defined by marked seasonality in the physical or economic environment. This type of circulation involves persons or groups who are absent from their permanent homes during a season or seasons of the year. Long-term circulation, defined as absence from home for longer than a year, affects groups such as wage laborers and traders, who maintain close social and economic ties with their home area and intend to return.\n\n【6】In terms of spatial dimensions, the movements to and from malarious areas are of epidemiologic importance. People who move can be categorized as either active transmitters or passive acquirers  . Active transmitters harbor the parasite and transmit the disease when they move to areas of low or sporadic transmission. Passive acquirers are exposed to the disease through movement from one environment to another; they may have low-level immunity or may be nonimmune, which increases their risk for disease.\n\n【7】Based on the above definitions, a typology can be devised that identifies categories of population movement. Different activities can be associated with these categories, and these activities, in turn, can be associated with differing risks for malaria transmission. All types of population movement can be accommodated in this typology, and people may exhibit more than one type of mobility.\n\n【8】### Population Movement and Malaria\n\n【9】In developing countries, activities involving population movement include urbanization, colonization, labor related to agriculture and mining, and conflict. In industrialized countries, the impact of population movements on malaria risk is mainly related to intercontinental travel.\n\n【10】In some regions, malaria risk may increase as a result of a combination of different forms of mobility, as well as other factors unrelated to population movements. For example, in the African highlands, many of the issues described below act concurrently  . The categorization of the various examples below simplifies a complex issue but is useful for indicating major processes related to mobility and malaria risk.\n\n【11】### Urbanization\n\n【12】The world's urban population is growing at four times the rate of the rural population  . Urban pull is prevalent throughout the developing world, with rural-to-urban migration taking place faster than ever before  . Sub-Saharan Africa is the most rapidly urbanizing region in the world  , and the urban population in India has doubled in the last 2 decades  . When accompanied by adequate housing and sanitation, urbanization can lead to a decrease in malaria through reductions in human-vector contact and vector breeding sites. However, in developing countries, rapid, unregulated urbanization often leads to an increase in or resumption of malaria transmission because of poor housing and sanitation, lack of proper drainage of surface water, and use of unprotected water reservoirs that increase human-vector contact and vector breeding.\n\n【13】Although water pollution in urban areas usually leads to decreases in vector populations, some vectors, such as An. arabiensis in the forest belt of West Africa, may adapt to breeding in polluted waters  . In Asia, An. stephensi is proving adaptable to urban conditions, and in India it is a well-established vector of urban malaria. In urban areas of India, water is not supplied regularly and is stored in houses, providing extensive breeding places for An. stephensi in overhead tanks and cisterns. In periurban areas, where 25% to 40% of the urban population live in poor housing without proper water supply and drainage, another vector, An. culcifacies, also transmits malaria  .\n\n【14】In India, the fact that the National Malaria Eradication Program concentrated only on rural areas, ignoring the problem of urban malaria, was one of the factors leading to a resurgence of malaria in the 1970s  . Several types of population movement contributed to malaria transmission in India. First, circulation from stable rural malaria areas to unstable urban areas had firmly established malaria transmission in urban areas. Then, after the National Malaria Eradication Program, rural areas became free of endemic malaria but were receptive, so circulation from urban areas back to rural areas reintroduced malaria transmission. Changes in vector behavior (exophilic and exophagic behavior limiting the effectiveness of spraying), vector resistance to insecticides, and increasing drug resistance, especially in Plasmodium falciparum  , also played a role. Population movement also contributed to drug resistance, with people of different immune status moving from endemic- to nonendemic-disease areas, accelerating transmission of resistant strains.\n\n【15】### Colonization of New Territory\n\n【16】Through the interaction of a number of factors, the colonization of unpopulated or sparsely populated areas may be accompanied by an increase in malaria. Settlers, who have low-level immunity or are nonimmune, may migrate into a disease-endemic area, spreading the disease. Initially, housing tends to be basic, leading to close human-vector contact. Moreover, housing is often near rivers or lakes to facilitate water collection, increasing the exposure of humans to mosquitoes. Activities to develop an area, such as deforestation and irrigation, can increase the number of vector breeding sites, contributing to an increase in malaria. Colonization may be accompanied by major building projects, such as dams, canals, highways, or mining activities--referred to as the tropical aggregation of labor--which can further enhance malaria transmission.\n\n【17】Reemergence of malaria through mobility related to colonization occurred in Brazil. Malaria had been practically eradicated from most areas of the Amazon region by the national malaria campaign in the 1950s and 1960s  . Since the 1960s, however, the incidence of malaria has increased dramatically because of massive population movements to colonize new territory. New highways were built in the 1960s, linking the Amazon region to the rest of the country and attracting laborers to work on road construction. In the 1970s, many more people were attracted to the region by agricultural settlements and hydroelectric projects. Finally, in the 1980s, the discovery of gold led to a greater influx of people, along with the establishment of hundreds of mines throughout the region. The population of Rondônia State, which received the greatest number of migrants, increased from 113,000 in 1970 to 1,200,000 in 1990. Malaria cases in Rondônia increased from 20,000 to 174,000 in the same period. In Brazil as a whole, approximately 50,000 cases of malaria were reported in 1970; by 1990, reports had increased to 577,520, representing 10% of the world's reported cases outside Africa  . Of this total, >98% were recorded in the Amazon region.\n\n【18】The types of population movement involved in the colonization of the Amazons are migration and long-term circulation from malaria-free areas of Brazil to the malaria-endemic Amazon region. The people involved are nonimmune passive acquirers who on becoming infected can become active transmitters. If these active transmitters return to their initial place of residence in a malaria-free but highly receptive area, they can reintroduce the parasite and initiate an outbreak of malaria. For example, in 1985, 26 new active foci of malaria were recorded in Brazilian states outside the Amazon region  . Settlers in the Amazon region are highly mobile, moving with daily, periodic, and seasonal circulation from settlements in unstable disease-endemic regions to hyperendemic-disease regions of the rainforests. This mobility keeps settlements unstable and at high risk for epidemics through the constant flow of parasitemic laborers .\n\n【19】### Agricultural Labor\n\n【20】Swaziland provides an example of how agricultural labor has changed the spread of malaria. In the 1950s, control measures (DDT spraying) were successfully implemented in the lowveld so that agricultural development could take place, and by 1959, malaria had been all but eradicated from Swaziland  . However, agricultural developments in the 1950s involving an irrigation project for the cultivation of sugar cane created conditions favorable for malaria. Vector density increased, along with a high frequency of feeding on humans, as no domestic or wild animals were around the project area to serve as alternative hosts. This resurgence of malaria was catalyzed by the reintroduction of parasite carriers in the form of migrant workers from disease-endemic areas of Mozambique, who were involved in migration or long-term circulation to work on the sugar estates in the 1960s and early 1970s  .\n\n【21】In Colombia, the annual parasite index (defined as the ratio between the number of cases reported and the population at risk) has increased threefold since the 1960s  . This increase seems to be related to the migration of nonimmune people to areas such as the Naya basin, where malaria is endemic, and to the circulation of groups within the Naya basin. The circulation is predominantly seasonal, related to agriculture. People descend from hills and terraces, where malaria risk is minimal, to the malarious delta zone to cultivate and harvest their crops. In doing so, they are exposed to the anopheline population of the area and are at high risk for malaria. A large number of people are involved in this circulation (approximately 60% of the area's population is mobile for approximately 4 months of the year), and this population density, combined with the large vector population, maintains transmission at high levels  .\n\n【22】### Refugees\n\n【23】The number of officially recognized refugees has steadily increased, from approximately 5 million in 1980 to >20 million in late 1994  . In addition, an estimated 25 million people have fled their homes but remain internally displaced in their countries of origin  . The displacement of large numbers of people and their circulation can favor malaria transmission. If refugees are nonimmune, they could travel through or to malarious regions and acquire the infection, and if they are infectious, they could disseminate the disease to other areas.\n\n【24】Malaria is one of the most commonly reported causes of death among refugees and has caused high rates of both illness and death among refugees and displaced persons in disease-endemic countries, such as Thailand, Sudan, Somalia, Burundi, Rwanda, and the Democratic Republic of Congo  . In recent outbreak among Burundian refugees at a refugee camp in northwestern Tanzania, deaths from malaria and anemia in children under 5 years of age have increased 10-fold since the outbreak, reflecting the lack of immunity in this age group  . In the Sahel region of Africa, where civil wars and conflicts have occurred for many decades and large numbers of displaced people live in resettlement or refugee camps often located in lowland disease-endemic areas, epidemics are common  .\n\n【25】As a result of 15 years of continuous war, which displaced hundreds of thousands of people, Luanda, the capital of Angola, underwent an unprecedented population increase in the 1980s. This population movement resulted in a shift in malaria endemicity in Luanda from hypoendemic to mesoendemic level within 5 years  . As a cause of child deaths, malaria moved from sixth to first place. Increasing parasite resistance to chloroquine also became a major problem. This situation arose because of the enormous influx of displaced people of low socioeconomic status into an environment with stagnant water reservoirs. The population movements that increased malaria transmission in Luanda were long-term circulation and migration from stable rural areas to an unstable urban area.\n\n【26】Besides movements of large numbers of people, wars and civil unrest tend to favor malaria transmission. The disruptive effect of war on agriculture and water management can increase vector breeding sites; the destruction of housing can increase human-vector contact; the destruction of cattle can prompt zoophilic vectors to become anthropophilic if their usual food supply is disrupted  ; and control measures can be seriously diminished if health-care facilities are reduced or unavailable.\n\n【27】### Intercontinental Travel\n\n【28】The intercontinental transfer of malaria can occur through the introduction of an infective vector into a nonendemic-disease area, as in so-called airport malaria, or through the movement of a parasitemic person to a nonendemic-disease area, as in imported malaria. Although the incidence of these cases is low, they account for most malaria transmission in industrialized countries.\n\n【29】### Airport Malaria\n\n【30】Airport malaria is defined as malaria acquired through the bite of an infected tropical anopheline mosquito by persons whose geographic history excludes exposure to this vector in its natural habitat  . The vector is usually introduced into a nonendemic-disease country on an international flight. For example, random searches of airplanes at Gatwick Airport (London) found that 12 of 67 airplanes from tropical countries contained mosquitoes  . After a mosquito leaves the aircraft, it may survive long enough to take a blood meal and transmit the disease, usually in the vicinity of an airport. In temperate climates, temperature and humidity can be favorable in the summer for the mosquito not only to survive but also to move around and perhaps lay eggs. With the enormous and continuing increase in air traffic, cases of airport malaria may increase. Several such cases are described below.\n\n【31】During a hot summer in 1994, six cases of airport malaria were identified in and around Roissy-Charles-de-Gaulle Airport  . Four of the patients were airport workers, and the others lived in Villeparisis, approximately 7.5 km away. Anopheline mosquitoes were thought to have traveled in the cars of airport workers who lived next door to two of the patients. In 1989, two cases of P. falciparum malaria were identified in Italy in two persons who lived in Geneva  . Another five cases of airport malaria were reported in Geneva in the summer of 1989  . High minimum temperatures were thought to have allowed the survival of infected anophelines introduced by aircraft. In Britain, two cases of P. falciparum malaria were observed in persons living 10 km and 15 km from Gatwick Airport  . Hot, humid weather in Britain may have facilitated the survival of an imported mosquito.\n\n【32】### Imported Malaria\n\n【33】Throughout the world, many countries are reporting an increasing number of cases of imported malaria  because of the great increase in long-distance travel in recent decades. For example, cases imported from Africa to the United Kingdom rose from 803 in 1987 to 1,165 in 1993, and the ratio of all imported cases of falciparum to vivax malaria rose from 0.76 in 1984 to 1.52 in 1993  .\n\n【34】Recently, a woman in Italy was infected with malaria through a bite from a local species, An. labranchiae  . This species was a common malaria vector in Italy until the country was declared malaria free in 1970. Local breeding sites, including isolated pools in dried-up irrigation channels, were identified, and the mosquito responsible is thought to have acquired the parasite after biting a parasitemic girl who had acquired malaria in India. Airport malaria was ruled out because of the distance from the nearest airport. This may be the first case of malaria introduced to Europe in 20 years and demonstrates the hazards of population movement (the parasite had been introduced from India) combined with human activities (providing vector breeding sites).\n\n【35】In the United States, recent outbreaks of presumed local mosquito-borne transmission have been reported in California, with migration from a disease-endemic area  . The people involved were migrant workers from malaria-endemic areas. An outbreak in 1986 involved 28 cases (in Mexican migrant workers) of P. vivax during a 3-month period  . The epidemic curve indicated secondary spread, which confirmed local mosquito-borne transmission.\n\n【36】In the early 1990s, outbreaks were identified in neighborhoods of Houston with many immigrants from countries with malaria transmission. These outbreaks occurred when the weather was hot and humid and thus conducive to the completion of the sporogonic cycle and the survival of female anophelines  . Given that climate change could lead to more favorable conditions for vector survival in Europe and the United States  , the increase in incidence in both airport and imported malaria is cause for concern. If temperatures increase, uninfected introduced or local mosquitoes could survive long enough after taking a blood meal from a parasitemic person for the completion of the sporogonic cycle of the parasite, thus enabling transmission.\n\n【37】### Conclusions\n\n【38】Population movements that either place people at risk for malaria or cause them to pose a risk to others cannot be stopped. However, prevention measures can address the causes of these movements which, in developing countries, are often prompted by need rather than choice; if living conditions and opportunities improved in their place of residence, people would not be forced to move. In cases where movements are unavoidable, people should be made aware of the risks and have adequate access to treatment. Epidemics are more likely to take place in areas where health care may be poor or lacking. Areas at risk for epidemics through the influx of infected people should be identified to avoid or control epidemics. Particular attention should be paid to urban areas, given the increasing number of cases of urban malaria and the ongoing trend toward uncontrolled urbanization.\n\n【39】The risk for increased malaria transmission through economic development of an area should be analyzed thoroughly before development begins. For example, sound environmental management can restrict vector breeding and reduce human-vector contact. Such management could include proper maintenance of irrigation systems and adequate water supply and sanitation facilities for workers. Personal vector-control measures, such as bed nets, and antimalarial drugs should be used.\n\n【40】Regarding the risks that airport and imported malaria pose for developed countries, measures should be taken to ensure that cases of malaria are promptly diagnosed and treated. Strengthened surveillance would help to prevent the reintroduction of malaria transmission by local mosquitoes, which could acquire the infection by biting persons with airport or imported malaria. Fortunately, given the economic resources and quality of health care available in the developed world, the likelihood is low that isolated outbreaks of malaria will lead to reestablishment of transmission.\n\n【41】The relationship between malaria transmission and population movement is complex, but future attempts to eradicate or control malaria will be futile if they are not based on understanding of this link. Because the current magnitude and diversity of population movements are unprecedented, this issue is worthy of attention.\n\n【42】Ms. Hall studied environmental health sciences at Maastricht University and toxicology and pharmacology at the University of London. The focus of her research is behavioral change in relation to environmental impact.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7ae7a268-9150-4f89-bbf3-61899468b0ab", "title": "Fatal Case of Enterovirus 71 Infection, France, 2007", "text": "【0】Fatal Case of Enterovirus 71 Infection, France, 2007\nEnterovirus 71 (EV71), like other enteroviruses (family _Picornaviridae_ ), induces mostly asymptomatic or clinically benign infections. The virus is a leading cause of foot and mouth disease and, above all, is an emerging agent of acute central nervous system disease (aseptic meningitis, flaccid paralysis, encephalitis). Since its identification in 1969, EV71 has been the subject of several studies, particularly over the past decade in the Asia-Pacific region where several outbreaks have been reported . Epidemics have also been described in the United States, Brazil, and Europe (Sweden in 1974, Bulgaria in 1975, and Hungary in 1978) . EV71 can be divided into 3 independent genogroups by molecular typing: A, B, and C . Genogroups B and C can be further subdivided into subgenogroups B1–B5 and C1–C5 . Subgenogroups can replace each other within a short period of time, but several genotypes can also cocirculate as seen in Malaysia .\n\n【1】Death caused by this virus occurs rarely; young children are especially at risk. This report describes a fatal case of acute pulmonary edema with rhombencephalitis that occurred within the course of an EV71 infection in France.\n\n【2】### The Case-Patient\n\n【3】In April 2007, a 17-month-old boy was referred to the pediatric emergency ward of the Brest University Hospital. He had hyperthermia, which had begun 48 hours earlier. Upon arrival, he appeared to be in good general condition, despite mild respiratory discomfort and episodes of vomiting. Nasopharyngitis was diagnosed. He was discharged with treatment consisting of symptomatic medication and oral rehydration.\n\n【4】He was readmitted to the pediatric emergency unit 12 hours later, in severe respiratory distress. Disorders of consciousness and drowsiness were observed. He was immediately given supportive intravenous corticotherapy in association with aerosol and oxygen therapy. He was subsequently transferred to the pediatric intensive care unit. Results of his chest radiograph were normal. Laboratory test results were as follows: leukocytosis 23,300/mm 3  (.5% polynuclear neutrophils), platelets 453,000/mm 3  , hemoglobin 116 g/L, and hematocrit 34.9%. The C-reactive protein value was elevated (mg/L) and liver function was conserved. Severe dyspnea, hyperglycemia (.02 g/L), and a decreased blood pressure (mm Hg) rapidly developed. After the onset of this acute respiratory distress syndrome, a second chest radiograph showed marked lung infiltration. The patient was intubated because of increased oxygen dependence. Cardiorespiratory arrest occurred during the induction of anesthesia. Despite cardiopulmonary resuscitation, he died <12 hours after his second admission to the pediatric emergency ward.\n\n【5】An autopsy was performed with the prior consent of the parents. Pulmonary edema and multiple foci of polymorph inflammatory infiltrate were present in lung samples. Encephalitic necrotic lesions were multifocal but predominant in the inferior brainstem and superior cervical medulla. Respiratory centers were affected, as were vegetative nucleates of the medulla oblongata . No inflammatory or necrotic areas were found in cardiac muscle.\n\n【6】An enterovirus was isolated in the MRC-5 cell culture from a bronchial aspirate taken just before death. An enterovirus was also isolated in an autopsy nasal swab. Other autopsy samples (lung, spleen, and urine) were positive for enterovirus by reverse transcription–PCR (RT-PCR) (Enterovirus consensus kit, Argene, Verniolle, France), but no virus was isolated from these samples. Brain tissue could not be used for virologic studies because of formalin fixation. The enterovirus strain isolated in cell culture was sent to the French National Reference Centre for Enteroviruses, where it was identified as EV71 by partial viral protein (VP) 1 sequencing . A second analysis was performed that targeted the 891 nucleotides of the VP1 gene, as described by Bible et al. This sequence was compared by phylogenetic analysis with 34 GenBank-selected VP1 enterovirus 71 strains . A phylogenetic tree was constructed by neighbor-joining, using the Kimura 2-parameter distance method to define relationships between the current isolate and other EV71 subgenogroups. The VP1 sequence of the French patient who died clustered with VP1 sequences belonging to the C2 subgenogroup . The 2 closest-matching isolates originated from 2 young children admitted to the Brest Hospital emergency ward a few months later, in June and July 2008, respectively. The first, 13 months of age, was admitted for acute respiratory syndrome, which rapidly resolved. Enterovirus 71 was isolated from the child’s bronchial secretions. The second child, 31 months of age, had acute gingivostomatitis, fever, and cerebellar syndrome evocative of meningoencephalitis. Enterovirus was isolated from buccal lesions, but no virus was detected by RT-PCR in cerebrospinal fluid (CSF), even though pleocytosis was present. He recovered quickly. The VP1 full-length sequences of the 3 strains were deposited in GenBank under the accession nos. FJ824734–FJ824736. The high nucleotide identity among the 3 strains (>98%), and their place and period of circulation suggest a probable identical ancestral strain, which remains undefined. The origin of the EV71 strain responsible for the substantial pulmonary edema described here remains unknown. The 3 strains isolated clustered with 3 other European strains, isolated in 2006 and 2008 in the United Kingdom , and with 2 Asian strains, isolated in 2007 in Thailand and in 2008 in Singapore and Thailand (GenBank, unpub. data).\n\n【7】### Conclusions\n\n【8】Acute pulmonary edema in EV71 infections was rarely reported before the 1998 outbreak in Taiwan . Since then, this disease, which is often fatal, has been more frequently described, with known prognostic factors, including clinical and biologic features such as central nervous system involvement, leukocytosis, decreased blood pressure, and hyperglycemia . The fatal infection reported here, which ran a biphasic course, featured all of these signs, with death occurring within a few hours of the onset of respiratory distress. This devastating syndrome is believed to result from the extensive damage of bulbar vasomotor and respiratory centers . In the study by Kao et al. all 21 patients who had acute pulmonary edema associated with these signs died within 4 hours of the development of acute respiratory distress syndrome .\n\n【9】No reports of fatal cases of EV71 infection in Europe have been made since cases were reported in Hungary in 1978  with the exception of a fatal case of panencephalitis associated with subgenogroup C2 in the United Kingdom . No single neurovirulent genotype appears to be associated with severe and fatal cases; at least 3 separate genotypes have been isolated from patients with fatal cases in Malaysia (Sarawak), Japan, and Taiwan . A similar subgenogroup such as C1 may be associated with complicated disease in Sarawak and asymptomatic infection in Norway . A specific marker for virulence has yet to be defined.\n\n【10】Further studies are required to estimate the prevalence of EV71 infection, and only extensive clinical, virologic, and anatomopathologic investigation may determine the actual prevalence of EV71 in severe and sometimes fatal neurologic diseases. EV71, like poliovirus, is not always recovered from CSF during central nervous system infection . This fact underscores the importance of virologic investigations on peripheral samples (throat swabs, urine, stool, and vesicles).\n\n【11】Since 2006, centers participating in the French Enterovirus Surveillance Network have declared a significantly growing number of EV71 infections . The international spread of EV71 outside the Asia-Pacific region needs to be vigilantly monitored, because both the possibility of an outbreak and the unpredictability of virus circulation patterns represent a public health concern.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6e78b301-ea74-406b-b913-aab0e9b3e21a", "title": "Drug-resistant Nontyphoidal Salmonella Bacteremia, Thailand", "text": "【0】Drug-resistant Nontyphoidal Salmonella Bacteremia, Thailand\n**To the Editor:** Despite improved public health, serious infections with nontyphoidal _Salmonella enterica_ remain a major clinical and public health concern in Thailand and worldwide . Life-threatening _Salmonella_ infections resistant to fluoroquinolones, extended-spectrum cephalosporins, or both, have been increasingly reported . Use of antimicrobial drugs for disease prevention and growth promotion in food animals has been implicated in this increase in drug resistance . Because of extensive global travel, such increases affect the medical community domestically and internationally . We report a pilot survey of drug resistance in _Salmonella_ spp. in Thailand.\n\n【1】We studied archival nontyphoidal _Salmonella_ isolates from bacteremic patients at King Chulalongkorn Memorial Hospital from January 2003 to October 2005 and from bacteremic patients in Thailand sent to the World Health Organization National _Salmonella_ and _Shigella_ Center in Bangkok during the first half of 2005. The isolates from these archives were nonoverlapping and were kept frozen at −80°C. Isolates were divided into _Salmonella_ serovar Choleraesuis and other nontyphoidal _Salmonella_ (non-Choleraesuis) because we observed that Choleraesuis isolates show a higher frequency of resistance to fluoroquinolones and extended-spectrum cephalosporins than non-Choleraesuis isolates. A standard Etest method (AB Biodisk, Solna, Sweden) was used to evaluate MICs for nalidixic acid, ciprofloxacin, and ceftriaxone. Susceptibility was defined according to the 2005 criteria for _Salmonella_ of the Clinical Laboratory Standards Institute (CLSI, formerly NCCLS) .\n\n【2】Isolates showed high frequencies of antimicrobial drug resistance . All _S_ . Choleraesuis isolates with ceftriaxone resistance also showed high levels of resistance to nalidixic acid (MIC ≥256 µg/mL); most of these also had reduced susceptibility to ciprofloxacin (MIC ≥0.125 µg/mL). Of 73 nalidixic acid–resistant _Salmonella_ isolates, 55 (%) required a ciprofloxacin MIC ≥0.125 µg/mL, 14 (%) required an MIC of 0.094 µg/mL, and 4 (%) required an MIC of 0.064 µg/mL. One patient with aortitis caused by ceftriaxone-resistant _S._ Choleraesuis died of a ruptured mycotic aneurysm.\n\n【3】In the food animal industry, the effect of using antimicrobial drugs has long been a subject of concern . Evidence from molecular epidemiologic studies  suggests that these concerns are genuine and that serious problems must be addressed. This concern is also supported by reports of fatal, invasive, nontyphoidal _Salmonella_ infections resistant to quinolones or extended-spectrum cephalosporins . In Thailand, enrofloxacin, a veterinary fluoroquinolone, is used in animals in the poultry, swine, and seafood industries. Ceftiofur, a third-generation cephalosporin, is used extensively in swine for treatment and prevention of disease and for growth promotion. When compared with previous susceptibility patterns , current nontyphoidal _Salmonella_ infections in humans in Thailand are more resistant to quinolones and cephalosporins. Susceptibility to nalidixic acid correlates well with reduced susceptibility to ciprofloxacin. An alarming increase in ceftriaxone resistance in _S_ . Choleraesuis may be associated with inappropriate cephalosporin use in swine farming. Major revisions in current policies for use of antimicrobial drugs in food animals in Thailand are warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "65d46e59-dd8a-4e73-baab-bda30e008303", "title": "Vibrio cholerae O1 Hybrid El Tor Strains, Asia and Africa", "text": "【0】Vibrio cholerae O1 Hybrid El Tor Strains, Asia and Africa\n**To the Editor:** _Vibrio cholerae_ is a water-borne pathogen that causes a severe watery diarrhea disease known as cholera. On the basis of variable somatic O antigen composition, >200 serogroups of _V. cholerae_ have been recognized. Classical and El Tor are 2 well-established biotypes within the _V. cholerae_ O1 serogroup, and they can be distinguished by differences in their biochemical reactions or phenotypic traits . In addition to phenotypic traits, genetic markers have recently been used in the identification of the biotypes of _V. cholerae_ . For example, the major toxin-coregulated pilus (TCP) gene, _tcpA_ , of the TCP cluster possesses classical- and El Tor–specific alleles that encode identical functions but differ in their DNA sequence composition; however, the _rtxC_ gene of the repeat in toxin (RTX) cluster is present in El Tor strains only and absent in classical strains . The cholera toxin, encoded by the _ctxA_ and _ctxB_ genes, is the principal toxin produced by _V. cholerae_ O1 and O139 and is responsible for the disease cholera. Heterogeneity within the _ctxB_ gene and protein was first reported in the early 1990s, and, on this basis, 3 _ctxB_ genotypes of the _V. cholerae_ O1 strains have been identified. Based on amino acid residue substitutions at positions 39, 46, and 68, all classical and US Gulf Coast El Tor strains have been categorized as genotype 1, the Australian El Tor strains as genotype 2, and the El Tor strains of the seventh pandemic and the Latin American epidemic as genotype 3. Genotyping of _ctxB_ has indicated that the classical strains harbor a unique cholera toxin gene that is not in the El Tor strains except for the US Gulf Coast El Tor clone . The US Gulf Coast hybrid El Tor strains that harbor the classical cholera toxin have been associated with sporadic outbreaks in the United States  and, until recently, had not been reported anywhere else in the world. Then in 2004, hybrid El Tor strains that encode the classical cholera toxin were isolated from cholera patients in Matlab, Bangladesh , and in Beira, Mozambique . In 2006, Nair et al. reported that the current seventh pandemic prototype El Tor strains had been replaced by hybrid El Tor strains in Bangladesh . We now report how far the hybrid El Tor strains have spread in Asia and Africa.\n\n【1】We examined 41 clinical _V. cholerae_ strains from Asia and Africa that were isolated from 1991 through 2004  and confirmed as serogroup O1 by O-antigen biosynthesis gene (rfbO1_ )-specific PCR. Biotyping was performed by using standard procedures, and all strains were confirmed as El Tor . All strains were PCR-positive for the El Tor–specific 451-bp _tcpA_ and 263-bp _rtxC_ amplicons but negative for the classical-specific 620-bp _tcpA_ amplicon. All 41 strains were PCR-positive for _ctxAB_ (bp) and produced cholera toxin, as demonstrated by the VET-RPLA Toxin Detection Kit (Oxoid, Basingstoke, UK). Sequence comparison of the PCR-amplified _ctxB_ gene (bp) of each strain with the reference strains (B and N16961) showed that 30 strains harbored classical cholera toxin (with histidine at position 39, phenylalanine at position 46, and threonine at position 68), whereas the remaining 11 strains carried the El Tor cholera toxin gene (with tyrosine at position 39, phenylalanine at position 46, and isoleucine at position 68) . The overall analysis showed that all test strains are El Tor biotype but that most harbor the classical cholera toxin gene.\n\n【2】The major finding of this study is that El Tor strains that harbor the classical cholera toxin gene are not limited to the US Gulf Coast, Bangladesh, and Mozambique; they have spread to several other countries in Asia and Africa. Since 1817, 7 cholera pandemics have occurred around the world. Firm evidence indicates that the fifth and sixth cholera pandemics were caused by the classical biotype whereas the most extensive and ongoing seventh pandemic is caused by the El Tor biotype. Since the onset of El Tor dominance in 1961, the classical strains have been gradually replaced by the El Tor strains and are now believed to be extinct. However, reports from Bangladesh , Mozambique , and this study have provided sufficient evidence to indicate that the classical cholera toxin gene has reappeared but that for these cases its carrier has been El Tor. Although how the classical cholera toxin in El Tor strains would affect _V. cholerae_ pathogenicity is unclear, cholera caused by the classical biotype is more severe, whereas the El Tor biotype is considered to be better able to survive in the environment . Given that cholera toxin is directly responsible for the major clinical sign of the disease, such a genetic change could result in substantial alteration in the clinical manifestation of cholera. Additionally, this subtle genetic change may also influence the effectiveness of current cholera vaccines, which could stimulate both antitoxic and antibacterial immunity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a3aaab48-880b-4b9c-98ae-771236941c24", "title": "Barriers to Creutzfeldt-Jakob Disease Autopsies, California", "text": "【0】Barriers to Creutzfeldt-Jakob Disease Autopsies, California\n**To the Editor:** The recent article by Louie et al. underscores a more general disparity between the need for autopsies in potential infectious disease deaths and our present national capacity . In addition to confirming Creutzfeldt-Jakob disease (CJD) and allowing the differentiation of classic and variant CJD, autopsies identify previously undetected infections, discover causative organisms in unexplained infectious disease deaths, and provide insights into the pathogenesis of new or unusual infections . This information is essential for public health and medical interventions.\n\n【1】As outlined by Louie et al. hospital autopsy rates have dropped to single digits, and concerns by pathologists about occupational risks and biosafety have likely contributed to this decline. Currently, the last stronghold of autopsy expertise is forensic pathology . However, the medicolegal death investigative system does not have jurisdiction over all potential infectious disease deaths nor is it adequately supported to assume the cases that are missed by our present hospital autopsy system. Additionally, many medicolegal and hospital autopsy facilities with outdated or poorly-designed air flow systems are ill suited to handle autopsies when infectious disease is suspected . Air-handling systems can be expensive to fix.\n\n【2】Reference centers such as the National Prion Disease Pathology Surveillance Center, while providing diagnostic expertise, fail to surmount the biosafety obstacles (real and perceived) that prevent pathologists from enthusiastically performing autopsies on those who died of potential infectious diseases, including prion diseases. One potential solution is the creation of regional centers of excellence for infectious disease autopsies that could operate in conjunction with a mobile containment autopsy facility . Such centers could provide diagnostic expertise as well as biosafety capacity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f40d7c55-861e-4fbb-805f-7ccd3a69cd25", "title": "Outbreak of Hantavirus Pulmonary Syndrome, Los Santos, Panama, 1999–2000", "text": "【0】Outbreak of Hantavirus Pulmonary Syndrome, Los Santos, Panama, 1999–2000\nHantavirus pulmonary syndrome (HPS) is an infectious disease typically characterized by fever, myalgia, and headache and followed by dyspnea, noncardiogenic pulmonary edema, hypotension, and shock . Common laboratory findings include elevated hematocrit, leukocytosis with the presence of immunoblasts, and thrombocytopenia . The case-fatality rate can be as high as 52% . The etiologic agent of HPS is any one of several hantaviruses carried by rodent hosts belonging to the family _Muridae_ , subfamily Sigmodontinae . Hantaviruses are most often transmitted to humans through the inhalation of infectious rodent feces, urine, or saliva. However, strain-specific virus transmission may occur from person to person .\n\n【1】HPS was first recognized in 1993 during an outbreak of severe respiratory disease in the Four Corners Region of the United States . Since then, 363 cases of HPS have been confirmed in the United States . Sin Nombre virus (SNV) was the first HPS-causing pathogen identified; its primary rodent reservoir host is the deer mouse, _Peromyscus maniculatus_ . However, four other hantaviruses, Bayou virus, Black Creek Canal virus, New York virus, and Monongahela virus, each with a different rodent reservoir, have been characterized in the United States and associated with HPS .\n\n【2】Since 1993, HPS has also been reported and confirmed in six countries in South America—Argentina, Bolivia, Brazil, Chile, Paraguay, and Uruguay . Several distinct hantaviruses have been associated with HPS, including Andes virus (Argentina, Bolivia, Chile, and Uruguay), Bermejo virus (Bolivia), Juquitiba virus (Brazil), Laguna Negra virus (Bolivia, Paraguay), Lechiguanas virus (Argentina), and Oran virus (Argentina) .\n\n【3】Before 2000, no human hantavirus infections had been reported in Central America. However, in February 2000, health officials in Panama reported a cluster of acute respiratory illnesses in residents of the district of Las Tablas in Los Santos Province from late December 1999 to February 2000 . Human illness was characterized by a febrile prodrome with rapid progression to moderate-to-severe respiratory distress. Serum specimens from three of four patients had immunoglobulin (Ig) M and IgG antibodies to SNV. In February and March 2000, an outbreak investigation by the Panamanian Ministry of Health, Gorgas Memorial Institute for Health Studies (Panama City), the U.S. Centers for Disease Control and Prevention (Atlanta, GA), and the Pan American Health Organization was conducted in collaboration with local medical and public health officials. Sequence analysis of virus genome from human serum samples and rodent tissue led to the identification of a novel hantavirus, Choclo virus, as the cause of HPS in this outbreak . This report summarizes the clinical, epidemiologic, and environmental findings of the investigation.\n\n【4】### Materials and Methods\n\n【5】##### Case Definition\n\n【6】A suspected case of HPS was defined as fever (temperature \\> 38.3°C) and unexplained acute respiratory distress requiring supplemental oxygen, with radiographic evidence of acute respiratory distress syndrome or bilateral interstitial pulmonary infiltrates . A suspected case was also defined as an unexplained respiratory illness resulting in death, with a postmortem examination indicating noncardiogenic pulmonary edema without identifiable cause . A confirmed case of HPS was defined as a clinically compatible illness plus the presence of one of the following; 1) hantavirus-specific IgM antibodies in acute-phase serum, 2) hantavirus-specific nucleic acid sequences by reverse transcriptase polymerase chain reaction (RT-PCR), or 3) hantavirus-specific antigen by immunohistochemistry .\n\n【7】##### Case Finding and Characterization\n\n【8】At visits to two hospitals in Panama City and one in the Las Tablas District that admitted patients with suspected HPS, hospitalized patients were examined and interviewed when possible, and medical charts were reviewed. To identify past HPS patients, retrospective chart reviews were conducted on admissions dating back to August 1999 at these hospitals. In addition, medical records of any previous or current suspected HPS patients admitted to other district hospitals in the Las Tablas Province were obtained and reviewed. Clinical case information was collected on a standard abstraction form.\n\n【9】To monitor the spread of disease in Las Tablas and other areas of Los Santos Province, an outbreak communications center was established at the Ministry of Health in Panama City and staffed by physicians, public health officials, and health educators. Operations of this center included 1) passive surveillance for suspected cases of HPS, 2) a public hotline that addressed symptoms and signs of HPS and methods of prevention, and 3) nationwide distribution of HPS educational materials. Staff physicians also called hospitals throughout Panama to promote awareness of HPS among medical providers.\n\n【10】##### Community Surveys\n\n【11】Surveys for hantavirus antibodies were conducted on March 5, 2000, in six neighborhoods in the Las Tablas District and one in the Guarare District in which confirmed HPS patients resided. The primary objectives of the surveys were to determine the prevalence of hantavirus infection within households and neighborhoods of case-patients and the frequency of mild and asymptomatic infection.\n\n【12】Approximately 10–15 households, including the case-patient household, were sampled in each of the seven neighborhoods. Teams composed of public health officials, nurses, and phlebotomists visited each neighborhood. The day before the survey, pamphlets explaining the survey in Spanish were given to each household that had agreed to participate. All participants were administered a standard questionnaire that asked about demographic characteristics, illness history, and rodent exposure. Team members were familiarized with the questionnaire before the survey started. A 10-mL blood specimen (mL for children) for hantavirus serologic testing was collected from each interviewed participant.\n\n【13】##### Hospital Survey\n\n【14】A hospital survey was conducted March 9–10, 2000, among healthcare workers who cared for confirmed HPS patients at a major hospital in Panama City. This hospital received most of its cases from Las Tablas. The objectives of the survey were to 1) serologically determine exposure to hantavirus among doctors, nurses, respiratory therapists, physiotherapists, and nurses’ aides who provided direct care to confirmed HPS patients and 2) assess whether person-to-person transmission occurred. Medical staff who provided direct medical care (i.e. < 1 m from the patient) to infected patients in the emergency room and medical intensive care unit were compared with those in the coronary or neurologic intensive care units who had no exposure to case-patients. A standard self-administered questionnaire was used to inquire about timing and amount of exposures, specific types of exposures, and precautionary measures taken. A 10-mL blood specimen for hantavirus serologic testing was collected from each surveyed participant.\n\n【15】##### Rodent Investigation\n\n【16】Small mammals were sampled to determine potential hantavirus reservoir hosts. Primary sampling methods were trapping and collecting small mammals around households of confirmed and suspected HPS patients and from two uninhabited locations in Los Santos. The habitat of each trapping area was described. Small mammals were initially identified in the field on the basis of external characters together with standard keys for the region. Definitive identification that used cranial characteristics was performed at the Museum of Southwest Biology, University of New Mexico, where voucher specimens from these small mammals are currently archived. Liver, kidney, spleen, lung, heart, and a whole blood sample were collected from each trapped rodent for diagnostic testing, and carcasses were preserved for rodent speciation. Trapping and sampling were performed according to established safety protocols . Panamanian team members were trained to trap and observe safety precautions when handling potentially infectious rodents.\n\n【17】##### Diagnostic Testing\n\n【18】All available serum specimens from suspected HPS patients were tested for IgM antibodies by using an IgM-capture format and inactivated SNV antigen and for IgG antibodies by an indirect enzyme-linked immunosorbent assay (ELISA) as previously described . Serum samples from survey participants, rodents, and other small mammals were tested for IgG antibodies by using SNV antigens. IgG-positive survey participants were also tested for IgM antibodies by use of SNV antigens. Positive findings with SNV antigens in the IgG- or IgM-capture ELISAs indicate infections with New World hantaviruses rather than with SNV specifically.\n\n【19】### Results\n\n【20】##### Characteristics of HPS Patients\n\n【21】Eleven case-patients with suspected HPS were identified and reported to the Panamanian Ministry of Health from December 25, 1999, to February 29, 2000 . Three patients (%) died; neither serum nor tissue samples were available for diagnostic testing from these three patients. The remaining eight cases were confirmed by the presence of IgM against hantavirus in at least two separate serum specimens from patients; all eight also had IgG antibodies, with five demonstrating IgG seroconversion after the first specimen. One additional patient, whose onset of illness was August 24, 1999, was identified retrospectively. Although acute-phase clinical specimens were not available for testing, review of medical records indicated that her illness was clinically compatible with HPS, and her IgG titer in February 2000 was elevated . She was subsequently confirmed as the ninth HPS patient.\n\n【22】The median age of the 12 patients was 42 years (range 26–58 years; seven (%) were women. The primary occupation of patients was as follows: secretary , housewife , truck driver/transporter , electrician , field pump worker , seamstress , teacher , security guard , and unidentified .\n\n【23】All cases occurred in the Las Tablas, Guarare, and Tonosi Districts of Los Santos Province. In Las Tablas and Guarare, eight towns had at least one patient with suspected HPS, and seven had one or more confirmed patients . Two cases, one suspected and one confirmed, were identified in Tonosi. No clustering of cases occurred at the household level.\n\n【24】Two patients denied handling or cleaning up rodent excreta before their illnesses, while a third admitted frequently killing and handling rodents. In interviews, family members of all case-patients from Las Tablas District noted marked increases in the number of rodents in and around the home from November 1999 to February 2000.\n\n【25】##### Clinical Description of Confirmed HPS Patients\n\n【26】The spectrum of preadmission symptoms of the nine confirmed patients is documented in Table 2 . The mean temperature on admission was 38.0°C (n = 6, range 37.3°C–39.6°C). The median systolic and diastolic blood pressure values were 100 mm Hg (n = 9, range 80–130) and 63 mm Hg (n = 9, range 50–80), respectively. The median pulse rate was 102/min (n = 8, range 60–172). The median respiratory rate was 23/min (n = 8, range 14–36). No patient was unconscious or cyanotic on admission. A temperature of \\> 38.5°C was documented in all patients at some time during hospitalization. Seven of nine patients were hypotensive during their illness (median lowest systolic blood pressure = 90, range 50–100), and three required inotropic therapy.\n\n【27】Laboratory values at admission and during hospitalization are listed in Table 3 . Eight of nine patients were thrombocytopenic either on admission or at some time during hospitalization. Five of nine had hematuria (urine erythroctyes >10 per how power field, while four of nine had moderate proteinuria (< 2+ on urine dipstick). Evidence of renal function abnormalities was present in three of nine patients whose serum creatinine levels were >2.0 mg/dL.\n\n【28】Three of nine patients had aspartate aminotransferase >500 U/L, alanine aminotransferase >300 U/L, total bilirubin >1.5 mg/dL, and direct bilirubin >1.0 mg/dL. Two patients had clinical evidence of liver disease—one with hepatomegaly and the other with scleral icterus. Hematemesis, melena, and coagulopathy (prothrombin time = 22 s, partial thromboplastin time = 167 s, fibrinogen = 161 mg/dL) also developed in one patient. Two of the three patients also had elevated serum amylase values (and 437 U/dL).\n\n【29】Seven of nine patients had chest x-ray examinations on admission. All had evidence of bilateral infiltrates, and one of seven had a radiographic pattern suggestive of pulmonary edema. Two patients who did not have chest x-ray examinations on admission subsequently were found to have bilateral interstitial infiltrates radiographically. Pulmonary edema of varying severity developed in four patients during the course of their illnesses; pleural effusions also occurred in three of these patients, and they were intubated within 24 to 72 hours after admission.\n\n【30】##### Community Survey\n\n【31】Interviews and serum specimens were obtained from 311 (%) of 376 residents of seven different neighborhoods of Las Tablas and Guarare Districts in which confirmed HPS patients were identified. These 311 survey participants represented 119 households. A minimum of one blood specimen was obtained from each household. Forty (%) of 311 survey participants had IgG against hantavirus. By sex, 25 (%) of 178 females and 15 (%) of 133 males had IgG antibodies. The median age of the 40 antibody-positive participants was 31.5 years (range 1–79). Age group prevalence estimates ranged from 9% (/53, >61years of age) to 21% (/28, 51–60 years of age). Of 47 children, 5 (%) < 10 years of age had IgG antibodies. Antibody prevalences among patients’ neighborhoods ranged from 6% to 31% . Two of 10 household members of confirmed patients had IgG antibodies. Six (%) of the 40 infected participants had visited a confirmed HPS patient.\n\n【32】Of 119 households surveyed, 31 (%) had at least one member who had IgG against hantavirus . Eight (%) of 96 households had two or more antibody-positive members, while 1 (%) of 54 households had three or more antibody-positive members. Prevalence of antibody-positive households (i.e. one or more antibody-positive members) per neighborhood ranged from 8% to 73%.\n\n【33】Among the 40 antibody-positive participants, occupations of those with the highest antibody prevalences were students (%), secretaries (%), agricultural workers (%), livestock or vegetable farmers (%), and housewives (%). Among the 40 infected participants, 33 (.5%) touched rodents, 31 (.5%) cleaned up rodent droppings (e.g. sweeping, mopping), and 29 (.5%) killed rodents after December 1, 1999. Fifteen (.5%) noted increased numbers of peridomestic rodents compared with previous years.\n\n【34】Only five (.5%) of 40 antibody-positive participants recalled fever or myalgia since December 1, 1999. In contrast, the most common symptoms reported were upper respiratory in nature, such as rhinorrhea (%), sore throat (%), and cough (%). Moreover, of two participants who had both IgM and IgG antibodies, one experienced only a cough without fever, while the other was asymptomatic.\n\n【35】##### Hospital Serosurvey\n\n【36】Questionnaires and serum samples were obtained from 38 directly exposed and 39 unexposed healthcare workers. No IgM antibodies were present in the 77 workers. Only one of 38 exposed workers had IgG antibodies. This person, a medical resident, directly cared for five confirmed HPS patients (each healthcare worker cared for an average of 4 patients, range 1–5) for a total of approximately 15 patient-care days (group mean 43, range 1–70). His first exposure occurred in late December 1999. While caring for patients, he wore gloves, gown, and mask most of the time and was not directly exposed to respiratory secretions. He denied any travel to Los Santos Province after December 1999 and had no history of exposure to rodents. However, he was uncertain about whether he had visited Los Santos Province before December 1999. He denied having any febrile illness after late December 1999. Of the remaining 37 healthcare workers who cared for HPS patients, four (%) were directly exposed to respiratory secretions in the eye, nose, or mouth; 26 (%) wore gloves, gowns, and masks most of the time.\n\n【37】One unexposed worker, an operating room assistant, also had IgG against hantavirus. She denied travel to Los Santos Province, exposure to rodents, or febrile illness after December 1999. However, history of travel to Los Santos Province before December 1999 was again uncertain.\n\n【38】##### Rodent Investigation\n\n【39】Rodent traps were set at 13 sites, 10 of which were homes and immediate surroundings of patients with confirmed and suspected HPS. One of the three remaining sites was the home of a patient previously suspected to have HPS who did not have antibodies to hantavirus. The other two locations were a rural agricultural area in the Pocri District of Los Santos Province and a late secondary forest area (two subsites) near the town of Portabelo, Montijo District, Veraguas Province. In all, 120 rodents representing nine species and seven opossums (two species) were captured . Of the 120 rodents, 52 (%) were caught from the 10 patient household areas. A trap success of approximately 5% was achieved at these homes. Only one of the six antibody-positive rodents (Oligoryzomy fulvescens_ ) was captured at the household of a confirmed HPS case-patient (Las Tablas town); five were captured in the Pocri District.\n\n【40】### Discussion\n\n【41】We have documented the first human cases of hantavirus infection in Central America. A novel Panamanian hantavirus, Choclo virus, was subsequently characterized and is thought to be responsible for HPS during the outbreak . Virus genetic sequences of Choclo virus from case-patients were identical to those from _O. fulvescens_ .\n\n【42】The clinical spectrum of pulmonary disease among HPS patients in Panama varied widely from severe disease requiring intubation and cardiovascular support to mild pulmonary involvement with a benign clinical course. Extrapulmonary manifestations, such as hepatobiliary disease, hemorrhage, and central nervous system sequelae, were also present. The case-fatality rate (% among 9 confirmed cases; 25% among 12 total suspected and confirmed cases) was noticeably lower than that described in Paraguay (%), Chile (%), and the United States (%) in confirmed cases .\n\n【43】We found an antibody prevalence of 13% among household and neighborhood members of all ages from the outbreak foci. This figure is comparable to that found in Paraguay, but higher than in Chile and the United States . The percentages of hantavirus antibody-positive persons and households were particularly high in San Jose, the only town with two confirmed case-patients. Clustering of cases of HPS was not observed, and household clustering of antibody-positive persons was infrequent. Only 13% of hantavirus antibody-positive persons had a febrile illness after early December 1999, and none had an illness compatible with HPS, which suggests that mild infections occurred, as documented with other hantaviruses in the United States and Chile .\n\n【44】Person-to-person transmission of hantavirus infection was not demonstrated during the outbreak. Infrequent clustering of antibody-positive serosurvey participants by household probably reflected common exposures to infected rodent excreta peridomestically. Furthermore, only 1 of 38 medical care workers who cared for HPS patients had IgG antibodies, while none had IgM antibodies. Similarly, 1 of 39 workers who did not care for HPS patients had IgG antibodies. Thus far, person-to-person transmission has only been suggested during outbreaks caused by Andes virus in Argentina and Chile .\n\n【45】Climate data from Los Santos Province clearly demonstrated a two- to threefold increase in rainfall in September and October 1999 when compared to similar periods in the previous 4 years . Such atypical rainfall patterns may have led to increases in rodent populations, which led to more frequent contact between infected rodents and humans and subsequent human infection. Many residents of patient-neighborhoods of Las Tablas reported an unusually large number of rodents from December 1999 through January 2000. Similar patterns of environmental change followed by outbreaks of human disease have been observed in the United States and South America .\n\n【46】Ecologic observations in Los Santos provided an impression of a dry, open, and deforested region. Long-term commercial logging, agriculture (primarily corn and sugarcane), and animal husbandry practices (mainly cattle and horses) have contributed to the deforestation process. The rodent species most closely associated with corn and sugarcane fields, _Zygodontomys brevicauda cherriei_ , is a likely reservoir host for a new hantavirus, Calabazo virus . _O. fulvescens_ , the likely reservoir for Choclo virus, was found in grass of varying heights near human habitation and in cattle and horse pastures. Given the observed habitat associations of these two species, agriculture and animal husbandry practices in the Los Santos regions may have had a positive effect on populations of rodents associated with hantaviruses and may continue to augment the risk for HPS as the human population increases.\n\n【47】The social and economic impact of this outbreak was substantial. The cancellation of Carnival in Las Tablas, one of Panama’s most celebrated festivals, had profound emotional and financial effects on residents of Las Tablas. Nevertheless, the public health impact of holding Carnival could also have been substantial, given the potential for rodent exposure among thousands of visitors and participants.\n\n【48】In conclusion, this outbreak resulted in the first documented cases of human hantavirus infections in Central America. Although cases were not reported from other districts of Los Santos or other provinces of Panama during the investigation, surveillance for HPS nationwide should continue, as serologic testing capabilities have since been implemented in Panama. More extensive sampling of rodent populations would help identify other areas in Panama with large numbers of _O. fulvescens_ that could place residents at risk for Choclo virus infection. Longitudinal studies of rodents, particularly those species implicated as reservoir hosts, will be necessary to monitor the fluctuation and distribution of rodent population numbers over time and their correlation with human infection . Educational campaigns promoting risk reduction, such as proper clean-up of rodent excrement, sealing homes against the entry of rodents, and other rodent-proofing techniques, should continue as additional cases of HPS are reported in Panama.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bad26d93-b695-49c8-a4b2-0634245251a9", "title": "Zoonotic Cutaneous Leishmaniasis, Afghanistan", "text": "【0】Zoonotic Cutaneous Leishmaniasis, Afghanistan\n**To the Editor:** Recent reports from Afghanistan have focused on the current status of war- and refugee-related anthroponotic cutaneous leishmaniasis (ACL), caused by _Leishmania tropica_ , in Kabul City, refugee camps, and Fayzabad City (Badakhshan Province) . In central Asia, ACL is transmitted mainly by the sandfly _Phlebotomus sergenti_ in urban or periurban environments .\n\n【1】Zoonotic cutaneous leishmaniasis (ZCL), caused by _L. major_ , occurs autochthonously in Afghanistan, but little is known about its regional and seasonal distribution or other disease characteristics . Recently, cases have been reported in troops deployed to the Mazar-e Sharif area: 19 cases among the British military contingent in 2004, 186 among Dutch troops in 2005 (overall attack rate 20.9%), and 14 among German troops in 2005 . This yet-undescribed _L. major_ variant is highly aggressive, often disseminates and causes nodular lymphangitis, and is associated with delayed or poor response to treatment with sodium stibogluconate or miltefosine.\n\n【2】Regional epidemiologic reports, when available from Afghan or international health authorities, usually mention cutaneous leishmaniasis cases without further elaboration. To evaluate the current threat and specific epidemiology of cutaneous leishmaniasis in Mazar-e Sharif, medical entomologic and epidemiologic field investigations were conducted in June and October 2005. Results show that ZCL, ACL, and visceral leishmaniasis (VL) are endemic to Mazar-e Sharif. Data from patients seeking treatment from March 21, 2004, through March 20, 2005, at the Balkh Province Leishmaniasis Center, Mazar-e Sharif, showed that of 3,958 cases, 3,782 (.5%) were ZCL, 174 (.4%) were ACL, and 2 (.05%) were VL; the number of unreported cases during this time is unknown. A sharp increase (.4- to 5.4-fold) in ZCL cases was found when data from July 21 through August 20 and from August 21 through September 20, 2004, (and 169 cases, respectively) were compared with data for the same periods in 2005 (and 744 cases, respectively). ZCL chiefly affects farmers, nomads, and refugees. By sex, cases occurred in 1,991 (.6%) male and 1,791 (.4%) female patients. By age group, the rate for ZCL in young children was similar to that for other age groups: 1,167 (.9%) cases in children <4 years of age, 1,218 (.2%) cases in children 5–14 years of age, and 1,397 (.9%) cases in persons >15 years of age. In the leishmaniasis center, cutaneous infections are confirmed by seeing _Leishmania_ parasites in Giemsa-stained smears obtained directly from skin lesions. Although sporadically confirmed by culture and PCR, ZCL and ACL are usually differentiated by specific clinical aspects, especially of skin lesions: dry lesions characterize urban ACL; wet lesions, often superinfected and disseminating, characterize rural ZCL.\n\n【3】In terms of seasonal patterns in Mazar-e Sharif, ZCL peaks in mid-October and most ACL cases occur during mid-February . Observations at the leishmaniasis center in Mazar-e Sharif indicate that the incubation period for ACL is 4–6 weeks (associated with the cold, wet season) and that for ZCL is 8–12 weeks (associated with the hot, dry season). The minimum incubation period of ZCL in German and Dutch patients was 7 weeks, a figure derived from the known period of troop deployment rather than from hospital records.\n\n【4】The Mazar-e Sharif area, especially near the airfield, is semidesert, although extensive farming of seasonal crops is possible because of irrigation systems built in the 1980s. Irrigation canals are usually several kilometers long and 1.5–2 m deep and wide. Earth removed during canal construction is deposited as embankments that attract large numbers of the great gerbil, _Rhombomys opimus_ , the main animal reservoir of _L. major_ . Our studies showed an average of 3,380 _R. opimus_ burrows per hectare, the highest density yet recorded, far exceeding the previous record of >1,000 rodent burrows per hectare in nearby Turkmenistan . Much lower _R. opimus_ population densities were found at the Sakhi refugee camp, in the suburbs of Mazar-e Sharif, where Sherman box trapping for 18 nights was unsuccessful. Gerbil species were identified by photographs of live gerbils and by capture of 2. Both captured animals had positive results for _L. major_ by microscopy and PCR. Because human enhancement of _R. opimus_ habitat favors high infestations, the Mazar-e Sharif outbreak is an example of an anthropogenically induced emerging zoonosis. Sandfly surveillance was conducted in October 2005 by using unbaited CDC light traps (John Hock Co. Gainesville, FL, USA) and sticky traps placed in areas that were heavily infested with _R. opimus_ . _Phlebotomus papatasi_ , the principal vector of ZCL , was caught at low densities only, because of the cold night temperatures in October.\n\n【5】Our data suggest that foci of high-density enzootic ZCL occur in northern Afghanistan, especially in the Mazar-e Sharif area. ACL may also pose future problems because of its epidemic urban transmission potential . Effective disease surveillance and preventive measures should be promptly implemented to mitigate these health threats.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9ec9169a-99ba-42db-9f51-eb0c738771dc", "title": "Sequence Variability and Geographic Distribution of Lassa Virus, Sierra Leone", "text": "【0】Sequence Variability and Geographic Distribution of Lassa Virus, Sierra Leone\nLassa fever (LF) belongs to a group of viral hemorrhagic fevers characterized by a febrile syndrome and high case-fatality rates . LF differs from most viral hemorrhagic fevers in that it is endemic to a large geographic area of sub-Saharan Africa. Human cases of LF have been reported in (or imported from) Guinea, Sierra Leone, Liberia, Mali, Burkina Faso, and Nigeria; however, LF outbreaks seem to be restricted to Guinea, Sierra Leone, Liberia (the Mano River Union region), and Nigeria . In some areas of Sierra Leone and Guinea, more than half of the population has antibodies against Lassa virus (LASV; family _Arenaviridae_ ), the etiologic agent of LF . According to various estimates, 300,000–500,000 cases of LF result in 5,000–10,000 deaths annually in West Africa . An analysis based on seroepidemiologic data suggested that the number of cases might be much higher, reaching 3 million cases and 67,000 fatalities per year . Overall, the population at risk might include as many as 200 million persons living in a large swath of West Africa from Senegal to Nigeria and beyond .\n\n【1】LASV can cause infection in the multimammate rat (Mastomys natalensis_ ), a natural host and reservoir of this pathogen . The multimammate rat is a commensal rodent ubiquitous in Africa . Although the routes of LASV infection are poorly characterized, humans probably get infected by eating contaminated food , by inhaling virus-contaminated aerosols , or while butchering infected rat meat . Person-to-person transmission of LASV is well documented, mostly in the form of nosocomial outbreaks .\n\n【2】Like other arenaviruses, LASV is an enveloped virus with a bisegmented single-stranded RNA genome encoding 4 proteins using an ambisense coding strategy . The small segment contains genes for the glycoprotein precursor (GPC) and nucleoprotein (NP), which serves as the main viral capsid protein. The large segment encodes the small zinc-binding protein (Z), which contains a RING motif, and another gene (L) containing the RNA-dependent RNA polymerase domain.\n\n【3】Complete genome sequences are available for several LASV strains, as are a considerable number of partial sequences from isolates originating from humans and rodents . Their analysis revealed the existence of high sequence diversity (up to 27% nt) and 4 major lineages of LASV, which correlate with geographic location . Lineages I, II, and III, and the greatest diversity of LASV strains, were found among isolates from Nigeria, whereas strains from Guinea, Sierra Leone, and Liberia seemed to be more closely related and belong exclusively to lineage IV. Sequence of the AV strain  and recently published sequences from rodent LASV isolates from Mali  suggest the existence of an additional clade (proposed as lineage V) . LASV sequences of isolates from humans and rodents are found interspersed throughout the phylogenetic tree, which is consistent with the notion that human cases typically result from transmission from rodents .\n\n【4】The high degree of sequence divergence of LASV genomes is a major problem affecting the development of molecular and immune-based diagnostic technologies, vaccines, and possibly antiviral drugs . Forty-seven unique partial LASV sequences from Sierra Leone were available in GenBank at the time of this analysis, which included fragments of NP (sequences), GPC (sequences), L (sequences), and Z (sequences) genes plus full sequences of small and large segments of 2 strains—Josiah and NL. Most of these sequences are from isolates collected >30 years ago; only 2 more recent sequences (GPC and L gene fragments) from strain SL06-2057 were isolated in 2006 .\n\n【5】To fill this gap, we investigated the sequence diversity of strains circulating among small rodents captured in peridomestic settings in Sierra Leone. In 2014, we screened 214 samples collected during 2009 from several species of rodents trapped in villages where LF was reported in humans. We used diagnostic reverse transcription PCR (RT-PCR) and high-density resequencing microarrays to detect LASV and amplify fragments of NP, GPC, and L genes. The obtained amplicons were sequenced and compared with previously published sequences from Sierra Leone to obtain a more complete and updated picture of the strains circulating in this country.\n\n【6】### Methods\n\n【7】##### Rodent Sample Collection\n\n【8】The rodent samples collected were part of a separate project (L.M. Moses, unpub. data). Thirteen locations were selected for study in the LF-endemic region of eastern Sierra Leone. The geographic coordinates of the sampling locations and details of rodent trapping methods are available in the online Technical Appendix . Traps with captured small animals were processed in remote areas outside of the villages according to approved guidelines . The animals were anesthetized with isoflurane, and their morphometrics recorded. Animals were euthanized by exsanguination using cardiac puncture or cervical dislocation, and necropsies were performed. Spleen sections were stored in RNALater or TRIzol for RNA extraction (Life Technologies, Grand Island, NY, USA). Rodents were identified to the genus level in the field. Animals identified as _Mastomys_ sp. were further identified down to species level by using molecular methods as described previously .\n\n【9】##### Nucleic Acid Extraction\n\n【10】RNA from 10 mg of spleen of each rodent was extracted with TRIzol following the manufacturer’s recommendations. The samples were stored at –80°C.\n\n【11】##### RT-PCR and Sequencing\n\n【12】RNA were reverse-transcribed by using the SuperScript III Reverse Transcriptase kit (Life Technologies) according to the manufacturer’s instructions, and RT products were stored at –20°C. Specific oligonucleotide primer pairs were used for the PCR targets of interest  at final concentrations of 0.25 μM each. For PCR, 2 μL of RT reaction was used as template in 25 μL reactions containing 1.25 mM dNTPs, 1× Taq buffer, 0.2 μM each of primers, and 1.25 U FastStart Taq enzyme (Roche Diagnostics, Indianapolis, IN, USA). NP targets were amplified by using an initial 2-min denaturation at 95°C, followed by 40 cycles of 95°C for 30 sec, 55°C for 30 sec, and 72°C for 1 min. Some specimens produced poor PCR products, with low yields or multiple bands when we used published primer pair 1010C/OW1696R ; 1 μL of PCR product from those specimens was amplified in nested PCR by using the primer pair LAS\\_NP\\_F\\_1/LAS\\_NP\\_R\\_1 and the same thermal cycling program to generate DNA fragments suitable for sequencing . GPC targets were amplified by using 36E2 and LVS339-rev primers  and a PCR profile consisting of 2-min denaturation at 95°C, followed by 45 cycles of 95°C for 30 sec, 58°C for 30 sec, and 72°C for 1 min. L gene targets were amplified by using modified primers, LVL3359-F and LVL3754-R, based on published sequences  and a PCR program consisting of 2-min denaturation at 95°C followed by 45 cycles of 95°C for 30 sec, 53°C for 30 sec, and 72°C for 1 min. PCR amplicons were size-confirmed by electrophoresis by using 1.2% FlashGel DNA cassettes (Lonza, Walkersville, MD, USA) and purified on Zymo DNA Clean & Concentrator columns (Zymo Research, Irvine, CA, USA). All DNA sequencing was performed by Eurofins MWG Operon (Huntsville, AL, USA). The sequences were deposited into GenBank under the following accession numbers: NP sequences, KM406518–KM406556; GPC sequences, KM406590–KM406623; and L sequences, KM406557–KM406589.\n\n【13】##### RPM-TEI Microarray Analysis\n\n【14】The resequencing pathogen microarray (RPM) analysis was conducted by using Tropical and Emerging Infections microarrays (RPM-TEI v. 1.0; TessArae, Potomac Falls, VA, USA). The RPM-TEI microarray enables detection of 84 biothreat agents, including all lineages of LASV . Sample preparation was conducted as previously described . Pathogen identification was performed using the “C3 Score” identification algorithm .\n\n【15】##### Phylogenetic Analysis\n\n【16】We conducted the sequence alignment using the MUSCLE algorithm implemented in the MEGA 6.0 software package . In addition to partial NP, GPC, and L sequences obtained in this study, we included in the alignments all homologous sequences from these genes in samples collected in Sierra Leone (or clustering with Sierra Leone sequences) available in GenBank. Twenty-seven NP, 10 GPC, and 8 L sequences were available that meet these criteria. To root the trees, sequences from more distantly related, lineage IV isolate Z-158, which originated from Macenta district in Guinea, were used as an outgroup on the basis of the previous phylogenetic analyses . We also used MEGA 6.0 to perform statistical selection of the nucleotide substitution model for each sequence collection. We selected the Tamura 3-parameter model with discrete γ-distributed rate variation as the best-fitting model for NP and L sequence sets and the Kimura 2-parameter model with a fraction of evolutionary invariant sites for GPC sequences. The phylogenies were inferred by using the Bayesian, Markov Chain Monte Carlo method, as implemented in MrBayes v3.2.2 . The analysis was run without an assumption of a molecular clock. The resulting phylogenies were presented as 50% majority rule consensus trees in which the branches with posterior probability <0.5 were collapsed into polytomies. We manually adjusted the trees using FigTree v1.4.2 .\n\n【17】### Results and Discussion\n\n【18】We collected 681 small mammals during the field survey. Of these, we analyzed 214 for this study on the basis of RNA availability at the time of the study . These samples were obtained from rodents captured at 13 locations in 3 districts within the southern and eastern provinces of Sierra Leone . The rodents belonged to 6 genera; 199 were identified as _M. natalensis_ , which is consistent with published data on the ubiquitous presence of this species in domestic environments in West Africa . The other rodents (identified to genus level only) were _Rattus_ sp. ([4.2%\\] rodents); _Cricetomys_ sp. ([1.4%\\] rodents); and _Mus_ sp. _Praomys_ sp. and _Hylomyscus_ sp. ([0.5%\\] rodent each).\n\n【19】We screened all samples for LASV by RT-PCR using pan–Old World arenavirus (OWA) primers , which amplify a 670-nt section of NP gene. Because of poor results of NP amplification using OWA primers (inefficient amplification, multiple bands), we modified the screening protocol to include a second nested PCR using primers internal to the OWA amplification product, designed to amplify 650-nt segment of NP sequence and be more specific for lineage IV NP gene sequences. We obtained sequencing-quality NP amplicons from 39 samples using this protocol. For all of these NP-positive samples, we attempted to obtain RT-PCR amplicons for fragments of GPC and L genes using previously published or modified primers . The GPC and L amplifications were successful for most NP-positive samples and failed only in 5 and 6 samples, respectively .\n\n【20】In addition, we screened a randomly selected subset of 51 samples (representing 8 collection sites and 4 rodent genera) for nucleic acids of 84 different pathogens using RPM-TEI v. 1.0. Of the 51 samples tested using RPM-TEI, 9 were positive for LASV, and no other pathogens were detected in the analyzed samples. Although the percentage of positive samples was similar to that from RT-PCR results, the RPM-TEI failed to detect LASV in 3 samples that were positive for the NP gene by 2-step RT-PCR (LM34, LM58, and LM68). All these samples originated from the same village (Bumpeh), and no sample from this site was RPM-TEI positive, which suggests that detection failures might have been caused by inefficient amplification of the sequence variant of LASV circulating in the Bumpeh area with the RPM-TEI primers. On the other hand, RPM-TEI detected viral RNA in 2 RT-PCR–negative samples (LM591 and LM649), bringing to 41 the number of samples positive for LASV nucleic acids .\n\n【21】In summary, 41 LASV-positive samples were obtained from animals captured in 8 locations: Barlie (samples), Largo (samples), Bumpeh (samples), Ngiehun (samples), Koi and Yawei (samples each), Taiama (samples), and Saama (sample) . No LASV RNA was detected in samples collected from Gouma, Joru, Kenema, Panguma, or Segbwema. Lack of LASV detection in Kenema, Panguma, and Segbwema, areas well known to have regular LASV transmission, might be due to the small number of traps used. The town of Joru was extensively trapped, and no LASV was found. This finding is not surprising because Joru is south of the area where LASV is usually found.\n\n【22】All positive samples came from multimammate rats, which is considered the sole vector species for LASV . The results of LASV detection using several different RT-PCR strategies and a broad-range resequencing microarray (RPM-TEI v. 1.0) showed that none of the techniques applied alone detected viral RNA in all positive samples. This result underscores the difficulty of developing a truly universal diagnostic assay for this highly variable virus, even in the case of closely related strains belonging to lineage IV.\n\n【23】The analysis of the new sequences of LASV strains circulating in rodents in Sierra Leone indicated that the viral genome diversity is higher than previously estimated . For all available Sierra Leone sequences (including this study) the mean difference calculated for partial NP, GPC, and L sequences was 7.01% nt, 8.92% nt, and 9.83% nt, respectively, and 2.82% aa, 4.06% aa, and 0.71% aa, respectively . These differences are higher than the reported 4.6% nt and 1.7% aa differences based on partial NP sequences in a study with fewer isolates . The L gene fragment seemed to vary the most at the nucleotide level, followed by GPC and NP, which is consistent with previous observations . However, at the amino acid level, the GPC gene varied most, followed by the NP and L genes. The high conservation of the protein sequence of L gene fragment analyzed in this study (.71% mean difference) seemed have resulted from selection of a highly conserved part of L gene (located in RNA polymerase domain) when the diagnostic assay was designed .\n\n【24】The analysis of phylogenetic trees constructed by using all available partial sequences of NP, GPC, and L genes from Sierra Leone confirmed previous findings that the strains circulating in this country belong to lineage IV and are closely related to each other . The topology of the largest NP-based tree  strongly supports the hypothesis that the isolates from Sierra Leone belong to at least 3 distinct major clades (posterior probability 1.00 in all cases): the first clade (A), including a large cluster of strains originating from a group of villages to the north and east of Kenema in the Eastern Province (Bumpeh, Gondama, Koi, Konia, Largo, Ngiehun, Panguma, Segbwema, Taiama, Tongo, and Yawei); the second clade (B), including several strains isolated from rodents captured in Barlie (located a few kilometers southeast of Bo) and 1 isolate from Saama (located northeast of Kenema); and the third clade (C) represented by just 2 older human isolates from Mano and Mobai.\n\n【25】Phylogenetic trees based on GPC and L sequences  had similar topology and supported existence of clades A (with posterior probabilities 0.74 and 1.00, respectively) and B (with posterior probabilities 1.00 for both trees). However, the clade C was not present because the sequences for GPC and L gene fragments were not available for the strains forming this cluster in the NP-based tree. In addition to clades A and B, GPC- and L-based trees suggested existence of 2 additional and distinct clades. Clade D was represented by 2 sequences from human isolates SL25 and SL26, which formed a separate cluster (posterior probability 1.00 for both trees), and clade E represented by sequences obtained from a single strain isolated in 2006 (SL06–2057). These clades are defined by a very small number of sequences, and the GPC- and L-based trees disagree on the order of their separation from other clades. In addition, no data have been published on geographic origin of clade D and E samples. More data are needed (including corresponding NP sequences) to establish the existence and position of clades D and E with more certainty.\n\n【26】All of the trees indicate a high degree of geographic clustering of the strains. This kind of clustering has been reported previously over large geographic distances and is believed to have resulted from limited dispersal and migration of the host species . Results of this study show that this phenomenon also can be observed over relatively short distances. Isolates originating from multimammate rat specimens obtained in a particular location tended to cluster, and conversely sequences present in specific branches of the trees in many cases originated from a single location or few locations not far from each other. This kind of clustering could be observed especially well in samples from Barlie, Largo, Bumpeh, Konia, and Yawei .\n\n【27】In addition to the general pattern of geographic clustering, in several cases single isolates clustered with strains from different locations. For example, the sequence from a single sample from Saama (LM513) was closely related to that of strains from Barlie (based on NP sequence analysis). In another example, 1 GPC sequence originating from Liberia  clustered with Sierra Lone clade A sequences. In some cases (e.g. Saama sample LM513), such unusual clustering patterns may be explained by cross contamination or mislabeling of the samples. They also might result from relative proximity of all sampling sites and inadvertent anthropogenic transfer of rodents. Massive population movements that occurred in Sierra Leone during the 1991–2002 civil war could contribute to the process of mixing multimammate rat subpopulations carrying different LASV strains .\n\n【28】The geographic location of human cases at such a fine spatial scale can be problematic because humans can move large distances after exposure before disease is detected. For the human isolates, the clustering inconsistent with geographic location might have resulted from recording of the hospital location or patient’s current location as strain’s origin instead of the actual location of rodent–human transmission. For example, the NP-based phylogenetic tree indicates that human isolates from Segbwema and Gondama (obtained in 1996 and 1977, respectively) most likely originated from the Yawei village area because they cluster closely. A few other human isolates (SL15, SL20, and SL21) for which no location information is available also clustered with Yawei isolates on the basis of GPC and L sequences, suggesting their origin in the same area. These sequences were obtained in 2002 from United Nations peacekeepers stationed in this part of Sierra Leone .\n\n【29】Recent epidemiologic data show that LF was detected in 10 of 13 districts in Sierra Leone, which suggests that the infection is much more common that previously recognized . Phylogenetic analysis of the sequences revealed that strains circulating in districts to the west of the traditional hyperendemic area from which most sequence information is available differ significantly (clade B), which suggests that these could be distinct LASV strains that circulated in local multimammate rat populations for a long time since diverging from a common ancestor and are unlikely to have resulted from recent expansion of this rodent to new areas, as was recently suggested to explain emergence of cases from districts in which LF was not previously reported . Furthermore, the presence of LASV in Barlie with such high prevalence was surprising because this area historically has had few reports of LASV until 2 human LF cases reported in 2009 (L.M. Moses, unpub. data). The lack of reported LF cases from this area leads to speculation that clade B may be a less pathogenic form of LASV, and transmission to humans might have occurred previously but went unrecognized because of milder, nonhemorrhagic symptoms. In fact, the idea of broader area of LASV endemicity in Sierra Leone is consistent with results of serosurveys conducted during the 1980s by McCormick, who found seroprevalence levels ranging from 8% in southern coastal areas to 15% in villages in Northern Province .\n\n【30】Molecular characterization of isolates from a wider geographic area of the country is needed to fully understand the diversity of the LASV strains in Sierra Leone and its impact on disease distribution and risk. Such information would be useful for developing efficient viral detection technologies, for example, enabling design of PCR primers and antibodies specific for a broad range of LASV types. These diagnostic tests are extremely relevant to disease surveillance and monitoring and evaluation of interventions to prevent primary LASV infection in humans. More extensive information about sequence diversity affecting the antigenicity of the virus or the function of its RNA-dependent RNA polymerase may help in the development of vaccines and antiviral drugs. It will also lead to deeper understanding of the biology and pathogenesis of LASV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ed2a16b7-0a47-4722-aa95-b8066e1fc0f8", "title": "Unmet Needs for a Rapid Diagnosis of Chikungunya Virus Infection", "text": "【0】Unmet Needs for a Rapid Diagnosis of Chikungunya Virus Infection\n**To the Editor** : Chikungunya virus (CHIKV) has become a global health problem. Clinical manifestations are not specific and are difficult to differentiate from those of similar viral diseases (e.g. dengue and Zika virus disease). Diagnostic laboratories must be prepared to meet the changing epidemiology of viral diseases. CHIKV infection is currently identified by viral genome detection, using reverse transcription PCR (RT-PCR), viral culture, and serologic testing for IgG and IgM by indirect immunofluorescence (IFA) or ELISA. RT-PCR is most sensitive during the early phase of CHIKV infection (within 5–7 days of symptom onset), but its use is limited by the short viremic phase of the disease. After the acute phase, serologic testing for IgG and IgM is a more accurate indicator of disease. Molecular and serologic tests are complementary, reliable, and sensitive methods, but they require special equipment and a medium-to-high level of technical skill that may not be available in many laboratories, especially those in rural areas, where outbreaks usually occur.\n\n【1】Accurate and rapid detection of CHIKV infection by reliable point-of-care (POC) assays has been recommended to facilitate outbreak control. To meet this need, rapid CHIKV IgM POC tests are now available, but little information exists regarding their performance. The sensitivity of these tests evaluated in settings with a high prevalence of CHIKV infection is poor (range 1.9%–50.8%) compared with that for reference assays, especially in the acute phase of disease . In low-prevalence settings, CHIKV infection generally occurs as imported cases in travelers returning from disease-endemic countries. Diagnosis of such cases requires discrimination between CHIKV, dengue, Zika, and other febrile diseases in the differential diagnosis; this discrimination could be facilitated by the use of a reliable POC assay. The recent Zika virus disease outbreak in South America also highlights the worldwide need for rapid reliable POC tests.\n\n【2】From June 2014 through November 2015, eight patients who had returned to Italy from the Caribbean and Latin America were referred to the regional Center for Infectious Diseases, Amedeo di Savoia Hospital, in Turin for travel-associated CHIKV infection. These cases were the first in the region after 3 years without imported cases. We used IFA (Euroimmun AG, Lubek, Germany) and real-time RT-PCR (TIB MOLBIOL GmbH, Berlin, Germany) for CHIKV diagnosis. In addition, we evaluated the OnSite Chikungunya IgM Combo Rapid Test CE (CTK Biotech, San Diego, CA, USA) for CHIKV infection.\n\n【3】The rapid test identified IgM in only 3 of 8 patients (sensitivity 37.5%). All patients were negative for viral RNA, probably due to the time elapsed between symptom onset and serum sample collection, as confirmed by the presence of CHIKV IgG in most patients. No false-positive or invalid results were recorded with the rapid test on 30 CHIKV-negative serum samples (specificity 100%; positive and negative predictive values 37.5% and 100%, respectively).\n\n【4】Rapid and appropriate diagnostic tools are needed to slow or stop the worldwide spread of CHIKV. Rapid POC tests are highly cost-effective because they are easy to perform and can be disseminated to many laboratories for differentiating between diseases that are similar. Moreover, their results can easily be evaluated and shared within networks of reference laboratories.\n\n【5】However, our findings, in agreement with those of others, show that current rapid CHIKV tests perform poorly and need major improvement  . This poor performance might have several explanations. For example, CHIKV patients do not often seek medical care in the early course of the disease. Most patients in our study were no longer in the acute phase of illness: the diagnosis was made a mean of 16.8 (range 7–30) days after fever onset, and when tested, all patients were viral RNA–negative by real-time RT-PCR. POC reactivity generally increases in patients with illness duration of >1 week , but this was not the case in our study. Genetic differences in circulating CHIKV lineages could also explain poor testing performance. Furthermore, the OnSite Chikungunya IgM Combo CE POC test uses a recombinant antigen covering the 226 residues of the E1 gene from CHIKV variant A226; recent studies on CHIKV protein characterization showed that more sensitive serologic assays can be obtained using specific early-phase E2 glycoprotein as antigens .\n\n【6】The successful use of rapid immunochromatography-based assays with monoclonal antibodies to detect viral diseases (e.g. dengue) has encouraged the development of rapid immunoassays for CHIKV antigens, and preliminary results for these assays seem promising . External quality assessment programs for POC tests and quality controls consisting of standardized positive serum could also be helpful for improving the performance of diagnostic tests.\n\n【7】In conclusion, returning travelers are sentinels of the rapidly changing epidemiology of CHIKV; thus, they require a prompt diagnosis and careful surveillance for their possible role in subsequent autochthonous disease transmission. Implementation of user-friendly, rapid, and easily deliverable POC tests for a prompt and accurate laboratory diagnosis is therefore needed to improve patient management and disease control measures.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e674ef18-a402-401a-8867-be4d793cef83", "title": "Association of Chronic Obstructive Pulmonary Disease With Increased Confusion or Memory Loss and Functional Limitations Among Adults in 21 States, 2011 Behavioral Risk Factor Surveillance System", "text": "【0】Association of Chronic Obstructive Pulmonary Disease With Increased Confusion or Memory Loss and Functional Limitations Among Adults in 21 States, 2011 Behavioral Risk Factor Surveillance System\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Chronic obstructive pulmonary disease (COPD) is associated with cognitive impairment, but consequences of this association on a person’s functional limitations are unclear. We examined the association between COPD and increased confusion and memory loss (ICML) and functional limitations among adults with COPD.\n\n【3】**Methods**\n\n【4】We studied adults aged 45 years or older in 21 states who participated in the 2011 Behavioral Risk Factor Surveillance System (n = 102,739). Presence of COPD was based on self-reported physician diagnosis. ICML was based on self-report that confusion or memory loss occurred more often or worsened during the prior year. ICML-associated difficulties were defined as giving up household chores and former activities, decreased ability to work or engage in social activities, or needing help from family or friends during the prior year due to ICML. General limitations were defined as needing special equipment as a result of a health condition, having had activity limitations for 2 weeks or more in the prior month, or being unable to work. Multivariable models were adjusted for demographics, health behaviors or conditions, and frequent mental distress.\n\n【5】**Results**\n\n【6】COPD was reported by 9.3% of adults. ICML was greater among those with COPD than among those without COPD (.8% vs 11%; adjusted prevalence ratio \\[aPR\\], 1.48; 95% confidence interval \\[CI\\], 1.32%–1.66%). People with COPD, either with or without ICML, were more likely than those without COPD to report general functional limitations. Among people reporting ICML, those with COPD were more likely to report interference with work or social activities than those without COPD (aPR, 1.17; 95% CI, 1.01%–1.36%).\n\n【7】**Conclusion**\n\n【8】Functional limitations were greater among those with COPD than among those without, and ICML may further affect these limitations. Results from our study can inform future studies of self- management and functional limitations for people with COPD.\n\n【9】Introduction\n------------\n\n【10】Cognitive impairment among people with chronic obstructive pulmonary disease (COPD) has been observed in community populations . Details of the association between cognitive impairment and COPD are emerging; however, its consequences for people living with COPD are unclear. A systematic review concluded that future research on the effects of cognitive impairment on people living with COPD should include its impact on daily living and self-care . At least 1 study observed that adults with COPD had more functional limitations in activities of daily living and self-care than adults without COPD . However, the impact of cognitive impairment on activities of daily living and self-care among adults with COPD has received little attention and requires assessment.\n\n【11】Cognitive health issues may range from mild impairment to some type of dementia, including Alzheimer’s disease. People with COPD may have overall cognitive impairment or impairment in specific cognitive domains that affect information processing, attention, concentration, memory, executive functioning, and self-control . Memory problems are a common early sign of cognitive impairment , which may begin many years before it is clinically diagnosed. Therefore, self-reported memory problems have generated interest as a marker of cognitive decline. People who complain about memory loss appear to be at greater risk of developing mild cognitive impairment or dementia . However, some studies suggest that complaints about memory loss may be related to psychological distress and not necessarily to cognitive impairment .\n\n【12】Cognitive impairment in conjunction with COPD may be related to hypoxia or hypercapnia  or to acute exacerbations of COPD . In addition, cigarette smoking, a major cause of COPD, is associated with decreased cognitive function and increased cognitive decline ; the 2011 Behavioral Risk Factor Surveillance System (BRFSS) showed that almost 40% of adults with COPD that year were smokers . Furthermore, COPD frequently co-occurs with other chronic conditions , many of which also can involve cognitive impairment, such as heart and vascular diseases , diabetes , and stroke .\n\n【13】The 2011 BRFSS collected self-reported information on both physician-diagnosed COPD  and on confusion or memory loss  among participants in 21 states. This large sample of more than 100,000 respondents is probably reflective of the heterogeneous population that physicians encounter in general practice. We conducted cross-sectional analyses to examine 1) the association between COPD and increased confusion or memory loss (ICML), adjusting for potential confounders, and 2) differences in functional limitations by ICML and COPD status, Results of our analyses can inform self-management and community-based programs about the cognitive and functional needs of people with COPD.\n\n【14】Methods\n-------\n\n【15】BRFSS is a state-based telephone survey of adults aged 18 years or older, which is conducted by state health departments with assistance from the Centers for Disease Control and Prevention (CDC). BRFSS samples people with landline and people with cellular telephones to obtain representative samples of the noninstitutionalized adult population. Data are weighted to state population estimates using iterative proportional fitting or “raking.” Details on BRFSS design, methodology, sampling procedures, reliability, and validity are available . The survey includes core questions asked in all states and optional modules on specific topics, which states select based on their programmatic needs. Optional modules may include the full state sample or split samples to allow states to obtain information on more public health topics of interest to the state programs. In 2011, COPD status was part of the core BRFSS survey. An optional module asked 138,874 people in 21 states about increased confusion or memory loss (ICML) and associated functional difficulties. The 21 states were Arkansas, California, Florida, Hawaii, Illinois, Iowa, Louisiana, Maryland, Michigan, Nebraska, New Hampshire, New York, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Utah, Washington, West Virginia, and Wisconsin. The median response rate was 53.4% using response rate standards of the American Association of Public Opinion Research . BRFSS procedures were reviewed by the CDC Human Research Protections Office and determined to be exempt research. All analyses were performed using data from the BRFSS public use files.\n\n【16】We determined COPD status from responses to the BRFSS question, “Have you ever been told by a doctor or health professional that you have COPD, emphysema, or chronic bronchitis?” Increased confusion or memory loss was determined from responses to the question, “During the past 12 months, have you experienced confusion or memory loss that is happening more often or is getting worse?” We examined both ICML-associated functional difficulties (questions asked only of those reporting ICML) and general physical limitations (questions asked of all adults regardless of ICML status). BRFSS assessed ICML-associated difficulties by asking respondents who reported ICML how often in the previous year 1) they gave up household activities or chores they formerly engaged in because of ICML or 2) how often in the previous year ICML interfered with their ability to work, volunteer, or engage in social activities. We grouped responses for these 2 questions as “always,” “usually,” or “sometimes” versus “rarely” or “never.” Respondents also noted in which areas they needed most help because of ICML (safety, transportation, household activities, personal care, some other area, or no areas). We determined whether a respondent needed help in any category of help compared with needing no help. Additionally, respondents noted if they received help from family or friends in the previous 30 days because of ICML; we categorized responses as “always,” “usually,” or “sometimes” versus “rarely” or “never.”). We examined 3 general areas of physical or mental limitations. First we determined whether a respondent needed special equipment because of a health condition. Second we determined whether a respondent had 2 or more weeks of limited physical activity in the prior month; such determination was based on responses to questions asking whether respondents had any days when physical health was not good, had days when mental health was not good, and the number of days that poor physical or mental health kept them from their usual activities (eg, self-care, work, recreation). Third, we determined respondents’ employment status from reports of “unable to work.” Specific health reasons for these general limitations were not ascertained.\n\n【17】Demographic variables used in our analyses were age, sex, race/ethnicity (non-Hispanic white, non-Hispanic black, Hispanic, all others), education (less than high school, high school or equivalent, more than high school), number of adults in the household (≥2), health care insurance coverage, and state of residence. Health behaviors used were smoking status, obesity status, and physical inactivity. Current smokers were those who smoked at least 100 cigarettes in their lifetime and currently smoked every day or some days. Former smokers were those who smoked at least 100 cigarettes in their lifetime but did not currently smoke. Never smokers were those who had never smoked at least 100 cigarettes in their lifetime. Body mass index (BMI), calculated from self-reported height and weight, was categorized as underweight (BMI <18.5 kg/m 2  ), normal weight (BMI 18.5–24.9 kg/m 2  ), overweight (BMI 25.0–29.9 kg/m 2  ), and obese (BMI ≥30 kg/m 2  ). Respondents were asked if they engaged in any leisure time physical activity in the prior month (yes/no). Chronic disease risk factors and conditions were determined by asking, “Have you ever been told by a doctor or health professional that you have . high blood pressure, high cholesterol, heart disease (myocardial infarction or coronary heart disease), stroke, diabetes, or current asthma?” Finally, frequent mental distress (defined as psychological distress experienced for 14 or more days in the prior month) was assessed with the question, “Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?” . For our purposes, frequent mental distress captures more than clinically diagnosed depression and reflects a wider range of psychological distress.\n\n【18】We combined data for the 21 states and restricted analyses to respondents aged 45 years or older who were not missing data for COPD status (N = 105,332: 10,476 with COPD and 94,856 without COPD), because the sample sizes for those with COPD younger than 45 were too small for reliable comparisons. We used appropriate population weights for full and split samples for the ICML module. SAS (SAS Institute) and SAS-callable SUDAAN (Research Triangle Institute) were used to account for the complex sampling design and for variance estimation. We first examined differences in demographic and health characteristics by COPD status. We then examined differences in ICML by COPD status. Finally, we compared functional limitations by ICML and COPD status. Adjusted prevalence ratios were calculated by using 3 models. Model 1 was adjusted for sex, race/ethnicity, education, number of adults in the household, health care coverage, state of residence, and age (as a continuous variable). Model 2 was adjusted for model 1 covariables plus health behaviors and risk factors (smoking, obesity status, physical inactivity, high blood pressure, high cholesterol) and presence of at least 1 other chronic condition (heart disease, stroke, diabetes, current asthma). Model 3 was adjusted for models 1 and 2 plus frequent mental distress. For descriptive analyses, sample sizes were allowed to vary. Multivariable analyses were restricted to respondents with no missing data for all covariables so we could compare prevalence ratios across models for the relevant outcome. Significance was determined as _P_ < .05. We checked relative standard errors to assess stability of estimates.\n\n【19】Results\n-------\n\n【20】Among people aged 45 years or older in the 21 states studied, the weighted percentage with COPD was 9.3% (% confidence interval \\[CI\\], 8.9%–9.7%). Compared with those without COPD, a higher percentage of people with COPD were aged 65 to 84 years, were women, were white, had a high school education or less, were current smokers, were physically inactive, were obese, had other chronic disease risk factors and conditions, and had frequent mental distress . People with COPD were less likely to report Hispanic ethnicity.\n\n【21】Overall, 12.4% (% CI, 11.9%–12.9%) of people reported ICML during the previous year, and ICML was greater among people with COPD (.8%; 95% CI, 23.9-27.8) than among those without COPD (.0%; 95% CI, 10.5–11.6). Overall, the percentage reporting ICML was higher among those reporting stroke (.7%; 95% CI, 26.9–32.7) than it was among those with COPD (.8%), but was lower for those reporting current asthma (.3%; 95% CI, 20.1–24.6), heart disease (.4%; 95% CI, 19.7–23.2) or diabetes (.2%; 95% CI, 15.9–18.6).\n\n【22】Within each demographic, health behavior, or health condition group, the percentage with ICML was greater among those with COPD than without COPD . The only exception was among underweight people, for whom the percentage of ICML did not differ by COPD status because of large confidence intervals. Interestingly, among those with COPD, the percentage with ICML declined with age, whereas ICML was slightly higher among those in the two oldest age groups of those without COPD. Among both those with and those without COPD, prevalence of ICML tended to be higher among Hispanics than whites, lower with more attained education, higher among current smokers than among former or never smokers, and higher among those with chronic conditions or frequent mental distress than among those without. Prevalence of ICML increased with body weight both among people with COPD and among those without except among underweight people without COPD.\n\n【23】We used logistic regression to further examine the inverse association of age and ICML among people with COPD. When age was examined as a continuous variable, the inverse association of age with ICML among people with COPD was significant (odds ratio \\[OR\\]: 0.98; 95% CI: 0.97–0.99, _P_ < .001). However, this association was not significant after we adjusted for frequent mental distress (OR: 0.99; 95% CI: 0.98–1.00, _P_ \\= .19). Similar patterns were observed when we examined age grouped into 5-year intervals.\n\n【24】In multivariable analyses (N = 82,013 for all models; 7,888 people with COPD and 74,125 people without COPD) adjusting for demographics, the prevalence ratio of ICML for people with COPD versus those without COPD was 2.25 (% CI, 2.03–2.49). When further adjusted for health risk behaviors and conditions, the prevalence ratio was 1.69 (% CI, 1.51–1.89). After additional adjustment for frequent mental distress, the prevalence ratio was 1.48 (% CI, 1.32–1.66).\n\n【25】The Figure shows 1) general limitations among people with and people without ICML and 2) ICML-associated limitations among those reporting ICML, by COPD status. General limitations among those either with ICML or without ICML were greater among those with COPD than without the condition (P_ < .05). People with both ICML and COPD were more likely than those with ICML but without COPD to report having ICML-associated limitations or having received assistance because of ICML. ICML-associated limitations among those with COPD ranged from almost one-third reporting having received assistance from family or friends because of ICML to almost two-thirds reporting needing assistance in at least 1 of 5 domains (safety, transportation, household activities, personal care, or some other domain).\n\n【26】When adjusted for all covariables , people with COPD remained more likely than those without COPD to report each general activity limitation. This was true for both those with and those without ICML. For ICML-related limitations, after full adjustment, people with ICML and COPD were more likely than those with ICML but no COPD to report interference with work, volunteering, or social activities (aPR,1.17; 95% CI, 1.01–1.36) because of ICML.\n\n【27】Discussion\n----------\n\n【28】Associations between COPD and cognitive impairment are complex and may be due to numerous factors, including hypoxia or hypercapnia, combined effects of co-occurring chronic conditions with COPD, or effects of tobacco smoking. Although we did not have information on severity of either COPD or ICML, our results in a large sample of community dwelling adults aged 45 years or older suggested that people with COPD were more likely than those without COPD to report confusion or memory loss that worsened over the previous year, even after controlling for numerous potential confounders, such as demographic variables, co-occurring health risk factors and conditions, and frequent mental distress. General limitations were greater among those with COPD than those without regardless of ICML status, and the percentages reporting ICML-associated difficulties were substantial in this community-dwelling sample.\n\n【29】Subjective cognitive decline has received considerable attention in the literature as interest has grown in identifying early markers of cognitive decline. Previous studies suggest that subjective memory complaint is associated with psychological distress, yet is also independently related to development of dementia . We observed a higher prevalence of frequent mental distress among people with COPD than among those without, and associations between COPD and ICML were attenuated but remained significant after adjustment for frequent mental distress, suggesting that both ICML and psychological distress are associated with COPD. Both psychological distress and cognitive functioning can affect treatment, self-management, and quality of life for people with COPD; therefore, consideration of both factors may be important. A review of a growing body of research suggests that specific behavioral and psychological aspects of dementia might be amenable to intervention . Psychological factors involved with COPD may be addressed by pulmonary rehabilitation, although the benefits of pulmonary rehabilitation for cognitive decline among people with COPD remains unclear .\n\n【30】Our observation of an inverse association of ICML with age among people with COPD was at least partly explained by frequent mental distress, confirming potential confounding between psychological distress and ICML . Several other reasons should also be considered for results based on surveys of the general population. First, it is possible that recognition of memory problems is greater when symptoms and their effects on functional status are first noticed, especially if attributed to another condition. Additionally, our sample consisted of community-dwelling adults; adults with COPD or ICML who reside in institutional settings (who may have more severe COPD or ICML) are not represented. In a study that included 27,106 people with COPD in US nursing homes from 2009 through 2010, 61.9% had short-term memory problems and 43.3% had moderately or severely impaired cognitive skills . Finally, although the association of ICML and age by COPD status may be confounded by psychological distress, the inverse association of ICML with age among people with COPD may signify a healthy survivor effect.\n\n【31】Several limitations should be noted . First, these analyses relied on self-report of both COPD and ICML. How cognitive impairment influences self-reports is unclear. For example, respondents need to be able to answer questions over the telephone for the BRFSS, and it is not known how ICML might affect responses or ability to report functional limitations. BRFSS questions were cognitively tested but were not validated against a clinical population or measurement for either condition, although the COPD question is structured as in other health surveys. Self-reported information is also subject to recall and social desirability biases. Additionally, although we had a large sample, these results were based on adults in 21 states, limiting generalizability. Finally, analyses were cross-sectional and causality cannot be inferred. Furthermore, although we controlled for numerous chronic conditions, we were not able to control for neurological conditions other than stroke. However, our results support prior observations that those with COPD have more self-reported memory complaints. Also, because cognitive problems may begin many years before other symptoms appear or a clinical diagnosis is made, self-reports or those of proxy respondents are important to consider.\n\n【32】Although further research is needed, our results can inform treatment and self-management of people living with COPD. Because even middle-aged adults (age 45–64 y) with COPD were more likely to report ICML than were middle-aged people without COPD, health care providers should consider assessing the cognitive functioning of their patients with COPD because impairments may affect treatment and management outcomes. Furthermore, although the associations between ICML and COPD status was attenuated with adjustment for frequent mental distress, these associations remained, suggesting that both cognitive impairment and psychological distress should be assessed and monitored for effective treatment and self-management. Finally, specific functional limitations that may be of particular concern to people with COPD can be addressed.\n\n【33】Tables\n------\n\n【34】#####  Table 1. Demographic and Health Characteristics of Adults Aged 45 Years or Older in 21 States a  , by COPD Status, 2011 Behavioral Risk Factor Surveillance System\n\n| Characteristic | With COPD (N = 10,476) | Without COPD (N = 94,856) |\n| --- | --- | --- |\n| n | % (% CI) b | n | % (% CI) b |\n| --- | --- | --- | --- |\n| **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** |\n| 45–54 | 1,670 | 25.3 (.2–27.6) | 22,522 | 37.8 (.0–38.5) |\n| 55–64 | 3,000 | 29.7 (.7–31.8) | 28,583 | 29.2 (.6–29.9) |\n| 65–75 | 3,173 | 24.7 (.9–26.7) | 23,485 | 17.7 (.2–18.2) |\n| 75–84 | 2,116 | 16.5 (.3–17.9) | 15,335 | 11.8 (.4–12.2) |\n| \\=85 | 517 | 3.7 (.1–4.3) | 4,931 | 3.4 (.2–3.6) |\n| **Women** | 7,043 | 59.2 (.8–61.5) | 59,021 | 52.5 (.7–53.2) |\n| **Race/ ethnicity** | **Race/ ethnicity** | **Race/ ethnicity** | **Race/ ethnicity** | **Race/ ethnicity** |\n| White non-Hispanic | 8,485 | 74.8 (.4–77.1) | 75,095 | 69.2 (.4–69.9) |\n| Black non-Hispanic | 901 | 9.5 (.1–11.0) | 8,985 | 10.2 (.8–10.7) |\n| Hispanic | 260 | 7.7 (.3–9.4) | 3,683 | 12.8 (.1–13.5) |\n| Other | 830 | 8.0 (.5–9.8) | 7,093 | 7.8 (.3–8.3) |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| <High school | 1,913 | 25.6 (.4–27.9) | 8,987 | 16.2 (.6–17.0) |\n| High school or equivalent | 3,863 | 32.1 (.2–34.1) | 29,533 | 27.7 (.1–28.4) |\n| \\>High school | 4,675 | 42.2 (.1–44.4) | 56,032 | 56.0 (.3–56.8) |\n| **Only adult in household** | 5,004 | 32.3 (.5–34.2) | 35,047 | 23.2 (.7–23.8) |\n| **Have health insurance** | 9,534 | 89.1 (.4–90.6) | 86,717 | 88.2 (.6–88.8) |\n| **Smoking status** | **Smoking status** | **Smoking status** | **Smoking status** | **Smoking status** |\n| Never smoker | 2,435 | 21.5 (.9–23.3) | 51,046 | 52.6 (.9–53.4) |\n| Former smoker | 4,693 | 44.9 (.6–47.2) | 31,392 | 32.7 (.0–33.4) |\n| Current smoker | 3,311 | 33.6 (.5–35.7) | 11,937 | 14.6 (.1–15.2) |\n| **Physically inactive** | 4663 | 45.0 (.7–47.3) | 25,674 | 27.6 (.9–28.3) |\n| **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** |\n| Underweight (<18.5) | 360 | 3.2 (.6–4.0) | 1,349 | 1.3 (.2–1.5) |\n| Normal weight (.5–24.9) | 2,865 | 27.1 (.0–29.2) | 29,450 | 30.6 (.9–31.3) |\n| Overweight (.0–29.9) | 3,181 | 31.7 (.7–33.8) | 34,263 | 38.0 (.2–38.7) |\n| Obese (=30.0) | 3,708 | 38.0 (.7–40.3) | 25,755 | 30.0 (.3–30.8) |\n| **Other chronic disease risk factors** | **Other chronic disease risk factors** | **Other chronic disease risk factors** | **Other chronic disease risk factors** | **Other chronic disease risk factors** |\n| High blood pressure | 6,895 | 65.8 (.6–67.8) | 47,242 | 46.8 (.0–47.5) |\n| High cholesterol | 6,135 | 65.8 (.7–67.8) | 43,488 | 48.6 (.9–49.4) |\n| **Chronic conditions** | **Chronic conditions** | **Chronic conditions** | **Chronic conditions** | **Chronic conditions** |\n| Heart disease | 3,018 | 28.0 (.0–30.0) | 10,163 | 9.8 (.4–10.3) |\n| Stroke | 1,291 | 11.1 (.9–12.5) | 4,774 | 4.3 (.0–4.5) |\n| Diabetes | 2,681 | 25.5 (.6–27.6) | 14,980 | 15.9 (.4–16.5) |\n| Current asthma | 3,600 | 36.7 (.5–38.9) | 5,242 | 5.9 (.5–6.2) |\n| Any chronic condition c | 6,709 | 64.5 (.3–66.6) | 27,813 | 28.2 (.5–28.9) |\n| Frequent mental distress | 2,426 | 26.9 (.8–29.1) | 8,254 | 10.5 (.0–11.1) |\n\n【36】Abbreviations: CI, confidence interval; COPD, chronic obstructive pulmonary disease.  \na  Arkansas, California, Florida, Hawaii, Illinois, Iowa, Louisiana, Maryland, Michigan, Nebraska, New Hampshire, New York, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Utah, Washington, West Virginia, and Wisconsin.  \nb  Percentages are weighted to state population estimates. Totals may vary because of missing responses.  \nc  Heart disease (heart attack, coronary heart disease), stroke, diabetes, current asthma.\n\n【37】#####  Table 2. Percentage of Adults Aged 45 Years or Older Reporting ICML, by Demographic and Health Characteristics, Among People With and People Without COPD, 21 States a  , 2011 Behavioral Risk Factor Surveillance System\n\n| Characteristic | With COPD | Without COPD |\n| --- | --- | --- |\n| N | % (% CI) b | N | % (% CI) b |\n| --- | --- | --- | --- |\n| Total | 9,512 | 25.8 (.9-27.8) | 87,352 | 11.0 (.5–11.6) |\n| **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** |\n| 45–54 | 1,510 | 32.5 (.7–37.8) | 20,929 | 10.7 (.7–11.8) |\n| 55–64 | 2,762 | 27.0 (.5–30.9) | 26,662 | 10.5 (.7–11.5) |\n| 65–75 | 2,894 | 21.4 (.2–25.0) | 21,662 | 10.5 (.6–11.4) |\n| 75–84 | 1,890 | 20.7 (.6–24.2) | 13,796 | 13.1 (.9–14.4) |\n| \\=85 | 456 | 23.2 (.2–30.4) | 4,303 | 14.4 (.4–16.7) |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 3,104 | 26.0 (.6–29.7) | 32,984 | 10.9 (.1–11.8) |\n| Female | 6,408 | 25.7 (.4–28.0) | 54,368 | 11.1 (.4–11.8) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| White non-Hispanic | 7,755 | 25.0 (.0–27.1) | 69,785 | 10.2 (.7–10.7) |\n| Black non-Hispanic | 784 | 23.7 (.4–31.5) | 7,946 | 11.2 (.6–13.1) |\n| Hispanic | 234 | 36.1 (.3–47.3) | 3,213 | 14.0 (.9–16.3) |\n| Other | 739 | 26.4 (.1–36.7) | 6,408 | 13.6 (.8–17.2) |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| <High school | 1,689 | 28.9 (.5–33.7) | 7,798 | 16.7 (.7–18.9) |\n| High school or equivalent | 3,472 | 24.8 (.7–28.3) | 26,858 | 11.5 (.6–12.5) |\n| \\>High school | 4,336 | 25.0 (.2–28.0) | 52,531 | 9.3 (.7–9.8) |\n| **Number of adults in household** | **Number of adults in household** | **Number of adults in household** | **Number of adults in household** | **Number of adults in household** |\n| 1 | 4,530 | 28.8 (.0–31.8) | 32,034 | 12.3 (.6–13.1) |\n| 2 or more | 4,730 | 24.4 (.9–27.1) | 53,346 | 10.6 (.0–11.3) |\n| **Have health insurance** | **Have health insurance** | **Have health insurance** | **Have health insurance** | **Have health insurance** |\n| No | 836 | 31.4 (.1–39.8) | 7,182 | 12.8 (.0–14.7) |\n| Yes | 8,663 | 25.1 (.1–27.2) | 80,024 | 10.8 (.3–11.4) |\n| **Smoking status** | **Smoking status** | **Smoking status** | **Smoking status** | **Smoking status** |\n| Never smoker | 2,213 | 24.3 (.5–28.6) | 47,077 | 9.2 (.5–9.9) |\n| Former smoker | 4,256 | 22.3 (.8–25.1) | 28,947 | 12.2 (.3–13.1) |\n| Current smoker | 3,016 | 31.5 (.8–35.4) | 10,913 | 15.2 (.6–17.0) |\n| **Physical activity status** | **Physical activity status** | **Physical activity status** | **Physical activity status** | **Physical activity status** |\n| Any physical activity | 5,087 | 24.7 (.1–27.6) | 62,759 | 9.7 (.1–10.3) |\n| No physical activity | 4,401 | 27.1 (.3–30.0) | 24,454 | 14.6 (.5–15.7) |\n| **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** | **Body mass index (kg/m2)** |\n| Underweight (<18.5) | 314 | 21.9 c (.2–36.1) | 1,224 | 17.1 (.1–25.4) |\n| Normal weight (.5-24.9) | 2,611 | 23.6 (.3–27.3) | 27,199 | 9.9 (.1–10.8) |\n| Overweight (.0-29.9) | 2,896 | 25.9 (.4–29.7) | 31,727 | 10.2 (.3–11.2) |\n| Obese (=30.0) | 3,387 | 28.2 (.9–31.7) | 23,927 | 13.2 (.2–14.3) |\n| **High blood pressure** | **High blood pressure** | **High blood pressure** | **High blood pressure** | **High blood pressure** |\n| No | 3,268 | 22.6 (.7–25.9) | 43,915 | 9.1 (.4–9.8) |\n| Yes | 6,223 | 27.5 (.1–30.1) | 43,221 | 13.2 (.5–14.0) |\n| **High cholesterol** | **High cholesterol** | **High cholesterol** | **High cholesterol** | **High cholesterol** |\n| No | 3,258 | 21.4 (.4–24.6) | 40,959 | 8.5 (.8–9.2) |\n| Yes | 5,593 | 27.9 (.3–30.7) | 40,211 | 13.3 (.5–14.1) |\n| **Heart disease** | **Heart disease** | **Heart disease** | **Heart disease** | **Heart disease** |\n| No | 6,759 | 23.3 (.2–25.6) | 77,068 | 10.2 (.7–10.8) |\n| Yes | 2,728 | 32.3 (.3–36.5) | 9,215 | 18.4 (.5–20.4) |\n| **Stroke** | **Stroke** | **Stroke** | **Stroke** | **Stroke** |\n| No | 8,306 | 23.9 (.9–26.1) | 82,854 | 10.3 (.7–10.8) |\n| Yes | 1,144 | 40.1 (.6–45.8) | 4,264 | 27.1 (.8–30.6) |\n| **Diabetes** | **Diabetes** | **Diabetes** | **Diabetes** | **Diabetes** |\n| No | 7,113 | 24.7 (.5–27.0) | 73,568 | 10.3 (.7–10.9) |\n| Yes | 2,383 | 29.2 (.2–33.5) | 13,665 | 15.1 (.8–16.5) |\n| **Current asthma** | **Current asthma** | **Current asthma** | **Current asthma** | **Current asthma** |\n| No | 6,084 | 23.7 (.4–26.2) | 82,192 | 10.5 (.0–11.1) |\n| Yes | 3,256 | 28.5 (.2–32.1) | 4,795 | 18.3 (.4–21.6) |\n| **Any chronic health condition d** | **Any chronic health condition d** | **Any chronic health condition d** | **Any chronic health condition d** | **Any chronic health condition d** |\n| No | 3,460 | 21.1 (.3–24.2) | 62,033 | 9.1 (.6–9.8) |\n| Yes | 6,052 | 28.4 (.9–31.1) | 25,319 | 15.8 (.8–16.9) |\n| **Frequent mental distress** | **Frequent mental distress** | **Frequent mental distress** | **Frequent mental distress** | **Frequent mental distress** |\n| No | 7,077 | 18.8 (.9–20.9) | 78,366 | 8.4 (.0–8.9) |\n| Yes | 2,185 | 44.1 (.4–48.9) | 7,515 | 33.2 (.5–36.0) |\n\n【39】Abbreviations: CI, confidence interval; COPD, chronic obstructive pulmonary disease.  \na  Arkansas, California, Florida, Hawaii, Illinois, Iowa, Louisiana, Maryland, Michigan, Nebraska, New Hampshire, New York, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Utah, Washington, West Virginia, and Wisconsin.  \nb  Percentages are weighted to state population estimates. Totals may vary because of missing responses.  \nc  Relative standard error is 20% to 30%. Estimate should be used with caution.  \nd  Any chronic health condition: heart disease (heart attack, coronary heart disease), stroke, diabetes, current asthma.\n\n【40】#####  Table 3. Multivariable Adjusted Associations of Functional Limitations Among Adults Aged 45 Years or Older With Versus Without COPD, By ICML Status, 21 States a  , 2011 Behavioral Risk Factor Surveillance System\n\n| Limitation | N | With ICML, PR (% CI) b | N | Without ICML, PR (% CI) b |\n| --- | --- | --- | --- | --- |\n| Model 1 c | Model 2 d | Model 3 e | Model 1 c | Model 2 d | Model 3 e |\n| --- | --- | --- | --- | --- | --- |\n| **General limitation** | **General limitation** | **General limitation** | **General limitation** | **General limitation** | **General limitation** | **General limitation** | **General limitation** | **General limitation** |\n| Activity limited for 2 weeks or more in prior month due to physical or mental health | 8,645 | 1.85 (.63–2.10) | 1.54 (.35–1.77) | 1.43 (.26–1.63) | 72,695 | 3.36 (.96–3.81) | 2.16 (.88–2.49) | 1.82 (.59–2.09) |\n| Health problem required use of special equipment, such as a cane, wheelchair, special bed, or special telephone | 8,798 | 1.65 (.42–1.91) | 1.34 (.16–1.57) | 1.33 (.14–1.54) | 73,165 | 2.53 (.27–2.82) | 1.83 (.64–2.05) | 1.74 (.55–1.95) |\n| Unable to work | 8,788 | 1.92 (.66–2.21) | 1.46 (.24–1.72) | 1.42 (.20–1.67) | 73,071 | 3.40 (.92–3.97) | 2.10 (.76–2.50) | 1.92 (.60–2.30) |\n| **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** | **ICML related limitation** |\n| Gave up household chores or former activities due to ICML | 8,589 | 1.44 (.25–1.66) | 1.19 (.02–1.38) | 1.15(.98–1.34) | — | — | — | — |\n| ICML interfered with ability to work, volunteer, or engage in social activities | 8,577 | 1.45 (.27–1.66) | 1.22 (.05–1.41) | 1.17 (.01–1.36) | — | — | — | — |\n| Needed assistance in a domain f | 8,446 | 1.23 (.14–1.34) | 1.11 (.01–1.21) | 1.09 (.00–1.19) | — | — | — | — |\n| Family or friends provided assistance in prior 30 days due to ICML | 8,613 | 1.33 (.10–1.60) | 1.10 (.89–1.35) | 1.06 (.87–1.30) | — | — | — | — |\n\n【42】Abbreviations: CI, confidence interval; COPD, chronic obstructive pulmonary disease; ICML, increased confusion or memory loss; PR, prevalence ratio; —, not applicable.  \na  Arkansas, California, Florida, Hawaii, Illinois, Iowa, Louisiana, Maryland, Michigan, Nebraska, New Hampshire, New York, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Utah, Washington, West Virginia, and Wisconsin.  \nb  Comparison is between people with COPD and people without COPD (referent group).  \nc  Adjusted for sex, race, education, state, and age.  \nd  Adjusted for model 1 plus risk behaviors/factors and at least 1 other chronic condition (heart disease, stroke, diabetes, asthma).  \ne  Adjusted for models 1 and 2 plus frequent mental distress (weeks or more when mental health was not good).  \nf  Domains are safety, transportation, household activities, personal care, and other.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cb62dc69-7c43-4d73-8e54-223975f46cb4", "title": "Zoonotic Pathogens in Wildlife Traded in Markets for Human Consumption, Laos", "text": "【0】Zoonotic Pathogens in Wildlife Traded in Markets for Human Consumption, Laos\nConsumption of wildlife meat drives emerging infectious diseases , often amplified by human encroachment into natural areas and changes in land use. Wildlife trade and consumption have been responsible for outbreaks of diseases such as HIV-1 , Ebola , and monkeypox  and possibly for the coronavirus disease pandemic . Wildlife markets bring diverse species into contact, usually in dense and unsanitary conditions, enabling mixing, amplification, and transmission of pathogens among species, including humans . Small mammals host diverse pathogenic bacteria and viruses , but little investigation of endemic bacteria transmission has occurred. Determining pathogens present in traded wildlife is vital to guide appropriate measures to combat zoonotic diseases and document societal and environmental costs of wildlife trade.\n\n【1】### The Study\n\n【2】During December 2014–September 2017, we collected samples from 9 wildlife trade hotspots  and 2 roadside stalls (hereafter all referred to as trade sites) in Laos . In addition, 3 Provincial Offices of Forest Inspection (POFI) collected samples from wildlife confiscated in markets by law enforcement. After identifying wildlife at trade sites , we asked vendors for permission to sample their animals. Depending on whether the animal was alive, dead, or butchered, we collected urogenital swabs, urine and blood samples, and kidney, liver, and spleen tissue samples .\n\n【3】We extracted nucleic acid using QIAamp Viral RNA Mini Kits  with modifications . We conducted PCRs targeting _Leptospira_ spp. _Rickettsia_ spp. _Orientia tsutsugamushi_ , Anaplasmataceae, _Ehrlichia chaffeensis_ , _Anaplasma phagocytophilum_ , _Coxiella burnetti_ , flaviviruses, hantavirus, dengue virus, Zika virus, and universal bacterial 16S rRNA . Where necessary, PCR products were sequenced  and compared against GenBank through blastn . We performed descriptive, univariate, and multivariate analyses by using R version 3.6.2 . We assessed the effect of the wild meat processing status (alive, fresh, or frozen) on the risk for _Leptospira_ detection by using a mixed effects logistic regression with species as random effect. Statistical significance was set at α = 0.05 .\n\n【4】We collected 717 samples from 359 animals (trade sites: 461 samples from 324 animals; POFI: 256 samples from 35 animals); animals sampled were from \\> 37 identifiable vertebrate species from 12 families . Most were Sciuridae squirrels (.0%, 262/359) and represented 16 species, most frequently Pallas’s squirrel (Callosciurus erythraeus_ ) (.3%, 73/359). From trade sites, 69 animals (.3%, 95% CI 17.0%–26.2%) had \\> 1 samples positive for \\> 1 pathogens in 10 of 11 sites (.9%, 95% CI 57.1%–99.5%) . Of 324 animals tested, 65 (.1%, 95% CI 15.9%–24.9%) were positive for _Leptospira_ spp. 4/41 were positive for _Rickettsia_ spp. (.8%, 95% CI 3.2%–24.1%), 0 for _O. tsutsugamushi_ (%, 95% CI 0%–10.7%), and 2 for Anaplasmataceae (.9%, 95% CI 0.8%–17.8%) . Positivity was higher among animals collected by POFI; 25/35 (.4%) animals tested positive for \\> 1 pathogens. Of those, 9 were positive for _Leptospira_ spp. (.7%, 95% CI 13.1%–43.6%), 20 for _Rickettsia_ spp. (.1%, 95% CI 39.5%–73.2%), 2 for _O. tsutsugamushi_ (.7%, 95% CI 1.0%–20.5%), and 6 for Anaplasmataceae (.1%, 95% CI 7.2%–34.3%) . Sequencing identified _R. typhi_ , _R. felis_ , _R. conorii_ , an _Anaplasma_ species (either _A. centrale_ , _A. capra_ , or _A. marginale_ ), _A. platys_ , _A. bovis_ , _A. phagocytophilum_ , _Ehrlichia chaffeensis_ , _Lactococcus garvieae_ , and _Kurthia populi_ . No samples were positive for _C. burnetii_ (/76), flaviviruses (/359), dengue virus (/359), or Zika virus (/358).\n\n【5】Among species for which >10 individual animals were sampled in trade sites, 2 had particularly high proportions of _Leptospira_ spp.–positive specimens: the variable squirrel (Callosciurus finlaysonii_ ) (/28; 46.4% 95% CI 28.0%–65.8%) and the common palm civet (Paradoxurus hermaphroditus)_ (/22; 45.5%, 95% CI 25.2%–67.3%). _Leptospira_ spp.–positivity was higher in dry (/195; 25.6%, 95% CI 19.8%–32.5%) than wet season (/129; 11.6%, 95% CI 6.9%–18.8%) (χ 2  \\= 8.7; p = 0.003). Data disaggregation by species and province suggested that observed seasonality was driven by results in common palm civets and variable squirrels in Champasak Province. No association was detected between the probability of an animal testing positive for _Leptospira_ and the animal being alive (/22; 14%, 95% CI 3.6%–36%), freshly dead (/293; 20%, 95% CI 16%–25%; p = 0.6), or frozen (/9; 44%, 95% CI 15%–77%; p = 0.1). In a subset of _Leptospira_ spp.–positive animals with multiple samples, 75% (/24; 95% CI 53%–89%) of urogenital swab samples and 50% (/18; 95% CI 29%–71%) of blood samples were positive (p = 0.11 by Fisher exact test). _Rickettsia_ spp. were detected exclusively in solid organs (liver, kidney, and spleen).\n\n【6】Zoonotic pathogens were nearly ubiquitous across sites; 10/11 sites yielded \\> 1 pathogens. Squirrels are frequently traded in Lao markets  and had the greatest pathogen diversity in this study. _Leptospira_ spp. was identified most frequently, found in 20.1% of animals (>45% in variable squirrels and common palm civets). Variable squirrels are commonly traded, often in batches of 2 to 3 squirrels ; hence, on average, someone purchasing 3 variable squirrels would have an 83% likelihood of buying \\> 1 infected squirrel (p = 1 – (prevalence) 3  \\= 1 – 0.55 3  \\= 0.83). The higher risk for _Leptospira_ detection in the dry season is at odds with the typically described correlation of transmission with precipitation and flooding , suggesting that much remains to be understood of _Leptospira_ ecology. Other studies have shown higher prevalence in rats , and although we are confident of the results from trade sites, storage of animals from POFI sites might have resulted in cross-contamination, which warrants cautious interpretation of results in this subset. Among _Leptospira_ spp.-positive animals, detection was more likely in urogenital swab samples, highlighting the risk for transmission through infected urine . Although reservoir rodents are characterized by chronic renal infections, septicemia occurs during initial infection , and the high proportion of positive blood samples indicates a public health risk in relation to the consumption of uncooked or undercooked meat, organs, and blood. The PCR used to detect leptospires is specific for pathogenic and intermediate species , but we could not confirm their human pathogenicity. The high volume of squirrel trade combined with high infection frequency suggests a high risk for exposure among wildlife consumers. Because leptospirosis is a key cause of fever in rural Laos , further work is needed to learn more about the relevance of contact with wildlife through trade and consumption.\n\n【7】The Rickettsiales species identified here are known to cause human infections in Laos . _R. typhi_ causes murine typhus, a major underrecognized cause of fever . _O. tsutsugamushi_ is responsible for up to 23% of fever , and although commonly associated with ground-dwelling rodents, the vectors (Leptotrombidium_ mites) parasitize squirrels , and _O. tsutsugamushi_ has been isolated from _Callosciurus notatus_ squirrels in Malaysia . Other bacteria identified are reviewed elsewhere .\n\n【8】Although many of the human pathogens identified are transmitted by arthropod vectors, we found few arthropods in the wildlife sampled, probably because vectors leave animals quickly after animal death . Therefore, because most market vendors sell dead animals obtained from hunters or intermediaries , vendors are less likely to be exposed to disease vectors, and hunters are possibly at greater risk than market vendors or consumers. _O. tsutsugamushi_ and _R. typhi_ can cause infections through aerosol exposure, bites from infected animals, and needlestick injuries , but whether such routes of infection occur at trade sites is unclear. The frequent occurrence of _Leptospira_ , which can be transmitted by direct contact with abraded skin and mucous membranes, may pose health risks to hunters, vendors, and consumers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ce7f7b6a-ad58-4dfa-899c-c2fdc2a22958", "title": "Community-associated Methicillin-Resistant Staphylococcus aureus in Outpatients, United States, 1999–2006", "text": "【0】Community-associated Methicillin-Resistant Staphylococcus aureus in Outpatients, United States, 1999–2006\nThe past decade has seen a large increase in infections with hospital-associated methicillin-resistant _Staphylococcus aureus_ (HA-MRSA) . MRSA is one of the most common causes of nosocomial infections, especially invasive bacterial infections , and is now endemic and even epidemic to many US hospitals, long-term care facilities , and communities . Although community-associated MRSA (CA-MRSA) strains have been recognized as a leading cause of skin and soft tissue infections , especially in patients with no established healthcare risk factors , they also cause severe invasive infections . Recent reports based on genotypic evidence have suggested that CA-MRSA is likely spreading within hospitals as well, blurring the line between CA-MRSA and HA-MRSA infections .\n\n【1】Molecular typing studies have identified 2 MRSA clones, USA300 and USA400, as the primary types that cause CA-MRSA infections . Evidence suggests that emergence of these strains was independent of hospital strains . Thus, understanding the role of outpatients, who are among the likely carriers of CA-MRSA into a hospital, is useful for understanding the changing epidemiology of MRSA in hospitals. Outpatients, who outnumber inpatients by ≈3:1, may play a major role in the spread of CA-MRSA strains from the community to the hospital through their interaction with hospital staff or use of similar hospital resources, such as surgical rooms. However, limited information hinders understanding of long-term trends in CA-MRSA in outpatients in the context of changing epidemiology of inpatients. This lack of information hinders the ability to evaluate infection control methods in the face of a possible emerging epidemic of nosocomial infections caused by CA-MRSA.\n\n【2】Knowledge of trends in antimicrobial drug resistance rates for emerging pathogens are useful to clinicians to ensure high-quality care, which is essential for antimicrobial drug therapy, in which different drugs can have different costs and effectiveness. These trends can also help hospital administrators and policy makers make infection control investments to address the role that large influxes of outpatients with CA-MRSA infections may play with regard to overall MRSA infection rates in the hospital.\n\n【3】### Methods\n\n【4】To analyze trends in frequency of CA-MRSA and HA-MRSA, we studied changes in the proportion of isolates of each type that were found in inpatient and outpatient settings from a nationally representative sample of US hospitals during 1999–2006. Although genotypic analysis is the most reliable way of identifying MRSA strains, historical genotypic data on isolates are not available at the national level. An alternative approach is to ascertain strain type by using phenotypic susceptibility profiles. _S_ . _aureus_ susceptibility profiles are determined by the staphylococcal cassette chromosome (SCC) types on which the methicillin resistance gene, _mec_ A, is carried. Because CA-MRSA and HA-MRSA strains typically have different SCC _mec_ types, rules have been developed for determining the likely genetic makeup of an isolate on the basis of susceptibility results .\n\n【5】Phenotypic susceptibility results were obtained from The Surveillance Network (TSN) Database-USA (Focus Diagnostics, Herndon, VA, USA). TSN is an electronic repository of antimicrobial drug susceptibility data from a national network of >300 microbiology laboratories in the United States. Participating laboratories are geographically dispersed and make up a nationally representative sample based on patient population and number of beds. Patient isolates are tested on site as part of routine diagnostic testing for susceptibility to different antimicrobial agents by using standards established by the Clinical and Laboratory Standards Institute  and approved by the US Food and Drug Administration. Results are then filtered to remove repeat isolates and identify microbiologically atypical results for confirmation or verification before being included in the TSN database. Data from the database have been used extensively to evaluate antimicrobial drug resistance patterns and trends .\n\n【6】Genotypic analysis of phenotypically defined strains has found that in general, isolates of the USA300 strain, the one most commonly associated with CA-MRSA infections, are resistant to fewer antimicrobial drugs . Naimi et al. tested genetically determined CA-MRSA isolates against several antimicrobial drugs and found that they were typically susceptible to ciprofloxacin (%) and clindamycin (%). Similarly, King et al. found that 88% of CA-MRSA strains were resistant only to a β-lactam and erythromycin or a β-lactam only. Popovich et al. also found that susceptibility to a fluoroquinolone had a 90% positive predictive value for predicting a community-associated strain. Additionally, the number of antimicrobial drugs to which an isolate was susceptible was a reliable predictor of the genotype .\n\n【7】We analyzed _S_ . _aureus_ isolates that were tested for susceptibility to oxacillin (a proxy for all β-lactam antimicrobial drugs). Isolates classified as resistant according to Clinical and Laboratory Standards Institute breakpoint criteria were considered MRSA (<0.01% had intermediate resistance and were classified as susceptible). MRSA isolates, regardless of source (outpatient or inpatient), that were tested against ciprofloxacin or clindamycin, and \\> 3 other drugs and found to be resistant only to oxacillin were classified as CA-MRSA strains. Isolates resistant to oxacillin and \\> 1 other drug were assumed to be HA-MRSA strains. Other drugs tested were gentamicin, tetracycline, sulfamethoxazole/trimethoprim, and vancomycin.\n\n【8】Using this framework, we determined that the mean number of outpatient isolates analyzed annually was >50,000. Isolates were stratified on the basis of source (blood, lungs, skin, and other organs). Confidence intervals (CIs) for TSN data were calculated by using the Wilson score method incorporating continuity correction as detailed by Newcombe . Statistical analysis was performed by using Stata version 10 software (StataCorp LP, College Station, TX, USA).\n\n【9】### Results\n\n【10】Susceptibility to clindamycin, ciprofloxacin, gentamicin, tetracycline, sulfamethoxazole/trimethoprim, and vancomycin was used to infer genotypes of MRSA isolates during 1999–2006. During this period, there was a statistically significant reduction (p<0.001) in the number of MRSA isolates in outpatient areas resistant to ciprofloxacin (% to 56%), clindamycin (% to 30%), gentamicin (% to 3%), and sulfamethoxazole/trimethoprim (% to 2%). Our phenotypic rule, which was based on susceptibility to all drugs, found qualitatively similar results, with the number of MRSA isolates resistant to \\> 1 other drug decreasing from 87% to 46% during the period .\n\n【11】For outpatient data, the proportion of all _S_ . _aureus_ infections that were MRSA infections nearly doubled, from 26.8% (% CI 26.3%–27.3%) to 52.4% (% CI 52.0%–52.9%), over the study period. This increase was caused almost entirely by increases in isolates resistant only to oxacillin, which increased >7× from 3.6% (% CI 3.5%–3.7%) to 28.2% (% CI 28.0%–28.5%). The proportion of isolates resistant only to oxacillin increased for skin and soft tissue infections. However, increases were also observed in invasive blood and lung infections and other infections. Isolates resistant to \\> 1 other drug increased ≈5% during 1999–2001 from 23.2% (% CI 23.0%–23.5%) to 28.2% (% CI 28.0%–28.5%) before reaching a plateau. In 2005, the proportion of isolates resistant to oxacillin and 1 other drug then decreased back to almost the same percentage it started at. This pattern was driven by overall increases at all infection sites during 1999–2001 and later decreases at all collection sites except skin infections .\n\n【12】Among inpatients, the proportion of _S_ . _aureus_ isolates that were MRSA increased 25% from 46.7% (% CI 46.2%–47.2%) to 58.5% (% CI 58.0%–58.9%). Again, the increase was driven primarily by increases in the rate of isolates resistant only to oxacillin, which increased >7× from 3.3% (% CI 3.1%–3.4%) to 19.8% (% CI 19.4%–20.1%). Similar to outpatient data, the frequency of skin and soft tissue infections increased for isolates resistant only to oxacillin, although increases in blood, lung, and other infections were also observed. For isolates resistant to \\> 1 other drug, a slightly different pattern was observed than for the pattern of outpatient isolate resistance. Instead of a large increase, the proportion of MRSA isolates resistant to \\> 1 drug remained the same (≈43%–44%) until 2003 before decreasing >5% from 44.1% (% CI 43.7%–44.5%) to 38.5% (% CI 38.2%–38.9%) during 2003–2005. This decrease was largely caused by reductions in lung infections, although decreases were also seen in blood and other infections. Also different was the increase in MRSA skin isolates resistant to multiple drugs. There was an increase from 1999, but the increase was less (only 3%–4%) and appeared to plateau at ≈12%–13%.\n\n【13】### Discussion\n\n【14】We found during 1999–2006 that the percentage of _S_ . _aureus_ infections resistant to methicillin increased >90%, or ≈10% a year, in outpatients admitted to US hospitals. This increase was caused almost entirely by CA-MRSA strains, which increased >33% annually. Increases in the proportion of HA-MRSA isolates among outpatients were more variable, increasing ≈10% per year during 1999–2001 before the increase slowed; the proportion then decreased over the second half of the study period. This reduction in the growth of HA-MRSA isolates corresponds to a steep increase in the frequency of CA-MRSA skin and soft tissue infections among outpatients over an extremely short period, mostly during 2003–2005.\n\n【15】The frequency of CA-MRSA among inpatients increased nearly in conjunction with outpatient rates, overall and at each infection site. However, increases in blood and lung infections increased more among inpatients than in outpatients, which likely reflected the more severe status and increased likelihood of open wounds in inpatients. During this same period, rates of HA-MRSA decreased only ≈10%. Most of this decrease occurred during 2003–2005 and was mainly the result of a decrease in the frequency of HA-MRSA lung infections. This decrease was more likely the result of changes in empirical antimicrobial drug therapy for ventilator-associated pneumonia  than a consequence of any changes in the epidemiology of MRSA.\n\n【16】Despite increases in the proportion of CA-MRSA strains among inpatients, the continuing high level of HA-MRSA suggests that in contrast to reports from local institutions , CA-MRSA strains are adding to the problem of MRSA rather than replacing HA-MRSA strains. The fact that the frequency of HA-MRSA has decreased implies that some crowding out of HA-MRSA strains within the hospital may be occurring. However, lack of a decrease suggests that within the hospital, HA-MRSA strains may be more fit, and thus CA-MRSA strains are unable to replace them fully. The result is a coexistence of both strains in the hospital and maintenance of CA-MRSA because of the large influx of colonized and infected patients.\n\n【17】This finding is consistent with the biology of the 2 strains, which suggests differential fitness on the basis of the size of SCC _mec_ . In CA-MRSA strains, the predominant SCC _mec_ elements are types IV and V, which are smaller than the SCC _mec_ types typically found in HA-MRSA strains. These smaller genetic elements may increase the fitness of CA-MRSA strains outside hospital-related antimicrobial drug pressures, presumably by increasing mobility and growth potential . However, their increased susceptibility to antibacterial agents in the hospital leaves them at a fitness disadvantage. The result is that although the community has effectively become a reservoir for the CA-MRSA strains that are continually introduced into the hospital population without genetic changes, they are unlikely to replace HA-MRSA strains in the hospital.\n\n【18】The large proportion of infections caused by CA-MRSA strains in hospitals with high frequencies of HA-MRSA has implications for drug-prescribing patterns within hospitals. Because CA-MRSA strains are generally susceptible to more antimicrobial drugs, persons with these infections may be able to be treated with less expensive antimicrobial drugs with fewer adverse outcomes. Moreover, appropriate therapy can reduce the likelihood of emergence of other resistant pathogens, such as vancomycin-resistant enterococci. Initial empiric therapy of infections with the suspected etiology of CA-MRSA must be tailored to antimicrobial drug susceptibility patterns within the local community and be based on efficacy studies that suggest specific effectiveness targets.\n\n【19】Kaplan suggested that empiric therapy should be modified if >10%–15% of CA-MRSA isolates become resistant to a specific empiric therapy . Conversely, it may be appropriate to reintroduce a specific agent when susceptibility levels increase above a threshold. However, cycling strategies may not always be optimal , and no efficacy studies have been conducted to establish this target. In addition, we urge caution in applying national results to the CA-MRSA antibiogram of a specific area. Although results showed an overall trend at the national level, specific results at individual testing centers tended to be more variable. Moreover, local health officials and hospitals should coordinate their efforts to identify susceptibility patterns at the community level, rather than at the hospital level, to optimize the gains from investments in infection control .\n\n【20】The results of our study should be interpreted with caution because TSN provides information concerning only the site of isolate collection and not the infection. In addition, TSN only provides information on the collection location (i.e. outpatient or inpatient) and not case histories. Thus, some isolates may be difficult to classify in situations such as when an isolate was collected in the emergency department and then the patient was admitted or the patient was discharged and then returned as an outpatient. However, the effect of these situations is likely to be small because most isolates are from patients who can be classified as inpatients or outpatients.\n\n【21】A further limitation of the study is that although CA-MRSA isolate drug susceptibility patterns are technically genetically determined, the data enabled only phenotypic classification of isolates. In addition, as with any large time-series database, changes in surveillance or bias in the types of infections cultured over time, such as more severe or unusual infections, could alter the results. These findings suggest that more complicated bacteriology could alter the results. However, no general trend in the number of isolates collected was seen at individual testing centers, and resistance results from the TSN database were comparable to results of other national studies . Furthermore, the striking increases over the study period suggest that the trends are likely robust to any bias.\n\n【22】In summary, we examined the frequency of CA-MRSA and HA-MRSA in inpatient and outpatient settings. Our results indicate that outpatients may be a major reservoir of CA-MRSA, which will continue to enter hospitals, exacerbating the problem of MRSA. However, although CA-MRSA isolates have undoubtedly spread within hospitals and are likely to continue to do so, without changes in the fitness of different strains, CA-MRSA strains are unlikely to displace HA-MRSA strains within the hospital.\n\n【23】Our findings have implications for local and national policies aimed at containing and preventing MRSA. More rapid diagnostic methods are urgently needed to better aid physicians in determining appropriate empiric therapy. Strategies for prevention of infection and treatment of patients with CA-MRSA within healthcare settings should be coordinated primarily at the local level in accordance with local susceptibility profiles. Lastly, infection control policies should take into account the role that outpatients likely play in the spread of MRSA and promote interventions that could prevent spread of MRSA from outpatient areas to inpatient areas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d9548ef5-2471-41ea-a14e-6c3b753e2bb4", "title": "The Impact of Repeat Hospitalizations on Hospitalization Rates for Selected Conditions Among Adults With and Without Diabetes, 12 US States, 2011", "text": "【0】The Impact of Repeat Hospitalizations on Hospitalization Rates for Selected Conditions Among Adults With and Without Diabetes, 12 US States, 2011\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Hospitalization data typically cannot be used to estimate the number of individuals hospitalized annually because individuals are not tracked over time and may be hospitalized multiple times annually. We examined the impact of repeat hospitalizations on hospitalization rates for various conditions and on comparison of rates by diabetes status.\n\n【3】**Methods**\n\n【4】We analyzed hospitalization data for which repeat hospitalizations could be distinguished among adults aged 18 or older from 12 states using the 2011 Agency for Healthcare Research and Quality’s State Inpatient Databases. The Behavioral Risk Factor Surveillance System was used to estimate the number of adults with and without diagnosed diabetes in each state (denominator). We calculated percentage increases due to repeat hospitalizations in rates and compared the ratio of diabetes with non-diabetes rates while excluding and including repeat hospitalizations.\n\n【5】**Results**\n\n【6】Regardless of diabetes status, hospitalization rates were considerably higher when repeat hospitalizations within a calendar year were included. The magnitude of the differences varied by condition. Among adults with diabetes, rates ranged from 13.0% higher for stroke to 41.6% higher for heart failure; for adults without diabetes, these rates ranged from 9.5% higher for stroke to 25.2% higher for heart failure. Ratios of diabetes versus non-diabetes rates were similar with and without repeat hospitalizations.\n\n【7】**Conclusion**\n\n【8】Hospitalization rates that include repeat hospitalizations overestimate rates in individuals, and this overestimation is especially pronounced for some causes. However, the inclusion of repeat hospitalizations for common diabetes-related causes had little impact on rates by diabetes status.\n\n【9】Introduction\n------------\n\n【10】Hospitalization discharge data are often used to monitor diabetes-related outcomes, such as amputation and heart attacks, and to compare the risk for these outcomes among adults with and without diabetes . However, most hospital discharge data sources do not provide the information needed to separate individuals from events . For this reason, annual hospitalization rates are typically calculated by using the total number of hospitalizations (including repeat hospitalizations) instead of the number of individuals hospitalized (excluding repeat hospitalizations) in the calendar year. Although the inclusion of repeat hospitalizations in rates may provide a measure of service utilization, their inclusion could bias estimates of disease incidence and complicate analysis of disease trends, especially for chronic diseases.\n\n【11】The purpose of our study was to examine the impact of repeat hospitalizations on the magnitude of hospitalization rates for common diabetes-related outcomes or comorbidities and on the comparison of hospitalization rates for these causes by diabetes status.\n\n【12】Methods\n-------\n\n【13】We analyzed 2011 hospitalization data for adults aged 18 years or older from the State Inpatient Databases (SID) of the Agency for Healthcare Research and Quality (AHRQ) . SID contains information on inpatient discharges from hospitals in a given state, regardless of payer. The participation of 48 states in SID enables it to capture approximately 97% of all US community hospital discharges. SID has more than 100 clinical and nonclinical variables, including principal and secondary diagnoses and procedures, admission and discharge status, patient demographic characteristics, expected payment source, total charges, and length of stay.\n\n【14】We analyzed data only from 12 states (Arkansas, California, Florida, Hawaii, Iowa, Massachusetts, Mississippi, Nebraska, New Mexico, New York, Vermont, and Washington) whose discharge data distinguished repeat hospitalizations among individuals. The hospitalization data in the 12 states provided synthetic identification numbers (IDs) to link persons across events (ie, hospitalizations, emergency department visits, and ambulatory surgeries). The percentage of missing synthetic IDs for patients aged 18 to 64 was 2.1% for Arizona, 11.7% for California, 4% for Florida, 30.9% for Iowa, 6.5% for Massachusetts, 7.6% for Mississippi, none for Nebraska, 2.4% for New Mexico, 4% for New York, 7.3% for Utah, 13% for Vermont, and none for Washington . The percentage missing synthetic IDs for patients aged 65 years or older was less than 0.4% for Arizona, 2.6% for California, 1.2% for Florida, 19.1% for Iowa, 2.3% for Massachusetts, 8.4% for Mississippi, none for Nebraska, 2.5% for New Mexico, less than 0.5% for New York, 2.4% for Utah, 13.8% for Vermont, and none for Washington . For records missing IDs, we multiply imputed whether a case was a repeated hospitalization based on the person’s age, sex, race/ethnicity, insurance, median income category of residence zip code, and comorbidity score , using fully conditional specification methods. A sensitivity analysis showed no significant differences between estimates that used imputed data and estimates based on deleting missing data.\n\n【15】We excluded noncommunity or rehabilitation hospitals on the basis of information from the American Hospital Association’s Annual Survey Database. In our analyses, we defined repeat hospitalizations as more than 1 hospitalization of the same person for the same condition or discharge diagnosis, in a given calendar year. This concept is different from that of readmission, which typically uses a 30-day period and is used to examine quality of care.\n\n【16】Hospitalization rates, with and without repeat hospitalizations for selected causes, were calculated for people with and without diabetes. We used the first-listed discharge diagnosis code to identify hospitalizations due to the following causes: major cardiovascular disease (International Classification of Diseases, Clinical Modification, 9th Revision_ \\[ICD-9-CM\\] code of 390–434, 436–448), ischemic heart disease (ICD-9-CM code of 410–414,429.2), acute myocardial infarction (ICD-9-CM code of 410), heart failure (ICD-9-CM code of 428), stroke (ICD-9-CM code of 430–434, 436–438), peripheral arterial disease (ICD-9-CM code of 250.7, 440.2, 442.3, 443.81, 443.89, 443.9, 444.22), and lower extremity ulcer/inflammation (ICD-9-CM code of 454, 707.1, 680. 6–680.7, 681.1, 682.6–682.7, 711.05–711.07, 730.05–730.07, 730.15–730.17, 730.25–730.27, 730.35–730.37, 730.85–730.87, 730.95–730.97, 785.4). Lower extremity amputations were identified by using ICD-9-CM procedure code of 84.1 on the basis of all listed procedures, excluding traumatic amputation based on diagnosis code of 895–897 in any listed diagnosis. Among patients with diabetes, we identified emergency department visits for hypoglycemia by using a first-listed diagnostic code of 251.0, 251.1, 251.2, or 962.3; or a first-listed diagnostic code of 250.8 that was not accompanied by any of the following codes: 259.8, 272.7, 681, 682, 686.9, 707.1, 707.8, 707.9, 709.3, 730.0, 730.1, 730.2, or 731.8. Visits for hyperglycemic crisis were defined as those with 250.1 or 250.2 as the first-listed diagnosis. Discharges with any diagnosis code for diabetes (ICD-9-CM 250.x) on any patient hospitalization were considered diabetes-related.\n\n【17】To determine the number of patients with and without diabetes to be used as the denominator for the calculation of hospitalization rates, we used data from the 2011 Behavioral Risk Factor Surveillance System (BRFSS). BRFSS is a collaborative project of the Centers for Disease Control and Prevention (CDC) and US states and territories that collects information on health behaviors and conditions using state-based, ongoing, random-digit–dialed telephone surveys of noninstitutionalized US civilian adults aged 18 or older (n = 144,433 for the 12 states). The median response rate of all states in 2011 was 49.7% . The prevalence of diagnosed diabetes was calculated as the percentage of the population answering yes to the question “Have you ever been told by a doctor that you have diabetes?” Women who had been told that they had diabetes only during pregnancy and respondents told they had prediabetes or borderline diabetes were not considered to have diabetes.\n\n【18】We calculated rates of hospitalization (with and without repeat hospitalizations) and calculated rate ratios by dividing rates for diabetes by non-diabetic rates. All rates were age-adjusted to the 2000 US census population using age groups 18 to 44 years, 45 to 64 years, 65 to 74 years, and 75 years or older. Because hospitalization data are a complete enumeration of events, we do not associate any sampling error with them; the confidence intervals (CIs) of the rates reflect the sampling error of the denominators derived from BRFSS and the variance from multiple imputation. We examined the extent to which age-adjusted rates were overestimated by calculating percentage increases in rates caused by including repeat hospitalizations of individuals. The percentage increases in rates simplify to a form that includes only complete counts of hospitalizations and the variance induced by our multiple imputation methods was so small that we ignored it; therefore, the variances of percentage increases are zero.\n\n【19】All analyses were conducted by using SAS 9.1.3 (SAS Institute, Inc) and SUDAAN 11.0.1 (RTI International) to account for the complex sample designs of BRFSS. We used the delta method to estimate the standard errors associated with hospitalization rates and rate ratios .\n\n【20】Results\n-------\n\n【21】Our study included 10,384,306 hospital discharges from 12 states in 2011. Regardless of diabetes status, repeat hospitalizations occurred for all examined causes, although their impact on rates varied by cause . Compared with rates excluding repeats, rates including repeat hospitalizations for adults with diabetes ranged from 13.0% higher for stroke to 41.6% higher for heart failure; for adults without diabetes, rates including repeats were 9.5% higher for stroke to 25.2% higher for heart failure . In general, for each state  and overall, increases in rates caused by including repeat hospitalizations were higher for adults with diabetes than for those without diabetes. For heart failure and cardiovascular disease, the increase in rates due to repeat hospitalizations were approximately 10 percentage points higher for patients with diabetes compared with patients without diabetes. Hospitalizations due to hypoglycemia and hyperglycemic crisis, which are specific to patients with diabetes, also increased hospitalization rates when repeat hospitalizations were included. For hypoglycemia, the rate of increase was 9.9%, and for hyperglycemic crisis it was 50.9%.\n\n【22】Regardless of cause of hospitalization, rates for adults with diabetes were considerably higher than rates for adults without diabetes . Ratios of diabetes versus non-diabetes rates were similar regardless of whether repeat hospitalizations were included. Overall, rate ratios including repeats ranged from 2.9 (% CI, 2.8–3.0) for stroke to 29.6 (% CI, 28.0–31.1) for lower extremity amputation . Rate ratios excluding repeats ranged from 2.8 (% CI, 2.7–3.0) for stroke to 27.4 (% CI, 25.9–28.8) for lower extremity amputation. At the state level, rate ratios that included and excluded repeat hospitalizations were also similar .\n\n【23】Discussion\n----------\n\n【24】Using hospitalization data from 12 states, this study demonstrated that repeat hospitalizations for the same causes among individuals within a calendar year are common in populations with and without diabetes and that repeat hospitalizations are more frequent for some causes than others. Annual hospitalization rates including repeat hospitalizations ranged from about 10% higher for stroke to about 30% higher for heart failure in the total population, compared with those not including repeat hospitalizations. This finding suggests that hospital rates based on discharge data that include repeat hospitalizations may be considerably biased for estimating disease incidence. This is particularly true for conditions with a high proportion of repeat hospitalizations such as hyperglycemic crisis, for which inclusion of repeat hospitalizations increased rates by approximately 50%. In addition, when such data are used to examine trends, consideration should be given to the possible impact of any change in frequency of repeat hospitalizations. For example, successful efforts to reduce 30-day readmission rates could reduce repeat hospitalizations without reducing disease incidence rates among individuals.\n\n【25】Consistent with prior research indicating that adults with diabetes are at excess risk of hospitalization from cardiovascular disease, stroke, and lower extremity disease (including amputation) , our study also found that adults with diabetes were significantly more likely to be hospitalized for these conditions than adults without diabetes, even after adjusting for age. Our results also demonstrated that including repeat hospitalizations in the calculation of rates had only a modest impact on rate comparisons of populations with and without diabetes for common diabetes-related causes. For example, for acute myocardial infarction, the ratio of diabetes to non-diabetes rates was 3.9 (% CI, 3.7–4.0) when including repeat hospitalizations versus 3.7 (% CI, 3.5–3.8) when excluding repeats. Thus, although hospitalization rates of individuals were overestimated by including repeat hospitalizations, including them had little impact on the ratio of diabetes to non-diabetes hospitalization rates for common diabetes-related causes.\n\n【26】This study had several limitations. Because data from only 12 states were used to calculate hospitalization rates, our findings may not be generalizable to the national population or any one region of the country. However, our findings on the impact of repeat hospitalizations on rates and rate ratios were, for the most part, consistent across all 12 states, suggesting that the relationships described may be robust, especially because the 12 states evaluated were distributed across the country. A limitation of using administrative data, such as those from the SID, is the potential for inaccurate coding, and coding practices may vary across hospitals and states. The extent to which diabetes status is misclassified due to diabetes not being listed or incorrectly listed on discharge abstracts is unknown. Finally, because BRFSS included only noninstitutionalized people, our denominators for hospitalization rates were underestimated for both the diabetic and non-diabetic populations. Hospitalization rates are overestimated for both persons with and without diabetes because institutionalized people were not included in the denominator but were included in the numerator.\n\n【27】Surveillance practice commonly uses hospitalization data that include repeat hospitalizations among individuals to monitor the burden of disease. To our knowledge, ours is the first study to quantify the overestimation of hospitalization rates of individuals for common diabetes-related causes due to the inclusion of repeat hospitalizations and to determine whether this in turns affects comparisons of diabetes to non-diabetes rates. The results of this study demonstrated that the use of repeat hospitalizations substantially overestimated hospitalization rates for both the diabetic and non-diabetic populations and that this overestimation varied by cause. However, the inclusion of repeat hospitalizations for common diabetes-related causes had little impact on the comparison of rates by diabetes status, suggesting that such surveillance data may provide valid information on relative rates by diabetes status.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7bc8bc4-e73a-48ff-abf3-016685cd3e0c", "title": "Human Cases of Methicillin-Resistant Staphylococcus aureus CC398, Finland", "text": "【0】Human Cases of Methicillin-Resistant Staphylococcus aureus CC398, Finland\nAnimals may serve as a reservoir for methicillin-resistant _Staphylococcus aureus_ (MRSA). The MRSA lineage clonal complex (CC) 398 has been reported to be common among pigs  and has also been found among other animal species. Occupational exposure to animals has been recognized as a new risk factor for MRSA . In Finland, MRSA has occasionally been detected in pets and farm animals. MRSA CC398 was first recognized in Finland in 2007 and involved a veterinary hospital epidemic of 13 horses and 1 employee. Since then, MRSA CC398 has also appeared in other persons and has been found in pig samples. The objective of this study was to recognize possible connections of emerging MRSA CC398.\n\n【1】### The Study\n\n【2】Nationwide surveillance of MRSA in Finland, including notification for human cases and isolate characterization, was started in 1995 . From 1995 through April 2009, a total of 10,615 nonduplicate human isolates of MRSA were typed by using pulsed-field gel electrophoresis (PFGE); 1 in 2007, 6 in 2008, and 3 in 2009 (n = 10, 0.09%) were nontypable, a typical feature of MRSA CC398 isolates . Samples of animal origin are usually investigated clinically and through official surveys. No systematic national MRSA surveillance for animals exists, but it is possible to send suspected isolates for confirmation. After the horse epidemic in 2007, some admission screening of animals started at the veterinary hospital where the epidemic occurred. From January 2007 through April 2009, a total of 35 animal MRSA isolates were typed by PFGE; 20 (%) were nontypable.\n\n【3】For this study, all human and animal isolates of MRSA nontypable by _Sma_ I PFGE were further analyzed by testing antimicrobial drug susceptibility against 14 different antimicrobial drug groups, typing the _ccr_ and _mec_ complex regions within the staphylococcal cassette chromosome _mec_ (SCC _mec_ ) by PCR , detecting the presence of Panton-Valentine leukocidin (PVL) genes , _spa_ typing, and PFGE by using _Apa_ I as the restriction endonuclease. _Apa_ I PFGE was performed as described , except that the restriction digestion was performed with _Apa_ I at 30°C for 4 h, and initial and final switching times of 5 s and 15 s, respectively, were used for a 20-h PFGE run. Multilocus sequence typing (MLST)  was performed on representative isolates of each different combination of origin (human vs. animal species), _spa_ type, and SCC _mec_ .\n\n【4】For persons whose MRSA isolate was nontypable by _Sma_ I PFGE, a structured questionnaire was used to inquire about the presence of commonly known healthcare-related risk factors for MRSA . These data were collected from infection control nurses at relevant healthcare districts or directly from patients by telephone interview.\n\n【5】PFGE-nontypable isolates were found in samples from 10 humans, 13 horses, and 7 pigs. Based on combined results from _spa_ typing, SCC _mec_ , and PVL PCR, 1 single t2922 isolate and 4 clusters of isolates were identified: 14 isolates (t011, SCC _mec_ IV) related to the horse epidemic; 3 human isolates (t011, SCC _mec_ V) from 2 hospital districts (A and B); 5 human isolates (t034, SCC _mec_ V, PVL positive) from 1 hospital district (C); and 7 isolates (t108, SCC _mec_ V) from pigs . MLST analysis identified sequence type (ST) 398 in all but 1 isolate. This isolate had ST1375, a double locus variant of ST398. In _Apa_ I PFGE, 3 groups were identified . The human t034 isolates from 1 hospital district (C) fell into 1 group. Although all but 1 of the human t011 isolates were clustered together with the pig t108 isolates, subtype-level differences were still detected within this group. Isolates from the horse epidemic formed the third group, showing subtype-level difference from the t2922 human isolate. All isolates were resistant to tetracycline. Resistance patterns to erythromycin, gentamicin, tobramycin, and clindamycin (inducible or noninducible) varied but followed the same grouping as in _Apa_ I PFGE .\n\n【6】The human MRSA cases with CC398 isolates had no contact with each other. In 2 persons, the specimen was taken because of clinical symptoms, in 6 because of screening, and in 2 persons for unknown reasons . Only the employee of the veterinary hospital had direct animal contact. For others, no direct contacts with horses or pigs were identified, and none owned a pet .\n\n【7】### Conclusions\n\n【8】Our nationwide population-based surveillance for MRSA showed the emergence of MRSA CC398 in 2007, and in total 10 findings in humans through April 2009. Molecular typing of both human and animal MRSA CC398 isolates identified 4 clusters; in only 1 was human carriage linked to occupation and related to the horse epidemic. Another cluster consisted of pig isolates, and 2 other clusters consisted of human isolates.\n\n【9】Animal contact is an established risk factor for MRSA CC398 . We identified direct animal contact for only 1 person. The other patients with MRSA lacked or had negligible animal contacts; instead, most of them had previous healthcare contacts. According to the national guidelines for MRSA control, screening is indicated when a patient has previously carried MRSA, has been exposed to a MRSA carrier, has been recently cared for at a facility where MRSA is endemic, or during an outbreak. Although some persons were possibly screened because of recent exposure to another MRSA carrier, direct transmission in these instances was unlikely because these persons carried different strains. Connections to Asia have previously been reported for persons with PVL-positive, t034 MRSA . Two cases of PVL-positive, _spa_ type t034, MRSA infections in persons who had no contact with animals have also been reported from Sweden .\n\n【10】The proportion of CC398 of all MRSA in humans was far lower in Finland (.09%) than in the Netherlands (% in 2007) . Notably, the prevalence of MRSA in pigs is 40% in the Netherlands , whereas in Finland it is expected to be lower . The densities for pig and humans in the Netherlands are ≈60 and 30 times higher than those in Finland  respectively, and there are differences in screening policies. In the Netherlands, people in close contact with pigs or cattle are screened at hospital admission. It remains to be investigated whether the low occurrence of CC398 and lack of direct animal contacts among CC398 case-patients are related to lower occurrence of MRSA in animals, sparse pig and human populations, or screening policies in Finland.\n\n【11】Our questionnaire focused on healthcare-related risk factors and animal contacts. Although questions about traveling abroad were included, these data were difficult to interpret in relation to MRSA acquisition because travel is common. Furthermore, data for MRSA risk factors were not uniformly available for all patients. Another limitation was the lack of representative data for livestock, including pigs.\n\n【12】Except for the isolates from the horse epidemic, human and animal isolates were distinguished from each other by using a combination of molecular typing techniques. Additional typing by the _Apa_ I–PFGE method may help in resolving outbreaks. Despite closely related and typical livestock associated _spa_ types, _Apa_ I PFGE distinguished the t108 pig isolates from the t011 human isolates at a possibly related level (band differences). In addition, the presence of 2 different SCC _mec_ types among t011 and nonexistent links in time and space between human cases may suggest unrecognized transmission chains in the community.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "97302a91-3e29-4111-a413-ad11ca897a49", "title": "Change in Japanese Encephalitis Virus Distribution,Thailand", "text": "【0】Change in Japanese Encephalitis Virus Distribution,Thailand\nThe origin of Japanese encephalitis virus (JEV) was recognized before 1935, and JEV was isolated in Japan in 1935. The virus has since spread from India to Indonesia and within the past 3 decades has reached previously unaffected parts of Asia and northern Australia . JEV is one of the most widespread causes of viral encephalitis worldwide; an estimated 3 billion persons are at risk for infection, and 10,000 to 15,000 die annually . Although most human infections are asymptomatic (/1,000), 1/300 infections causes symptomatic infections, and 1/4 patients seeking treatment have symptoms of brain inflammation, which can lead to permanent neurologic sequelae and a 1/4 death rate .\n\n【1】JEV is a flavivirus transmitted by _Culex_ mosquitoes to birds and pigs; humans are dead-end incidental hosts. On the basis of nucleotide sequencing of capsid/premembrane protein (C/PrM) and envelope (E) genes, 5 virus genotypes have been identified, including genotypes I to III (GI, GII, GIII). These have been found distributed all over southern Asia; a GIV strain was isolated from eastern Indonesia, and an isolate originating in Malaysia may represent a fifth genotype .\n\n【2】Three vaccines, derived from JEV GIII strains, are currently in use. Since the 1960s JEV immunization campaigns have dramatically reduced the effects of the disease in southern and Southeast Asia . In Thailand, JEV immunization began as a part of childhood vaccination program in the northern provinces in 1990; this program rapidly expanded to 36 provinces that had reported a persistent incidence of encephalitis .\n\n【3】### The Study\n\n【4】To study the JEV genotype distribution in Thailand and to eventually detect changes in Japanese encephalitis epidemiologic patterns, we conducted a 3-year survey  of JEV incidence in 7 provinces representative of the 4 regions of Thailand (north, Chiang Mai Province; northeast, Khon Khen Province; central plain, Nakhon Pathom, Ratchaburi, and Samut Songkram Provinces; south, Phuket and Chumphon Provinces). Pig farms and rice fields within a 2-km radius around houses of confirmed human cases of Japanese encephalitis were targeted for sample collection. Ten healthy sentinel piglets (weeks of age) were surveyed in each province, and blood samples were collected weekly for 14 weeks. Adult mosquitoes were collected on a monthly basis according to the targeted pig farm and availability of breeding sites for vectors  by using both the CDC gravid trap (Model 1712) and the CDC light trap (P. Reiter, Centers for Disease Control and Prevention, Fort Collins, CO, USA).\n\n【5】Fifty microliters each from pig serum specimens and from filtered suspension of crushed mosquitoes were used for virus isolation on C6/36 cells. We tested JEV propagation by immunofluorescent assay. RNA extraction was done from a supernatant of JEV-positive cell culture, after first passage, according to manufacturer’s protocol as well as RNA reverse transcription–PCR (RT-PCR) (GIBCO-BRL, Gaithersburg, MD, USA). RT-PCR was performed on 4 μL of cDNA template by using 2.5 units of AmpliTaq Gold DNA Polymerase (PerkinElmer, Foster City, CA, USA). Overlapping JEV E gene fragments were amplified with 2 sets of primers: Ea forward primer (′-ATA GTA GCT ATG TGT GCA AAC AAG G 5-3′), Ea reverse primer (′-GAA TTC RGT YGT GCC YTT CAG AGC-3′); and Eb forward primer (′-AGC TCA GTR AAG TTR ACA TCA GG-3′), Eb reverse primer (′-GAA TTC AAT GGC ACA KCC WGT GTC-3′), respectively . The 1,216 nucleotides generated partial sequences of the JEV E gene that were compiled by using Sequence-Alignment Editor software version 2.0a11 (A. Rambaut, Department of Zoology, University of Oxford, Oxford, UK); pairwise genetic distances were calculated with MEGA software version 2.0  .\n\n【6】Twelve JEV strains were isolated, 3 from mosquitoes in 2003 and 10 from pigs in 2004 and 2005. The new JEV sequences were analyzed with a group of 22 previously published JEV strain sequences, including 6 from Thailand and 16 from other Asian countries. A phylogenetic tree was generated, and all 12 JEV new isolates fit into the same GI cluster, as did 3 other Thai strains previously isolated in 1982, 1984, and 1992 . Eleven of the newly identified isolates formed a subcluster  with 2 other strains previously isolated from Chiang Mai and Khon Ken in 1984 and 1982, respectively (B2239NThailand, P19Br NThailand); the remaining new isolate (JE KK 1116NEThailand2005) was associated with another subcluster , including strains isolated in 1992 from Chiang Mai (ThCMAr4492) and 3 others isolated from Vietnam, Japan, and Korea (O2VN22; JaNAr0102; and K94P05). Both subclusters were supported by 1,000 bootstrap replications and were consistent with the taxa distance (data not shown) showing introductions of GI in 1982 (within the GIa subcluster followed by a recent dispersion all over the country), and in 1992 for the GIb subcluster followed by local transmission.\n\n【7】GI strains appeared to cluster phylogenetically but not geographically, which suggests virus strains were transported over noncontiguous domains at variable geographic distances. Major environmental changes have occurred since the early 1950s with the increase in local and international transportation systems. Some researchers  consider the increase of the virus incidence in the human population to be associated with increased commercial activity. However, because of the low level of viremia in humans, traditionally considered dead-end hosts for JEV, it is more likely that the virus was spread within the country and to neighboring countries by migratory birds, infected domestic pigs, or infected mosquitoes (or their eggs) .\n\n【8】Although GIII strains were historically reported to circulate mostly in northern Thailand in the early 1980s, GI and GIII were found co-circulating from the north to the south; thereafter, only the GI strain was isolated in Thailand . The same genotype shift of GIII to GI, dating back to the early 1990s, was reported by several other Asian countries, including Japan and Korea in 1991 and Vietnam in 2001 ; a steady emergence and dispersion of GI was also noticed in China in 1979, in Taiwan in the 1980s , and in Australia in 2000 . Altogether, such unique endemic expansion of GI occurred over a 25-year period in several countries of Southeast Asia, replacing the GIII genotype, which was present all over the region since the beginning of the virus genotype identification (prospectively and retrospectively).\n\n【9】### Conclusions\n\n【10】In Thailand, the epidemiologic pattern of Japanese encephalitis first showed a visible decline in incidence with the development of immunization programs, but this decline also corresponded to the late 1980s when the practice of raising pigs in the backyard evolved into industrialized pig farming and the high rate of piglet seroconversion showed an intense virus circulation. The dramatic increase of industrial pig farming and trading must have played a major role in the dispersion of JEV genotypes within past decades in Asia. Concurrently with pig farming, the culicid main vectors have changed  and such factors as their ecology, trophic preferences, host competence, and virus fitness could play a role in an evolving rural environment. Moreover, further studies are needed to clarify the expansion of JEV GI strains, including the efficiency of a human and pig GIII-derived vaccine and the role of potential cross-immunity between another circulating flavivirus .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0a908ca9-fdb8-41d8-937c-c2d7d9244105", "title": "Elevated Pertussis Reporting in Response to 2011–2012 Outbreak, New York City, New York, USA", "text": "【0】Elevated Pertussis Reporting in Response to 2011–2012 Outbreak, New York City, New York, USA\n**To the Editor:** Pertussis is a highly communicable, acute bacterial respiratory infection caused by _Bordetella pertussis_ . In the United States, the incidence of pertussis declined dramatically after pertussis-containing vaccine was introduced in the 1940s . However, a resurgence of disease results in widespread outbreaks of pertussis nationally .\n\n【1】Beginning in August 2011, an outbreak of pertussis occurred in New York City (NYC), New York, USA. Reported pertussis incidence by month peaked in December 2011 (.03 cases/100,000 persons) and remained above the baseline average monthly incidence of 0.11 cases/100,000 persons until February 2013. We hypothesized that provider awareness and altered practices after the start of the outbreak contributed to the sustained elevation in reported pertussis incidence.\n\n【2】To test this hypothesis, we surveyed NYC providers to assess their awareness of the outbreak, their consideration of pertussis in symptomatic patients, and the type and frequency of diagnostic testing ordered. The survey (available on request) was designed in FeedbackServer 5  and consisted of 20 questions that required ≈5 minutes to complete by using a Web link. We distributed the survey in January 2013 to providers through 3 health department email lists: the NYC Health Alert Network, the Citywide Immunization Registry, and the Primary Care Information Project. The lists included ≈30,000 email addresses that were not mutually exclusive and that included nonmedical providers.\n\n【3】Through March 7, 2013, we received 1,316 responses; 887 (%) were excluded from analyses for \\> 1 reason: respondent did not complete all survey questions (%); respondent did not practice in a hospital or outpatient facility (%); respondent indicated that his or her primary facility was located outside NYC (%); or response was a duplicate (<1%). Of the 429 (%) responses included in our analyses, 69% of respondents served adults and 54% served children (% served both adults and children); 38% practiced in a hospital, and 81% practiced in an outpatient setting (% practiced in both hospital and outpatient settings).\n\n【4】Respondents were asked if and how they were aware of the pertussis outbreak; 84% reported previous awareness of the outbreak. The top reported sources contributing to respondents’ outbreak awareness included health advisory alerts (local \\[80%\\], state \\[36%\\], and national \\[40%\\]); media reports (%); and discussion with colleagues (%).\n\n【5】In addition, respondents were asked how likely they were to consider pertussis infection in patients with prolonged cough before 2012 and currently. Reported consideration of pertussis before 2012 varied: 35% of respondents were likely or very likely to consider pertussis, 33% were somewhat likely to consider pertussis, 30% were unlikely to consider pertussis, and 3% did not know (unknown). However, 73% of respondents said that they were more likely to consider pertussis at the time of the survey than before 2012. The top reported sources contributing to increased consideration of pertussis mirrored those contributing to outbreak awareness.\n\n【6】Respondents were last asked to assess the type and frequency of diagnostic testing they used before and since 2012 . Most (%) respondents indicated that they did not perform diagnostic testing for pertussis before 2012. Among the 34% who tested for pertussis before 2012, the main diagnostic methods used were bacterial culture (%) and PCR (%). However, 12% of respondents indicated that they had changed the type of diagnostic test they used beginning in 2012; among these respondents, 33% were more likely to use pertussis culture and 63% were more likely to use PCR or to use culture and PCR. Of total respondents, 22% indicated that they ordered diagnostic tests more frequently since the beginning of 2012.\n\n【7】Our investigation has limitations. We could not determine a survey response rate because of extensive overlap of the email lists used, and we lacked access to the lists; the response rate is assumed to be very low. Respondents included in the analysis may not have been representative of the broader NYC provider community. In addition, respondents may not have uniformly interpreted the survey because of the subjective nature of some survey questions, and recall bias may have affected responses. Also, respondent awareness of the outbreak is likely overestimated because the email lists used for survey distribution were used during the outbreak to distribute health alerts. Despite these limitations, our investigation shows the value of Web-based surveys distributed by email to gather information rapidly from a large provider community in a cost-effective and practical manner.\n\n【8】This investigation indicates the importance of provider knowledge and practices for public health surveillance data. High awareness of an outbreak, increased clinical suspicion of pertussis, and increased frequency of diagnostic testing likely contributed to a sustained elevation in pertussis incidence. Advisory alerts and media reports were successful mechanisms for disseminating information to providers during the outbreak and likely altered provider behaviors that contributed to the increase in reported pertussis incidence. Previous reports have documented increased submission of disease notifications after media coverage of health concerns . Responses to our survey also highlight how pertussis incidence may be routinely underestimated because providers do not suspect the disease or test for it consistently.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bc84ea30-bc0b-4586-9b92-91ee31608148", "title": "Legionella pneumophila Subspecies fraseri Infection after Allogeneic Hematopoietic Stem Cell Transplant, China", "text": "【0】Legionella pneumophila Subspecies fraseri Infection after Allogeneic Hematopoietic Stem Cell Transplant, China\n_Legionella pneumophila_ is an opportunistic atypical pathogen of community-acquired or hospital-acquired pneumonia . Underestimates of its prevalence are likely because the _Legionella_ urinary antigen testing and buffered-charcoal yeast extract (BCYE) culture routinely used for diagnosis are sometimes available only in a few tertiary hospitals that specialize in respiratory diseases. Therefore, early diagnosis and prompt therapy for _L. pneumophila_ infection are crucial. We report a case of _L. pneumophila_ subspecies _fraseri_ bloodstream infection in a patient in China after allogeneic hematopoietic stem cell transplant (aHSCT). We confirmed this diagnosis by using nanopore sequencing of positive blood cultures and by recovering _L. pneumofila_ using BCYE medium. Chest radiography and computed tomography (CT) suggested acute _L. pneumophila_ pneumonia. The study was approved by the Institutional Review Board of the Peking University People’s Hospital .\n\n【1】A 55-year-old man with acute T/myeloid mixed-cell leukemia was hospitalized because of nasal bleeding on day 62 after aHSCT. After treatment for thrombocytopenia, he experienced mild diarrhea on day 101. Three days later (day 104), his temperature was 38.8°C , but he had no accompanying symptoms. One aerobic blood culture vial, 1 anaerobic blood culture vial, and 1 Myco/F blood culture vial  per puncture point (puncture points) were collected for culture. Chest radiography and CT displayed multiple solid nodules in both lungs; the largest was in the right lower lobe, which suggested infection . The patient then experienced a 3-day continuous high fever with a maximum temperature of 39.5°C, a small amount of white sputum, and a blood oxygen saturation of 90% on day 107; the same number of blood culture vials were collected. CT showed a large, solid, fuzzy shadow with a unclear boundary, uneven internal density, and low-density plaques in the lower lobe of the right lung . The patient was administered linezolid (.6 g every 12 h) and imipenem (.5 g every 8 h) to control the infection. The patient’s temperature decreased to a normal level, but the C-reactive protein values did not decrease to reference range .\n\n【2】After ≈10 days (h) of incubation, we observed that the blood cultures in the Myco/F vial collected on day 104 after aHSCT, which are commonly used to culture _Mycobacterium_ spp. and fungus, contained the only growing gram-negative bacilli. We then transferred the positive blood cultures to blood nutrition plates for further culture in 5% carbon dioxide, anaerobic, and microaerobic environments, but this culture failed. Simultaneously, we directly used a serum separator-gel tube and matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry  to identify positive blood cultures but were unsuccessful. Subsequently, we extracted DNA from the positive blood cultures for nanopore sequencing ; _L. pneumophila_ subsp. _fraseri_ was detected after 1 hour . The coverage length and depth were 37.67% and 2.04 with 1,337 raw reads. After timely communication of the sequencing results to the clinical department, the patient was administered azithromycin (.5 g 1×/d for 1 d) , followed by moxifloxacin (.4 g 1×/d for 9 d) to ease symptoms of discomfort caused by azithromycin. The infection was finally controlled, and diarrhea symptoms improved 3 days after appropriate moxifloxacin therapy was initiated.\n\n【3】Afterward, we analyzed serum collected on May 24 (day 114) and May 27 (day 117) and detected _L. pneumophila_ IgM by using an ELISA kit . _Legionella_ urinary antigen testing was not performed. Meanwhile, 100 μL of the positive blood cultures was inoculated on the in-house BCYE agar supplied with _Legionella_ BCYE growth supplement medium . Two days later, we successfully isolated _L. pneumophila_ and displayed wet blue-purple luster colonies ; _L. pneumophila_ was finally identified using MALDI-TOF mass spectrometry. During the course of infection, no more sputum specimens were available because the patient had no obvious cough or expectoration.\n\n【4】The fatality rate of Legionnaires’ disease is between 5% and 30% . Risk factors for _L. pneumophila_ infection include age >50 years, solid tumors or hematologic malignancies, solid organ transplant, and immunosuppression . Hence, shortening the turnaround time to identify microorganisms is crucial for timely diagnosis and appropriate therapy, which influence death rates. In this case, the diagnosis of _L. pneumophila_ pneumonia was not possible on the basis of the atypical radiologic evidence, high fever (>38°C), elevated C-reactive protein, and diarrhea, although such manifestations are the most common symptoms of _L. pneumophila_ infections . Furthermore, administration of corticosteroids and immunosuppressive drugs likely obscured the respiratory symptoms. Rapid nanopore sequencing with a short turnaround time has the potential to effectively expedite the detection of _L. pneumophila_ infection, guaranteeing the appropriate antibiotic therapy, especially for immunosuppressed patients with atypical symptoms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c209fe29-e720-4e41-b537-73ae647cd9d1", "title": "Bacillus anthracis Bioterrorism Incident, Kameido, Tokyo, 1993", "text": "【0】Bacillus anthracis Bioterrorism Incident, Kameido, Tokyo, 1993\n### Incident\n\n【1】On June 29, 1993, five residents of Kameido, Koto-ward, an eastern area of Tokyo, reported foul odors to local environmental health authorities. On investigation, officials found that the odors originated from the eight-story headquarters building of the religious group Aum Shinrikyo. The group was suspected of abducting several escaped members and anti–Aum Shinrikyo activists; however, lacking strong evidence of criminal activity, national security and law enforcement authorities had not restricted Aum Shinrikyo’s activities.\n\n【2】On June 30, the local environmental health authority registered 41 complaints that foul odors were causing appetite loss, nausea, and vomiting in some exposed persons. Because of the complaints, officials requested permission to inspect the building’s interior, but Aum Shinrikyo members refused. Officials checked the building’s surroundings, collected air samples, and began to survey activity at the building, but other than the nuisance posed by the odor, no readily apparent risk to human health could be found.\n\n【3】On the morning of July 1, neighbors reported loud noises and an intermittent mist emanating from one of two cooling towers on the building’s roof . As the day progressed, residents (mostly living south of the building) lodged 118 complaints about the foul odors with the environmental health office. Light rain fell early in the day (a total of 7 mm, 1 mm each hour from 1:00–7:00 a.m.). Wind (m/sec) blew from north-northeast to northeast in the morning and northeast to east-northeast in the afternoon. The minimum and maximum temperatures were 16.9°C at 3:00 a.m. and 4:00 a.m. and 19.9°C at 3:00 p.m. respectively. The day was rainy and cloudy, with no direct sunlight.\n\n【4】The same day, residents in the neighborhood reported a “gelatin-like, oily, gray-to-black” fluid from the mist from the cooling towers collecting on the side of the building. Environmental officials collected samples of this fluid and stored them in a refrigerator (°C) for later testing.\n\n【5】Intermittent misting continued until demands from local residents forced Shoko Asahara, founder of Aum Shinrikyo, to agree on the morning of July 2 to cease using the rooftop device and to clean and vacate the building. No equipment remained when officials inspected the building on July 16, although they noted black stains on the walls.\n\n【6】This incident was largely forgotten until, in the aftermath of the March 1995 sarin gas attack on the Tokyo subway system, police investigations uncovered evidence that Aum Shinrikyo was involved in bioterrorism. The true nature of the Kameido incident was not revealed to the public until Asahara was arraigned on May 23, 1996. Aum Shinrikyo members testified that the odors were caused by their efforts to aerosolize a liquid suspension of _Bacillus anthracis_ in an attempt to cause an inhalational anthrax epidemic. They believed this epidemic would trigger a world war and lead to Asahara’s ruling the world.\n\n【7】At the time of the incident, the illnesses reportedly associated with the release were not well studied. In particular, no one sought evidence of inhalational anthrax or other syndromes caused by the anthrax bacillus, since the true nature of the mist was not recognized. Reports of short-term loss of appetite, nausea, and vomiting (symptoms not typical of _B. anthracis_ infection) among some residents were the only contemporary evidence of human illness associated with the incident. Vague reports of illness in birds and pets were also noted in local media  , but the exact nature of these illnesses remained unclear.\n\n【8】### Laboratory Findings\n\n【9】In November 1999, after long negotiation, local environmental authorities agreed that the one remaining fluid sample, collected as part of the 1993 investigation, could be tested for microbiologic pathogens. The test tube, which contained 2.6 mL of a turbid, gray-to-black fluid, was transferred to Northern Arizona University in Flagstaff, Arizona, for testing to identify and characterize its microbial flora.\n\n【10】Provisional microscopy examination of the fluid stained by malachite green/safranin showed bacterial spores, a large amount of debris, and vegetative bacterial cells. Aliquots of the fluid were streaked on sheep blood agar plates and incubated aerobically at 37°C. After overnight growth, the plates were found to contain mixed bacterial flora; approximately 10% of the colonies were similar in appearance to _B. anthracis_ . Suspect colonies were nonhemolytic and had the “gray ground glass” appearance typical of _B. anthracis_ . Based on the number of colonies on the plates, the original liquid suspension had about 4 x 10 3  colony-forming units (CFU) of the _B. anthracis_ –like agent per milliliter.\n\n【11】A representative selection of 48 colonies of the _B. anthracis_ –like agent was purified by single colony streaking and then subjected to multiple-locus, variable-number tandem repeat analysis (MLVA). The MLVA polymerase chain reaction (PCR) primers are specific for eight amplicon sites unique to _B. anthracis_  , two of which are plasmids carrying genes for anthrax toxin (pX01) and capsule (pX02). Amplicon-size patterns are diagnostic for particular diversity groups and strains within _B. anthracis_  .\n\n【12】Analysis of the 48 suspect colonies confirmed them to be _B. anthracis_ , and all had the same genotype. Of the eight marker sites, one—the locus for the pX02 plasmid coding for the anthrax capsule—consistently failed to amplify  . All colonies contained the pX01 plasmid (coding for anthrax toxin) but lacked the pX02 plasmid . This genotype was identical to that of the Sterne 34F2 strain, used commercially in Japan to vaccinate animals against anthrax.\n\n【13】### Epidemiologic Findings\n\n【14】In Japan, culture-confirmed human anthrax is on the national notifiable disease list, and physicians are required to report all cases to the government. During the 1990s, only four human anthrax cases were reported  . One of these cases, in Tokyo in August 1994, was in a man in his eighties from Sumida-ward, adjacent to Koto-ward (the location of Kameido); however, this case had no apparent association with the July 1993 incident.\n\n【15】A retrospective case-detection survey was conducted to assess the possibility that some anthrax cases might have gone unrecognized or unreported. Using the official “foul odor” complaints as a guide, residences of the 118 complainants from July 1, 1993, were mapped to identify the area with the presumed highest risk for infection . The high-risk area included a 7-digit zip code area (.33 km 2  ) in Kameido, containing approximately 3,400 households and 7,000 residents. In 1999, physicians at 39 medical facilities (internal medicine, 7 dermatology, and 17 other specialties) serving the high-risk area were surveyed by telephone. None of these physicians reported having seen cases of anthrax, unexplained serious respiratory illnesses, or hemorrhagic meningitis, which is often a complication of systemic anthrax  in residents of the high-risk area.\n\n【16】### Discussion\n\n【17】The Kameido incident is the first documented instance of bioterrorism with an aerosol containing _B. anthracis_ . Aum Shinrikyo members testified in 1995 that they were working with _B. anthracis_ , but 6 years passed before the strain was isolated and characterized  .\n\n【18】Why the Kameido incident failed to produce any documented cases of anthrax has not been fully explained, but the basis may be multifactorial. A virulent strain of _B. anthracis_ , a sufficient concentration to cause disease, effective aerosolization, and favorable weather conditions would all have been necessary to produce the anthrax epidemic Aum Shinrikyo members said they wanted.\n\n【19】Molecular subtype analysis results demonstrate that the strain used in Kameido was a vaccine strain of _B. anthracis_ without the ability to produce a protective capsule  . This strain is generally regarded as nonpathogenic for immunocompetent people and is widely used in livestock without adverse consequences. Even if the strain had been virulent, however, the concentration of spores in the liquid suspension (/mL) was significantly less than the 10 9  to 10 10  organisms/mL considered to be optimal in a liquid-based biologic weapon.\n\n【20】The viscosity of the suspension was also greater than desirable. Successfully weaponizing anthrax spores requires creating a fine-particle cloud with a sufficiently high concentration of _B. anthracis_ . The human respiratory infectious dose 50 (dose that will produce an infection in 50% of exposed persons) is unknown but has been estimated to be 8,000 to 10,000 spore-bearing particles <5 μm in diameter  . Kameido residents described a gelatinous substance, suggesting the suspension would be poorly dispersed and droplets would be too large to form particles <5 μm in diameter. Additionally, the effectiveness of the spray system (code named “Water Mach” by the Aum Shinrikyo) was questionable; it apparently broke down repeatedly, and hydrostatic back pressure caused the suspension to leak from tubing used to transport it up eight stories. The spray head may have clogged with the high-viscosity fluid, contributing to back pressure.\n\n【21】Climate could have been another mitigating factor. While _B. anthracis_ spores resist many environmental influences, they are killed by sunlight  , with an estimated survival time in direct sunlight in July of <2.5 h. In Kameido, survival time may have been longer since the weather was overcast on July 1 (the day the mist was reported), but spore inactivation by solar radiation would still have reduced the already-low potential for infection.\n\n【22】Because of the associated foul odor, residents quickly detected a problem, but local officials did not suspect Aum Shinrikyo of developing biologic weapons, and they conducted no microbiologic examination at the time. The actual cause of the foul odor remains undetermined, but it may have been caused by heating the medium used to grow _B. anthracis_ or failing to wash the medium from the suspension before dispersal. Geographic distribution of complaints about the odor corresponded with expected dispersal patterns of the aerosol under prevailing weather conditions. Infection risk could theoretically extend beyond the area of the foul odor complaints; however, focusing the telephone survey on this “high-risk” area provided the greatest chance of finding related cases of anthrax. Routine disease reporting by Tokyo-area medical association members did not provide evidence of potentially related cases outside the high-risk area.\n\n【23】### Conclusions\n\n【24】The Kameido investigation first showed the value of a high-resolution subtyping system for _B. anthracis_ in forensic investigations. Its value was confirmed during investigations of the “anthrax letters” mailed to several persons in the United States in 2001  .\n\n【25】The details of Aum Shinrikyo activities led to a wider appreciation that subnational organizations may use biologic agents as weapons. Awareness is especially important in being prepared for a bioterrorist attack, since recognizing its nature early can substantially reduce associated sickness and death . Early recognition, however, requires training health professionals to recognize these diseases, having laboratories available to rapidly confirm clinical suspicions, and developing an active national surveillance program. Countries must also be able to rapidly deploy trained medical personnel, medical materials, and epidemiologists to affected communities. Most countries will need coordination among government agencies and private facilities with expertise relevant to the agents involved. To be effective, these measures require ongoing planning, preparation, and practice.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "72dd6c30-14f4-4a54-8c59-63efcf918286", "title": "Human Infection with MERS Coronavirus after Exposure to Infected Camels, Saudi Arabia, 2013", "text": "【0】Human Infection with MERS Coronavirus after Exposure to Infected Camels, Saudi Arabia, 2013\nMiddle East respiratory syndrome coronavirus (MERS-CoV) was identified in 2012 in a cell culture taken from a patient who died of pneumonia in Saudi Arabia . Since 2012, at least 187 laboratory-confirmed human cases of MERS-CoV infection, most resulting in respiratory tract illness, have been reported to the World Health Organization; 97 of these cases were fatal. Known cases have been directly or indirectly linked to countries in the Arabian Peninsula . Dromedary camels across and beyond the region show high rates of antibodies against MERS-CoV , and viral RNA has been detected in camels in different countries . In 1 instance, a camel and 2 humans caring for the camel were found to be infected with viruses that were highly similar but distinct within 4,395 nt of the camel-derived virus sequence, including several phylogenetically informative nucleotide changes . To investigate possible camel–human virus transmission, we analyzed an infection with MERS-CoV in a man after he had contact with an infected camel.\n\n【1】### The Study\n\n【2】On November 3, 2013, the Ministry of Health of Saudi Arabia was notified of a suspected case of MERS-CoV infection in a 43-year-old male patient at King Abdulaziz University Hospital in Jeddah. The patient had cared for ill camels in his herd of 9 animals starting in early October, when the patient noted respiratory signs of illness with nasal discharge in several animals; he continued caring for the sick animals until October 27, the day of onset of his own illness. The patient cared for the animals for ≈3 hours per day 3 days per week, applying herbal remedies to the animals’ snouts and nostrils. He did not clean the stables or milk the animals, but he routinely consumed raw, unpasteurized camel milk from the herd.\n\n【3】Presence of MERS-CoV RNA in the patient was confirmed at Jeddah Regional Laboratory by using reverse transcription PCR (RT-PCR) targeting the _upE_ and _orfA_ gene fragments . Respiratory swab specimens yielded detectable signal after 28 RT-PCR cycles, indicative of an approximate viral load of 350,000 RNA copies per sample. A nearly complete viral genome was obtained , confirming the presence of a typical MERS-CoV whose closest relatives were in the Riyadh\\_3 clade, as defined in  .\n\n【4】To identify potential sources of infection, on November 9, the Ministry of Health investigated 5 close household contacts and the animal attendant on a farm owned by the patient. Nasopharyngeal swab samples were taken and tested at Jeddah Regional Laboratory by using RT-PCR. Deep nasal swab specimens were taken on the same day from 3 of the 9 camels at the farm. Testing of all samples by RT-PCR using the _upE_ assay  did not detect MERS-CoV RNA in any of the human patients, but 1 of the 3 camels (camel G) tested positive (cycle threshold \\[C t  \\] = 33). On November 13, nasal swab samples were obtained from all 9 animals. _upE_ RT-PCR results were positive for camel G (C t  \\= 38) and a second camel (camel B; C t  \\= 39).\n\n【5】Samples from November 13 and a small remaining amount of RNA extract from camel G from November 9 were sent to the Sanger Institute in Cambridge, UK, and confirmation of reactivity (C t  ≈ 38) was obtained for pooled samples with the _upE_ assay from camel G but not for camel B. The result for camel G was confirmed at the Institute of Virology in Bonn, Germany, for the same samples by using real-time RT-PCRs targeting the _upE_ and 1A diagnostic target regions.\n\n【6】A sequence of ≈15% of the camel-derived genome was determined from 8 RT-PCR fragments, 2 of them partially overlapping (nt total) . Phylogenetic analyses supported the conclusion that transmission occurred between camel and patient, but no direction was implied (e.g, camel to human vs. human to camel; online Technical Appendix Figure 1). The human- and camel-derived sequences shared a signature of single nucleotide polymorphisms that occurred in no other known MERS-CoV sequences . Single nucleotide exchanges occurred at nt positions 21945 and 29662; these exchanges might have arisen during virus transmission, as described . However, because of the low RNA concentration in the samples, reamplification of the material and investigation of possible PCR-based mutations could not be done.\n\n【7】A serum sample taken from camel G during the initial investigation on November 9 was tested by recombinant immunofluorescence assay (IFA) as described  and showed reactivity to MERS-CoV (titer 320). To investigate signs of recent MERS-CoV infection in the group of camels, we obtained blood samples at short intervals from all 9 animals during November 14–December 9, 2013. All samples were tested by ELISA against recombinant MERS-CoV spike antigen domain S1 fused to human Fc fragment, using a formulation as described . Serum samples from all 9 animals showed reactivity to the MERS-CoV antigen; serum samples from control animals showed no reactivity . ELISA signals were constant over time in most animals, but a small, yet visible, change of signal over the observation period was noted for camels B and G. To clarify the reasons for this putative signal increase, the first and last serum samples (obtained on November 14 and December 9) from all animals were re-tested by IFA. As summarized in the Table, camels B and G showed 4-fold increases of titer for the paired first and last serum samples. In serologic tests that rely on 2-fold endpoint titrations, a titer increase \\> 2 dilution steps is considered a significant sign of recent acute infection. For additional confirmation of rises of titer, sequential samples from camels B and G were compared with sequential samples from camels E and I by using IFA with endpoint titration. These data confirmed the increases of titers for camels B and G .\n\n【8】Because bovine CoV occurs in camels, we tested for antibodies against bovine CoV in camels B and G to exclude potential cross-reactions. Using IFA , we found no bovine CoV antibodies in camel G, but camel B showed rising bovine CoV antibody titers . To obtain further differentiation, we performed neutralization assays against MERS-CoV and bovine CoV . Titers in serum samples from November 14 and December 9, respectively, were 160 and 320 for camel B and 160 and 160 for camel G. None of the animals showed serum neutralization against bovine CoV.\n\n【9】### Conclusions\n\n【10】These data add to recent findings showing high similarity of MERS-CoVs carried by humans and camels , supporting the hypothesis that human MERS-CoV infection may be acquired directly from camels. In addition, both animals that showed signs of recent infection were juvenile, which provides further support to previous findings that mainly young animals are infected by MERS-CoV . Given the synchronized parturition pattern of dromedary camels, with birthing in the winter months, an increase of epizootic activity might be expected after some latency during the first half of each year.\n\n【11】Our data provide particular insight into the timing of infections and transmission. Antibody titers rose and viral RNA concentrations were already on the decline in the camels while the patient was hospitalized with acute symptoms. Assuming a time before appearance of antibodies of 10–21 days, at least some of the camels would have been actively infected during middle to late October, when some animals showed signs of respiratory illness and the patient acquired his infection. Nevertheless, we cannot rule out other infectious causes of the animals’ upper respiratory signs. Also, because of the retrospective nature of this investigation, we cannot rule out the possibility of a third source of MERS-CoV infection for camels and humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2bcda1a3-c3be-47ff-8cff-ca010182fdcc", "title": "A Qualitative Study of Perceptions, Strengths, and Opportunities in Cardiometabolic Risk Management During Pregnancy and Postpartum in a Georgia Safety-Net Hospital, 2021", "text": "【0】A Qualitative Study of Perceptions, Strengths, and Opportunities in Cardiometabolic Risk Management During Pregnancy and Postpartum in a Georgia Safety-Net Hospital, 2021\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Despite the strong link between cardiometabolic pregnancy complications and future heart disease, there are documented gaps in engaging those who experience such conditions in recommended postpartum follow-up and preventive care. The goal of our study was to understand how people in a Medicaid-insured population perceive and manage risks during and after pregnancy related to an ongoing cardiometabolic disorder.\n\n【3】**Methods**\n\n【4】We conducted in-depth qualitative interviews with postpartum participants who had a cardiometabolic conditions during pregnancy (chronic or gestational diabetes, chronic or gestational hypertension, or preeclampsia). We recruited postpartum participants from a single safety-net hospital system in Atlanta, Georgia, and conducted virtual interviews during January through May 2021. We conducted a content analysis guided by the Health Belief Model and present themes related to risk management.\n\n【5】**Results**\n\n【6】From the 28 interviews we conducted, we found that during pregnancy, advice and intervention by the clinical care team facilitated management behaviors for high-risk conditions. However, participants described limited understanding of how pregnancy complications might affect future outcomes, and few described engaging in postpartum management behaviors.\n\n【7】**Conclusion**\n\n【8】Improving continuity and content of care during postpartum may improve uptake of preventive behaviors among postpartum patients at risk of heart disease.\n\n【9】Introduction\n------------\n\n【10】Cardiometabolic heath conditions are critical determinants of perinatal risk. Pregnant women with chronic diabetes or hypertension are at increased risk of infant and maternal morbidity . Additionally, incident cardiometabolic dysfunction during pregnancy (eg, gestational hypertension, preeclampsia, or gestational diabetes) is associated with elevated perinatal risk . Pregnant women with incident or chronic cardiometabolic dysfunction require monitoring during pregnancy to prevent and mitigate potential adverse outcomes .\n\n【11】After delivery, women who experienced cardiometabolic dysfunction during pregnancy are at elevated risk of severe maternal morbidity, postpartum complications, and future development of cardiovascular disease, diabetes, and hypertension . To detect and prevent complications, the obstetric care team should screen postpartum patients for ongoing hypertension or glucose intolerance in the postpartum period, counsel them on heart disease risk and prevention, and refer them to primary care for ongoing surveillance and management . However, less than one-half of patients with cardiometabolic complications of pregnancy receive guideline-concordant postpartum blood pressure or glucose screening .\n\n【12】Engagement in beneficial postpartum health behaviors can mitigate risks related to cardiometabolic conditions. Returning to or attaining a healthy weight, lactation, and control of glucose and blood pressure are evidence-based strategies to reduce future heart disease risk and might also reduce risk in a future pregnancy . If patients are unaware of their heart disease risk or prevention strategies, however, they may be less likely to engage in optimal behaviors. Limited data suggest that people have limited knowledge about the link between pregnancy complications and future disease risk .\n\n【13】Limited data exist on patient understanding and management of cardiometabolic risks during pregnancy, particularly in Medicaid-insured low-income populations, in which maternal morbidity and mortality are highest . By using the Health Belief Model , we sought to understand how , our study’s goal was to understand how postpartum participants perceive and manage risks related to an ongoing cardiometabolic disorder during and after pregnancy.\n\n【14】Methods\n-------\n\n【15】### Study design\n\n【16】Our study consisted of in-depth interviews with low-income postpartum patients at a safety-net hospital in Georgia and was part of a larger study to develop, implement, and test a postpartum planning intervention for patients at high risk of severe maternal morbidity. We received approval for this study from the Emory University Institutional Review Board (STUDY00001427).\n\n【17】### Participants\n\n【18】Patients for this study were recruited from a single safety-net hospital system in Atlanta, Georgia, (Grady Hospital) and were eligible if they 1) were within 3 to 6 months postpartum after delivering a liveborn infant from October 2020 through January 2021; 2) received prenatal care in the Grady Health System; and 3) had a prenatal diagnosis of diabetes, chronic hypertension, a hypertensive disorder of pregnancy (HDP), inclusive of gestational hypertension or preeclampsia), or gestational diabetes. We identified potentially eligible patients through diagnostic codes in the electronic medical records and contacted them by telephone to invite them to participate. We conducted purposive sampling of postpartum participants who both had and had not attended their postpartum visit. Interviews were conducted during January through May 2021. Participants provided written informed consent before participating and were given a $50 gift card after interview completion.\n\n【19】### Data collection\n\n【20】We developed a semistructured interview guide to assess how patients understood and managed cardiometabolic risk conditions during pregnancy and postpartum . After developing a draft of the guide, we shared it with members of our community advisory board, which consisted of local maternal health leaders, and revised the language according to board feedback. We then piloted the guide with 3 initial interviews, making slight modifications to language, and developed probes after each pilot interview.\n\n【21】Interviews lasted an average of 78 minutes and were conducted by using Zoom. One or 2 trained study team members conducted each interview, and a team member took detailed notes. Interviews were recorded with the permission of the participant.\n\n【22】### Analysis\n\n【23】All interview recordings were professionally transcribed. We used MAXQDA (VERBI GmbH Berlin) for data management and transcript analysis . A directed content approach for coding data was guided by research questions . We developed a codebook by using a team-based approach in which all members read an initial 5 transcripts, wrote analytic notes with initial interpretations of the text, and developed candidate deductive and inductive codes. We applied the initial codebook to transcripts in teams of 2 and iteratively updated the codebook to produce a final codebook that was applied to the remaining data. Further identification of patterns across and within data were used to develop themes during a final stage of interpretation.\n\n【24】### Theoretical framework\n\n【25】We used the Health Belief Model to organize and interpret thematic results about participant management of pregnancy and chronic disease during pregnancy and postpartum and presented key themes by element of the Health Belief Model and timing (pregnancy or postpartum) . A study team member mapped and coded segments related to understanding and managing cardiometabolic risk conditions and general health to applicable elements of the Health Belief Model (threat, benefits, barriers, self-efficacy, and cues to action) and identified patterns across and within data to develop themes .\n\n【26】**  \nDiagram of the constructs of the Health Belief Model , as applied to the current study of high-risk cardiometabolic conditions during pregnancy and postpartum, adapted from . \n\n【27】Health Belief Model asserts that people make choices about their health risk behaviors depending on their perceived susceptibility to an adverse outcome, their perceived severity of the outcome, and their perceived benefits from and barriers to a given behavior . Perceived self-efficacy and external cues to action facilitate uptake of health-promoting behaviors. This framework helps identify possible opportunities for health care providers to improve patient support systems for managing cardiometabolic risk conditions.\n\n【28】Results\n-------\n\n【29】Of the 93 postpartum participants identified as potentially eligible through electronic medical records, 28 (%) completed an interview. By design, all study participants had 1 or more cardiometabolic risk conditions complicating pregnancy as recorded in the medical record . Fifteen (%) had hypertension. An additional 12 (%) had a gestational hypertension diagnosis. Many (%) of those with gestational or chronic hypertension developed preeclampsia. Diabetes was rare, only occurring in 2 participants. Six (%) participants had gestational diabetes. Most participants (%) had at least 1 previous pregnancy, 24 (%) identified as non-Hispanic Black, and 26 (%) were insured by Medicaid during pregnancy. Participants ranged in age from 18 years to 42 years, with a median age of 27 years.\n\n【30】### Thematic findings\n\n【31】#### Perceived susceptibility\n\n【32】Perceived susceptibility is a person’s belief of how probable or improbable a given adverse outcome is for them. We considered beliefs about susceptibility to be both the participant’s understanding of their pregnancy-related diagnosis and their perception of their risk for future adverse outcomes.\n\n【33】Almost all participants understood that they had a cardiometabolic risk condition during pregnancy ; however, understanding of their specific diagnosis varied. Although all participants with diabetes or gestational diabetes understood their diagnosis, 9 of 16 participants with HDP were unclear about their exact diagnosis. For example, one participant with HDP explained that her blood pressure was not of concern until a spike immediately before her pregnancy. Another one explained, “Since it wasn’t always persistent on high . next visit it would be a little bit lower. Then it’ll be high . About the last month is when it started staying consistent. So that’s when they was like, ‘Oh, I think we going to have to get him out ASAP.’ _”_\n\n【34】Most (of 16) participants with HDP or gestational diabetes in the postpartum period believed their condition was no longer an issue. Additionally, 5 of 15 participants with hypertension were unconcerned about their blood pressure, stating that it was high during pregnancy but not otherwise of concern . “They tried to give me some blood pressure meds, and I told them, ‘it’s normal, it will go away in due time.’ I know my body because that’s what happened with the last one,” one participant with hypertension said.\n\n【35】Patients who attended the postpartum visit (attendees) and those who did not (nonattenders) described similar concerns about complications, primarily about infant complications. However, 2 of 16 attendees described discussions with the postpartum visit provider about hypertension that helped them understand their own risk.\n\n【36】#### Perceived severity\n\n【37】Perceived severity is a person’s perception of potential danger from a given disease or an adverse outcome. We focused on participants’ perceptions of adverse consequences of their cardiometabolic disease diagnosis. Many (of 28) participants were concerned about risks to the developing fetus, such as a miscarriage or preterm birth, because of their cardiometabolic risk condition. A nonattender with gestational diabetes said, “Mainly, \\[gestational diabetes\\] can affect \\[the baby\\]. Sometimes, baby come out real, real small. Sometimes, people have miscarriage, \\[because of\\] it, and sometimes, your baby can come out real, real big.”\n\n【38】In contrast, 6 of 28 nonattender participants expressed concern over risks to their own health, primarily describing possible strokes either during labor or at any time. A nonattender with HDP said, “Because I was getting to the point where I could have had a stroke, a seizure, heart attack, because all that anger and my blood pressure . the doctor said just go ahead and induce \\[me\\]. Because if they leave the baby in there, it could either come to the baby or my life. So, I was like, wow. I started crying when she told me that, because I was like, I’m going to die.”\n\n【39】After pregnancy, 4 of 16 participants described concern about potential risk because of gestational diabetes or HDP. Eight of 15 participants with hypertension and diabetes expressed understanding of their continued risk, although this was not true for participants who believed their hypertension to be relevant only during pregnancy. Both postpartum visit attendees and nonattenders described similar levels of concern about high-risk conditions during pregnancy, primarily for the baby. Slightly more attendees (of 16) than nonattenders (of 12) described concerns for future risks.\n\n【40】#### Perceived benefits\n\n【41】Perceived benefits are perceived or actual positive consequences resulting from a given health behavior. During pregnancy, 6 of 28 participants described engaging in specific behaviors to prevent adverse infant outcomes or to optimize their own health during pregnancy by managing weight gain and glucose levels. Perceived healthy behaviors included glucose and blood pressure monitoring, insulin injections, and exercise. One participant with diabetes and HDP explained, “I mean, diet is a main thing and exercise. I walked like 30 minutes to 40 minutes every day. Yeah. It helps, because whenever I test on home, the sugar level was perfect, and on every visit, they saw the sugar level and all that, that I tested. So, it was good, and they told me to do the same thing, like diet, exercise.”\n\n【42】During the postpartum period, some participants described engaging in prevention behaviors such as following up with primary care, maintaining a healthy diet, and exercising to stay healthy and to take care of children. An attendee with hypertension explained, “I’m 26 now and I had my first child when I was 19, so when I was younger, I wasn’t thinking about stuff like that \\[primary care visits\\]. But now that I had the hypertension when I was pregnant and I’m getting older, I feel like I need to focus on that because I want to be here to see my kids grow up.”\n\n【43】A few (of 28) participants exercised, dieted, or followed up with primary care specifically to lose weight. A nonattender explained, “That was another thing, my weight. Oh, my goodness. I was not happy and I’m still not happy, but I joined the gym.” One participant described engaging in healthy behaviors (exercise, diet, and smoking cessation) to prevent heart disease . Postpartum visit attendees and nonattenders described similar perceived benefits to prevention behaviors during and after pregnancy. However, more attendees (of 16) than nonattenders (of 12) described plans to engage in primary care following pregnancy.\n\n【44】#### Perceived barriers\n\n【45】Perceived barriers are perceived or negative consequences of a given behavior or costs associated with that behavior. During pregnancy, participants described medication side effects and costs as barriers to engaging in blood pressure monitoring or medication use. An attending participant with hypertension said, “They tried to give me some blood pressure meds, and I told them, ‘It’s normal,’ it will go away in due time. I’m not going to take it because it made me feel nauseous and it tires me even more than what I am.”\n\n【46】During postpartum, 6 of 28 participants identified childcare as a barrier to self-care, or participants prioritized their children’s needs over their own, which limited their ability to engage in self-care behaviors or to seek follow-up care. A nonattender with hypertension needed to remain at home to care for her preterm infant with a feeding tube, although her care team asked her to return to the hospital for readmission because of her dangerously high blood pressure, as measured at a home visit. “Just because of how high they said \\[my blood pressure\\] was that day, but I think he was still on the feeding tube at that time, so my other children, they don’t know how to change a feeding tube. It was just, I couldn’t go \\[to be readmitted\\] at that time.”\n\n【47】Additionally, 5 of 28 postpartum participants explained how financial and insurance barriers prevented them from attending needed follow-up for primary or specialty care after their pregnancy. Participants were limited in their choice of providers, because some providers did not accept Medicaid or uninsured patients. Primary care practices that accepted Medicaid had limited availability, and participants were often unable to find an available appointment. Other participants described similar barriers to engaging in prevention behaviors during and after pregnancy. Two nonattenders at postpartum visits described mistrust of doctors as a barrier to seeking primary care.\n\n【48】#### Cues to action\n\n【49】Cues to action are reminders or triggers to engage in health-promoting behaviors when a person is ready. During pregnancy, cues to action stemmed primarily from counseling, reminders, or check-ins from the clinical care team. For example, participants with diabetes or gestational diabetes reported receiving support through one-on-one counseling and check-ins with a trained nurse and materials and guidance on testing blood glucose. This counseling helped remind participants to monitor glucose levels. In contrast, participants with hypertension or HDP largely created their own systems for reminding themselves to take medication (eg, pill organizers, alarms). One attending participant with gestational diabetes and hypertension explained, “Mainly what I did for myself was to try to set an alarm so that we know it’s time to check your blood pressure, and I bought a pill organizer so I can keep my medicine by my bed and wake up and have the medicines right there.”\n\n【50】After pregnancy, participants described few cues to action from the clinical care team to support heart health behaviors. Despite explicit probes, only 8 of 16 reported discussing relevant healthy behaviors at the postpartum visit. For example, in one interview, the interviewer asked if the attending participant with HDP was asked about blood pressure at all at her postpartum visit and if \\[the clinician\\] was aware that the participant had high blood pressure during pregnancy. The participant indicated a negative response and stated, “They don’t ask that there.” The participant went on to remark that when discussions did occur, participants valued them; however, in some cases, reminders from the provider were insufficient, given the barriers. For example, 12 of 16 attending participants said that their postpartum provider told them to follow up with primary care, but 5 had not yet done so because they could not find a childcare provider or did not have time in addition to the demands of childcare.\n\n【51】Participants also received advice and support from family as cues to action for postpartum behaviors, describing how family members counseled them on their diet, encouraged them to take walks or time for themselves (and watched children while they did), or reminded them to check their blood pressure. One attending participant with HDP and gestational diabetes explained, “Because she \\[participant’s mother\\] has high blood pressure and has been taking her own blood pressure for years, she knew . what was normal for me and what was not. When I came home and I had a headache, she encouraged me to take it, and at that time it was high.”\n\n【52】Finally, 2 participants reported that outreach from Medicaid encouraged them to engage in healthy behaviors (exercise and scheduling primary care visits). For example, one participant explained how she started exercising after receiving a brochure from her Medicaid provider stating that her Medicaid covered exercise and childcare at the YMCA .\n\n【53】Postpartum visit attendees and nonattenders received similar cues to action during pregnancy but described differences postpartum. First, attendees were more likely than nonattenders to report having a primary care provider outside of pregnancy (of 16 vs 0 of 12). Second, all attendees with hypertension described counseling on blood pressure management at the postpartum visit, which nonattenders did not receive.\n\n【54】#### Self-efficacy\n\n【55】Self-efficacy is a person’s belief that they are capable of engaging in an action that will result in positive change. During pregnancy, 5 of 15 participants with a chronic condition expressed comfort and confidence in understanding and managing their condition. Similarly, multiparous participants who had an HDP diagnosis in a previous pregnancy described confidence in managing HDP in each of their recent pregnancies. Home glucose and blood pressure monitoring, paired with training on how to monitor them accurately and when to report results to a physician, gave participants a sense of control over their condition during pregnancy.\n\n【56】After delivery, 7 of 28 participants described how monitoring and understanding their blood sugar or blood pressure levels postpartum gave them confidence in managing their chronic conditions. An attending participant with HDP and gestational diabetes remarked, “Since I’m not consistent, and I’m not on high blood pressure medication, I just take it and watch myself and try to record, so when I go back to the doctor, I’ll let him see it.”\n\n【57】Participants who were already engaged in care for a chronic condition before pregnancy expressed confidence about managing their condition postpartum. In contrast, participants who had a pregnancy-induced condition or who did not understand their diagnosis did not express confidence in their ability to manage or follow up on their condition. Postpartum visit attendees and nonattenders did not vary in perceived self-efficacy for managing high-risk conditions.\n\n【58】Discussion\n----------\n\n【59】We presented the narratives of 28 low-income postpartum women of color from a high-risk, safety-net hospital about their perceptions and understanding of their cardiometabolic risks during and after pregnancy. Applying the Health Behavior Model, we described successful pathways through which participants engaged in prevention and management behaviors. We also noted multiple opportunities to address barriers, improve cues to action, and facilitate optimal postpartum health for patients with cardiometabolic conditions. Beyond gaps in knowledge and clinical support postpartum, our findings demonstrate how structural barriers (childcare, insurance, transportation) must be addressed to improve postpartum and long-term health for people giving birth who have high-risk cardiometabolic complications.\n\n【60】Given the findings of our study and prior research , clinicians should expand and improve counseling related to cardiometabolic risk conditions during pregnancy, particularly around the need for future and ongoing surveillance and management. In our study, and prior studies, we found a disconnect between patient diagnoses and their own understanding of their pregnancy and future risk related to cardiometabolic disease . Consistent with prior research, participants in our study prioritized the needs of their developing fetuses and babies over their own health . Effective postpartum counseling may include value-based discussions helping mothers see heart disease prevention as part of caring for their family . Counseling that connects weight management, exercise, and diet to heart disease prevention may help motivate some people. In our study, only one participant explicitly connected her weight management goals to managing her hypertension. Our findings, however, showed that even when motivated, some participants were unable to overcome structural barriers to engage in healthy postpartum behaviors. Thus, innovative strategies are necessary to improve postpartum follow-up for patients with cardiometabolic complications of pregnancy in low-income populations. Strategies might include telehealth, home visiting, or specialty postpartum transition clinics following high-risk pregnancies . Clinics could also implement warm handoffs, in which a care coordinator assists the patient in identifying and contacting a provider to make an appointment for needed care .\n\n【61】The results of this study should be interpreted in light of its limitations. First, because of the diversity of diagnoses in our sample (by design), our guide did not probe all potential management behaviors for each diagnosis and asked open-ended questions (eg, we asked, what have you been doing to take care of yourself, rather than, do you take blood pressure medication). Thus, we are only able to base our analysis on information from the targeted questions asked during the interview, paired with information abstracted from the medical record. Second, the sample of 28 participants represents only 30% of potentially eligible patients. The low participation rate might reflect that the postpartum period is a busy time for most, or it might have been related to lingering effects of the COVID-19 pandemic. Finally, we did not systematically ask about postpartum care experiences in previous pregnancies, which might guide a person’s approach to condition management.\n\n【62】Improving understanding of the link between cardiometabolic complications of pregnancy and future heart disease risk can empower pregnant and postpartum women to better manage their own health. Study participants were interested in weight loss and disease management, but they received little guidance from their clinical care team, even when they attended the postpartum visit. Health systems must implement innovative strategies to support postpartum women, particularly those at high risk of severe maternal morbidity and future heart disease. In this low-income, Medicaid-insured population, few participants were engaged in care before pregnancy, and the postpartum period represents a unique opportunity to engage them in prevention and disease management . However, because of the many structural barriers noted, education or written referrals alone are insufficient. Successful strategies should both build on existing values, such as the desire to stay healthy for their family and address the demands of childcare and finances postpartum.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "adbf2a53-db39-4eb4-a8ae-39fb938229dd", "title": "Molecular Evidence of Interhuman Transmission of Pneumocystis Pneumonia among Renal Transplant Recipients Hospitalized with HIV-Infected Patients", "text": "【0】Molecular Evidence of Interhuman Transmission of Pneumocystis Pneumonia among Renal Transplant Recipients Hospitalized with HIV-Infected Patients\n_Pneumocystis jirovecii_ pneumonia (PCP) is a severe opportunistic infection in immunocompromised patients . It remains a major problem in some HIV-infected persons who are not receiving or not responding to highly active antiretroviral triple therapy and among those who are unaware of their HIV status. PCP is also of clinical importance in immunosuppressed patients, e.g. transplant recipients and those receiving chemotherapy for malignant diseases, who are not infected with HIV. Host specificity suggests that the reservoir of _P. jirovecii_ is limited to humans. Primary infection in infants , as well as asymptomatic carriage by immunosuppressed persons , may serve as infectious reservoirs or sources in the community. Reactivation of a past infection was a postulate mechanism of infection in immunosuppressed patients, but de novo infection in recurrent episodes of the disease  has suggested that infection or reinfection from exogenous sources may occur. Horizontal airborne transmission has been demonstrated in several animal models .\n\n【1】Transmission of _P. jirovecii_ from patients with active PCP to susceptible persons has been suspected in numerous descriptions of nosocomial clusters of PCP cases . Although a common environmental source of the infection was difficult to exclude, many patients in the clusters had contact with each other, which suggests that they may have transmitted _P. jirovecii_ to one another. The early reports of PCP epidemics among malnourished children in orphanages and hospitals in the 1950s were also compatible with interhuman transmission of _P. jirovecii_ . The strongest suspicion of transmission was provided by a case-control study performed for a cluster of five PCP cases in transplant recipients . This analysis showed that the cases had more encounters than matched controls at the outpatient facility with AIDS patients who had or subsequently developed PCP. However, in these studies, transmission of _P. jirovecii_ could not be assessed at the molecular level because no molecular typing method for _P. jirovecii_ existed. Such methods were developed in the 1990s, and new clusters were analyzed. However, the few published anecdotal analyses often reported different genotypes within the clusters . Thus, interhuman transmission of _P. jirovecii_ from PCP cases is still an open issue.\n\n【2】The latest guidelines developed by the U.S. Public Health Service and the Infectious Diseases Society of America for preventing opportunistic infections in persons infected with HIV state that although some authorities recommend that persons who are at risk for _P. jirovecii_ pneumonia not share a hospital room with a patient who has PCP, data are insufficient to support this recommendation as standard practice .\n\n【3】In our molecular epidemiologic study, we investigated the possibility of _P. jirovecii_ transmission between persons during a 3-year period in a hospital building that simultaneously hosted AIDS patients (with and without PCP) and renal transplant recipients (RTR) (often during rejection episodes), and in which a cluster of PCP was observed.\n\n【4】### Material and Methods\n\n【5】##### Hospital Setting\n\n【6】Edouard-Herriot hospital is a 1,200-bed healthcare facility in Lyon, France, and is made up of several buildings. One of these buildings (building A, 80 beds) accommodates one hospital ward, an intensive care unit, an outpatient clinic, and a radiodiagnostic facility, which are mostly devoted to renal transplant medicine and clinical immunology, including HIV medicine. Another building (building B) hosts only patients with hematologic malignancies and is located 100 m away from building A.\n\n【7】##### Data Collection\n\n【8】Our investigation included the 39 patients with PCP who were hospitalized in building A and whose bronchoalveolar lavage (BAL) specimen was available for molecular typing. These patients were chosen because interhuman transmission of _P. jirovecii_ was suspected in this building. The database of the Department of Medical Information of the University Hospitals of Lyon was used to identify the demographic and clinical characteristics of the patients. Relevant data (prophylaxis regimen, hospitalization periods, dates of outpatient visits, immunosuppressive regimen) were also extracted from the medical charts of patients by using a questionnaire and log books of the outpatient clinic and radiodiagnostic facility.\n\n【9】##### Laboratory Diagnosis and Storage of Specimens\n\n【10】PCP was diagnosed by using methenamine-silver nitrate  and Giemsa stains on BAL specimens in the parasitology laboratory of Claude-Bernard University, which has processed all specimens using the same techniques for many years. The number of BAL specimens submitted for patients seen at Edouard-Herriot Hospital has been stable over the years (: 235, 241, 277, 254, 290, 215, 215, respectively). BAL specimens of patients with proven PCP were stored at –20°C.\n\n【11】##### Molecular Typing\n\n【12】BAL specimens were typed as described previously  with the polymerase chain reaction (PCR)–single-strand conformation polymorphism (SSCP) method for typing _P. carinii_ , now named _P. jirovecii_ , in humans. The method consists of amplifying four variable regions of the _P. jirovecii_ genome, followed by the detecting the polymorphisms with SSCP. The variable regions analyzed are the internal transcribed spacer 1 of the nuclear rDNA operon, the intron of the nuclear 26S rRNA gene, the variable region of the mitochondrial 26S rRNA gene, and the region surrounding the intron 6 of the β-tubulin gene. The different SSCP patterns observed are caused by one to four base-pair polymorphisms . A _P. jirovecii_ type is defined by a combination of four alleles, which corresponds to the four genomic regions. If a specimen harbors two alleles of one or more of the four genomic regions, the patient was considered coinfected with two or more _P. jirovecii_ types . For a given patient, each type is defined as an “isolate.” Molecular typing was performed on specimens from patients in building A from 1994 to 1996, as well as on representative specimens collected during the same period in building B of the Edouard-Herriot hospital and in other university hospitals of Lyon. In addition, the dihydropteroate synthase (DHPS) genotype was determined by using PCR-SSCP as described . Four DHPS alleles have been described in _P. jirovecii_ . The mutated alleles result in an amino acid change in the active site of the enzyme at position 55 (allele M1) or 57 (M2), or both polymorphisms (M3).\n\n【13】##### Definitions\n\n【14】In the absence of knowledge of many biologic and epidemiologic characteristics of _P. jirovecii_ infection, the incubation period of the not yet symptomatic patients and the period of infectivity of patients with PCP were postulated on the basis of available human and experimental data. Described clusters of PCP  suggest that the incubation period of de novo infection is 3–12 weeks. Accordingly, we assumed that a new infection with _P. jirovecii_ (as opposed to reactivation) would occur 3–12 weeks before laboratory diagnosis of PCP. This finding is also in accordance with experiments in animals . Similarly, we considered that the risk of transmission from a _P. jirovecii_ \\-infected patient to a susceptible one was likely to be highest from early symptoms to the middle of treatment. We assumed that an infected patient could transmit _P. jirovecii_ from 3 weeks before to 2 weeks after the PCP diagnosis. We hypothesized that transmission was airborne and defined that a potentially infectious encounter occurred if a patient within his or her susceptible period and another patient within his or her infectious period visited the same location in building A on the same day. Transmission was considered possible if the patients encountered at least once and shared a common _P. jirovecii_ type.\n\n【15】##### PCP Prophylaxis, Isolation, and Immunosuppression\n\n【16】Four HIV-infected patients and four transplant recipients at risk for PCP were receiving sulfadoxine-pyrimethamine, but at a dosage lower than recommended for anti- _P. jirovecii_ prophylaxis (mg pyrimethamine plus 500 mg sulfadoxine in one tablet taken once a week or every 2 weeks versus two tablets per week ). In addition, four HIV-infected patients were receiving aerosolized pentamidine (mg every 2 weeks). The other 27 patients did not receive any anti- _Pneumocystis_ prophylaxis. A policy for isolating patients according to their underlying disease or to the occurrence of a PCP episode did not exist. Patients were allowed to move freely in the units and shared a TV room when permitted by their general condition. All RTRs, including those who experienced PCP, received usual inductive and maintenance treatments with prednisone, azathioprine, and cyclosporine. Treating rejection included high-dose corticosteroids and, if necessary, monoclonal antibodies.\n\n【17】##### Case-Case Comparison\n\n【18】Case-case comparison based on molecular typing of the pathogen  was performed. Groups of cases infected with different _P. jirovecii_ molecular types were compared to investigate differences in exposure histories.\n\n【19】### Results\n\n【20】From 1994 to 1996, a total of 45 patients with 46 episodes of PCP were hospitalized in building A of the Edouard-Herriot Hospital. Their age ranged from 23 to 56 years (median 41), and most of them were male (%). Thirty-six episodes were observed in 35 HIV-infected patients and 10 episodes in 10 RTRs. Thirty-one HIV-infected patients were admitted because of PCP, and PCP developed in all 10 RTRs and 4 HIV-infected patients during or shortly after hospitalization. These numbers represented a substantial increase compared to previous years, particularly in transplant recipients in whom only one case had been diagnosed during the 7 preceding years . During the period from 1993 to 1996, the number of hospital patient-days for transplant recipients decreased by 37%, whereas those of the populations of HIV-infected patients and of patients with PCP increased, respectively, by 36% and 63% . The number of admissions of HIV-infected patients in building A was 339 in 1993, 364 in 1994, 401 in 1995, and 445 in 1996. The number of admissions of transplant recipients was 469 in 1993, 311 in 1994, 319 in 1995, and 297 in 1996. Precise admission figures before 1993 are not available, but the number of renal transplants performed at Edouard-Herriot Hospital has been decreasing from 128 in 1987 to 74 in 1996 and has been stable since then . The immunosuppressive regimen has not changed for RTRs from 1990 to 1997 in Edouard-Herriot Hospital.\n\n【21】##### Molecular Typing _P. jirovecii_\n\n【22】Thirty-nine of the 46 BAL specimens collected from 1994 to 1996 were available for typing (in HIV-infected patients and 9 in transplant recipients). Nineteen (%) specimens corresponded to an infection with a single _P. jirovecii_ type, 15 (%) with two types, and 5 (%) with more than two types. A total of 19 different _P. jirovecii_ types were observed. In building A, the frequency of each type was 2%–12% of the _P. jirovecii_ isolates, except for type 1 which represented 39% of the isolates and was isolated in 19 patients. Type 1, the most prevalent, represented 10%–20% of the isolates in Switzerland and other European cities , as well as in building B and other hospital facilities of Lyon . In particular, the frequency of type 1 was significantly higher in building A than in the other hospital facilities of Lyon (of 45 versus 28 of 145, Fisher exact test p = 0.003). Moreover, the frequency distribution of type 1 in the different categories of PCP patients hosted in building A was significantly different: it represented 31% (of 39) of the isolates from the HIV-infected patients, but 70% (of 10) of those from the transplant recipients . Seven of the 12 HIV-infected patients infected with _P. jirovecii_ type 1 also harbored another type (coinfection), whereas one of the seven transplant recipients had a coinfection.\n\n【23】##### Encounters and Exposures between Patients with _P. jirovecii_\n\n【24】From 1994 to 1996, 14 of 39 patients with PCP and available BAL specimens had prior encounters with patients with PCP (of the 30 HIV-infected patients and 8 of the 9 transplant recipients). A total of 118 potential encounters between patients with active PCP and patients who developed PCP 3–12 weeks after the encounter could be retrieved . These 118 encounters corresponded to one or several encounters for the 14 patients. Among these 14 patients, PCP developed in 6 due to the same _P. jirovecii_ type as 1 or 2 encountered PCP source patients, and _P. jirovecii_ type 1 was involved in all 6 patients (transplant recipients, 1 HIV-infected patient). Of the 80 exposures involving _P. jirovecii_ type 1, six PCP episodes were observed compared to no episodes of 38 exposures not involving _P. jirovecii_ type 1.\n\n【25】Case-case comparison was used to compare exposure histories of two groups of patients, those harboring type 1 and those who did not. The proportion of patients who had at least one encounter during their susceptible period with a patient with active PCP harboring type 1 was higher in the first group (/19 vs. 2/20), although not significant (p = 0.13, Fisher exact test). The frequency of the M2 mutation in patients not receiving any sulfa prophylaxis was significantly higher in building A than in other hospital facilities of Lyon (/27 vs. 6/85, χ 2  test p = 0.0004).\n\n【26】##### Intervention\n\n【27】By mid-1996, all susceptible patients were placed on appropriate prophylaxis with co-trimoxazole (sulfamethoxazole plus trimethoprim), and HIV-infected patients had been started on highly active antiretroviral therapy. No PCP case was observed in 1997 , and no PCP cases were observed in transplant recipients as of December 2003.\n\n【28】### Discussion\n\n【29】During a 3-year period, 10 cases of PCP in transplant recipients occurred in a building of the Edouard-Herriot Hospital in Lyon, whereas only one case was observed in the preceding 7 years. These cases could not be attributed to improved diagnosis, change of immunosuppression regimen, an increase (a decrease actually occurred) of transplant recipients hospitalized in the facility. However, the outbreak occurred concomitantly with a progressive increase in the number of HIV-infected patients with and without PCP hospitalized in the same facility. Thorough molecular and epidemiologic analyses of the PCP cases showed the following facts: 1) transplant recipients, often in a stage of severe immunosuppression, shared the facility with HIV-infected patients with and without active PCP; 2) both transplant recipients and HIV-infected patients were receiving no or suboptimal anti-PCP prophylaxis; 3) _P. jirovecii_ type 1 represented 70% of the isolates from the transplant PCP cases, but it represented < 31% of the isolates in the HIV-infected patients with PCP in Lyon and elsewhere; and 4) all the transplant recipients (and some AIDS patients) in whom PCP developed had been hospitalized in the facility at some point during the 3 months preceding their PCP episodes. Moreover, the proportion of patients exposed to _P. jirovecii_ type 1 during their susceptible period was higher among cases infected with type 1 than among those not harboring type 1, although it did not reach statistical significance (p = 0.05). Finally, review of the medical charts indicated potential encounters between PCP cases during their susceptible period and other patients with active PCP . Encounters with patients with a PCP episode involving the same _P. jirovecii_ type, which may have led to transmission, were possible in 5 of the 10 transplant recipients and 1 of the 30 AIDS patients. Taken together, these facts suggest that at least half of the PCP cases in transplant recipients (and possibly some in AIDS patients) may be the result of a nosocomial acquisition of _P. jirovecii_ .\n\n【30】However, alternative explanations exist that cannot be excluded. First, the transient presence of _P. jirovecii_ in the air of hospital corridors has been described , which raises the possibility of an environmental source of _P. jirovecii_ type 1 in building A. The following facts argue against this possibility: 1) the existence of a long-lasting environmental source of _P. jirovecii_ has never been established, and 2) a high prevalence of type 1 was not observed in building B, which is located 100 m from building A and hosts patients with hematologic malignancies. Second, our study provides epidemiologic and molecular evidence that nosocomial transmission of _P. jirovecii_ can occur, but whether this transmission would have occurred directly or indirectly through carriers is unclear. Indeed, carriage of _P. jirovecii_ DNA has been described in the lungs of asymptomatic, immunosuppressed persons , as well as in the nose of immunocompetent relatives and healthcare workers in close contact with a PCP patient . Moreover, transmission by immunocompetent carriers to susceptible hosts has been demonstrated in the mouse model . Thus, indirect transmission through healthcare workers, physicians, or asymptomatic immunosuppressed patients cannot be ruled out.\n\n【31】For 14 of the 39 patients with PCP observed during the 3-year period, potential encounters with patients with active PCP during the 3 months preceding their episode had been documented . However, only _P. jirovecii_ type 1 was involved in encounters that apparently resulted in secondary cases. Part of this observation may be related to the higher prevalence of type 1 in Lyon (≈20%), although this could not explain the 40% rate of type 1 in building A. Another possibility is that type 1 might be more transmissible or virulent. This finding would be consistent with the fact that this type was one of the most prevalent types also in other geographic areas  . Moreover, specific _P. jirovecii_ genotypes have been associated with more severe clinical symptoms  or with resistance to certain drugs . In our study, a mutation in the active site of DHPS was present in all six presumptive nosocomial PCP cases. The mutation may have favored acquisition of type 1 rather than another type by the three patients who were receiving suboptimal prophylaxis with Fansidar (Roche, Nutley, NJ). The presence of this mutation in the nosocomial PCP cases of our study suggests that _P. jirovecii_ was acquired shortly before the episode because the frequency of DHPS mutations greatly increased only in the 1990s . Moreover, in patients not receiving any sulfa prophylaxis, the frequency of M2 mutation was significantly higher in building A than in other hospital facilities of Lyon (p = 0.0004), a fact that suggests nosocomial interhuman transmission of _P. jirovecii_ .\n\n【32】Our study provides insight into the relative importance of nosocomial acquisition of _P. jirovecii_ if infectious and susceptible patients are in close contact. Even though infected and susceptible patients were kept in unusually close proximity in this hospital, relatively few cases compatible with nosocomial interhuman transmission seem to have occurred. This finding suggests that transmission from patients with active PCP is limited, which is consistent with studies that we performed in HIV outpatient clinics that suggested infrequent cross-infections , as well as with a study comparing contact histories of patients with or without PCP . The source remains undetermined for the infection in the five transplant recipients for whom no potentially infectious encounters were found. One possibility is that carriers of _P. jirovecii_ in the hospital have played a role.\n\n【33】The available data and the arbitrary definitions we had to use, in light of the absence of precise scientific data on _P. jirovecii_ infection, are a limitation of our study. We could not demonstrate that the presumed encounters actually occurred or define the precise nature of the encounter. Also, we could not firmly exclude other potential sources of _P. jirovecii_ , such as the environment or asymptomatic carriers. Nevertheless, to our knowledge, this study is the first to suggest that _P. jirovecii_ may be nosocomially transmitted and acquired by severely immunosuppressed patients. Given the increased number of reports relating resistance to anti- _Pneumocystis_ drugs, prophylaxis of patients at risk might not be sufficient to achieve prevention. Moreover, prophylaxis is often not satisfactory because of secondary effects. Consequently, avoiding contact between persons at risk for PCP and patients with active PCP may be warranted and should be added to prevention guidelines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "531569c4-c6c9-459b-9a11-5da0a795a639", "title": "Delicacy and Durability: The Microbiological Sublime", "text": "【0】Delicacy and Durability: The Microbiological Sublime\nRogan Brown  Outbreak, 2014. Hand cut paper, foam board, transparent domes (.1 × 57.9 × 9.8 in /79 × 147 × 25 cm) © The Artist/Image Courtesy of the Artist.\n\n【1】Anglo-Irish artist Rogan Brown, who creates monochromatic sculptures from layers of paper, considers _Outbreak_ , this month’s cover image, an exploration “of the microbiological sublime.” Spilling from their petri dishes, overflowing into the space between them, the organisms appear kinetic; the sweep of light and shadow across the image adds depth and dimension to the all-white forms. For full impact, though, this sculpture needs to be viewed in person. In the artist’s words, “Each piece suddenly comes alive when it is placed vertically in the light. Photos only catch them at a certain moment. In reality, the pieces move with the changes in the ambient lighting, so they are always slightly different.” (The quoted text in this essay is from personal communication with Rogan Brown, November 17, 2014.)\n\n【2】To prepare for his work on _Outbreak_ , Brown studied myriad photos and diagrams depicting bacteria and viruses. “For the bacteria, I was attracted more by the flagellate forms (salmonella, _E. coli_ , _Vibrio cholerae_ ) simply because they offer a more aesthetically interesting bug-like shape, both beautiful and creepy at the same time, which was the effect I wanted to create. For the viruses, I was looking at influenza and HIV because these are forms that are most accessible and familiar to the public.” He notes that “Although I reference the aesthetics of scientific illustration, diagrams, cut-away models, and so forth, my goal is not to create accurate representations but works of art that create a visual, sensual impact.”\n\n【3】Pathogens and microbes vary immensely in their form, color, and complexity. Stains and dyes are often used to highlight pathogens viewed through microscopes, and those colored, contrasted images may be the first to come to mind. Instead of color, however, Brown focuses on contour and shape, methodically sculpting sheets of paper. For that task, he relies on both traditional tools such as knives and scalpels and modern tools such as laser cutters.\n\n【4】He cites the gluing process as being more stressful that cutting: “Each layer has to be placed with perfect precision on top of the preceding one. There are usually about 8 layers of paper separated by a hidden spacer to create the illusion of floating. The glue does not allow repositioning. I have only one shot, and mistakes are sometimes made.” Brown, who sometimes spends up to 5 months on a project, explains that “The finished artefact is really only the ghostly fossilized vestige of this slow, long process . I have chosen paper as a medium because it captures perfectly that mixture of delicacy and durability that for me characterizes the natural world.”\n\n【5】Originally, Brown planned to create a large installation from which the pathogens would swarm beyond the frame and flow over its walls, spilling onto floor of the gallery space—an idea that may yet see fruition. He found inspiration for this sculpture while attending a microbiology seminar at the Eden Project, a visitor attraction in the United Kingdom; during this seminar, a planned exhibition space focused on the Human Microbiome Project was discussed.\n\n【6】High-consequence pathogens, several of which are highlighted in this issue of _Emerging Infectious Diseases_ , provide a rich vein of inspiration for fiction writers and filmmakers. Actual outbreaks caused by such pathogens attract feverish media attention. Brown explains, “I wanted to create this sense of ferocious energy bursting out of the petri-dome, mocking our attempts to control it. Although created before the recent Ebola outbreak, the installation plays on our fears of the microbiological and our sense of powerlessness when confronted by nature.” The frozen tension inherent in _Outbreak_ plays to the imagination, starkly capturing the potential genesis of a frightening occurrence.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "587dd293-e50d-441d-af5b-370b5ad58418", "title": "Number of Physically Inactive Adults With Arthritis in the United States Who Could Improve Physical Function and Pain Control by Exercising", "text": "【0】Number of Physically Inactive Adults With Arthritis in the United States Who Could Improve Physical Function and Pain Control by Exercising\nAbstract\n--------\n\n【1】We estimated the number of physically inactive US adults with arthritis by state and nationally who could improve their physical function and pain control by participating in an exercise program. Our calculations were based on number-needed-to-treat, arthritis prevalence, physical inactivity, and 2010 US Census data. Estimates were lowest in the District of Columbia (physical function, n = 4,412; pain, n = 2,451) and highest in Texas (physical function, n = 325,504; pain, n = 180,835). Overall estimates were 4,119,792 for physical function and 2,288,771 for pain control. State-level estimates are important for allocating resources, public health program planning, and future research.\n\n【2】Objective\n---------\n\n【3】Arthritis is a major public health problem in the United States, where it affects approximately 54.4 million adults . Physical function and pain control are 2 major problems in adults with arthritis . An estimated 23.7 million adults aged 18 or older with arthritis have arthritis-attributable activity limitations , and approximately 14.6 million report experiencing severe joint pain . In addition, more than half of adults with arthritis report persistent pain . Although exercise has been shown to improve physical function and reduce pain , physical activity levels in adults with arthritis are low, with state-level estimates of physical activity varying widely . Because national data may not be suitable for factors such as incidence, prevalence, and policy in each state, our objective was to provide overall and state-level estimates of the number of physically inactive adults with arthritis who could improve their physical function and pain control by participating in a community-based exercise program.\n\n【4】Methods\n-------\n\n【5】We derived data for our study from 3 sources: 1) number-needed-to-treat (NNT) data from a previous meta-analysis that examined the effects of community-based exercise on physical function and pain in adults with arthritis aged 18 or older , 2) 2017 state-level estimates of the prevalence of arthritis and physical inactivity in adults with arthritis , and 3) state-level population data from the 2010 US Census . National estimates were also calculated.\n\n【6】We calculated the number of physically inactive adults with arthritis aged 18 or older who could improve their physical function and pain control by participating in a community-based exercise program by multiplying the reciprocal of the NNT by the number of physically inactive adults with arthritis in each state. The NNT represents the number of physically inactive adults with arthritis who would need to participate in a community-based exercise program for 1 person to improve physical function or pain control. Therefore, to calculate absolute estimates, we used the reciprocal of the NNT. For example, if the NNT for physical function is 5, then 1 in 5 (%) physically inactive adults with arthritis would need to participate in a community-based exercise program for 1 person to benefit. For 10,000 physically inactive adults with arthritis, 2,000 (% of 10,000) could improve their physical function by participating in a community-based exercise program. NNT, a well-established measure, presumes that not every individual participant in an intervention will experience its beneficial effects on a specific outcome. For example, a previous systematic review with meta-analysis reported that the NNT for adults with chronic musculoskeletal pain treated with topical nonsteroidal anti-inflammatory drugs (NSAIDs) was 5 for 1 (%) to benefit . When using standardized mean difference effect sizes to calculate the NNT, the larger the effect size, the smaller the NNT and the potential for a greater number of participants to benefit . Details regarding the approach we used are available elsewhere .\n\n【7】We calculated NNT by using data from the meta-analysis of 33 randomized controlled trials representing 3,180 men and women  and a previously developed formula , with NNT values of 5 for physical function and 9 for pain derived from standardized mean difference effect size improvements of 0.34 for physical function and 0.20 for pain . Lower and upper 95% CIs were generated by following the same approach. State-level prevalence estimates for arthritis and physical inactivity, defined as a response of no to the question “During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise,” were derived from 2017 Behavioral Risk Factor Surveillance System (BRFSS) data reported in a recent study  and state-level estimates for the number of adults aged 18 or older were obtained from 2010 US Census tables . These data were then used to calculate state-level prevalence estimates of arthritis and physical inactivity in adults with arthritis . Crude, rather than adjusted, estimates were used because age-standardized estimates are weighted to a standard population, and thus do not normally represent the percentage of individuals in the population when calculation of the absolute number of individuals is of interest. State-level data from the 2010 Census  were used because of the need to include the absolute number of all adults aged 18 or older in our calculations, data that were not available in the 2017 BRFSS report . For example, 2010 census data for the state of Alabama  with a focus on physical function showed 3,647,277 adults aged 18 or older, data that were not available in the 2017 BRFSS study . The population of 3,647,277 was then multiplied by the crude prevalence estimate of those with arthritis reported in the 2017 BRFSS data for Alabama (.3%) . This resulted in an estimated 1,214,543 adults in Alabama with arthritis (× 0.333 = 1,214,543). We then multiplied the estimated number of adults in Alabama with arthritis  by the percentage of physically inactive adults in Alabama based on the 2017 BRFSS study (.6%), arriving at an estimate of 541,686 (× 0.446). For physical function, the reciprocal of the NNT (/5, or 0.20) from the authors’ prior meta-analysis  was multiplied by 541,686, to arrive at an average of 108,337 physically inactive adults with arthritis in Alabama who could improve their physical function by participating in a community-based exercise program. Unless noted, results are reported using absolute values. All data were analyzed by using Microsoft Excel 2016 (Microsoft Corp).\n\n【8】Results\n-------\n\n【9】State-level estimates for the mean number of people who could improve their physical function by initiating a community-based exercise program ranged from a low of 4,412 (% CI, 3,882–4,985) in the District of Columbia to a high of 325,504 (% CI, 292,798–358,210) in Texas . For pain control, estimates were also lowest in the District of Columbia (n = 2,451; 95% CI, 2,157–2,770) and highest in Texas (n = 180,835; 95% CI, 162,665–199,005). Across all states and the District of Columbia, the estimated mean number of physically inactive adults in the United States with arthritis who could improve their physical function by initiating a community-based exercise program was 4,119,792 (% CI, 3,805,203– 4,444,217). For pain control, the estimate was 2,288,771 (% CI, 2,114,000– 2,469,007).\n\n【10】Discussion\n----------\n\n【11】Our findings provide support for promoting community-based exercise programs to improve pain control and physical function in physically inactive adults with arthritis. These findings are similar to those previously reported for depression and anxiety . Although these findings should be helpful to researchers and practitioners, they may be especially useful for decision makers (eg, funding agencies, state legislators), because they provide state-level information for prioritizing resources aimed at community-based programs that may yield the greatest return on investment, several of which already exist for exercise . For example, at the national level, identification of the absolute number of physically inactive adults with arthritis in each state who may improve their physical function and pain control by participating in a community-based exercise program enables funding agencies to better allocate appropriate financial resources in support of such programs in each state. At the state level, decision makers such as state legislators have quantitative data (ie, number of people who could improve their pain control and physical function by participating in community-based exercise programs) that should be helpful when lobbying for resources to support programs designed to decrease the number of physically inactive adults with arthritis in their state.\n\n【12】Our findings should be interpreted along with relative state and national data regarding physically inactive adults with arthritis. Consideration of both absolute and relative data are important because the choice of one over the other may influence conclusions regarding the magnitude, direction, significance, and implications of the issue being addressed . Future research should consider generating similar absolute, state-level estimates based on other outcomes considered important in adults with arthritis, such as quality of life and fatigue . For example, Furner et al reported that overall, 27% of adults with arthritis reported fair or poor health compared with 12% of adults without arthritis . However, for those adults with arthritis who participated in recommended amounts of physical activity, prevalence was reduced to 19.2% . The prevalence of fatigue has also been shown to be greater among adults with arthritis than those without . Hootman et al reported that the number of adults who reported significantly greater fatigue on most days was higher among adults with arthritis (.7%) than those without (.2%) and among those reporting fatigue every day (.2% vs 4.1%) . In addition, the prevalence of adults reporting fatigue every day was lower among those with arthritis who were physically active . Including estimates for such variables is considered important because it could help further demonstrate the multitude of outcomes that might be positively affected by engaging physically inactive adults with arthritis in a community-based exercise program rather than pharmacologic interventions such as NSAIDs. NSAIDs traditionally target one outcome , although that one outcome may be associated with improvements in other outcomes.\n\n【13】The major strength of our study is our use of different sources of information to arrive at state-level estimates of the number of physically inactive US adults with arthritis who could improve their physical function and pain control by exercising. However, our study has at least 5 potential limitations: 1) ecological fallacy, an inherent issue with aggregate data meta-analyses in which inferences regarding individual characteristics are made based on aggregate statistics, potentially making the inferences incorrect ; 2) lack of state-level data to examine the potential impact of the uncertainty of NNT estimates that use sensitivity analyses, for example, use of the approach described by Furukawa  because of a lack of control group risk data for pain and physical function in community-based exercise studies; 3) limitations of BRFSS data (self-report bias, low response rates for some states, exclusion of institutionalized populations) ; 4) lack of state-level data to examine subgroups (eg, sex, race/ethnicity, education, income); and 5) focus of the meta-analysis of randomized controlled intervention studies on structured community-based exercise , whereas data from the BRFSS were derived from the broader category of physical activity . With respect to the fifth limitation, the focus on community-based exercise programs in physically inactive adults with arthritis does not address the question of whether pain control and physical function could be improved if this population moved out of the category of being physical inactive, as defined by BRFSS , but did not meet the criteria for exercise . Thus, our findings may be underestimates given that our focus was on community-based exercise programs and not on the broader and more inclusive category of physical activity. Given these potential limitations, future research focused on state-level estimates of the benefits of exercise for improving physical function and pain control in adults with arthritis appears warranted, assuming necessary data are available.\n\n【14】In conclusion, our findings provide important state- and national-level information regarding the number of physically inactive adults with arthritis who could improve their physical function and pain control by participating in a community-based exercise program. This information should be helpful for allocating resources, public health program planning, and future research.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71018e43-1920-489b-ab74-035e97102a90", "title": "Coccidioidomycosis Skin Testing in a Commercially Insured Population, United States, 2014–2017", "text": "【0】Coccidioidomycosis Skin Testing in a Commercially Insured Population, United States, 2014–2017\nCoccidioidal skin testing has been a valuable epidemiologic and clinical tool for estimating the prevalence of previous _Coccidioides_ spp. exposure and monitoring treatment response . Such testing could also be useful for evaluating healthy persons’ risk of developing coccidioidomycosis . The skin test became commercially available again in 2014 after more than a decade; it is approved for adults 18–64 of age who have a history of pulmonary coccidioidomycosis . However, little is known about its use in the general population with unknown exposure to _Coccidioides_ . We describe features of patients who have employer-sponsored insurance who received a _Coccidioides_ skin test.\n\n【1】We used the IBM MarketScan Research Databases  to identify patients with a Current Procedural Terminology  code for a coccidioidomycosis skin test during 2014–2017. MarketScan health insurance claims data include outpatient visits and prescriptions and hospitalizations for employees, dependents, and retirees, representing >25% of all employer-sponsored beneficiaries throughout the United States. This analysis was not subject to review by the Centers for Disease Control and Prevention institutional review board because the data are fully deidentified.\n\n【2】We accessed the data through MarketScan Treatment Pathways, a web-based platform that includes data from persons with health insurance plans that contribute prescription drug data to MarketScan. We limited the analysis to patients continuously enrolled during the 3 months before and after the skin test. We examined periods up to 3 years before and 1 year after; because the primary features of interest did not change substantially, we focused on the smaller period to retain a larger study population.\n\n【3】We analyzed patient demographics; visits within 3 days to estimate the proportion who returned to have their test results read after 48 hours (compared with patients with a CPT code for tuberculosis skin testing); coccidioidomycosis diagnoses (International Classification of Diseases \\[ICD\\], 9th Revision, Clinical Modification, codes 115.00–115.99; ICD, 10th Revision, Clinical Modification, code B38); laboratory testing; and fluconazole prescriptions. We also examined certain underlying medical conditions and assessed the cost of skin test claims to patients and insurers among patients with noncapitated health plans.\n\n【4】Among ≈57 million MarketScan enrollees, 505 had a coccidioidomycosis skin test; 407 of those were continuously enrolled. Of those 407, most (n = 391, 89%) were 18–64 years of age, female (n = 243, 60%), and in California (n = 367, 90%) . Thirty-five percent had a code for a subsequent visit within 3 days, compared with 24% of 1,061,118 patients who had a tuberculosis skin test. Test results were not available.\n\n【5】In the 3 months before the skin test, 5% had a coccidioidomycosis diagnosis code, 5% had a coccidioidomycosis serologic test code, and 5% had a fluconazole prescription. On the skin test date and in the 3 months after, 7% had a coccidioidomycosis diagnosis code, 15% had a serologic test, and 9% had a fluconazole prescription. Forty-four patients (%) had noncapitated health plans; among those, the mean cost of skin test claims was $43.66 (range $0–$264). Mean costs were $31.57 (range $0–$184) to insurers and $12.09 (range $0–$264) to patients.\n\n【6】In the context of the large at-risk population in _Coccidioides_ \\-endemic areas, coccidioidomycosis skin testing appears to be uncommon in this privately insured population. Real-world data on the test’s use and performance in the general population are lacking, although it performs well for risk-stratifying prison inmates . Reasons for its low use could be its limited approved clinical indication to detect delayed-type hypersensitivity to _Coccidioides_ in persons with a known history of disease or that the clinical implications of such testing may be unclear. Cost may also play a role, although it is unclear why most patients had capitated health plans. Reasons why most tests were performed in California rather than in Arizona (states where most coccidioidomycosis cases occur) are unknown.\n\n【7】Patient age was consistent with the test’s approval for use in adults. However, few patients had coccidioidomycosis diagnosis codes, suggesting possible use of this test to screen for immunity in those with unknown exposure to _Coccidioides_ , which has not been evaluated. Another explanation for the low frequency of coccidioidomycosis diagnosis codes in the 3 months before testing is a more distant coccidioidomycosis history. We observed laboratory testing and fluconazole prescription patterns that suggest that the test might be occasionally used as a supplemental diagnostic tool.\n\n【8】Patient return visit rate (%) was comparable to that of tuberculosis skin testing. This proportion could appear falsely low if providers chose not to bill for reading the test results. In addition to lack of test results, limitations of this analysis include potential coding misclassification.\n\n【9】In summary, skin testing could be useful for evaluating healthy persons’ risk of developing coccidioidomycosis but appears to be rare, even in endemic areas. Determining features of patients who receive a coccidioidomycosis skin test and assessing clinicians’ knowledge and attitudes could provide insight into the test’s clinical and epidemiologic value.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3d8bded7-1460-4987-9f6e-c4f09c3a4fbd", "title": "Natural Vertical Transmission of Zika Virus in Larval Aedes aegypti Populations, Morelos, Mexico", "text": "【0】Natural Vertical Transmission of Zika Virus in Larval Aedes aegypti Populations, Morelos, Mexico\nZika virus is an enveloped, positive-sense, single-stranded sRNA, arthropod-borne virus (arbovirus) that is classified in the genus _Flavivirus_ , family _Flaviviridae_ . Zika virus is closely related to other viruses of medical importance, such as dengue, West Nile, and yellow fever viruses . Zika virus was discovered in Uganda in 1947, but it has attracted the attention of specialists in the past few years because of its rapid spread through the Pacific and into the Americas in 2015, as well as the severe neurologic manifestations associated with Zika virus infections, such as neonatal microcephaly and Guillain-Barré syndrome .\n\n【1】Several studies carried out under laboratory conditions have demonstrated that Zika virus can infect many different _Aedes_ mosquito species ; still, the key species for the transmission of Zika virus to humans are _Ae. aegypti_ and _Ae. albopictus_ . In this study, we focused on the species _Ae. aegypti_ , which has an urban behavior and is usually in close contact with humans . To date, 2 different mechanisms by which _Ae. aegypti_ mosquitoes can become infected with flaviviruses have been described: horizontal transmission and vertical transmission .\n\n【2】Horizontal transmission is considered the most common mode of transmission of arboviruses between mosquitoes and their vertebrate hosts and is responsible for the maintenance of arboviruses in nature, particularly during disease outbreaks. In contrast, the environmental maintenance of dengue virus (DENV) during interepidemic periods is thought to be caused by vertical transmission of the virus from the infected adult mosquitoes to their offspring for \\> 7 successive generations . Both mechanisms together are thought to be essential for the survival of viral pathogens in their habitats, preventing their extinction during harsh environmental conditions or in populations in which the presence of susceptible mammal hosts is low.\n\n【3】Vertical transmission of Zika virus in _Ae. aegypti_ mosquitoes has been evaluated under laboratory conditions by searching for the presence of Zika virus RNA in several organs from the offspring of infected mosquitoes, demonstrating the presence of viral RNA in the guts and salivary glands of the offspring . Additional studies have also demonstrated the presence of infectious Zika virus in the offspring of artificially infected _Ae. aegypti_ and _Ae. albopictus_ mosquitoes, suggesting that vertical transmission can occur in laboratory-bred mosquitoes . However, evidence is insufficient to confirm that vertical transmission is a principal maintenance mechanism for Zika virus in wild _Aedes_ mosquitoes.\n\n【4】In this study, we sought to demonstrate natural vertical transmission in _Ae. aegypti_ mosquitoes by detecting viral RNA and isolating infectious Zika virus from larvae hatched from field-collected eggs. We also assessed the infectivity of the isolate in a mammalian cell line and obtained the complete genome of the virus by next-generation sequencing (NGS).\n\n【5】### Materials and Methods\n\n【6】##### Study Site\n\n【7】We collected mosquito egg samples in the municipality of Jojutla, located in the southern region of the state of Morelos, Mexico (°36′N, 99°10′W). Arboviral infections are common in this municipality, which is usually hot and dry during October–April and warm and humid during May–September. During 2016, the incidence of Zika virus infections increased in Morelos; 269 cases were reported by the end of the year, of which 69 cases were from Jojutla. The reported cases in Jojutla were recorded as follows: 1 in August, 17 in September, 20 in October, 25 in November, and 6 in December .\n\n【8】##### Egg Collection and Hatching\n\n【9】We collected _Ae. aegypti_ eggs in collaboration with the National Center of Preventive Programs and Disease Control (CENAPRECE), following regulations for entomological surveillance with ovitraps (NOM-032-SSA2–2014). In 2016, 180 ovitraps were placed and maintained throughout the year in different locations in Jojutla . We determined the locations of the ovitraps on the basis of the incidence of human arbovirus infections. We collected the eggs using filter paper, which we then air dried for 2 days and stored in paper bags for further use.\n\n【10】We placed the egg papers in water containers and incubated them at 37°C for 1 wk under 24-h light/dark cycles. After the incubation period, we collected larvae in stages 2–3 and separated them into pools of 20–30 larvae. We macerated each of the pools in viral transport media (% BSA, 100 U/mL penicillin, 100 μg/mL streptomycin, 2.5 μg/mL fungizone, and NaHCO 3  in Hank’s solution) using a pestle mixer  and stored pools at −80°C for further analysis.\n\n【11】##### Detection of Zika Virus in Larvae of _Aedes aegypti_\n\n【12】We incubated the macerated larvae pools with Trizol  for 10 min, extracted total RNA from the lysates using Zymo-Spin RNA extraction columns , and quantified the RNA with a Nano Drop 2000 . We detected the presence of Zika virus RNA by quantitative reverse transcription PCR (qRT-PCR) using the TaqMan system _ with primers Zikv 1086 Fw 5′-CCGCTGCCCAACACAAG-3′ and Zikv 1162c Rv 5′-CCACTAACGTTCTTTTGCAGACAT-3′; and probe Zikv 1107-FAM probe 5′-AGCCTACCTTGACAAGCAGTCAGACACTCAA-3′, as previously described by Lanciotti et al. We calculated the minimum infection rate (MIR) of the larvae pools by dividing the number of positive larvae pools by the total number of larvae tested and multiplying by 1,000.\n\n【13】##### Viral Isolation and Identification\n\n【14】We selected the larvae pools that tested positive for Zika virus RNA and displayed the lowest quantitation cycle (Cq) values (range 22.3–33.4) for viral isolation. In brief, we diluted macerated larvae pools in maintenance medium (EMEM medium with 1% FBS, 100 U/mL penicillin, and 100 μg/mL streptomycin) and filtered them through 0.22 μm membranes . Twenty-four hours before infection, we seeded C6/36 cells in 24-well plates at 80% of confluence in growth medium (EMEM medium supplemented with 10% FBS, 100 U/mL penicillin, and 100 μg/mL streptomycin) and incubated them at 28°C in a 5% CO 2  atmosphere. Cells were adsorbed with the clarified larvae macerates for 2 h at 28°C. After the incubation period, we added fresh maintenance medium to the cells and left the infection to proceed for 7 d at 28°C and a 5% CO 2  atmosphere until a cytopathic effect was observable. We tested the supernatants of the infected cells for the presence of Zika virus RNA by qRT-PCR, as described earlier in this section. We performed 4 passages of each isolate in C6/36 cells to increase viral titers for NGS.\n\n【15】##### Indirect Immunofluorescence Assay\n\n【16】For the indirect immunofluorescence assays, we grew Vero cells over glass coverslips and then infected them with the Zika virus isolate. At 48 hours postinfection, we fixed cells with 4% paraformaldehyde and then incubated them with ice-cold methanol for 10 min. We blocked the cells using 10% FBS in phosphate-buffered saline (PBS) for 1 h and then incubated them overnight at 4°C with a 1:500 dilution of the monoclonal antibody 4G2. After the incubation period, we washed the cells twice with PBS and then incubated them with an Alexa Fluor 488-conjugated goat antimouse IgG antibody  for 1 h. We washed the cells twice with PBS and then mounted them with VECTASHIELD medium with DAPI  for confocal microscopy .\n\n【17】##### Plaque Assays\n\n【18】For the plaque assays, we seeded Vero cells in 24-well plates until they reached a confluence of 80%. We used 10-fold serial dilutions of the Zika virus isolate to infect the cell monolayers and left them to adsorb for 1 h. After the adsorption period, we removed the virus and overlaid cell monolayers with 1 mL of DMEM (Invitrogen) supplemented with 2% carboxymethyl-cellulose , 2% FBS, 100 U/mL penicillin, 100 μg/mL streptomycin, and 2 mM L-glutamine  and incubated at 37°C for 4 d. We fixed the cells with 4% paraformaldehyde in PBS and counterstained them with crystal violet-formaldehyde (Sigma-Aldrich).\n\n【19】##### Full Genome Sequencing and Assembly\n\n【20】For the complete genome sequencing of the Zika virus isolate, we depleted 200 ng of total RNA extracted from the supernatants of infected Vero cell monolayers of rRNA using the NEBNext rRNA Depletion Kit (human/mouse/rat) following the manufacturer’s instructions . We constructed RNaseq Illumina shotgun libraries at the Unidad Universitaria de Secuenciación Masiva y Bioinformática-Instituto de Biotecnología and sequenced them using paired-end sequencing with a MySeq system . We assessed the quality of reads using FASTQC ; low-quality positions and reads were eliminated using in-house scripts. We assembled the valid reads without reference using the program Trinity 2.8  and evaluated the assembly using Qualimap . Finally, we verified the identity of the contig using blastn .\n\n【21】##### Phylogenetic Reconstruction\n\n【22】To determine the phylogenetic relatedness of the Zika virus isolate, we retrieved relevant Zika virus open reading frame (ORF) RNA sequences from the Virus Pathogen Resource (ViPR) database  . We removed identical sequences and those with undetermined bases from alignment. We applied near-identity clustering (.999) to the remaining sequences in Cd-hit-test  and used 98 sequences for phylogenetic inference by maximum likelihood in RAxML  under a general time reversible plus gamma substitution model. We reconstructed the tree with 200 bootstrap replicas. We normalized branch lengths and condensed nodes with bootstrap values <50% to emphasize tree topology.\n\n【23】##### Variant Analysis\n\n【24】To determine the presence of variants in the larva-derived sequence, we used the alignment to identify positions that had mutations unique to our sequence or that were primarily shared with other mosquito-derived sequences and that were absent or rare in the human-derived genomes. To this end, we examined the alignment with the tool meta-CATS , which performs a χ 2  test to find positions with different polymorphism distribution between groups.\n\n【25】### Results\n\n【26】##### Zika Virus Isolation and Identification in _Ae. aegypti_ Larvae\n\n【27】During the study period, we analyzed 151 larvae pools by qRT-PCR in search of Zika virus RNA. Only 17 (.8%) of the pools tested positive: 9 (.7%) from the larvae raised from the eggs collected in June and 8 (.1%) from larvae raised from the eggs collected in November. To determine the proportion of infected larvae in the population, we calculated MIRs for the 2 collection periods (June and November 2016) and found an increase in the MIR observed from the larvae raised from the eggs collected in June (.5) to the MIR from the eggs collected in November (.9) .\n\n【28】To confirm the presence of infectious Zika virus in the larvae pools, we used 11 of the 17 pools that tested positive for Zika virus RNA that displayed the lowest Cq values to attempt viral isolation. We were able to isolate infectious Zika virus from only 1 of the larvae pools (N), as determined by the presence of cytopathic effect in C6/36 cells characterized by monolayer detachment  and the detection of Zika virus RNA in culture supernatants by qRT-PCR with a Cq value of 23.73.\n\n【29】The ability of the 31N isolate to infect a mammalian cell line was confirmed by the appearance of the characteristic cytopathic effect of Zika virus in infected Vero cell cultures, characterized by cell rounding and detachment . Moreover, immunofluorescent detection of the viral envelope (E) protein using the monoclonal antibody 4G2 revealed perinuclear staining . The plaque assays carried out in Vero cells revealed that the 31N isolate has the ability to produce lytic plaques; thus, this isolate can be considered cytopathic in mammalian cell culture .\n\n【30】##### Complete Genome Sequencing of Zika Virus Isolate 31N\n\n【31】We isolated viral RNA from the supernatant of Vero cells infected with the fourth passage of Zika virus isolate 31N and processed it by RNA sequencing by the NGS MiSeq Illumina protocol. Around 40% of the total reads  were assembled in a single contig of 10,795 nt in length, with a mean depth of 42,000 reads. BLAST results of the assembled contig revealed that the strain ZIKV/Aedes.sp/MEX/MEX\\_I-7/2016, which belongs to a Zika virus isolated from _Aedes_ mosquitoes obtained in the state of Chiapas, Mexico, had the highest identity (.8%) with our isolate.\n\n【32】##### Variant Analysis\n\n【33】We identified single-nucleotide variants (SNVs) in the Zika virus isolate 31N genome by aligning the sequence with other mosquito- and human-derived sequences obtained from the ViPR database and running the meta-CATS analytic tool, as described in the Materials and Methods section. To carry out this analysis, we grouped the sequences by the host of origin. We identified SNVs in positions 3176, 3286, and 5636 that were unique to the larva genome. On the other hand, the SNVs in positions 2071 and 3333 were found in human-derived sequences but were not present in other mosquito genomes. The SNV in position 2071 was shared only with a sequence from French Polynesia, likely an example of parallel evolution, whereas the polymorphism in position 3333 was common with 2 sequences from Mexico and may correspond to a local variant. Finally, we found 8 SNVs that were also present in other human and mosquito sequences but were more common in mosquitoes. Only 1 SNV was found in the E gene, whereas the rest were found in nonstructural (NS) genes: 5 in NS2A, 4 in NS3, 2 in NS5, and 1 in NS1 .\n\n【34】##### Phylogenetic Reconstruction of Zika Virus\n\n【35】To characterize the evolutionary relationship between the 31N isolate and other Zika virus genomes that have been previously reported, we performed a phylogenetic analysis. The phylogenetic reconstruction of the complete genomes of 98 Zika virus sequences, including the 31N isolate, revealed that the 31N isolate belongs to the Asian-American lineage of Zika virus and clusters together with other sequences of human and mosquito origin from Mexico .\n\n【36】### Discussion\n\n【37】Natural vertical transmission of mosquito-carried viruses has been proposed as one of the main ecologic processes involved in the maintenance of these viruses in susceptible mosquito populations, particularly during interepidemic periods and harsh climate conditions when horizontal transmission becomes difficult . In Mexico, most of the studies regarding natural vertical transmission of arboviruses in mosquitoes have been carried out using DENV as a study model , so the diversity of viruses that can be transmitted from infected adult mosquitoes to their offspring still needs to be characterized.\n\n【38】Only a few studies have addressed the natural vertical transmission of Zika virus in wild mosquito populations; most of these have been carried out in Brazil, where Zika virus RNA has been detected in male _Ae. aegypti_ mosquitoes and in adult _Ae. albopictus_ mosquitoes raised from field-collected eggs .\n\n【39】In this study, we demonstrated the occurrence of natural vertical transmission of Zika virus in wild mosquito populations from the municipality of Jojutla in the state of Morelos, Mexico. The RNA of Zika virus was detected in 17 larvae pools; the rates of vertical transmission of Zika virus in the wild _Ae. aegypti_ populations were estimated by calculating the MIR. In Morelos, the rainy season begins in late May and extends through the end of September, after which both the precipitation and the mosquito populations start to decrease. Thus, the higher MIR (.95) calculated from the larvae hatched from the eggs collected in November 2016, in contrast to the MIR from the larvae hatched from the eggs collected in June (.6), might be correlated with the increased number of human Zika virus cases that were reported in Jojutla during November, whereas during June no human cases were reported. The absence of reported Zika virus human cases in June could be the result of asymptomatic cases, cases clinically misdiagnosed as dengue virus infections, or both.\n\n【40】The higher number of persons infected with Zika virus in Jojutla during November reflects an increase in the number of infected mosquitoes resulting from horizontal transmission of the virus between mosquitoes and infected humans. It is possible that vertical transmission is contributing to the number of infected mosquitoes, which are, in turn, capable of transmitting this virus to a higher number of humans. However, the relative importance of vertical transmission for the maintenance and spread of the virus cannot be elucidated with the data available so far.\n\n【41】The MIR is usually affected by the number of mosquitoes that make up the pool of mosquitoes (or their immature stages) tested, which usually causes an underestimation of the real number of infected mosquitoes in a population . Thus, the rate of vertical transmission in the mosquito populations from Morelos might be even higher than estimated. Previous studies performed with other flaviviruses, such as DENV, have reported MIRs as low as 0.18  and 0.6  or as high as 40  in wild larvae, usually associated with the collection date and the place of sampling. Under laboratory conditions, the filial infectious rate of Zika virus in _Ae. aegypti_ mosquitoes ranged from 4.9 to 24.6, depending on the strain of the virus tested , which corroborates that our results are comparable to the MIRs reported for other Zika viruses and other flaviviruses, including DENV .\n\n【42】In this work, we were also able to demonstrate the natural vertical transmission of Zika virus in _Ae. aegypti_ mosquitoes by the successful isolation of infectious Zika virus (N) from larvae raised from field-collected eggs. This evidence strongly suggests that infectious Zika virus can be transmitted from adult female mosquitoes to their offspring and increases the evidence of the role of natural vertical transmission in the maintenance of Zika virus in wild mosquito populations.\n\n【43】The isolation of infective Zika virus vertically transmitted from adult mosquitoes to their offspring suggests that this virus could be potentially transmissible between mosquitoes and their vertebrate hosts; nevertheless, this transmission still needs to be demonstrated. Moreover, we were able to detect several SNVs in the larvae-derived Zika virus isolate 31N, of which 3 were specific to this larvae-derived genome, as well as 10 others that were shared between the larvae and other mosquito and human sequences. Although it is plausible that these SNVs were acquired during virus culture, we think this is unlikely because 72 of the sequences included in the alignment corresponded to cultured viruses and none of them presented these mutations; however, the sequences from more larvae-derived viruses need to be determined to establish the significance of these SNVs. The larvae-specific SNVs were located in the coding regions for the NS2A and NS3 proteins, which are involved in the replication of the viral RNA . Nevertheless, whether the larvae-specific SNVs are associated with the maintenance and vertical transmission of infectious Zika virus from adult mosquitoes to their offspring still needs to be determined.\n\n【44】In summary, we demonstrated natural vertical transmission of Zika virus in wild _Ae. aegypti_ mosquitoes. Our results suggest that this transmission mode could aid in the spread and maintenance of Zika virus in nature, expanding the ongoing zoonotic threat from this virus to human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0841ffcb-cbb3-4e4e-98fa-a1228e1f356c", "title": "Atypical Q Fever in US Soldiers", "text": "【0】Atypical Q Fever in US Soldiers\nQ fever, caused by infection with _Coxiella burnettii,_ is an emerging infectious disease among US soldiers deployed to Iraq and Afghanistan; >30 cases have been reported . We describe 3 cases of Q fever in soldiers treated from July through December 2006 at Walter Reed Army Medical Center (WRAMC).\n\n【1】### The Patients\n\n【2】In December 2006, 1 week after returning from Iraq, a 22-year-old white male Army National Guard member was seen at a New Hampshire hospital, with flulike symptoms, pleuritic chest pain, and mild abdominal pain. His initial examination noted temperature of 38.3°C, leukocytes 3.3 × 10 9  cells/µL (normal 4.5–10.5 × 10 3  cells/μL), platelets 121 × 10 3  cells/μL (normal 150–450 × 10 3  cells/μL), aspartate aminotransferase (AST) 144 IU/L (normal 15–46 IU/L), and alanine aminotransferase (ALT) 154 IU/L (reference 11–66 IU/L). He was admitted and treated with ceftriaxone and azithromycin. Although his fever decreased within 48 h, he had persistent abdominal pain, worsening liver function test results (AST 779, ALT 993, alkaline phosphatase 269 U/L \\[reference 38–126 U/L\\]), and increasing shortness of breath. An ultrasound examination of the right upper quadrant showed hepatosplenomegaly and a thickened gall bladder wall without evidence of cholelithiasis. Despite initially normal chest radiographic results, a repeat radiographic examination showed bilateral pulmonary infiltrates. Ceftriaxone therapy was discontinued, pipercillin/tazobactam therapy was started, and azithromycin was continued. General surgery stated that the patient had a nonsurgical abdomen. After consultation with WRAMC, the patient was given a dose of doxycycline and gentamicin before being transferred to a New Hampshire medical center. Blood cultures and serologic tests for Epstein Barr virus and cytomegalovirus were pending. A computed tomographic (CT) examination of the chest, abdomen, and pelvis  showed gall bladder wall thickening (mm) without ductal dilatation, hepatosplenomegaly, and bilateral ground glass pulmonary infiltrates. Serologic tests were negative for hepatitis B and C. Thick and thin smears were negative for parasitic disease. Despite the findings on CT scan, the patient began to improve clinically and had resolution of abdominal pain and shortness of breath. He was transferred to WRAMC, where he continued to improve. Pipercillin/tazobactam was discontinued, but doxycycline was continued. A presumptive diagnosis of Q fever was made, and he was discharged to complete a 14-day course of doxycycline. Serologic tests for _C. burnettii_ were positive with a phase 2 immunoglobulin M (IgM) titer of 256 (negative <1:64), phase 2 IgG titer of 128 (negative <16), and negative phase 1 serologic results. A month later he felt well and had normal liver function test results. No exposure factors were identified.\n\n【3】The second case occurred in December 2006, when a previously healthy 24-year-old male Army National Guard member was admitted to the 28th Combat Support Hospital (CSH) in Baghdad, Iraq, with flulike symptoms, mild nausea, and a dry, 10-day cough. At admission, his temperature was 40.2°C, but his other vital signs were normal. He had mild epigastric tenderness to palpation; otherwise, examination results were normal. Laboratory results included leukocytes 3.9×10 3  cells/μL, platelets of 130×10 3  cells/μL, alkaline phosphatase 104 U/L, AST 824 U/L, ALT 786 U/L, total bilirubin 1.2 mg/dL (reference 0.2–1.3 mg/dL), and gamma glutamyl transferase (GGT) 97 (reference 12–58). Initial erythrocyte sedimentation rate was within normal limits at 18 mm/hr (reference <20 mm/h). Results of blood cultures, monospot, and hepatitis B, C, and HIV screens were negative. A CT scan showed diffuse enhancement of the gallbladder with gallbladder wall thickening . A small amount of pericholecystic fluid was seen, but no distension of the gallbladder or gallstones were noted. These findings prompted a general surgery evaluation for acute cholecystitis, but their examination results were not consistent with this diagnosis. Given the patient’s flulike symptoms and laboratory abnormalities, the diagnosis of Q fever was considered. The patient had initially been treated with doxycycline and metronidazole, but metronidazole was discontinued when his physical examination results remained benign. His fever curve decreased within 2 days of receiving doxycycline. He was transferred out of theater to Landstuhl Regional Medical Center in Germany for further evaluation. Q fever was confirmed with _C. burnettii_ serum titers of 2,048 for phases 1 and 2 IgM. He improved with doxycycline, 100 mg twice a day for 14 days, and was subsequently returned to duty. No exposure factors were identified.\n\n【4】The third case occurred in July 2006 in a 34-year-old female active duty soldier with a history of asthma. She was seen at the troop medical clinic in Baghdad, Iraq, with flulike symptoms. She was given symptomatic treatment and released but returned with altered mental status, shortness of breath, and abdominal pain. A CT scan of her chest showed a left lower lobe infiltrate and bilateral pleural effusions. An ultrasound examination of the right upper quadrant showed no abnormalities. She was transferred to the 10th CSH in Baghdad for further care. She remained febrile (.8°C) and tachycardic and required 4 L/min of oxygen via nasal cannula to maintain an oxygen saturation of 96%. Results of laboratory tests conducted at the time of admission were unremarkable except for a mild transaminitis (AST 139 and ALT 96). She was treated with levofloxacin, 500 mg per day intravenously, for suspected pneumonia. She had rapid worsening of her respiratory status over the next 8 hours and required intubation. Antimicrobial drug coverage was broadened to include piperacillin/tazobactam 3.375 parenterally every 6 hours; solumedrol was added, given her history of asthma. She was evacuated to Landstuhl Regional Medical Center in Germany. A bronchoscopy was performed, but results were unremarkable. Her chest radiographs showed progression to acute respiratory distress syndrome (ARDS), and arterial blood gas testing showed partial pressure of arterial oxygen to be 50–60 mm Hg. Blood, sputum, and urine cultures were negative. Doxycycline was prescribed for possible Q fever. She improved and was evacuated to WRAMC, where she was afebrile (.2°C) at admission. Her pulmonary status improved quickly, and she was extubated. She was discharged and completed 14-day courses of levofloxacin and doxycycline. Her serologic test results were positive for Q fever with phase 2 IgM titer of 1,024. No exposure risks were identified.\n\n【5】### Conclusions\n\n【6】Fever, pneumonia, and/or hepatitis are the most common signs of acute infection with Q fever . In those in whom chronic disease develops, infective endocarditis is the initial condition in >70% of cases. Asymptomatic infection may occur in >50% of infected patients . Despite its typical signs and symptoms, Q fever is known to have a multitude of clinical manifestations. Raoult described >7 distinct presentations : fever, pneumonia, hepatitis, meningitis, meningoencephalitis, pericarditis, and myocarditis. Parker et al. described >30 clinical syndromes . This broad variation can result in delayed diagnosis.\n\n【7】Only 12 cases of acute cholecystitis associated with Q fever have been reported in the English medical literature . The largest and most detailed description is from a case series by Rolain , who described 9 patients whose initial sign of Q fever was acute cholecystitis. Clinical data are available for only 1 other case . The most appropriate treatment for these patients remains a question. For these 10 patients, 6 had cholecystectomy. The remaining 4 and our 2 patients did well with medical management alone. Four of the 6 patients received doxycycline, 1 received ofloxacin, and 1 received no treatment. Q fever is often self-limiting; yet treatment is recommended to shorten duration of symptoms and prevent chronic disease .\n\n【8】Reina-Serrano recently suggested that patients with Q fever–associated cholecystitis could be managed medically . Two of our patients had evidence of cholecystitis on imaging studies but did not have evidence of peritonitis on physical examination. Our 2 patients with radiographic cholecystitis responded quickly to doxycycline. We propose that for patients with acute acalculous cholecystitis and a high suspicion for Q fever, doxycycline be given empirically. The patients’ clinical response should be evident within 48 hours and surgery may be avoided. If a patient has gallstones or acute abdominal pain, a standard approach for treating acute cholecystitis should be followed.\n\n【9】The third patient in our series progressed to ARDS, which has been reported, albeit rarely, with Q fever . More typically, pneumonia secondary to acute Q fever infection results in a dry to productive cough, pleuritic chest pain, and focal or bilateral infiltrates on chest radiographs .\n\n【10】Our patients denied having typical risk factors, including exposure to livestock or consumption of local meat or dairy products. However, direct exposure to such products is not necessary . We agree with Anderson et al. who suggested that providers strongly consider adding doxycycline to the treatment regimen for deployed soldiers with severe pneumonia .\n\n【11】Q fever is a Category B biologic agent and must be considered as a potential threat to deployed soldiers . The most likely mode of attack would be aerosolization; given the low dose required for infection (organisms), multiple cases would follow. We considered bioterrorism unlikely, given the limited number of clinically symptomatic cases and the lack of a cluster of cases.\n\n【12】Q fever continues to be a threat to deployed US soldiers in Southwest Asia. Lack of knowledge about it can delay diagnosis and treatment. It should be considered in the differential diagnosis of any deployed or recently deployed soldier with a febrile illness, especially when hepatitis or pneumonia is present.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d42fd5d-046a-4ede-9895-b58979cbc839", "title": "Introduction of Eurasian-Origin Influenza A(H8N4) Virus into North America by Migratory Birds", "text": "【0】Introduction of Eurasian-Origin Influenza A(H8N4) Virus into North America by Migratory Birds\nResearch of and surveillance for influenza A viruses in wild birds inhabiting western Alaska have consistently provided support for the exchange of viruses between East Asia and North America via Beringia . Sampling of wild birds inhabiting Izembek National Wildlife Refuge (NWR) and surrounding areas in Alaska (≈55°N, 163°W) conducted during 2011–2015 has been used in recent research to identify the dispersal of influenza A(H9N2) viruses among China, South Korea, and Alaska ; provide inference about the evolutionary pathways of economically important foreign-origin poultry pathogens introduced into North America ; and identify sampling efficiencies for optimizing the detection of evidence for intercontinental virus exchange .\n\n【1】During September–October 2016, we collected 541 combined oral-pharyngeal and cloacal swab samples from hunter-harvested waterfowl (Anseriformes_ spp.) and 401 environmental fecal samples from monospecific flocks of either emperor geese (Chen canagica_ ) or glaucous-winged gulls (Larus glaucescens_ ) within and around Izembek NWR. Samples were deposited into viral transport media, placed in dry shippers charged with liquid nitrogen within 24 h, shipped, and stored frozen at −80°C before laboratory analysis. We screened samples for the influenza A virus matrix gene and subjected them to virus isolation; resultant isolates were genomically sequenced in accordance with previously reported methods . A total of 116 samples tested positive for the matrix gene, and 38 isolates were recovered of the following combined subtypes: H1N2, H3N2, H3N2/N6 (mixed infection), H3N8, H4N6, H5N2, H6N2, H7N3, H8N4, and H12N2. We selected the single H8N4 isolate, A/northern pintail/Alaska/UGAI16-3997/2016(H8N4) (GenBank accession nos. MG976689–96), for genomic characterization as part of this investigation.\n\n【2】We queried sequence information for the complete coding region of each gene segment of A/northern pintail/Alaska/UGAI16-3997/2016(H8N4) against the GenBank database to identify strains sharing \\> 99% nt identity. We then reconstructed maximum-likelihood phylogenetic trees for each gene segment in MEGA 7.0.21  by incorporating sequence information for representative reference sequences from avian-origin influenza A virus isolates from Eurasia and North America using the general time-reversible plus invariant sites (G+I) model with 1,000 bootstrap replications.\n\n【3】Gene segments for A/northern pintail/Alaska/UGAI16-3997/2016(H8N4), isolated from a sample collected from a hunter-harvested duck on September 6, 2016, shared \\> 99% nt identity to those of \\> 1 isolates recovered from wild and domestic birds sampled in East Asia during 2006–2016 . This isolate also shared \\> 99% nt identity with 1–4 isolates recovered from wild bird samples collected at Izembek NWR during 2012–2015 at the polymerase acidic and polymerase basic 2 gene segments . A/northern pintail/Alaska/UGAI16–3997/2016(H8N4) did not, however, share \\> 99% nt identity at all 8 gene segments with any other influenza A virus isolate for which genomic information was available, indicating that this H8N4 isolate might represent a previously unidentified or unreported genome constellation .\n\n【4】Phylogenetic analyses strongly supported structuring of tree topologies into major clades by continental affiliation of reference sequences (bootstrap values \\> 99). Sequence information for all 8 gene segments of A/northern pintail/Alaska/UGAI16-3997/2016(H8N4) clustered within clades composed of reference sequences for influenza A viruses originating from samples collected in Eurasia . Therefore, phylogenetic analyses provided support for Eurasian ancestry of this genomic constellation. We inferred our results to provide evidence for the introduction of this foreign-origin H8N4 virus into North America by migratory birds given previous support for intercontinental viral dispersal derived through genetic characterization of avian influenza A viruses originating from western Alaska , the intercontinental migratory tendencies of northern pintails  and other species inhabiting Izembek NWR at the time of sampling , the paucity of domestic poultry in this region, and the proximity of Izembek NWR to East Asia.\n\n【5】During 2010–2016, research and surveillance for influenza A viruses in wild birds inhabiting North America have provided evidence for the intercontinental dispersal of the following 4 viral genome constellations between Eurasia and North America: H16N3 , H9N2 , highly pathogenic clade 2.3.4.4 H5N8 , and H8N4 (this study). Four reports of independent purported intercontinental dispersal events for influenza A viruses via migratory birds during 7 years of sampling do not disprove the paradigm of restricted viral dispersal between Eurasia and North America. However, repeated detections of these viruses crossing the Bering Strait (this study) suggest that viral dispersal between East Asia and North America might not be exceedingly rare. Thus, a lack of selective advantage for comparatively rare foreign-origin influenza A viruses, purifying selection for endemic viruses, or both might be important mechanisms regulating the establishment of these viruses within the wild bird reservoir. Therefore, additional research directed toward understanding selection pressures regulating the establishment of these viruses might provide useful inference for informing surveillance and response activities for economically costly or potentially pandemic foreign-origin viruses in wild birds inhabiting North America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "79ad3dfe-4a6a-41ae-88e7-e5df83edc59b", "title": "New School Meal Regulations and Consumption of Flavored Milk in Ten US Elementary Schools, 2010 and 2013", "text": "【0】New School Meal Regulations and Consumption of Flavored Milk in Ten US Elementary Schools, 2010 and 2013\nAbstract\n--------\n\n【1】Milk is a source of shortfall nutrients in children’s diets, but most children do not consume recommended amounts. We measured consumption of milk by elementary-schoolchildren (grades 3–5) in a diverse sample of schools before and after implementation of the US Department of Agriculture’s updated meal regulations requiring flavored milk to be fat-free. Flavored milk consumption did not change from 2010 to 2013; 52.2% of students in 2010 and 49.7% in 2013 consumed 7 ounces or more of an 8-ounce container. Updated regulations succeeded in lowering the amount of fat, added sugars, and calories in school milk but did not change overall milk consumption, thus improving children’s diet quality.\n\n【2】Objective\n---------\n\n【3】Milk is a source of shortfall nutrients (vitamin D, calcium, and potassium) in children’s diets, supporting bone mass accumulation . Eating patterns are established early; children consuming soft drinks as early as age 5 years drink less milk as teenagers and consume more added sugars , which is associated with higher risk of obesity, diabetes , and coronary heart disease . Consuming milk, including flavored milk, is positively correlated with diet quality  and is not associated with excess weight gain . However, nearly 25% of children drink no milk on any given day . School meals are healthier than those sent from home ; consuming milk with school lunch supports overall diet quality . Most (%) elementary school students choose flavored milk over plain milk at school ; children’s flavored milk consumption is not associated with higher intakes of added sugars . Banning flavored milk in school because of concerns about added sugars may lead to fewer children consuming milk . Children rejecting milk at school are unlikely to make up for the lost nutrients by consuming milk outside school . Updated US Department of Agriculture (USDA) regulations require all milk served in schools to be low-fat or fat-free . We assessed the consumption of flavored milk by schoolchildren after updated school meal regulations took effect.\n\n【4】Methods\n-------\n\n【5】As part of a larger study evaluating the acceptance of lower-calorie (≤150 kcal per 8-oz serving) flavored milk, we asked 10 milk processors across the United States serving school districts in 23 states to identify public school districts using such flavored milk during the 2008–2009 or 2009–2010 school year. A purposive sample (n = 35) was selected for recruitment, and 22 school nutrition directors returned a signed informed consent form. We selected a convenience sample of 10 school districts in urban, suburban, or rural regions for a quasi-experimental plate-waste study (including 4 districts using standard flavored milk \\[>150 kcal per 8 oz\\]). Seven of the 10 districts are in the northeast (Massachusetts, New York, Rhode Island, and Vermont), and 3 are in the southern United States (Georgia and Texas). One elementary school serving grades 3 through 5 was randomly selected from each district for data collection in spring 2010 (baseline) and again in 2013 (follow-up) after flavored milk formulations changed to comply with updated USDA regulations .\n\n【6】At baseline and follow-up, milk containers were collected from students at the end of lunch and weighed to measure consumption. The grade, sex (identified by teachers), and container weights for each student were recorded. Descriptive analyses were used to describe school and student characteristics; we used χ 2  analyses to examine proportional differences. Data on milk consumption were positively skewed, unresponsive to transformation, and thus reclassified into a binary variable (oz or >7 oz). Generalized linear mixed models (PROC GLIMMIX) were used to test differences in milk consumption while accounting for students nested within schools. A categorical variable for region was entered into the model as a proxy for differences between northeastern and southern schools in race/ethnicity and percentage of students eligible for free or reduced-price meals. Data were calculated as model-based least square means adjusted for region, grade, and sex. All analyses were 2-tailed tests performed using SPSS (version 21.0, IBM Corp) and SAS (version 9.4, SAS Institute, Inc). The University of Vermont Institutional Review Board approved this research as exempt.\n\n【7】Results\n-------\n\n【8】The sample of schools was diverse in location, size, and racial/ethnic enrollment . Flavored milks in all schools were fat-free during the 2012–2013 school year. Approximately 1.25 teaspoons of sugar per serving were removed from flavored milk in these schools; calories were reduced by up to 40 calories per 8-oz container. A total of 1,718 containers (n = 885 in 2010; n = 833 in 2013) of flavored milk (% chocolate) were collected from students (.5% boys) in 10 schools. Of these containers, 8.1% in 2010 and 10.8% in 2013 were unopened (χ 2  \\= 3.6, _P_ \\= .06). Compared with northeastern schools, a larger proportion of containers were unopened in southern schools (.3% unopened in 2010, χ 2  \\= 25.6, _P_ < .001; 23.1% unopened in 2013, χ 2  \\= 72.9, _P_ < .001), where a higher percentage of students were eligible for free or reduced-priced meals.\n\n【9】Milk consumption patterns differed by sex, grade, and region . Overall, boys were more likely than girls to consume most (>7 oz) of their milk (odds ratio \\[OR\\], 1.53; 95% confidence interval \\[CI\\], 1.29–1.81; _P_ < .001), this pattern was most evident in 2010 when 63.5% of boys and 53.6% of girls drank most of their milk (χ 2  \\= 9.0, _P_ \\= .003). Overall, students in grade 3 or 4 were more likely than students in grade 5 to consume most of their milk (OR, 1.45; 95% CI, 1.18–1.77; _P_ \\= .001). Children in northern schools were slightly, but not significantly, more likely to consume most of their milk compared with children in southern schools over time (P_ \\= .12). After accounting for differences in region, sex, and grade, the likelihood that children would consume most of their milk did not change from 2010 to 2013 (OR, 0.90; 95% CI, 0.72–1.12; _P_ \\= .35) .\n\n【10】Discussion\n----------\n\n【11】This research showed that, among elementary schoolchildren selecting flavored milk with school lunch, consumption did not change from 2010 to 2013. Our study’s main strength is that we objectively measured milk consumption in a large, diverse sample of schools. The study had limitations. Although the flavored milks offered in schools participating in this study were reformulated, the nutrition profile of milk served varied among schools. Fewer children participated in school lunch programs under the new regulations; therefore our sample was smaller in 2013. Milk consumption may be influenced by other foods served in school. We collected data for only 1 day each year, and results may not be generalizable to schools in other parts of the country.\n\n【12】A recent American Academy of Pediatrics policy statement on school foods and beverages  supports the addition of small amounts of sugar to nutrient-dense foods to increase consumption by children. Updated USDA school meal standards have succeeded in removing fat and decreasing the amount of added sugars and calories in flavored milk served in schools without decreasing consumption. Public health and medical professionals can feel confident in encouraging children to drink milk, including flavored milk, with school meals because milk improves children’s nutrient intake and is low-fat (plain) or fat-free and limited in added sugars.\n\n【13】Tables\n------\n\n【14】#####  Table 1. Demographic Characteristics of 10 Elementary Schools That Participated in Study of Flavored Milk Consumption and Flavored Milk Nutrition Profile, Before (Spring 2010) and After (Spring 2013) Updated USDA School Meal Regulations a, b  \n\n| Characteristic | Spring 2010 | Spring 2013 |\n| --- | --- | --- |\n| **Elementary schools** | **Elementary schools** | **Elementary schools** |\n| School enrollment, mean (range), n | 524  | 519  |\n| Student eligibility for free or reduced-priced school meals, mean (range), % | 52.9 (.6–88.9) | 53.9 (.6–87.7) |\n| Student race/ethnicity, % (SD) | Student race/ethnicity, % (SD) | Student race/ethnicity, % (SD) |\n| White non-Hispanic | 80.6 (.6) | 77.6 (.6) |\n| Black non-Hispanic | 9.2 (.3) | 9.2 (.3) |\n| Hispanic | 13.0 (.3) | 14.4 (.4) |\n| Asian | 2.7 (.7) | 3.3 (.2) |\n| **Flavored milk nutrition profile per 8-oz container** | **Flavored milk nutrition profile per 8-oz container** | **Flavored milk nutrition profile per 8-oz container** |\n| kcal, range, n | 150–170 | 110–130 |\n| Fat, % | 0–1 | 0 |\n| Total sugars, range, g | 22–27 | 18–22 |\n| Added sugars, range, tsp | 2.5–3.75 | 1.5–2.5 |\n\n【16】Abbreviations: USDA, US Department of Agriculture; SD, standard deviation.  \na  USDA school regulations required flavored milk to be fat-free as of 2012–2013 school year.  \nb  Study was conducted in 7 schools in Northeast (Massachusetts, New York, Rhode Island, Vermont) and 3 schools in the South (Georgia, Texas).\n\n【17】#####  Table 2. Consumption of Flavored Milk by Students in Grades 3–5 With Lunch Before (Spring 2010) a  and After (Spring 2013) b  Updated USDA School Meal Regulations c, d  \n\n| Measure | Spring 2010 (n = 885 Milk Containers) | Spring 2013 (n = 833 Milk Containers) | _P_ Value |\n| --- | --- | --- | --- |\n| **Flavored milk consumption, overall, unadjusted mean (SD), oz** | 5.6 (.8) | 5.4 (.9) | — e |\n| **Children consuming >7 oz of 8-oz container of flavored milk, overall, mean f , %** | 52.2 | 49.7 | .35 |\n| **Children consuming >7 oz of 8-oz container of flavored milk, by region, unadjusted mean, %** | **Children consuming >7 oz of 8-oz container of flavored milk, by region, unadjusted mean, %** | **Children consuming >7 oz of 8-oz container of flavored milk, by region, unadjusted mean, %** | **Children consuming >7 oz of 8-oz container of flavored milk, by region, unadjusted mean, %** |\n| Northeastern schools | 67.0 | 67.0 | .12 |\n| Southern schools | 44.4 | 37.8 | .12 |\n| **Children consuming >7 oz flavored milk, by sex, unadjusted mean, %** | **Children consuming >7 oz flavored milk, by sex, unadjusted mean, %** | **Children consuming >7 oz flavored milk, by sex, unadjusted mean, %** | **Children consuming >7 oz flavored milk, by sex, unadjusted mean, %** |\n| Boys | 63.5 | 58.1 | .003 |\n| Girls | 53.6 | 54.8 | .003 |\n\n【19】Abbreviation: USDA, US Department of Agriculture; SD, standard deviation.  \na  Milk profile in 2010: 150–170 kcal per 8-oz container, 0%–1% fat, 22–27 g total sugars, 2.5–3.75 tsp added sugars.  \nb  Milk profile in 2013: 110–130 kcal per 8-oz container, fat-free, 18–22 g total sugars, 1.5–2.5 tsp added sugars.  \nc  USDA school regulations required flavored milk to be fat-free as of 2012–2013 school year.  \nd  Study was conducted in 7 schools in Northeast (Massachusetts, New York, Rhode Island, Vermont) and 3 schools in the South (Georgia, Texas).  \ne  Could not be calculated because data were skewed.  \nf  Means are model-based least squared means adjusted for region, grade, and sex.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ba147c81-2603-4f10-8f84-8778d85b441d", "title": "Correction: Vol. 25, No. 2", "text": "【0】Correction: Vol. 25, No. 2\nThe number of cases of West Nile neuroinvasive disease was listed incorrectly in the abstract of Acute and Delayed Deaths after West Nile Virus Infection, Texas, USA, 2002–2012 (D.C.E. Philpott et al.). The article has been corrected online .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "67ea5fab-cc36-48a4-8b51-983c15401ec2", "title": "Diagnostic Challenges of Central Nervous System Tuberculosis", "text": "【0】Diagnostic Challenges of Central Nervous System Tuberculosis\nTuberculosis (TB) of the central nervous system (CNS) is classically described as meningitis. However, altered mental status, including encephalitis, is within the spectrum of clinical manifestations. Because early treatment can dramatically improve outcomes, consideration of TB as a potential pathogen in CNS infections, including encephalitis, is vital. The California Encephalitis Project (CEP), initiated in 1998 to study the causative agents, epidemiology, and clinical features of encephalitis, has identified 20 cases of culture-confirmed tuberculous encephalitis. In most instances, TB was not initially considered to be a likely cause.\n\n【1】### The Study\n\n【2】Referrals are received by the CEP statewide from clinicians seeking diagnostic testing for immunocompetent patients, including TB PCR testing when appropriate, who meet the CEP case definition of encephalitis . Mycobacterial testing was often also conducted by the referring hospital. Inclusion criteria for this report were a positive cerebrospinal fluid (CSF) culture for _Mycobacterium tuberculosis_ complex or a positive CSF TB PCR result. Clinical data were compiled from case history forms and other medical records when available. To evaluate differences among causes of encephalitis, TB patients were compared with CEP patients with cases of enterovirus and herpes simplex virus 1 (HSV-1) encephalitis. Demographic, clinical, and laboratory data were compared by using the Fisher exact test, χ 2  test, or Kruskal-Wallis test as appropriate (statistical significance was set at α = 0.05).\n\n【3】From June 1998 through October 2005, a total of 1,587 patients were enrolled in the CEP; 20 patients fulfilled criteria as TB cases. Demographic and clinical information for the study population are detailed in the Appendix Table . Median age was 41 years (range 8 months to 77 years). The median time from symptom onset to first lumbar puncture was 5 days (range 0–62 days). Seventeen patients (%) had a second lumber puncture.\n\n【4】In general, CSF values became more abnormal over time, with increasing leukocyte counts and protein levels and decreasing glucose levels . Most patients had a CSF mononuclear cell predominance, although 4 patients (%) had a neutrophil predominance. All patients had cranial neuroimaging, magnetic resonance imaging (of 20), and computed tomography (of 20) . Results of computed tomography scans were often normal (%).\n\n【5】Of patients in whom the results of a recent tuberculin skin test (TST) were known, 59% (of 17) had a negative result . Many chest radiographs (of 18, 50%) showed no abnormalities. Concurrent culture positive pulmonary disease was found in 4 (%) of 8 patients tested. A history of foreign birth (%) or foreign travel (%) was common. When these factors were reported, 5 patients (%) had a history of treatment for TB and 5 patients (%) had contact with a known case of TB. Only 2 patients did not have at least 1 of these risk factors.\n\n【6】Of the 20 cases identified, all had a positive CSF culture for _M_ . _tuberculosis_ complex. Only 4 (%) of 17 were CSF TB PCR positive and none had a positive CSF acid-fast bacilli smear . All but 3 patients had pan-susceptible _M_ . _tuberculosis_ isolates; 2 patients had _M_ . _bovis_ isolates (resistant to pyrazinamide) and 1 patient had an isoniazid-resistant isolate.\n\n【7】When patients with TB were compared with patients with viral causes of encephalitis , those with enterovirus encephalitis were significantly younger, were less likely to require intensive care, had shorter hospitalizations, had fewer abnormal results for CSF and neuroimaging, and were less likely to die (all p<0.05). Patients with HSV-1 encephalitis were more likely than those with CNS TB to be white and non-Hispanic and to have shorter hospital stays, lower CSF leukocyte and protein levels, and higher CSF glucose levels.\n\n【8】### Conclusions\n\n【9】Although tuberculous meningitis is well described, prominent encephalitic features are less commonly reported. Illness and death associated with neurotuberculosis are highly dependent on the stage of disease at diagnosis; early diagnosis and treatment correlates with better outcomes . Although the TB cases reported here represent only a small percentage of CEP cases (<1%), CNS TB with an encephalitic picture warrants further discussion because of high morbidity and mortality rates and need for early diagnosis and appropriate treatment.\n\n【10】This study found atypical features of CNS disease that may have confounded early diagnosis. Tuberculous CNS disease is typically described as a chronic meningitis with insidious onset in children <5 years of age or in older adults with relatively few cases during school age years or adolescence . In contrast, CEP TB patients came to a hospital within 2 weeks of symptom onset and the greatest percentage of CEP TB patients was found in persons 10–19 years of age (%).\n\n【11】Although typical CSF studies (mononuclear cell pleocytosis, low glucose levels, and elevated protein levels)  were often found in CEP CNS TB patients, atypical findings were noted. CSF glucose levels were often normal in patients with diabetes, although the ratio of CSF to serum glucose was invariably low. Additionally, a CSF neutrophil predominance was found in 4 patients, erroneously suggesting pyogenic meningitis. Although clinicians may be tempted to ascribe abnormal CSF values to viral meningitis or encephalitis based on abnormal CSF values, CEP patients with enterovirus and HSV-1 encephalitis rarely had glucose levels <40 mg/dL. Median protein levels were significantly higher in patients with CNS tuberculosis (mg/dL in TB) than in patients infected with HSV-1 (mg/dL) or enterovirus (mg/dL) (p<0.001).\n\n【12】Diagnostically, the low sensitivity of CSF TB PCR is problematic. Potential explanations for the lack of sensitivity in CSF specimens include low bacillary load in CSF, small sample volumes, and PCR inhibitors in the sample . Given that all of our patients had positive CSF cultures, we would have expected a higher PCR yield. Most concerning was the finding that many providers caring for these patients were dissuaded from pursuing TB as a diagnostic possibility when the PCR result was negative.\n\n【13】Given the difficulties in obtaining a rapid diagnosis, therapy must often be initiated empirically. Unfortunately, a history of TB and TST or chest radiograph results were not reliable indicators of active disease and might be difficult to obtain. Further complicating therapy, lengthy cultures, and isolate sensitivities are necessary to optimize choices of antimicrobial drugs. Ten percent  of our patients were infected with _M_ . _bovis_ , a relatively higher percentage than in other reports . Intrinsic pyrazinamide resistance  of _M_ . _bovis_ required modification of use of empiric antimicrobial drugs.\n\n【14】A limitation of this series is inclusion of only patients with a positive CSF culture. Because historical data suggest that only 25%–70% of patients with a diagnosis of CNS TB have a diagnosis confirmed by microbiologic testing , there were likely additional CEP patients with CNS tuberculous disease without a positive acid-fast bacilli culture who were not included in this series. Additionally, the series was limited by the referral bias inherent in the project; CEP patients are typically sicker and present greater diagnostic challenges. Thus, those with obvious or mild CNS tuberculous disease would be underrepresented. Despite this potential bias, outcomes were similar (mortality rate 30%) compared with reported mortality rates (%–72%) and morbidity rates (%–48%) in previous studies .\n\n【15】This report emphasizes some atypical features of CNS TB manifested as encephalitis. Encephalopathic changes, a relatively rapid course, nonclassic age distribution, and negative TB PCR and TST results should not dissuade a clinician from considering TB, particularly when CSF:serum glucose ratio  is <0.5 and CSF protein level is >100 mg/dL. The CEP TB patients reported here may represent a severe part of the continuum of TB meningitis or may represent a distinct encephalitic subset with atypical features.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ac521a9d-fe48-4dbd-a1ae-68b0c20b2010", "title": "Rural–Urban Differences in Baseline Dietary Intake and Physical Activity Levels of Adolescents", "text": "【0】Rural–Urban Differences in Baseline Dietary Intake and Physical Activity Levels of Adolescents\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Differences in dietary intake and physical activity may explain the higher prevalence of obesity among adolescents living in rural versus urban settings. The objective of this cross-sectional secondary analysis was to compare baseline dietary intake and physical activity of adolescents by rurality.\n\n【3】**Methods**\n\n【4】We analyzed data on 940 adolescents who participated in ACTION PAC (Adolescents Committed to Improvement of Nutrition and Physical Activity), an obesity prevention and management intervention trial conducted from 2014 through 2017 in 8 public high schools in the southwestern United States. Dietary intake was assessed with the Block Food Screener, and participants completed an exercise log and wore an accelerometer to provide data on physical activity. We compared data by rural–urban commuting area (RUCA) codes and log population density by using multilevel models, with students nested within zip code and repeated measures for accelerometer analysis.\n\n【5】**Results**\n\n【6】After adjusting for socioeconomic status and ethnicity, accelerometer data indicated that moderate-to-vigorous physical activity was 8.17 min/d (P_ \\= .02) higher and sedentary time was 20.42 min/d (P_ \\= .02) lower in moderately urban areas than in the urban reference area. Each 1-unit increase in log population density was associated with higher reported intake of whole grains (.02 ounce equivalents, _P_ \\= .03), potatoes (.01 cup equivalents, _P_ \\= .02), and added sugar (.37 tsp, _P_ \\= .02) after adjusting for socioeconomic status and ethnicity.\n\n【7】**Conclusion**\n\n【8】Differences in reported dietary intake and physical activity level by measures of rurality were small and inconsistent in direction to explain the disparities observed in rural versus urban areas.\n\n【9】Introduction\n------------\n\n【10】One in 5 US adolescents are obese, and nationally representative data indicate that adolescent obesity prevalence is increasing . Overweight and obese adolescents are at risk for continued obesity and for heart disease, diabetes, cancer, and osteoarthritis as adults, and greater weight gain in early adulthood is associated with greater risk .\n\n【11】Adolescent obesity is a complex issue. Identifying behavioral, social, and environmental causes is imperative for designing effective obesity prevention and treatment strategies. Rural residency, an environmental factor, is associated with increased prevalence of childhood obesity . A recent meta-analysis of 10 studies examining urban and rural differences in childhood obesity in the United States found that in a pooled population of more than 74,000 children aged 2 to 19 years, children in rural areas had a 26% greater risk of obesity compared with urban children . The 2 studies that reported on adolescents aged 10 to 17 years found a similar difference, with adolescents in nonmetropolitan areas having a 28% greater odds of obesity compared with adolescents in metropolitan areas . No studies of children or adolescents have examined which environmental factors in rural areas contribute to obesity disparities.\n\n【12】Differences in dietary intake and physical activity levels could potentially explain the higher prevalence of obesity in rural versus urban populations. However, a narrative review of 17 studies examining rural–urban differences in the nutrition and physical activity behaviors of children and adolescents noted inconsistent findings, with few studies examining dietary intake and measuring physical activity using accelerometers . In addition, there was substantial variation in the way rurality was defined across studies .\n\n【13】The aim of this study was to compare the baseline dietary intake and physical activity levels of 9th- and 10th-grade public school students in the Southwest who enrolled in the ACTION PAC (Adolescents Committed to Improvement of Nutrition and Physical Activity) intervention trial, by measures of rurality.\n\n【14】Methods\n-------\n\n【15】### Study population\n\n【16】We conducted a cross-sectional secondary analysis of baseline data collected from a subset of participants of ACTION PAC, a cluster-randomized, longitudinal trial of an adolescent obesity prevention and management intervention in school-based health centers . Participants with complete data for the variables of interest were included in the analysis.\n\n【17】Adolescents from 8 public high schools in the Southwest were recruited to participate in the ACTION PAC trial. All participating high schools had functioning school-based health centers and similar food and physical activity environments. All high schools had more than 700 students, of whom more than 40% identified as Hispanic. Participant inclusion criteria were being enrolled in 9th or 10th grade at a participating school and having written informed adolescent assent and parental consent to participate in the longitudinal study. Exclusion criteria were 1) having blood pressure in the range of stage 2 hypertension; 2) having diagnosed diabetes; 3) using corticosteroids, antipsychotics, or medications for the treatment of diabetes, hypertension, or hyperlipidemia; 4) being unable to perform moderate-to-vigorous physical activity (MVPA) or not ambulatory; 5) having a score of 20 or more on the Eating Attitudes Test ; 6) having developmental disorders that affect weight or ability to understand the study procedures or counseling; and 7) being pregnant. The study protocol was approved by the University of New Mexico Health Sciences Center Human Research Protections Office.\n\n【18】### Data collection\n\n【19】Data were collected at 2 baseline study visits that occurred 1 week apart. Height was measured by using a portable stadiometer (±0.1 cm; Seca Model 213), and weight was measured with a portable electronic scale (±0.1 kg; Seca Model 770). Weight status was determined according to body mass index percentile .\n\n【20】Adolescents reported their intake of foods during the past week via the Block Food Screener for Ages 2–17 (version, NutritionQuest). The Block Food Screener estimates average daily intake of fruit and fruit juice (cup equivalents \\[CEs\\]); vegetables excluding potatoes and legumes (CEs); whole grains (ounce equivalents \\[OEs\\]); legumes (CEs); dairy (CEs); meat, poultry, and fish (OEs); potatoes (CEs); saturated fat (grams); and added sugar (tsp) . Reported dietary intake was compared with the 2015–2020 Dietary Guidelines for Americans Recommended Intakes for age (years) and sex .\n\n【21】Physical activity was measured by using the GENEActiv triaxial accelerometer (Activinsights Ltd) for 7 days and the 3-Day Physical Activity Report (D PAR) . Both tools have been validated in children or adolescents, and participants received instructions before use. The accelerometer records movement in acceleration values by using units of gravity (mG, where 1 mG = 0.00981 m/s 2  ). An R package (GGIR version 1.5–18)  was used to reduce accelerometer data to minutes of sedentary and MVPA per day during the hours of 5:00 AM to 11:00 PM. Activity that met the valid wear score generated in GGIR was classified as being sedentary when acceleration was less than 50 mG on average for 60 seconds and as being MVPA when acceleration was above 150 mG. Activity thresholds were chosen on the basis of validation research for the GENEActiv accelerometer . The 3D PAR was completed for 3 days of the same week, including 1 weekend day and 2 weekdays. Adolescents selected items from 74 predetermined activities or wrote in other activities for every 30-minute block of time between 5:00 AM and midnight (total blocks). They also recorded the intensity of the selected activity (light, medium, hard, or very hard) for each block. The blocks were scored by using a standard scoring system  that produced total blocks per day spent in MVPA or sedentary activity and total daily metabolic equivalents.\n\n【22】Demographic data, including participant zip code, parental education level, and annual household income, were collected from a health history form completed by the adolescent and parent.\n\n【23】### Measures of rurality\n\n【24】There is no universally recognized classification system or definition of rurality. Commonly used delineations include the Rural–Urban Continuum Codes (RUCCs) , Rural–Urban Commuting Area (RUCA) codes , and Frontier and Remote Area (FAR) codes . This study used both zip code–level RUCA approximation codes developed by the University of Washington Rural Health Research Center  and 2010 US Census population density (number of people per square mile) data  as measures of rurality.\n\n【25】RUCA codes 1 through 3 are considered metropolitan (urban), codes 4 through 6 are micropolitan, codes 7 through 9 are small town, and code 10 is rural . The codes are based on population density, urbanization, and the size and direction of primary daily commuter flow between areas. They are further subdivided on the basis of secondary daily commuter flow size and direction. There is no standard definition or cutoff for population density.\n\n【26】### Data analysis\n\n【27】R version 3.4.3 was used for analysis . Relationships with RUCA code and population density were tabulated separately, because each captured substantially unique variance. RUCA codes explain about 23% of the variation in log population density. Four RUCA codes were present in the data: 1.0 (metropolitan area core: primary commuting flow within an urbanized area), 2.0 (metropolitan area high commuting: primary flow of ≥30% to an urbanized area), 2.1 (metropolitan area high commuting: secondary flow of 30%–50% to a larger urbanized area), and 10.0 (rural areas: primary commuting flow to a tract outside an urbanized area or urban cluster). Because so few participants were represented in RUCA code 10.0 (n = 3), they were dropped from analysis. The cutoff for rural versus urban population density was set at fewer than 1,000 people per square mile for descriptive analyses ; the continuous variable log population density was used in all models.\n\n【28】Because measures of rurality were determined at the zip code level, there were multiple respondents per zip code, and multilevel models in which participants were nested within zip code were used for all analyses. For the accelerometer data, 3-level repeated measures models were used (ie, day nested within participant nested within zip code). To account for nonwear of accelerometers, each day was weighted by the proportion of nonwear, and all available data were included in the analysis. To examine the overall relationship between RUCA code and reported dietary intake and physical activity, we conducted 2-degrees-of-freedom log-likelihood (LL) ratio tests, which examine whether the RUCA codes were associated together with each outcome. Two dummy variables represented RUCA codes at the 3 included levels (.0, 2.0, 2.1), with 1.0 as the reference category. If we found an overall effect of RUCA code, differences in means between the reference category and RUCA codes 2.0 and 2.1 were interpreted. We controlled for socioeconomic status as measured by annual family income (reference group, ≥$20,000 per year) and parental education level (reference group, <high school graduation). A difference in LL ratio test was used to test whether RUCA code contributed to variation in reported dietary intake and physical activity beyond the socioeconomic variables. LL tests for fixed effects were conducted by using maximum likelihood estimation.\n\n【29】Each of the dietary and physical activity outcomes was separately regressed on log population density, alone and then including the socioeconomic variables. In these analyses the hypotheses were assessed directly by the regression parameter for log density and no LL ratio test was needed. Intra-class correlation coefficients were tabulated to describe the extent to which participants’ reported dietary intake and physical activity differed within their zip codes or across zip codes. We did not tabulate results on participant weight status, because the design and recruitment methods of the longitudinal trial influenced the prevalence of overweight and obesity; approximately 40% of participants were overweight or obese at baseline by design.\n\n【30】Results\n-------\n\n【31】### Participant characteristics\n\n【32】Most participants (%) lived in RUCA code 1.0, identified as Hispanic (%), and were female (%) . The proportion of Hispanic participants increased with less urban RUCA codes, and the proportion of female participants decreased with less urban RUCA codes. Participants from RUCA code 2.0 (moderately urban) had the highest proportion of annual family income less than $20,000 (%) and the lowest proportion of parents who were college graduates (%). Differences using the population density cutoff of 1,000 people per square mile were inconsistent with RUCA code observations; less densely populated areas (<1,000 people/mile 2  ) had a higher proportion of female participants, a lower proportion of families with income less than $20,000 per year, and a higher proportion of parents who were college graduates.\n\n【33】Overall reported diet quality was poor; 73% to 99% of adolescents reported that they consumed less than the 2015–2020 Dietary Guidelines for Americans sex- and age-specific recommended intake of fruits, vegetables, whole grains, dairy, and legumes. Less than 1% of participants met the recommended intake of legumes, and more participants met the recommendations for fruit than for all other food groups.\n\n【34】### Differences in dietary intake and physical activity level by RUCA code\n\n【35】For most dietary intake variables and for blocks of MVPA and sedentary time on the 3D PAR, we found no significant relationship with RUCA code overall . We found a significant overall relationship with RUCA code for whole grains (P_ \\= .02) and minutes per day of MVPA (P_ \\= .02) and of sedentary time (P_ \\= .02) as measured by accelerometer. The overall relationship with whole grains did not persist after controlling for family income, parent education level, and ethnicity. As measured by accelerometer, MVPA and sedentary time were 8.71 min/d (P_ \\= .02) higher and 20.42 min/d (P_ \\= .02) lower in RUCA code 2.0 than in RUCA code 1.0 after controlling for socioeconomic status and ethnicity. Minutes per day of MVPA and sedentary time for RUCA 2.1 fell in between and were not significantly different from the minutes per day for the other RUCA codes.\n\n【36】### Differences in dietary intake and physical activity level by log population density\n\n【37】Each 1-unit increase in log population density was associated with increases in reported intake of whole grains (.02 OE, _P_ \\= .03), potatoes (.01 CE, _P_ \\= .02), and added sugar (.37 tsp, _P_ \\= .02), after adjustment for socioeconomic status and ethnicity . We found no significant relationship between other dietary intake variables or any of the physical activity variables and log population density. Overall, intraclass correlation coefficients indicated that more than 98% of the variation in the sample with respect to reported dietary intake and physical activity occurred within zip codes as opposed to between zip codes.\n\n【38】Discussion\n----------\n\n【39】Overall, differences in reported dietary intake and physical activity level by RUCA code and log population density were small and not entirely consistent with the hypothesis that differences in dietary intake and physical activity have a prominent role in explaining observed rural versus urban obesity disparities in adolescents. The observed differences mostly persisted after controlling for socioeconomic status and ethnicity, indicating that perhaps other community level access factors were driving the differences. For example, the observed relationships between dietary intake and population density could reflect increased access to grocery stores (whole grains) and fast food restaurants (potatoes, added sugar) in more densely populated areas.\n\n【40】Our finding of a few significant differences in dietary intake by measures of rurality is consistent with a recent narrative review . We found little consistency in observed differences in food groups or nutrients by measures of rurality across 5 studies assessing dietary intake in US children or adolescents by measures of rurality . In the 3 studies that included adolescents, one found no significant differences in dietary intake between urban and rural adolescents . The second study noted a slightly smaller percentage of rural adolescents (.2%) than urban adolescents (.5%) who reported consuming 2 or more cups of fruit per day , and the third study found that nonmetropolitan and metropolitan black youth consumed fatty snack foods more often than did white metropolitan youth .\n\n【41】Our observation that physical activity level was higher with decreasing urbanization is also consistent with a recent narrative review, which found that urban youth were less active than rural youth in 9 of 16 studies examining physical activity levels in US children or adolescents by measures of rurality . The only 2 studies to use accelerometers, both conducted by Moore et al, had inconsistent findings, noting that MVPA was higher among urban middle school students than among rural middle school students in the southeastern United States  and that MVPA was higher in rural 4th- through 8th-grade girls compared with suburban and urban girls, but not boys, in North Carolina . Observed inconsistencies may reflect regional differences in physical activity infrastructure and employment and recreational opportunities for physical activity by measures of rurality or differences in how rurality is defined. In an examination of determinants of physical activity, both rural and urban families expressed the following as barriers: physical distance to activity areas, cost, electronic media, safety concerns, and the need for parental supervision . Some of the observed differences in our study could be related to access to electronic media and the internet. Internet connectivity is less reliable and less available in more rural settings and, conversely, ubiquitous in urban settings. This fact is supported by our finding that sedentary time was highest in youth from the most urban areas. We found inconsistent results for physical activity using RUCA codes versus log population density in our analysis.\n\n【42】Our study has several strengths, including a large sample size, the reporting of both dietary intake and physical activity data, and the objective measure of physical activity through the use of accelerometers. Our study also has limitations. First, the development patterns of the western US make it difficult to differentiate between subtle variations in rurality that may affect access to health care and nutrition services and the food and physical activity environment. Research staff members who observed the actual settings were surprised to find little variation in RUCA codes among participants in the study. Almost all of the communities involved were considered metropolitan/urban based on RUCA code, but the most urban community had many grocery stores and fast-food restaurants with easy access to the schools, compared with another community that had only 1 restaurant. Although zip code–level RUCA codes and population density were both used in this study, there are additional measures of rurality, and ours may not reflect differences within the zip code–level RUCA codes. For example, some of the communities involved in the study are health professional shortage areas, while others are not. Findings for RUCA code 2.1 should be interpreted cautiously, because this group included fewer than 50 participants. Dietary intake was assessed by using a food frequency screener, which allowed us to report on a limited set of variables. Use of a full food frequency questionnaire could have provided a more comprehensive, and potentially accurate, picture of dietary intake. Both the food frequency screener and the physical activity record have limitations related to social desirability bias, although we would not expect the magnitude of the bias to vary by measures of rurality. Finally, participants in this study were public school students who chose to participate in a longitudinal obesity prevention and management intervention trial, limiting the generalizability of the results.\n\n【43】Our findings have public health implications. The reported baseline dietary intake of all adolescents in the study was inadequate compared with the Dietary Guidelines for Americans, indicating an ongoing need for nutrition education and for policies that support access to nutritious foods in schools and communities. Given the inconsistent findings related to differences in dietary intake and physical activity in children and adolescents by measures of rurality, additional research is needed to understand the underlying causes of rural–urban obesity disparities. Future research should be conducted with representative rural and urban populations and include standardized, appropriate measures of rurality; comprehensive measures of dietary intake; objective measures of physical activity, such as accelerometers; and assessment of additional environmental and health system factors that could be causing obesity disparities among adolescents.\n\n【44】Tables\n------\n\n【45】Table 1. Characteristics of Participants, by Measures of Rurality a  , Study of Rural–Urban Differences in Baseline Dietary Intake and Physical Activity Levels Among Adolescents, ACTION PAC Cluster-Randomized Trial\n\n| Characteristic | Total (N = 940) | Zip Code–Level RUCA Code b | Population Density |\n| --- | --- | --- | --- |\n| 1.0 (n = 749) | 2.0 (n = 144) | 2.1 (n = 47) | ≥1,000 People/mile 2 (n = 396) | <1,000 People/mile 2 (n = 510) |\n| --- | --- | --- | --- | --- |\n| **Age, mean (SD), y** | 15.3 (.7) | 15.4 (.7) | 15.1 (.7) | 15.2 (.7) | 15.5 (.7) | 15.2 (.7) |\n| **Female sex** | 519  | 419  | 75  | 25  | 210  | 291  |\n| **Hispanic ethnicity** | 810  | 628  | 136  | 46  | 339  | 439  |\n| **Race** **c** **,** | **Race** **c** **,** | **Race** **c** **,** | **Race** **c** **,** | **Race** **c** **,** | **Race** **c** **,** | **Race** **c** **,** |\n| White | 119  | 111  | 7  | 1  | 45  | 70  |\n| Black | 34  | 34  | 0 | 0 | 16  | 18  |\n| American Indian | 26  | 25  | 1  | 0 | 18  | 7  |\n| Asian | 6  | 5  | 1  | 0 | 5  | 1  |\n| Pacific Islander | 1  | 1  | 0 | 0 | 1  | 0 |\n| Multiple | 24  | 22  | 1  | 1  | 15  | 9  |\n| **Annual household income** **d** **, $** | **Annual household income** **d** **, $** | **Annual household income** **d** **, $** | **Annual household income** **d** **, $** | **Annual household income** **d** **, $** | **Annual household income** **d** **, $** | **Annual household income** **d** **, $** |\n| <20,000 | 392  | 300  | 78  | 14  | 182  | 196  |\n| ≥20,000 | 548  | 449  | 66  | 33  | 214  | 314  |\n| **Parent/guardian education level** **e** | **Parent/guardian education level** **e** | **Parent/guardian education level** **e** | **Parent/guardian education level** **e** | **Parent/guardian education level** **e** | **Parent/guardian education level** **e** | **Parent/guardian education level** **e** |\n| Less than high school graduate | 291  | 219  | 53  | 19  | 135  | 150  |\n| High school graduate or some college | 506  | 403  | 79  | 24  | 214  | 271  |\n| College graduate | 143  | 127  | 12  | 4  | 47  | 89  |\n\n【47】Abbreviations: ACTION PAC, Adolescents Committed to Improvement of Nutrition and Physical Activity; RUCA, rural–urban commuting area; SD, standard deviation.  \na  Values are no. (%) unless otherwise indicated.  \nb  1.0 = Metropolitan area core: primary commuting flow within an urbanized area; 2.0 = Metropolitan area high commuting: primary flow 30% or more to an urbanized area; 2.1 = Metropolitan area high commuting: secondary flow 30% to 50% to a larger urbanized area.  \nc  Seventy-eight percent of participants selected only an ethnicity.  \nd  Four percent of participants had missing data for family income.  \ne  One percent of participants had missing data for parent/guardian education level.\n\n【48】Table 2. Differences in Reported Dietary Intake and Physical Activity a  by Zip Code–Level Rural–Urban Commuting Area (RUCA) Codes, Study of Rural–Urban Differences in Baseline Dietary Intake and Physical Activity Levels Among Adolescents, ACTION PAC Cluster-Randomized Trial\n\n| Variable | Unadjusted Results | Adjusted Results |\n| --- | --- | --- |\n| Intercept (SE) | Zip Code–Level RUCA Code b | LL Test c (P_ Value) | Intercept (SE) | Zip Code–Level RUCA Code b | Annual Household Income <$20,000 (SE) | Parent Education | Hispanic (SE) | LL Test c (P_ Value) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 2.0 (SE) | 2.1 (SE) | 2.0 (SE) | 2.1 (SE) | <High School (SE) | High School Graduate or Some College (SE) |\n| --- | --- | --- | --- | --- | --- |\n| **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** | **Dietary intake as estimated by Block Food Screener (N = 940)** |\n| Fruit/fruit juice, CE | 1.44 (.04) | −0.21 (.10) | −0.02 (.16) | 4.49 (.11) | 1.49 (.12) | −0.21 (.10) | −0.02 (.16) | 0.07 (.09) | −0.02 (.13) | −0.04 (.11) | −0.09 (.05) | 4.34 (.11) |\n| Vegetables d , CE | 0.72 (.02) | −0.07 (.05) | 0.08 (.08) | 3.40 (.18) | 0.85 (.06) | −0.05 (.05) | 0.10 (.08) | −0.03 (.05) | −0.06 (.06) | −0.03 (.06) | −0.09 (.05) | 3.17 (.21) |\n| Legumes, CE | 0.14 (.01) | 0.02 (.02) | 0.05 (.03) | 2.69 (.26) | 0.11 (.02) | 0.02 (.02) | 0.04 (.03) | 0.01 (.02) | 0.02 (.02) | 0.02 (.02) | 0.04 (.02) | 1.98 (.37) |\n| Whole grains, OE | 0.52 (.02) | −0.11 (.04) | −0.09 (.07) | 7.47 (.02) | 0.61 (.05) | −0.10 (.04) | −0.07 (.07) | −0.05 (.04) | −0.05 (.05) | −0.04 (.05) | −0.02 (.05) | 5.33 (.07) |\n| Meat/poultry/fish, OE | 2.73 (.12) | −0.07 (.32) | 0.55 (.43) | 1.67 (.44) | 3.01 (.29) | −0.02 (.34) | 0.61 (.45) | −0.02 (.22) | −0.21 (.31) | 0.04 (.28) | −0.28 (.26) | 1.87 (.39) |\n| Dairy, CE | 1.32 (.04) | 0.06 (.09) | 0.13 (.15) | 1.23 (.54) | 1.42 (.10) | 0.08 (.09) | 0.15 (.15) | −0.08 (.08) | −0.03 (.12) | −0.02 (.10) | −0.03 (.10) | 1.62 (.45) |\n| Potato, CE | 0.31 (.01) | −0.06 (.03) | −0.04 (.04) | 4.37 (.11) | 0.33 (.03) | −0.05 (.03) | −0.03 (.04) | −0.02 (.02) | 0.08 (.03) | 0.07 (.03) | −0.08 (.03) | 3.55 (.17) |\n| Saturated fat, g | 18.31 (.49) | −0.65 (.24) | 2.24 (.92) | 1.76 (.41) | 19.42 (.34) | −0.47 (.32) | 2.44 (.96) | −0.75 (.04) | 0.14 (.48) | 0.32 (.31) | −1.01 (.24) | 1.79 (.41) |\n| Added sugar, tsp | 8.36 (.26) | −1.04 (.64) | 1.03 (.06) | 3.71 (.16) | 7.45 (.75) | −1.17 (.65) | 0.88 (.06) | 0.04 (.58) | 1.45 (.83) | 0.79 (.74) | 0.03 (.70) | 4.34 (.11) |\n| **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** | **3-Day physical activity record (N = 795)** |\n| Total activity, MET | 75.24 (.68) | 2.31 (.66) | 6.00 (.85) | 5.30 (.07) | 75.72 (.49) | 2.37 (.87) | 5.84 (.94) | 6.25 (.87) | −0.62 (.20) | 0.26 (.96) | 6.25 (.87) | 5.50 (.06) |\n| MVPA, 30-min block | 5.56 (.13) | 0.26 (.33) | 0.41 (.56) | 1.03 (.60) | 5.59 (.38) | 0.26 (.33) | 0.34 (.60) | 0.44 (.57) | 0.11 (.43) | 0.18 (.39) | 0.44 (.57) | 1.10 (.58) |\n| Sedentary, 30-min block | 29.41 (.17) | −0.08 (.43) | −0.25 (.64) | 0.62 (.92) | 29.68 (.42) | −.004 (.49) | −0.04 (0.64 | −0.24 (.63) | −0.16 (.48) | −0.23 (.42) | −0.24 (.63) | .15 (.93) |\n| **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** | **Accelerometer data (N = 891)** |\n| MVPA, min/d | 52.61 (.14) | 8.10 (.85) | 3.42 (.92) | 7.59 (.02) | 53.71 (.29) | 8.17 (.87) | 3.78 (.93) | 4.65 (.61) | −5.31 (.71) | −1.94 (.29) | −2.10 (.09) | 7.71 (.02) |\n| Sedentary, min/d | 818.25 (.79) | −22.13 (.00) | −6.28 (.95) | 7.70 (.02) | 830.46 (.93) | −20.42 (.91) | −5.29 (.90) | −14.17 (.30) | 5.33 (.95) | −2.52 (.95) | −3.02 (.46) | 7.62 (.02) |\n\n【50】Abbreviations: ACTION PAC, Adolescents Committed to Improvement of Nutrition and Physical Activity; CE, cup equivalent; LL, log-likelihood ratio; MET, metabolic equivalent of task, MVPA, moderate to vigorous physical activity; OE, ounce equivalent; RUCA, rural–urban commuting area; SE, standard error.  \na  Dietary intake was measured by using the Block Food Screener for ages 2–17 (version, NutritionQuest) . Physical activity was measured by using the GENEActiv triaxial accelerometer (Activinsights Ltd) for 7 days and the 3-Day Physical Activity Report (D PAR) .  \nb  1.0 = Metropolitan area core: primary commuting flow within an urbanized area (reference group); 2.0 = metropolitan area high commuting: primary flow 30% or more to an urbanized area; 2.1 = metropolitan area high commuting: secondary flow 30% to 50% to a larger urbanized area.  \nc  Multilevel models in which participants were nested within zip code were used for all analyses. For the accelerometer data, 3-level repeated measures models with day nested within participant nested within zip code were used. To examine the overall relationship between RUCA code and reported dietary intake and physical activity, 2-degrees-of-freedom LL tests, which examine whether together the 3 RUCA codes predicted each outcome, were used.  \nd  Vegetables not including potatoes or legumes.\n\n【51】Table 3. Differences in Reported Dietary Intake and Physical Activity, a  by Log Population Density, Study of Rural-Urban Differences in Baseline Dietary Intake and Physical Activity Levels Among Adolescents, ACTION PAC Cluster-Randomized Trial\n\n| Variable | Unadjusted Results | Adjusted Results |\n| --- | --- | --- |\n| Intercept (SE) | Log Population Density (SE) | _P_ b | ICC | Intercept (SE) | Log Population Density (SE) | Family Income <$20K (SE) | Parent Education | Hispanic (SE) | _P_ b |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| <High School (SE) | High School Graduate or Some College(SE) |\n| --- | --- |\n| **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** | **Dietary intake as estimated by Block Food Screener (N = 906)** |\n| Fruit/fruit juice, CE | 1.19 (.14) | 0.03 (.02) | .13 | 0 | 1.28 (.18) | 0.03 (.02) | 0.02 (.09) | −0.01 (.13) | −0.03 (.12) | −0.10 (.11) | .15 |\n| Vegetables, c CE | 0.76 (.08) | −0.01 (.01) | .63 | 0.005 | 0.87 (.09) | −0.003 (.01) | −0.03 (.05) | −0.07 (.07) | −0.03 (.06) | −0.08 (.06) | .77 |\n| Legumes, CE | 0.17 (.03) | −0.01 (.01) | .33 | 0.019 | 0.12 (.04) | −0.01 (.01) | 0.01 (.02) | 0.02 (.02) | 0.02 (.02) | 0.04 (.02) | .23 |\n| Whole grains, OE | 0.38 (.06) | 0.02 (.01) | .07 | 0.002 | 0.47 (.08) | 0.02 (.01) | −0.06 (.04) | −0.06 (.06) | −0.05 (.05) | −0.03 (.05) | .03 |\n| Meat/ poultry/ fish, OE | 2.53 (.44) | 0.04 (.07) | .56 | 0.014 | 2.80 (.52) | 0.04 (.07) | 0.01 (.23) | .029 (.23) | −0.01 (.28) | −0.24 (.27) | .57 |\n| Dairy, CE | 1.40 (.13) | −0.01 (.02) | .62 | 0 | 1.46 (.16) | −0.01 (.02) | −0.05 (.08) | −0.05 (.12) | −0.04 (.11) | −0.01 (.10) | .73 |\n| Potato, CE | 0.21 (.04) | 0.02 (.01) | .02 | 0.002 | 0.25 (.05) | 0.01 (.01) | −0.03 (.02) | .07 (.03) | 0.07 (.03) | −0.08 (.03) | .02 |\n| Saturated fat, g | 16.95 (.78) | 0.23 (.28) | .42 | 0.003 | 17.98 (.21) | 0.24 (.29) | −0.57 (.07) | −0.25 (.05) | 0.04 (.35) | −0.82 (.26) | .41 |\n| Added sugar, tsp | 5.77 (.93) | 0.40 (.15) | .01 | 0 | 4.99 (.16) | 0.37 (.15) | −0.07 (.60) | 1.50 (.85) | 0.85 (.76) | 0.08 (.71) | .02 |\n| **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** | **3-Day physical activity record (N = 764)** |\n| Total activity, MET | 80.81 (.80) | −0.79 (.44) | .08 | 0.007 | 81.21 (.34) | −0.83 (.44) | 1.65 (.60) | −1.34 (.27) | −0.53 (.03) | −0.47 (.92) | .07 |\n| MVPA, 30-min block | 6.06 (.56) | −0.07 (.09) | .41 | 0.007 | 6.20 (.66) | −0.09 (.09) | 0.28 (.32) | −0.09 (.45) | 0.02 (.40) | −0.27 (.38) | .34 |\n| Sedentary, 30-min block | 29.06 (.64) | 0.05 (.10) | .60 | 0.010 | 29.15 (.72) | 0.07 (.10) | −0.46 (.35) | .04 (.49) | −0.08 (.44) | 0.14 (.41) | .45 |\n| **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** | **Accelerometer data (N = 860)** |\n| MVPA, min/d | 59.78 (.48) | −0.93 (.70) | .19 | — d | 59.72 (.28) | −1.07 (.69) | 4.99 (.71) | −3.62 (.83) | −0.33 (.41) | −1.58 (.16) | .13 |\n| Sedentary, min/d | 805.87 (.09) | 1.41 (.89) | .46 | — d | 817.68 (.33) | 1.91 (.77) | −13.85 (.54) | 1.56 (.23) | −6.42 (.21) | −2.63 (.61) | .29 |\n\n【53】Abbreviations: ACTION PAC, Adolescents Committed to Improvement of Nutrition and Physical Activity; CE, cup equivalents; ICC, intraclass correlation coefficient; MET, metabolic equivalent of task; MVPA, moderate to vigorous physical activity; OE, ounce equivalents; SE, standard error.  \na  Dietary intake was measured by using the Block Food Screener for ages 2–17 (version, NutritionQuest) . Physical activity was measured by using the GENEActiv triaxial accelerometer (Activinsights Ltd) for 7 days and the 3-Day Physical Activity Report (D PAR) .  \nb  Multilevel models in which participants were nested within zip code were used for all analyses. For the accelerometer data, 3-level repeated measures models with day nested within participant nested within zip code were used. In these analyses, the hypotheses were assessed directly by the regression parameter for log density.  \nc  Vegetables not including potatoes or legumes.  \nd  Because there are multiple ICCs for analyses with 3 levels, we reported the variance components instead. The variance for zip code is 3.44 for MVPA, 749.97 for individual, and 1332.47 for time within individual; for sedentary behavior, the variances are 63.75, 4110.56, and 9298.99, respectively.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "27cb2955-9ad7-46d3-b933-fff71030ca40", "title": "Macrolide Resistance of Mycoplasma pneumoniae, South Korea, 2000–2011", "text": "【0】Macrolide Resistance of Mycoplasma pneumoniae, South Korea, 2000–2011\n_Mycoplasma pneumoniae_ is 1 of the most common causes of community-acquired pneumonia in children and young adults . Epidemics of mycoplasma pneumonia typically occur every 4–7 years; however, epidemics have occurred every 3–4 years in South Korea . The first-line treatment for mycoplasma pneumonia is macrolide antimicrobial drugs, but macrolide-resistant infections have been recognized in conjunction with an increase in cases in children in Japan, China, Germany, France, Israel, and the United States . Because of the risk to children administered tetracycline and fluoroquinolone (nonmacrolide drugs), _M. pneumoniae_ resistance to macrolide drugs has critical implications for the treatment of mycoplasma pneumonia in children. This study was conducted to identify the prevalence of macrolide resistance among _M. pneumoniae_ strains isolated from children with lower respiratory tract infections (LRTIs) during 4 consecutive epidemics  in South Korea.\n\n【1】### The Study\n\n【2】A total of 2,089 respiratory samples were tested for the presence of _M. pneumoniae_ . Of these, a total of 378 were archived samples collected during epidemics in 2000 (samples), 2003 (samples), and 2006 (samples), and 1,711 were samples collected and tested during August 2010–December 2011. Specimens from the 2010–2011 epidemic were collected from children at Seoul National University Children’s Hospital, Seoul National University Bundang Hospital, and Seoul Eulji Hospital. All samples were obtained from children (median age 5 years, range 6 months–18 years) with a diagnosis of community-acquired LRTI.\n\n【3】P1 adhesin was amplified by PCR for the detection of _M. pneumoniae_ from the 378 archived samples. _M. pneumoniae_ was cultivated by using pleuropneumonia-like organism broth and agar for the 1,711 samples collected during 2010–2011. Media were incubated aerobically at 37°C for 6 weeks. Plates were observed daily to identify change in the color of the broth from red to transparent orange. When the color changed, the samples were subcultured on agar plates. Spherical _M. pneumoniae_ colonies were observed by using a microscope.\n\n【4】For the cultured _M. pneumoniae_ isolates, we amplified domain V of the 23S rRNA gene by PCR; for the archived samples, we extracted DNA. For PCR, we used primers MP23SV-F 5′-TAACTATAACGGTCCTAAGG-3′ and MP23SV-R 5′-ACACTTAGATGCTTTCAGCG-3′. The PCR products were sequenced to identify mutations. Sixty-four of the _M. pneumoniae_ –positive samples from 2000 and 2003 had been previously tested for mutations in the 23S rRNA gene.\n\n【5】Minimum inhibitory concentrations (MICs) were measured by using the microdilution method in triplicate for the following antimicrobial agents: erythromycin, clarithromycin, azithromycin, roxithromycin, josamycin, tetracycline, doxycycline, levofloxacin, moxifloxacin, and ciprofloxacin. MIC was defined as the lowest antimicrobial drug concentration at which the media color did not change at the time when the color of the positive control media (containing _M. pneumoniae_ strains only) changed .\n\n【6】_M. pneumoniae_ was detected in 255 (.2%) of 2,089 clinical samples; 132 (.8%) of the positive samples were among the 378 archived samples, and 123 (.2%) were among the 1,711 samples from 2010–2011. For the 132 archived samples, _M. pneumoniae_ was detected by PCR: 30 (.8%) were among the 71 samples from 2000, 34 (25.8% were among the 112 samples from 2003, and 68 (.5%) were among the 195 samples from 2006. For the 123 samples from 2010–2011, _M. pneumoniae_ was detected by culture.\n\n【7】Overall, 80 (.4%) of the 255 _M. pneumoniae_ –positive samples carried mutations in the 23S rRNA gene. Of these, 78 had the A2063G transition and 2 exhibited the A2064G transition. The prevalence of the 23S rRNA mutation increased significantly over the 4 consecutive epidemics, as follows: 2000 epidemic, 0 of 30 samples; 2003 epidemic, 1 (.9%) of 34 samples; 2006 epidemic, 10 (.7%) of 68 samples; and 2010–2011 epidemic, 25 (.2%) of 53 samples in 2010 and 44 (.9%) of 70 samples in 2011 (p<0.001 for trend) .\n\n【8】Among 123 samples culture-positive for _M. pneumoniae_ , 69 that carried the 23S rRNA mutation exhibited significantly higher MIC 50  (MIC for 50% of strains) and MIC 90  when tested with 5 macrolides, compared with 54 strains that lacked the mutation. For example, the MIC 50  and MIC 90  of erythromycin were 16 µg/mL and 128 µg/mL, respectively, for strains with the 23S rRNA mutation and 0.001 µg/mL and 0.002 µg/mL, respectively, for strains without the mutation (p<0.0001) . All 123 _M. pneumoniae_ strains were susceptible to nonmacrolide antimicrobial drugs, including tetracycline, doxycycline, levofloxacin, ciprofloxacin, and moxifloxacin .\n\n【9】Macrolide resistance is associated with point mutations in domain V of the _M. pneumoniae_ 23S rRNA gene, especially those corresponding to A2063G or A2064G transitions . Thus, emergence of macrolide-resistant strains may result in treatment failure of _M. pneumoniae_ infections .\n\n【10】We did not detect macrolide resistance among _M. pneumoniae_ strains collected during 2000; thereafter, the prevalence of macrolide resistance remained low through the 2003 epidemic. Macrolide resistance then increased to 14.7% during the epidemic of 2006 and to 56.1% during the epidemic of 2010–2011, as indicated by substantially higher MICs against macrolide agents in association with the presence of the 23S rRNA gene mutation in _M. pneumoniae_ isolates.\n\n【11】Macrolide resistance has been detected with increasing frequency in many parts of the world, highlighting the importance of knowing the geographic distribution and temporal patterns of macrolide-resistant _M. pneumoniae_ . After the first isolation of a macrolide-resistant strain in 2001, Japan reported a dramatic increase in macrolide resistance among children with mycoplasma pneumonia, and in 2011 resistance was >80% . China identified an 83%–92% prevalence of macrolide-resistant _M. pneumoniae_ isolates . In contrast, France identified only 2 resistant _M. pneumoniae_ isolates during 1994–2006, and the United States reported a 30% prevalence of macrolide-resistant strains . Israel reported that in 2010, ≈30% of _M. pneumoniae_ isolates carried an A2063G transition in domain V of the 23S rRNA gene . In Italy, 26% of _M. pneumoniae_ –infected children harbored strains with point mutations in domain V of the 23S rRNA gene . Thus, there is great variability in the prevalence of macrolide resistance in _M. pneumoniae_ isolates.\n\n【12】### Conclusions\n\n【13】A key finding of this study is the increasing prevalence of macrolide resistance over time. Several factors may have led to this increase. First, the increased use of macrolide antimicrobial drugs may be responsible for the development and spread of macrolide resistance. A recent study showed a correlation between increased use of oral macrolides and an increase in macrolide resistance by selective pressure in _M. pneumoniae_ and other respiratory pathogens . A comprehensive trend analysis of the national data showed an increase in macrolide use in the community (expressed in defined daily doses \\[DDD\\]/1,000 inhabitants/day) during 2005–2009 . Penicillins and cephalosporins are the 2 most frequently used classes of oral antimicrobial drugs. There were decreasing trends in penicillin use and a subtle increase in cephalosporin use during 2005–2009. Macrolide use remained steady until 2007; however, there was an increase of >30% in DDD/1,000 inhabitants/day between 2007 (.5 DDD/1,000 inhabitants/day) and 2009 (.3 DDD/1,000 inhabitants/day). Our data on macrolide use do not fully explain the 10-year change in _M. pneumoniae_ resistance to macrolide drugs because data were available only for 2005–2009. In addition, the spread of resistant strains could have been facilitated by other factors, such as high population density or geographic closeness with the 2 neighboring countries, where resistant strains were highly prevalent.\n\n【14】We found an increasing prevalence of the 23S rRNA gene mutation in _M. pneumoniae_ isolates during 2000–2011 in South Korea. We did not address the clinical issues regarding antimicrobial drug choices for macrolide-resistant mycoplasma pneumonia or compare the clinical outcomes for macrolide-resistant and macrolide-sensitive infections; nevertheless, we believe that the evidence of a recent increase in macrolide resistance provides guidance for additional clinical investigations and new therapeutic strategies. The incidences of macrolide-resistant _M. pneumoniae_ infection should be carefully monitored, particularly among children, for whom treatment can be challenging. Further studies are needed to evaluate the clinical significance of macrolide-resistant _M. pneumoniae_ pneumonia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b0ffae77-b733-43bb-99f9-b036298d0530", "title": "Lobomycosis in Man and Lobomycosis-like Disease in Bottlenose Dolphin, Venezuela", "text": "【0】Lobomycosis in Man and Lobomycosis-like Disease in Bottlenose Dolphin, Venezuela\nLobomycosis (lacaziosis) is a chronic, granulomatous, fungal infection of the skin and subcutaneous tissues that affects humans and members of the family Delphinidae . It is caused by the noncultivable yeast-like organism (Lacazia loboi_ ) of the order Onygenales .\n\n【1】Rare in humans, lobomycosis was first reported in Recife, Brazil, in 1930  and subsequently in other countries in South and Central America, where it seems to be endemic . It was also recently reported in Europe, Canada, the United States, and South Africa, mostly in persons who had traveled to Central or South America or had contact with an infected dolphin . Geographic and climatic conditions associated with endemic human lobomycosis are those of tropical continental areas that are generally located 200–250 m above sea level and characterized by dense vegetation, annual rainfall < 2,000 mm, mean temperature of 24°C, and mean relative humidity >75% .\n\n【2】Cases of lobomycosis have been found in bottlenose dolphins (Tursiops truncatus_ ) and Guiana dolphins (Sotalia guianensis_ ) in North and South America since the 1970s and are being increasingly reported . The disease in these mammals is characterized by white to pink, verrucous lesions, often in pronounced relief that may ulcerate and form large plaques. Lobomycosis-like disease, a syndrome pathoanatomically consistent with lobomycosis but for which a histologic diagnosis is lacking, has been found in coastal bottlenose dolphins from Colombia, Ecuador, Peru, and Brazil, Guiana dolphins in Brazil , and Indo-Pacific bottlenose dolphins (T_ . _aduncus_ ) in the tropical lagoon of Mayotte in the Indian Ocean .\n\n【3】_L_ . _loboi_ cells from lesions in bottlenose dolphins were smaller than those found in infected tissues in humans, which suggests that the organism may not be identical in the 2 hosts . Serologic data have indicated that dolphins and humans are infected with similar _L_ . _loboi_ strains . The possibility of humans acquiring lobomycosis from dolphins appears low; only 1 documented case of disease transmission from a rescued dolphin to its attendant occurred in the early 1970s . The ecology of lobomycosis in humans and odontocetes seems to be unconnected. Infections occur mostly in persons inhabiting the Amazon Basin and in inshore and estuarine dolphins in North and South America . We report cases of lobomycosis in a fisherman and lobomycosis-like disease in a bottlenose dolphin along the coast of Venezuela.\n\n【4】### The Cases\n\n【5】##### Human\n\n【6】A 62-year-old fisherman from the central coast of Venezuela (Puerto Cruz and Chichiriviche de la Costa, 10°32′N, 67°14′W) was examined during a fieldwork expedition in March 2008. He had extensive lesions on the left ear and recalled that his illness began when he was ≈52 years of age and had accidentally injured the posterior portion of the helix of that ear with a fishhook. A small, solitary, hard nodule subsequently developed and was later accompanied by similar satellite lesions that tended to become confluent and form harder nodules, sometimes hyperchromic with flat and shiny surfaces . These nodules slowly invaded the entire free border, posterior aspect, and lobule of the ear and caused occasional pruritus and a tingling sensation. Because of the diffuse infiltration of the ear, the condition was initially diagnosed as diffuse cutaneous leishmaniasis or lepromatous leprosy.\n\n【7】Microscopic findings included a granulomatous reaction indicated by lymphohistiocytic elements with large numbers of multinucleated Langhan-type giant cells and numerous isolated yeast-like organisms with a birefringent membrane isolated or in chains alternating with some piriform elements showing simple gemation . The patient refused otoplasty and was treated with itraconazole; some nodules partially regressed.\n\n【8】##### Dolphin\n\n【9】On June 28, 2004, an adult male, likely inshore, bottlenose dolphin, which had recently died, was found on a beach of La Restinga National Park (°01′N, 64°10′W) on Margarita Island, Venezuela. The dolphin was 3.8 m long and was emaciated. Several teeth were missing, especially at the distal end of the beak, and an 8-cm _Conchoderma auritum_ stalked barnacle was attached to the right 10th mandibular tooth. The dolphin had severe lobomycosis-like disease with a large number of white, gray, and pink proliferating, congregating lesions, some bleeding, with keloidal and verrucous characteristics that formed rosettes on the beak, back, flanks, dorsal fin, tailstock, and tail . The dorsal fin was severely affected and the asymmetric distribution of the lesions caused the fin to bend. Granulomas extended into the oral cavity between the maxillar teeth and the palate. Unfortunately, because of a variety of factors, including a lack of field sampling capabilities, presence of crowd, and limited beach access for transport, no necropsy was conducted and no samples were available. However, the severe emaciation suggested that the dolphin had a chronic debilitating disease. Whether its poor health status favored the wide dissemination of lobomycosis-like disease or whether lobomycosis-like disease was the primary undermining factor remains unknown.\n\n【10】### Conclusions\n\n【11】We report lobomycosis in a fisherman and lobomycosis-like disease in a bottlenose dolphin along the coast of Venezuela. The fisherman likely contracted the disease from the marine environment after pathogen inoculation with a fishing hook. He had never visited the Amazon Basin. Although the human and dolphin cases were probably not related, they suggest the role of the marine environment as a likely natural habitat for _L_ . _loboi_ and as a reservoir for infection. Along the central coasts of Venezuela and Margarita Island, temperatures range from 22°C to 28°C, annual rainfall ranges from 0 mm to 500 mm (Margarita Island) or >500 mm (central coast), and the mean relative humidity is ≈50%.\n\n【12】Many aspects of transmission, pathogenesis, and ecology of lobomycosis are still poorly understood. Transmission of lobomycosis among Delphinidae may occur by contact, as suggested by the endemic status of the disease in bottlenose dolphins in the Indian River Lagoon in Florida, USA, and possible transmission from mother to calf in an Indo-Pacific bottlenose dolphin from the Mayotte Lagoon . Humans may also acquire the infection through rare contact with infected free-ranging Delphinidae. The disease signs and pathologic changes are similar in humans and dolphins. In humans, lobomycosis is associated with an apparent partial deficit of cell-mediated immunity and no alterations of humoral immunity . In dolphins, the disease is related to a substantial decrease in CD4+ helper T-lymphocytes and CD19+ and CD21+ B cells . Lesions are also similar in humans and cetaceans, although they tend to be larger in cetaceans. These lesions cover a wide and pleiomorphic clinical spectrum, ranging from the typical smooth and shiny nodular lesions with keloidal aspect to the extensive and confluent verrucous lesions. They occur predominately on the most exposed and cooler areas : i.e. head, back, dorsal fin, flanks, caudal peduncle, and tail in dolphins; and lower limbs, outer ears, upper limbs, and face in humans.\n\n【13】The apparent emergence of lobomycosis, lobomycosis-like disease, and other skin diseases in coastal cetaceans from South America and the Indian Ocean  is cause for concern. This emergence may be indicative of increased biological contamination and environmental changes, including climatic changes worldwide, which may represent a potential threat to human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8b9532e8-92f4-4608-8a20-df7f4383be62", "title": "Pathogenic Responses among Young Adults during the 1918 Influenza Pandemic", "text": "【0】Pathogenic Responses among Young Adults during the 1918 Influenza Pandemic\nThe influenza pandemic of 1918–19 was the most deadly single event in recorded history. Because of its unique severity and global effects, it is the prototype of a global natural disaster. In recent years, fears of recurrence of an influenza pandemic similar to that in 1918 have motivated planning, preparations, and allocations of resources by public health and other government agencies, nongovernmental organizations, medical care providers, pharmaceutical and medical device manufacturers, medical researchers, private businesses, and persons worldwide .\n\n【1】Because of severe consequences and current relevance of the 1918 pandemic, it is essential to review its events and effects, determine their underlying causes, and assess likelihood of a recurrence. These tasks are difficult because the 1918 pandemic occurred at the end of World War I, before influenza viruses were discovered and before influenza vaccines, antiviral and antibacterial drugs, and intensive medical care were available. Fortunately, abundant and detailed written records exist of clinical, laboratory, and epidemiologic events during the pandemic period . In addition, isolates of the virus that caused the lethal second wave of the pandemic (in the fall of 1918 in most locations) have been reconstructed from preserved remains of patients who died . These isolates have been used to determine the genetic relationships between the 1918 pandemic influenza strain and subsequent seasonal and pandemic A/H1N1 strains . Genetic relationships between the 1918 pandemic strain and strains that caused the clinically mild first wave of epidemics in 1918 and pandemics before 1918 remain undefined .\n\n【2】It is commonly believed that the 1918 pandemic resulted from the sudden emergence and worldwide spread of an inherently hypervirulent influenza strain. However, this view is inconsistent with several well-documented characteristics of the pandemic. In this report, we review unique, poorly understood, or unexplained clinical and epidemiologic characteristics of the 1918 pandemic. Also, we present hypotheses that are scientifically credible, consistent with the historical record, and account for epidemiologic and clinical manifestations of the pandemic. Finally, we discuss implications of our hypotheses regarding pandemic influenza preparedness and research and development of universal influenza vaccines .\n\n【3】### Unique and Unexplained Characteristics of the 1918 Pandemic\n\n【4】##### Mortality and Case-Fatality Rates\n\n【5】Because the 1918 pandemic spread worldwide and caused unprecedented numbers of deaths, it is often presumed that the pandemic strain was unusually transmissible and that infection with the virus was inherently lethal (i.e. direct effects of the virus routinely and rapidly caused death). During the 1918 pandemic, influenza infection rates were similar to those during other pandemics of the last century; and in most affected populations, overall mortality rates were <1%, and case-fatality rates were <3% . Thus, the 1918 pandemic strain was not unusually transmissible compared with other pandemic strains ; and even without definitive treatments (e.g. antiviral and antibacterial drugs) or modern life-preserving measures (e.g. mechanical ventilation, medical intensive care), most infected persons survived .\n\n【6】##### Deaths Caused by Secondary Bacterial Pneumonia\n\n【7】In 1918, most pandemic-related deaths were not caused by primary influenza-related pneumonia or acute respiratory distress syndrome, and relatively few deaths occurred within the first few days after illness onset . Most deaths occurred \\> 7 days after illness onset and were the result of secondary bacterial pneumonia caused by common colonizers of the respiratory tract, e.g. _Haemophilus influenzae_ , _Streptococcus pneumoniae_ , _S. pyogenes_ , and _Staphylococcus aureus_ . Clinical and pathologic records suggest that lethal secondary bacterial pneumonias often followed dysregulated immune responses to infections with influenza .\n\n【8】##### Increased Mortality Rate in Young Adults (W-shaped Mortality Curve)\n\n【9】In general, during the 1918–19 influenza pandemic period, illness rates were highest among children of school age. However, mortality rates were highest among infants, young adults, and the elderly  . The W-shaped relationship between mortality rate and age is a unique and unexplained characteristic of the 1918 pandemic. The lack of correspondence between illness and mortality rate in relation to age belies the common views that direct pathologic effects of the virus were independently and invariably life threatening and that the usual clinical course after infection was rapid deterioration of respiratory function terminating in death.\n\n【10】In 1889–90, pandemic influenza (Russian flu) spread rapidly throughout the world, and from 1890 through the winter of 1900–01, widespread epidemics of influenza-like illness recurred . The 1890–91 and subsequent epidemic waves likely were caused by variants of the 1889–90 pandemic strain . Thus, before 1918, most members of the 1875–1900 birth cohorts had been exposed to the 1889–90 pandemic influenza strain. These persons were 18–43 years old, the age groups at highest mortality risk, during the lethal second wave of the 1918 pandemic .\n\n【11】##### Timing and Characteristics of Epidemic Waves\n\n【12】During 1918–19, three distinct influenza epidemic waves occurred. The first wave (mid-1918 in most locations) caused widespread illness but few deaths (day fever). The second wave (fall of 1918 in most locations) caused widespread illness and high mortality rates . The third wave (winter 1919) caused widespread illness but affected fewer persons and caused fewer deaths than the second wave.\n\n【13】The sharp contrast in the clinical expressions of infections during the first and second waves suggests that they were caused by different influenza virus strains. If the first 2 epidemic waves of the 1918–19 pandemic were caused by the same or immunologically cross-reactive influenza A (H1N1) viruses, persons affected during the first wave should have been protected from infection and, in turn, illness, secondary pneumonia, and death during the second wave. Protection from infection would have derived from neutralizing antibodies against the same or similar viral surface antigens (e.g. hemagglutinin).\n\n【14】There are conflicting reports regarding the immunologic susceptibility to infection during the second wave among persons who were infected during the first wave. Several reports of the experiences of localized groups (e.g. students, prisoners, military units) suggest that illness during the first wave protected from influenza during the second wave . However, our review of the medical records of all persons who served in the Australian Imperial Forces in Europe and the Middle East in 1918 documents that persons affected during the first wave were as likely to become ill, but were much less likely to die, from influenza–pneumonia during the second wave . Together, the findings suggest that infections during the first wave altered immune responses to the pandemic strain during the second wave. In turn, persons infected during the first wave had milder clinical expressions and lower mortality rates when infected with the pandemic strain during the second wave.\n\n【15】##### Mortality Rates among Nurses and Medical Officers\n\n【16】During the 1918 pandemic period, military nurses and medical officers were intensively and repeatedly exposed to the influenza A (H1N1) pandemic strain in clinics, in ambulances, and on crowded open wards. However, during the lethal second wave, nurses and medical officers of the Australian Army had influenza-related illness rates similar to, but mortality rates lower than, any other occupational group . Similar observations were made in other groups of military and civilian health care workers . These findings suggest that the occupational group with the most intensive exposure to the pandemic strain had relatively low influenza-related pneumonia mortality rates during the second wave .\n\n【17】##### Mortality Rates among Military Members with Least Service\n\n【18】During the fall of 1918, all 40 large mobilization/training camps throughout the United States and Puerto Rico were affected by influenza epidemics . During the camp epidemics, influenza–pneumonia mortality rates were inevitably highest among the soldiers with the least military service. In the US Army overall, 60% of those who died of influenza-related pneumonia were soldiers with <4 months of military service .\n\n【19】Among Australian soldiers in Europe and the Middle East in the fall of 1918, persons with the least military service also had the highest influenza-related pneumonia mortality rate . In general, in deployed Australian Army formations, soldiers were not segregated by time in military service. Thus, the nature or intensity of soldiers’ exposures to the pandemic strain likely did not vary in relation to their length of military service. During epidemics of the second wave, most soldiers were likely exposed to the same influenza A (H1N1) strain, and most of those affected were treated in the same military medical system as their counterparts regardless of seniority. Thus, the sharp differences in mortality rates in relation to length of service, in mobilization camps in the United States and deployed settings in Europe, likely reflected differences in host immune responses to the pandemic strain.\n\n【20】##### Mortality Rates among Passengers and Crews on US Troop Transport Ships\n\n【21】Influenza illness rates were similar on US troop transports. However, case-fatality rates were sharply higher among soldiers who had recently congregated on the ships than among permanently assigned crewmen .\n\n【22】##### Mortality Rates among Residents and Soldiers from Urban and Rural Areas\n\n【23】In the United States during the pandemic period, the influenza-related mortality rate was higher among residents of urban areas than among residents of rural areas. In contrast, the mortality rate was higher among soldiers from rural than among those from urban areas .\n\n【24】##### Mortality Rates among Island Populations\n\n【25】When pandemic influenza attacked island populations, mortality rates were often high, sometimes extraordinarily so, e.g. Western Samoa (%) and Nauru (%) . However, some island populations had relatively low mortality rates during pandemic-related epidemics, e.g. the Philippines (%), Puerto Rico (%), and Hawaii (.5%) . On other islands, mortality rates varied widely among different groups of island residents, e.g. Chamorrans (%) versus Caroline Islanders (.4%) on Saipan ; indigenous Fijians (.7%) versus Europeans (.4%) on Fiji ; and indigenous (Maori) (.2%) versus European (.5%) residents of New Zealand . Thus, on some islands, subgroups of residents who shared the same microbiological and environmental exposures had markedly different influenza-related mortality rates. Because the same influenza strain likely caused all pandemic-related island epidemics, the wide variability in mortality rates across island populations suggests that host immune factors were determinants (perhaps with other factors such as poverty, overcrowding, and malnutrition) of the clinical courses and outcomes of infections with the pandemic influenza strain.\n\n【26】##### Clinical Expression of Infections with Similar A/H1N1 Strains in 2009 and\n\n【27】The 1918 influenza A (H1N1) pandemic strain is genetically similar to the novel pandemic (H1N1) 2009 strain. However, clinical expressions of infections in 2009 were much less severe than in 1918, e.g. mortality rates in 1918 were >100× higher than in 2009 .\n\n【28】### Hypotheses\n\n【29】Unique and unexplained characteristics of the 1918 pandemic suggest that the risk for lethal secondary bacterial pneumonia after influenza infections depended on the nature, timing, and intensity of immune responses to the pandemic strain; and subsequently on the likelihood of exposure during transient periods of increased susceptibility to bacterial strains against which affected persons had no protective antibodies. In 1918, nearly all humans were immunologically susceptible to infection with the A/H1N1 pandemic strain; not surprisingly, the pandemic spread rapidly worldwide. The rapid spread of the pandemic with high illness attack rates in most age groups indicates that an influenza virus antigenically similar to the pandemic strain did not widely circulate among humans within at least several decades before 1918.\n\n【30】We hypothesize that in 1918 many persons had second lifetime exposures to an immunodominant T-cell epitope that was conserved on an internal protein of the 1918 pandemic strain and a heterosubtypic other strain (e.g. 1889 pandemic strain). When persons were reexposed to the identical epitope in 1918, epitope-specific memory CD8+ T-cells produced excessive cytokines, chemokines, immune cell activation, and epithelial cell necrosis. The immunopathologic effects of the dysregulated T-cell response transiently increased susceptibility of infected hosts to respiratory bacterial strains against which they lacked protective antibodies.\n\n【31】In contrast, persons who were first exposed in 1918 to the immunodominant T-cell epitope of hypothesized concern may have had primary T-cell responses that controlled virus replication without increasing susceptibility to bacterial invasion of the lower respiratory tract. Persons who had multiple prior exposures to influenza viruses and other respiratory infectious agents before 1918 had diversely partitioned memory CD8+ T-cell repertoires and extensive portfolios of bacterial strain–specific antibodies. Their immune responses to infection with the 1918 pandemic strain may have controlled virus replication without increasing their susceptibility to bacterial invasion.\n\n【32】Modern genetic analyses have estimated that 3 distinct variants of influenza A (H1N1) viruses co-circulated in the early 1900s . These variants were the respective prototypes of all pandemic, seasonal, and classical swine influenza A (H1N1) viruses since 1918. The first epidemic wave of the 1918 pandemic may have been the last wave of the 1889–90 Russian flu pandemic. If so, the first wave spread widely and rapidly in the face of background immunity to an influenza strain that had been circulating among humans for nearly 3 decades .\n\n【33】Alternatively, the first wave may have been caused by an antigenically distinct seasonal strain of influenza A (H1N1) . If so, antibodies against hemagglutinin of the seasonal strain did not provide complete protection against infection with the pandemic strain. However, because many internal proteins of human influenza viruses are conserved and strongly immunogenic (e.g. matrix 2, nucleoprotein \\[NP\\]), antibodies and memory CD8+ T lymphocytes that were produced during the first wave may have altered clinical expressions, decreased susceptibility to secondary bacterial pneumonia, and reduced deaths during the second wave .\n\n【34】The peak of mortality rates among young adults (W-shaped mortality curve) remains a unique and unexplained characteristic of the 1918 pandemic . Before World War I, there was relatively little global interconnectedness. Even in the most industrialized countries, many persons lived their entire lives in their birth communities and had relatively little exposure to outsiders. The situation sharply and permanently changed with the social disruptions and population dislocations precipitated by worldwide armed conflict.\n\n【35】Persons born before 1901 (the last year of widespread circulation of the 1890 Russian flu pandemic strain) and after 1875 (the first year after widespread epidemics of a poorly characterized influenza-like illness) were 18–43 years of age in 1918. Worldwide, members of these birth cohorts had high influenza attack rates and were likely to die from secondary bacterial pneumonia during the 1918 pandemic period .\n\n【36】Persons born before 1875 were >43 years of age in 1918. Before the 1918 pandemic period, they likely had been exposed to more heterosubtypes of influenza A and more respiratory bacterial strains than their younger counterparts. In general, during the pandemic period, middle-age and elderly adults had lower influenza attack rates and were less likely to die than their younger counterparts .\n\n【37】Also in 1918, persons born after 1901 were less likely than those older to have been exposed to the 1890 pandemic strain or displaced by war-related activities. During the pandemic period, persons <17 years of age had relatively high influenza attack rates but relatively low mortality rates (except for infants) .\n\n【38】We hypothesize that, soon after infection with the 1918 pandemic influenza strain, infected persons experienced a transient increase in susceptibility of the lower respiratory tract to invasion by bacteria to which they were immunologically naive. Thus, for example, those who were relatively new to their living or work environments (e.g. military recruits, soldiers on troop ships, patients on hospital wards) at the time they were infected with the 1918 pandemic strain had a relatively high risk of death from secondary bacterial pneumonia . During the course of their influenza illnesses, such persons were likely to be exposed to bacterial strains to which they lacked protective antibodies . Thus, for example, residents of rural areas were relatively unlikely to be exposed to novel strains of bacteria while recovering from influenza, and they had low pandemic-related mortality rates. In contrast, military recruits from the same rural areas were likely to be exposed to novel strains of bacteria while recovering from influenza, and they had relatively high pandemic-related mortality rates .\n\n【39】This interpretation explicates the somewhat counterintuitive finding that nurses, medical officers, and the crews of troop ships had high influenza attack rates but relatively low mortality rates during the lethal second wave of the 1918 pandemic. Before being infected with the pandemic influenza strain, these persons were often exposed in their occupational settings to high concentrations of diverse strains of respiratory infectious agents. Because of their extensive portfolios of respiratory bacteria strain–specific antibodies, they were naturally immune to and protected from secondary pneumonia caused by these agents .\n\n【40】The hypotheses presented here are consistent with the historical record and scientifically plausible. For example, studies in humans have identified an immunodominant NP-derived CD8+ T-cell epitope that is consistently presented by high frequency HLA class I molecules and recognized by cytotoxic T lymphocytes. The epitope is present on the NP of the 1918, 1976, and 2009 human pandemic strains and on most swine strains, but not on most other human strains of the past century .\n\n【41】Studies in pigs suggest that the NP of most swine influenza strains contains a strongly immunogenic CD8+ T-cell epitope. For example, pigs that were primed with a DNA vaccine that expresses NP, and subsequently challenged with an influenza A strain with the same NP, had dysregulated, pathogenic immune responses . Also, pigs that were primed with an inactivated swine influenza A vaccine (A/swine/Iowa/15/1930 H1N1) and subsequently challenged with a later generation swine influenza A strain with markedly different surface proteins (A/swine/Minnesota/00194/2003 H1N2) showed development of enhanced (immunologically potentiated) pneumonia that were not observed after challenge with the homologous strain . The findings have been reproduced by using pandemic (H1N1) 2009 virus as the challenge strain and adding a recombinant matrix 2 protein to the vaccine construct .\n\n【42】Studies in mice have documented that T-cell–mediated immunopathologic responses can contribute to severe pneumonitis when mice are exposed to a highly glycosylated influenza virus and subsequently infected with a poorly glycosylated strain. Infection with a recent seasonal influenza virus (H1N1), followed by infection with pandemic (H1N1) 2009 virus, elicited severe immunopathogenic responses . Finally, studies in ferrets have documented that those infected with pneumococci after acquiring influenza, but not before, showed development of lethal secondary pneumonia and other invasive complications .\n\n【43】In summary, we hypothesize that mortality risk after infection with the 1918 pandemic influenza A (H1N1) strain depended on the number, nature, and diversity of prior infections with influenza virus and respiratory bacteria. Specifically, mortality rates during the lethal second wave were highest among persons with prior exposures to heterosubtypic influenza strains that enhanced immunopathogenic effects when a person was infected with the 1918 pandemic strain and had limited exposures to other respiratory infectious agents. In such persons, infection with the pandemic strain caused high viral loads, dysregulated and pathogenic cell mediated immune responses, and transient increases in susceptibility to invasive bacterial infections. If such influenza virus–infected hosts were subsequently exposed to bacterial strains to which they had no protective antibodies, they were at high risk of acquiring life-threatening secondary bacterial pneumonia.\n\n【44】The unique circumstances that enabled the unprecedented mortality rates of the 1918 pandemic no longer exist on a global scale. For example, in modern times, even the most isolated communities (e.g. Pacific islanders, indigenous populations of North America, Australia, and New Zealand) are interconnected through myriad commercial and sociopolitical activities. As a result, most populations are exposed to annual seasonal influenza viruses, and most young adults are exposed to numerous viral and bacterial respiratory pathogens. Thus, compared with the situation in 1918, adults in modern communities have more diversified immune repertoires against influenza strains and bacterial respiratory pathogens. The hypotheses presented may explain at least in part the relatively low mortality rate associated with pandemic (H1N1) 2009 virus. During the 2009 pandemic, many persons who died had underlying medical conditions, including obesity, asthma, cardiovascular diseases, diabetes, and pregnancy; histopathologic changes consistent with diffuse alveolar damage; and evidence of bacterial co-infections .\n\n【45】Finally, the findings of this report are relevant to the research and development of a universal influenza vaccine. Candidate vaccines that contain antigens that are highly conserved across influenza A strains and strongly immunogenic must be closely monitored to ensure that T-cell–mediated immune responses to future seasonal and pandemic strains are protective but not pathogenic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f50d7d25-6ddc-411d-8470-125abf732689", "title": "Multidrug-resistant Mycobacterium tuberculosis in HIV-Infected Persons, Peru", "text": "【0】Multidrug-resistant Mycobacterium tuberculosis in HIV-Infected Persons, Peru\nMultidrug-resistant tuberculosis (MDR-TB) threatens global TB control and has been identified in almost all surveyed countries. During 1994 to 1997, the World Health Organization (WHO)-International Union against Tuberculosis and Lung Disease identified high prevalences of MDR-TB in the former Soviet Union, Asia, the Dominican Republic, and Argentina .\n\n【1】MDR-TB has been associated with inadequate treatment regimens, poor adherence to treatment, poorly managed TB-control programs, and unenforced hospital infection control programs, as well as HIV infection . HIV infection influences the natural history of TB in several ways: active TB occurs within 6 months of acquiring _Mycobacterium tuberculosis_ infection in 37% of persons with HIV-induced immunosuppression  but occurs in 2% of immunocompetent adults during the first year after acquiring _M. tuberculosis_ infection . HIV-infected persons with TB also have increased frequency of disseminated and meningeal disease, other extrapulmonary diseases, atypical clinical signs and symptoms, drug-related adverse events, and negative sputum smears for acid-fast bacilli .\n\n【2】Nosocomial outbreaks of MDR-TB involving HIV-infected persons have occurred in the United States and other industrialized countries , but improved treatment and hospital infection control programs have contained these initial outbreaks of MDR-TB. However, such outbreaks are increasingly recognized in countries with more limited resources .\n\n【3】In Peru, the National TB Control Program received reports of 35,685 new TB cases in 1999 (incidence 141.4/100,000) . Passive reporting to the National TB Control Program showed HIV prevalences from 1% to 1.5% in TB patients during 1996 to 1999 , but prospective testing of 1,043 patients with a new event of TB in Lima and Callao from September to October 1999 found HIV seropositivity in 2.3% (P.E. Campos, unpub. data). Furthermore, TB was the AIDS-defining illness in 10,939 (%) AIDS cases in Peru , and 50% of persons with AIDS in Peru develop TB at some point in their disease.\n\n【4】A National TB Control Program survey during 1999 found MDR-TB in 57 (%) of 1,879 Peruvians with a first episode of TB and in 32 (.3%) of 260 with previously treated TB . Of 2,101 TB patients in that survey, 8 had a previous diagnosis of HIV infection. In 1997, at the Dos de Mayo Hospital in Lima, which provides care to the largest number of HIV-infected persons, 9 (.6%) of 26 HIV-infected patients with TB had MDR-TB .\n\n【5】Mortality rates during TB treatment of Peruvian HIV-infected patients has been high; 63%, 51%, 49%, and 39% of patients with a first episode of TB died during treatment that was started in 1996, 1997, 1998, and 1999, respectively . These rates contrast with a mortality rate of 9% in HIV-infected persons with pulmonary TB in Haiti . Treatment regimens prescribed for TB in developing countries are rarely based on susceptibility testing, and since 1987, HIV-infected patients with a first episode of TB in Peru have received a standard course of rifampin, isoniazid, pyrazinamide, and ethambutol in compliance with WHO recommendations . Therefore, MDR-TB might contribute to the high mortality rate for TB in HIV-infected Peruvians.\n\n【6】We performed this study to further characterize the prevalence and pattern of MDR-TB in HIV-infected adults in Lima and the adjacent port of Callao in Peru and to explore preventable risk factors that could be used to design, implement, and evaluate better preventive and therapeutic interventions.\n\n【7】### Methods\n\n【8】##### Study Design and Population\n\n【9】This study was designed to evaluate the prevalence of MDR-TB in HIV-infected patients \\> 18 years of age and to assess potential risk factors for MDR-TB in HIV-infected persons with TB. The study was carried out from February 1999 to January 2000 in Lima and Callao. Lima is the capital city of Peru, and Callao is the adjacent main port; together they have a population of 8,239,891, representing 32% of the national population. Lima and Callao accounted for 78% of all Peruvian AIDS cases reported from 1983 to September 2000  and for 56% of all TB cases reported in 1999 . The research protocol for using human participants in this study has been reviewed and approved by the Human Subject Review Committee of the University of Washington and the Ethical Committee of the Scientific Research Office at the Cayetano Heredia University. Informed consent was obtained from the participants.\n\n【10】From February 1999 to January 2000, we introduced an active surveillance system to identify HIV-infected adults with a new event of TB, defined as a first episode of TB or a relapse, at the 10 public hospitals that provide care for most HIV-infected persons living in Lima and Callao. Each of these 10 hospitals has a unit of the National TB Control Program where every patient with a suspected or confirmed diagnosis of TB is referred for further work-up, treatment, follow-up, or referral. To identify HIV-infected patients with a new episode of TB, one trained interviewer periodically visited each of these units. In addition, we encouraged all clinicians to send clinical specimens for isolation of _M. tuberculosis_ , and all laboratories to submit such isolates to TB control program reference laboratories for susceptibility testing.\n\n【11】MDR-TB was defined as resistance to both isoniazid and rifampin, with or without resistance to other drugs. Case-patients were adults with previously diagnosed HIV infection and with a new episode of TB, whose isolates of _M. tuberculosis_ were MDR-TB. HIV-seropositive controls were adults with previously diagnosed HIV infection and a new event of TB, whose isolates of _M. tuberculosis_ were susceptible to isoniazid or rifampin. HIV-positive status was defined by previous, repeatedly positive enzyme-linked immunosorbent assay (ELISA) confirmed by immunofluorescence or Western blot.\n\n【12】From February trough September 1999, a total of 972 adult patients with a new episode of smear-positive TB in Lima and Callao who received care at Ministry of Health facilities were included in the National TB Control Program’s surveillance of resistance to anti-TB drugs. Of these 972, a total of 116 had been previously tested for HIV: 7 (%) were HIV seropositive. From the remaining 965 participants who were HIV seronegative or not tested for HIV infection, we randomly selected a second control group of 153 participants.\n\n【13】##### Enrollment, Interview, and Treatment\n\n【14】Both trained interviewers periodically visited each of the 10 hospitals to identify, enroll, and interview HIV-infected patients with a new episode of TB. Because classification as “case” or “control” for HIV-infected patients was determined by drug susceptibility-test results, which became available after 3 or 4 months, each HIV-infected adult with a new episode of TB was eligible to participate, and interviewers were blinded as to susceptibility test results. One member of the local TB team introduced the interviewer, who explained the study and invited the patient to participate. Interviewers contacted adults with TB who were HIV seronegative or of unknown HIV status belonging to the HIV–seronegative control group at their homes, where they were invited to participate, enrolled, and interviewed. Patients giving written informed consent underwent a standardized face-to-face interview concerning demographic characteristics, past and current medical history, and potential exposures to _M. tuberculosis_ during the 12 months before onset of TB symptoms. Healthcare workers at each facility followed national guidelines of the TB and the AIDS control programs in providing further treatment and follow up of study participants. Data collected were handled exclusively by the study team, ensuring confidentiality.\n\n【15】##### Laboratory Methods\n\n【16】Primary isolations were attempted by using Lowenstein-Jensen or Ogawa medium at each hospital laboratory and at local reference laboratories serving the national surveillance study. Susceptibility testing was carried out at the Mycobacteria Laboratory of the Peruvian National Institute of Health, the national reference laboratory for susceptibility testing, or at one of three local mycobacterium reference laboratories. These laboratories used the proportion method to determine the sensitivity profile of each strain , using the following critical concentrations (μg/mL): isoniazid, 0.2; rifampin, 40; streptomycin, 4; ethambutol, 2; and pyrazinamide, 100. The National Reference Laboratory underwent external quality control by the Pan American Health Organization/WHO Instituto Panamericano de Protección de Alimentos y Zoonosis (INPPAZ) and performed quality control for the three local reference laboratories .\n\n【17】##### Data Analyses\n\n【18】SPSS 10.0 software (SPSS, Inc. Chicago, IL) was used for data entry and analyses. Percentages of MDR-TB were compared in HIV-positive and HIV-negative patients with TB, and risk factors for MDR-TB in HIV-infected patients were explored by calculation of odds ratios (OR) for dichotomous variables. Medians of continuous variables were compared by using Student t test for independent samples.\n\n【19】### Results\n\n【20】##### Characteristics of Patients with TB\n\n【21】Of 415 HIV-seropositive patients diagnosed and reported with a new episode of TB (on the basis of acid-fast smear or culture results or on clinical characteristics) at the 10 hospitals during the study period, 157 (%) were not interviewed: 87 had already left the hospital and could not be located at referral health centers, 67 were in poor clinical condition and unable to answer the questionnaire, and 3 declined to participate. The remaining 258 seropositive participants interviewed averaged 32 years of age (range 18 to 62) and reported 9.8 years of education; 77% were men, 57% single, and 20% had had at least one previous episode of TB. Isolation of _M. tuberculosis_ was attempted in 239 (%); 135 were culture-positive, 61 were smear positive for acid-fast bacilli, and 62 were diagnosed on the basis of clinical criteria. Drug-susceptibility testing was requested for all positive cultures and completed on 81, who were the focus of subsequent analyses; the remaining strains were lost before or during transport to the National Reference Laboratory for susceptibility testing.\n\n【22】Of the 153 TB patients randomly selected as controls without known HIV infection, we located 110; 108 agreed to participate. They averaged 29.3 years of age (range 18 to 80) and reported 10.3 years of education; 57% were men, 52% single, and 9.3% had had at least one previous episode of TB.\n\n【23】##### Drug-Susceptibility Test Results\n\n【24】The prevalence of MDR-TB was 43% in the 81 HIV-seropositive patients with available susceptibility test results and 3.9% in the 965 patients whose HIV status was negative or unknown (p < 0.001); only 1 (.9%) of 108 patients whom we randomly selected and interviewed from this group of 965 TB patients had an MDR-TB isolate . The prevalence of resistance to any drug was also higher in _M. tuberculosis_ isolates from HIV-positive patients than in isolates from the 965 without known HIV infection.\n\n【25】Drug-susceptibility test results were available for 26 (.6%) of the 157 HIV-seropositive patients we did not interview; 35% of these patients had MDR-TB, not significantly different from the 43% of those we did interview (p = 0.44).\n\n【26】##### HIV Patients with MDR-TB and with Non–MDR-TB\n\n【27】Of HIV-seropositive participants with TB , MDR-TB was significantly associated with TB diagnosed at hospital A (OR 3.7, 95% confidence interval \\[CI\\] 1.3 to 10), with employment during the 12 months before the onset of symptoms, and with exposure to TB at work, but not with age, sex, marital status, education level, crowding in the home, or low income. MDR-TB was not significantly associated with previous episodes of TB or with TB prophylaxis; the proportion who had received TB prophylaxis or had a previous episode of treatment for TB was 17 (%) of 35 with MDR-TB versus 20 (%) of 46 without MDR-TB (OR 1.2, 95% CI 0.5 to 3.0).\n\n【28】##### Hospital Characteristics\n\n【29】##### Assessment of Possible Bias\n\n【30】Interviewed patients with (N = 81) or without (N = 177) susceptibility results were similar with respect to age, years of education, family income, symptom duration, time since previous TB, TB prophylaxis in the past, crowding in the home, gender, marital status, home ownership, and employment; and during the past 12 months, exposure to TB at work, exposure at home, ambulatory care at health centers, ambulatory care at hospitals, inpatient hospital care, and participation in HIV or TB support groups. History of previous TB was somewhat more frequent in those tested (%) than in those not tested (%, p = 0.09), and history of _Pneumocystis carinii_ pneumonia (PCP) prophylaxis was more frequent among those tested (%) than in those not tested (%, p < 0.01).\n\n【31】### Discussion\n\n【32】This study documents high rates of MDR-TB in HIV-infected persons with TB receiving care at public hospitals in Lima and Callao; MDR-TB was 11 times more common in these patients than in 965 TB patients without known HIV infection in Lima and Callao. In comparison to persons having TB without known HIV infection, the HIV-infected patients with TB had higher frequency of contact with health centers and hospitals, and MDR-TB was found mainly in patients receiving care at the three hospitals serving the largest number of HIV-infected patients with TB. Although participants with HIV infection more often had had previous episodes of TB, and presumably, only the HIV-infected persons had received TB prophylaxis, these factors were not significantly associated with MDR-TB in the HIV-infected patients with TB.\n\n【33】The 43% prevalence of MDR-TB among HIV-infected persons with TB found in this study is higher than the 36% previously found in Italy , and the 28.3% found in Argentina, both during epidemics of nosocomial MDR-TB .\n\n【34】Because early clinical manifestations of TB often develop in HIV-infected persons , they mark sentinel cases that first indicate outbreaks of nosocomial transmission of TB and may represent a larger number of nosocomial transmission of TB.\n\n【35】Three factors drive TB transmission: the rate of exposure of susceptible to infectious persons, the efficiency of transmission per exposure, and the average duration of infectiousness once infection has occurred . For HIV-infected persons, higher exposure to TB in general could easily result from routine periodic visits to clinical settings also frequented by patients with TB (as occurs in Lima and Callao, where most persons with HIV infection attend HIV/AIDS clinics located at large hospitals); from the mixing of HIV-infected patients with TB patients in these settings because of the archaic practice of mixing patients who have communicable diseases (like TB and HIV infection) in large multiple-bed wards. MDR-TB case-patients tend to be selectively hospitalized because they do not respond to routine therapy.\n\n【36】Conditions that increase exposure to TB, such as overcrowding, long waiting times in clinics, sharing of facilities, and large open multiple-bed wards, are also common in medical institutions in the developing world. Increased efficiency of transmission per exposure could occur if HIV-related immunosuppression increases host susceptibility to acquisition of infection by _M. tuberculosis_ in general or by MDR-TB strains of TB in particular.\n\n【37】Several factors could prolong the duration of infectiousness of MDR-TB in HIV-immunosuppressed persons, including continued use of isoniazid- and rifampin-based regimens as initial therapy for TB in HIV-infected persons, even during ongoing outbreaks of MDR-TB transmission. Patients receiving ineffective treatment will not improve and will more often use clinical services as outpatients and as inpatients, increasing the exposure to MDR-TB of other patients. In addition, increased frequency of atypical clinical pictures and smear-negative results, relatively common features in HIV-infected patients with TB, can contribute to delayed diagnosis and prolonged infectivity.\n\n【38】Evidence suggests that HIV infection favors the emergence of acquired drug resistance in individual patients during treatment . HIV-infected persons have a higher risk for acquisition of isolated rifampin resistance , and once- or twice-weekly rifamycin-based regimens increase the risk for acquired rifamycin resistance in TB patients with advanced HIV disease . MDR-TB is more often a consequence of addition of rifampin resistance than of addition of isoniazid resistance .\n\n【39】Limitations of this study included the fact that TB cases with HIV infection were recruited from hospitals where patients with known HIV infection receive care in Lima, whereas HIV-negative controls were recruited from nonhospital clinics. However, methodologic features and results of this evaluation suggest that results may be generalizable to all HIV-infected patients with TB receiving care within the Ministry of Health system in Lima and Callao. Of 457 new episodes of TB in HIV-infected persons \\> 18 years in Lima and Callao during 1999 , we identified 415 cases (%) through the TB units at each of these 10 hospitals during the 12-month study. We systematically interviewed HIV-infected patients well in advance of knowing whether they had MDR-TB, and we systematically interviewed a random sample of TB patients without known HIV infection from a larger representative sample of TB patients from Lima and Callao. Participants with known HIV infection were selected consecutively and patients without known HIV infection were selected randomly from all case-patients of TB receiving care at public health services. Comparisons of those interviewed versus not interviewed, and those tested and not tested for drug susceptibility suggested no bias, except that those tested had somewhat higher frequencies of previous TB and of PCP prophylaxis. The latter could reflect greater immunosuppression or greater contact with the medical care system. Although both could have biased results toward higher estimates of MDR-TB prevalence among HIV-seropositive patients, a previous history of TB or of TB prophylaxis was not associated with MDR-TB.\n\n【40】We were unable to interview all HIV-positive patients with TB diagnosed during the study, raising the question of whether the prevalence of MDR-TB was higher in those we interviewed than in those not interviewed. However, the 43% frequency of MDR-TB in those we interviewed did not differ significantly from the 35% prevalence of MDR-TB in the 26 patients who we did not interview for whom susceptibility testing was performed. Finally, concerning the fact that we did not obtain HIV serologic finding from all of the 108 controls with negative or unknown HIV status, if the prevalence of HIV had been higher than the 2.3% prevalence observed during prospective testing in Lima in 1999, such misclassification bias would only have reduced the differences observed between groups.\n\n【41】Community-based studies throughout the United States have documented increased prevalence of MDR-TB in HIV-infected people , and in a recent report of several surveys , MDR-TB was found more often in HIV-infected patients than in HIV-uninfected patients, but after adjustment for previous treatment for TB the difference between HIV-infected and HIV-uninfected patients was no longer statistically significance. However, in our survey, even after eliminating patients with history of prior episodes of TB or TB prophylaxis, MDR-TB was still seen in 18 (%) of 44 isolates from HIV-infected persons versus 24 (%) of 814 HIV-negative controls (p < 0.001).\n\n【42】### Conclusions\n\n【43】Although rigorous compliance with infection control recommendations , particularly those related to engineering control, is difficult in developing countries, the combined epidemics of TB, MDR-TB, and HIV/AIDS make infection control measures essential. WHO guidelines recommend a hierarchy of controls (administrative, environmental, and personal respiratory protection), many of which entail little or no cost . Minimal measures must include strict respiratory isolation for patients with confirmed or suspected TB and mandatory wearing of appropriate masks for persons entering all patient rooms, for patients leaving their rooms when unavoidable, and for patients with cough when seen in clinics. These essential measures to avert TB transmission in healthcare settings have been demonstrated . Segregating persons with TB from those with HIV in individual rooms with negative air flow, establishing safer sputum sampling collection procedures, improving the laboratory support for early identification of TB and of MDR-TB, and providing more effective treatment regimens to patients at increased risk for MDR-TB are necessary. HIV testing of patients with TB and susceptibility testing of _M. tuberculosis_ isolates from HIV-infected patients should be routine in settings where outbreaks or endemic transmission of MDR-TB is occurring in HIV-infected patients.\n\n【44】Nosocomial MDR-TB transmission at hospital A has been ongoing since 1997. Recently published IS6110 restriction fragment length polymorphism analysis of _M. tuberculosis_ strains collected between July 1997 and April 1999 and belonging to HIV-positive inpatients clearly implicate nosocomial transmission of MDR-TB in this hospital . Similarly, outbreaks of nosocomial MDR-TB in HIV-infected persons have emerged first in New York, Buenos Aires, and Lima (settings providing hospital-based care, including antiretroviral therapy, for HIV-infected persons). Unrecognized MDR-TB outbreaks in other developing countries are likely. As delivery of antiretroviral therapy for HIV in developing countries proceeds, nosocomial exposure of HIV-infected persons to TB must be minimized. More effective surveillance, prevention, and treatment for MDR-TB are essential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d1880ab-c5e6-4910-bb31-4cb369c61d4f", "title": "Clinical and Laboratory Characteristics and Outcome of Illness Caused by Tick-Borne Encephalitis Virus without Central Nervous System Involvement", "text": "【0】Clinical and Laboratory Characteristics and Outcome of Illness Caused by Tick-Borne Encephalitis Virus without Central Nervous System Involvement\nTick-borne encephalitis virus (TBEV) is a member of the genus _Flavivirus_ in the family _Flaviviridae_ and is transmitted to humans predominantly through _Ixodes_ spp. tick bites. In addition to 3 well-known subtypes of TBEV (European, Siberian, and Far Eastern) that cause disease in humans, other subtypes, including the currently named Baikalian and Himalayan subtypes, have been reported .\n\n【1】Infection with TBEV can be symptomatic or asymptomatic. As is the case for infections with other flaviviruses, most (%–98%) persons infected with TBEV do not experience symptoms; however, some findings in blood donors suggest that asymptomatic infections might be rare . Nevertheless, when infection with TBEV is symptomatic, it can manifest as a febrile illness without central nervous system (CNS) involvement  but often progresses to tick-borne encephalitis (TBE) (i.e. CNS involvement caused by the virus) . Clinical manifestation differs in some respects according to virus subtype. In 13%–44% of patients, TBE caused by the European subtype manifests with direct CNS involvement  , whereas in most patients, CNS inflammation is preceded by a febrile illness, resulting in a biphasic course . The initial phase, which corresponds to viremia, manifests as fever, fatigue, malaise, headache, and muscle and joint pain, but in the absence of CNS inflammation; this phase usually lasts <1 week , and the illness then improves over a few days. The hallmark of the second phase of the disease is CNS involvement. Meningitis is the predominant manifestation in children. In adults, meningitis occurs in ≈50% of patients, meningoencephalitis in ≈40%, and meningoencephalomyelitis in ≈5%–10%. The case-fatality rate of TBE caused by the European subtype of TBEV is 0.5%–2%. In addition, ≈5% of adult patients are affected by permanent pareses, and at least one third suffer from a postencephalitic syndrome .\n\n【2】In general, clinical manifestations and laboratory characteristics of symptomatic TBEV infection are well described. However, this statement is valid for cases with neurologic involvement (i.e. for TBE) but less so for the initial phase of TBE, and much less so for TBEV infection manifesting solely as febrile illness without later CNS involvement. That manifestation, also called isolated initial phase of TBE, abortive form of TBE, febrile headache, summer flu, or fever form, is postulated to match clinically and serologically the initial phase of TBE, with the exception that subsequent CNS involvement does not occur. TBEV infection manifesting as febrile illness without later CNS involvement is suggested to be frequent , although not in all reports , and the scientific basis for such a conclusion is unclear. Furthermore, although the outcome of symptomatic TBEV infection without CNS involvement is believed to be favorable, no reliable data on the outcome have been published. Because clinical symptoms and signs of the illness are nonspecific, and because, in parallel to the initial phase of TBE, serum antibodies to TBEV are not yet expected to have developed, the only option for diagnosis at the time of actual illness is demonstrating the presence of TBEV RNA in the blood. However, this approach is not routine and might have a low diagnostic yield owing to several other known or unknown causes of fever, even in a region that is highly endemic for TBE. Therefore, the possibility that a febrile illness is the result of TBEV infection is usually tested for and established only after signs or symptoms of CNS involvement appear, which does not happen in case of the fever form. In that case (and if PCR detection of viral RNA in blood is not available), further clinical and microbiologic (serologic) follow-up after improvement is needed to establish the diagnosis. In this study, we analyzed in detail the clinical and laboratory characteristics of febrile illness after tick bite or exposure to ticks and its outcome in patients in whom infection with TBEV was established by the presence of viral RNA in the blood.\n\n【3】### Materials and Methods\n\n【4】##### Definitions\n\n【5】Febrile illness resulting from infection with TBEV was defined by the presence of fever and constitutional symptoms, demonstration of viral RNA in serum specimens, and the absence of signs or symptoms of CNS involvement at the time of illness. In patients with clinical signs potentially suggesting CNS involvement, cerebrospinal fluid (CSF) samples were examined; the threshold for lumbar puncture was low. A CSF leukocyte count < 5 × 10 6  /L was interpreted as excluding CNS inflammation.\n\n【6】According to the later appearance (or absence) of neurologic involvement, the febrile illness was further subclassified as either the initial phase of TBE (defined as a febrile illness with demonstration of viral RNA in serum samples that, after a clinical improvement, was followed by neurologic involvement within a 2-month follow-up period and fulfilling criteria for TBE) or as febrile illness resulting from infection with TBEV in a narrow sense (fever form, febrile headache) when no signs of CNS involvement were present at the time of actual illness or within a 2-month follow-up period. TBE was defined as the presence of clinical signs or symptoms of meningitis or meningoencephalitis, increased CSF leukocyte counts (>5 × 10 6  cells/L), and demonstration of a recent infection with TBEV indicated by serum IgM and IgG or IgG seroconversion in paired serum samples.\n\n【7】##### Patients and Samples\n\n【8】Adult patients examined for febrile illness at the Department of Infectious Diseases, University Medical Center Ljubljana (Ljubljana, Slovenia), during 2003–2019, in whom the presence of TBEV RNA was identified by PCR in serum specimens, qualified for the study. Serum samples were obtained either during a prospective study on the etiology of febrile illness after a tick bite or exposure to ticks (patients, 63.3%) or represented remnants of samples collected as a part of routine diagnostic testing of patients with febrile illness in whom TBE later occurred (patients, 36.7%). Serum specimens were stored at –80°C until further processing. For the 62 patients, we obtained clinical and laboratory information on the etiology of febrile illness occurring after tick bite or tick exposure prospectively. Clinical and laboratory follow-up occurred for these patients for at least 2 months (i.e. at first evaluation and at follow-up visits 1 week, 2 weeks, and 2 months later). For the other 36 patients, we obtained clinical and laboratory information from medical charts.\n\n【9】##### TBEV Antibodies and RNA Load\n\n【10】We determined the presence of TBEV antibodies in serum samples by using the Enzygnost Anti-TBE/FSME Virus (IgM, IgG) test , according to the manufacturer’s instructions. We extracted total RNA from serum samples by using the QIAamp Viral RNA Mini Kit , according to the manufacturer’s instructions. For the detection of TBEV RNA, we performed quantitative reverse transcription PCR as reported previously .\n\n【11】##### Statistical Analysis\n\n【12】We summarized continuous variables as median values and interquartile ranges (IQRs), and discrete variables as counts and percentages with 95% CIs. We based comparisons between groups on Wilcoxon rank-sum tests for continuous variables and Fisher exact tests for discrete variables. We defined statistical significance as a p value of < 0.05.\n\n【13】We examined associations between variables by using linear regression modeling . We used log 10  \\-transformed viral RNA loads, and domain experts (P.B. and F.S.) selected included covariates. We modeled continuous covariates that demonstrated a nonlinear relationship by using restricted cubic splines  and imputed missing values by using multiple imputation on the basis of additive regression, bootstrapping, and predictive mean matching . We used R software for all statistical analyses .\n\n【14】##### Ethics\n\n【15】The study was conducted in accordance with the principles of the Declaration of Helsinki, the Oviedo Convention on Human Rights and Biomedicine, and the Slovene Code of Medical Deontology. The study was approved by the National Medical Ethics Committee of Slovenia (approval nos. 152/06/13, 178/02/13, and 37/12/13). Patients whose specimens were obtained in the study on the etiology of febrile illness after a tick bite or exposure to ticks signed an informed consent form. The Ethics Committee waived the need for written informed consent for patients for whom remnants of routinely collected serum specimens were used.\n\n【16】### Results\n\n【17】A total of 98 adult patients examined for febrile illness in whom TBEV RNA was identified by PCR in their serum specimens were enrolled in the study. The median age of the patients was 51 years; 52% were women.\n\n【18】##### Clinical and Laboratory Characteristics of Febrile Illness Caused by TBEV\n\n【19】Most (.7%) patients reported a tick bite within 4 weeks of the onset of illness. The median time from the bite to illness onset was 6 days, median duration of illness before evaluation was 5 days, and total duration of the illness was 7 days. A total of 37/98 (.8%) patients were hospitalized for a median of 3 days. The most frequent symptoms or signs were malaise and fatigue (%), fever (.9%), headache (.7%), and myalgias (.1%) . The most frequent laboratory findings were leukopenia (.5%), thrombocytopenia (.4%), and abnormal liver test results (.5% of patients had \\> 1 abnormal liver test result, most often elevated aspartate aminotransferase \\[AST, 55.0%\\] and alanine aminotransferase \\[ALT, 26.3%\\]) .\n\n【20】Individual laboratory parameters according to duration of illness before testing demonstrated heterogeneous results. Counts of total peripheral blood leukocytes, neutrophils, lymphocytes, and monocytes were lowest early in the course of illness and tended to increase (improve) with the duration of illness, whereas thrombocytopenia became more pronounced with the duration of illness. Liver tests, including AST, ALT, gamma-glutamyl transferase (GGT), and lactate dehydrogenase, also tended to deteriorate with the duration of illness. These tendencies were noticeably uniform and were significant for total leukocyte and monocyte counts and for AST, ALT, and GGT levels .\n\n【21】None of the 98 patients had serum IgG to TBEV at the time of a positive PCR result (median of 5 days after illness onset) and only 2/98 (.0%) had TBEV-specific IgM. In these 2 patients, the serologic tests were performed on days 7 and 13 of the illness. An additional 5 patients (.1%) had borderline specific IgM levels; for most, the duration of illness before testing was somewhat longer (median 6 days, range 3–9 days). Viral RNA load was higher in hospitalized patients with more severe illness than in those who did not need hospitalization but did not differ substantially according to age, sex, duration of illness before testing, or total duration of the actual febrile illness, or for patients with undetectable viral IgM in serum samples when compared with patients in whom antibodies were detectable .\n\n【22】##### Outcome\n\n【23】Of the 62 patients who received a diagnosis during a prospective study on the etiology of febrile illness after a tick bite or exposure to ticks and in whom TBEV was present in serum samples, 6 (.7%) did not experience any symptoms or signs during the follow-up period of 2 months. In 4 (.5%) patients, mild constitutional symptoms not suggesting CNS involvement and without meningeal signs reappeared after a symptom-free interval of up to 12 days. In contrast, the other 52 (.9%) patients experienced overt signs of meningitis or meningoencephalitis associated with CSF pleocytosis and fulfilled serologic criteria for TBE; in this subgroup, the longest symptom-free interval was 18 days.\n\n【24】The clinical characteristics of the initial phase of TBE and febrile illness without subsequent CNS involvement were not formally compared because the number of patients was too small, but the clinical and laboratory manifestation of the 2 entities appears comparable . All patients developed IgM and IgG to the virus during follow-up.\n\n【25】Of the 62 prospectively followed patients, 27 (.5%) were hospitalized. The likelihood of later CNS involvement in hospitalized patients was similar to that in patients with less severe illness who were treated as outpatients (/27 \\[88.9%\\] vs. 28/35 \\[80%\\]; p = 0.49). Furthermore, the level of viral RNA in serum samples in the group in which no CNS involvement occurred was similar to the level in those in whom meningitis or meningoencephalitis later occurred .\n\n【26】### Discussion\n\n【27】Symptomatic infection with TBEV can manifest as CNS involvement (TBE) or as a febrile illness with or without subsequent CNS involvement . Although the epidemiology and clinical manifestation of symptomatic TBEV infection is considered well-established, this statement is valid only for neurologic involvement (TBE) and is less well-established for the initial phase of TBE and even less so for TBEV infection manifesting solely as febrile illness without later CNS involvement.\n\n【28】TBE is highly endemic to Slovenia. For as many as 70% of patients in Slovenia who have notified cases of TBE, diagnosis occurs at University Medical Center Ljubljana , which provided access to detailed information on a large number of patients with TBE and enabled this study.\n\n【29】The initial phase of TBE consists of fever, headache, myalgias, arthralgias, and fatigue , but reliable information on the relative frequency of individual symptoms is limited, and results are variable. In this study, which encompasses TBEV febrile illness with and without later CNS involvement (the initial phase of TBE and febrile headache) and is based on a well-defined group of patients with definite proof of TBEV infection (the presence of virus in blood at the time of actual illness and later seroconversion), our findings corroborate previous results on the spectrum of symptom manifestation and add reliable information on their relative frequency. Thus, >85% of patients have malaise or fatigue, fever, and headache; ≈50% of the patients report myalgias, arthralgias, and, rather unexpectedly, gastrointestinal symptoms (abdominal pain, nausea, vomiting, or diarrhea); and the frequency of respiratory symptoms or chills is almost 20%. The finding of chills is somewhat surprising because chills are not common in patients with viral infections and are more characteristic of diseases caused by bacteria. Illness duration in our patients (median 7 days) was somewhat longer than reported in the literature (median 4–6, range 1–19 days) . In addition, in some patients, the illness was relatively severe: more than one third of patients were hospitalized. Laboratory findings possibly contributed to decisions to hospitalize, because leukopenia and thrombocytopenia might suggest serious disease in a patient with fever.\n\n【30】The first phase of TBE is known to be accompanied by leukopenia; thrombocytopenia and abnormal liver test results also might be present, although to a lesser extent . In contrast, in the second phase of TBE, the blood leukocyte count is elevated or in normal range. Laboratory tests in the patients in our study very often demonstrated abnormalities (leukopenia in 88%, abnormal liver tests in 70%, and thrombocytopenia in 59%) that were more common than previously described . Our study corroborates previous findings that in most patients, the concentration of total leukocytes in the peripheral blood is reduced. In addition, we offer several new findings, such as a reduction in all major subgroups of leukocytes and a tendency for total numbers of leukocytes, neutrophils, lymphocytes, and monocytes to increase (i.e. improve). In contrast, thrombocytopenia, liver tests (including AST, ALT, and GGT), and lactate dehydrogenase tended to deteriorate with duration of illness. Although we do not have an exact explanation for these laboratory abnormalities, they seem biologically plausible. During illness caused by TBEV without CNS involvement (including illness with subsequent CNS involvement), TBEV replicates in various organs and tissues, and this might affect test results. Later, however, when viremia vanishes and CNS damage occurs, the abnormalities are not present  and obviously are temporally associated with viremia, suggesting a direct or indirect effect of the virus on the bone marrow and liver.\n\n【31】Our results corroborate previous findings  that the appearance of antibodies to TBEV greatly diminishes the likelihood of detecting viral RNA in blood: none of our 98 patients had detectable serum IgG to TBEV at the time of a positive PCR finding, although 7 (.6%) patients had specific IgM. As expected, in these 7 patients, the duration of illness was longer than for patients who were completely seronegative. We also expected that the viral RNA load would be lower in patients with detectable serum IgM to the virus than in patients in whom the antibodies were undetectable, but we did not confirm this premise. Furthermore, viral RNA load did not differ substantially in relation to age, sex, duration of illness before testing, or total duration of the actual febrile illness; however, viral RNA load was higher in patients with more severe illness (those who were hospitalized compared with those who were not). Information on TBEV RNA load in humans is very limited . In our previous report on viral RNA load in patients with biphasic course of TBE, the load was higher in women than in men but was not significantly associated with clinical and laboratory characteristics of the initial phase of illness or with characteristics of the later meningoencephalitic phase of TBE . However, because several associations tested in this study were not analyzed in our previous study (and vice versa), direct comparison is restricted to matching approaches; within them, the only discordant result was for viral RNA load according to sex, which was significantly higher in women than men in the previous study but not in this study.\n\n【32】Some review articles have stated that febrile illness without later CNS involvement is a frequent clinical manifestation of infection with TBEV, representing approximately two thirds of all clinically manifested infections with TBEV . However, a PubMed literature search of titles and abstracts using the search terms “(tick-borne encephalitis) AND (initial phase OR first phase)” and without time limitation (i.e. from 1966 onward) did not reveal any primary source firmly supporting such statements for Western and Central Europe. Nevertheless, febrile illness resulting from TBEV infection without subsequent CNS involvement is usually not recognized, possibly because of the small proportion of such cases , which is in accordance with some other reports . Our previous findings, which were based on clinical and serologic analyses of febrile illness after a tick bite, suggested that febrile illness resulting from infection with TBEV occurs as a rule with subsequent CNS involvement (TBE) after an improvement of up to 12 days . In this study, which was focused on 62 patients with well-defined TBEV infection, 52 (%) patients experienced overt symptoms and signs of CNS involvement associated with CSF lymphocytic pleocytosis, 6 (%) patients remained completely asymptomatic, and 4 (%) patients in whom CSF was not examined experienced mild symptoms not associated with meningeal signs for a duration of 2–7 days. We expected that patients in whom TBE did not occur would have lower blood levels of virus than those in whom CNS involvement occurred; these speculations were not confirmed.\n\n【33】The main strengths of our study are the sufficiently large number of patients with a well-defined diagnosis of febrile illness resulting from TBEV infection (on the basis of demonstration of viral RNA in serum samples during actual illness and on later seroconversion), together with the comprehensive collection of clinical and laboratory data and assessment of the course and outcome of the illness. We reveal several new clinical and laboratory details of febrile illness caused by TBEV, confirming that in most (at least 84%) of these patients, TBE (i.e. CNS inflammation) later develops. However, because we included only patients referred to us by family physicians, one of the limitations of the study is a potential selection bias: patients with very mild illness probably do not visit primary physicians and, even if they do, the likelihood of referral to us is lower than for patients with more severe disease or unusual laboratory findings, such as leukopenia or thrombocytopenia. Thus, our findings are limited to a subset of patients with more severe disease; consequently, our conclusions might not be valid for milder clinical cases.\n\n【34】In conclusion, febrile illness caused by TBEV infection is characterized clinically by the presence of malaise or fatigue (%), fever (%), headache (%), and myalgias (%) and in laboratory tests by leukopenia (%), thrombocytopenia (%), and abnormal liver results (% of patients had \\> 1 abnormal liver test, usually elevated AST and ALT) but normal inflammatory markers. The infection proceeded to TBE in \\> 5/6 (%) patients within 18 days after defervescence. Clinical and laboratory findings in patients with TBEV febrile illness do not distinguish between patients in whom TBE later develops and those in whom it does not.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "94c59455-00e6-49c2-b68c-0958189050fc", "title": "Association of Behavioral Risk Factors for Chronic Diseases With Physical and Mental Health in European Adults Aged 50 Years or Older, 2004–2005", "text": "【0】Association of Behavioral Risk Factors for Chronic Diseases With Physical and Mental Health in European Adults Aged 50 Years or Older, 2004–2005\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Noncommunicable diseases are the leading cause of illness and death worldwide; behavioral risk factors (BRFs) contribute to these diseases. We assessed the presence of multiple BRFs among European adults according to their physical and mental health status.\n\n【3】**Methods**\n\n【4】We used data from 26,026 adults aged 50 years or older from 11 countries that participated in the Survey of Health, Ageing and Retirement in Europe . BRFs (overweight or obesity, smoking, physical inactivity, and risky alcohol consumption) were assessed according to physical health (ie, presence of chronic diseases, disease symptoms, or limitations in activities of daily living) and mental health (depression) through multiple regression estimations.\n\n【5】**Results**\n\n【6】Overweight or obesity in men and physical inactivity in women were the most prevalent BRFs. Compared with physically active adults, physically inactive adults had a higher mean number of chronic diseases (.33 vs 1.26) and chronic disease symptoms (.55 vs 1.47). Risky alcohol consumption (≥4 servings of an alcohol beverage ≥3 times a week) was associated with a higher mean depression score (.84 vs 2.47). Compared with adults with 0 or 1 BRF, adults with 2 or more BRFs had significantly higher odds of having 1 or more chronic diseases (men: 1.52; women: 1.73) and functional limitations (men: 1.65; women: 1.79) and higher prevalence of high blood pressure (.8% vs 28.2). Belgian adults with BRFs had the highest mean number of chronic diseases or functional limitations among those who were overweight or obese and the highest mean number of chronic diseases and disease symptoms among those who smoked and were physically inactive.\n\n【7】**Conclusion**\n\n【8】We found revealed significant positive associations between BRFs and poor health among middle-aged and older European adults. Primary health care intervention programs should focus on developing ways to reduce BRF prevalence in this population.\n\n【9】Introduction\n------------\n\n【10】Noncommunicable diseases (NCDs) are the leading causes of disease and death worldwide, and their symptoms and resulting functional limitations are related to impaired quality of life . NCDs accounted for 57 million deaths in 2008, of which 63% were attributed to cardiovascular disease, diabetes, cancer, and chronic respiratory disease .\n\n【11】Numerous lifestyle habits, identified as behavioral risk factors (BRFs), may increase NCD risk. These risk factors include overweight or obesity, smoking, physical inactivity, and risky alcohol consumption . Each of these risk factors alone can cause numerous health problems. For example, from 2008 to 2030 deaths attributed to smoking worldwide are expected to double, from 3.4 to 6.8 million . Overweight and obesity, direct consequences of physical inactivity and unhealthy diet, are responsible for 2.8 million deaths annually .\n\n【12】The greatest burden of disease and death related to BRFs from 2009 through 2011 occurred in countries of the World Health Organization European Region, the Eastern Mediterranean Region, and the Region of the Americas . The prevalence of BRFs varies among populations; clustering of 2 or more BRFs, an indication of increased risk for chronic diseases , occurs in approximately 57% of the adult population in the United States and 50% of Canadian adults aged 50 years or older .\n\n【13】Few large-scale studies examined the presence of BRFs in European adults according to physical and mental health. The aim of this study was to assess the presence of multiple BRFs in adults aged 50 years or older in 11 European countries, according to their physical and mental health status.\n\n【14】Methods\n-------\n\n【15】Cross-sectional data were collected from 26,026 adults aged 50 years or older (range, 50–104 y), during the first wave  of SHARE (Survey of Health, Ageing, and Retirement in Europe) in 11 European countries (Austria, Belgium, Denmark, France, Germany, Greece, Italy, Netherlands, Spain, Sweden, and Switzerland). A subsample was selected in each country according to complex multistage stratification design. The target population consisted of households with at least 1 person aged 50 years or older. The overall weighted country-average response rate (households and individuals) was 61.8% and ranged from 37.6% (Switzerland) to 73.6% (France). Comparable differences in response rates have been reported in similar surveys . A detailed description of the sampling procedures, recruitment rates, and other survey features has been reported elsewhere .\n\n【16】### Data collection: questionnaires\n\n【17】Computer-aided personal interviews (CAPIs), consisting of 21 modules, were used to collect data in person (eg, demographic characteristics, physical and mental health, BRFs) . Proxy interviews (.0% of all completed interviews) were allowed when physical or mental health limitations (eg, Alzheimer’s disease, hearing loss) prevented a selected participant from completing the CAPIs . Example show cards (ie, a page of the questionnaire with a list of diseases, symptoms, limitations, etc) were used in some modules to help participants understand the questions, enhancing the validity of the questionnaire . Data were missing from only 5% of the CAPIs.\n\n【18】### Physical and mental health assessment\n\n【19】Physical health was assessed by the presence of chronic diseases, disease symptoms, functional limitations, or disabilities during the previous 6 months; these features were recorded by using validated scales via personal interviews . To assess chronic diseases, participants were asked if a doctor had diagnosed any of 11 diseases: heart attack, high blood pressure, high blood cholesterol, stroke, diabetes or high blood glucose, chronic lung disease, asthma, arthritis, osteoporosis, cancer, and stomach or duodenal/peptic ulcer. To assess symptoms, participants were asked if they had any of 11 conditions: pain in back, knees, hips or other joints; heart trouble; breathlessness; persistent cough; swollen legs; sleeping problems; falls; fear of falling; dizziness, faints, or blackouts; stomach or intestine problems; and incontinence. Functional limitations of daily living, defined as activities and instrumental activities ([I\\]ADLs), were assessed by asking participants if they had limitations in any of 13 activities: dressing (including shoes and socks), walking across a room, bathing or showering, eating or cutting up food, getting in or out of bed, using the toilet (including getting up or down), using a map in a strange place, preparing a hot meal, shopping for groceries, making telephone calls, taking medications, doing work around the house or garden, or managing money. Mental health was assessed by using the European Depression (Euro-D) Scale, which defines clinically depressive symptoms by a total score of 4 or more in the 12-item validated questionnaire .\n\n【20】To assess physical and mental health status, the presence of 1 or more chronic diseases, disease symptoms, or (I)ADL limitations was defined separately for every component as its presence (= 1) or absence (= 0) or as a high score (≥4) in the depression scale. A total clustering score for physical and mental health was then calculated by summing the resulting binary variables for each adult. This clustering score ranged from 0 to 4, and 4 cluster categories were created, combining the 4 components as 0, 1 or 2, 3 or 4, or 1 or more of these components. This clustering score depicts the presence of multiple components as a higher burden on health or poor physical or mental health.\n\n【21】### Behavioral risk factors\n\n【22】Four health-related BRFs were assessed, namely overweight or obesity, smoking, physical inactivity, and risky alcohol consumption . Overweight or obesity were determined by self-reported body weight in kilograms and height in meters . Body mass index (BMI) was calculated as kg/m 2  , and participants with a BMI of 25 or greater were considered overweight or obese . Smoking was assessed from self-reported use of cigarettes, cigars, or pipes during the year preceding the survey. To determine physical activity levels, activities, activities such as gardening or walking were considered moderate physical activities, whereas activities such as sports or heavy home labor were considered vigorous physical activity. Frequency of physical activity was classified as more than once per week, once per week, 1 to 3 times per month, or hardly ever or never. Physical inactivity was defined as not engaging in any moderate-to-vigorous physical activity or having a low frequency of physical activity (once a week, 1 to 3 times a month, or hardly ever or never) . Risky alcohol consumption was defined as the consumption of 4 or more servings of alcoholic beverages on at least 3 days a week during the 6 months preceding the survey .\n\n【23】The clustering of BRFs was estimated by adding the number of individual factors that were present (= absence, 1 = presence) to create an average clustering or mean factors score, ranging from 0 to 4. The clustering of 2 or more risk factors was considered to depict high risk for chronic disease .\n\n【24】### Self-rated health\n\n【25】Participants self-rated their health by using the World Health Organization scale, reporting health as very good, good, fair, or bad or very bad .\n\n【26】### Socioeconomic characteristics\n\n【27】The social and demographic variables of age, living status, retirement, and educational status were assessed. Living status consisted of 2 categories, living alone and living with a partner or spouse. Retirement consisted of 2 categories (yes or no) and educational status was calculated as total years of schooling . Financial status was assessed as the gross household income in the previous year .\n\n【28】### Statistical analysis\n\n【29】Data were analyzed using SPSS software, version 21.0 (IBM Corp). Weights were applied to reflect nonresponses and stratification design. The prevalence of individual and clustering (or 2, 3 or 4, or ≥1) of physical and mental health components and BRFs was estimated with the corresponding 95% confidence intervals (CIs). Weighted means of BRFs and their 95% CIs were estimated for each cluster category of physical and mental health status for each sex by using analysis of covariance according to complex sample design procedures. Age, education, living status, country region (north, central, south) , self-rated health, income, and retirement status were used as covariates. Multiple logistic regression analysis (using the same covariates) was conducted to compute adjusted odds ratios (ORs) of participants who had 1 or more chronic diseases, disease symptoms, or (I)ADL limitations; Euro-D scores of 4 or more; or clustering (or 2, 3 or4, or ≥1) of physical or mental health components. Adjusted ORs were estimated separately for the presence of 1 BRF and 2 or more BRFs. Differences in mean numbers of chronic diseases, symptoms, (I)ADL limitations, and Euro-D Scale scores were assessed, and analysis of covariance was used to estimate weighted means according to presence or absence of individual BRFs. Age, sex, education, living status, country region, self-rated health, income, and retirement status were used as covariates. Finally, prevalence of all chronic diseases, disease symptoms, and (I)ADL limitations according to clustering of BRFs were illustrated as weighted percentages and 95% CIs.\n\n【30】Results\n-------\n\n【31】Mean age and years of education of participants were 65.2 and 9.7 years, respectively . A greater percentage of men than women were living with a partner or spouse or were retired, and a smaller percentage of men self-rated their health as bad or very bad. Overweight or obesity, smoking, and risky alcohol consumption were more prevalent in men. Physical inactivity was more prevalent in women. The prevalence of 2 or more BRFs was significantly higher among men (.4%; 95% CI, 57.1%–60.0%) than among women (.0%; 95% CI, 47.7%–50.4%). In contrast, men had significantly lower prevalence of 1 or more chronic diseases, disease symptoms, and (I)ADL limitations, and a score or 4 or more on the Euro-D Scale. Men also had higher prevalence of having 0 components of physical and mental health (.1%; 95% CI,16.0%–18.2%), compared with women (.2%; 95% CI,10.4%–12.0%).\n\n【32】Both men and women with 1 or more chronic diseases or 1 or more (I)ADL limitations had significantly greater weighted mean numbers of BRFs than those with none . Respectively, the adjusted ORs in men with 2 or more BRFs were higher for those with 1 or more chronic diseases (.52; 95% CI, 1.20–1.91) or (I)ADL limitations (.65; 95% CI,1.10–2.48). Women with 2 or more BRFs also had higher odds of having 1 or more chronic diseases (.73; 95% CI,1.42–2.12), disease symptoms (.39; 95% CI,1.10–1.72), or (I)ADL limitations (.79; 95% CI,1.30–2.43); they also had higher odds of having any of the 3 physical and mental health status cluster component combinations than women with 1 BRF (or 2 components: 1.34 \\[95% CI,1.04–1.74\\]; 3 or 4 components: 1.86 \\[95% CI,1.28–2.69\\]; ≥1 components: 1.43 \\[95% CI,1.11–1.84\\]).\n\n【33】Overweight or obese participants had a higher mean number of chronic diseases (.43 vs 1.13, _P_ < .001) and disease symptoms (.62 vs. 1.39, _P_ < .001) than participants of normal weight, whereas smokers had a lower mean number of chronic diseases (.23 vs. 1.32, _P_ \\= .005) than nonsmokers . Physically inactive participants had a higher mean number of chronic diseases (.33 vs. 1.26, _P_ \\= .009), disease symptoms (.55 vs 1.47, _P_ \\= .01), and (I)ADL limitations (.63 vs 0.44, _P_ < .001) than physically active participants. Risky drinkers had a higher mean score on the Euro-D Scale (.84 vs 2.47, _P_ \\= .003), compared with nonrisky drinkers.\n\n【34】Belgian adults had the highest mean number of chronic diseases and (I)ADL limitations among overweight or obese participants  and the highest mean number of chronic diseases and disease symptoms among participants who smoked and were physically inactive and had the highest number of chronic diseases among risky alcohol drinkers. In contrast, among participants who were overweight or obese, Austrian adults had the lowest mean number of chronic diseases (.24; 95% CI, 1.16–1.31). Austrian adults also had the lowest risky alcohol consumption, and among those with risky alcohol consumption, the lowest mean Euro-D score (.59; 95% CI, 1.19–1.98).\n\n【35】Participants with 2 or more BRFs, compared with those with none or 1 BRF, had significantly higher prevalence rates of high blood pressure (.8%; 95% CI, 36.4%–39.1% vs 28.2%; 95% CI, 26.9%–29.6%)  and pain in back, knees, hips or other joints (.3%; 95% CI, 54.0%–56.7% vs 48.3%; 95% CI, 46.8%–49.8%).\n\n【36】Discussion\n----------\n\n【37】We examined the presence of BRFs in European adults aged 50 years or older, according to their physical and mental health status. The main findings were 1) the most prevalent BRFs were overweight or obesity in men and physical inactivity in women; 2) prevalence of 2 or more BRFs was higher in men, and prevalence of physical and mental health status components was lower in men; 3) men with 2 or more BRFs had higher odds for having 1 or more chronic diseases and (I)ADL limitations, and women with 2 or more BRFs had higher odds for having 1 or more of all health status components; 4) physically inactive adults had higher mean numbers of chronic diseases, disease symptoms, and (I)ADL limitations; 5) adults from Belgium with BRFs had the poorest physical health status among the 11 countries studied; and 6) among adults with 2 or more BRFs, high blood pressure was the most prevalent disease.\n\n【38】Our findings agree with earlier literature suggesting that men generally have a higher prevalence of BRFs for chronic disease . The findings of our study also indicate a positive relationship between the presence of BRFs and physical and mental health status components; women had 2 or more BRFs, displaying higher presence and clustering of these components. Women in our study had a higher prevalence of chronic disease, disease symptoms, and functional disabilities than men; however, men had a higher prevalence of BRFs. We cannot explain the cause–effect relationship of BRFs and health because of differences in health status among men and women. Women seem to have more health problems but men have more BRFs . The National Health Interview Survey, 2001, of adults aged 18 years or older in the United States also found that people with 2,3, or 4 risk factors had significantly higher odds of having a chronic disease . In a similar manner, a 2007 study of the Chinese population aged 15 to 69 years with chronic disease reported a significantly higher odds of BRF clustering as well as a significantly greater weighted mean number of BRFs . In addition, the survey of the Australian Institute of Health and Welfare  showed that an increasing number of BRFs was associated with increased odds of arthritis, ischemic heart disease, stroke, depression, and chronic obstructive pulmonary disease . Our study also showed that participants with 2 or more BRFs had significantly higher odds of having (I)ADL limitations than those with 0 or 1 BRF. The 2010 national Behavioral Risk Factor Surveillance System survey conducted in US adults aged 18 years or older  showed that participants with disabilities had higher odds of being both physically inactive, obese, or smokers, confirming the relationship between (I)ADL limitations and BRFs. These findings indicate the relationship between multiple BRFs and physical health and the necessity of prevention programs to focus on reducing BRF prevalence.\n\n【39】Our study showed that overweight or obesity and physical inactivity, which were present in more than half of adults participating in the SHARE survey, were the most prevalent BRFs. An earlier analysis of the same data also showed that women exhibited lower prevalence of overweight or obesity, smoking, and risky alcohol consumption, but higher prevalence of physical inactivity, than men and adults aged 80 years or older . Sufficient levels of physical activity have generally been shown to be protective against the development of chronic diseases (eg, cardiovascular disease) but may also protect mental health by reducing anxiety or depression . Thus, the high prevalence of physical inactivity in our sample could explain the high observed weighted mean numbers of chronic diseases, disease symptoms, (I)ADL limitations, and depression. In contrast, the potentially controversial finding of the association between normal weight status and higher depression score may be explained by the relationship between weight loss and depression . Additionally, overweight and obesity are known risk factors for high blood pressure, a biomedical risk factor with the highest contribution to chronic disease burden and mortality . Overweight or obesity was the second most prevalent BRF in the current study, which could explain our finding that high blood pressure was the most prevalent disease, particularly among participants with 2 or more BRFs. Our study showed that overweight or obese adults also had a higher mean number of symptoms of aging, such as pain in the back, knees, hips or other joints, confirming the reported association between overweight and obesity and these symptoms .\n\n【40】The association of BRFs with mortality has been widely documented. For example, in their recent systematic review, Loef and Walach  estimated that the adoption of 4 or more healthy lifestyle behaviors is associated with a 66% decrease in mortality risk. The Health and Retirement Study, which studied Americans aged 50 years or older, also showed that the combination of multiple unhealthy lifestyle factors was related to increased risk of mortality . This risk was 1.9 times higher for smokers, independently of other factors, and 4.0 times higher for participants with the combined behaviors of smoking and physical inactivity and heavy drinking. These findings are crucial for emphasizing the need for national agencies, organizations, and European governments to identify ways to address and prevent their relative consequences. In our study, however, smokers exhibited a lower number of chronic diseases than nonsmokers and ex-smokers. This might be explained by the fact that smokers in our study were younger , and older age is often associated with smoking cessation due to disease burden . Nevertheless, the increased mortality risk appears to be associated with the lifetime of smoking . Other effective interventions that could contribute to a decrease in the prevalence of some BRFs are the imposition of high taxation on tobacco products, alcohol, and foods with high energy density; clean indoor air laws; and advertising restrictions or mass media campaigns promoting health lifestyle behaviors . These efforts should be country-specific due to differences in income and social and health inequalities among countries .\n\n【41】Comparisons of countries revealed that adults from Belgium with BRFs had the poorest physical health status. An earlier report of the SHARE survey also showed that Belgians had a high prevalence of 2 or more BRFs (.2%), although this prevalence was lower than that among their Austrian, Greek, and Spanish counterparts . Nevertheless, both this earlier report and the current study showed that less than 50% of adults had 2 or more BRFs, which was associated with high odds of poor physical and mental health, especially among women.\n\n【42】Our study has various methodological weaknesses that limit its external validity. The association of BRFs with physical and mental health was based on a cross-sectional design and therefore cannot be substantiated as a causal relationship. Similar studies, however, have also shown associations between individual BRFs and chronic diseases and disabilities , as well as prospective associations with mortality rates . In addition, it is difficult to compare findings among studies because of the different definitions of BRFs used in different studies. Self-reported body weight or height can also differ from objective measurements, and physical inactivity was estimated by using approximate cut-off points that might restrict assessment in older adults . Chronic diseases were assessed by asking participants if they had had a disease diagnosed. Ideally, future studies should include a validation of self-reported data by conducting objective measurements in a subsample of the population. Nevertheless, similar large-scale studies have also based their results on self-reported data . Also, risky alcohol consumption was not estimated by sex due to the use of fixed, closed-ended questions for both sexes. Assessment of smoking prevalence was based only on smoking occurrence and not on total burden of this behavior (eg, pack-years) . Likewise, the 11 chronic diseases included in the physical health status definition did not have the same burden on health. Furthermore, presence of dementia was not assessed during the first wave. Finally, our analysis did not allow the examination of the association of BRFs with individual chronic diseases . Nevertheless, we combined a large number of physical and mental health components, thereby adding to the literature examining the associations of BRFs with physical and mental health.\n\n【43】This study of middle-aged and older European adults showed that significant positive associations exist between unhealthy lifestyle behaviors and poor physical and mental health. Primary health promotion program should focus on examining whether any causal relationships exist and if so, identify ways to reduce BRFs, taking into account that interventions should consider the health care systems in individual countries . Such interventions could be applied at the community or home level or in primary health care and should involve patients and health care providers at all stages of their development and implementation to increase their chance of effectiveness. These program should focus on adults with multiple BRFs and offer them individualized care and guidance according to personal motivations, skills, and barriers for lifestyle changes, and should support use of screening and preventive health services to reduce BRFs .\n\n【44】Tables\n------\n\n【45】#####  Table 1. Characteristics, Physical and Mental Health Status Components, and Behavioral Risk Factors for Chronic Disease Among European Adults (n = 26,026) Aged 50 Years or Older, Survey of Health, Ageing, and Retirement in Europe, 2004–2005 a  \n\n| Characteristic | Total (n = 26,026) | Men (n = 12,030) | Women (n = 13,996) |\n| --- | --- | --- | --- |\n| Age, y, weighted mean (SD) | 65.2 (.4) | 64.1 (.8) | 66.2 (.8) |\n| Education, y, weighted mean (SD) | 9.7 (.9) | 10.4 (.9) | 9.1 (.8) |\n| **Living status** | **Living status** | **Living status** | **Living status** |\n| Lives alone | 33.2 | 21.0 (.7–22.5) | 43.6 (.2–45.0) |\n| Lives with partner or spouse | 66.8 | 79.0 (.5–80.3) | 56.4 (.0–57.8) |\n| **Retirement status** | **Retirement status** | **Retirement status** | **Retirement status** |\n| Retired | 50.9 | 58.1 (.6–59.6) | 44.7 (.4–46.1) |\n| **Self-rated health** | **Self-rated health** | **Self-rated health** | **Self-rated health** |\n| Bad or very bad | 11.7 | 10.5 (.6–11.5) | 12.6 (.7–13.7) |\n| **Components of physical and mental health status c** | **Components of physical and mental health status c** | **Components of physical and mental health status c** | **Components of physical and mental health status c** |\n| Chronic diseases b , =1 | 67.7 | 64.6 (.2–66.0) | 70.2 (.0–71.5) |\n| Disease symptoms c , =1 | 68.9 | 62.0 (.6–63.4) | 73.6 (.8 –75.9) |\n| (I)ADL d limitations, =1 | 20.5 | 16.7 (.6–17.8) | 23.8 (.6–25.1) |\n| **European Depression Scale e** | **European Depression Scale e** | **European Depression Scale e** | **European Depression Scale e** |\n| Score =4 | 26.6 | 19.3 (.1–20.5) | 32.7 (.4–34.1) |\n| **No. of health status components** | **No. of health status components** | **No. of health status components** | **No. of health status components** |\n| 0 | 13.9 | 17.1 (.0–18.2) | 11.2 (.4–12.0) |\n| 1 or 2 | 58.8 | 63.1 (.6–64.5) | 55.2 (.8–56.6) |\n| 3 or 4 | 27.3 | 19.9 (.7–21.1) | 33.6 (.3–34.9) |\n| **Behavioral risk factor** | **Behavioral risk factor** | **Behavioral risk factor** | **Behavioral risk factor** |\n| Overweight or obese f | 60.0 | 66.7 (.3–68.1) | 54.2 (.8–55.6) |\n| Smoker g | 18.3 | 24.3 (.0–25.6) | 13.1 (.3–14.1) |\n| Physical inactivity h | 70.8 | 65.9 (.5–67.3) | 75.0 (.9–76.2) |\n| Risky alcohol consumption i | 4.3 | 8.0 (.2–8.8) | 1.3 (.0–1.6) |\n| **No. of behavioral risk factors** | **No. of behavioral risk factors** | **No. of behavioral risk factors** | **No. of behavioral risk factors** |\n| 0 | 9.2 | 8.1 (.4–8.9) | 10.2 (.4–11.0) |\n| 1 | 37.4 | 33.4 (.0–34.8) | 40.7 (.4–42.1) |\n| \\=2 | 53.4 | 58.4 (.1–60.0) | 49.0 (.7–50.4) |\n\n【47】Abbreviation: (I)ADL, activities and instrumental activities of daily living.  \na  Values are weighted percentage and confidence intervals unless otherwise noted.  \nb  Chronic diseases refer to the following chronic diseases: heart attack, high blood pressure, high blood cholesterol, stroke, diabetes or high blood glucose, chronic lung disease, asthma, arthritis, osteoporosis, cancer, and stomach or duodenal/peptic ulcer.  \nc  Symptoms refer to the following: pain in back, knees, hips or other joints; heart trouble; breathlessness; persistent cough; swollen legs; sleeping problems; falls; fear of falling down; dizziness; faints or blackouts; stomach or intestine problems; and incontinence.  \nd  (I)ADL refers to having a limitation in the following activities: dressing (including shoes and socks), walking across a room, bathing or showering, eating, cutting up food, getting in or out of bed, using the toilet (including getting up or down), using a map in a strange place, preparing a hot meal, shopping for groceries, making telephone calls, taking medications, doing work around the house or garden, and managing money.  \ne  The European Depression Scale was used to define clinically depressive symptoms, as indicated by a total score of =4 symptoms in the 12-item validated questionnaire.  \nf  Overweight or obesity were determined by self-reported body weight in kilograms and height in meters. Body mass index (BMI) was calculated as kg/m 2  , and participants with a BMI of =25 were considered overweight or obese.  \ng  Smoking was assessed from self-reported use of cigarettes, cigars, or pipes during the year preceding the survey.  \nh  Physical inactivity was defined as the lack of weekly engagement in moderate-to-vigorous activities (per week) during the research period. Activities such as gardening or walking were considered moderate physical activities, whereas activities such as sports or heavy home labor were considered vigorous physical activity. Frequency was classified as less than once per week, once per week, 1 to 3 times per month, or hardly ever or never. Physical inactivity was defined as not engaging in any moderate-to-vigorous physical activity or having low frequency of physical activity (once per week, 1 to 3 times per month, or hardly ever or never).  \ni  Risky alcohol consumption was defined as the consumption of 4 or more servings of alcoholic beverages on at least 3 days a week during the 6 months preceding the survey.\n\n【48】#####  Table 2. Clustering of Behavioral Risk Factors According to Presence of Physical and Mental Health Components in European Adults (n = 26,026) Aged 50 Years or Older, Survey of Health, Ageing, and Retirement in Europe, 2004–2005\n\n| Characteristic | Number | Behavioral Risk Factors |\n| --- | --- | --- |\n| Weighted Mean (% CI) | Adjusted Odds Ratio a (% CI) b |\n| --- | --- |\n| 1 | \\=2 |\n| --- | --- |\n| **Men** | **Men** | **Men** | **Men** | **Men** |\n| Chronic diseases | 0 | 1.59 (.55–1.64) | Reference | Reference |\n| Chronic diseases | \\=1 | 1.68 (.65–1.71) | 1.28 (.01–1.62) | 1.52 (.20–1.91) |\n| Disease symptoms | 0 | 1.61 (.57–1.66) | Reference | Reference |\n| Disease symptoms | \\=1 | 1.67 (.64–1.70) | 0.99 (.78–1.25) | 1.17 (.93–1.47) |\n| (I)ADL limitations | 0 | 1.63 (.61–1.66) | Reference | Reference |\n| (I)ADL limitations | \\=1 | 1.73 (.67–1.79) | 1.36 (.88–2.12) | 1.65 (.10–2.48) |\n| Euro-D Scale score | <4 | 1.64 (.62–1.67) | Reference | Reference |\n| Euro-D Scale score | \\=4 | 1.67 (.61–1.72) | 1.21 (.86–1.72) | 1.26 (.91–1.75) |\n| Health status components | 0 | 1.59 (.52–1.65) | Reference | Reference |\n| Health status components | 1–2 | 1.64 (.61–1.67) | 1.05 (.80–1.38) | 1.27 (.98–1.66) |\n| Health status components | 3–4 | 1.74 (.67–1.80) | 1.13 (.66–1.92) | 1.68 (.01–2.78) |\n| Health status components | \\=1 | 1.66 (.64–1.69) | 1.04 (.80–1.36) | 1.29 (.99–1.68) |\n| **Women** | **Women** | **Women** | **Women** | **Women** |\n| Chronic diseases | 0 | 1.33 (.29–1.37) | Reference | Reference |\n| Chronic diseases | \\=1 | 1.48 (.46–1.51) | 1.30 (.07–1.58) | 1.73 (.42–2.12) |\n| Disease symptoms | 0 | 1.36 (.31–1.40) | Reference | Reference |\n| Disease symptoms | \\=1 | 1.47 (.44–1.49) | 1.20 (.98–1.47) | 1.39 (.10–1.72) |\n| (I)ADL limitations | 0 | 1.42 (.39–1.44) | Reference | Reference |\n| (I)ADL limitations | \\=1 | 1.51 (.47–1.55) | 1.43 (.04–1.98) | 1.79 (.30–2.43) |\n| European Depression Scale score | <4 | 1.44 (.41–1.46) | Reference | Reference |\n| European Depression Scale score | \\=4 | 1.44 (.40–1.48) | 1.01 (.80–1.27) | 1.09 (.87–1.36) |\n| Health status components | 0 | 1.32 (.26–1.38) | Reference | Reference |\n| Health status components | 1 or 2 | 1.39 (.36–1.41) | 1.18 (.92–1.50) | 1.34 (.04–1.74) |\n| Health status components | 3 or 4 | 1.55 (.51–1.58) | 1.36 (.95–1.95) | 1.86 (.28–2.69) |\n| Health status components | \\=1 | 1.45 (.43–1.47) | 1.21 (.95–1.54) | 1.43 (.11–1.84) |\n\n【50】Abbreviations: CI, confidence interval; (I)ADL, activities and instrumental activities of daily living.  \na  In relation to having 0 behavioral risk factors.  \nb  Confidence intervals are based on analysis of covariance and logistic regression analysis (using complex sample design procedure). In both methods, age, education, living with a partner or spouse, country region (north, central, south), self-rated health, income, and retirement status were used as covariates.\n\n【51】#####  Table 3. Chronic Diseases, Disease Symptoms, (I)ADL Limitations, and Depression Score According to Presence or Absence of Behavioral Risk Factors Among European Adults (n = 26,026) Aged 50 Years or Older, Survey of Health, Ageing, and Retirement in Europe, 2004–2005\n\n| Behavioral Risk Factor | Physical and Mental Health Status Components, Weighted Mean (Standard Error) a |\n| --- | --- |\n| Chronic Diseases | Disease Symptoms | (I)ADL Limitations | Euro-D Score |\n| --- | --- | --- | --- |\n| **Body weight b** | **Body weight b** | **Body weight b** | **Body weight b** | **Body weight b** |\n| Overweight or obese | 1.43 (.01) | 1.62 (.02) | 0.55 (.02) | 2.45 (.03) |\n| Normal c | 1.13 (.02) | 1.39 (.02) | 0.62 (.03) | 2.55 (.03) |\n| _P_ value | <.001 | <.001 | .07 | .03 |\n| **Smoking status d** | **Smoking status d** | **Smoking status d** | **Smoking status d** | **Smoking status d** |\n| Smoker | 1.23 (.02) | 1.55 (.03) | 0.57 (.03) | 2.51 (.05) |\n| Nonsmoker or former smoker | 1.32 (.01) | 1.53 (.01) | 0.57 (.02) | 2.49 (.02) |\n| _P_ value | .005 | .53 | .87 | .61 |\n| **Physically inactive e** | **Physically inactive e** | **Physically inactive e** | **Physically inactive e** | **Physically inactive e** |\n| Yes | 1.33 (.01) | 1.55 (.02) | 0.63 (.02) | 2.51 (.02) |\n| No | 1.26 (.02) | 1.47 (.02) | 0.44 (.02) | 2.43 (.04) |\n| _P_ value | .009 | .01 | <.001 | .09 |\n| **Alcohol consumption f** | **Alcohol consumption f** | **Alcohol consumption f** | **Alcohol consumption f** | **Alcohol consumption f** |\n| Risky drinker | 1.28 (.05) | 1.50 (.05) | 0.50 (.05) | 2.84 (.09) |\n| Nonrisky drinker | 1.31 (.01) | 1.53 (.01) | 0.58 (.02) | 2.47 (.02) |\n| _P_ value | .58 | .62 | .17 | .003 |\n\n【53】Abbreviations: Euro-D, European Depression Scale; (I)ADL, activities and instrumental activities of daily living.  \na  Comparisons were made by using analysis of covariance (according to the complex sample design procedure), with sex, age (y), education (y), living with a partner or spouse, country regions (north, central, south), self-rated health, income, and retirement status as covariates.  \nb  Overweight or obesity were determined by self-reported body weight in kilograms and height in meters. Body mass index (BMI) was calculated as kg/m 2  , and participants with a BMI of =25 were considered overweight or obese.  \nc  Normal weight was BMI<25.  \nd  Smoking was assessed from self-reported use of cigarettes, cigars, or pipes during the year preceding the survey.  \ne  Physical inactivity was defined as the lack of weekly engagement in moderate-to-vigorous activities (per week) during the research period. Activities such as gardening or walking were considered moderate physical activities, whereas activities such as sports or heavy home labor were considered vigorous physical activity. Frequency was classified as less than once per week, once per week, 1 to 3 times per month, or hardly ever or never. Physical inactivity was defined as not engaging in any moderate-to-vigorous physical activity or having a low frequency of physical activity (once per week, 1 to 3 times per month, or hardly ever or never).  \nf  A risky drinker was defined a person who consumed 4 or more servings of alcoholic beverages on at least 3 days per week during the 6 months preceding the survey. A nonrisky drinker was defined as a person who consumed fewer than 4 servings of alcoholic beverages 3 days a week.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f402d85e-e07a-4c5c-a1bd-cca4e2a1ecdc", "title": "Anthrax Postexposure Prophylaxis in Postal Workers, Connecticut, 2001", "text": "【0】Anthrax Postexposure Prophylaxis in Postal Workers, Connecticut, 2001\nOn November 20, 2001, _Bacillus anthracis_ was confirmed in blood cultures from a 94-year-old woman in rural Oxford, Connecticut, who was diagnosed with inhalational anthrax and died 1 day later . No obvious source of exposure to _B. anthracis_ was identified. She was the 22nd patient diagnosed with anthrax in the United States in 2001  . Before this case, all patients diagnosed with inhalational anthrax had had contact with intentionally contaminated mail delivered through the postal system, with the exception of a patient in New York City (where an investigation was under way). Since the source of transmission was identified as the mail for all but one anthrax case, investigation of area postal facilities began immediately.\n\n【1】The mail was considered a likely source of contamination for the patient in Connecticut, and postexposure antimicrobial prophylaxis was recommended for postal workers employed in the regional distribution center and local post office serving the patient’s area. At the regional postal distribution center, which operates 24 h a day and employs 1,122 workers, employees work one of three 8-h shifts and process approximately 3 million pieces of mail daily.\n\n【2】The regional processing center contains 29 high-speed sorting machines. In contrast, the local post office, a two-room structure with 48 employees, has no high-speed sorting machines. All mail collected in the local post office is sent to the regional processing center. The post office serves two zip code areas; mail to be delivered to the two zip codes is hand-sorted at the local level by carrier route.\n\n【3】The Connecticut Department of Public Health (CDPH), in consultation with the Centers for Disease Control and Prevention (CDC), recommended postexposure prophylaxis as a precaution to protect the health of the postal workers in these facilities  . As part of a national distribution center sampling protocol, an independent contractor working for the United States Postal Service (USPS) took environmental samples on November 11, but anthrax spores had not been isolated in the regional distribution center. The decision was made to offer prophylaxis to postal workers pending the results of additional, more focused testing.\n\n【4】The first of many postexposure prophylaxis clinics was held on November 21, 2001. Postal workers were given an initial 10-day course of ciprofloxacin unless contraindicated . Nasal swabs were collected from the postal workers at the first clinics to determine if contamination was present in the facilities, rather than to diagnose or define individual exposure  . _B. anthracis_ was not isolated from any of 485 nasal swabs taken from postal workers.\n\n【5】On November 21, 25, and 28 and December 2, increasingly focused environmental sampling was performed of both the regional distribution center and the local post office to determine whether any contaminated mail had passed through the facilities  . Samples obtained on November 21 and 25 were negative; samples taken on November 28 and December 2 from the four high-speed sorting machines in the regional distribution center were positive. No contamination was identified in the local post office. Based on the positive results, the CDPH recommended that prophylaxis be extended for a full course of 60 days for all postal workers in the regional facility. Facility management conducted a progressive series of town hall meetings to notify postal employees of the test results at the various facilities, as well as results of postal worker nasal swabs. Although contaminated sorting machines were shut down for machine-specific decontamination, the regional distribution facility remained opened.\n\n【6】Antimicrobial testing of the Connecticut patient’s isolates confirmed the sensitivity of this _B. anthracis_ strain to both doxycycline and ciprofloxacin. For the continuation phase of prophylaxis, doxycycline was offered as the primary antibiotic unless contraindications existed or the workers specifically requested to continue on ciprofloxacin.\n\n【7】On December 10, 2001, we conducted a survey to evaluate postal workers’ adherence to postexposure prophylaxis and to identify factors influencing their degree of adherence. This article describes the findings of the study.\n\n【8】### Methods\n\n【9】Of the 1,122 postal workers at the regional distribution center, we randomly selected 100 from the night and day shifts. Five workers declined; five additional workers were randomly selected and agreed to participate (refusal rate 5%). CDC health officials interviewed the group of postal workers using a standardized questionnaire to collect information on demographics, adherence, side effects, and attitudes regarding postexposure prophylaxis and exposure risk. Several characteristics were examined for determinants of starting prophylaxis, including sex, race, and age, as well as whether the postal worker worked on high-speed machinery or obtained an influenza vaccine. For comparison, age was divided into quartiles. The lowest quartile (age < 37 years) was compared with the top three quartiles, and the highest quartile (age \\> 52) was compared with the bottom three quartiles. Serious side effects were defined as those causing death, hospitalization, persistent or substantial disability, or birth defects, or requiring intervention to avoid these outcomes  . We conducted our analysis using SAS software, version 8.2 (SAS Institute, Inc. Cary, NC).\n\n【10】### Results\n\n【11】Of the 100 postal workers sampled, 66% were men. Mean age was 45 years (range 19–65 years). Ethnicities reported were Caucasian (%), African-American (%), Asian/Pacific Islander (%), and Hispanic (%). None of the respondents were pregnant. Fifteen employees worked on high-speed sorting machines. Forty-two postal workers reported obtaining an influenza vaccine during the previous 3 months.\n\n【12】Ninety-four of the 100 workers surveyed acquired antibiotics from postexposure prophylaxis clinics sponsored by the USPS; 6 workers did not attend the clinics. Of the 94 workers who acquired prophylaxis, only 68 started the antibiotics to prevent anthrax; therefore, of those surveyed, 32 postal workers did not initiate prophylaxis. Postal workers were given ciprofloxacin at initial prophylaxis clinics unless they reported contraindications. Of the 68 postal workers starting antibiotics, 54 persons started ciprofloxacin, 12 doxycycline, and 2 other antibiotics.\n\n【13】Characteristics of the persons who started prophylaxis versus those who did not are presented in Table 1 . Male postal workers were 1.5 times more likely to start prophylaxis than female postal workers (relative risk \\[RR\\] 1.52; 95% confidence interval \\[CI\\] 1.1 to 2.2; p<0.01). Persons who reported obtaining an influenza vaccine were more likely to start postexposure prophylaxis (RR 1.26; 95% CI 1.0 to 1.6; p=0.07), although this observation did not reach statistical significance. Working on high-speed sorting machines, race, and age were not predictors of starting prophylaxis.\n\n【14】We asked the 32 postal workers who never started postexposure prophylaxis to identify all reasons for declining prophylaxis  and to indicate the single most important reason. Nineteen (%) workers stated that they did not feel they were at personal risk for anthrax. Equal proportions of postal workers (%) cited negative nasal swabs of workers and concerns about side effects as reasons for not starting prophylaxis. Additional reasons included apprehension about antibiotic resistance, waiting to see if personal exposure had occurred, initial negative environmental samples, and fears that prophylaxis would weaken immune systems. When postal workers were asked to identify the single most important reason for not starting postexposure prophylaxis, 25% of workers reported not personally believing they were at risk for anthrax. An additional 13% cited concerns about side effects as the most important reason for not starting the regimen.\n\n【15】### Adherence to Postexposure Prophylaxis\n\n【16】Adherence to the prophylaxis regimen was examined in the 68 workers who started the prophylaxis. We grouped adherence by an average of how many days the worker reported being able to take antibiotic exactly as prescribed. Thirty-one (%) postal workers reported taking the prophylaxis regimen correctly every day; 23 (%) took antibiotics correctly 5–6 days per week; and 10 (%) of workers took antibiotics correctly <4 days per week. Adherence information was not available for four postal workers. Of those starting postexposure prophylaxis, 37 (%) persons reported missing doses. The top two reasons workers cited for missing a dose were forgetting to take the antibiotic (%) and side effects (%).\n\n【17】##### Reasons for Stopping Postexposure Prophylaxis\n\n【18】Twenty-one (%) of 68 postal workers had discontinued the prophylaxis regimen at the time of the survey. We asked these workers to identify all reasons for discontinuation  and to indicate the single most important reason why they stopped. Over half (%) of all who discontinued believed they were not at personal risk or did not believe they had been exposed to _B. anthracis._ Nine (%) cited side effects as a reason for stopping. Additional concerns were the initial negative environmental findings and the negative nasal swabs. When postal workers were asked to identify the single most important reason for discontinuing prophylaxis, 33% of postal workers reported experiencing side effects; 19% cited initial negative environmental samples from the facility; and 19% did not feel personally exposed.\n\n【19】##### Side Effects\n\n【20】After susceptibility testing of isolates was confirmed, postal workers were switched to doxycycline by USPS physicians, unless that switch was contraindicated; of 47 workers continuing antibiotics, 43 (%) were switched to doxycycline during the second round of prophylaxis clinics. Six (%) workers were switched because of side effects. At the time of the survey, postal workers had taken each medication for approximately the same number of days.\n\n【21】Equal numbers of postal workers surveyed took at least some ciprofloxacin (n=55) and some doxycycline (n=56). Twenty-three (%) postal workers experienced side effects while taking ciprofloxacin, with 22% reporting multiple symptoms. Twenty-one (%) postal workers experienced side effects while taking doxycycline, with 21% reporting multiple symptoms. Overall, 35 (%) of those who began postexposure prophylaxis experienced symptoms while on antibiotics.\n\n【22】Of side effects most frequently reported by postal workers for both antibiotics, the most common were gastrointestinal complaints . Diarrhea and abdominal pain were reported by 22% of workers on ciprofloxacin and 13% of workers on doxycycline. Nausea and vomiting were reported by 15% of the postal workers taking ciprofloxacin and 18% taking doxycycline. Fatigue was cited by 9% of the postal workers taking either drug. No significant differences between the proportions of postal workers reporting side effects while taking either medication were reported. No serious side effects were noted.\n\n【23】Only four persons missed work secondary to side effects of the prophylaxis (mean=1 day); only two physician visits for side effects occurred. No hospitalizations were reported.\n\n【24】### Discussion\n\n【25】The findings of this study extend the data on adherence with postexposure prophylaxis and substantiate other similar surveys  . Despite concerns about the safety of postal workers with potential exposures to _B. anthracis_ , our survey demonstrates that many workers did not take adequate prophylaxis. Adherence in this population was apparently affected by a low perceived risk for anthrax and a concern about side effects. Concern about side effects was present even before postal workers started taking antibiotics; 47% of the 32 workers who never started prophylaxis cited concern about side effects as a reason. Although many workers did experience side effects, the side effects they reported were not severe. In addition, many postal workers had difficulty taking their medications as prescribed, and they missed doses of prophylaxis.\n\n【26】Two factors may have contributed to the low perceived risk of inhalational anthrax among postal workers. First, results from the first three efforts to collect samples at the postal facilities and the nasal swabs taken at the onset of the investigation were negative for anthrax spores. Second, postal, medical, and union leaders providing information on environmental sampling results and their interpretation at USPS town meetings tried to put the risk in the perspective as explained to them by the Department of Public Health. Overall, the data suggested a possible, but not high, risk for inhalational anthrax. Spores were likely introduced in mid-October before the New Jersey and Washington D.C. regional distribution centers that handled the contaminated Daschle and Leahy letters closed down. Use of compressed air to clean sorting machines, which might have caused aerosolization of spores, had ceased by October 23, when a general USPS advisory against it was circulated. Maximum risk of exposure to aerosolized spores likely occurred during that time. By the time the postexposure prophylaxis clinics began, 30–40 days had passed since the maximum risk period without the occurrence of any cases of inhalational anthrax in regional facility workers. In addition, the initial samples taken on November 11 and 21, with methods that readily identified spores in New Jersey and Washington, D.C. had failed to identify any spores. These factors were discussed during town meetings in an effort to reassure postal workers, while still emphasizing that a period did occur when spores were in the air, especially around the sorting machines.\n\n【27】In this setting, the numbers of postal workers who accepted antibiotics could not be used as a measure for the numbers of postal workers who actually took prophylaxis. Anecdotally, many postal workers reported obtaining the antibiotics to “have on hand” in the event “I start to feel sick.” The postexposure prophylaxis survey was critical in determining the level of adherence and identifying issues affecting adherence in this population.\n\n【28】The circumstances of this prophylaxis campaign, along with the small sample size and potential for recall bias associated with this survey, limit the inferences that may be drawn. For example, some misclassification of side effects as doxycycline- or ciprofloxacin-related may have accompanied the switch in medications. In addition, the study size limits any speculation as to the nature of the relationship between being men and starting prophylaxis. Larger postexposure prophylaxis surveys may identify the reason for this and other associations that were not significant in our analysis. Nonetheless, the survey provided important information on adherence to prophylaxis and reasons for nonadherence.\n\n【29】In the event of another bioterrorism attack, public health officials must communicate, early and effectively, the need for potentially exposed persons to initiate and continue postexposure prophylaxis. Specifically, officials should clearly communicate to at-risk persons the explanation that epidemiologic tools such as nasal swabs are poor indicators of past personal exposure and are, at best, indicators only of recent exposure. While important, reassurance must be balanced with clear explanations of risk. Of note in our study is the fact that the one group deemed to be at higher risk—those working on high-speed mail sorting machines—was found no more likely to begin or continue on prophylaxis than persons working elsewhere in the facility.\n\n【30】Potentially exposed persons need to be aware that side effects are to be expected, but that the vast majority of side effects will be mild. Education should center on how to recognize and minimize minor side effects while describing which side effects require immediate medical assistance. Amelioration of side effects is essential if persons are to stay on their regimens, especially if the time period is lengthy. In addition, antibiotic reminder programs such as signs in common areas or buddy systems may improve adherence to postexposure prophylaxis.\n\n【31】In conclusion, if public health officials deem initiating prophylaxis programs necessary, conducting frequent follow-up surveys to measure adherence and identify obstacles to prophylaxis in a specific population will be important in identifying perception problems and maximizing the benefits of preventive therapy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4f45a5a9-c21b-42c6-ac59-691d23eaeba4", "title": "Availability and Promotion of Healthful Foods in Stores and Restaurants ― Guam, 2015", "text": "【0】Availability and Promotion of Healthful Foods in Stores and Restaurants ― Guam, 2015\nAbstract\n--------\n\n【1】Chronic disease, which is linked to unhealthy nutrition environments, is highly prevalent in Guam. The nutrition environment was assessed in 114 stores and 63 restaurants in Guam. Stores had limited availability of some healthier foods such as lean ground meat (.5%) and 100% whole-wheat bread (.4%), while fruits (.0%) and vegetables (.8%) were more commonly available; 43.7% of restaurants offered a healthy entrée or main dish salad, 4.1% provided calorie information, and 15.7% denoted healthier choices on menus. Improving the nutrition environment could help customers make healthier choices.\n\n【2】Objective\n---------\n\n【3】Guam, a US Affiliated Pacific Island, has a similarly high prevalence of self-reported obesity and chronic disease as the continental United States; 31.6% of adults have obesity, 32.0% have hypertension, and 12.0% have diagnosed diabetes . The nutrition environment — the availability, pricing, and promotion of foods in stores and restaurants — may contribute to chronic disease . The Guam Department of Public Health and Social Services (DPHSS) and its partners outlined chronic disease prevention strategies, including increasing fruit and vegetable intake and decreasing salt consumption . To guide these strategies, we conducted assessments of the nutrition environment in stores and restaurants.\n\n【4】Methods\n-------\n\n【5】A sampling frame of 607 stores and 711 restaurants was developed using business listings from the Guam DPHSS, Division of Environmental Health, Supplemental Nutrition Assistance Program, and the telephone directory. A regionally stratified, disproportionate allocation sampling method was used to randomly select 114 stores, classified as large stores (≥2 cash registers, n = 37) and small stores (cash register, n = 77), and 63 restaurants (sit-down, 20 fast-casual/fast-food). Stores were surveyed if they were open to the public, had a permanent or nonmobile structure, and sold at least 3 of 5 staple foods (milk, bread, eggs, meat or fish, or produce). Three large stores, where many residents shopped, were deliberately sampled. Restaurants were included if they met the first 2 criteria and offered 5 or more breakfast, lunch, or dinner entrées.\n\n【6】The Nutrition Environment Measures Survey (NEMS) is a validated assessment tool for stores (NEMS–S)  and restaurants (NEMS–R) . Being a US territory, Guam has food options in restaurants and stores that are similar to those in the continental United States. Additionally, NEMS was further adapted for Guam through a literature review on local dietary patterns , consultation with local dietitians, and field testing of the survey tools. In September 2015, surveyors used NEMS to assess food options through on-site observations and menu reviews. In stores, NEMS defined healthier options as fruits and vegetables, reduced-fat milk, low-calorie beverages, whole grains, lean meats, and reduced-fat condiments or snacks. Because many stores lacked visible prices, a few commonly available staple foods were preselected for pricing. Within-store price comparisons were made between healthier items and their regular counterparts: reduced-fat (≤2%) milk versus whole milk and brown rice versus white rice. Price comparisons were made between large and small stores for 4 healthier items: bananas, cabbage, reduced-fat milk, and brown rice. In restaurants, healthy dishes were defined based on calorie and fat content or menu icons denoting healthy items. Within-restaurant price comparisons were made for the least expensive healthy entrées and sides versus the least expensive less-healthy entrées and sides. We used χ 2  tests to examine differences in availability and _t_ tests for pricing comparisons (significant at _P_ < .05). Analyses were weighted to account for survey design.\n\n【7】Results\n-------\n\n【8】Stores had limited availability for several healthier items like 100% whole-wheat bread (.4%), whole-grain cereal with less than 7 g of sugar per serving (.4%), lean ground meat with 10% or less fat (.5%), and 1% fat/skim milk (.6%) . Availability was greater for other items like fruits (.0%) and vegetables (.8%), including fresh fruits (.3%) and fresh vegetables (.8%). Among stores that sold fresh fruits and vegetables, variety was somewhat limited; 46.9% sold more than 2 varieties of fresh fruits, and 57.9% sold more than 2 varieties of fresh vegetables. For the majority of healthier foods, availability was greater in large stores than in small stores. There was no difference in the average price of reduced-fat milk ($5.94 per 0.5 gallon) versus whole milk ($5.98 per 0.5 gallon), or brown rice ($6.35 per 5 lb) versus white rice ($6.46 per 5 lb). Small stores had significantly higher prices than large stores for bananas ($1.82 vs $1.44 for 1 lb), reduced-fat milk ($6.06 vs $5.76 for 0.5 gallon), and brown rice ($6.78 vs $6.01 for 5 lb). Stores more commonly promoted less-healthy eating (.1%) than healthy eating (.7%) through store signage.\n\n【9】Although only 12.6% of restaurants offered 1 or more healthy entrées on the adult menu, 39.4% had a healthy main salad . Combined, 43.7% offered a healthy entrée or main dish salad. Over half (.8%) offered free refills of sugar-sweetened beverages. Thirty-three percent of restaurants had a kids’ menu; of these, 62.8% offered 1 or more healthy entrées and 48.8% provided a healthy beverage by default. Calorie information was available in 4.1% of restaurants, whereas 15.7% had menu icons denoting healthier dishes. The only significant difference in pricing was found in sit-down restaurants, where the least expensive healthy entrée cost $16.53 on average versus $13.86 for the least expensive less-healthy entrée. Restaurants more commonly used signs and displays to encourage unhealthy eating (.1%) than healthy eating (.2%).\n\n【10】Discussion\n----------\n\n【11】Availability of healthier foods in stores varied across items, with some healthier options having limited availability. Small stores generally had less availability and higher prices for healthier foods than did large stores. In restaurants, availability of healthier foods was limited and nutrition information was generally unavailable. Promotional materials in stores and restaurants more commonly encouraged unhealthy eating. These findings are similar to the findings of surveys in the continental United States  and American Samoa , which found more limited availability of healthier options in small stores. Findings from restaurants in Guam are consistent with findings from a restaurant survey in Minnesota, which found limited availability of healthy entrées and nutrition information .\n\n【12】This study is the only nutrition environment assessment of its kind in Guam. By assessing both stores and restaurants, the survey provided a comprehensive picture of the nutrition environment on the island. However, this assessment has limitations. Challenges in creating an accurate sampling frame because of business closures, name changes, and venue duplication may have influenced sampling weights. Store price comparisons were restricted by the absence of displayed prices in most small stores. Additionally, the assessment of healthy entrées in restaurants was limited by the lack of nutrition information or menu icons denoting healthy items in most restaurants.\n\n【13】These findings can be used to develop strategies to help customers choose healthier options. Stores and restaurants could increase the availability of healthy foods and promote the healthy foods they offer. Promotion strategies could include encouraging healthy eating through signage, providing menu icons to denote healthy dishes in restaurants, assigning healthier beverages as the default for kids’ menus, or placing healthier foods in more prominent locations in stores. Improving the availability, pricing, and promotion of healthier options in stores and restaurants could help efforts to increase consumption of healthful foods in Guam.\n\n【14】Tables\n------\n\n【15】#####  Table 1. Availability of Healthier Food Options and Promotion of Healthier Choices and Less-Healthy Choices, by Store Size — Guam,\n\n| Survey Item | All Stores, % a | By Store Size |\n| --- | --- | --- |\n| Large Stores, b % a | Small Stores, c % a |\n| --- | --- |\n| **Availability of Food Item** | **Availability of Food Item** | **Availability of Food Item** | **Availability of Food Item** |\n| **Grains (n = 110) d** | **Grains (n = 110) d** | **Grains (n = 110) d** | **Grains (n = 110) d** |\n| 100% whole-wheat bread | 11.4 | 30.3 e | 1.3 e |\n| Brown rice | 45.3 | 80.7 e | 26.3 e |\n| Whole-grain cereal (<7 g sugar per serving) | 25.4 | 57.6 e | 8.2 e |\n| **Meat (n = 113) d** | **Meat (n = 113) d** | **Meat (n = 113) d** | **Meat (n = 113) d** |\n| Lean ground meat (≤10% fat) | 7.5 | 21.9 e | 0.0 e |\n| Reduced-fat hot dogs (≤9 g fat per serving) | 27.5 | 47.4 e | 17.2 e |\n| Fish (frozen or fresh, not breaded) | 60.3 | 91.6 e | 44.0 e |\n| Canned tuna in water | 58.8 | 82.6 e | 46.4 e |\n| Reduced-fat Spam (≤8 g fat per serving) | 75.3 | 85.3 e | 70.1 e |\n| **Fruits and vegetables (n = 114) d** | **Fruits and vegetables (n = 114) d** | **Fruits and vegetables (n = 114) d** | **Fruits and vegetables (n = 114) d** |\n| Any fruit | 81.0 | 93.9 e | 74.1 e |\n| Fresh fruit | 65.3 | 92.7 e | 50.7 e |\n| \\>2 varieties of fresh fruit (n = 71) d,f | 46.9 | 71.6 e | 22.9 e |\n| Fruit canned in water or 100% juice | 61.5 | 80.7 e | 51.3 e |\n| Frozen fruit | 14.0 | 40.3 e | 0.0 e |\n| Any vegetable | 94.8 | 96.5 | 94.0 |\n| Fresh vegetables | 62.8 | 91.7 e | 47.1 e |\n| \\>2 varieties of fresh vegetables (n = 70) d,f | 57.9 | 81.0 e | 34.1 e |\n| Vegetables canned without sauce | 94.8 | 96.5 | 94.0 |\n| Frozen vegetables | 71.1 | 85.7 e | 63.3 e |\n| **Beverages (n = 108) d** | **Beverages (n = 108) d** | **Beverages (n = 108) d** | **Beverages (n = 108) d** |\n| Low-fat (%) or skim milk | 20.6 | 49.2 e | 6.0 e |\n| Diet soda | 97.4 | 100.0 | 96.1 |\n| 100% fruit juice | 89.7 | 96.2 e | 86.4 e |\n| Water | 100.0 | 100.0 | 100.0 |\n| **Condiments or snacks (n = 112) d** | **Condiments or snacks (n = 112) d** | **Condiments or snacks (n = 112) d** | **Condiments or snacks (n = 112) d** |\n| Coconut milk (≤4.5 g fat per serving) | 5.6 | 15.8 e | 0.0 e |\n| Salad dressing (≤3 g fat per serving) | 23.1 | 40.8 e | 13.4 e |\n| Baked chips (≤3 g fat per serving) | 18.6 | 26.2 | 14.4 |\n| **Messages and Practices** | **Messages and Practices** | **Messages and Practices** | **Messages and Practices** |\n| **Healthy promotion and placement practices (n = 112) g** | **Healthy promotion and placement practices (n = 112) g** | **Healthy promotion and placement practices (n = 112) g** | **Healthy promotion and placement practices (n = 112) g** |\n| Promotion of healthy eating through signs or displays h | 10.7 | 21.5 e | 4.8 e |\n| Healthier foods present at the point of purchase | 7.6 | 9.8 | 6.4 |\n| Healthier foods present at the ends of aisles | 20.8 | 27.0 | 17.5 |\n| **Less-healthy promotion and placement practices (n = 112) g** | **Less-healthy promotion and placement practices (n = 112) g** | **Less-healthy promotion and placement practices (n = 112) g** | **Less-healthy promotion and placement practices (n = 112) g** |\n| Promotion of less-healthy eating through signs or displays i | 57.1 | 69.9 e | 50.1 e |\n| Less-healthy foods present at the point of purchase | 96.6 | 96.4 | 96.7 |\n| Less-healthy foods present at the ends of aisles | 91.5 | 91.2 | 91.6 |\n\n【17】a  Weighted percentage of stores.  \nb  ≥2 Cash registers.  \nc  1 Cash register.  \nd  Unweighted sample size. For each group of foods, the analysis was limited to stores that had information for all foods in the group. Within any group, some stores were missing data for 1 or more of the foods.  \ne  Indicates significant difference at _P_ < .05 between large stores versus small stores.  \nf  Among stores selling fresh fruit or vegetables, variety was defined as different types of fruits or vegetables (eg, apples, bananas, pears).  \ng  Unweighted sample size.  \nh  Signs or displays that promoted healthy eating, including the consumption of fresh or frozen fruits and vegetables, 100% whole-wheat bread, brown rice, whole-grain cereals that are low in sugar, healthy protein such as lean meat (chicken or fish) or beans, grilled chicken or fish rather than fried, fat-free or low-fat dairy products (milk, yogurt, cheese), water or 100% fruit juice, or healthier versions of snack foods (eg, baked chips rather than fried).  \ni  Signs or displays that promoted unhealthy eating, including the consumption of sugar-sweetened beverages, fried foods, foods high in sugar (eg, candy, cookies, sugary cereals), foods high in salt (eg, fried chips), or baked goods high in fat and sugar.\n\n【18】#####  Table 2. Availability of Healthy and Regular Food Options and Promotion of Healthy and Less-Healthy Choices, by Restaurant Type — Guam, 2015 a  \n\n| Survey Item | All Restaurants (n = 63), % | By Restaurant Type |\n| --- | --- | --- |\n| Sit-Down (n = 43), % | Fast-Casual/Fast-Food (n = 20), % |\n| --- | --- |\n| **Availability** | **Availability** | **Availability** | **Availability** |\n| **Entrées and salads on adult menu** | **Entrées and salads on adult menu** | **Entrées and salads on adult menu** | **Entrées and salads on adult menu** |\n| ≥1 healthy entrées b | 12.6 | 9.1 | 20.9 |\n| ≥1 healthy main salads b | 39.4 | 35.5 | 48.9 |\n| ≥1 healthy entrées or main salads | 43.7 | 39.9 | 53.0 |\n| ≥1 healthier-preparation entrées c | 92.7 | 90.1 | 99.3 |\n| **Sides and healthy alternatives on adult menu** | **Sides and healthy alternatives on adult menu** | **Sides and healthy alternatives on adult menu** | **Sides and healthy alternatives on adult menu** |\n| ≥1 healthy fruit side d | 29.6 | 37.0 | 11.4 |\n| ≥1 healthy vegetable side e | 61.5 | 72.5 f | 34.8 f |\n| Baked chips (n = 14) g | 9.0 | 0.0 | 14.4 |\n| 100% whole-wheat bread (n = 40) g | 5.6 | 8.0 | 0.0 |\n| Brown rice (n = 52) g | 11.6 | 11.1 | 12.9 |\n| **Beverages** | **Beverages** | **Beverages** | **Beverages** |\n| ≥1 healthier or low-calorie beverages h | 100.0 | 100.0 | 100.0 |\n| ≥1 sugar-sweetened beverages i | 96.9 | 95.6 | 100.0 |\n| Free refills on sugar-sweetened beverages | 58.8 | 61.2 | 53.0 |\n| Low-fat (≤1%), unflavored milk | 9.8 | 4.5 | 22.8 |\n| **Kids’ menu** | **Kids’ menu** | **Kids’ menu** | **Kids’ menu** |\n| Kids’ menu available (n = 63) | 32.7 | 39.0 | 17.5 |\n| ≥1 healthy entrées (n = 20) j,k | 62.8 | 59.6 | 80.5 |\n| ≥1 healthy sides (n = 20) j,l | 77.8 | 77.3 | 80.5 |\n| ≥1 healthy desserts (n = 20) j,m | 12.0 | 0.0 | 76.4 |\n| Healthier drinks h are default beverage (n = 18) n | 48.8 | 44.9 | 65.1 |\n| Free refills on unhealthy drinks (n = 20) j,i | 50.2 | 56.7 | 15.4 |\n| **Menu labeling** | **Menu labeling** | **Menu labeling** | **Menu labeling** |\n| Calorie content information | 4.1 | 0.0 | 14.1 |\n| Fat content information | 3.9 | 0.0 | 13.4 |\n| Calorie and fat content information | 3.9 | 0.0 | 13.4 |\n| “Healthy” menu icons or lighter fare section | 15.7 | 9.7 | 30.2 |\n| Fat and calories content information or “healthy” menu icons or lighter fare section | 18.8 | 9.7 | 40.9 |\n| **Messages and Practices** | **Messages and Practices** | **Messages and Practices** | **Messages and Practices** |\n| **Messages promoting healthy choices o** | **Messages promoting healthy choices o** | **Messages promoting healthy choices o** | **Messages promoting healthy choices o** |\n| Highlight healthy menu options | 17.7 | 11.4 | 32.9 |\n| Encourage healthy eating p | 19.2 | 14.7 | 30.2 |\n| Promote free refills on healthier or low-calorie drinks | 11.0 | 9.9 | 13.4 |\n| **Messages promoting less-healthy choices** o | **Messages promoting less-healthy choices** o | **Messages promoting less-healthy choices** o | **Messages promoting less-healthy choices** o |\n| Encourage unhealthy eating q | 29.1 | 16.7 f | 59.1 f |\n| Encourage overeating | 4.7 | 4.4 | 5.4 |\n| Promote free refills on sugary drinks | 14.9 | 19.9 | 2.7 |\n\n【20】a  Unweighted sample size and weighted percentage are presented.  \nb  Defined using Nutrition Environment Measures Survey (NEMS) calorie and fat criteria, or healthy icons or lighter fare sections on menu. If calorie and fat information were available, an entrée or main dish salad could be considered healthy if it had ≤800 calories (≤650 calories for burgers or sandwiches), ≤30% of calories from fat, and ≤10% of calories from saturated fat. If calorie and fat information were not available, a salad could still be considered healthy if it had ≤2 high-fat ingredients, and if low-fat (≤3 g of fat/serving) or fat-free dressing (g of fat per serving) was available, or if dressing could be ordered on the side.  \nc  Defined as poultry, fish, or main dish vegetables prepared using healthier methods (eg, grilled or baked).  \nd  Fruit with no added sugar, syrup, glaze, or sauce.  \ne  Nonfried vegetables with no added sauce. Side salads counted as a healthy vegetable side if the restaurant had low-fat or fat-free dressing (or dressing could be ordered on the side).  \nf  Significant difference at _P_ < .05 between sit-down and fast-casual/fast-food.  \ng  Calculated only for restaurants that served chips, bread, or rice, respectively.  \nh  Healthier beverages: diet soda, water, 100% fruit juice, unsweetened tea or coffee, and unflavored low-fat milk.  \ni  Sugar-sweetened beverages: soda, juice drink, flavored milk, sweetened tea, and energy or sports drinks.  \nj  Calculated only for the 20 restaurants with a kids’ menu.  \nk  Defined by preparation method (ie, grilled or baked rather than fried), “healthy” menu icons or “lighter fare” sections, and other criteria (ie, not prepared with red meat, cheese, or cream sauce).  \nl  Fruit without added sugar, nonfried vegetables without added sauce, rice, salad, beans, low-fat yogurt, cottage cheese, or applesauce.  \nm  Low-fat yogurt or fruit without added sugar.  \nn  This question was only applicable for 18 restaurants that had a default beverage assigned to the kids’ menu.  \no  Messages on signs, table tents, and displays.  \np  Messages encourage healthy eating; for example, by choosing fruits and vegetables, reduced fat menu options, baked or grilled foods rather than fried foods, or brown rice instead of white rice.  \nq  Messages encourage unhealthy eating; for example, by choosing desserts that are high in calories or fat, fried foods, or sugar-sweetened beverages.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0ed3f5fa-eb6f-4182-a335-d8bff549402c", "title": "Molecular Confirmation of Rickettsia parkeri in Amblyomma ovale Ticks, Veracruz, Mexico", "text": "【0】Molecular Confirmation of Rickettsia parkeri in Amblyomma ovale Ticks, Veracruz, Mexico\n_Amblyomma ovale_ hard ticks are located predominantly in South and Central America but can also be found in areas of the nearctic, particularly Mexico and the southern United States . Immature stages of this species parasitize many mammal and bird species, and adults complete their life cycle on artiodactyls and carnivores, particularly canids . _A_ . _ovale_ ticks have been collected predominantly in sylvatic areas, but because free-roaming dogs often enter sylvatic habitats and return to peridomestic settings with attached ticks, these ticks have become distributed into transitional and rural environments . In Brazil, this species has been implicated as the main vector of the _Rickettsia parkeri_ strain Atlantic Rainforest, an eschar-associated spotted fever pathogen . Since its discovery, strain Atlantic Rainforest has been detected in other hard tick species, including _A_ . _aureolatum_ and _Rhipicephalus sanguineus_ sensu lato in Argentina, Colombia, and Belize .\n\n【1】In Mexico, _A_ . _ovale_ ticks have been collected from 8 species of mammals in 10 of 32 states . Despite the wide distribution of _A_ . _ovale_ ticks in Mexico, attempts to identify _R_ . _parkeri_ strain Atlantic Rainforest in this species are lacking.\n\n【2】During July–August 2018, we collected _A_ . _ovale_ ticks from dogs in 3 municipalities, Alvarado (°46′52′′N, 95°45′26′′W), Catemaco (°30′36.30′′N, 95°02′08.61′′W), and Martínez de la Torre (°04′00′′N, 97°03′00′′W), in the state of Veracruz, Mexico . Ticks were harvested from owned dogs during their evaluations at veterinary clinics and from free-roaming dogs during vaccination campaigns conducted by local rabies vaccination programs. We identified ticks morphologically using a standard taxonomic key , fixed them in absolute ethanol, and stored them at 4°C.\n\n【3】To extract DNA, we used the Cheelex-100 protocol as previously reported . To evaluate the DNA quality of samples, we amplified a 400-bp segment of the ixodid 16S rRNA gene . We screened DNA extracts for _Rickettsia_ species using a PCR targeting an 800-bp segment of the citrate synthase (gltA_ ) gene. With _gltA_ \\-positive samples, we performed PCRs amplifying segments of the _htrA_ (bp), _sca0_ (bp), and _sca5_ (bp) genes . We purified PCR products using Agencourt AMPure XP  and sequenced amplicons on the ABI 3730xL DNA Analyzer  at the Sequencing Unit of the National Institute of Genomic Medicine (Mexico City, Mexico). We generated consensus sequences using Geneious 2019.1.3  and compared these sequences with those of validated _Rickettsia_ species deposited in GenBank using the blastn tool . We performed global alignments using ClustalW , concatenated sequences in BioEdit , and then constructed phylogenetic trees in MEGA 6.0  using the maximum-likelihood method and 10,000 bootstrap replicates.\n\n【4】We collected 22 adult (female, 6 male) _A_ . _ovale_ ticks from 6 dogs (tick density of 2–5 ticks per dog). We could amplify ixodid 16S sequences from all samples. We sequenced the 16S gene of 1 female  and 1 male tick, and both exhibited 99.5% (/406 bp) sequence identity with sequences of _A_ . _ovale_ ticks from Colombia (GenBank accession nos. MF353104.1–5.1). Six (.3%) specimens tested positive for _Rickettsia_ DNA, including 1 female specimen from Alvarado, 2 female specimens from Martínez de la Torre, and 2 female specimens and 1 male specimen from Catemaco. The _gltA_ , _htrA_ , _sca0_ , and _sca5_ gene segments could be amplified for all 6 samples. Each gene segment was 99%–100% identical to that of the _R_ . _parkeri_ strain Atlantic Rainforest from Brazil and Argentina . Phylogenetic analysis corroborated the presence of 2 _R_ . _parkeri_ strain Atlantic Rainforest haplotypes: 1 for the northern region (Martínez de la Torre; GenBank accession nos. MK844821, MK844823, MK844825, MK844827) and 1 for the central and southern regions (Alvarado and Catemaco; GenBank accession nos. MK844820, MK844822, MK844824, MK844826) of Veracruz. With a bootstrap value of 100, both haplotypes clustered in a clade comprising other _R_ . _parkeri_ strains.\n\n【5】Our findings document _R_ . _parkeri_ strain Atlantic Rainforest farther north than previous reports . The discovery of this pathogen in ticks associated with dogs in different localities of Veracruz has implications for public health safety. In this state, the Ministry of Health reported 22 cases of spotted fever during 2015–2017 . _R. rickettsii_ , the etiologic agent of Rocky Mountain spotted fever, has been previously described in _A_ . _mixtum_ (formerly _A_ . _cajennense_ ) ticks collected from Veracruz , suggesting the potential for co-circulation of _R_ . _rickettsii_ and _R_ . _parkeri_ in ticks in this state. Two other _R_ . _parkeri_ lineages have been detected circulating in Mexico: _R_ . _parkeri_ strain black gap in the rabbit tick (Dermacentor parumapertus_ ) in Sonora and Chihuahua  and _R_ . _parkeri_ sensu stricto associated with _A_ . _maculatum_ ticks . These findings emphasize the need for enhanced surveillance studies of these rickettsia in Mexico to better elucidate the evolutionary, ecologic, and public health relevance of the various _R_ . _parkeri_ strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6631e7f7-f6e1-405a-9d5f-b86569ff7fa9", "title": "Antigenic and Genetic Variability of Human Metapneumoviruses", "text": "【0】Antigenic and Genetic Variability of Human Metapneumoviruses\nHuman metapneumovirus (HMPV) has recently been identified as a causative agent of respiratory tract illnesses in humans worldwide  and is a member of the _Pneumovirinae_ subfamily within the _Paramyxoviridae_ family . The _Pneumovirinae_ subfamily consists of two genera: the pneumoviruses and the metapneumoviruses. Human respiratory syncytial virus (HRSV), the major viral cause of severe respiratory tract illnesses in children, is the type species of the pneumoviruses . Avian pneumovirus (APV), the causative agent of respiratory tract illnesses in turkeys and chickens , was the sole member of the _Metapneumovirus_ genus until the discovery of HMPV .\n\n【1】For most pneumoviruses, different subgroups or subtypes have been identified. For HRSV, two subgroups have been identified on the basis of differences in nucleotide sequences, reactivity patterns with monoclonal antibodies, and in vitro neutralization assays with subgroup-specific antisera . Additional genotypes have been identified within subgroups, largely on the basis of the high variability of the attachment protein gene . The fusion (F) and the attachment (G) proteins are the main targets for the neutralizing and protective antibody response , with F being one of the most conserved proteins and G the most variable . For APV, two different subgroups (A and B) have been defined on the basis of nucleotide sequences of the G protein and neutralization tests by using monoclonal antibodies that also recognize the G protein, but these subgroups belonged to one serotype . APV type C, a possible second serotype, was identified based on the lack of cross-reactivity with antisera specific for groups A and B, and the nucleotide sequences also proved to be substantially different from strains belonging to group A or B . In addition, subgroup D may exist, which contains isolates from France that are not neutralized by monoclonal antibodies raised against viruses belonging to either subgroup A, B, or C .\n\n【2】For HMPV, two major genetic lineages have been identified worldwide on the basis of analysis of a limited set of sequences . One feature of HMPV that poses a challenge in developing a future vaccine is that infections may occur in the presence of preexisting immunity. Very young children (<1 year) have been infected by the virus, and reinfections have also been demonstrated . HMPV might cause repeated infections throughout life, similar to HRSV, which could be either due to incomplete immunity or to genetic heterogeneity of the virus.\n\n【3】To develop vaccines, the extent of genetic and antigenic variability of the different HMPV transmembrane glycoproteins must be understood. We analyzed the genetic diversity of HMPV by phylogenetic analyses of sequences obtained for part of the F (n = 84) and the complete G open reading frames (ORFs) (n = 35). In addition, we addressed the antigenic relationship between the different lineages with virus neutralization assays using lineage-specific antisera raised in ferrets. Virologic studies have used a definition of a homologous-to-heterologous virus neutralization titer ratio of >16 as a definition for serotypes . On the basis of our results and the described definition, we now define the two major lineages of HMPV as serotype A and B. In accordance with the definition and our results, the sublineages within each serotype are not identified as different serotypes. At least two serotypes of HMPV are present in the human population, a finding that has implications for developing intervention strategies, such as immunization and vaccination.\n\n【4】### Materials and Methods\n\n【5】##### Sample Collection, RNA Isolation, RT-PCR Assays, and Sequencing\n\n【6】HMPV-positive nasopharyngeal aspirate samples were obtained from different cohort studies: 61 samples from the Netherlands, 11 samples from Finland, 8 samples from England, 1 from Hong Kong, and 2 from Brazil. Clinical samples had been obtained from 1981 to 2002. Samples were obtained from young children, infants, adults, the elderly, and immunocompromised persons, who had mild to severe respiratory tract illnesses. Epidemiologic and clinical data for most isolates have been described elsewhere .\n\n【7】Similar to the influenza nomenclature, sequences are identified by country of origin, identification number, and year of isolation. RNA isolation was performed as described previously . cDNA was synthesized at 42°C for 60 min with random hexamer primers (Promega, Leiden, the Netherlands) and superscript II RNase H-reverse transcriptase (RT) (Invitrogen, Merelbeke, Belgium). An aliquot of cDNA was used in a polymerase chain reaction (PCR) assay to amplify the full-length G ORF or a fragment of the F ORF. Primers: SH7: 5′- TACAAAACAAGAACATGGGACAAG-3′ and SH-8 5′-GAGATAGACATTAACAGTGGATT-3′ (G ORF), BF100 5′-CAATGCAGGTAT AACACCAGCAATATC-3′, and BF101 5′-GCAACAATTGAACTGATCTTCAGGAAAC-3′ (F ORF). Thermocycling was performed under the following conditions: 94°C for 1 min, 40°C for 2 min, 72°C for 3 min (cycles). When necessary, a nested PCR was performed by using 5 μL of PCR product with primers SH7 and SH8 for the G ORF or primers BF103 5′-ACATGCCAACATCTGCAGGACAAA TAAAAC-3′ and BF104 5′-ACATGCTGTTCACCTTCAACTTTGC-3′ for the F ORF. PCR products were sequenced directly on both strands with multiple primers as described previously . When identical sequences were obtained (suspicious of laboratory contamination) and to confirm sequence uncertainties such as frame shifts, we repeated the RNA isolation, RT-PCR, and subsequent sequencing with the original materials.\n\n【8】##### Phylogenetic Analysis\n\n【9】Nucleotide sequences were aligned with the Clustal W program running within the Bioedit software package, version 5.0.9. Maximum likelihood trees were generated with the Seqboot and Dnaml packages of Phylip version 3.6 by using 100 bootstraps and 3 jumbles. The consensus tree was calculated by using the Consense package of Phylip 3.6 and was subsequently used as usertree in Dnaml to recalculate the branch lengths from the nucleotide sequences. Finally, the trees were rerooted at midpoint by using the Retree software of Phylip 3.6. Trees were visualized with the Treeview 1.6.6 program distributed with Bioedit version 5.0.9 . Sequences are available from GenBank under accession no. AY295930 to AY296012 (F partial) and AY304360 to AY304362 (complete F for NL/17/00, NL/1/99, and NL/1/94, respectively), AF371337 (complete genome NL/1/00), and AY296014 to AY296047 (complete G regions).\n\n【10】##### Virus Preparations and Titrations\n\n【11】Viruses were isolated on tertiary monkey kidney (tMK) cells as previously described . For each genetic lineage a prototype virus isolate was chosen on the basis of its ability to grow to high titers on tMK cells and to reflect the specific genotype for the lineage. Virus titrations were cultured for 7 days, and infected wells were identified by immune fluorescence assays (IFA) with HMPV-specific polyclonal antiserum raised in guinea pigs. Titers were expressed in 50% tissue culture infectious dose (TCID 50  ).\n\n【12】##### Antisera\n\n【13】Lineage-specific polyclonal HMPV antisera were raised by infecting ferrets with 1 mL of virus-infected tMK supernatants containing approximately 10 4  –10 5  TCID 50  virus. All infections were performed in duplo, and the animals with the highest antibody responses are shown. Serum samples were collected at days 0 and 28 postinfection (ferret 1 and 2) or at days 0 and 21 (ferret 3 to 6). Infections were performed as follows: ferrets 1 and 3: HMPV NL/1/00, prototype virus for lineage A1. Ferrets 2 and 5: HMPV NL/1/99, prototype virus for lineage B1. Ferret 4: HMPV NL/17/00, prototype virus for lineage A2 and ferret 6: HMPV UK/5/01 a virus from lineage B2. Ferrets were housed in isolator cages to avoid cross-infections.\n\n【14】HMPV-specific polyclonal antisera were raised in guinea pigs as previously described . Antisera raised in separate guinea pigs against viruses from the two main genetic lineages (A and B) were mixed 1:1, and this mixture tested positive against all HMPV isolates in IFA.\n\n【15】##### Virus Neutralization Assays\n\n【16】Virus neutralization assays of heat-inactivated (min 56°C) ferret serum samples were performed as previously described . Briefly, twofold serial serum dilutions starting at 1:8 were incubated with approximately 30 TCID 50  virus. Seven days after infection of tMK cells with the antibody and virus mixture, IFA was performed with the guinea pig antiserum. The virus neutralization titer was defined as the reciprocal of the highest serum dilution at which no positive IFA signal was obtained (depicted as means of duplicate measurements). Each experiment included virus titrations of the working solution of the virus, using twofold dilutions, and 10–100 TCID 50  per well was considered acceptable.\n\n【17】### Results\n\n【18】##### Variation in the Fusion Protein Gene\n\n【19】Partial F gene sequences (nucleotide \\[nt\\] 780–1,221 in the F ORF) were obtained from clinical samples collected from 84 HMPV-infected patients. Phylogenetic analysis of these sequences confirmed two main genetic lineages, A and B. Each of these lineages appeared to consist of two sublineages, which were tentatively named A1, A2, B1, and B2 .\n\n【20】Comparison of the sequences showed high percentage identities between members of the same sublineage (nt: 97%–100%, amino acids \\[aa\\]: 99%–100%), members of the two different sublineages within each main lineage (nt: 94%–96%, aa: 97%–99%), and between members of the two different main lineages A and B (nt: 84%–86%, aa: 94%–97%). Whereas no specific amino acid residue substitutions could be found between sequences from subgroups A1 and A2, there were 5 specific aa substitutions between sequences from genotypes A and B, and one substitution between B1 and B2 . The low variability was also observed when complete F protein genes from prototype viruses for each sublineage were sequenced .\n\n【21】##### Variation in the Attachment Protein Gene\n\n【22】Nucleotide sequences of the region between the start codons of the G and the polymerase (L) reading frame were obtained for 35 samples. Phylogenetic analysis showed the same clustering of the sequences over the four sublineages as seen for the F protein gene . The G region showed some variation in length, from 860 nt to 908 nt. The first 657–708 nt have been described as the putative primary G ORF . Alignment of the primary G ORFs showed a variation in length, even for members of the same sublineage, due to single nucleotide substitutions that resulted in premature termination codons . For two samples a change in ORF was observed as a result of an addition (BR/2/01: G at position 519) or a deletion (NL/2/93: C at position 243) of a single nucleotide. These mutations resulted in relatively short G ORFs (NL/2/93: 110 aa; BR/2/01: 193 aa) because of premature termination and in drastic changes in the deduced amino acid sequences of the carboxy-terminus of the G proteins. Comparison of the primary G ORF sequences, excluding sequences of NL/2/93 and BR/2/01 because of the putative frame shifts, showed a relatively high percentage identity between members of the same sublineage (nt: 93%–100%, aa: 75%–99.5%), less identity between members of the two different sublineages within each main lineage (nt: 76%–83%, aa: 60%–75%), and low sequence identity between members of the two different main lineages A and B (nt: 50%–57%, aa: 30%–37%).\n\n【23】The position of the hydrophobic domain, a high percentage of proline, serine, and threonine residues and a cysteine residue at position 27 are features shared by all HMPVs. Whereas the cytoplasmic tail was conserved among all members (%–70% aa identity), the proposed ectodomains (start aa 51) were quite variable (%–25% aa identity between lineage A and B). The number and position of potential sites for _N_ \\-linked glycosylation sites varied even within each sublineage, from two to six potential sites, with one located at the proposed cytoplasmic tail conserved among all lineages.\n\n【24】##### Geographic and Temporal Distribution\n\n【25】Analysis of HMPV sequences obtained from samples received from different countries indicated that sequences from Finland, the United Kingdom, and the limited sequences from Asia and South America, were found on branches between the Dutch sequences in the F tree, and not as a separate lineage. The variation between sequences obtained from samples from a single country was found in the same range as the variation found between samples obtained from different countries. In agreement with the genetic lineages of HMPV observed worldwide, which usually includes sequences similar to those of isolate NL/1/00 or NL/1/99, geographic clustering does not appear to apply to HMPV.\n\n【26】The different HMPV samples were obtained during the last 20 years with most from 2000 to 2002 and 14 in the 1990s. As indicator for possible fixation of amino acid variation over time, we analyzed the G ORF amino acid sequence of members in sublineage A2 and B1 (containing samples from 1981 to 2002) in more detail. The amino acid sequence variation between the viruses from 1981 and 2001 was in the same range as the variation found between viruses from 2001, and alignments of the sequences did not indicate fixation of amino acid changes between 1981 and 2002. Thus, antigenic drift, as observed for influenza A and B viruses, does not appear to be an important phenomenon for HMPV.\n\n【27】##### Antigenic Variation\n\n【28】To address the antigenic variation between the genetic lineages A and B, we raised antisera in ferrets against isolate NL/1/00, the prototype virus for lineage A1, and against isolate NL/1/99, the prototype virus for lineage B1. The serum samples were collected 28 days postinfection and tested in virus neutralization assays against the homologous and heterologous viruses. In three independent experiments, the virus titer used per well varied from 10 to 50 TCID 50  ; this variation did not affect the measured virus neutralization titers . Ferret 1, infected with the lineage A prototype virus (NL/1/00), showed a 48- to 128-fold higher virus neutralization titer against the homologous virus NL/1/00 than to the heterologous virus NL/1/99. Similarly, ferret 2, infected with the lineage B prototype virus NL/1/99, had a 16- to 96-fold higher homologous than heterologous virus neutralization titer.\n\n【29】In a second experiment, ferret antisera were raised to viruses from all four sublineages. To measure the most specific serologic response, serum samples were collected 21 days postinfection, after which homologous and heterologous virus neutralization titers were measured . Within each main genetic lineage, a high degree of cross-neutralization was observed between viruses from the two sublineages (e.g. A1 vs. A2 and B1 vs. B2), which is reflected in the low ratio between homologous to heterologous virus neutralization titer (.5 to 3.0). Although serum samples from ferrets 3 to 6 had slightly lower homologous virus neutralization titers than those of ferrets 1 and 2, serum samples raised against viruses from the main lineage A still showed a 12- to 24-fold higher virus neutralization titer against the lineage A viruses than to lineage B viruses. Similarly, serum samples raised against viruses from lineage B had a 16- to 43-fold higher virus neutralization titer against the lineage B viruses than to lineage A viruses.\n\n【30】### Discussion\n\n【31】In this study, the genetic heterogeneity of HMPV was addressed by analysis of the nucleotide and predicted amino acid sequences of part of the F (n = 84), complete F (n = 4), and the complete G (n = 35) protein genes. Phylogenetic analysis of these sequences showed two main lineages (A and B) with each divided into two sublineages (and 2). As was described for HRSV and APV, the F protein was highly conserved, which is in agreement with F proteins of pneumoviruses having structural and functional constraints for amino acid mutations . On the basis of the high percentage sequence identity for the complete F proteins of the prototype viruses of the four lineages, sequences for the complete F proteins of all 84 samples would probably demonstrate similar low variability. In contrast to the F protein, the nucleotide and predicted amino acid sequences of the complete G coding regions showed high sequence diversity (as low as 30%–37% aa identity). Besides the high amino acid sequence variation, we observed variation in length of the different G proteins. Where most of the length variation was due to nucleotide substitutions, two of the samples showed a change in reading frame due to deletion or addition of single nucleotides. Frame shift mutations and use of alternative reading frames have been described for HRSV . As described for isolate NL/1/00, the G coding region of the 35 samples sequenced in the present work indicated long alternative ORFs. However, these secondary ORFs varied in length and position compared to the ones described . Whether premature stop codons, the incidence of frame shift mutations, and possible use of alternative reading frames influence the antigenic properties of the viruses needs to be examined in more detail.\n\n【32】Phylogenetic analyses showed that the HMPV samples obtained from different years and from different countries were randomly distributed over all four sublineages. For HRSV it has also been reported that very similar viruses were isolated at different times and from geographically distant sites . Different lineages within HRSV subgroup A and B have been found on the basis of the variation in the G protein. Within each subgroup, progressive accumulation of amino acid changes was noted, suggesting that the G protein of HRSV might be susceptible to immune pressure . Analysis of the amino acid sequences of the HMPV samples described in this study did not indicate such accumulation over time. However the following observations indicate that the variation of the HMPV G protein might occur as a result of immunogenic pressure in a same manner as was postulated for the RSV G protein: 1) most of the amino acid sequence variation was found in the extracellular domain of the G protein, 2) the variation found at the amino acid sequence level was higher than that at the nucleotide sequence level, 3) the number and position of potential glycosylation sites were not conserved, and 4) deletions, additions and substitutions of single nucleotides resulted in premature stop codons and drastic changes of the carboxy terminal of the protein . Until a larger number of more chronologically diverse HMPV samples have been examined, this issue remains inconclusive.\n\n【33】To address the antigenic relationship between members of the different HMPV lineages, we tested ferret sera raised against viruses from the four sublineages in virus neutralization assays. Serologic responses upon infections tend to broaden over time. On the basis of the relatively close genetic relationship between sublineages A1 and A2 or B1 and B2, we decided to collect serum samples at an early time point, to obtain large antigenic differences between the four sublineages. The low homologous virus neutralization titers in serum samples collected 21 days postinfection may explain the lower ratio between homologous and heterologous virus neutralization titers as compared to sera collected 28 days postinfection. The studies with serum samples collected at 21 days postinfection showed that viruses within one main lineage (e.g. A1 and A2 or B1 and B2) were antigenically closely related. The difference in homologous and heterologous virus neutralization titers between members of the two different lineages A and B titers (to 128-fold higher homologous titer than heterologous titer) indicate a difference in antigenicity between lineage A and B. Classic virology studies have used a definition of a homologous-to-heterologous virus neutralization titer ratio of >16 for defining serotypes. This same definition notes that if neutralization shows a certain degree of cross-reaction between two viruses in either or both directions (homologous-to-heterologous titer ratio of 8 or 16), distinctiveness of serotype is assumed if substantial differences in sequences are observed . On the basis of our results, and based on the described definition, we propose defining the two main lineages of HMPV as serotypes A and B. The HMPV samples were obtained from different study populations, from different countries, and from patients with a wide spectrum of clinical signs. So far, we have no indication of an association between infection with either of the serotypes and a specific study group or with severity of disease. More epidemiologic studies are needed to address this issue.\n\n【34】The circulation of two serotypes of HMPV might have implications for the development of vaccines. Studies in cynomolgous macaques showed that reinfection is suppressed by high titers of virus neutralization antibodies against the homologous virus and far less by heterologous virus neutralization antibodies (data not shown). So far, one heterologous reinfection has been reported in humans . However, children approximately \\> 5 years of age have higher virus neutralization antibody titers than those 1–2 years of age , which suggests that reinfections may occur frequently, most likely with the viruses from the heterologous serotype. For RSV, the importance of difference in antigenicity between the two subgroups regarding protective immunity and vaccine development is still a subject of discussion. However, in animals and humans, the neutralizing capacity against homologous viruses is higher than that against heterologous viruses, and in animals high homologous virus neutralization titers protect against reinfection. In humans, reinfection often occurs with a strain from the heterologous group, and high homologous virus neutralization antibody titers protect against severe infection . The two serotypes of HMPV might resemble the two subgroups of HRSV in immunogenic properties, although more extensive epidemiologic and immunologic studies have to prove this. The cross-reactive immunity provided by the F protein may be sufficient to overcome the effects of changes in the G protein. For HRSV the immune response against the F protein is cross-reactive between subgroup A and B, whereas the response against the G protein is subgroup (and sometimes even genotype) specific . The prophylactic use of a virus neutralization monoclonal antibody preparation directed against the HRSV-F protein has been shown to decrease the severity of lower respiratory tract diseases caused by both subgroups of RSV . In a similar way, the conserved F protein of HMPV could be a target for the development of monoclonal antibodies for treatment of HMPV-infected persons.\n\n【35】Our data support a technical description of two serotypes of HMPV in experimentally infected ferrets. The existence and relevance of these serotypes in other animal species, including humans, has yet to be determined.\n\n【36】Our results in combination with data published by others  demonstrate that HMPV clusters in two globally distributed serotypes. However, the identification of two serotypes does not exclude the possible existence of more serotypes or sublineages. The described viruses were all identified by using primers against conserved regions in the genome of the four prototype viruses, but in order to allow identification of more diverse HMPV strains, virus isolation of original materials is a standard procedure in our laboratory.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "15a819e9-efce-45cc-84a0-fbc198a9797a", "title": "Super-Sentinel Chickens and Detection of Low-Pathogenicity Influenza Virus", "text": "【0】Super-Sentinel Chickens and Detection of Low-Pathogenicity Influenza Virus\nEarly detection of low-pathogenicity type A influenza virus (LPAI) circulating among chickens is important for 3 reasons: 1) these are the most prevalent strains in nature and can cause substantial losses for commercial poultry producers , 2) these strains can contribute genetic material to high-pathogenicity type A influenza virus (HPAI) , and 3) the H5 and H7 LPAI strains can mutate to HPAI with catastrophic effects in birds, and with the potential for transmission to humans with lethal consequences . Kuiken et al. reported that an HPAI (H7N7) isolate was observed in February 2003 in the Netherlands, which most likely originated in free-living ducks and had evolved into a highly pathogenic variant after introduction into poultry farms . Although subsequent serologic screening of poultry showed that the H7 influenza virus had been affecting the Dutch poultry industry several months before the major epidemic, its presence had not been recognized . Our study addresses this problem by using a novel method that causes chickens to seroconvert under conditions in which LPAI would otherwise go undetected. This report shows that recombinant chicken interferon-α (rChIFN-α)  administered perorally in drinking water  acts as an adjuvant to produce a super-sentinel chicken that is a sensitive and early detector of clinically inapparent LPAI.\n\n【1】### The Study\n\n【2】In 2003, the first clue to an aberrant condition in a commercial flock of laying hens in Connecticut was signaled by a drop in feed consumption and then in egg production. It took 6–7 weeks from the time tracheal samples were sent to a diagnostic laboratory to confirm the diagnosis of LPAI (H7N2) infection at National Veterinary Services Laboratory (NVSL) . One such isolate, A/CK/CT/72/2003(H7N2), was obtained from the US Department of Agriculture, NVSL, Ames, Iowa, and used throughout this study to determine whether the peroral administration of rChIFN-α under conditions found to ameliorate Newcastle disease , infectious bronchitis , and infectious bursal disease , would similarly affect avian influenza. We reasoned that if the spread of LPAI could be slowed or prevented, the probability of its mutating to HPAI would be proportionately reduced, thereby lowering the chances of transmission to humans. In the course of this study, we observed a strong adjuvant effect of rChIFN-α administered in drinking water under conditions of virus transmission that mimic natural infection in chickens. This led to the concept of the super-sentinel chicken described here.\n\n【3】Three-week-old specific-pathogen-free (SPF) white leghorns (Charles River Specific Pathogen Free Avian Supplies \\[SPFAS\\], Inc. Storrs, CT, USA) were tagged and divided into 2 groups of 10 chickens each. Two birds in each group were overtly infected intravenously or intranasally with 10 6  infectious particles, measured as plaque-forming particles in primary chicken kidney cells (Charles River SPAFAS, Inc.). This strain of LPAI (H7N2) required a high inoculum to ensure infection (data not shown), comparable to that reported for another LPAI (H7N2) strain evaluated in SPF chickens . The 8 remaining cage mates in each group served as sentinel birds naturally subject to infection by the respiratory tract, ingestion of fecal material, or both. One group of birds received plain drinking water; the other group received drinking water that contained 2,000 U/mL rChIFN-α. The water was provided ad libitum and changed daily. Water consumption was the same in both groups, as determined from the amount remaining after a known volume was provided each day (data not shown). With a half-life of 3–5 days in water at room temperature , this concentration of interferon (IFN) delivered an average dose of ≈3 × 10 5  U rChIFN-α/bird/day. Fourteen days post overt infection (dpi), the ChIFN-α-water was replaced with plain water for the remaining 14 days of the study. This dose of rChIFN-α was sufficient to ameliorate Newcastle disease .\n\n【4】Following overt infection of 2 birds per cage, and the natural cross-infection of the 8 cage mates, serum samples were taken from each of the 10 birds at the intervals indicated in Figure 1 . This figure shows data from 2 independent studies that used agar gel precipitin (AGP) tests to detect antibody against avian influenza virus nucleoprotein and M1 antigens. This qualitative test demonstrated that of the 16 naturally infected chickens given plain water, none seroconverted over the 28-day period they were exposed to the 2 infected cage mates. In marked contrast, of the 16 naturally infected chickens given water containing IFN, 14 were seropositive by 14 dpi and remained so during the 28-day test period.\n\n【5】Although the sensitivity of LPAI to the action of IFN is well documented , rChIFN-α in the drinking water may have been exacerbating the infection, thereby leading to high levels of virus and antigen and high levels of seroconversion. This possibility was tested by using quantitative real-time reverse transcriptase (qRRT)–PCR to determine the amount of avian influenza virus in tracheal samples at 2, 4, and 10 dpi. Table 2 shows that within the error expected from testing individual chickens, the amount of infectious particle equivalents were not significantly different in birds given plain water or IFN-water. Thus, that more avian influenza virus antigen was produced in chickens that were given IFN-water is an unlikely explanation.\n\n【6】### Conclusions\n\n【7】Although the role of IFN as an adjuvant when delivered perorally has been established in mammals , our data demonstrate for the first time, to our knowledge, that avian IFN administered in drinking water to naturally infected chickens lowers the threshold of antigen required to stimulate the adaptive immune response to an LPAI isolate. As a consequence, the action of perorally administered rChIFN-α in effect creates super-sentinel chickens that seroconvert in response to levels of antigen that would otherwise go undetected. Super-sentinel chickens would thus provide a novel means of detecting otherwise inapparent infections of LPAI, thereby buying time for its control or eradication.\n\n【8】We envision the introduction into a large flock of a number of small cages containing chickens in which IFN-water replaces plain water. These super-sentinel chickens will serve as sensitive early detectors of LPAI, like the proverbial canary used in mines to detect low levels of toxic gases. Because of the cross-reaction between chicken and turkey IFN-α , super-sentinel turkeys could likely be created in a similar manner. Super-sentinel birds could be replaced every month and possibly returned to production.\n\n【9】All strains of chickens tested, including those in the People’s Republic of China, have proved to be sensitive to the action of rChIFN-α . Genetically engineered production of rChIFN-α , treatment with it optimized for dose and duration, and its long half-life in water may make it economically feasible to convert many birds in a flock to super-sentinel status. It also may be prudent to set up super-sentinel birds in areas of high risk for avian influenza virus outbreaks, such as live-bird markets. Surveillance of other families of birds might be possible with species-specific IFN. Further studies are required to test these possibilities and the extent to which rChIFN-α functions as an adjuvant with other strains of avian influenza virus and chickens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a2e8f582-10d1-4108-89d7-c963debe52d0", "title": "SARS-CoV-2 B.1.1.7 Variant Infection in Malayan Tigers, Virginia, USA", "text": "【0】SARS-CoV-2 B.1.1.7 Variant Infection in Malayan Tigers, Virginia, USA\nOn April 4, 2021, a 5-year-old male Malayan tiger (Panthera tigris jacksoni_ ) at the Virginia Zoo (Norfolk, VA, USA) began exhibiting lethargy, labored breathing, coughing, intermittent upper respiratory sounds, hyporexia, and mucoid nasal discharge. On April 7, another 5-year-old male Malayan tiger began experiencing labored breathing, cough, clear nasal discharge, and hyporexia. On April 10, a third Malayan tiger, a 10-year-old male, had cough and later clear nasal discharge. The tigers’ clinical signs resolved by April 15, eleven days after the outbreak began.\n\n【1】Zoo staff collected nasal swab and fecal samples from the 5-year-old tigers on April 9 and the 10-year-old tiger on April 13 and submitted these to Cornell University’s Animal Health Diagnostic Center (AHDC; Ithaca, NY, USA). AHDC tested samples for _Bordetella_ sp. _Chlamydia felis_ , _Mycoplasma cynos_ , _M. felis_ , _Streptococcus equi_ subspecies _zooepidemicus_ , influenza virus, pneumovirus, feline calicivirus, and feline herpesvirus; all results were negative. All samples tested positive for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) by EZ-SARS-CoV-2 Real-Time RT-PCR Test . We isolated SARS-CoV-2 from respiratory and fecal specimens from the first tiger. Testing at the US Department of Agriculture National Veterinary Services Laboratories (Ames, IA, USA) confirmed SARS-CoV-2 infection. We screened the tiger samples using TaqPath COVID-19 RT-PCR Kit , which revealed a spike gene dropout in samples from all 3 tigers; only the nucleoprotein and open reading frame 1ab gene targets were detected, suggesting B.1.1.7 variant infection.\n\n【2】We performed whole-genome sequencing on all samples by using MinION , as previously described . We assembled reads using the ARTIC ncov-2019 protocol  and Medaka (Oxford Nanopore Technologies) for variant calling. We obtained near-complete (bp) assemblies from all nasal swab specimens (GenBank accession nos. MZ305031–3) but no assemblies from fecal samples. We identified respiratory specimen genomes as lineage B.1.1.7 (Alpha variant) by using Pangolin version 2.4.2 . We used Nextstrain  for phylogenetic analysis of tiger-derived sequences and other B.1.1.7 sequences downloaded from GISAID  on April 15, 2021 . Tiger-derived sequences all were identical, except 1 manually corrected homopolymer repeat error, and fell into a clade defined by a C4900T mutation containing other samples collected primarily in the United States. Tiger-derived sequences differed from others in the clade by 1 single-nucleotide polymorphism in the spike gene (K558N) . Using the vdb tool , we found 46 additional B.1.1.7 sequences that had the K558N mutation in GISAID on July 22, 2021; all were collected from Virginia during March 27–July 7, 2021. However, phylogenetic analysis of these sequences and the tiger-derived sequences showed divergence of 11 single-nucleotide polymorphism, minus the divergence producing the K558N mutation , indicating the sequences are not related epidemiologically.\n\n【3】The source of the tigers’ infection is unknown. The zoo has been open to the public, but transmission from a visitor is unlikely because tiger exhibit areas are separated from visitors by either a glass enclosure or \\> 9 m distance. The most plausible explanation is that \\> 1 tiger acquired the virus from a keeper because they had close contact. However, no employees tested positive for SARS-CoV-2 nor had symptoms during the 4 weeks before the tigers’ symptom onset. Nine keepers were responsible for the animals’ daily care; 2 other persons prepared animal diets daily. Employees were required to wear facemasks always, indoors and outdoors; everyone wore standard 2-ply surgical masks or homemade cloth facemasks. Staff also were required to wear gloves when handling and preparing food and when servicing animal areas. Furthermore, staff were required to step into an accelerated hydrogen peroxide disinfectant footbath when entering the tiger building and diet kitchen. The 3 tigers might have been infected by an employee, or 1 tiger was infected, then transmission occurred to the others. Two tigers lived in the same enclosure and had no direct contact with the third, but all 3 rotated through common enclosure spaces.\n\n【4】After identification of the tiger infections, 4 additional zoo animals were tested: 1 lion (Panthera leo_ ) with lethargy and hyporexia ≈1 week after SARS-CoV-2 diagnosis in the tigers; another asymptomatic lion because of age and proximity to the first lion; and 2 degus (Octodon degus_ ) that died in late March and had interstitial pneumonia on necropsy. AHDC tested nasal swab samples from the lions and frozen spleen and cecum samples from the degus by reverse transcription PCR; all results were negative for SARS-CoV-2.\n\n【5】Our findings underscore felid susceptibility to SARS-CoV-2, which also has been detected in captive snow leopards (Panthera uncia_ ) and pumas (Puma concolor_ ) . Other nonhuman species, including gorillas (Gorilla gorilla_ ), minks (Neovison vison_ ), and ferrets (Mustela putorius furo_ ), have acquired SARS-CoV-2; additional species have been shown to be susceptible experimentally . Domestic cats and dogs in the United Kingdom and United States reportedly had B.1.1.7 infections, suggesting that mutations characterizing this lineage are not constrained to a host range . Monitoring animals for SARS-CoV-2 infection is critical to determining potential host range, particularly as new virus variants emerge and spread.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dd733a82-95ad-4ea7-b663-53fcf836e396", "title": "Correction: Vol. 23, No. 10", "text": "【0】Correction: Vol. 23, No. 10\nThe author list has been corrected for Enterovirus D68–Associated Acute Flaccid Myelitis in Immunocompromised Woman, Italy . The article has been corrected online .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35d75f24-bc5b-4045-8044-f0335145eee5", "title": "Absence of Fluoride Varnish–Related Adverse Events in Caries Prevention Trials in Young Children, United States", "text": "【0】Abstract\n--------\n\n【1】**Introduction**\n\n【2】Fluoride varnish is an effective prevention intervention for caries in young children. Its routine use in clinical care is supported by meta-analyses and recommended by clinical guidelines, including the US Preventive Services Task Force (B rating). This report is the first prospective systematic assessment of adverse events related to fluoride varnish treatment in young children.\n\n【3】**Methods**\n\n【4】We determined the incidence of adverse events related to fluoride varnish treatment in 3 clinical trials on the prevention of early childhood caries, conducted under the auspices of the Early Childhood Caries Collaborating Centers, an initiative sponsored by the National Institute of Dental and Craniofacial Research. Each trial incorporated use of fluoride varnish in its protocol and systematically queried all children’s parents or legal guardians about the occurrence of acute adverse events after each fluoride varnish treatment.\n\n【5】**Results**\n\n【6】A total of 2,424 community-dwelling, dentate children aged 0 to 5 years were enrolled and followed for up to 3 years. These children received a cumulative total of 10,249 fluoride varnish treatments. On average, each child received 4.2 fluoride varnish treatments. We found zero fluoride varnish–related adverse events.\n\n【7】**Conclusion**\n\n【8】Fluoride varnish was not associated with treatment-related adverse events in young children. Our findings support its safety as an effective prevention intervention for caries in young children.\n\n【9】Introduction\n------------\n\n【10】Dental caries in children is highly prevalent; approximately 37% of US children aged 2 to 8 years had dental caries in primary teeth in 2011–2012, with significant disparities related to race/ethnicity and poverty status . Effective caries prevention interventions exist, including periodically applying 5% fluoride varnish to children’s primary teeth at the age of primary tooth eruption . In 2014, the US Preventive Services Task Force (USPSTF)  concluded that evidence supported use of fluoride varnish in all children, from birth through age 5 years, and gave a B evidence grade to its use. Professional associations, including the American Academy of Pediatrics  and the American Academy of Pediatric Dentistry , also endorse fluoride varnish for caries prevention. Third-party payers, including commercial dental insurers and federal programs (eg, Medicaid, State Children’s Health Insurance Program) reimburse dentists for fluoride varnish treatments in children, and most state Medicaid programs reimburse medical providers for applying fluoride varnish .\n\n【11】Despite evidence for fluoride varnish effectiveness in caries prevention and its broad clinical acceptance, little systematically collected prospective data on its safety exist. The Food and Drug Administration (FDA) regulates clinical fluoride varnish use as a Class II medical device . After FDA’s 510(k) premarket notification process , fluoride varnish was cleared as a cavity liner and desensitizer, or cavity varnish . Using fluoride varnish for caries prevention in children or adults is considered an off-label use, because anticaries agents are considered drugs, not devices . The FDA advises that “good medical practice and the best interests of the patient require that physicians use legally available drugs, biologics, and devices according to their best knowledge and judgment. If physicians use a product for an indication not in the approved labeling, they have the responsibility to be well informed about the product, to base its use on firm scientific rationale and on sound medical evidence, and to maintain records of the product’s use and effects” .\n\n【12】The National Institute of Dental and Craniofacial Research’s (NIDCR’s) initiation of the Early Childhood Caries Collaborating Centers (EC4) in 2008 provided an opportunity to prospectively and systematically collect data on fluoride varnish safety. NIDCR awardees based at the University of Colorado Anschutz Medical Campus (UCAMC), University of California, San Francisco (UCSF), and Boston University (BU) each conducted a randomized controlled trial (RCT) on caries prevention in young children. Each trial used fluoride varnish in its protocol and systematically queried each child’s parents or legal guardians (hereinafter referred to as parents) about adverse events after each fluoride varnish treatment. The objective of this study was to assess the incidence of adverse events related to fluoride varnish treatment in young children in these 3 trials.\n\n【13】Methods\n-------\n\n【14】### Study participants and data sources\n\n【15】We obtained data on adverse events and serious adverse events that resulted in medically attended visits in children enrolled in 3 RCTs under the auspices of the NIDCR-supported EC4. Each RCT used generally accepted definitions for identifying adverse events and serious adverse events . Each RCT tested an intervention designed to reduce caries incidence in young children, and each used fluoride varnish treatments. Fluoride varnish was a randomized component in 1 RCT; the other 2 RCTs assigned all enrolled children to receive fluoride varnish. A single data coordinating center at UCSF served all 3 EC4 trials, and NIDCR used a single data and safety monitoring board to oversee them.\n\n【16】The participating EC4 research centers  are the Center for Native Oral Health Research at UCAMC ; the Center to Address Disparities in Children’s Oral Health at UCSF ; and the Northeast Center for Research to Evaluate and Eliminate Dental Disparities at BU . Details on each trial, including inclusion and exclusion criteria, are available at ClinicalTrials.gov . No children were excluded from any trial because of a history of asthma, peanut or nut allergies, or other food allergies.\n\n【17】The UCAMC trial enrolled 1,016 children aged 3 to 5 years in Navajo Nation Head Start programs to test whether specially trained community lay health workers who administered fluoride varnish (up to 4 times per school year) and oral health promotion education (up to 5 times per school year) in Head Start classrooms for up to 2 years, reduced the number of decayed, missing due to caries, or filled primary tooth surfaces (dmfs) compared with the number of dmfs in nonintervention children; 518 children were randomized to receive fluoride varnish. In the intervention group, fluoride varnish applications were spaced 6 weeks apart on average. Each fluoride varnish application used Vanish (M ESPE), a 5% sodium-fluoride white varnish with tri-calcium phosphate in 0.5-mL sealed-unit–dose packages, and a prepackaged brush. This RCT collected data from September 2011 to December 2014.\n\n【18】The UCSF trial enrolled children aged 2.5 to 3 years, primarily Hispanics, at 2 community health centers in San Diego County, California, to compare the caries prevention efficacy of fluoride varnish applied every 6 months with the efficacy of fluoride varnish applied every 6 months combined with annual fluoride-releasing glass ionomer sealants to eligible occlusal surfaces of primary molar teeth. The primary outcomes were caries incidence and caries increment at 3 annual follow-ups. Incidence refers to any increase in dmfs (ie, a binary indicator of Δ dmfs  \\> 0), whereas increment refers to a (positive) change in dmfs count over time (ie, Δ dmfs  ). Fluoride varnish was applied semiannually to 597 participants from baseline through the 30-month visit. Each fluoride varnish application used a 0.25-mL–unit dose of CavityShield (M ESPE), a 5% sodium-fluoride varnish, per mouth and a prepackaged brush. This RCT collected data from June 2011 to March 2016.\n\n【19】The BU trial enrolled children aged 0 to 5 years, residing in public housing developments in the greater Boston area. A total of 1,837 children were enrolled in 26 housing developments. All children were assigned to receive fluoride varnish. The RCT compared 2 community-based multimodal interventions: 1) motivational interviewing (counseling) by dental health advocates for participating parents, child fluoride varnish application, child oral health assessment, and referral to dental health services; or 2) written oral health education materials on the prevention of early childhood caries, child fluoride varnish application, child oral health assessment, and referral to dental health services. All intervention components, including fluoride varnish, were delivered quarterly for 2 years. The primary outcome was 2-year incidence of early childhood caries. Each fluoride varnish application used a 0.40-mL–unit dose of CavityShield (M ESPE) 5% sodium-fluoride varnish and a prepackaged brush. This RCT collected data from January 2011 to January 2017.\n\n【20】### Determination of adverse events after each fluoride varnish application\n\n【21】Each RCT protocol specified a procedure for contacting each child’s parent after each fluoride varnish application. Typically, a study staff member made contact by telephone and used a standardized script. The purpose of the contact was to assess whether the child experienced any adverse health events after his or her most recent study-related fluoride varnish application, the nature of any such event, and whether the event resulted in a medically attended visit. To assess safety of fluoride varnish treatments, study-related adverse events were defined as health events that resulted in a medically attended visit within a prescribed timeframe (termed the fluoride varnish–adverse event \\[FV–AE\\] window) after the fluoride varnish application. The UCSF and BU trials targeted adverse events that occurred within the first 7 calendar days after fluoride varnish application. The UCAMC trial targeted adverse events that occurred 3 to 10 business days after fluoride varnish application.\n\n【22】In addition to the data collected as part of active safety monitoring after each fluoride varnish application during the FV–AE windows, we also recorded other health-related information that parents reported in telephone calls or in person outside the FV–AE windows. The UCSF and BU trials used medical history forms that asked parents the following yes/no questions about their study-enrolled child: Does your child have an allergy to fluoride varnish or any of its components? Has your child ever had any complications with dental treatment? Both questions also had corresponding free-text fields where details could be provided. The BU trial asked these questions every 3 months, and the UCSF trial, every 6 months. The UCAMC trial used an “extemporaneous encounter form.” It was completed by study staff members if they had a chance encounter with a study participant in which the participant revealed health-related information about a study-enrolled child. We used data collected by each trial using such forms to determine whether any additional fluoride varnish–related adverse events were reported.\n\n【23】Each adverse event identified by study staff members was subsequently adjudicated by a clinician principal investigator to determine whether it was related to participation in the RCT. The NIDCR medical monitor, the data and safety monitoring board, and each trial’s institutional review board reviewed all adverse events at least annually. The principal investigators reported serious adverse events within 72 hours of learning of an event to NIDCR’s clinical research operations management systems contractor. Also, as appropriate, the RCT reported relevant occurrences to FDA by using the agency’s voluntary Safety Information and Adverse Event Reporting Program’s Form 3500 .\n\n【24】### Data analysis\n\n【25】We generated the following summary statistics for each RCT: number of enrolled children who had at least 1 fluoride varnish application, total number of completed fluoride varnish applications, mean number of fluoride varnish applications per child, and the distribution of the number of fluoride varnish applications per child. We also pooled these data for all 3 RCTs. We calculated the timing of the intervals among repeated fluoride varnish applications in each trial and tabulated data on follow-up contacts with parents. Lastly, we calculated the number of adverse events reported for each RCT and across all RCTs. Because we found no trial-related adverse events or serious adverse events after fluoride varnish treatment, we estimated the 1-sided 95% confidence interval (CI) upper bound for adverse events or serious adverse events by using the rule-of-three approximation . The calculation also required adjustment for intracluster correlation (ICC) from the clustered sampling designs used in the UCAMC and BU trials as well as potential intraperson correlation of adverse events across repeated fluoride varnish applications. However, because we found no trial-related adverse events, ICC values could not be estimated from the data. Instead, we estimated 95% CI upper bounds for 3 ICC values (.5, and 1.0) representing levels of intraperson correlation of repeated response. For this calculation, cluster size was specified as the per-child average number of fluoride varnish applications among children with at least 1 fluoride varnish application.\n\n【26】Results\n-------\n\n【27】Across the 3 trials, 2,424 children received 10,249 fluoride varnish applications . The number of completed fluoride varnish applications in each trial partly depended on the scheduled fluoride varnish application frequency and trial duration, which varied by trial . Of 2,424 children with at least 1 fluoride varnish application, 1,863 (.9%) had at least 3 applications and 1,562 (.4%) had at least 4 applications. The mean intervals between fluoride varnish applications varied across trials, from 39 to 182 days. On average, across trials, each treated child received 4.2 fluoride varnish applications .\n\n【28】The 3 trials reached from 75.7% (BU) to 96.9% (UCSF) of parents at follow-up . For the trials at BU and UCSF, the number of contact attempts totaled 13,769 telephone calls, corresponding to approximate average of 1.6 contact attempts per successful contact . Across all 3 trials, the 8,548 successful contacts identified 8 adverse events and 1 serious adverse event, each from a different study participant .\n\n【29】The 8 all-cause adverse events consisted of reports of a cold, cold or influenza, cough or fever, influenza or ear infection, fever, pneumonia, streptococcus infection, or viral infection. The single all-cause serious adverse event was frostbite. The principal investigators determined that all 9 events were not related to the trial or to fluoride varnish treatment. Subsequent external oversight review concurred with the determinations.\n\n【30】Using the rule-of-three approximation, we found upper-bound estimates for the expected percentage of trial-related adverse events and serious adverse events to be 0.035% (ICC = 0), 0.092% (ICC = 0.5), and 0.148% (ICC = 1.0). Thus, the upper-bound estimates ranged from one-twenty-ninth of 1% (ICC = 0) to one-seventh of 1% (ICC = 1).\n\n【31】### Other reported incidents\n\n【32】After the FV–AE window had closed, in 10 children, across 2 studies, parents reported that their children were allergic to fluoride varnish. However, 6 of those 10 parents reported at a subsequent study visit that their child was not allergic to fluoride varnish. In 1 such case, a parent reported that her child had blisters on her tongue and difficulty swallowing after the 2 most recent fluoride varnish applications, even though the report was made 6 months after the most recent fluoride varnish application and no problems had been reported by the parent during follow-up calls that occurred within 1 week of each fluoride varnish application. Although the case did not meet the trial protocol’s definition of an adverse event, after consulting the trial’s NIDCR medical monitor, the trial’s principal investigator reported this case to the FDA via Form 3500. In another 2 children, the following incidents were reported by parents after the FV–AE window had closed: 1 child had a case of herpetic lesions that was medically attended, but the incident occurred 3 days after the trial’s FV–AE window had closed and was determined to be unrelated to fluoride varnish; and another child vomited while receiving a fluoride varnish application. Finally, in 2 other children, the following incidents occurred within a trial’s follow-up FV–AE window, but neither incident was medically attended: 1 parent reported that a child had diarrhea, and another parent reported that a child had “a rash under (his/her) lip less than dime size” after the first of 8 total fluoride varnish applications and “a little rash around (his/her) mouth” after the seventh application.\n\n【33】Discussion\n----------\n\n【34】In 1995, fluoride varnish received FDA clearance as a Class II medical device in the United States. Fluoride varnish has been used in Europe for caries prevention in children and adults since 1964 . Over several decades, evidence has mounted on both its clinical effectiveness and safety . Despite widespread use of fluoride varnish, evidence on fluoride varnish–related adverse events is minimal and few large-scale fluoride varnish clinical trials have been conducted that include a data and safety monitoring board and a standardized protocol for soliciting, monitoring, documenting, and following up on adverse events.\n\n【35】Potential concerns about the safety of fluoride varnish relate primarily to systemic effects from chronic fluoride ingestion, including increased risk of enamel fluorosis and renal toxicity, with acute topical effects predominantly related to contact hypersensitivity involving the oral mucosa. Previous reviews did not report evidence of acute toxicity from fluoride varnish application . A Cochrane systematic review in 2013  found “little information concerning possible adverse effects” of fluoride varnish treatment.\n\n【36】Fluoride varnish use outside of dental settings has increased in the United States. In the North Carolina program, “Into the Mouths of Babes,” primary medical care providers treated more than 250,000 children aged 0 to 3 years with no reports of fluoride varnish–related adverse events . However, the estimates generated by that study may have underrepresented the incidence of adverse events because the estimates depended on providers taking the time to report them.\n\n【37】Fluorosis (ie, changes in the appearance of tooth enamel that are caused by long-term ingestion of fluoride during the time teeth are forming) is cited as a potential concern, although there is no published evidence indicating professionally applied fluoride varnish is a risk factor for fluorosis from prolonged or repeated exposure even in children under 6 years old . Fluorosis is unlikely if not impossible to occur with the recommended frequency and dosage schedules for fluoride varnish. After fluoride varnish application, plasma fluoride concentrations peak within 2 hours and then rapidly decrease . The plasma fluoride concentrations reached and the kinetics were similar to those found after brushing with fluoridated toothpaste . A study of the pharmacokinetics of fluoride varnish application 5 hours after application in children aged 12 to 15 months showed systemic exposure well below levels associated with acute toxicity and similar to control levels . The USPSTF also found no evidence of “risk for fluorosis with fluoride varnish application” . Other potential health effects of fluoride ingestion include gastric irritation and nausea. Compared with ingestion of fluoride from other forms of fluoride, however, ingestion from fluoride varnish is less because only a sparse amount is required for treatment .\n\n【38】The FDA Manufacturer and User Facility Device Experience Database  reported 2 cases of severe anaphylactic or allergic reactions to fluoride varnish treatment. However, these incidents were associated with 1 fluoride varnish product (Recaldent \\[Recaldent Pty Ltd\\]), which also contains casein phosphopeptides, which are derived from the casein protein in milk, in combination with amorphous calcium phosphate. Reactions reported were likely related to the antigenic properties of casein phosphopeptide–amorphous calcium phosphate and not fluoride varnish itself.\n\n【39】Contact irritation of the oral mucosa resulting from fluoride varnish has been sporadically reported . Such reactions likely relate to the colophony base serving as fluoride’s vehicle and are described as mucosal rashes, aphthous ulcer-like lesions, or a short-term “burning sensation” . The manufacturers’ product information typically notes that fluoride varnish is contraindicated in patients with ulcerative gingivitis or stomatitis or patients with known sensitivity to colophony or other ingredients. Colophony is a complex mixture of more than 100 compounds from pine trees, and varies with climate, type of pine tree, and extraction and storage methods . The main colophony component is resin and/or rosin acid derived from the tree wood and not from tree nuts. Manufacturers thus do not name tree nut allergies as contraindications. Meaningful conclusions about the prevalence of colophony allergy are difficult because of colophony’s widespread use outside of health care settings, including in paper, ink, chewing gum, adhesives, and detergents .\n\n【40】Among 376 children followed up to 2 years in an RCT , no adverse events were reported in association with fluoride varnish applications, including enrolled children with asthma diagnoses . In contrast, a single case report in 1998 made note of a fluoride varnish adverse event in an asthmatic child . More recently, in a 2-year RCT in Brazil , among 200 children treated with fluoride varnish or placebo varnish, investigators reported only a single incident of “burning sensation” in a child receiving placebo varnish and no adverse events in any children with asthma. Fluoride bioavailability studies after fluoride varnish application found urinary fluoride temporarily increased and returned to baseline within 24 hours, concluding fluoride varnish is safe for children .\n\n【41】The data used in our analysis had some limitations. First, slightly different FV–AE windows were used among the 3 trials (calendar days by the UCSF and BU trials; 3 to 10 business days by the UCAMC trial); however, any acute adverse events would have been reported in either window. Another limitation was that the UCAMC trial did not record the number of telephone attempts required to reach parents; nevertheless, that trial successfully completed more than 80% of the telephone calls made during the FV–AE window.\n\n【42】Among the 3 RCTs examined in our study, we found no evidence of fluoride varnish–related adverse events after more than 10,000 fluoride varnish applications in more than 2,400 children. Our study is the first large-scale systematic prospective assessment of fluoride varnish–related adverse events in young children. The use of fluoride varnish for caries prevention in young children is expected to increase as a result of the USPSTF recommendation  and growth in government and commercial insurance coverage for physicians and dentists who apply fluoride varnish. Safety concerns are likely to remain an important consideration in decision making for health care providers and families of young children. Thus, our study is a timely contribution to the evidence base. Future clinical studies of fluoride varnish should systematically assess data on safety and investigate the effectiveness of fluoride varnish in preventing caries in the primary dentition, including the optimal dose or frequency of fluoride varnish treatments .\n\n【43】Tables\n------\n\n【44】#####  Table 1. Design Elements and Participant Characteristics of 3 Randomized Controlled Trials Conducted Under the Auspices of the NIDCR-Supported Early Childhood Caries Collaborating Centers\n\n| Element or Characteristic | Location of Study Team | Total |\n| --- | --- | --- |\n| University of Colorado Anschutz Medical Campus a | University of California, San Francisco | Boston University |\n| --- | --- | --- |\n| Name | Preventing Caries in Preschoolers: Testing a Unique Service Delivery Model in American Indian Head Start Programs | Glass Ionomer Sealant and Fluoride Varnish Trial to Prevent Early Childhood Caries | Tooth Smart Healthy Start: Oral Health Advocates in Public Housing | — |\n| ClinicalTrials.gov identifier | NCT01116739 | NCT01129440 | NCT01205971 | — |\n| Institutional review board approval | Colorado Multiple Institutional Review Board | University of California, San Francisco, Committee on Human Research | Boston University Medical Campus Institutional Review Board | — |\n| Location | 52 Head Start Centers on an American Indian reservation in the southwestern United States | 2 Community health centers in San Diego County, California | 26 Public housing developments in the Boston, Massachusetts, area | — |\n| Race/ethnicity of children | American Indian | Hispanic | Hispanic, African American, non-Hispanic white | — |\n| Dates of trial of onset and completion | September 2011–December 2014 | June 2011–March 2016 | January 2011–January 2017 | — |\n| Total no. of children enrolled | 1,016 | 597 | 1,837 | 3,450 |\n| No. of children enrolled to receive fluoride varnish | 518 | 597 | 1,837 | 2,952 |\n| Baseline eligibility age range of children, y | 3–5 | 2.5–3 | 0–5 | 0–5 |\n| Mean baseline age of children enrolled to receive fluoride varnish, y (SD) | 3.6 (.5) | 3.3 (.5) | 3.0 (.8) | 3.2 (.5) |\n| Frequency of fluoride varnish application | 4 Times per Head Start school year | Semiannually | Quarterly | — |\n| Maximum no. of fluoride varnish applications per protocol | 4 b | 6 | 9 | — |\n\n【46】Abbreviations: NIDCR, National Institute of Dental and Craniofacial Research; SD, standard deviation.  \na  Intervention group only.  \nb  Children who enrolled in fall 2011 (n = 55) were eligible for up to 8 fluoride varnish applications. Most children who enrolled in fall 2011 and all children enrolled in fall 2012 were eligible for 4 applications.\n\n【47】#####  Table 2. Descriptive Statistics on Fluoride Varnish Applications in 3 Randomized Controlled Trials Conducted Under the Auspices of the NIDCR-Supported Early Childhood Caries Collaborating Centers\n\n| Characteristic | Location of Study Team | Total |\n| --- | --- | --- |\n| University of Colorado Anschutz Medical Campus a | University of California, San Francisco b | Boston University c |\n| --- | --- | --- |\n| **No. of children with 0 fluoride varnish applications** | 39 | 1 | 488 | 528 |\n| **No. of children with ≥1 fluoride varnish application** | 479 | 596 | 1,349 | 2,424 |\n| **Total no. of fluoride varnish applications completed** | 1,893 | 3,188 | 5,168 | 10,249 |\n| **Mean no. of fluoride varnish applications per child d** | 4.0 | 5.3 | 3.8 | 4.2 |\n| **Mean (SD) no. of days between fluoride varnish treatments** | 39.1 (.8) | 181.5 (.7) | 96.7 (.4) | NA |\n| **Median (IQR) no. of days between fluoride varnish treatments** | 36  | 182  | 95  | NA |\n| **Range of no. of days between fluoride varnish treatments** | 14–70 | 83–532 | 16–595 | NA |\n| **No. (%) of fluoride varnish applications per child** | **No. (%) of fluoride varnish applications per child** | **No. (%) of fluoride varnish applications per child** | **No. (%) of fluoride varnish applications per child** | **No. (%) of fluoride varnish applications per child** |\n| 1 | 21 (.1) | 26 (.4) | 220 (.3) | 267 (.0) |\n| 2 | 27 (.2) | 27 (.5) | 240 (.8) | 294 (.1) |\n| 3 | 77 (.9) | 22 (.7) | 202 (.0) | 301 (.4) |\n| 4 | 304 (.7) | 21 (.5) | 208 (.4) | 533 (.0) |\n| 5 | 3 (.6) | 42 (.0) | 159 (.8) | 204 (.4) |\n| 6 | 4 (.8) | 458 (.8) | 134 (.9) | 596 (.6) |\n| 7 | 12 (.3) | NA e | 82 (.1) | 94 (.9) |\n| 8 | 31 (.0) | NA e | 79 (.9) | 110 (.5) |\n| 9 | NA e | NA e | 25 (.9) | 25 (.0) |\n\n【49】Abbreviations: IQR, interquartile range; NA, not applicable; NIDCR, National Institute of Dental and Craniofacial Research; SD, standard deviation.  \na  Intervention group only. Children who enrolled in fall 2011 (n = 55) were eligible for up to 8 fluoride varnish applications. Most children who enrolled in fall 2011 and all children enrolled in fall 2012 were eligible for 4 applications. Fluoride varnish applied 4 times per Head Start school year.  \nb  Fluoride varnish applied semiannually.  \nc  Fluoride varnish applied quarterly.  \nd  Of those with ≥1 fluoride varnish application.  \ne  Per protocol, this number of fluoride varnish applications was not planned.\n\n【50】#####  Table 3. Fluoride Varnish–Related Adverse Events Among Children With at Least 1 Fluoride Varnish Application, 3 Randomized Controlled Trials Conducted Under the Auspices of the NIDCR-Supported Early Childhood Caries Collaborating Centers\n\n| Variable | Location of Study Team | Total |\n| --- | --- | --- |\n| University of Colorado Anschutz Medical Campus a | University of California, San Francisco b | Boston University c |\n| --- | --- | --- |\n| **No. of fluoride varnish applications completed** | 1,893 | 3,188 | 5,168 | 10,249 |\n| **Follow-up on adverse events** | **Follow-up on adverse events** | **Follow-up on adverse events** | **Follow-up on adverse events** | **Follow-up on adverse events** |\n| No. of contact attempts | NA d | 4,612 | 9,157 | 13,769 |\n| No. (%) of successful contacts e | 1,546 (.7) | 3,090 (.9) | 3,912 (.7) | 8,548 (.4) |\n| Average no. of attempts per contact | NA d | 1.5 | 2.3 | 1.6 |\n| **Adverse events or serious adverse events that resulted in a medically attended visit** | **Adverse events or serious adverse events that resulted in a medically attended visit** | **Adverse events or serious adverse events that resulted in a medically attended visit** | **Adverse events or serious adverse events that resulted in a medically attended visit** | **Adverse events or serious adverse events that resulted in a medically attended visit** |\n| No. (%) of all-cause adverse events or serious adverse events f | 8 (.09) | 1 (.03) | 0 | 9 (.11) |\n| No. of study-related adverse events or serious adverse events | 0 | 0 | 0 | 0 |\n\n【52】Abbreviations: NA, not applicable NIDCR, National Institute of Dental and Craniofacial Research.  \na  Intervention group only. Children who enrolled in fall 2011 (n = 55) were eligible for up to 8 fluoride varnish applications. Most children who enrolled in fall 2011 and all children enrolled in fall 2012 were eligible for 4 applications. Fluoride varnish applied 4 times per Head Start school year.  \nb  Fluoride varnish applied semiannually.  \nc  Fluoride varnish applied quarterly.  \nd  Data not electronically recorded.  \ne  Percentage calculated by using the corresponding number of fluoride varnish applications completed as the denominator.  \nf  Percentage calculated by using the corresponding number of successful contacts as the denominator.\n\n【53】Post-Test Information\n---------------------\n\n【54】To obtain credit, you should first read the journal article. After reading the article, you should be able to answer the following, related, multiple-choice questions. Credit cannot be obtained for tests completed on paper, although you may use the worksheet below to keep a record of your answers.org, please click on the Register link on the right hand side of the website to register. Only one answer is correct for each question. Once you successfully answer all post-test questions you will be able to view and/or print your certificate. For questions regarding the content of this activity, contact the accredited provider, CME@medscape.net . For technical assistance, contact CME@webmd.net . American Medical Association’s Physician’s Recognition Award (AMA PRA) credits are accepted in the US as evidence of participation in CME activities. The AMA has determined that physicians not licensed in the US who participate in this CME activity are eligible for **_AMA PRA Category 1 Credits™_** . Through agreements that the AMA has made with agencies in some countries, AMA PRA credit may be acceptable as evidence of participation in CME activities. If you are not licensed in the US, please complete the questions online, print the AMA PRA CME credit certificate and present it to your national medical association for review.\n\n【55】Post-Test Questions\n-------------------\n\n【56】### Study Title: Absence of Fluoride Varnish–Related Adverse Events in Caries Prevention Trials in Young Children, United States\n\n【57】#### CME Questions\n\n【58】1.  Your patient is a 2-year-old boy in whom fluoride varnish is being considered for caries prevention. According to the prospective systematic assessment by Garcia and colleagues, which of the following statements about findings regarding adverse events (AEs) of fluoride varnish in young children enrolled in caries prevention trials is **_most_** accurate?\n\n【59】    1.  Because each child received only 1 to 2 treatments, it is difficult to confirm the safety of fluoride varnish\n\n【60】    2.  Because of the small sample size, it is difficult to confirm the safety of fluoride varnish\n\n【61】    3.  Rate of fluoride varnish-related AEs was 1.0%\n\n【62】    4.  A total of 8 all-cause AEs (cold, cold/flu, cough/fever, flu/ear infection, fever, pneumonia, streptococcus or viral infection) and 1 serious AE (frostbite) were all unrelated to fluoride varnish treatment\n\n【63】2.  According to the prospective systematic assessment by Garcia and colleagues, which of the following statements about the clinical implications of these findings regarding AEs of fluoride varnish in young children enrolled in caries prevention trials is **_correct_** ?\n\n【64】    1.  Findings of this study do not support the safety of fluoride varnish as an effective prevention intervention for caries in young children\n\n【65】    2.  Fluoride varnish received clearance from the US Food and Drug Administration as a class II medical device in the United States in\n\n【66】    3.  Fluoride varnish has only been in widespread use for the past 2 decades\n\n【67】    4.  Clinical guidelines have not recommended routine use of fluoride varnish\n\n【68】3.  According to the prospective systematic assessment by Garcia and colleagues, what is a potential concern regarding the safety of fluoride varnish?\n\n【69】    1.  Systemic effects from long-term ingestion of fluoride may include increased risks for enamel fluorosis and renal toxicity\n\n【70】    2.  Acute topical AEs predominantly involve enamel discoloration\n\n【71】    3.  At recommended frequency and dosage schedules for fluoride varnish, fluorosis is likely to affect 1% to 2% of children younger than 6 years\n\n【72】    4.  Severe anaphylactic or allergic reactions have been reported with a variety of fluoride varnish products\n\n【73】##### Evaluation\n\n| **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** |\n| --- | --- | --- | --- | --- |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| 1 | 2 | 3 | 4 | 5 |\n| **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| 1 | 2 | 3 | 4 | 5 |\n| **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| 1 | 2 | 3 | 4 | 5 |\n| **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| 1 | 2 | 3 | 4 | 5 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "399e40d8-4ccc-478e-93e3-2c42b46d4173", "title": "Rickettsia massiliae Human Isolation", "text": "【0】Rickettsia massiliae Human Isolation\n**To the Editor:** The number of new rickettsial species that cause diseases in humans is rapidly increasing . Moreover, many of the species first described in ticks have been recently shown to be pathogenic. Of the 10 species or subspecies found to be pathogens after 1984, a total of 7 were first isolated from ticks . We report the first isolation of _Rickettsia massiliae_ from a patient. The bacterium was isolated in Sicily in 1985 and identified in 2005.\n\n【1】\\[\\[AA:FA1:PREVIEWHTML\\]\\]\n\n【2】A 45-year-old man was hospitalized in Palermo, Italy, on June 6, 1985, for fever and a rash. He had been febrile since May 25 and did not respond to antimicrobial drug treatment using cefamezin, a first-generation cephalosporin. On examination, he had a necrotic eschar on his right ankle, a maculopapular rash on his palms and soles , and slight hepatomegaly. Leukocyte count was normal; he received tetracyclines for 13 days and fully recovered. He seroconverted (from 0 to 1:80 between day 11 and day 24) by indirect immunofluorescence to _Rickettsia conorii_ (R. conorii_ spot, bioMérieux, Marcy l'Étoile, France).\n\n【3】Four milliliters of heparinized blood sampled before treatment were inoculated in a 25-cm 2  flask containing Vero cells and incubated at 33°C in a CO 2  incubator . Direct immunofluorescence test on a sample of the patient's serum was positive 7 days later. The strain was stored for 20 years and tested in 2005 at the Unité des Rickettsies for identification, and _R. massiliae_ was identified. DNA was extracted from the cell culture supernatant and used as template in 2 previously described polymerase chain reaction (PCR) assays that targeted a portion of the rickettsial _ompA_ gene as well as a portion of the rickettsial _gltA_ gene . Amplification products of the expected size were obtained from this extract but from no concurrently processed control materials, including 3 negative controls. DNA sequencing of the positive PCR products gave 100% identity with _R. massiliae_ for _ompA_  and 99.9% homology for _gltA_ .\n\n【4】_R. massiliae_ was first isolated from _Rhipicephalus_ ticks in Marseilles . It is transmitted transovarially in _Rhipicephalus turanicus_ . _R. massiliae_ is commonly found in _Rhipicephalus sanguineus_ or _R. turanicus_ in France, Greece, Spain (identified as Bar 29) , Portugal, Switzerland, Sicily (D. Raoult, unpub. data), Central Africa, and Mali . _R. massiliae_ may be commonly associated with these ticks, which are distributed worldwide.\n\n【5】\\[\\[AA:FA2:PREVIEWHTML\\]\\]\n\n【6】_R. massiliae_ is grouped phylogenically with _Rickettsia rhipicephali_ and _Rickettsia aeschlimannii_ . Bacteria from this group have a natural resistance to rifampin that is associated with an _rpo_ B sequence that is different from that of other rickettsiae. This isolate was not tested for antimicrobial drug susceptibly . Rifampin resistance leads us to believe that this isolate may cause a Mediterranean spotted fever–like disease that was described in children in Spain . Serologic findings were recently reported that showed some patients in Barcelona, Spain, with reactions that indicate _R. massiliae_ (B29 strain) rather than _R. conorii_ . However, serologic reactions are only presumptive; isolation from a patient is the required to initially describe a new disease .\n\n【7】This Sicilian index case shows that _R. massiliae_ is a human pathogen. It contraindicates using rifampin to treat Mediterranean spotted fever in areas where _R. massiliae_ is endemic, as it cannot as yet be differentiated from _R. conorii_ infection. _R. massiliae_ is a new example of a strain identified in ticks for several years before its first isolation from a human patient . The longest delay was observed for _Rickettsia parkeri_ , which was isolated from ticks in 1939 but not from a patient until 2004. Many authors labeled _R. parkeri_ a nonpathogenic rickettsia during this time . In the present case, the human isolate was obtained before the tick isolate but was not further identified. When this strain was isolated, _R. conorii_ was the sole _Rickettsia_ sp. found in ticks in southern Europe. Moreover, only 1 tickborne pathogenic _Rickettsia_ sp. was believed to circulate in a single area. Since that time, several tickborne rickettsial diseases have been shown to exist in the same area, which prompted us to retrospectively identify this strain. The patient was reexamined in May 2005, after this identification. He is healthy and has no remaining antibodies against _Rickettsia_ spp.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4f1c80c4-09fc-4c90-ba57-9ef598459d67", "title": "Community-Centered Health Home: Life on the Other Side of the Wall", "text": "【0】Community-Centered Health Home: Life on the Other Side of the Wall\n*   Food Insecurity\n*   Substandard Housing\n*   Future Directions\n*   Acknowledgments\n*   Author Information\n*   References\n\n【1】Families living in poverty face barriers that threaten their fundamental life-sustaining needs. The crisis-of-the-moment (eg, hunger, shelter) can take immediate precedence over education, environmental conditions, and preventive health care. Maslow’s hierarchy of needs suggests that socioeconomic groups differ in how they prioritize their daily needs, including healthful food choices . The socioeconomic conditions in which families live may pose barriers to achieving favorable health outcomes. Social determinants of health are “conditions in the environments in which people are born, live, learn, work, play, worship, and age that affect a wide range of health, functioning, and quality-of-life outcomes and risks” . Communities must assess these complex and interconnected determinants of health to identify root causes of health problems. Cross-sector collaborations are necessary for assessing these determinants and building healthier communities to support children and families .\n\n【2】Escambia Community Clinics (ECC), a federally qualified health center based in Pensacola, Florida, was one of 5 Gulf Coast community health centers to participate in the Louisiana Public Health Institute’s (LPHI’s) Community-Centered Health Home Demonstration Project . The community-centered health home model was developed by the Prevention Institute in 2011 as a framework for community health centers to engage in creating safer, healthier, and more equitable communities . The model addresses the underlying factors that affect injury and illness, such as the environment, poverty, and safety, to improve health equity and reduce the need for medical treatment . ECC adopted the model as a baseline diagnosis approach to target and remedy these underlying factors. The community-centered health home concept recognizes that the goal of the community health center is to tailor care to the sociodemographics of the neighborhood in which the community health center is located .\n\n【3】LPHI developed a pilot of this model under the Gulf Region Health Outreach Program, funded by the Deepwater Horizon Medical Benefits Class Action Settlement. With guidance from LPHI, ECC embraced the community-centered health home model and applied it to a local Pensacola school, C.A. Weis Elementary School. ECC and 3 additional core partnering agencies signed a 20-year community school partnership agreement to embed health care, behavior health services, and social service support directly inside the school. A community school partnership offers students and families extended resources, while promoting academics, community, and family engagement.\n\n【4】ECC’s role is to serve the health care needs of students through a pediatric primary care practice within the school. A pediatrician and public health expert defined the scope and vision of the demonstration project, wherein the federally qualified health centers are challenged, as the primary service delivery organizations, to use their position of influence in the community to improve the social, environmental, and economic conditions that determine health .\n\n【5】This essay describes how partnering agencies in the Demonstration Project collaborated to work toward solving 2 common problems among families living in poverty: food insecurity and substandard housing. Phase I of the project identified community health indicators, as these community health indicators demonstrated trends related to food insecurity, safety, and housing. Phase II of the project addressed these indicators.\n\n【6】Food Insecurity\n---------------\n\n【7】Families living in poverty often have limitations in accessing affordable, healthful food in their communities, whereas fast food restaurants and food with little to no nutritional value from nearby overpriced convenience stores are often readily available. These limitations include safety issues (eg, high levels of crime, unsafe sidewalks, crossing high-traffic intersections by foot with children), access to community resources (eg, lack of direct transportation to and from the grocery store), and lack of information about health-promoting resources (eg, health and wellness programs such as healthy shopping and cooking education) . Case studies report that access barriers to stores offering fresh fruits and vegetables are directly linked to high levels of health deprivation .\n\n【8】In October 2016, ECC leveraged its partnership in the Weis Community Partnership School by offering supportive services in the community of Oakwood Terrace, a low-income housing community in which many of the school’s students reside. ECC learned from the school’s staff members that 42% of the students attending the Weis Community Partnership School received a backpack of food each Friday, donated by two local churches, to provide additional food throughout the weekend. Recognition of the need for a weekend supply of food sparked concern that some of the school’s families may be experiencing a food shortage. ECC decided to use LPHI supplemental funds to provide more food to these students through a 6-month Food Extension Program.\n\n【9】The 6-month Food Extension Program, developed in partnership with Weis Community Partnership School, Manna Food Pantries, the University of West Florida, and the University of Florida IFAS Extension/Expanded Food and Nutrition Education Program, provided bimonthly food boxes, health education sessions, and cooking classes to families who attended the classes at the Oakwood Terrace Community Center. In addition to bus passes to assist with transportation to grocery stores offering fresh produce and healthful food choices, participants received free cooking tools, such as crock pots and cookbooks, to prepare meals for under $7.00. Manna Food Pantry is a local food distributor that provided food boxes containing basic staples: rice, beans, and fresh fruits and vegetables. The Expanded Food and Nutrition Education Program is a free nutrition education program that teaches participants skills and strategies to stretch food dollars, eat nutritious meals, and improve overall health . The class was designed to meet families where they are: it was hosted onsite in the low-income housing project where many of the children attending the Weis Community Partnership School live. Thus, the program connected the school to the housing community, two important hubs in the community.\n\n【10】Substandard Housing\n-------------------\n\n【11】Oakwood Terrace is a privately owned US Department of Housing and Urban Development Section 8 apartment complex that comprises 300 apartments. Approximately 200 of the more than 500 students attending Weis Community Partnership School live in this complex. Surrounded by an 8-foot cement wall, Oakwood Terrace is in one of Escambia County’s most socially vulnerable zip codes. _Life on the other side of the wall_ is nothing short of surviving severe socioeconomic deprivation. In 2015, 11% of households in Escambia County were living in poverty as determined by the yearly federal poverty guidelines. In addition, 27% of children in Escambia County, compared with 23% of children in Florida, were living in poverty . Poor housing and poor health create a need for a targeted public investment. Intervention studies link housing improvements to health improvements, and housing improvements may promote improved social relationships within and beyond the household .\n\n【12】ECC, in collaboration with the University of West Florida, conducted a survey among residents of Oakwood Terrace to identify social problems in the complex. The survey addressed topics such as housing conditions and access to healthful foods. Eighteen women and 2 men representing more than 70 family members aged 0 to 71 participated in the survey. The survey results showed that housing conditions were substandard and the complex was riddled with crime. Poor insulation of the apartment units led to high utility bills, resulting in utilities being disconnected. Food could not be stored in apartments that did not have utilities, and families that did not have utilities could have faced eviction from their apartment. Families found themselves living within this vicious cycle of crisis.\n\n【13】After examining the survey results, ECC shared the survey information with the owners of the complex, who live out of state, and the two groups established a working relationship. This relationship bridged the gap between residents and management. The owners learned about the needs of their residents and began to implement the changes that were most important to them. In addition, the owners hired a new on-site property management team. This event set the stage for new collaborative momentum to implement a spectrum of components for transforming the community.\n\n【14】Future Directions\n-----------------\n\n【15】The philosophy of the community-centered health home is that the health care institution has a voice that is well-regarded and that can affect local policy or community conditions. The actions taken by community-centered health homes typically include partnering with external organizations and a diverse group of community stakeholders through a two-way relationship . As such, fostering partnerships and enhancing community engagement were the driving forces behind ECC’s role throughout the Demonstration Project.\n\n【16】The Demonstration Project continued to address the food insecurity crisis and also looked more closely at the housing survey responses to determine further underlying adverse conditions. The owners integrated funding resources with ECC to sustain and expand the food extension program. Apartment renovations began in 2016 and are scheduled through 2018. Most recently, apartment windows were tinted to reduce apartment temperatures during the warm months, lower utility bills, and improve aesthetics. Hands-on education for residents on cleaning and sanitizing units will help maintain newly updated apartments and assist in instilling a sense of ownership and pride among residents. Health screenings were offered, along with continued nutritional education, adult educational resources, employment enrichment, public transportation education, and plans for a resident-managed community garden.\n\n【17】As part of Phase II, we conducted a second survey. The survey was conducted among participants attending the Food Extension Program in June 2017. The purpose of the survey was to identify the collective impact and outcomes of the program. Fifteen residents participated in the survey. These survey results showed that families had increased their monthly vegetable intake by more than half since the first survey was administered. However, intake of items such as cakes, cookies, and sugar-sweetened beverages decreased only slightly; these items are still readily available to families at the local corner convenience store, the only store in the neighborhood that offers immediate access to any kind of food. This survey also showed that transportation continued to affect the ability to make healthful decisions about food. A transportation option, now in the planning stages, will offer paratransit routes directly to and from Oakwood Terrace to the grocery store, with the transportation expense paid by a private donor.\n\n【18】The owners invested more than $1 million into the safety and renovations of Oakwood Terrace. The newly renovated apartments were designed to enhance a sense of community and create physical activity resources to ameliorate a host of health conditions, from obesity to depression. A new playground for children to safely play within the walls of the complex was built as well as a new community center. Before transformation of the complex, families kept their children in poorly insulated apartments in fear of high levels of crime. New security measures to improve safety at Oakwood Terrace include a 7-day-per-week police presence and an internet surveillance system. Additionally, some families are now inspired to apply what was learned throughout the program by migrating from Oakwood Terrace’s low-income housing to affordable housing and eventually homeownership.\n\n【19】In June 2017, Morgan Cox, partner of DM Oakwood Terrace, LLC, owners of Oakwood Terrace, stated, “We are so thrilled about the progress taking place at Oakwood Terrace, and it is a testimony to public and private entities joining forces and working together.”\n\n【20】Systemic cultural and social norms take time to transform into an acceptance of change. The concrete wall surrounding the community of Oakwood Terrace continues to stand, but there is now a beacon of hope shining through as that wall begins to be disassembled one metaphorical brick at a time.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a186cf67-9f65-4a5c-bc2e-8d3f76f5b5fb", "title": "Osler and the Infected Letter", "text": "【0】Osler and the Infected Letter\nShips docking at the Lazzaretto Vecchio , Venice, 14th century .\n\n【1】In January 1876, William Osler, a young Canadian physician, was recovering from a mild case of smallpox contracted while attending patients at Montreal General Hospital . In a letter written that same month to an old schoolmate (Arthus Jarvis), Osler described his illness and noted in closing, \"You need not be afraid of this letter. I will disinfect it before sending\" . Concern about disseminating smallpox through this letter was well founded. In his medical textbook of 1892, Osler would later write that smallpox can be conveyed by fomites: \"the dried scales \\[of variola scabs\\] … as a dust-like powder … become attached to clothing and various articles ….\" .\n\n【2】### Stability of the Smallpox Virus\n\n【3】Long before Osler's time, the stability and infectivity of variola virus was well known, as illustrated by an example of germ warfare during the French and Indian War. In 1763, the British general Sir Jeffrey Amherst ordered that scab-ladened \"Sundries\" be delivered to the Ottawa Indians of Pennsylvania. Amherst hoped thereby to induce a debilitating smallpox epidemic among the Indians and conquer them .\n\n【4】Much later in the 1860s, a professional grave robber for the Medical College of Ohio in Cincinnati became incensed at tricks played on him by medical students. He delivered the corpse of a smallpox victim recently buried to the dissecting laboratory and intentionally infected many anatomy students .\n\n【5】The stability of the smallpox virus was often noted by 18th-century physicians in debates over the comparative merits of variolation and vaccination. Razzell cited a 1792 article describing how an English amateur inoculator dried smallpox scabs in peat smoke, stored them underground covered with camphor, and used them as long as 8 years later .\n\n【6】Razzell also reported the longest supposed survival of the variola virus, which caused an outbreak of smallpox in a town in Somerset in 1759. The coffin of a villager who had died of smallpox 30 years before was exhumed for transfer to a new grave site. The sexton accidentally put his spade through the oak coffin, which released \"a most nauseous stench.\" The deceased was of such eminence that most of the villagers had attended the exhumation and reburial. \"In a few days afterwards \\[sic\\], fourteen persons were seized with smallpox in one day\" .\n\n【7】Today we would be skeptical of this story from 18th-century Somerset, but outbreaks of smallpox appeared occasionally in other isolated communities in England without any recognized living human source. For example, from 1908 to 1952, sporadic cases of variola appeared in Lancaster and Cheshire, textile mill counties. These outbreaks were likely introduced there by cotton imported from Egypt, where months before it had been contaminated with smallpox scabs .\n\n【8】Samples of the smallpox virus freeze-dried in a laboratory have been revived after storage for 20 years at Liverpool University . But long-term survival of this virus under natural conditions is another issue. In 1957, two London virologists using cell cultures examined smallpox scabs that had been stored in test tubes under ambient laboratory conditions. They found that the variola virus survived as long as 18 months . A comparable study from Leiden in 1968 used variola minor scabs collected and stored in 12 unsealed envelopes kept at room temperature. Each year the contents of a single envelope were cultured. The final envelope was examined 13 years later and, like the previous ones, showed virus particles still capable of replicating in culture .\n\n【9】The search for variola viruses surviving even longer was pursued in 1991 near Novosibirsk, Russia . \"Bioweapons experts\" searched for the variola virus in 19th-century smallpox victims mummified in the permafrost above the Arctic Circle. In the event of unusual thawing and flooding, the concern was that these corpses might become exposed and release infectious virus into the environment. In the 19th century, this region of Russia (Sakha Republic) was \"ravaged by smallpox strains of extraordinary lethality\" . Isolating and comparing them with preserved modern strains might identify genes contributing to virulence. To date, no live variola viruses have been isolated from Sakha. But the threat now is that \"a sophisticated terrorist team might … go smallpox hunting on the permafrost\" .\n\n【10】##### Smallpox Transmitted in Letters\n\n【11】In 1876, Osler's concern was the danger his letter might pose to his friend Jarvis and not to cell cultures. Indeed, years later in 1901, articles in 2 respected medical journals incriminated letters as sources of 2 separate epidemics of smallpox .\n\n【12】The New York Medical Journal reported that smallpox had developed in a young lady in Saginaw, Michigan, after she received a letter from her sweetheart, a soldier in Alaska. He had written it while recovering from this disease. The infection subsequently spread to 33 other persons in Saginaw .\n\n【13】In addition, the April 1901 issue of the British Medical Journal reported an outbreak of 5 cases of smallpox at the Mormon headquarters in Nottingham, England, apparently after receipt of \"letters or other fomites\" from Salt Lake City, Utah, where smallpox was widespread . According to February 1901 issue of the Journal of the American Medical Association, 314 cases had been reported during the previous 3 months in Salt Lake City itself . The year before, the New York Times noted that Mormons opposed vaccination and had introduced a bill in the state legislature making it unlawful to compel vaccination .\n\n【14】##### Other Fearsome Epidemics\n\n【15】In the fall of 2001, anthrax spores were sent in letters through the US mail. This event resulted in 18 confirmed cases of the disease, 5 deaths, and cross-contamination of perhaps 5,000 letters . Not since the fifth plague of Egypt, which may have been anthrax (\"a very grievous murrain,\" Exodus 9), have people so panicked over the threat of this disease. However, in centuries past, many have fled from the sudden appearance of 5 other contagious diseases: smallpox, bubonic plague, yellow fever, typhus, and cholera. Malaria was such an expected seasonal affliction in many parts of the world that it was never perceived as an acute contagion.\n\n【16】When a smallpox epidemic struck Rome around 164 A.D. Galen is said to have hastily returned to his home in Pergamon on the Ionian Coast of modern-day Turkey. When plague returned to London in 1665, Thomas Sydenham, a physician, prudently sought safety in the countryside. In 1793, yellow fever swept through Philadelphia, then our federal capital. Alexander Hamilton left town, and President George Washington remained at Mount Vernon until the fall frost had been reported up North. In 1813, typhus decimated the French army in Moscow, forcing Napoleon to retreat to Paris. And in 1832, when cholera came to Kentucky, US Senator Henry Clay established a tent city on his estate outside Lexington for the 2,000 citizens who fled the town.\n\n【17】##### Origin of Quarantines\n\n【18】In the 14th century, most citizens could not flee pestilences threatening their towns, but civil authorities sought to protect them by excluding suspected human carriers and merchandise from outside. Garrison gives a succinct history of early quarantines . When bubonic plague reached Europe in 1347, ports on the Mediterranean and Adriatic Sea were among the first to deny entry to ships coming from pestilential areas, notably from Turkey, the Middle East, or North Africa. Florence, on the Arno River, issued restrictions on travelers and goods as early as 1348. The Venetian Republic formally excluded \"infected and suspected ships\" in 1374 . The earliest such action in the Americans was by the Massachusetts Bay Colony in 1647 to 1648, when it barred ships coming from the West Indies thought to be carrying yellow fever .\n\n【19】The first official quarantine system is commonly ascribed to Ragusa (now called Dubrovnik), a port city located on the Dalmatian coast of the Adriatic Sea. There in 1377, and later when pestilences were abroad, incoming persons and ships were first isolated on a nearby island for 30 days (trentina_ ) to await clinical signs of a contagion or evidence of continued good health. Detention of 40 days (quarantina_ ) was instituted by the city of Marseille in 1383 and soon became the standard period of quarantine.\n\n【20】Later, other cities established isolation stations on shore or on nearby islands. Ragusa's use of an offshore island in 1377 was an early example of such a quarantine station. In spite of Ragusa's seeming priority, various sources claim that the first such station was a pest house built on the island of Sardinia in 1453 or buildings erected at Pisa near the church of San Lazzaro in 1464 . In North America during the 1743 epidemics of smallpox and yellow fever, an early quarantine station was established in Philadelphia on Providence Island in the Schuylkill River . Other major U.S. cities soon thereafter organized quarantine stations to cope with later epidemics of smallpox, yellow fever, typhus, and cholera.\n\n【21】##### Lazarettos\n\n【22】Quarantine stations in southern Europe were originally called lazarettos. The origin of the term is uncertain. One 19th-century historian suggested that it is a corruption of the name of the church of Santa Maria di Nazaret, used as pest house in 15th-century Rome . But the Crusaders, who captured Jerusalem in 1099, had isolated and treated people with contagious diseases outside the city in the Hospital of St. Lazarus, the patron saint of lepers . In Venice  quarantined ships were anchored at Lazzaretto Vecchio, an island in the lagoon. When the island acquired this particular name is not known. As noted above, in Pisa in 1464, persons were quarantined in a special building near the Church of San Lazzaro . An exhaustive, illustrated survey of lazarettos is given in John Howard's 1789 treatise on the subject  .\n\n【23】##### Early Decontamination Measures\n\n【24】During the early Renaissance, clothing and other possessions of plague victims were often burned. In Italy and France during this period, the threat of plague compelled the destruction of great quantities of cloth prepared from cotton, wool, and silk recently imported from suspect countries. This precaution resulted in enormous economic loss and often in an immediate devastating local poverty .\n\n【25】The earliest attempts to decontaminate merchandise on ships coming from pestilential shores were made in Venice in the mid-1400s. Cargo was unloaded and fumigated with smoke from burning straw, pitch, tobacco, or even gunpowder. Cargo was also \"perfumed.\" This term likely derived from burning fragrant herbs, juniper berries, aromatic gums (e.g. myrrh), and resinous wood in attempts to sterilize items. Early on, smoking sulfur was frequently employed, while in 18th-century Germany a mixture of sulfur, potassium nitrate (saltpeter), and wheaten bran (Raucher Pulver_ ) was used .\n\n【26】##### Disinfection of Mail\n\n【27】Karl F. Meyer, a physician-pathologist from Louisville, Kentucky, spent a lifetime researching the disinfection of mail . He determined that disinfection was first attempted in Venice around 1493 by dipping letters in vinegar. Later, other methods were used. By the early 1600s, decontamination of mail was practiced in much of Europe. In the United States in 1712, when yellow fever threatened Boston, mail from docking ships was first exposed to burning sulfur .\n\n【28】Very few letters from the early centuries of decontaminating mail are available today. But 1 rare specimen from 1485 does show evidence of having been dipped in vinegar . Since such treatment often rendered parts of a letter illegible, other less-damaging methods were employed, such as exposure to smoke and various fumes. The eventual widespread use of burning sulfur yielding sulfur dioxide (with its \"sharp, irritating odor\") may have been based on an ancient idea that the more foul a medicine, the more effective it might be.\n\n【29】In the late 19th century, sulfur gave way to chlorine or formaldehyde gas. In November 2001, chlorine dioxide gas was sprayed into the partly contaminated Hart Senate Office Building, while the Postal Service used a 10% solution of bleach to \"sterilize\" its mail sorting centers . Ion beam sterilization (high-energy electrons) and x-ray radiation have been considered for use on individual letters.\n\n【30】##### Decontaminating Letters \"Inside and Outside\"\n\n【31】Sterilizing the outside of sealed envelopes did not ensure that the letter inside was safe. To allow penetration of sterilizing fumes or gases, initially envelopes were breached by cutting a small tip off one or more corners without exposing the content of the letter inside. In later years, multiple small holes were made in the envelope and its letter by means of a rastel, a hairbrush-size instrument consisting of 2 hinged metal plates (jaws), one of which held several rows of nails or small metal spikes . Clamping each letter between the jaws of the rastel produced several rows of small holes through the envelope and its contents, enabling gas to penetrate the interior .\n\n【32】When thousands of letters required fumigation, perforating each individually was not practical. Instead, they were laid out on screens, placed in an air-tight box (or in a boxcar), and exposed to burning sulfur for ≈6 hours . Sterilizing the outside of envelopes protected the mail handlers, but the reader still remained at risk from the interior.\n\n【33】##### Certification of Decontamination\n\n【34】Once letters had been decontaminated, some sort of certification had to be noted on them. As late as 1837, a paste or wax seal was affixed to fumigated letters. But this certification was impractical with large numbers of letters, so soon each letter was simply stamped, much like a modern-day postal cancellation. The following descriptions of these cancellation marks are taken from illustrations of some early 19th-century cachets .\n\n【35】A letter that had passed through Genoa in 1813 during the Napoleonic occupation bore the French stamp \"Purifié à Gènes.\" Letters stamped at Leghorn in 1829 showed \"LAZZERETTO SAN ROCCO DI LIVORNO.\" San Rocco was 1 of the 2 plague saints. During the early period when only the outside of envelopes were disinfected, an Italian cachet read \"NETTA FUORAI E SPORCA DENTRO,\" or \"clean outside and dirty inside.\" An 1830 cachet with the Papal insignia noted \"NETTA DENTRO E FUORI\" (clean inside and out). An 1831 stamp from Vienna read \"Rein von innen und aussen\" (clean within and out). A first-class letter from Jacksonville, Florida, in 1888 read simply \"Fumigated\" and thus did not define the extent of the procedure.\n\n【36】##### Decline in Mail Disinfection\n\n【37】By the early 20th century, plague, typhus, and yellow fever were known to be transmitted by arthropod vectors, and cholera was known to be waterborne. Since letters seemed an unlikely means of spreading pestilences, disinfection of mail declined, but some authorities continued to see a potential risk in mail from patients with tuberculosis and leprosy. Meyer noted that as late as 1953, letters leaving a German tuberculosis sanatorium were first fumigated with formaldehyde fumes. Likewise in the United States, as late as 1968 mail leaving the leprosarium at Carville, Louisiana, was first sterilized by baking in electric ovens .\n\n【38】##### Conclusions about Osler's Letter\n\n【39】Osler did not say how he would disinfect his 1876 letter to Jarvis. The letter shows no vinegar stains. The sterilizing value of dry heat (oven) and moist heat (autoclave) was not established until 1881 by Koch and others . The causative agent of smallpox was not visualized microscopically until 1887, when Buist first observed small clumps of virus particles now called Guarnieri bodies in infected tissues. The variola virus was first cultivated in 1935 by Torres and Teixeria on the chorioallantoic membrane of embryonated eggs .\n\n【40】Being a pathologist, Osler may have used formaldehyde vapors to sterilize his letter. In any case, we do not hear that Jarvis ever contracted smallpox from it. Indeed, correspondence between Osler and Jarvis continued at least through 1910 . Osler died in 1919 and Jarvis in 1936 .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2c218a7d-332a-4c75-9506-b430be364ba7", "title": "Rise in Group W Meningococcal Carriage in University Students, United Kingdom", "text": "【0】Rise in Group W Meningococcal Carriage in University Students, United Kingdom\n_Neisseria meningitidis_ causes severe sepsis and meningitis. The main reservoir in most populations is asymptomatic pharyngeal carriage in older adolescents and young adults . High carriage rates are particularly evident in semiclosed communities of young adults, such as university student populations, where persons live, work, and socialize together .\n\n【1】Meningococcal carriage was previously assessed in university students in the United Kingdom during 2009–10 at the University of Nottingham (UoN) when a high prevalence of capsular group Y (MenY) meningococcal carriage was detected . This high level of MenY carriage was concomitant with a rise in disease caused by MenY strains in the United Kingdom . Since 2009, although MenY disease has plateaued, capsular group W (MenW) disease has steadily increased because of the rapid expansion of hypervirulent strains belonging to the sequence type 11 clonal complex (MenW:ST-11) .\n\n【2】Analysis of whole-genome sequence data has shown that isolates from the same MenW:ST-11 lineage, termed the South American/UK strain, are also endemic to Chile, Brazil, and Argentina  and were recently reported in Australia . In response, Public Health England introduced the MenACWY vaccine in the routine adolescent school program for 14- and 15-year-olds and first-year university students . A catch-up campaign was used to offer MenACWY vaccine to all 14–18-year-old persons, with persons who left school in 2015 (years of age) given priority.\n\n【3】The rationale for targeting the vaccine to older adolescents and young adults stems from carriage studies showing that this demographic represents the principal reservoir of meningococcal carriage, as well as experience with other polysaccharide conjugate vaccines. Introduction of the MenC monovalent conjugate vaccines previously reduced carriage acquisition of MenC strains in adolescents and young adults . There is evidence, albeit limited, to suggest that the quadrivalent vaccine may have a similar effect on carriage of MenCWY strains . Reduced carriage in this population should lead to indirect protection of other age groups through herd immunity, thus enhancing the public health effect and cost-effectiveness of this approach.\n\n【4】### The Study\n\n【5】To assess trends in meningococcal strain carriage, and to determine the immediate effect of the MenACWY vaccine on carriage of MenW/Y strains, we conducted a cross-sectional study in first-year students at the UoN from registration during September 2015 through March 2016. MenACWY vaccination coverage in this student population increased from 31% (preregistration) to ≈70% (immediately postregistration) as a result of a campus-based vaccination campaign targeting freshmen .\n\n【6】The study was approved by the Research Ethics Committee at the UoN, and written informed consent was obtained from all participants. We recruited convenience samples of first-year students in September and November 2015 and March 2016. In September, students were recruited during registration; in November and March, students were recruited in 5 dormitories (A–E) with single-occupancy rooms. We searched the UoN Health Service registration database (EMIS Web software; EMIS Health, Leeds, UK) to determine vaccination status in registered first-year students on arrival at the UoN and after the campus-based vaccination campaign.\n\n【7】In September, we obtained pharyngeal swab specimens from eligible students immediately before they received the MenACWY vaccine. We immediately inoculated all pharyngeal swabs onto GC selective agar (Oxoid, Basingstoke, UK) and incubated them at 37°C in air containing 5% CO 2  . After 24 and 48 hours, we selected oxidase-positive colonies suggestive of _Neisseria_ spp. and confirmed their identity by amplification of the meningococcal gene _crgA_ plus _ctrA_ and/or _porA_ , as described previously .\n\n【8】We performed PCR-based genogrouping as described previously . The Meningococcal Reference Unit, Public Health England, Manchester, UK, performed serogrouping and serotyping of MenW isolates using dot-blot enzyme immunoassay. Sequence data derived from amplified _porA_ and factor H–binding protein alleles was queried against the PubMLST/Neisseria database . We performed χ 2  tests for significance by using STATCALC (Epi Info version 7.2.0.1; Centers for Disease Control and Prevention, Atlanta, GA, USA).\n\n【9】The September sample of 769 first-year students represented 10.9% of the 7,049 first-year students registered in 2015. Carriage rates among this group increased from 14% in late September 2015 to 39% by mid-November 2015 and 46% in March 2016 . The characteristics of enrolled students and behavioral risk factors for carriage were similar at the 3 time points. The initial carriage rate of 14% was much lower for first-year students in September 2015 than in September 2009 (.2%; χ 2  \\= 34, df = 1; p<0.00001) , suggesting a reduction in meningococcal carriage in adolescents in the United Kingdom, possibly attributable to an alteration in risk factors for carriage. The MenY carriage rate for incoming students (.8%) was also lower than that detected in 2009 (.9%; χ 2  \\= 2.0, df = 1; p = 0.15) .\n\n【10】In September 2015, carriage rates were 4.2% for genogroup capsule null locus (cnl), 3.3% for B, 1.8% for Y, and 0.7% for W strains. A substantial part of the increase in carriage from September 2015 through March 2016 was the result of a notable increase (.7% to 8.0%) in carriage of MenW strains . No change in the carriage of MenY strains was detected . Of the 50 students colonized with MenW, 36 (%) had received MenACWY vaccine either before or during registration, which is consistent with the overall MenACWY coverage in our first-year student cohort. Students colonized with MenW at the second and third sampling times were distributed across all 5 dormitories, suggesting widespread dissemination, and 21 (%) of the MenW carriers in the last time point (March 2016) had been vaccinated at least 5 months before sampling .\n\n【11】Analysis of the genogroup W isolates showed that 47 (%) of 52 were serotype 2a  and harbored alleles for factor H binding protein peptide 22 and PorA P1.5,2, identical to the corresponding alleles harbored by the MenW:ST-11 clone responsible for the increase in invasive MenW disease in the United Kingdom . We examined capsular expression by serogrouping and found 32 (%) of the MenW isolates expressed the W capsular polysaccharide. Of the 21 MenW carriers in March 2016 who had been vaccinated at least 5 months before sampling, 15 (%) were harboring isolates expressing the W capsule .\n\n【12】### Conclusions\n\n【13】We detected a rapid rise in carriage of MenW among first-year students in a university setting in the United Kingdom. In comparison, carriage of MenC in adolescents and young adults in the United Kingdom, including in university students, was rare before the introduction of MenC monovalent conjugate vaccines . The rise in MenW carriage is most likely due to acquisition within student dormitories or social spaces on campus but was unexpected because no such isolates were found in a similar study at the UoN in 2008–09 , and only a limited number in a multicenter carriage study involving UK universities in 2010–11 .\n\n【14】The increase in MenW carriage among university students merits further monitoring because it could contribute further to the sustained increase in MenW disease in the United Kingdom. Two cases of MenW disease were reported in unvaccinated students in Nottingham during 2015–16. Students attending universities exhibit high mobility and may represent an ongoing vehicle for amplification and spread of MenW into communities throughout the United Kingdom or beyond, with implications for vaccination policy and future research.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0fbc41bb-42c1-4eeb-9897-8137de05e1c3", "title": "Hemorrhagic Diathesis in Borrelia recurrentis Infection Imported to Germany", "text": "【0】Hemorrhagic Diathesis in Borrelia recurrentis Infection Imported to Germany\n**To the Editor:** Relapsing fevers are paroxysmal bloodstream infections caused by spirochetes of the genus _Borrelia_ . Louseborne relapsing fever (LBRF; i.e. epidemic relapsing fever) is caused by _B. recurrentis_ and transmitted by the human body louse (Pediculus humanus_ ). Soft ticks of the Argasidae family (e.g. _Ornithodorus moubata_ ) are vectors for tickborne relapsing fever (TBRF) borreliae, which encompass several human-pathogenic species. In Europe, LBRF was epidemic in the early 20th century but is now rarely seen. We report an infection with _B. recurrentis_ imported to Germany by a Somalian refugee who had high fever and hemoptysis and describe the process of molecular diagnosis.\n\n【1】In August 2015, an 18-year-old man sought asylum in Germany after travel through Somalia, Ethiopia, Sudan, Libya, and Italy. He reported general weakness and fever while in Libya, ≈16 days before seeking care, and started coughing up blood after arriving in Italy. At hospital admission in Germany, he had a temperature up to 40.4°C, cough, and hemoptysis; his suspected diagnosis was tuberculosis. No ectoparasites were reported or found on physical examination. Abnormal laboratory findings included relative neutrophilia (% \\[reference 39%–77%\\]), thrombocytopenia (platelets 112 × 10³/μL \\[reference 160–385 × 10³/μL\\]), and prolonged activated partial thromboplastin time (APTT) . Because of highly elevated levels of C-reactive protein (mg/L \\[reference <5 mg/L\\]) and procalcitonin (.4 µg/L \\[reference <0.5 µg/L\\]), the patient was treated with ceftriaxone (g/d intravenously), metronidazole (mg/d intravenously), and paracetamol (acetaminophen). Repeated examinations of Giemsa-stained thick and thin blood slides were negative for malaria parasites. Blood cultures, tests for tuberculosis, and PCRs for Rift Valley fever, yellow fever, dengue, and chikungunya viruses also were negative. With antimicrobial therapy, the patient’s fever declined within 12 hours, but platelet counts further decreased and APTT continued to increase .\n\n【2】The patient’s symptoms and travel history raised suspicion of a spirochete infection. A plasma sample from his second day in the hospital tested positive for _Borrelia_ spp. 16S DNA by real-time PCR . Retrospective microscopy revealed a low number of extracellular spirochetes in thin blood smears . The antimicrobial regimen was changed to doxycycline (mg 2×/d) on day 7 after admission and, because species identification had not been completed, continued for 10 days. No signs of a Jarisch-Herxheimer reaction were seen. During days 4–9 after admission, APTT, platelet counts , and C-reactive protein values returned to normal, and the patient was discharged.\n\n【3】For species identification, we amplified the entire coding sequence of _glpQ_ (glycerophosphodiester phosphodiesterase) with newly designed primers . The amplicon was 100% (/1,002 bp) identical to _B. recurrentis_ A1  and 99% identical (/1,002 bp) to _B. duttonii_ Ly . A phylogenetic analysis that included 7 published _glpQ_ sequences from _B. recurrentis_ and 4 from _B. duttonii_ suggested that the detected pathogen clustered with _B. recurrentis_ and not _B. duttonii_ .\n\n【4】Borreliae have been recognized as a frequent cause of febrile infections in West and East Africa . Data on the incidence in immigrants are not available, but the recent increase in asylum seekers from East Africa arriving in Central Europe has increased attention of _Borrelia_ as a pathogen to be included in differential diagnoses of febrile infections . Because symptom onset in the patient we report occurred in Libya, he most likely acquired infection on the African continent, although local transmission in Europe can occur .\n\n【5】Blood slide examination, which would show spirochetes, is routinely requested to detect _Plasmodium_ parasites, but its sensitivity in detecting borreliae is strikingly inferior to molecular tools (%–56%, depending on laboratory conditions) . Pan- _Borrelia_ real-time PCRs enable sensitive detection of DNA in blood samples, followed by sequencing  or confirmatory PCRs for relapsing fever _Borrelia_ –specific genes (e.g. _glpQ_ ) . _B. recurrentis_ is genetically highly similar to _B. duttonii,_ suggesting it might be a degraded subset of its tickborne counterpart rather than a distinct species . Yet, phylogeny of whole _glpQ_ sequences enables separation of _B. recurrentis_ from _B. duttonii_ on the basis of distinct single-nucleotide variations. Alternatively, differentiation can be achieved by phylogenetic analysis of concatenated partial 16s, _glpQ_ and _flaB_ (flagellin) sequences . Differentiation between TBRF and LBRF is crucial for the correct clinical decision on therapy duration, independent of the antimicrobial substance chosen: at least 7 days of treatment is recommended for TBRF to prevent relapses after early invasion of spirochetes into the central nervous system , whereas a single-dose regimen is sufficient for LBRF , although longer treatment courses tend to be used.\n\n【6】In summary, our report emphasizes that LBRF can be complicated by pulmonary hemorrhages associated with impaired platelet and plasmatic coagulation , which can be mistaken for signs of tuberculosis. Considering the poor hygienic conditions among refugees, LBRF has become an important differential diagnosis in Europe in times of increasing migration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a2168048-5791-4e64-81c8-ab738c9ba607", "title": "National Home and Hospice Care Survey", "text": "【0】NOTE: This website contains information on the National Home and Hospice Care Survey (NHHCS), which was last fielded by NCHS in 2007. In 2012, NCHS initiated the National Study of Long-Term Care Providers (NSLTCP)—a biennial study of adult day services centers, residential care communities, nursing homes, home health agencies, and hospice agencies. NSLTCP uses administrative data for home health and hospice agencies obtained from the Centers for Medicare & Medicaid Services (CMS). Please visit the NSLTCP website for more information.\n\n【1】The 2007 National Home and Hospice Care Survey (NHHCS) is one in a series of nationally representative sample surveys of U.S. home health and hospice agencies. It is designed to provide descriptive information on home health and hospice agencies, their staffs, their services, and their patients. NHHCS was first conducted in 1992 and was repeated in 1993, 1994, 1996, 1998, and 2000, and most recently in 2007.\n\n【2】*   Home Health and Hospice Care Agencies Fact Sheet Cdc-pdf \\[PDF – 325 KB\\]\n*   Home Health Care Patients and Hospice Care Discharges Fact Sheet Cdc-pdf \\[PDF – 435 KB\\]\n\n【3】#### National Home Health Aide Survey\n\n【4】The first national probability survey of home health aides, was designed to provide national estimates of home health aides employed by agencies that provide home health and/or hospice care.\n\n【5】What's New\n\n【6】##### Publications\n\n【7】*   Adoption and Use of Electronic Health Records and Mobile Technology by Home Health and Hospice Care Agencies Cdc-pdf \\[PDF – 176 KB\\] (/2013)\n*   Characteristics and Use of Home Health Care by Men and Women Aged 65 and Over Cdc-pdf \\[PDF – 130 KB\\] (/2012)\n*   An Overview of Home Health Aides: United States, 2007 Cdc-pdf \\[PDF – 569 KB\\] (/2011)\n*   Home Health Care and Discharged Hospice Care Patients: United States, 2000 and 2007 Cdc-pdf \\[PDF – 416 KB\\] (/2011)\n*   Complementary and Alternative Therapies in Hospice: The National Home and Hospice Care Survey: United States, 2007 Cdc-pdf \\[PDF – 327 KB\\] (/2011)\n*   Use of Advance Directives in Long-term Care Populations (/2011)\n\n【8】##### Announcements\n\n【9】*   Corrections to the Technical Notes for the 2007 NHHCS Medication Public-Use File – Changes to selected category IDs in Multum Cdc-pdf \\[PDF – 76 KB\\] (/2011)\n\n【10】More", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7435d29a-2423-4464-bbc1-95591bc748f2", "title": "A Summary Measure of Health Inequalities for a Pay-for-Population Health Performance System", "text": "【0】A Summary Measure of Health Inequalities for a Pay-for-Population Health Performance System\nBackground\n----------\n\n【1】Because health inequality is an established field of research and policy making, we might expect that a well-tested template would be available for measuring health inequalities that could be used in a pay-for-population health performance system. However, such guidance has not yet been established. Over the past century, many empirical studies have described health inequalities , and useful guides for measuring health inequalities are now available . In the past few decades, jurisdictions and organizations have endorsed reducing health inequalities  and have focused their efforts accordingly. The World Health Organizations (WHOs) Commission on Social Determinants of Health  is a notable example of such concerted efforts. Despite these efforts, progress has been inadequate in reducing health inequalities. One reason could be the lack of an effective strategy to measure and track health inequalities.Health inequalities have most commonly been measured in a bivariate fashion, as a joint distribution of health and another attribute, such as income, education, sex, or race/ethnicity . A typical measure of bivariate health inequality assesses 1 attribute at a time, for example, different levels of health across income groups . The degree of health inequality across groups can be quantified by an index such as a range measure that compares the health of 2 groups . A more sophisticated approach assesses the level of income (or another attribute) for each individual rather than the average level of health of each group. An index that quantifies the degree of inequality can be complex, for example, the Concentration Index, which compares the health of every individual or income group . Regardless of the unit of analysis (group or individual) or the inequality index used, measures of bivariate health inequalities always assess health inequality in relation to another attribute. Around 2000, there was a brief but heated debate about whether we should continue to measure bivariate health inequalities or start measuring univariate health inequality . Regardless of their association with other attributes, measures of univariate health inequality assess health inequality across individuals in the same way that income inequality is typically assessed . A few researchers had measured health inequalities in a univariate fashion , but Murray and colleagues proposed univariate health inequality as the best focus in the assessment of population health .  **Figure 1.** A hypothetical presentation of a bivariate health inequality. Measures of bivariate health inequality assess the association of health inequality with another attribute, in this example, income.  **Figure 2.** A hypothetical presentation of a univariate health inequality. Measures of univariate health inequality assess health inequality across individuals regardless of its association with other attributes. This debate raised moral and policy questions . Health has an intrinsic importance, those who support measuring univariate health inequality argued, and we should not only be interested in health inequality by socioeconomic status, as most studies have focused on, but also in how health itself is distributed. The supporters of measuring bivariate health inequalities believed that health inequalities are significant when they are associated with other attributes, such as income. Simply put, with an example of income, this debate was about whether we should be worried about sick people regardless of their income level (univariate health inequality), or about impoverished sick people more than the wealthy sick people (bivariate health inequality). Furthermore, those who support measuring univariate health inequality argued that the choice of which attributes to study is generally driven by the investigator’s intuition or interest. Accordingly, we now have numerous empirical descriptions of health inequalities by various attributes, which are not necessarily comparable and do not immediately offer an overall picture of health inequalities. Univariate health inequality, they maintained, can offer an overall picture of health inequality in the population in a way that is comparable across populations. The advocates of measuring bivariate health inequalities, on the other hand, argued that univariate health inequality does not suggest how to tailor interventions or policies to reduce health inequalities. The result of this debate was an acknowledgment — primarily from supporters of univariate health inequality that bivariate and univariate health inequalities are complementary (though exactly how they are complementary has not been specified) . Most empirical work has continued to measure bivariate health inequalities. Regarding univariate health inequality as a rarely used alternative, however, is a missed opportunity for health inequality research and policy. This debate points to a need for a better strategy to measure and track health inequalities. This debate also suggests a strong resistance among health inequality researchers to abandoning bivariate health inequalities. They may be resistant because 1) they view health as not only intrinsically important but also as valuable in terms of its associations with other attributes, and 2) it is useful to know who is sick in order to develop policies. Arguments for measuring univariate health inequality also have merit. Lack of comparability of results and an overall view of health inequalities may be a barrier between numerous descriptions of health inequalities and effective policy making. A lesson from this debate may be that we need to develop a summary measure of health inequalities, which gives an overall picture of health inequalities in the population while maintaining pertinent information on bivariate health inequalities.  \n\n【2】Two Approaches for a Summary Measure of Health Inequalities\n-----------------------------------------------------------\n\n【3】Relevant literature suggests 2 approaches to developing a summary measure of health inequalities: the bottom-up and top-down approaches. \n\n【4】### The bottom-up approach\n\n【5】The bottom-up approach first defines attributes of interest and measures bivariate health inequalities related to these attributes separately. It then combines these bivariate health inequalities into a summary index. An example is the inequality measure developed for the _Health of Wisconsin Report Card 2007_ (hereafter, the “Wisconsin inequality measure”) . The Wisconsin inequality measure extends the Index of Disparity , a modified coefficient of variation defined as equation no. 1. Equation 1  Where _r j_ is health of the _j_ th group, _r ref_ is health of the reference group, and _J_ is the number of groups compared. The Index of Disparity is the average deviation of the health of groups compared with the reference group’s health, expressed as a percentage. When all groups have the same health, the index value is 0. Higher values suggest more inequality. The Wisconsin inequality measure calculated the Index of Disparity by using all 14 groups (sex groups, 3 education groups, 4 rurality groups, and 5 race/ethnicity groups) and converted the index to a letter grade for ease of communication. All attributes (sex, education, rurality, and race/ethnicity) are considered to be of equal importance. The reference is set as the best health level among all groups .  **Figure 3.** A simplified example of the Wisconsin health inequality measure. To obtain the overall health inequality, calculate the difference from the reference health level (rich) for each group (poor, low education, high education, male, and female), sum them, and divide by the number of groups minus 1 (= 5).  \n\n【6】### The top-down approach\n\n【7】The top-down approach first measures univariate health inequality, then breaks it down into health inequalities related to different attributes. Unlike the bottom-up approach, there is no known example of a summary measure of health inequalities using this approach. However, this approach comes close to the principal idea underlying WHO’s health inequality measurement in the _World Health Report 2000_ , and similar methods have been proposed in other contexts. For example, this approach is similar to the framework of unfair inequalities in health and health care proposed by Fleurbaey and Schokkaert , although they do not propose it for a summary measure. It is also akin to inequality measure decomposition by attributes, though in health research this technique is most often used with the Concentration Index , a sophisticated measure of bivariate health inequality. Using decomposition, we can tell which attributes (eg, education and sex) explain a bivariate health inequality (eg, income-related health inequality) and to what degree. Although the Concentration Index decomposition is a useful tool to understand bivariate health inequality, it is different from decomposing univariate health inequality as a summary measure. The top-down approach first attempts to explain the level of health of individual _i_ by determinants of health. In the simplest form, Fleurbaey and Schokkaert define such a structural model as equation no. 2. Equation 2 _h i_ \\= F(N i_ , _S i_ , _I i_ , _P i_ , _Z i_ ) Where _N_ is biologically determined health endowments, _S_ is social background, _I_ is available information, _P_ is individual preferences, and _Z_ is health care supply. At the risk of a gross simplification, empirically, _N_ might be captured by age, _S_ by income, _I_ by education, _P_ by health behavior such as smoking, and _Z_ by health insurance. Variables can be extended to the community level, for example, adding neighborhood income for _S_ , and rurality for _Z_ . The top-down approach then asks which of these determinants or attributes are, following the increasingly used term in health economics, illegitimate or result in unfair inequality across individuals. For some attributes, there is a consensus on this question. For example, health inequality associated with social background typically is considered unfair. The top-down approach measures the distribution of _h i_ (univariate health inequality) and identifies the contribution of each of the illegitimate attributes, however, defined, to univariate health inequality. Figure 4 is an example of information that the top-down approach can give. AttributeDegree of Health Inequality% ContributionOverallIncomeEducationRace/ethnicityOther (residual)**Figure 4.** An example of information given by the top-down approach. The top-down approach provides information on univariate health inequality (as overall health inequality) and identifies contributions of the attributes we select (eg, income, education, and race/ethnicity). “Other (residual)” shows univariate health inequality that is not associated with the chosen attributes.  \n\n【8】Issues for Developing a Summary Measure of Health Inequalities\n--------------------------------------------------------------\n\n【9】Which approach is better suited to develop a summary measure of health inequalities? To answer this question, I address the following 5 issues: building blocks, aggregation, value, data and sample size requirements, and communication. Building blocks are common to both the bottom-up and top-down approaches. The subsequent 4 issues separate these 2 approaches. \n\n【10】### Building blocks\n\n【11】Whichever approach we take, we should carefully choose a bivariate or univariate measure that becomes a building block of a summary measure. The building block for the Wisconsin inequality measure, an example of the bottom-up approach, is the Index of Disparity, and the Gini coefficient  can be used as a building block for the top-down approach. To decide whether they are appropriate building blocks on which to base a summary measure, we must examine the questions researchers ask when choosing health inequality measures  . All measurement properties of the Index of Disparity and the Gini coefficient coincide with the current discussion , except sensitivity to the mean (both measures) and subgroup considerations (Index of Disparity) . The literature often recommends that researchers use both an absolute (ie, translation invariant) and a relative (ie, scale invariant) measure . This recommendation reflects the lack of consensus among researchers on the issue of sensitivity to the mean. However, researchers should choose one after trying both measures and understanding the nature and limitation of the chosen measure. Policy makers and the general public should not be given 2 measures (and possibly two different answers) without guidance. Insensitivity to the group size of the Index of Disparity contradicts the recommendation in the health inequality literature . Measuring bivariate health inequality with the Index of Disparity, we would consider the 2 populations in Figure 5, with 2 groups of different sizes, have the same degree of inequality. We may judge that the degrees of health inequality in these 2 populations are different because, for example, suffering is likely to be more prevalent in Population A than in Population B, given its larger proportion of poor people . In this case, bivariate inequality measures should be sensitive to group size because a measure of inequality should reflect our perception of inequality. Sensitivity to the group size, in practice, can be incorporated in the measure by giving a proportional weight to each group .  **Figure 5.** Inequality judgment and subgroup population size. The width of the bars suggests the proportion of poor and rich people in the 2 populations. If we consider the degree of income-related health inequality differs in these populations, an inequality measure should be sensitive to this difference.  \n\n【12】### Aggregation\n\n【13】The bottom-up and top-down approaches aggregate bivariate inequalities to overall health inequality differently. The bottom-up approach aggregates bivariate inequalities arbitrarily, and the top-down approach decomposes univariate inequality into bivariate inequalities. This difference has 3 implications. First, the top-down approach can identify an independent association between each attribute and health and also interactive associations between attributes and health. Although possible, identifying independent and interactive effects is cumbersome in the bottom-up approach. The bottom-up approach starts by measuring unadjusted bivariate health inequalities, where each attribute of health inequality is measured without consideration for other attributes. We can categorize groups further, for example, from rich and poor (income) and male and female (sex) to rich male, rich female, poor male, and poor female. However, this is a time-consuming way to describe independent and interactive effects of multiple determinants of health. Second, the difference in aggregation between the 2 approaches leads to a difference in the meaning of an overall picture of health inequalities. An overall health inequality is a composite in the bottom-up approach, but it is univariate health inequality in the top-down approach. The top-down approach has a logical and mathematical hierarchy from bivariate health inequalities to univariate health inequality; the sum of bivariate health inequalities equals univariate health inequality. The bottom-up approach does not have such a hierarchy. Because each individual in the population belongs to multiple groups (eg, an individual is female, rich, educated, and minority), it is unclear exactly what an aggregation of non-mutually exclusive bivariate health inequalities means. Finally, by decomposing univariate health inequality into bivariate health inequalities, the top-down approach can identify the contribution of each bivariate health inequality to univariate health inequality and thus the relative importance of bivariate health inequalities. For example, Wagstaff and van Doorslaer  reported that income-related health inequality accounted for approximately 25% of univariate inequality in malnutrition among Vietnamese children and general health status among Canadian adults, by using a subgroup decomposition technique that focuses on 1 attribute (as opposed to multiple attributes, as I am proposing here). Because of the use of a composite to indicate overall health inequality, the bottom-up approach cannot identify the relative contribution of each bivariate attribute. \n\n【14】### Value\n\n【15】A measure can be descriptive (describing the object) or normative (incorporating our value of the object). Using either the bottom-up or top-down approach, a summary measure of health inequalities is normative in the most fundamental sense; it measures health inequalities that we value. But these approaches differ in terms of how normativity is introduced, and the top-down approach offers a richer framework than the bottom-up approach. The bottom-up approach starts by selecting attributes that we believe to be important in relation to health inequality. The top-down approach, on the other hand, starts by describing health inequalities and moves on to normative assessment of fair and unfair health inequalities . This assessment is done by selecting attributes that we believe to cause unfair health inequalities, and the top-down approach can embed the reasons these attributes are important, as Fleurbaey and Schokkaert suggest in the formation of _N_ (health endowments), _S_ (social background), _I_ (available information), _P_ (individual preferences), and _Z_ (health care supply) . These selections and considerations can be incorporated in the bottom-up approach but are not built into it. Furthermore, in either approach we must ask whether a summary measure of health inequalities should incorporate the relative importance of different attributes. According to Wagstaff and van Doorslaer , income-related health inequality explains approximately 25% of overall, univariate health inequality. If we believe that income-related health inequality is more important than other bivariate health inequalities (eg, education-, sex-, or geography-related health inequalities), then we might wish to reflect our value in the measurement by giving more weight to income-related health inequality than 25%. The Wisconsin inequality measure treats all bivariate health inequalities as equally important. The top-down approach describes the contribution of each attribute to univariate health inequality without considering which attribute is more important than others. If we wish to develop a summary measure of health inequalities to incorporate the importance of different attributes, whose values should be included and in what way? What about concentration of burden? We may not merely consider 1 attribute to be more important than another but multi-attribute correlations (for example, the sick who are poor, uneducated, and a minority) to be morally problematic. Not surprisingly, given the uncoordinated numerous descriptions of bivariate health inequalities, the current empirical health literature is silent about these value questions. \n\n【16】### Data and sample size requirements\n\n【17】Generally, the top-down approach requires more data than the bottom-up approach. The top-down approach works best with individual-level data on health and determinants of health, while the bottom-up approach can be pursued with group-level data. Population health surveys, possibly linked with census data, may offer enough information for the top-down approach, but the sample size of the survey determines how small the population can be for which a summary measure of health inequalities can be calculated. Despite the clear advantage of the top-down approach in terms of aggregation and value, data and sample size requirements may be a critical hindrance to its policy application. These considerations for data and sample size requirements are typical in any quantitative analysis, but the use of a summary measure of health inequalities for a system of pay-for-population health performance requires at least 2 further considerations. First, how sensitive should a summary measure be to changes? If we agree to reward performance in the short term (eg, in 3-5 years), a summary measure should be sensitive to changes that occur in this time frame, and data should be updated regularly. Second, for which population (eg, state, county, community) does it make the most sense to establish a pay-for-performance system? The smallest population for which data are available may not necessarily be the most appropriate size. \n\n【18】### Communication\n\n【19】Effective use of a summary measure of health inequalities demands clear communication. Ideally, a measure should be conceptually and methodologically sound and easy to communicate. The bottom-up approach is arguably methodologically simpler than the top-down approach. However, ease of communication does not necessarily equal simplicity in concepts and methods. A complex Concentration Index decomposition, similar to the top-down approach, has been increasingly used in policy-oriented work . Complex concepts and methods require an effective communication strategy. I suggest a summary measure of health inequalities using the top-down approach and a strong communication strategy when data and sample size requirements are surmountable. Compared with the bottom-up approach, it offers a conceptually clearer meaning of overall health inequality and a richer framework for choosing relevant attributes associated with health inequality. In addition, development of a summary measure of health inequalities requires clarification of value questions.  \n\n【20】Recommendations\n---------------\n\n【21】First, a system of pay-for-population health performance should incorporate measurement of health inequalities. Second, measurement of bivariate health inequalities, the most common way to measure health inequalities, may not be the most effective mechanism to reduce health inequalities. A system that rewards population health should seek to develop a summary measure of health inequalities. Third, a summary measure of health inequalities can be developed by adopting the bottom-up or top-down approach. When data are available, a summary measure using the top-down approach should be used, along with a strong communication strategy to help users understand what the measure means and how it was calculated. Finally, clarification of value questions is a high priority for development of a summary measure of health inequalities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5b1ac6b3-f2eb-44b7-85e9-5b1535d57159", "title": "Salmonella Typhimurium Outbreak Associated with Veterinary Clinic", "text": "【0】Salmonella Typhimurium Outbreak Associated with Veterinary Clinic\nZoonotic transmission of _Salmonella enterica_ has been associated with exposure to sick and healthy cattle on farms , sick cats at animal shelters , and cats at small animal veterinary clinics . Salmonellosis is a well-recognized nosocomial problem at large-animal veterinary hospitals , but it is associated with few, if any, human outbreaks. In small-animal medicine, salmonellosis is likely underrecognized because gastrointestinal illness is common and often self-limiting. As in human medicine, salmonellosis is rarely confirmed in a laboratory, which results in underreporting of cases. However, in 1999 two salmonellosis outbreaks at veterinary clinics were linked to cats with either confirmed or suspected salmonellosis . From September to October 2003, the New York State Department of Health and three local health departments identified seven human infections with _S. enterica_ serovar Typhimurium, which exhibited an uncommon pulsed-field gel electrophoresis (PFGE) pattern. These cases had an apparent link to a veterinary clinic. This report describes the outbreak investigation and underscores the importance of integrating veterinary medicine into public health surveillance.\n\n【1】### The Study\n\n【2】In September 2003, five culture-positive human cases of _S._ Typhimurium infection were identified in three adjacent counties in New York State. All five isolates were indistinguishable by PFGE and were resistant to ampicillin, chloramphenicol, sulfisoxazole, streptomycin, and tetracycline. Onset dates were July 22 to August 22, 2003 . A local veterinary hospital, clinic X, was the only exposure common to all patients . Patients 1 and 2 were veterinary technicians at clinic X, and patients 3–5 were pet owners whose pets had visited clinic X from July 15 to July 22, 2003. Laboratory surveillance identified two additional cases (cases 6 and 7) with matching PFGE patterns . Symptoms of the infection included diarrhea, cramps, fever, and nausea. The median duration of illness was 8 days. A full investigation was conducted, including a site visit, case-finding, and diagnostic testing of clients and staff of clinic X.\n\n【3】Interviews with clinic X staff and veterinary chart reviews determined that two cats and one dog owned by patients 3, 4, and 5 (pets A, B and C, respectively) were admitted on two different dates in July for dental procedures . All three procedures were performed by one veterinarian and one technician (patient 1). All three animals were held overnight, with evening treatments performed by patient 2, who works 1 evening per week. All procedures were performed in a designated room that was also used for other nonsterile procedures. The cats were held overnight in procedure room cages; the dog was held overnight either in the procedure room or in a dog run in a separate room.\n\n【4】All three animal patients were treated after the procedure with a prophylactic course of clindamycin. Pet B had a history of diabetes with chronic intermittent diarrhea attributed to diabetes-related dietary changes. The other pets had no prior illness. All three owners reported transient diarrhea in the pets after the dental procedure. Pet B developed severe mucoid diarrhea ≈5 days postsurgery and was treated with additional antibiotics (amoxicillin and enrofloxacin).\n\n【5】Patient 6 owns a dog but does not use clinic X. Patient 6 had occasional contact with his neighbor’s dog (pet D), which had been to clinic X for several overnight visits because of severe vomiting and diarrhea attributed to eating mulch. Pet D was last discharged from clinic X on July 21, 5 days before patient 6’s onset date. Whether patient 6 had contact with pet D between July 21 and July 26 is unclear.\n\n【6】The seventh patient was an emergency room nurse at a hospital in the outbreak area. She has a dog but does not use clinic X; the dog had no recent illness and had not recently been to a veterinarian. No other exposures to clinic X or other patients and pets could be identified. Stool culture of the patient’s dog was negative.\n\n【7】After identification of the outbreak in September 2003, stool cultures were collected from the pets of all patients, including healthy contact pets from the same households. Only pet B, the diabetic cat, had a positive stool specimen collected at the end of September with a PFGE match to the human isolates.\n\n【8】Clinic X is a large, multidoctor practice that primarily treats dogs and cats, although one clinician (not linked to this outbreak) sees exotic animals, including reptiles. In addition to exam rooms, procedure room, and sterile surgery suite, the practice has separate rooms for isolation, animal wards, dog runs, surgery preparation, laboratory, reception, and patient files. No animals had been placed in isolation during the outbreak. A break room and meeting room are on a separate floor of the practice.\n\n【9】Thirty-seven of 38 uninfected staff members completed questionnaires regarding exposure and illness history. Seven reported diarrhea, and two reported nausea only between June 1 and August 31, 2003. None of the staff members submitted stool cultures. Stool culture from the asymptomatic veterinarian for the case-pets was negative, but patient 1 had continued signs of illness and was still culture-positive in mid-September. She voluntarily excluded herself from direct patient care until illness resolved.\n\n【10】A review of infection control practices in mid-September 2003 did not identify significant lapses in handwashing; cleaning; or disinfecting instruments, floors, or surfaces. No food was visible in the work areas during a walk-through, but the owner reported that he frequently reminds staff to avoid eating in work areas. Twenty-three environmental swabs were taken from the procedure room, anesthesia machines, animal wards (including isolation), and the laboratory (including a microscope used for fecal parasitology exams). Samples were collected by using sterile gauze sponges dampened with sterile double-strength skim milk. All were negative for _Salmonella_ .\n\n【11】Clinic X staff telephoned dental clients treated since June 1, 2003 with a questionnaire developed by health department staff. The script asked about illness in pets or people in the household. No additional human or animal illnesses were identified.\n\n【12】### Conclusions\n\n【13】A likely source of _Salmonella_ for this outbreak was not identified. The animal with the earliest illness onset (pet D) could have contaminated the clinic. However, pet D was not confirmed with _Salmonella_ infection, was never in the dental room, and had no exposure to other case-pets. Another, unidentified animal patient may have been the source of contamination, or a person on the clinic staff may have been the source. If the clinic environment was the source of infection, cleaning apparently eliminated contamination by the time environmental specimens were collected in late September. Polymerase chain reaction (PCR) testing may have yielded different results; however, a positive result on a PCR test might represent nonviable bacterial DNA .\n\n【14】No epidemiologic link could be found to patient 7. However, out of 457 _Salmonella_ isolates tested at Wadsworth Center Laboratories from January 2003 through July 2004, only the seven human patients and one cat reported here had this PFGE pattern. Patient 7 may have been exposed to undiagnosed cases through her work at the emergency room or through some other unidentified common exposure.\n\n【15】The outbreak described here was identified because the standard questionnaire used to interview patients included animal exposure questions, which shows the importance of animal exposure history in detecting potentially zoonotic diseases. This outbreak also shows the importance of zoonotic disease education for pet owners and increased awareness of zoonotic diseases by veterinarians. As is often the case with gastroenteritis in pets, the potential for transmission to humans was not considered. Since pet owners are frequently unwilling or unable to pay for diagnostic testing, veterinarians often do not consider stool culture for animals with diarrhea, which might have prevented some human cases in this outbreak. However, even without a definitive diagnosis, veterinarians can emphasize infection control practices with staff and educate owners that, although pets are rarely confirmed as the source of human salmonellosis, zoonotic transmission of gastrointestinal illnesses from sick pets may occur. Veterinarians should emphasize handwashing and infection control in the home. This practice is particularly important for households with immunocompromised persons or young children, who could become a source of secondary infection to other children, especially in daycare settings.\n\n【16】Finally, the importance of a good working relationship between public health and the veterinary community is underscored by the strong, cooperative relationship between public health authorities and clinic X. The clinic owner and staff were enlisted early as public health partners who took an active role in preventing further cases. Increasing concern about emerging zoonotic diseases and zoonotic agents as bioweapons has raised awareness of the risk of zoonotic disease exposure for persons employed in animal health. Agriculture, veterinary, and public health agencies in many states are promoting zoonotic disease awareness among veterinary professionals. As we continue to integrate the veterinary community into public health, information from these types of outbreaks should be used to develop protocols for zoonotic disease response and education in the veterinary and pet-owning communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9d05698c-3f3f-4328-bd33-1c1b6f1f55aa", "title": "Effects of COVID-19 Pandemic Response on Service Provision for Sexually Transmitted Infections, HIV, and Viral Hepatitis, England", "text": "【0】Effects of COVID-19 Pandemic Response on Service Provision for Sexually Transmitted Infections, HIV, and Viral Hepatitis, England\nBeginning March 23, 2020, the UK government introduced social and physical distancing (SPD) measures to reduce transmission of severe acute respiratory syndrome 2 (SARS-CoV-2), and health staff were redeployed to the coronavirus disease (COVID-19) pandemic response. The staffing shift and SPD measures affected clinical services for sexually transmitted infections (STIs), HIV, and hepatitis A, B, and C (HAV, HBV, and HCV) provided through the National Health Service . We assessed the effects of COVID-19 measures in England on service provision in these areas and on health outcomes for persons with STIs, HIV, or hepatitis.\n\n【1】### The Study\n\n【2】In England, surveillance of STIs, HIV, and hepatitis relies on patient-level data on consultations, tests, diagnoses, vaccinations, treatment, and outcomes from sexual health services (SHS), general practitioners, hospital outpatient clinics, and drug treatment centers . Laboratories also submit patient-level reports of tests and diagnoses for hepatitis and chlamydia. Given the disruption in routine reporting in 2020 (only 71%–98% complete for STI and HIV data), when possible, we analyzed data from clinics and laboratories who provided complete reports for January–September in both 2019 and 2020.\n\n【3】Testing at SHS declined by 77%, from 95,455 to 22,332, for HIV and by 71%, from 391,006 to 112,441, for STIs during January–April 2020, and although there was a modest increase beginning in May, testing remained far lower than in 2019 . For January–September 2020 compared with the same period in 2019, overall numbers of tests were lower by 36% (vs. 494,433) for HIV and 28% (vs. 2,244,153) for STIs . However, the proportion of tests accessed through internet services (self-sampling kits returned directly to the laboratory with results provided by text message, email, letter, or online) increased substantially beginning in April 2020 . Internet services accounted for ≥63% of HIV and \\> 51% of STI tests during April–September 2020, compared with 25% for HIV and 22% for STIs in 2019.\n\n【4】During January–April 2020, the largest proportional declines in testing occurred among persons 15–19 years of age (% for HIV, 75% for STIs) and ≥45 years of age (% for HIV, 76% for STIs); for persons 20–44 years of age, testing for HIV declined by 76% and for STIs by 70%. The 15–19- and \\> 45-year age groups also showed the slowest relative recovery towards prepandemic levels of testing during June–September 2020. Over the same period, we observed larger proportional declines in testing among heterosexual men (% for HIV, 79% for STIs) and heterosexual or bisexual women (women who have sex with men or women; 76% for HIV, 75% for STIs) compared with gay, bisexual, and other men who have sex with men (MSM) (% for HIV, 71% for STIs) and lesbian and other women who have sex exclusively with women (% for HIV, 65% for STIs); recovery was slowest among heterosexual men. We observed the largest declines among persons of Asian (% for HIV, 77% for STIs), Black (% for HIV, 76% for STIs), and other (% for HIV, 76% for STIs) races; persons of Black race showed the slowest recovery.\n\n【5】We also observed a sharp decline in the number of persons tested for hepatitis during January–April 2020: by 63% (from 4,295 to 1,610) for HAV, 61% (from 57,392 to 22,224) for HBV, and 74% (from 43,238 to 11,250) for HCV . The number of persons tested for HCV in community drug treatment facilities showed the greatest decline (%, from 3,324 to 74) and a slow recovery to prepandemic levels; testing was 58% lower in September 2020 than for 2019 .\n\n【6】Consistent with testing patterns, the number of diagnoses for HIV, STIs, and hepatitis declined during January–April 2020, followed by a partial recovery . Bacterial STI positivity  increased during March and April 2020 (% in January 2020 vs. 17% in April 2020) then returned to 2019 and early 2020 levels, whereas HIV test positivity peaked in April 2020 (.20%) and remained at a higher level until September 2020 (.09% vs 0.10% in September 2019). By contrast, HBV and HCV positivity declined during January–April 2020 (from 0.8% to 0.4% for HBV surface antigen and from 2.8% to 1.4% for HCV antibody); although there was a slight increase thereafter, positivity remained lower for the rest of 2020 than in 2019.\n\n【7】The number of first-dose vaccinations administered to MSM at SHS during January–April 2020 fell by 97% for HAV (from 841 to 22) and human papillomavirus (from 1,507 to 47) and by 96% (from 757 to 34) for HBV . A slight increase was reported beginning in May 2020, but rates for HAV, HBV, and HPV vaccinations were >50% lower in September 2020 than in September 2019.\n\n【8】HCV treatment initiations declined by 66% (from 1,004 to 341) during January–April 2020; although recovering slightly, the overall number of treatment initiations during January–September 2020 was 27% lower than in the same period in 2019 . We saw the largest relative declines for referrals from drug services and prisons, with some recovery during June–September 2020. Delays commonly occur between HCV diagnoses and treatment, so reductions in treatment initiations likely reflected reduced access to services rather than new diagnoses alone.\n\n【9】### Conclusions\n\n【10】The COVID-19 pandemic response in England, including the introduction of SPD measures, coincided with a decline in the provision of, and access to, health services for STIs, HIV, and hepatitis. We observed the greatest decline in services that cannot be provided remotely, such as vaccination. Some reduction in infections and need for services might be a consequence of reduced exposure because of compliance with SPD measures, leading to fewer opportunities for socializing and meeting sexual partners. The partial rebound in the summer of 2020 might indicate some recovery in service provision and demand, with increased demand also influenced by changes in risk perception and behaviors. However, these levels remained below prepandemic levels.\n\n【11】Declines in the numbers of STI, HIV, and hepatitis tests  and diagnoses  after COVID-19 restrictions began have also been reported across Europe and elsewhere. Disruption in HCV treatment provision is of concern as direct-acting antivirals clear the virus, minimizing long-term harms. HCV treatment disruptions have also been reported in Spain  and Germany ; in Germany, a minority of treatment providers also reported an increase in delayed diagnoses of liver decompensation and hepatocellular carcinoma .\n\n【12】During the early stages of the pandemic, there was a rapid shift in service delivery toward online, remote, and outreach provision in England; similar shifts were reported in the United States and Croatia . While enabling service access during the pandemic, it will be important to evaluate the effects on health inequalities of changing to remote services, because hepatitis, HIV, and STIs already disproportionately affect socially disadvantaged and excluded groups. Whereas our findings suggest SHS were accessed by some populations of need, such as MSM, the decline in access by young adults and persons of Asian, Black, or other races requires further investigation. Furthermore, early indications of adverse effects on access to harm reduction and bloodborne virus testing services for people who inject drugs is concerning and requires mitigating actions. The full effects of the COVID-19 pandemic response on STI, HIV, and hepatitis infection control, including efforts to eliminate HIV and hepatitis, and longer-term health outcomes will take time to emerge. These effects warrant close monitoring and assessment to ensure services are accessible and used by all who need them.\n\n【13】Additional members of the UK Health Security Agency National STI, HIV and Viral Hepatitis Surveillance Group who contributed to data collection, analysis and interpretation: Ana Harb, Galena Kuyumdzhieva, Tamilore Sonubi, Alireza Talebi, Stephen Duffell, Mateo Prochazka, Louise Thorn, Hannah Charles, Koye Balogun, Rebecca Wilkinson, Sara Croxford, Claire Edmundson, Mark McCall, Louise Logan, Adam Winter, Helen Harris, Kate Folkard, and Emily Phipps.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d204f3d1-80f5-4689-9bd3-557662f7f0c1", "title": "Human Campylobacteriosis in Developing Countries", "text": "【0】Campylobacteriosis is a collective description for infectious diseases caused by members of the bacterial genus _Campylobacter_ . The only form of campylobacteriosis of major public health importance is _Campylobacter_ enteritis due to _C. jejuni_ and _C. coli_  . The rate of _Campylobacter_ infections worldwide has been increasing, with the number of cases often exceeding those of salmonellosis and shigellosis  . This increase, as well as the expanding spectrum of diseases caused by the organisms, necessitates a clearer understanding of the epidemiology and control of campylobacteriosis.\n\n【1】Surveillance and control of diseases of public health importance in developing countries have focused on diseases such as malaria, tuberculosis, trypanosomiasis, onchocerciasis, and schistosomiasis  . Programs for diarrhea and acute respiratory illness also exist  . These programs have extensive support from the World Health Organization (WHO).\n\n【2】_Campylobacter_ is one of the most frequently isolated bacteria from stools of infants with diarrhea in developing countries—a result of contaminated food or water . However, national surveillance programs for campylobacteriosis generally do not exist in most developing countries despite the substantial burden of disease. Most data available on campylobacteriosis in developing countries were collected as a result of support provided by WHO to many laboratories in developing countries, including grants for epidemiologic studies and Lior serotyping antisera provided by the Public Health Service of Canada . The number of reviews and updates on human campylobacteriosis in developed countries  is greater than that for developing countries . This disparity may be because _Campylobacter_ \\-associated diarrhea in developing countries is not pathogenic in patients >6 months of age. However, a community-based longitudinal study provided evidence that infection could be pathogenic beyond the first 6 months of life in developing countries  . Furthermore, the sequencing and publication of the complete genome of _C. jejuni_ NCTC 11168 have heralded a renaissance of interest in this organism, offering researchers worldwide, including in developing countries, novel ways to contribute to the understanding the organism’s biology  . Thus, to promote research and control of campylobacteriosis in developing countries, review information on human campylobacteriosis in these countries is urgently needed. We present the distinguishing features of campylobacteriosis in developing countries relative to developed countries.\n\n【3】### Incidence\n\n【4】Generally, developing countries do not have national surveillance programs for campylobacteriosis; therefore, incidence values in terms of number of cases for a population do not exist. Availability of national surveillance programs in developed countries has facilitated monitoring of sporadic cases as well as outbreaks of human campylobacteriosis . Most estimates of incidence in developing countries are from laboratory-based surveillance of pathogens responsible for diarrhea. _Campylobacter_ isolation rates in developing countries range from 5 to 20%  . Table 1 shows isolation rates for some countries according to WHO regions from studies of diarrhea in children <5 years old . Despite the lack of incidence data from national surveys, case-control community-based studies have provided estimates of 40,000 to 60,000/100,000 for children <5 years of age . In contrast, the figure for developed countries is 300/100,000  . Estimates in the general population in developing and developed countries are similar, approximately 90/100,000 , confirming the observation that campylobacteriosis is often a pediatric disease in developing countries. The isolation and incidence rates in some developing countries have increased since their initial reports  . This increase has often been attributed to improved diagnostic methods, but an actual increase in incidence was observed in _Campylobacter_ \\-associated diarrhea in the Caribbean island of Curaçao  .\n\n【5】### Age of Infection\n\n【6】In developing countries, _Campylobacter_ is the most commonly isolated bacterial pathogen from <2-year-old children with diarrhea . The disease does not appear to be important in adults. In contrast, infection occurs in adults and children in developed countries. Poor hygiene and sanitation and the close proximity to animals in developing countries all contribute to easy and frequent acquisition of any enteric pathogen, including _Campylobacter_ . Although infections in infants appear to decline with age , a comprehensive community-based cohort study in Egypt has shown that infection could be pathogenic regardless the age of the child, underscoring the need for strengthening prevention and control strategies for campylobacteriosis  .\n\n【7】### Polymicrobial Infections Involving _Campylobacter_\n\n【8】_Campylobacter_ is isolated relatively frequently with another enteric pathogen in patients with diarrhea in developing countries. In some cases half or more patients with _Campylobacter_ enteritis also had other enteric pathogens . Organisms reported include _Escherichia coli_ , _Salmonella_ , _Shigella_ , _Giardia lamblia_ , and _Rotavirus_ . Polymicrobial infections involving _Campylobacter_ are rare in developed countries  _._\n\n【9】### Isolation of _Campylobacter_ in Healthy Children\n\n【10】The recovery of _Campylobacter_ organisms from children without diarrhea is common in developing countries. In some reports the isolation rates for symptomatic and asymptomatic children were not statistically significant. Values as high as 14.9% in controls have been observed  . Acquisition of the pathogen because of poor sanitation and contact with animals early in life may explain the isolation from healthy children. _Campylobacter_ is not frequently recovered from asymptomatic persons in developed countries, as observed in the Netherlands, where a 0.5% isolation rate has been reported  .\n\n【11】### Seasonal Variation\n\n【12】In developing countries, _Campylobacter_ enteritis has no seasonal preference; in contrast, in developed countries epidemics occur in summer and autumn  . Isolation peaks vary from one country to another and also within countries . The lack of seasonal preference may be due to lack of extreme temperature variation as well as lack of adequate surveillance for epidemics .\n\n【13】### Distribution of _Campylobacter_ Species\n\n【14】_C. jejuni_ and _C. coli_ are the two main species isolated in developing countries. The isolation rate of _C. jejuni_ exceeds that of _C. coli_ , similar to observations in most developed countries . Lior biotyping and serotyping methods have been used in developing countries to subtype strains of _C. jejuni_ and _C. coli_ . Table 3 shows the distribution of the subtypes from three African countries. Biotype I was the most common, followed by biotype II. The prevalence of specific serotypes only in symptomatic children may indicate virulence traits or treatment, in cases of gastroenteritis  . Furthermore, correlation between biotypes and serotypes isolated from humans and animals indicates that campylobacteriosis is zoonotic  . Penner serotyping scheme and DNA-based typing, extensively used in developed countries, have been proposed for use in developing countries  .\n\n【15】Species other than _C.jejuni_ and _C. coli—_ such as _C. upsaliensis_ , _C. concisus_ , and aerotolerant campylobacters (Arcobacter_ )—may also be of pathogenic importance; however, diagnostic capacities to determine their distribution are lacking in developing countries  . These other _Campylobacter_ species constitute over 50% of campylobacters isolated at the Red Cross Children’s Hospital, Cape Town, South Africa, for example. (A method termed the Cape Town Protocol is used to isolate _Campylobacter_ species at this facility ). This higher incidence is also supported by a 16% isolation rate of _Arcobacter_ species in a 4-month survey from poultry drainage water in Lagos, Nigeria  .\n\n【16】### Antibiotic Resistance in _Campylobacter_ Isolates\n\n【17】_Campylobacter_ enteritis is a self-limiting disease, and antimicrobial therapy is not generally recommended. However, antimicrobial agents are recommended for extraintestinal infections and for treating immunocompromised persons. Erythromycin and ciprofloxacin are drugs of choice  . The rate of resistance to these drugs is increasing in both developed and developing countries, although the incidence is higher in developing countries. Use of these drugs for infections other than gastroenteritis and self-medication are often the causes of resistance in developing countries; in developed countries, resistance is due to their use in food animals and travel to developing countries. The increase in erythromycin resistance in developed countries is often low and stable at approximately 1% to 2%; this is not true for developing countries . For example, in 1984, 82% of _Campylobacter_ strains from Lagos, Nigeria, were sensitive to erythromycin; 10 years later, only 20.8% were sensitive  . In addition, resistance to another macrolide, azithromycin, was found in 7% to 15% of _Campylobacter_ isolates in 1994 and 1995 in Thailand  . The increasing rate of resistance to the fluoroquinolone, ciprofloxacin limits its clinical usefulness. In Thailand, ciprofloxacin resistance among _Campylobacter_ species increased from zero before 1991 to 84% in 1995  . Recent data have shown a marked increase in resistance to quinolones in developed countries  .\n\n【18】### _Campylobacter_ as a Cause of Travelers’ Diarrhea\n\n【19】Travel to a developing country is a risk factor for acquiring _Campylobacter_ \\-associated diarrhea. The diarrhea is more severe, and strains are associated with antibiotic resistance . Furthermore, campylobacteriosis acquired abroad contribute to the number of cases reported in developed countries  . Among Finnish tourists visiting Morocco, the disease was more prevalent in winter months  .\n\n【20】### Clinical Features\n\n【21】The clinical spectrum of _Campylobacter_ enteritis ranges from a watery, nonbloody, noninflammatory diarrhea to a severe inflammatory diarrhea with abdominal pain and fever. Disease is less severe in developing countries than in developed countries . In developed countries, disease is characterized by bloody stool, fever, and abdominal pain that is often more severe than that observed for _Shigella_ and _Salmonella_ infections. In developing countries the features reported are watery stool, fever, abdominal pain, vomiting, dehydration, and presence of fecal leukocytes; patients are also often underweight and malnourished  _._ In Lagos, Nigeria, _Campylobacter_ enteritis is characterized by a history of watery offensive-smelling stool lasting <5 days  .\n\n【22】### Guillain-Barré Syndrome\n\n【23】Guillain-Barré Syndrome (GBS) is an autoimmune disorder of the peripheral nervous system, which is characterized by acute flaccid paralysis. _C. jejuni_ infection is the most frequently identified infection preceding GBS  . In the developing world, sporadic GBS cases associated with _C. jejuni_ infection have been reported from Curaçao, China, India, and South Africa . A comparative study between Curaçao and southwest Netherlands indicated that disease in Curaçao was more severe, had a higher incidence of preceding gastroenteritis, and had greater seasonal fluctuation  _._ Serotype O:19 is most prevalent worldwide, although other serotypes, such as O:1, O:2, O:57, O:16, O:23, O:37, O:41, and O:44, have been reported  . _C. jejuni_ strain O:41 appears to be restricted to Cape Town, South Africa, and represents a genetically stable clone . Detailed studies of the role of GBS in acute flaccid paralysis in developing countries, especially in polio-endemic areas, are needed.\n\n【24】### _Campylobacter_ Infection in the Setting of HIV\n\n【25】_Campylobacter_ \\-associated diarrhea and bacteremia occur in HIV/AIDS patients worldwide. The species isolated include _C. jejuni_ , _C. coli_ , _C. upsaliensis_ , _Arcobacter butzleri, Helicobacter fennelliae_ , and _H. cinaedii_ . The incidence of clinical manifestations is higher than in HIV-negative patients, with substantial mortality and morbidity. Furthermore, antibiotic resistance and recurrent infections have been observed  . The incidence of HIV/AIDS is higher in developing countries than in developed countries and contributes substantially to deaths among <5-year-old children in epidemic settings  . Thus, infants in developing countries are at risk of impaired immunity to _Campylobacter_ enteritis. In addition, HIV/AIDS can increase the number of cases of campylobacteriosis in the adult population in these countries. These observations further support the need for improved understanding of the epidemiology of campylobacteriosis in developing countries.\n\n【26】### Immunologic Aspects\n\n【27】In developing countries, such as Bangladesh, Thailand, Central African Republic, and Mexico, healthy children and adults are constantly exposed to _Campylobacter_ antigens in the environment. As a consequence, serum antibodies to _Campylobacter_ species develop very early in life in children in developing countries, and the levels of such antibodies tend to be much higher than those in children in the developed world such as in the United States . In Nigeria, children who had diarrhea and children who were healthy both had antibodies in their sera that could agglutinate _C. jejuni_ ; the difference in antibody responses between these groups of children was not statistically significant  . Thus, antibody responses alone should be interpreted with caution in diagnosing _Campylobacter_ infections.\n\n【28】In spite of shortcomings in the use of antibodies for diagnosis, increase in the level of anti-flagellar antibody had an inverse correlation with the rates of _Campylobacter_ enteritis in the Central African Republic  . An age-related relationship in the development of immunity to _Campylobacter_ antigens has also been suggested to account for the age-related declines in the case-to-infection ratio and the period of excretion during the convalescent phase .\n\n【29】Usually, as age increases, level of antibody tends to increase. At the earliest stages in life (first 6 months), immunoglobulin (Ig) A, IgG, and IgM levels in response to _Campylobacter_ infection are minimal, but thereafter increases are observed in response to infection. The poor serologic response during the first 6 months of life may be due either to a primary response to _Campylobacter_ or to the presence of maternal antibodies via the placenta or breast milk.\n\n【30】Breast-feeding has been reported to play a role in _C. jejuni-_ induced diarrhea. It decreases the number of episodes and the duration of diarrhea  . In Algeria, exclusively breast-fed infants had fewer symptomatic _Campylobacter_ infections than infants who were both breast-fed and bottle-fed  .\n\n【31】Results of experimental observations among Mexican children have also shown that immunity to _Campylobacter_ after primary infection may prevent bloody diarrhea from developing and subsequently prevent any disease from manifesting  . In the developed world, the epidemiology may be different because most cases are usually primary infections with more severe clinical manifestations, greater numbers of people with bloody diarrhea (%, as opposed to 15% in developing countries), and a more prolonged duration of excretion (approximately 15 days, compared with 7 days in developing countries)  . The widespread immunity seen among adults in developing countries is absent in adults in developed countries  .\n\n【32】### Sources of Human Campylobacteriosis\n\n【33】_Campylobacter_ infection is hyperendemic in developing countries. The major sources of human infections are environmental contamination and foods. Human-to-human transmission as a result of prolonged convalescent-phase excretion and high population density have also been suggested , although observations from developed countries show these are less likely factors  .\n\n【34】### Environmental Contamination\n\n【35】Wild birds as well as domestic and companion animals are known reservoirs for _Campylobacter_ species, and shedding of the bacteria from them causes contamination of the environment. _C. jejuni_ and _C. coli_ have been isolated from chickens, goats, sheep, and pigs in developing countries . Strains isolated from human and chickens were phenotypically and genotypically correlated, confirming that chickens are an important source of human campylobacteriosis in developing countries  . Poultry is also an important source of campylobacteriosis in developed countries. Extensive epidemiologic investigations have been done in those countries to identify sources of contamination and routes of transmission to humans to facilitate control efforts  . Risk factors for acquiring campylobacters in developing countries include presence of an animal in the cooking area, uncovered garbage in cooking areas, and lack of piped water  .\n\n【36】### Foods\n\n【37】_Campylobacter_ \\-contaminated foods—the result of poor sanitation—are an important potential source of infection in humans. For example, campylobacters were isolated from 40% and 77% of retail poultry meat sold in Bangkok, Thailand, and Nairobi, Kenya, respectively . The serotypes of the organisms isolated in Thailand were similar to those of organisms isolated from humans. In Mexico City, a survey of ready-to-eat roasted chickens showed that they were contaminated with campylobacters  . In developed countries, risk factors associated with foods include occupational exposure to farm animals, consumption of raw milk or milk products, and unhygienic food preparation practices  .\n\n【38】### Estimates of Impact of Human Campylobacteriosis in Developing Countries\n\n【39】The Disability Adjusted Life Year (DALY) is the basic unit used in Burden of Disease (BoD) methodology to quantify the impact of disease on a population  . DALYs have been applied in the Dutch population to measure the mean health burden of _Campylobacter_ \\-associated illness in the period 1990–1995  . The mean estimate was 1,400 DALYs per year; the main determinants of health burden were acute gastroenteritis (DALYs), gastroenteritis-related mortality (DALYs), and residual symptoms of GBS (DALYs). Although data on DALYs due to campylobacteriosis in developing countries are not available, diarrhea, which is a clinical manifestation of campylobacteriosis, was one of the top three causes of death and disease in developing countries in 1990  . The disease is projected globally to remain one of the top 10 by 2020. (The burden of campylobacteriosis in developing countries may increase by 2020 because HIV is projected to move up to the 10th position from 28th by 2020.) Considering the higher incidence of campylobacteriosis in developing countries, DALYs for the disease in developing countries will likely be higher than those of the Dutch population.\n\n【40】### Conclusions\n\n【41】The incidence of human campylobacteriosis is increasing worldwide and has attracted the attention of WHO . Ssubstantial gaps in knowledge about the epidemiology of campylobacteriosis in developing countries still exist. Present reported estimates of incidence are based on isolation rates from laboratory- and community-based studies conducted from 1980 to 1995. When various socioeconomic and health changes in developing countries are taken into account, these values may have changed considerably. Thus, public health awareness about the problem is needed, as are strengthened diagnostic facilities for campylobacteriosis, with a view towards setting up national surveillance programs. Such programs would determine the incidence rates, epidemiologic risk factors, interaction of HIV/AIDS and campylobacteriosis, seasonal variation, current state of resistance to antimicrobial agents, role of species other than _C. jejuni_ and _C. coli,_ and the role of campylobacteriosis in GBS. Collaboration among researchers in developed and developing countries needs to be strengthened, leading to development of regional centers of excellence. Funding organizations should provide incentives for North-South collaborations in _Campylobacter_ research, as is done in other diseases such as malaria and trypanosomiasis that are endemic in some developing countries. All these should contribute to understanding of the global epidemiology of human campylobacteriosis.\n\n【42】Akitoye O. Coker is a Consultant Microbiologist and Professor of Medical Microbiology at the University of Lagos, Nigeria. He pioneered research into _Campylobacter_ enteritis in Lagos, Nigeria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "46f929c9-84a4-42f1-94f6-44fb24b23b13", "title": "Effectiveness of the MOVE! Multidisciplinary Weight Loss Program for Veterans in Los Angeles", "text": "【0】Effectiveness of the MOVE! Multidisciplinary Weight Loss Program for Veterans in Los Angeles\nAbstract\n--------\n\n【1】**Introduction**  \nThe purpose of this study was to evaluate the effectiveness of the MOVE! Weight Management Program for Veterans (MOVE!) in achieving weight loss in veterans who attended the multidisciplinary weight management program in the VA Greater Los Angeles Healthcare System.\n\n【2】**Methods**  \nFrom April 1, 2006, to December 31, 2009, 382 veterans enrolled in the MOVE! program; 377 veterans attended at least 3 group sessions and were included in this study. All veterans were encouraged to complete 8 weekly group sessions on nutrition, lifestyle changes, and behavior modification in a group setting led by a multidisciplinary team. After completing the session, veterans had the option of continuing with a support group that meets monthly. The change in weight from 1 year pre-enrollment in MOVE! to 1, 2, and 3 years postenrollment was analyzed.\n\n【3】**Results**  \nVeterans gained 1.4 kg per year (standard error \\[SE\\] = 0.47, _P_ \\= .003) before enrolling in MOVE!. One year after the enrollment participants lost on average 2.2 kg (SE = 0.42; _P_ < .001). The pre-enrollment slope for weight change was significantly different from the postenrollment slope.\n\n【4】**Conclusion**  \nFindings from this study support the need for a long-term weight management program such as MOVE! in primary care settings to assist overweight and obese VA patients in achieving and maintaining weight loss to reduce the risk and progression of age-related chronic diseases such as diabetes and heart disease.\n\n【5】Introduction\n------------\n\n【6】Obesity is associated with higher rates of hypertension, dyslipidemia, diabetes mellitus, degenerative joint disease, and coronary artery disease . These diseases and their treatment may increase the risk of weight gain and make weight loss more challenging . Obesity and overweight are also associated with poor quality of life, premature death, and increased health care costs. Lifestyle factors including excess intake of calories and low levels of physical activity are central causes of obesity, and the combination of increased physical activity, reduced caloric intake, and other behavior modifications results in weight loss for overweight people .\n\n【7】The prevalence of overweight and obesity in veterans is higher than in the general population but may be similar when the demographics of the veteran population are considered . Programs that can help participants achieve weight losses as small as 5% of their body weight can help them reduce the risk of chronic conditions such as type 2 diabetes . Intensive interventions such as the Diabetes Prevention Program and the Look Ahead trials have been effective in helping participants achieve clinically significant weight loss .\n\n【8】Adapting these research programs into ambulatory programs that can be feasibly delivered through the US Department of Veterans Affairs (VA) is challenging; the Veterans Health Administration (VHA) provides care to almost 6 million veterans through a nationwide network . In response to the challenge of obesity in VHA, the MOVE! Weight Management Program for Veterans (MOVE!) program was piloted between 2002 and 2004 and was nationally implemented in 2006. By 2009, nearly all (.7%) of the 155 medical centers in VHA reported having MOVE! programs in place . In the Greater Los Angeles VA, a multidisciplinary MOVE! weight management program has been in place since April 2006. However, a recent survey of VA patients in 4 Western states  indicated that less than 5% of veterans who were candidates for MOVE! participated. Therefore, optimizing this program and increasing general awareness of its beneficial long-term results could be used to market this no-cost program to more veterans. In that cross-sectional and longitudinal study , women were more likely to participate than men but were less likely to have clinically relevant weight loss. Therefore, improvements to the MOVE! program can be made in different locations, and different features can be emphasized. Evaluation of components that appear to be working among the various sites of this VA program is needed.\n\n【9】The purpose of this study was to determine significant predictors of weight loss at 3 years in veterans who attended the MOVE! multidisciplinary weight management program at the VA Greater Los Angeles Healthcare System (VAGLAHS). Findings from this study can provide evidence to support the need for a lifestyle modification program such as MOVE! in primary care settings to assist overweight and obese veterans in managing their weight.\n\n【10】Methods\n-------\n\n【11】### Participants\n\n【12】This study was approved by the VAGLAHS institutional review board. All charts of veterans enrolled in MOVE! from April 1, 2006, to December 31, 2009, were reviewed. Veterans were referred to MOVE! by their primary care provider if they were obese (body mass index \\[BMI\\] >30 kg/m 2  ) or overweight (BMI 25.0−29.9 kg/m 2  ) and had comorbid conditions such as hypertension, hyperlipidemia, and type 2 diabetes.\n\n【13】### MOVE! program\n\n【14】The VA National Center for Health Promotion and Disease Prevention (NCP) developed MOVE! to provide a standardized format for weight management . To disseminate the program, NCP created handouts for veterans, training modules for staff, curricula for group sessions, weight management assessment tools, and methods for electronic tracking of participation in program activities. Each facility was permitted to determine its own methods to identify veterans for the program and the types and extent of offerings in the program. All veterans enrolled into the MOVE! program at VAGLAHS attended a 2-hour nutrition class led by a dietitian. Veterans then had the option of participating in 8 weekly multidisciplinary education sessions in a group setting. Those sessions lasted 1 hour and were led by physicians, dietitians, physical and recreational therapists, and psychologists. Group sessions focused on a particular theme and included nutrition, physical activity, and behavioral health perspectives.\n\n【15】Typically, during the first encounter, staff provided an overview of the program and instructed veterans to complete a 23-item questionnaire on their diet, physical activity, health status, and prior weight loss attempts. An individualized report was then generated; it included a list of recommended print-ready materials on nutrition, physical activity, and healthy behavior changes available from the MOVE! website . Veterans were encouraged to set realistic and attainable goals and were instructed that a sustainable rate of weight loss was about 0.5 to 1 kg per week. In addition to group-based sessions, a one-on-one counseling session was provided as needed.\n\n【16】### Weight measurement\n\n【17】Pre-enrollment and postenrollment weight was obtained from veterans’ electronic medical records. Weight was measured and entered by the medical staff when the veteran attended his or her regularly scheduled medical appointment. Pre-enrollment weights consisted of measurements from 1 year before enrollment in the program, and postenrollment weight consisted of measurements available in the veteran’s medical record postenrollment in MOVE!. A 3-month window was used for pre-enrollment weights and at 1-, 2-, and 3-year follow-up, whereas a 1-month window was used at 3- and 6-month follow-up to increase the likelihood of obtaining a valid weight value from the medical record. Demographic characteristics, baseline weight, and comorbid conditions for the MOVE! participants were obtained, and all active medical diagnoses in patients’ medical records were included in the analysis.\n\n【18】### Statistical analysis\n\n【19】Linear mixed-effects regression models with a piecewise linear function specified for each study participant (ie, participant-level random effects) were used to evaluate changes in body weight before and after enrollment in MOVE!. Covariates were age, sex, and linear piecewise-time segments (slopes: before MOVE!, and 0 to 1 year, >1 to 2 years, and >2 years post-MOVE!). The advantage of using a piecewise approach was that we could estimate the slopes simultaneously because we expected the time trends to be different for different time windows. The slopes before MOVE! versus 1 year post-MOVE! were estimated and compared through model contrasts. The model included participant-level random effects (ie, individual’s intercept and 4 slopes) to account for correlation among repeated observations among participants. For exploratory purposes, we conducted several subgroup analyses for the selected comorbid conditions using the same modeling approach as in the main analysis with the following additional terms in the model: condition (yes vs no) and condition-by-time interaction terms (for each time segment). All statistical analyses were conducted using SAS version 9.2 for Windows (SAS Institute, Inc, Cary, North Carolina).\n\n【20】Results\n-------\n\n【21】### Demographic characteristics\n\n【22】A total of 382 veterans enrolled in the MOVE! program . Approximately 88% of the veterans were men; the average age for the veterans at time of enrollment was 60 (standard deviation \\[SD\\] = 11.1; range, 25–92), and 6.5% of them were older than 75. The average body weight at time of enrollment was 110 kg (SD = 23.6 kg; range, 54.7–194.2 kg), and more than 80% of participants had a BMI of 30 kg/m 2  or higher. More than 70% of veterans had hyperlipidemia or hypertension; approximately 40% had mood disorder or type 2 diabetes; 26% to 33% had a substance abuse condition, osteoarthritis, obstructive sleep apnea, or posttraumatic stress disorder; and less than 20% had gastroesophageal reflux disease, anxiety, coronary artery disease, or schizophrenia.\n\n【23】### Weight change pre-MOVE! and post-MOVE! enrollment\n\n【24】MOVE! participants who attended at least 3 sessions were included in the weight-change analysis (N = 377). Results from the mixed-effects piecewise regression model, adjusting for age and sex, indicated that veterans gained an average of 1.4 kg per year (standard error \\[SE\\] = 0.47, _P_ \\= .003) before they enrolled in MOVE! . After enrolling in the program, veterans on average lost 2.2 kg per year (SE = 0.42; _P_ < .001) 1 year post-MOVE!. The difference in slopes pre-MOVE! and 1 year post-MOVE! was significant (P_ < .001). We observed a nonsignificant increase in weight (.95 kg; SE = 0.57) during the second year post-MOVE! and a nonsignificant decrease in weight (.54 kg; SE = 0.76) after the second year of MOVE! . Older age (P_ \\= .01) and female sex (P_ < .001) were significantly associated with lower body weight at baseline.\n\n【25】A significant weight loss (.68 to 2.89 kg/y) was observed 1 year post-MOVE! among veterans who did not have comorbid conditions, except for those with hyperlipidemia or hypertension . A significant weight reduction 1 year post-MOVE! was observed for veterans who had the following comorbid conditions: osteoarthritis (.67 kg/y), type 2 diabetes (.02 kg/y), obstructive sleep apnea (.79 kg/y), gastroesophageal reflux disease (.47 kg/y), hyperlipidemia (.54 kg/y), and hypertension (.57 kg/y). One hundred sixty-two (%) veterans had at least 1 psychiatric condition (anxiety, posttraumatic stress disorder, or schizophrenia), and 287 (%) veterans had 3 or more comorbid conditions. Veterans who did not have a psychiatric condition showed significant weight reduction 1 year post-MOVE! (.89 kg/y, _P_ < .001); the difference in 1-year post-MOVE! slopes for veterans without versus with psychiatric conditions was not significant (slopes: 1.25 vs 2.89 kg/y, _P_ \\= .052). Similarly, veterans who had 3 or more comorbid conditions showed significant weight reduction 1 year post-MOVE! (.05 kg/y; SE = 0.48; _P_ < .001). No significant difference in 1-year post-MOVE! slopes between veterans who had 3 or more comorbid conditions versus those who had fewer than 3 comorbid conditions was found.\n\n【26】Discussion\n----------\n\n【27】We assessed treatment effects of MOVE! in a large sample of overweight and obese veterans by comparing the pre-MOVE! and post-MOVE! weight trajectory of veterans. Results indicated that veterans gained an average of 1.4 kg per year before enrolling in MOVE! and lost an average of 2.2 kg per year after enrolling in MOVE!; enrollment in MOVE! appeared to prevent further weight gain. Our results are consistent with those reported by Dahn et al , who assessed the trajectory of change in weight postintervention (and 12 months postenrollment) from a preintervention period (and 5 years before enrollment). The sample consisted of 862 veterans participating in MOVE! at the Miami VA. All veterans participated in a 2-hour self-management support session, which involved completion of a self-assessment questionnaire and a nutrition education group session. After completing the self-management support session, veterans had the option of continuing with supportive group sessions, which included 10-weekly group sessions led by a multidisciplinary team.\n\n【28】Williamson and colleagues  identified behavioral and dietary components as key parts in adherence to a weight management program. Behavioral adherence, including attendance of counseling sessions and self-monitoring, predicted reductions in body weight, waist circumference, and body fat . In our VAGLAHS MOVE! program, we aimed to create an estimated 500-calorie deficit daily in a program tailored to individual needs based on medical history, weight and weight management history, motivational factors, barriers to modifying physical activity, diet and weight-related behavior, and veterans’ readiness to change these behaviors. Staff assisted the patient with setting 1 to 3 specific short-term nutrition, physical activity, or behavior-change goals and supported the veteran weekly for 8 weeks and then through attendance of monthly weigh-ins and lectures. In this study, participants who attended the MOVE! program session at least 3 times demonstrated significant weight loss in 1 year and 2 years, suggesting the need for ongoing support to achieve long-term weight management success in a veteran population.\n\n【29】Multimorbidity was found in all age groups in this study. Major consequences of multimorbidity are disability and functional decline, poor quality of life, and high health care costs . VHA users have more physical and mental health conditions than people in community health care settings . Obesity-associated conditions such as hypertension, diabetes, ischemic heart disease, and arthritis are also prevalent in the VHA population . Lifestyle interventions result in reduction of obesity comorbidities, including mobility improvement . Long-term weight loss in epidemiological studies was associated with reduced risk of type 2 diabetes and may be beneficial for cardiovascular disease . A successful weight management program can help veterans to prevent weight gain, lose weight, decrease risk of long-term metabolic complications, improve quality of life and reduce the costs of medical care. These factors justify the additional expense of providing ongoing support after achievement of desired weight loss in the MOVE! program.\n\n【30】The prevalence of obesity among older adults has increased during the past 20 years and will affect both medical and social services. Along with an increased risk of cardiovascular disease, diabetes, and several cancers, obesity is associated with increased risk of physical and cognitive disability . However, little attention has been given to the issue of weight management among community-dwelling older adults. Intentional weight loss in obese older adults has not been widely advocated by health care providers because of the uncertainty of whether the benefits outweigh the risks and the potential for sarcopenia and loss of muscle mass, which can result in frailty . Limited data for older adults show that intentional weight loss is effective in improving physical function, quality of life, and the medical complications associated with obesity in older people . Therefore, weight-loss therapy that minimizes muscle and bone loss is recommended for older people who are obese and who have functional impairments or medical complications that can be ameliorated through weight loss. The veterans we studied had a mean age of 60, and the oldest participant was 92.\n\n【31】Our study has limitations. We evaluated the effectiveness of MOVE! as a large-scale, hospital-based program targeting overweight and obese veterans. Although the results might underestimate intervention effects, they provide a more realistic estimate of change given the actual clinical limitations affecting both veterans and treatment providers. Furthermore, the findings from this study support the implementation of a prevention-oriented health program that has been called for by VHA policy and clinical practice guidelines. The effects of the program should be further addressed by examining the implications of weight maintenance and weight reduction on health outcomes (including medication use and number of newly diagnosed cases of diabetes or cardiovascular disease) and health-care costs. Veterans receive care, including MOVE!, without cost, but they do not receive compensation for their participation in the program as participants in randomized control trials do. Future studies should examine the effect of monetary rewards on program participation, attrition, and weight loss maintenance.\n\n【32】A behavioral, multidisciplinary group weight-management program for veterans implemented at VAGLAHS was effective in achieving and maintaining weight loss over a 3-year period. Findings from this study support the need for a lifestyle modification program such as MOVE! in primary care settings to assist overweight and obese veterans in managing their weight over the long term and should include a maintenance program with monthly visits.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a3e792fb-33e1-4440-afe9-42dd074dc885", "title": "Influenza Virus Type A Serosurvey in Cats", "text": "【0】Influenza Virus Type A Serosurvey in Cats\n**To the Editor** : Recent reports of cats positive for H5N1 type A influenza virus  raised the hypothesis that cats might have an epidemiologic role in this disease. Experimental findings seem to support this hypothesis. Experimentally infected cats might act as aberrant hosts (as do humans and other mammals), with symptoms and lesions developing and the virus subsequently spreading to other cats . The experimental conditions under which this occurs, however, can rarely be observed for domestic or wild cats. No spontaneous cases of transmission from cat to cat or cat to mammal have been reported, and scientifically validated reports about spontaneous disease in cats are rare . Reports about cats with circulating influenza virus antibodies are even more rare and occur in unusual epidemiologic situations . The true susceptibility of cats to type A influenza viruses in field conditions thus remains to be elucidated.\n\n【1】Based on the assumption that partially susceptible animals should mount an antibody response, we investigated the possible presence of antibodies against the nucleocapsid protein A (NPA), a common antigen of type A influenza viruses, expressed by both avian and human strains , in feline serum samples stored at the University of Milan and collected from 1999 to 2005. Only samples for which complete information regarding the cat (owned vs. free-roaming) and its health status were included in the study. Cats were grouped as healthy or sick on the basis of clinical signs; a complete clinicopathologic screening that included routine hematologic tests, clinical biochemical tests, and serum protein electrophoresis; serologic tests for feline immunodeficiency virus and feline leukemia virus infection, which are known to induce immunosuppression; and information regarding the follow-up, including postmortem examination for dead animals. Specifically, 196 serum samples satisfied the inclusion criteria in terms of anamnestic information about the sampled cat and, according to the above-mentioned diagnostic approach, cats were grouped as reported in the Table . Owned cats were mainly living in the urban area of Milan. By contrast, approximately half of the free-roaming cats included came from rescue shelters from a rural area northwest of Milan. Sixty samples (.8%) from owned cats and 51 samples (.2%) from free-roaming cats were collected from September to February, when seasonal human influenza peaks.\n\n【2】Serologic tests for antibodies to type A influenza virus were performed with a competitive ELISA to detect NPA antibodies . Negative control serum from specific-pathogen-free chickens and positive control serum specimens from different species (avian, swine, and equine) were included in each plate to provide a full range of controls. Serum samples were considered positive when the absorbance value was reduced to at least 75% compared with 100% for negative control wells.\n\n【3】All cats were negative for type A influenza virus antibodies. The ELISA we used has been validated in several species, including humans . Antibodies against NPA are not a major response to influenza infection but likely would have been detected if infections of cat were widespread. Thus, although no positive feline serum samples were used as positive controls, the negative results are not likely false negatives. Indeed, the negative results of many cats included in the study (the free-roaming ones, especially those affected by severe illness, for which a natural cat/flu virus interaction is unrealistic) might be due to low exposure to the virus because avian influenza outbreaks never occurred in the sampling area included in this study . By contrast, many owned cats (those sampled during the winter) likely were exposed to human type A influenza viruses, since approximately half of the viruses responsible for human seasonal influenza isolated in Europe, especially in Italy, are type A . The close contact between pets and their owners probably exposed cats to these viruses; nevertheless, none of the pet cats seroconverted, even when they had severe systemic diseases or viral induced immunosuppression. Although the number of cats in this study might be statistically insufficient to show low seroprevalences, our results further support the hypothesis that, in field conditions, cats are most probably not susceptible to type A influenza viruses, especially to the human ones (e.g. H3N2, the most diffused among humans, which also did not induced symptoms or lesions in experimental conditions ) circulating in the “pre-cat flu era.” In futures studies, these results can be used to compare the results of seroepidemiologic investigations among cats living in sites contaminated by avian viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2554e687-1401-402f-9f8b-1ffe62e8f155", "title": "Burkholderia pseudomallei Isolates in 2 Pet Iguanas, California, USA", "text": "【0】Burkholderia pseudomallei Isolates in 2 Pet Iguanas, California, USA\n_Burkholderia pseudomallei_ , a gram-negative bacterium, is the causative agent of melioidosis. Melioidosis is endemic in countries in Southeast Asia and in northern Australia, and has been sporadically reported from Central and South America . In the United States, most case-patients have traveled to disease-endemic areas .\n\n【1】_B. pseudomallei_ infection occurs through direct cutaneous inoculation with soil or water containing _B. pseudomallei_ and through ingestion or inhalation of aerosolized bacteria. In humans, the incubation period is typically 1–21 days, but some patients demonstrate clinical signs years after exposure . Acute melioidosis can manifest as a severe pneumonia and septicemia, with death rates >40% in countries where access to medical care is limited. In chronic melioidosis, abscesses occur in various organs, including the lungs, liver, spleen, and cutaneous sites . In animals, abscesses and acute illness are common .\n\n【2】_B. pseudomallei_ is classified by US federal agencies as a tier 1 select agent. Tier 1 agents are believed to pose the greatest threat for deliberate misuse and potential harm to public health. Multiple regulations restrict access to these agents and reduce the risk of their release from secure settings . Infection is generally diagnosed by culture. Commercially available bacterial identification systems may provide initial identification; however, _B. pseudomallei_ may be misidentified by some systems . Other identification tests are available, including PCR and antigen detection; these are not commonly used outside disease-endemic regions .\n\n【3】### Case Reports\n\n【4】A 2.5-year-old female green iguana (Iguana iguana_ ) was referred to the William R. Pritchard Veterinary Medical Teaching Hospital, University of California, Davis, in 2007 for evaluation of a coelomic mass and leukocytosis (.0–42.0 × 10 3  cells/mL \\[reference 3–10 × 10 3  cells/mL\\]). Radiographs and ultrasound results confirmed multiple hepatic masses. Initial aspiration and culture of 1 mass yielded 3 colonies of a _Bacillus_ spp. Empirical antimicrobial drug therapy with ceftazidime (mg/kg intramuscularly every 48 h for 30 d) was initiated. The iguana was brought for treatment of additional coelomic masses 6 months later. Aspiration of the masses and culture of the sample yielded small colonies of suspected _B. pseudomallei_ ; blood samples collected for culture were negative for the organism. Results of in-house biochemical testing and PCR were consistent with _B. pseudomallei_ or _B. mallei_ . The culture was submitted to Stanford University School of Medicine, Department of Comparative Medicine. The Biolog Dangerous Pathogens Database ID system  identified _B. pseudomallei_ . Laboratory Response Network PCR protocols and biochemical testing performed at the Santa Clara Public Health Laboratory (Santa Clara, CA, USA) confirmed _B. pseudomallei_ .\n\n【5】The isolate was then sent to the Centers for Disease Control and Prevention (CDC) for further characterization by multilocus sequence typing (MLST), a method of molecular subtyping that compares sequences from 7 housekeeping genes . This isolate’s sequence type (ST) isolate is ST518 . No environmental testing was performed. Treatment included trimethoprim/sulfamethoxazole (mg/kg by mouth every 24 h) and doxycycline (mg/kg by mouth every 24 h), according to published treatment regimens and in-house susceptibility testing . Against recommendations, therapy was discontinued by the owner after 3 months. Re-evaluation of the patient 13 months after diagnosis revealed persistent leukocytosis. Euthanasia of the iguana was recommended, but the owner declined. The iguana died at home ≈2.5 years after initial diagnosis; a necropsy was not permitted.\n\n【6】In December 2012, a 1.6-year-old female green iguana was brought to a veterinarian in Los Angeles County, California, ≈400 miles south of the first case. An abscess, ≈40 mm in diameter, appeared on the iguana’s left shoulder 2 days after it fell from a height of 1 meter. The abscess was surgically removed and submitted to a commercial veterinary laboratory, which cultured β-hemolytic _Streptococcus_ spp. and large numbers of unidentified gram-negative rods. Marbofloxacin (.5 mg/kg by mouth, every other day, for 40 days) was prescribed. The laboratory forwarded the unidentified isolate to the Sacramento Public Health Laboratory, which identified it as _B. pseudomallei_ by using Laboratory Response Network PCR protocols and biochemical testing. The isolate was forwarded to CDC where MLST was performed. The isolate was ST518, which matched the isolate from the first infected iguana and another isolate recovered from a tourist from Arizona who had been infected in Costa Rica in 2010 .\n\n【7】Staff from Los Angeles County Department of Public Health visited the iguana owner’s home 3 weeks after the iguana completed marbofloxacin treatment. The iguana had a firm swelling on the left shoulder that was ≈25-mm in diameter, with a central flat, red crust that was 5 mm in diameter . The owner had not yet disinfected the animal’s housing. Sterile rayon swabs in liquid Amies medium were used to collect swab specimens from the iguana: 1 from the bottom of the feet, 1 from inside the cloacal opening (vent), and 2 from the red crust on the shoulder swelling. Four specimens were also collected from the iguana’s housing: 50 mL of water from its aluminum water bowl, a fecal sample in the water, a cloth containing feces from its tank, and a biofilm sample, recovered with a sterile polyurethane foam swab, from the inside of the emptied water bowl. The 8 samples underwent culture and type III secretion system real-time PCR for _B. pseudomallei_ . PCR results from the water sample, the water bowl biofilm, and a shoulder swab specimen were positive. _B. pseudomallei_ was cultured from the second shoulder swab specimen. CDC confirmed the isolate as _B. pseudomallei_ . The owner euthanized the iguana, and tissue samples were collected in a Biosafety Level 3 cabinet at Los Angeles County Department of Public Health. The iguana’s shoulder swelling had grown to ≈30 mm in diameter, and multiple 5-mm yellow masses were found in the iguana’s liver, lungs, and spleen.\n\n【8】Both owners stated that the iguanas had been purchased at young ages from local pet stores, had been kept indoors, and had not left California. The pathogen’s zoonotic potential was discussed with the owners and veterinarians in both cases. The owners were advised to consult with their physicians about potential exposure and were advised to use personal protective equipment (e.g. gloves, masks, and dedicated clothing) when handling the pets, to wash their hands afterward, and to disinfect the pets’ environment with 0.5%–1% sodium hypochlorite. No pet-related _B. pseudomallei_ infections in humans have been reported.\n\n【9】### Conclusions\n\n【10】This report identifies _B. pseudomallei_ isolates in 2 pet green iguanas and the environment of one of the infected pets. Their clinical disease developed when they were adults and was refractory to antimicrobial drug treatment. The second iguana suffered mild trauma shortly before its abscess appeared. Human case reports suggest that trauma may trigger the onset of disease .\n\n【11】Most green iguanas entering the pet trade in the United States originate from Central America . Both iguanas in this report and a person exposed in Costa Rica were infected with isolates with an identical sequence type by MLST. These results suggest that the iguanas were infected in Central America and experienced prolonged incubation periods (>1.5 years).\n\n【12】Positive PCR results from the water bowl suggest environmental contamination. The open abscess on the second iguana tested positive by both PCR and culture and was the likely source. However, negative PCR and culture results from the iguana’s vent, feet, and feces in the water bowl suggest contamination may not have been widespread.\n\n【13】Reptile owners, sellers, veterinarians, and staff in veterinary laboratories should be aware that green iguanas with abscesses may be infected with _B. pseudomallei_ and that they should use appropriate personal protective equipment when handling them. Laboratories accepting samples from animals possibly infected with _B. pseudomallei_ should anticipate risks associated with culturing this bacteria and take steps to protect workers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "22ff5280-7f4d-468b-b844-7163c09a69f0", "title": "Risk Factors for Middle East Respiratory Syndrome Coronavirus Infection among Healthcare Personnel", "text": "【0】Risk Factors for Middle East Respiratory Syndrome Coronavirus Infection among Healthcare Personnel\nMiddle East respiratory syndrome coronavirus (MERS-CoV), first identified in 2012, has emerged as a cause of severe acute respiratory illness in humans. As of May 1, 2016, a total of 1,728 laboratory-confirmed cases, including 624 deaths, have been reported globally . All reported cases have been directly or indirectly linked to countries in or near the Arabian Peninsula, including a recent outbreak in South Korea resulting from a single imported case in a person with history of travel to the Middle East . Increasing evidence suggests that dromedary camels are a natural host for MERS-CoV and that camel-to-human transmission can occur, initiating short chains of human-to-human transmission . Numerous questions about the epidemiology of MERS-CoV remain unanswered.\n\n【1】Healthcare settings are important amplifiers of transmission . A 2014 case series of 255 MERS-CoV infections in Saudi Arabia found that 31% of cases occurred among healthcare personnel (HCP), and among case-patients who were not HCP, 87.5% had recent healthcare exposure . Current MERS-CoV infection control recommendations are based on experience with other viruses rather than on a complete understanding of the epidemiology of MERS-CoV transmission .\n\n【2】The World Health Organization recently issued an urgent call for studies to better understand risk factors for infection and transmission . Published case series of healthcare-associated MERS-CoV infections have major limitations, including lack of control groups and lack of serologic confirmation of infection status, leaving wide knowledge gaps, such as mode of and risk factors for transmission in healthcare settings, attack rate among HCP, and spectrum of illness for MERS-CoV infection . To address these gaps, we retrospectively studied MERS-CoV infection among a cohort of HCP in a hospital in Saudi Arabia.\n\n【3】### Methods\n\n【4】The study was conducted at King Faisal Specialist Hospital and Research Center (Jeddah, Saudi Arabia) during May–June 2014. This multispecialty hospital has 360 beds, including an 18-bed medical intensive care unit (MICU) and a 38-bed emergency department (ED). Seventeen patients with confirmed MERS-CoV infection were in the hospital during March 24–May 3, 2014. The hospital had no cases of MERS-CoV before March 24, 2014. All patients with suspected or confirmed MERS-CoV infection were placed in private rooms equipped with negative pressure ventilation. Patients in whom MERS-CoV infection was not suspected initially were transferred to negative-pressure rooms as soon as the diagnosis was suspected or confirmed. During the outbreak, all HCP who had contact with MERS-CoV cases were screened for symptoms and underwent testing for MERS-CoV RNA by real-time reverse transcription PCR (rRT-PCR) of nasopharyngeal swab specimens.\n\n【5】We assessed risk factors for a case, defined as a MERS-CoV antibody–positive serum sample from an HCP, among 3 cohorts of HCP. Two cohorts, 1 each from the ED and MICU, comprised all HCP who worked in those hospital units during March 24–May 14, 2014, the period during which those units treated patients known to have MERS-CoV infection. In addition, we included a cohort of all HCP who worked in a unit (neurology) that was not known to house any MERS-CoV patients during the study period.\n\n【6】Every healthcare worker in each cohort was recruited to enroll. Participants provided a serum sample and were interviewed by trained study personnel using a standardized questionnaire. Although HCP were from different cultural, language, and educational backgrounds, all spoke English fluently. All questionnaires were conducted in English. Interviews were conducted during May 28–July 10, 2014. In addition to age, sex, occupation, and co-morbidities, we collected information on signs, symptoms, and treatment from March 31, 2014, through the day of interview. Contacts with MERS-CoV patients were described, including patient care activities, duration of contact, and exposure to body fluids. Information about infection control training and use of personal protective equipment (PPE) during encounters with MERS-CoV patients was collected. We assessed exposures outside the hospital, including household exposures to persons with MERS-CoV, contact with animals, and travel.\n\n【7】Serum samples were screened for antibodies against MERS-CoV (Hu/Jordan-N3/2012) nucleocapsid (N) protein by ELISA. The recombinant MERS-CoV N indirect ELISA was developed by using a modified version of the HKU5.2 N ELISA previously described  Serum was considered positive when the optical density values were \\> 0.36 (mean absorbance 405 nm of serum from US blood donors + 3 SD) with an assay specificity of 98.1% (/555). Samples that were positive by ELISA were confirmed by immunofluorescence assay, microneutralization assay, or both . A positive serologic test result required confirmation by immunofluorescence assay or microneutralization assay. HCP whose serum sample tested positive for MERS-CoV antibodies were considered to have evidence of MERS-CoV infection (case-HCP); seronegative persons were considered uninfected.\n\n【8】We analyzed data using SAS version 9.3 (SAS Institute, Cary, NC, USA). As appropriate, we compared dichotomous variables using χ 2  and Fisher exact tests. Cochran-Armitage tests for trend were used for ordinal variables. We performed multivariate logistic regression with backward stepwise elimination for exposures with univariate p < 0.2. Variables with p < 0.1 were retained in the final generalized linear model using a logit link to estimate risk.\n\n【9】We obtained written informed consent from all participants. The Institutional Review Board of the King Faisal Specialist Hospital and Research Centre approved the study.\n\n【10】### Results\n\n【11】Of 363 HCP eligible for the MICU (HCP), ED (HCP), and neurology unit (HCP) cohorts, 292 (.4%) HCP were enrolled: 131 (.5%) from the MICU, 127 (.7%) from the ED, and 34 (.8%) from the neurology unit. Of the 292 enrolled persons, 9 were excluded because serum specimens were unavailable.\n\n【12】For study participants who worked in units that treated MERS-CoV patients, the attack rate was 8.0% (/250) and varied by hospital unit: MICU, 11.7% (/128); ED, 4.1% (/122). (The attack rate in the neurology unit, where no known MERS-CoV patients were treated, was 0% \\[0/33\\].) Attack rates in the MICU and ED also varied by occupation; radiology technicians had the highest attack rate (.4% \\[5/17\\]), followed by nurses (.4% \\[13/138\\]), respiratory therapists (.2% \\[1/31\\]), and physicians (.4% \\[1/41\\]). No clerical staff (participants) or patient transporters (participants) were seropositive. Most participants (.4% \\[161/250\\]) were female; attack rate did not differ by sex (male 7.9%, female 8.1%; p = 0.95). The mean age of seropositive HCP was 40 years (range 29–59 years) and of seronegative HCP 37 years (range 18–66 years).\n\n【13】The most common manifestations of illness among case-HCP were muscle pain, fever, headache, and dry cough . These signs and symptoms, along with shortness of breath, occurred significantly more often among seropositive than among seronegative HCP. Seropositive HCP were also more likely to report gastrointestinal symptoms (p<0.001). Of the 20 case-HCP, 3 (%) were asymptomatic, 12 (%) had mild illness (symptomatic illness not requiring hospital admission), 2 (%) had moderate illness (required hospital admission but not mechanical ventilation), and 3 (%) had severe illness (required mechanical ventilation). All case-HCP survived, and all had been previously tested for MERS-CoV by rRT-PCR of nasopharyngeal swab specimens, but only 5 (%) rRT-PCRs were positive.\n\n【14】Nineteen (%) of 20 case-HCP reported having been in the same room as or within 2 meters of a patient known to be infected with MERS-CoV. The 1 seropositive HCP who had no MERS-CoV patient contact reported being in an automobile with a symptomatic person subsequently confirmed to have MERS-CoV infection. We therefore limited our analysis of risk factors, including PPE use, to any study participant who reported direct contact (i.e. within 2 meters) with MERS-CoV patients in the hospital . Total time spent in a MERS-CoV patient’s room or handling the patient’s bedding, equipment, or fluids did not significantly differ between seropositive and seronegative HCP (p = 0.93), nor did the number of MERS-CoV patients cared for during the study period (median 3.0 and 5.0 patients for seropositive and seronegative HCP, respectively; p = 0.75). We found no association between animal contact and infection.\n\n【15】We assessed HCP’s self-reported use of PPE during care of MERS-CoV patients, stratified by type of equipment and type of patient interaction . HCP who reported always covering their nose and mouth with either a medical mask or N95 respirator had lower risk for infection than did HCP reporting not always or never doing so, although this association was statistically significant only among HCP present in the room where aerosol-generating procedures were conducted. HCP who reported always using a medical mask for direct patient contact were ≈3 times more likely to have MERS-CoV infection than were HCP who reported not always or never using a medical mask (% of whom reported always or sometimes using an N95 respirator), a trend that was not statistically significant (p = 0.10). Conversely, those who reported always using N95 respirators for direct patient contact were less likely to be seropositive, a trend that approached statistical significance (p = 0.07).\n\n【16】Because medical mask and N95 respirator use were strongly and inversely correlated, we built separate multivariate models, one that assessed risk for medical mask use (model 1) and another that assessed risk for N95 respirator use (model 2). In both models, having participated in infection control training that included information about MERS-CoV prevention was associated with a significant and strong protective effect, and there was a strong but statistically insignificant trend toward increased risk among smokers. In model 1, HCP who reported always using a medical mask for direct MERS-CoV patient care were significantly more likely to be seropositive than those who reported not always or never wearing a medical mask (almost all of whom sometimes or always wore an N95 respirator) (relative risk \\[RR\\] 2.73, 95% CI 0.99–7.54). This model also included past or current smoking (RR 2.54, 95% CI 0.93–6.96) and participation in MERS-CoV infection control training (RR 0.28, 95% CI 0.10–0.80). In model 2, N95 respirator use was associated with a strong protective trend; HCP who always used an N95 respirator for direct MERS-CoV patient care were 56% less likely to be seropositive than were those who reported not always or never using an N95 respirator (almost all of whom sometimes or always wore a medical mask) (RR 0.44, 95% CI 0.15–1.24). This model also included past or current smoking (RR 2.51, 95% CI 0.92–6.87) and participation in MERS-CoV infection control training (RR 0.33, 95% CI 0.12–0.90).\n\n【17】### Discussion\n\n【18】We report this seroepidemiologic study to quantify the risk for MERS-CoV infection among HCP. The findings have important implications for infection control practice. Our results suggest that the attack rate of MERS-CoV infection among healthcare workers is substantially higher than that in previous reports that used nonserologic methods of detection . The spectrum of illness appears to be broader than previously described; infection caused a relatively mild illness in most cases. Infections occurred almost exclusively among HCP having close contact with a MERS-CoV patient.\n\n【19】Most HCP in this cohort reported always covering their nose and mouth with a medical mask or N95 respirator when caring for a MERS-CoV patient, which appeared to protect against infection among HCP participating in aerosol-generating procedures. When we stratified by type of mask, we observed an increased risk for MERS-CoV infection among HCP who reported always using medical masks and, conversely, a lower risk among those who reported always using N95 respirators. Taken together, these results raise the hypothesis that short-range aerosol transmission might have factored in transmission. Previous studies suggest that some respiratory viruses (e.g. influenza, severe acute respiratory syndrome coronavirus, rhinovirus) that are transmitted primarily by droplets and/or contact might simultaneously be spread through aerosol under certain conditions and perhaps by certain patients . Aerosol transmission in close proximity to the patient might not necessarily be accompanied by long-range transmission because the risk for such transmission might be affected by the infectious dose, the amount of aerosolized particles generated at the source, and the rate of biologic decay of the agent . We found no evidence of long-range aerosol transmission. Until additional information about the mode of MERS-CoV transmission is available, it seems prudent to take precautions against aerosol spread in healthcare settings when feasible to do so.\n\n【20】The combined attack rate for HCP who worked in units known to house patients with MERS-CoV infection (%) was substantially higher than that in previous studies, which described attack rates for HCP of < 1% . These prior studies did not use serologic methods to detect infection but rather relied on rRT-PCR of nasopharyngeal swabs. All 20 seropositive HCP in our study were screened with nasopharyngeal swabs, and only 5 (%) of these tests showed evidence of MERS-CoV by rRT-PCR. Therefore, screening for viral shedding using nasopharyngeal swabs might be an insensitive method for detecting infection, perhaps because of variability in timing of samples in relationship to exposure, and studies relying solely on this method of case detection might underestimate attack rates.\n\n【21】Our study suggests that almost all MERS-CoV infection among HCP occurs among those having close contact with patients known to be infected with MERS-CoV. We observed the highest attack rates among radiology technicians, followed by nurses. We hypothesize that radiology technicians most likely were exposed while obtaining portable chest radiographs, a procedure that requires close contact (e.g. positioning the patient for cassette placement) with patients who might be likely to have worsening respiratory status and be highly contagious. We identified no seropositive HCP who worked in the unit not known to house any MERS-CoV patients, suggesting that the background rate of MERS-CoV infection among HCP was low in the absence of known exposure to infected patients and that the virus was not circulating widely among staff.\n\n【22】HCP who had undergone infection control training specific to MERS-CoV had a lower risk for infection. This finding underscores the critical need for adequate infection control training, especially in settings with ongoing transmission of epidemiologically important pathogens.\n\n【23】We observed a broad spectrum of illness among HCP, and in most cases illness was relatively mild. Most illnesses were characterized by myalgia, fever, headache, and dry cough. Gastrointestinal symptoms were present in 50% of infected HCP; and 3 (%) reported no symptoms. Most seropositive HCP with symptoms sought care, but only a small minority were recognized as having MERS-CoV infection. All 20 infected HCP survived, and only 5 required hospitalization. The spectrum of illness we observed was broader than that described in previous case series of MERS-CoV infection , which probably were biased toward identifying patients with more severe illness because testing for MERS-CoV infection has largely been triggered by case definitions requiring evidence of pneumonia . The observation that most MERS-CoV infections among HCP are likely to be relatively mild and unrecognized has potentially important implications for infection control practice. Although little is known about risk for transmission from persons with mild MERS-CoV-infection, HCP with unrecognized MERS-CoV infection might be a reservoir for transmission to hospitalized patients who are more susceptible to severe illness because of underlying illnesses. Transmission from persons with unrecognized MERS-CoV infections might have contributed to the major role healthcare-associated transmission has played in the epidemiology of MERS-CoV . Thus, control of transmission in healthcare settings might depend on maintaining a low threshold for suspicion of MERS-CoV infection among exposed HCP and other persons with a relatively mild viral syndrome.\n\n【24】Our study did not identify strong associations with underlying chronic illnesses, most likely because the prevalence of such conditions was low (<10%) in this population. HCPs with a history of smoking had a risk for infection almost 3 times that of nonsmokers. We found no association between MERS-CoV infection and sex. Most case series to date have demonstrated a male predominance among case-patients , but our study suggests this association might be explained by social and behavioral factors that increase exposure to MERS-CoV, rather than a sex-specific difference in biological susceptibility.\n\n【25】Our study has several strengths. We compared MERS-CoV infected and uninfected HCP to determine risk factors for acquiring infection during patient care. The use of serologic testing to determine infection status enabled unbiased case ascertainment, an examination of the full spectrum of disease, and a comparison of the risks associated with a wide range of specific patient care activities.\n\n【26】Our study also has limitations. First, questionnaires were administered several weeks after possible exposures, and therefore the potential exists for recall bias. Recall bias can limit assessment of important variables, such as frequency of exposure and duration of contact during specific procedure. However, HCP and interviewers were unaware of their serologic status at the time of interview; their answers would not have been influenced by knowledge of these results. Moreover, symptoms of illness were unlikely to have introduced systematic bias to responses because most uninfected and infected groups reported illness. Second, we used only 1 serum sample for serologic testing. Because of the retrospective nature of our study, baseline serologic tests were not conducted, and therefore the potential exists for false-positive results. However, seroprevalence of MERS-CoV antibodies in Saudi Arabia is low (.15%), making misclassification bias unlikely . Third, infected asymptomatic HCP could serve as a potential source of infection to other HCP. Given the retrospective nature of our study, we were not able to characterize these potential exposures. Fourth, as is common with early studies of emerging infectious diseases, sufficiently powering studies can be difficult. Whether negative findings were true null findings or due to small sample sizes is unclear.\n\n【27】In conclusion, we report results of a seroepidemiologic study to quantify risk for MERS-CoV infection among HCP. The attack rate appears to be substantially higher than that in prior reports that used nonserologic methods of detection. Infection in this population most often results in mild illness that might be overlooked; programs to identify and exclude ill HCP who have been exposed to patients with MERS-CoV might help eliminate this reservoir for transmission. Our findings also suggest N95 respirators might be more protective against MERS-CoV infection while in close contact with an infected patient and highlight the possible role of short-range aerosol transmission of MERS-CoV in healthcare settings. Education about standard and MERS-CoV infection control practices appears to be protective, suggesting that adherence to basic practices can effectively prevent MERS-CoV infection among HCP.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7d43bdcb-4ab4-4daf-9f6d-b699c1f01ae1", "title": "Multiple Introductions of Avian Influenza Viruses (H5N1), Laos, 2009–2010", "text": "【0】Multiple Introductions of Avian Influenza Viruses (H5N1), Laos, 2009–2010\nSince 2003, highly pathogenic avian influenza virus (H5N1) has spread from southern China throughout Southeast Asia and to Europe and Africa . Since 2003, Laos has experienced outbreaks of clade 1 , clade 2.3.4 , and clade 2.3.2 viruses (twice in 2008) . Active surveillance of domestic ducks and chickens in Laos has been limited, but serum antibodies against subtypes H5 and H9 have been detected in ducks. In addition, subtype H5N1 virus was isolated from healthy ducks in 2006, and subtype H3N8 virus was detected in 2007 . To explore the diversity, extent, and endemicity of avian influenza viruses in Laos, we conducted a survey of healthy domestic poultry throughout the country in March 2010.\n\n【1】### The Study\n\n【2】Serum samples were collected in 9 of 17 provinces in Laos from healthy ducks and chickens in live-bird markets, village backyard flocks, and layer duck farms. Cloacal, tracheal, and environmental (fecal and water) swab specimens were also collected and placed immediately in transport medium . Swab specimens were screened in pools of 4 by using a real-time reverse transcription PCR for the matrix (M) gene segment . Positive pools were reextracted individually, retested, tested for hemagglutinin 5 (H5) by real-time reverse transcription PCR , and injected into 10–11-day-old embryonated chicken eggs.\n\n【3】Sequencing was conducted by using an Illumina (San Diego, CA, USA) platform (swabs and isolates)  and conventional Sanger sequencing (isolates). Illumina reads were first mapped to a database of publicly available human, avian, and swine influenza virus reference sequences from the Western Hemisphere Americas and Eurasia, and then remapped against references with the highest number of reads and best average coverage. Final average coverage varied between samples and segments with ranges of 123–24,163 (samples), 21–18,671 (samples) and 9–436 for sample A/duck/Lao/670/10. Isolate genotypes were verified by using Sanger sequencing. Mixed infections could not be excluded for direct sequencing in the absence of an isolate.\n\n【4】Phylogenetic analysis  was conducted by using MEGA version 5.02 . Sequences are available in GenBank (CY098294–CY098334, CY098336–CY098340, CY098342–CY098368, and CY098370–CY098464). Serum samples were screened by using an ELISA (FlockChek MultiS Screen; IDEXX Laboratories, Westbrook, ME, USA), and antibody-positive serum samples were tested by using a hemagglutinin inhibition assay for subtypes H3, H4, H5 (clades 2.3.2 and 2.3.4), H6, and H9 (lineages G1 and Y280) as described .\n\n【5】During March 2010, a total of 3,695 swab specimens were collected (duck and 279 chicken cloacal samples, 446 duck tracheal samples, 675 fecal samples, and 367 water samples). M gene prevalence was 4.0% (ducks), 1.8% (chickens), and 0.3% (environment samples). Five isolates were obtained . All M gene–positive swab specimens were collected in 13 locations (backyards, 2 markets, and 3 farms)  Sample protection was suboptimal, and only 21 samples could be subtyped by real-time RT-PCR and sequencing .\n\n【6】Phylogenetic analysis identified 3 groups of viruses: clades 2.3.2.1, 2.3.4.1, and 2.3.4.2 . Two samples were closely related to A/chicken/Lao/LH1/2010–like virus (outbreak in Vientiane in April–May 2010) and to A/chicken/Laos/C100209–194-PTK/2009–like virus (outbreak in Phongsaly in February 2009) (clade 2.3.4.1) . These viruses were closely related to A/Guizhou/1/2009 and A/chicken/Vietnam/NCVD-404/2010 .\n\n【7】Sixteen surveillance viruses of clade 2.3.4.2 were highly homogeneous  and closely related to A/environment/Guizhou/4/2009 and A/chicken/Vietnam/NCVD-394/2010–like viruses, although they contained the polymerase basic 2 (PB2) gene of a clade 2.3.2.1 donor virus and were therefore interclade reassortants . The genotype of the 4 isolates was verified by using Sanger sequencing. The PB2 gene was most closely related to A/grey\\_heron/Hong\\_Kong/1046/2008–like viruses. These reassortants were detected in 3 locations in northern (backyard), central (backyard), and southern (farm) Laos, suggesting multiple introductions of reassortants into Laos .\n\n【8】Three clade 2.3.2.1 viruses were detected in 2 ducks and 1 environmental sample (same trader). These viruses were interclade reassortants. Two (isolate and 1 direct sequence) contained 6 or 7 segments that were A/whooper swan/Mongolia/6/2009 like (clade 2.3.2.1), and the nucleoprotein gene was A/tree\\_sparrow/Jiangsu/1/08 like (clade 2.3.4). The environmental sample (direct sequence) contained A/whooper swan/Mongolia/6/2009–like hemagglutinin and M genes and A/Guizhou/1/2009–like PB2, nucleoprotein, and neuraminidase genes (clade 2.3.4.1) . The genotype of the isolate was verified by using Sanger sequencing. Hemagglutinin segments of the reassortants were identical (% nt identity). This identity and the source of the 3 samples (trader) suggest that reassortment occurred recently, likely in Laos.\n\n【9】Antibody titers to H5 and H9 and a subtype H3N8 virus isolate have been reported in Laos . Subtypes H4 and H6 also circulate in this region . For this study, 2,148 serum samples (from ducks, 200 from chickens, and 49 from unspecified species) were collected and 267 antibody-positive (seroprevalence 14%) duck and 15 (.5%) chicken serum samples were detected. Hemagglutination inhibition testing for specific antibodies against H3, H4, H5, H6, and H9 detected all antibodies but to H3 . Antibodies against H9 were detected at highest titers and most frequently (.1% of ducks for each H9 lineage), although some cross-reactivity between G1 and Y280 likely occurred . These ducks were most widely distributed ([19%\\] of 97 locations in all 9 provinces). Antibodies against H5 clade 2.3.4 were found at the detection limit in few serum samples (.4% of ducks in 3 northern and 2 southern provinces). Antibodies against H5 clade 2.3.2 were detected in 0.4% of ducks in 4 northern provinces and the capital of Vientiane.\n\n【10】Among ducks, 34 (.7%) were either shedding or had antibodies against avian influenza virus (H5N1), and there was \\> 1 virus-positive or antibody-positive duck in each of the 9 provinces sampled. One village had 36% of sampled ducks exposed to this virus; 18% shed 2.3.4 virus and 18% had antibody to 2.3.2 virus . One duck in this village had antibodies against 2.3.2 and 2.3.4 clade viruses. Exposure to 2.3.2 and 2.3.4 viruses was evident in ducks from locations in 3 other provinces (district each in Champasak and Xiengkhoung, and several districts in Luang Prabang).\n\n【11】### Conclusions\n\n【12】This study showed that 3 groups of avian influenza viruses (H5N1) were likely introduced into Laos in 2009–2010, one of which resulted in 2 outbreaks . In all 9 provinces where surveillance was conducted, ducks had been exposed to this virus. Evidence of clades 2.3.2 and 2.3.4 virus activity was detected in 4 provinces. Several interclade reassortants were identified, demonstrating the high genetic mobility of these viruses in the region. Since 2004, Laos has had repeated outbreaks of highly pathogenic avian influenza viruses, which have also been detected in China and Vietnam. There is no evidence that a particular virus lineage has established itself in Laos. The frequency of introduction, diversity, and extent of these viruses in Laos suggests considerable movement of viruses into the country from surrounding territories (China and Vietnam, but not Cambodia) and within the country.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2b0a2648-18f3-4952-adbd-4f84bba6bb45", "title": "Territorywide Study of Early Coronavirus Disease Outbreak, Hong Kong, China", "text": "【0】Territorywide Study of Early Coronavirus Disease Outbreak, Hong Kong, China\nCoronavirus disease (COVID-19) refers to a cluster of viral pneumonia cases that first occurred in Wuhan, a city in Hubei Province, China, beginning in December 2019. Etiology was unknown during the early stage of the outbreak until a novel coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), was isolated on January 7, 2020, and the genome was sequenced .\n\n【1】During the initial outbreak, fever was the main symptom of COVID-19, and about one third of patients experienced acute respiratory distress syndrome. Approximately 16% of patients were in severe condition at admission, and the estimated mortality rate was 1.4% . Sustained human-to-human transmission was confirmed upon identification of cases clustering among families and transmission from patients to healthcare workers , which triggered China’s urgent public health actions and international concern.\n\n【2】As of February 28, 2020, a total of 78,824 COVID-19 cases had been diagnosed in mainland China and 2,788 persons had died. The disease had also spread to 50 other countries . The World Health Organization declared COVID-19 a pandemic in March 2020.\n\n【3】In Hong Kong, the first imported case was identified on January 23, 2020; the case-patient was a resident of mainland China who traveled to Hong Kong from Wuhan through Shenzhen by high-speed rail. The first local case with unknown source (i.e. patient who had no travel record during the 14-day incubation period) was reported on February 4, 2020 .\n\n【4】By February 28, 2020, a total of 93 COVID-19 cases had been recorded in Hong Kong; \\> 70 (.3%) of those were local cases and those case-patients’ close contacts . Secondary and tertiary transmissions were observed in some case clusters. Because source of infection is unknown in most index cases in these clusters, hidden transmission chains were believed to be present in the community.\n\n【5】We report the demographic, clinical, and epidemiologic data of 50 hospitalized patients who accounted for 53.8% of COVID-19 cases in Hong Kong at the data cutoff point (February 28, 2020), including 3 imported cases and 6 transmission clusters of local infections. We characterized viral genomes in all these cases by using nanopore and Illumina sequencing. Phylogenetic and molecular evolutionary analyses were performed to determine the transmission link and the evolutionary rate of COVID-19 cases in the community.\n\n【6】### Methods\n\n【7】##### Cases\n\n【8】For this retrospective, multicenter study, we enrolled case-patients with laboratory-confirmed COVID-19 from 4 public hospital clusters managed under the Hospital Authority of Hong Kong, namely Hong Kong East Cluster, Kowloon East Cluster, Kowloon West Cluster, and New Territories West Cluster, during January 26–February 28, 2020. Sputum specimens and throat swab specimens pooled with nasopharyngeal aspirates were collected from patients who fulfilled the reporting or enhanced surveillance criteria at hospital admission . Laboratory-confirmed infection was defined as the detection of SARS-CoV-2 by real-time reverse transcription PCR, which amplified the envelope (E_ ) gene and RNA-dependent RNA polymerase (RdRp_ ) gene .\n\n【9】We obtained demographic, clinical, and microbiologic data from patients’ medical records. The definitions of clinical symptoms and complications are based on World Health Organization guidance . We adopted the Centre for Health Protection case numbering system, which is based on the date of case confirmation. This study was approved by the Institutional Review Boards of The Hong Kong Polytechnic University  and the public hospitals involved (HKECREC-20200014; KCC/KEC-20200070; KWC-20200040; NTWC-20200038).\n\n【10】##### Specimen Preparation\n\n【11】The respiratory specimens were centrifuged at 16,000 × _g_ for 2 minutes. Total nucleic acid was extracted from supernatant using MagNA Pure 96 System  or NucliSENS easyMAG  according to the manufacturers’ instructions. DNase treatment was done by using TURBO DNA-free Kit  to remove residual host DNA.\n\n【12】##### Reverse Transcription and Viral Genome Amplification Using Multiplex PCR\n\n【13】DNase-treated RNA was reverse-transcribed using random hexamers and SuperScript IV Reverse Transcriptase (ThermoFisher Scientific) as previously described . Viral cDNA was then amplified by using 2 PCRs containing tiled, multiplexed primers  described in the ARTIC protocol  . Details of the multiplex PCR are provided in Appendix 2 .\n\n【14】##### Nanopore MinION Sequencing\n\n【15】Ligation-based 1D sequencing was carried out by using Litigation Sequencing Kit SQK-LSK109  according to manufacturer’s instructions. Multiplex PCR amplicons of each sample were normalized to 1 ng/µL before end-repair and native barcode ligation by using EXP-NBD104/114 (Oxford Nanopore Technologies). Barcoded samples were pooled and ligated to AMII sequencing adaptor. Sequencing was performed with Nanopore MinION device (Oxford Nanopore Technologies) by using R9.4.1 flow cell for 48 hours.\n\n【16】##### Illumina MiSeq Sequencing\n\n【17】Multiplex PCR amplicons were subjected to library preparation and dual-indexing by using KAPA HyperPrep Kit and Unique Dual-Indexed Adaptor Kit (Roche) according to manufacturer’s instructions. Ligated libraries were enriched by 6-cycle PCR amplification and purification and size selection by using AMPure XP beads . The pooled library was sequenced with the MiSeq Reagent Kit V2 Nano on an Illumina MiSeq System .\n\n【18】##### Bioinformatic Analysis\n\n【19】We analyzed nanopore sequencing data using modified Artic Network nCoV-2019 novel coronavirus bioinformatics protocol  . Illumina sequencing reads were mapped with reference to respective consensus genome of each sample constructed from nanopore data. Variants were called by using freebayes version 1.0.0  with haploid decoding and minimum base quality set at Q30. Consensus genomes were constructed by GATK 4.1.4.1 based on the VCF file . SPAdes genome assembler 3.14.0  and minimap2 version 2.17  were used to combine nanopore and Illumina sequencing results for de novo assembly and to identify the sequence of the unmapped gap regions. The sequences have been submitted to GenBank (accession nos. MT232662711).\n\n【20】##### Genomic and Phylogenetic Analysis\n\n【21】To identify the amino acid change caused by each single-nucleotide polymorphism, we BLAST-searched the consensus genome of each specimen against the reference NC\\_045512.2 using BLASTX . Nonsynonymous mutations were identified using custom Python script .\n\n【22】Consensus genomes were aligned by Clustal Omega 1.2.4 . Phylogenetic tree was constructed with PhyML 3.0  using the maximum-likelihood algorithm. Best-fitting substitution model was selected by Akaike information criteria, in which we selected the general time-reversible model with fixed proportion of invariable sites . Bootstrap replicates were set at 1,000×, and maximum-likelihood phylogenetic tree was rooted on the earliest published genome . Transmission clusters were defined by clear epidemiologic and onset-time relationship. Meanwhile, we downloaded an additional 478 SARS-CoV-2 genomes from the GISAID  SARS-CoV-2 data hub  and analyzed the phylogenetic relationships by using maximum-likelihood with bootstrap value set at 500× and rooted on SARS-CoV-2 genome NC\\_045512.2.\n\n【23】##### Estimation of Evolutionary Rate and Divergence Time of Transmission\n\n【24】To reconstruct the evolutionary model of COVID-19 cases using the viral genomes obtained in Hong Kong, we implemented Bayesian inference through Markov Chain Monte Carlo (MCMC) framework in BEAST version 2.6.2 . Death rate δ (which refers to the time needed for a case-patient to become noncontagious) was determined as the lag time between date of symptom onset and date of hospital admission, because the transmission link in Hong Kong was practically stopped once the patient was hospitalized. Bayesian phylodynamic analysis was performed using strict clock and relaxed clock models with coalescent exponential growth tree priors. We ran MCMC chains for 10 9  generations and sampled every 500 steps. Bayesian output was analyzed after the results were visualized by Tracer version 1.7.1 . All parameters had an effective sample size of >200, indicating sufficient sampling.\n\n【25】### Results\n\n【26】Our investigation included 50 COVID-19 patients; 54.0% were women, and the mean age was 55.2 (range 22–96) years . Of the case-patients, we categorized 3 cases as imported because the patients stayed in Wuhan before traveling to Hong Kong in mid-January. Four patients traveled to Japan and other provinces of China in mid-January and were hospitalized after they returned to Hong Kong, but active community transmission of COVID-19 was not officially reported in these areas during the study period, so these cases were considered possible local infections. The other 43 patients’ cases were categorized as local infection because of no recent travel history.\n\n【27】Eighteen (.0%) patients had chronic illnesses, of which cardiovascular and cerebrovascular diseases were the most common . In total, 74.0% of the patients were experiencing cough at admission. Fever occurred in 58.0% of patients at time of admission, but that rate gradually increased to 64.0% during the course of hospitalization. Other less common symptoms were muscle aches (.0%), sore throat (.0%), shortness of breath (.0%), and diarrhea (.3%) . Two persons (.0%) were asymptomatic throughout the study period. On radiologic examination, 27 (.0%) had bilateral pneumonia, 11 (.0%) had unilateral pneumonia, and 17 (.7%) showed multiple areas of mottling and ground-glass opacity. None of the patients were co-infected with other respiratory viruses or fungi.\n\n【28】Intensive care unit admission was relatively uncommon (/50, 6.0%) in our cohort compared with admission rates in previous studies . This difference could be attributed to underdiagnosis of milder cases during the initial COVID-19 outbreak in China. One patient’s sputum specimen was culture-positive for _Klebsiella aerogenes_ bacteria; a second patient’s specimen was culture-positive for _Ralstonia pickettii_ bacteria. Both had acute respiratory distress syndrome and acute respiratory injury accompanied by septic shock or acute renal injury and required admission to the intensive care unit.\n\n【29】Of the 50 case-patients, 42 (.0%) could be clustered based on their epidemiologic links . We identified 6 transmission clusters (clusters 1–6). Cluster 1 involved a 4-member family. The father, who traveled to Guangdong, China, in late January 2020, was believed to have infected his wife and subsequently their daughter and son-in-law at a family gathering. Clusters 2 and 3 were family clusters of local infection and unknown source. Both clusters involved 3 household members with no recent travel history. Cluster 4 was attributed to a superspreading event (SSE): a barbecue and hotpot party involving 19 family members in late January. Symptom onset in these patients occurred during days 2–13 after the party. A colleague of 1 infected person, who did not attend the party, also tested positive for SARS-CoV-2. Cluster 5 initiated from a resident of a public housing estate, in whom COVID-19 was diagnosed on January 30. Eleven days later, diagnoses were made in 3 members of a household that resided in the same building (stories below) the index case-patient. Two household members subsequently attended a family gathering of 29 persons at a Chinese restaurant during the incubation period. COVID-19 was diagnosed consecutively in 3 persons ≈2 weeks after the gathering. In addition, a Filipino domestic aide of 1 infected family member, who did not attend the family gathering, also tested positive. For cluster 6, the first reported case was in a 70-year-old woman who visited a Buddhist worship hall during the Chinese New Year. A further 8 persons who visited the same Buddhist worship hall during this period later tested positive for SARS-CoV-2. At the data cutoff point, \\> 4 other household members who had never been to the worship hall also tested positive. Details of the demographic and epidemiologic information on the cases and clusters are provided in Appendix 1 Tables 2–8.\n\n【30】Consensus genomes of all 50 cases were constructed based on nanopore sequencing and refined by Illumina sequencing. On average, 62,387 reads/genome were obtained with 550× coverage for nanopore, and 18,747 reads/genome were obtained with 132× coverage for Illumina platform. The consensus genome size was ≈29.9 kbp with GC content ≈38%. The genomes were highly conserved with the first SARS-CoV-2 genome and had an average sequence identity of 99.98% (range 99.94%–100.0%). We identified 64 nonsynonymous substitutions from all 50 genomes . _Orf3a_ \\-G251V was the most frequent amino acid substitution; 44/50 (.0%) of the samples harbored this mutation, after which _Orf1ab_ \\-H3233Y (/50, 60.0%) and _S-_ L8V (/50, 54.0%) were most common.\n\n【31】Genomewide single-nucleotide polymorphisms were used to contextualize phylogenetic placement of Hong Kong strains in SARS-CoV-2 global phylogeny . However, because the samples were taken at the early stage of global outbreak, the genetic variability between strains was limited, resulting in several unresolved branches and marginal supporting bootstrap values. Nevertheless, when compared with SARS-CoV-2 strains isolated from other regions, Hong Kong strains tended to aggregate mainly in 2 lineages. Lineage 1 consisted of 4 Hong Kong strains that clustered with most isolates from China (n = 32). Lineage 2, which consisted of 44 Hong Kong strains, was more closely related to strains seen in South Korea (n = 7) and France (n = 5).\n\n【32】In examining the phylogeny of the COVID-19 outbreak in Hong Kong, we identified 2 distinctive groups . The first group consisted of 2 imported cases and the cases in cluster 1. The second group originated with a single robust node with bootstrap value of 94% and a common mutation _Orf3a-_ G251V. The second group could be further separated into 3 subgroups. The first subgroup mainly consisted of the cases in cluster 5, the public housing estate–related SSE. The second subgroup included cases in cluster 4 associated with the family hotpot party, cluster 3, and 2 isolated cases (case 23 and case 43). These samples shared the same missense mutations at _S_ \\-L8V and _Orf1ab-_ H3233Y. Finally, the third subgroup included the cases from cluster 6, an SSE originating from a Buddhist worship hall, in which _Orf1ab_ \\-G295V were identified.\n\n【33】According to Bayesian time-scaled phylodynamic analysis, strict clock and relaxed clock models estimated the time of most recent common ancestor of COVID-19 outbreak in Hong Kong as December 24, 2019 (% Bayesian credible interval \\[BCI\\] December 11, 2019–January 5, 2020). The evolutionary rate was 3.04 × 10 −3  substitutions/site/year (% BCI 2.04–4.09 × 10 −3  substitutions/site/year) . Based on demographic data, the average time from symptom onset to hospital admission was ≈8.5 days. The estimated reproduction number was calculated at 1.84 (% BCI 1.37–2.35).\n\n【34】### Discussion\n\n【35】This study provides a territorywide overview of early COVID-19 outbreak in Hong Kong, an international city with borders connecting to mainland China, by integrating demographic, clinical, epidemiologic, phylogenomic, and phylodynamic data. In Hong Kong, most cases recorded in January 2020 were imported cases. After February 1, most were local cases and close contacts of those case-patients, indicating local community transmissions. Transmission in closed settings, especially during family and religious gatherings, is a hallmark of recent cases recorded in Hong Kong. Among 6 clusters identified on the basis of epidemiologic links, 3 (clusters 4–6) were considered SSEs because of the larger number of persons involved (n = 8–13). We performed whole-genome sequencing on all 50 cases to investigate phylogenetic relationship and transmission link.\n\n【36】The SARS-CoV-2 samples in Hong Kong had 99.98% identity to the reference genome  and demonstrated no apparent major genome modification since the initial COVID-19 outbreak in Wuhan. As shown in global phylogeny, SARS-CoV-2 genomes isolated in Hong Kong could be segregated into 2 lineages. Lineage 1 was phylogenetically related to the strains isolated from China and was the cause of the cases in Cluster 1. Lineage 2 was more closely related to strains from France and South Korea. It also harbored a common mutation at _Orf3a-_ G251V, which accounted for 88.0% of cases in this study.\n\n【37】Regarding the local phylogenetic analysis, clustering of samples was highly concordant to the epidemiologic link, despite the marginally supportive bootstrap value of the nodes because of the limited genetic variability. Cluster 1 demonstrated the closest genetic distance to the reference genome among all cases reported in Hong Kong . The index case of cluster 1 (case 66) was initially defined as possible local infection because the patient traveled to Guangdong Province, which was not considered to have active community transmission at that time. However, our sequencing result demonstrated that the genome of case 66 was 100% identical to the first published SARS-CoV-2 genome, and all cases in cluster 1 did not harbor _Orf3a_ \\-G251V, which was recognized as a hallmark of local cases with unknown source in our community. Therefore, instead of possible local infections, cluster 1 was more likely imported from mainland china through index case-patient 66.\n\n【38】Cluster 5 originated within a public housing estate, in which a family of 3 members (cases 42, 48, and 49) were suspected to have been infected through a confirmed case-patient (case 12) who lived 10 stories above them in the same building, through a potentially faulty sewage pipe setup or other environmental exposure. Based on phylogenetic analysis, viral genomes in cluster 5 shared a similar genetic distance from the reference genome and were assigned to the same branch of the tree. This finding supports a potential transmission link among these cases.\n\n【39】Cluster 4 was a family gathering–associated SSE during Chinese New Year. In concert with epidemiologic information, all 11 cases from cluster 4 shared 3 common missense mutations, namely _S_ \\-L8V, _Orf1ab_ \\-H3233Y, and _Orf3a_ \\-G251V; 7 cases shared identical genomes. Considering the fast-evolving property of RNA viruses, the presence of identical genetic sequences among the strains implies that transmission occurred over a short period or even in a single event. Meanwhile, 2 isolated cases (case 23 and case 43) and 3 cases from another local cluster (case 38, case 39, and case 40) shared highly similar genomes to those of cluster 4 . Although no apparent epidemiologic links were observed, the high degree of genomic similarity suggests that these cases might have originated in a single source. That speculation was further supported by the geographic distribution of case-patients who lived near one another and whose social circles might have overlapped . Our results demonstrate that the integration of epidemiologic and phylogenetic data is critical for providing more accurate information about transmission patterns.\n\n【40】Cluster 6 was an SSE occurring in a Buddhist worship hall. Two missense mutations, _Orf1ab_ \\-G295V and _Orf1ab_ \\-L3606F, were unique to this cluster. Epidemiologic investigation identified a 43-year-old monk (case 102), who was the abbot of the worship hall and had traveled to mainland China in early January. He was sent to a quarantine center in late February after being linked to a series of confirmed cases connected to the worship hall. He was asymptomatic throughout the study period. Phylogenetic analysis showed that this case was closest to the root of the cluster , suggesting that case 102 could be the index patient of cluster 6. By the time of data cutoff, the cluster involved 13 patients and spread was ongoing. This pattern demonstrates the possibility of a hidden spreader as a source of COVID-19 community outbreak. That likelihood also highlights the importance of rapid quarantine of close contacts of confirmed case-patients, regardless of the presence of symptoms, to halt community spread.\n\n【41】In the evolutionary clock study, the reproduction number of COVID-19 within Hong Kong as of February 28, 2020, was estimated at 1.84 (% BCI 1.37–2.35). That value strongly indicated that the outbreak in Hong Kong was ongoing, but it was smaller than the estimated reproduction number of 2.6 in Wuhan . The smaller value is a combined outcome of reduced growth rate and increased δ. The reduced growth rate is attributed to strong public health awareness among the general public, which resulted in greatly reduced social activities and strong compliance with mask-wearing during this period . The increased δ can be attributed to robust laboratory surveillance and fast quarantine time. In addition, time of most recent common ancestor for the cases in Hong Kong was determined to be December 24, 2019, ≈25 days before the first patient in our cohort (Case 2) demonstrated symptoms on January 18, 2020 .\n\n【42】Our study has several limitations. Although we included 53.8% of the cases reported in Hong Kong as of February 28, another 43 cases, including 2 fatal cases, were not analyzed in this study. Moreover, incubation periods of cases in which the source of infection is unknown might vary widely. Studies have demonstrated that incubation periods can vary from 4.5 to 15.8 days  and can be longer for patients experiencing mild symptoms. However, because patients might already be infectious during the incubation period, the reproductive number in this study could be underestimated. Furthermore, our calculations were based solely on phylodynamic analysis, which could differ from calculations on the basis of epidemiologic models. Finally, ambiguous bases were observed in some of our consensus genomes. This ambiguity is mainly because whole-genome sequencing was performed on respiratory specimens instead of viral culture, in which viral load plays a critical role in the subsequent genome quality as reflected by the cycle threshold of each specimen. The paucity of viral load in specimens could affect the yield of sequencing libraries. In our study, specimens with cycle threshold <28 were usually free of ambiguous bases. Nevertheless, the uncovered area only accounted for ≈1–3% of the entire viral genome, although the remaining mapped regions had an average coverage of >100×, which should provide sufficient and accurate information for subsequent analyses .\n\n【43】In conclusion, phylogenomic data were consistent with epidemiologic findings that transmission in closed settings, especially during family and religious gatherings, is a hallmark of COVID-19 outbreak in Hong Kong. Social distancing and vigilant infection control measures, such as rapid isolation of suspected or confirmed case-patients and their close contacts, are crucial for containing COVID-19 in the community.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bca202f2-9356-4e4b-95e9-232a02859054", "title": "Arenaviruses ()", "text": "【0】Arenaviruses ()\nViruses in the family _Arenaviridae_ are generally spread by rodents, with each virus associated with one, or a few, closely related rodent species that serve as the virus’ natural reservoir. The types of rodents that spread arenaviruses are located across much of the world, including Europe, Asia, Africa, and the Americas. In some areas of the world, arenavirus infections in people are relatively common and can cause severe disease.\n\n【1】Discovery of Arenaviruses\n-------------------------\n\n【2】Lymphocytic choriomeningitis virus (LCMV) was the first arenavirus identified. It was isolated in 1933 during a study of a St. Louis encephalitis epidemic. Although not the cause of the outbreak, LCMV was found to be a cause of nonbacterial meningitis. Since then, new arenaviruses have been discovered regularly.\n\n【3】How viruses in the Arenavirus family spread\n-------------------------------------------\n\n【4】While rodent hosts are chronically infected with an arenavirus, they do not appear to become ill. Most infections spread among adult rodents through scratches and bites, although for certain arenaviruses, the virus passes from mother to offspring during pregnancy. Arenaviruses are shed into the environment in the urine, saliva, or droppings of infected rodent hosts. People become infected by breathing in the virus after rodent urine, droppings, or nesting materials are stirred up, such as during cleaning. People can also get infected by touching their face after touching the virus, through the bites or scratches of infected rodents, and by eating contaminated food. In some instances, arenaviruses can spread to people when consuming infected rodents as a food source.\n\n【5】Person-to-person transmission can occur with certain arenaviruses, such as Chapare, Lassa, Machupo, and Lujo viruses. This type of transmission usually occurs when there is direct contact with the blood or other body fluids of infected individuals. Contact with contaminated objects, such as medical equipment, is also associated with spreading these viruses. Use of protective clothing and disinfection procedures while caring for a sick person can help prevent further spread of disease.\n\n【6】Old World/New World Arenaviruses\n--------------------------------\n\n【7】Arenaviruses are divided into two groups – New World and Old World viruses – based on genetic differences as well as where the viruses are geographically distributed. New World viruses are found in the Western Hemisphere — North and South America. They include Chapare virus, a severe or fatal hemorrhagic fever, found in Bolivia, as well as Guanarito virus found in Venezuela.\n\n【8】Old World viruses occur in the Eastern Hemisphere — Africa, Europe, and Asia. Lassa fever, which can cause mild to severe disease in people, is found in West Africa. Lujo, a fatal hemorrhagic fever, is found in southern Africa. LCMV, classified as an Old World arenavirus, is the only arenavirus found in both the Western and Eastern Hemisphere.\n\n【9】There are some arenaviruses – both New and Old World — that have been identified in host animals, but no human infection has been reported yet.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a61ee4db-4af4-48c5-8b7d-4497613d521a", "title": "SARS-CoV-2–Specific Antibodies in Domestic Cats during First COVID-19 Wave, Europe", "text": "【0】SARS-CoV-2–Specific Antibodies in Domestic Cats during First COVID-19 Wave, Europe\nSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the cause of the ongoing coronavirus disease (COVID-19) pandemic, causes high rates of illness and death among humans. SARS-CoV-2 is a newly recognized member of the genus _Betacoronavirus_ , family _Coronaviridae_ , that infects humans. An early serosurvey among domestic cats in Wuhan, China, during January–March 2020 reported 14.7% seropositivity . Experimental infections demonstrated susceptibility to SARS-CoV-2 infection in cats and other carnivore species, such as ferrets (Mustela putorius furo_ ), minks (Neovison vison_ ), and to a lesser extent domestic dogs , and confirmed anecdotal observations of naturally occurring human-to-animal transmissions . Respiratory and gastrointestinal signs were observed in SARS-CoV-2–infected cats . We conducted a seroprevalence study for SARS-CoV-2–specific antibodies among domestic cats in Europe during and after the first COVID-19 pandemic wave, using a plaque-reduction virus neutralization test (VNT) and a SARS-CoV-2 receptor-binding domain–specific ELISA (RBD-ELISA).\n\n【1】### The Study\n\n【2】We analyzed serum samples collected from 2,160 domestic cats during April–June 2020. Samples had been sent to a veterinary diagnostic laboratory (LABOklin; Kissingen, Germany) for diagnostic purposes unrelated to suspicion of SARS-CoV-2 infection . Samples were from 1,136 cats in Germany, 331 in the United Kingdom, 333 in Italy, and 360 in Spain. Among 1,799 samples with demographic data, cats ranged from 0.1–23 years of age (median and mean age 11 years). We estimated a minimum of 300 total samples per location to enable a realistic estimation for each location. To confirm specificity of the assays to detect SARS-CoV-2–specific antibodies, we included 25 prepandemic cat serum samples and 25 serum samples from cats that tested positive for feline coronavirus/feline infectious peritonitis (FCoV/FIP) by NovaTec VetLine , a commercial antibody test, in the screening.\n\n【3】We tested all serum samples by VNT, as previously described . We considered serum samples positive when titers were >20, expressed as the reciprocal of the dilution that gave >80% reduction of stained cells in the plaque reduction neutralization test (PRNT 80  ) .\n\n【4】We also tested serum samples with an indirect ELISA we developed and validated inhouse. We used an ELISA previously used for detecting SARS-CoV-2 RBD antibodies in human serum  and replaced the anti-human IgG conjugate with an anti-cat IgG conjugate .\n\n【5】We evaluated performance characteristics of the cat ELISA-RBD by using Pearson correlation of the results obtained by ELISA-RBD and Gaussian distribution analyses for the VNT. We also calculated diagnostic sensitivity and specificity of the ELISA-RBD compared with VNT. We conducted data analyses using R  and Prism version 9 . We calculated SARS-CoV-2 seroprevalence in cats separately for each country.\n\n【6】We found overall SARS-CoV-2 seroprevalence among cats was 4.2% in Germany, 3.3% in the United Kingdom, 4.2% in Italy, and 6.4% in Spain . Among all 2,160 cat serum samples tested, 96 (.4%, 95% CI 3.6%–5.4%) were positive by VNT and 92 (.3%, 95% CI 3.4%–5.2%) by RBD-ELISA. The RBD-ELISA showed a diagnostic sensitivity of 90.6% (% CI 90.0%–91.2%) and specificity of 99.8% (% CI 99.8%–99.8%) compared with VNT . Furthermore, correlation (r_ \\= 0.9, 95% CI 0.9–0.9) and Gaussian distribution analyses (r 2 _ \\>0.7) revealed high agreement between VNT and RBD-ELISA sensitivities. All 25 prepandemic serum samples and 25 FCoV/FIP-positive samples tested SARS-CoV-2–negative in both the VNT and RBD-ELISA (data not shown), confirming the specificity of the assay for measuring SARS-CoV-2–specific antibodies.\n\n【7】Our study of domestic cat serum from 4 selected countries showed that during the first COVID-19 wave in Europe, >4% of domestic cats had been infected with SARS-CoV-2, probably through their contacts with infected humans. Because serum samples were sent to the veterinary diagnostic laboratory for conditions unrelated to a suspected SARS-CoV-2 infection, our data might not fully represent the overall seropositivity of the domestic cat population in Europe.\n\n【8】We used a VNT and an RBD-ELISA based on the original SARS-CoV-2 wild-type isolate . The RBD-ELISA proved to have a high sensitivity and specificity compared with the VNT , but 5 low-titer (titer = 20) VNT-positive samples remained undetected by the RBD-ELISA. These samples might have remained undetected because of the high specificity of RBD-ELISA, which detects antibodies toward the single spike protein ectodomain. Unlike RBD-ELISA, VNT might identify a broader range of virus neutralizing antibodies, including those directed against other domains of the spike protein. Of note, the only correlation of virus protection we have to date is virus neutralization, which apparently correlates well with RBD-ELISA positivity. For serologic screening and for individual diagnostic testing of domestic cats, the RBD-ELISA could replace the VNT, thus avoiding the use of live SARS-CoV-2 under Biosafety Level 3 laboratory conditions. We further confirmed specificities of the VNT and RBD-ELISA by showing that prepandemic and FCoV/FIP-positive cat serum samples were negative in both assays. This finding excluded the detection of cross-reactive antibodies against feline alphacoronaviruses  and alphacoronaviruses of other animal species that might infect cats . Our data contrast a heavily affected area in China at the onset of the pandemic from which seropositivity levels of domestic cats ranged < 15% , although those results were from relatively fewer tested cats and used a different assay.\n\n【9】### Conclusions\n\n【10】During the first COVID-19 pandemic wave, reported seroprevalence levels in domestic cats ranged from 0.4% in the Netherlands  to 23% among cats in COVID-19–positive households in France . Similar seroprevalence levels in cats and humans in the same areas found by us and others suggest that in the absence of another known source (C. Schulz, unpublished data) , SARS-CoV-2 infections in cats are most likely due to human-to-cat contact transmission.\n\n【11】Most natural SARS-CoV-2 infections of cats appear to run a mild or subclinical course, with respiratory or gastrointestinal clinical signs reported in confirmed natural infections . Evidence from experimental studies suggests that cats are susceptible to SARS-CoV-2 infection and can maintain the virus within a cat population and spill the infection backward or forward to other species . However, no evidence of cat-to-human transmission, nor of cat-specific mutations or variants of SARS-CoV-2, has been detected thus far . This finding contrasts reports on minks kept in farms, where mink-to-human spillback infections and mink-specific mutations have been reported . Although no evidence currently suggests that domestic cats play a role in the epidemiology of human SARS-CoV-2 infection, clinicians and veterinary practitioners should recommend that SARS-CoV-2–infected persons avoid close contact with their domestic cats and practice the same nonpharmaceutical prevention measures toward cats as they do to prevent human-to-human infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35baef60-30f0-4a63-a64d-8986aefd623f", "title": "National Vital Statistics Reports", "text": "【0】National Vital Statistics Reports (NVSR) are regular issues from the National Vital Statistics System (NVSS) that cover provisional birth, death, marriage, and divorce statistics. Four to six special reports are issued per year. Special reports cover final data on births for the previous year, and special topic analyses are issued from time to time. This list of all National Vital Statistics Reports (NVSR) that have been released to date is presented in an interactive, searchable, and sortable data tables format. Subscribe to CDC’s free email service to receive alerts by e-mail when new reports are available.\n\n【1】Data Tables Grouping Controls\n\n| Clear search terms | Reset to default view |\n| --- | --- |\n\n【3】All Volumes Volume 72 \\\\ Volume 71 Volume 70 Volume 69 Volume 68 Volume 67 Volume 66 Volume 65 Volume 64 Volume 63 Volume 62 Volume 61 Volume 60 Volume 59 Volume 58 Volume 57 Volume 56 Volume 55 Volume 54 Volume 53 Volume 52 Volume 51 Volume 50 Volume 49 Volume 48 Volume\n\n【4】All Years 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008\n\n【5】All Months January February March April May June July August September October November December\n\n【6】All Authors Abma, Joyce C. Anderson, Robert N. Arias, Elizabeth Atkinson, Jonnae O. Bachrach, Christine A. Bastian, Brigham A. Boudreault, Manon A. Boulet, Sheree Branum, Amy M. Copen, Casey E. Curtin, Lester R. Curtin, Sally C. Declercq, Eugene DeTurk, Peter B. Drake, Patrick Driscoll, Anne K. Eldridge III, Raymond I. Ely, Danielle M. Fingerhut, Lois A. Gregory, Elizabeth C.W. Gregory, Kimberly D. Hamilton, Brady E. Hedegaard, Holly Heinen, Melissa A. Henshaw, Stanley K. Heron, Melonie Hoyert, Donna L. Kirmeyer, Sharon E. Kissin, Dmitry Kochanek, Kenneth D. Korst, Lisa M. Kung, Hsiang-Ching MacDorman, Marian F. Martin, Joyce A. Mathews, T.J. Menacker, Fay Miniño, Arialdi M. Mohangoo, Ashna D. Mosher, William D. Munson, Martha L. Murphy, Sherry L. Osterman, Michelle J.K. Park, Melissa M. Peters, Kimberley D. Reed, Phyllis R. Rosenberg, Harry M. Rostron, Brian L. Saadi, Elizabeth W. Scott, Chester Smith, Betty L. Sutton, Paul D. Sutton, Shae R. Tejada-Vera, Betzaida Thoma, Marie E. Trinidad, James P. Uddin, Sayeedha F. Ventura, Stephanie J. Warner, Margaret Wei, Rong Wilson, Elizabeth C. Xu, Jiaquan Zeitlin, Jennifer\n\n【7】All Keywords accidents age age of mother antenatal steroids APNCU benzodiazepine birth birth certificate birth order birth rates birth spacing birthing center birth birthplace births birthweight Boston University School of Public Health bridge-coding cause of death Cedars-Sinai Medical Center cesarean Childbirth Research Associates, LLC cigarettes comparability contraception data quality death certificate death rates deaths decennial life tables dementia demographic and health characteristics demographic characteristics of births by State Division of Health and Nutrition Examination Surveys Division of Vital Statistics divorces drug overdose drug-involved death early teenage pregnancy educational attainment Euro-Peristat Project external cause fertility trends fetal death fetal losses fetal mortality gestational age gestational age-specific infant mortality rates gross reproduction rate health insurance Heligman-Pollard model Hispanic origin home birth homicide ICD–10 ICU admission induced abortions induction of labor infant characteristics infant health infant mortality infertility treatment initiating cause of death injury Inserm, France intrinsic birth rate intrinsic death rate intrinsic rate of natural increase labor and delivery leading causes of death statistics leading causes of death life expectancy life tables LMP-based gestational age estimate low risk marital status marriages maternal maternal age maternal and birth rates maternal and infant maternal and infant health maternal characteristics mean age Medicaid Medicare data method of delivery methodology midwife mortality mortality rates multiple cause multiple causes of death multiple race natality natality data National Center for Health Statistics National Institute of Child Health and Human Development National Vital Statistics System nature of injury net reproduction rate nonmarital births obesity Office of Analysis and Epidemiology Office of Research and Methodology opioid out-of-hospital birth out-of-wedlock births pain relief payment for delivery perinatal death perinatal mortality poisoning pregnancy pregnancy and marital status pregnancy interval pregnancy loss pregnancy rates pregnancy risk factors preliminary prenatal care prenatal care initiation prenatal smoking prepregnancy BMI preterm birth primary cesarean provisional data race race and Hispanic origin racial and ethnic differences repeat cesarean revised birth and fertility rates ruptured uterus selected cause of death sex sex differences sex ratio sexual activity smoking smoking cessation South Carolina Department of Health and Environmental Control State birth rates State fertility rates state mortality state rates State-specific birth rates State-specific mean age stillbirth stimulant suicide surfactant survival teen birth rates teen pregnancy teenage fertility teenage pregnancy teenagers text analysis The Guttmacher Institute TNO Child Health, Netherlands tobacco tobacco use during pregnancy total fertility rate transfusion trial of labor triplet births twin births U.S. Food and Drug Administration underlying cause uninsured validity VBAC vital statistics Vital Statistics Cooperative Program Washington Center for Health Statistics weight WIC women\n\n【8】Loading.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5e44c9d-38d5-4986-82fb-ce5d4a787431", "title": "Association Between Living Alone and Physical Inactivity Among People With and Without Disability, Florida Behavioral Risk Factor Surveillance System, 2009", "text": "【0】Association Between Living Alone and Physical Inactivity Among People With and Without Disability, Florida Behavioral Risk Factor Surveillance System, 2009\nAbstract\n--------\n\n【1】People with disability may be at risk of developing diseases due to physical inactivity; social support from family and friends is positively related to engaging in regular physical activity. We compared the association between living alone and engagement in physical activity among people with and without disability in Florida. We used multivariate logistical regression to analyze 2009 Florida Behavioral Risk Factor Surveillance System data (n = 10,902) to assess differences in physical activity in disability levels for respondents who lived alone versus those who did not. Respondents with a disability were less likely to engage in physical activity than were people without a disability, regardless of disability type, and the lowest rates of engaging in physical activity were found for people with disability who lived alone. Public health efforts should consider the role of household composition when targeting physical activity interventions among people with disability.\n\n【2】Objective\n---------\n\n【3】People with disability may be at risk of developing diseases due to physical inactivity . The US Department of Health and Human Services recommends that people with disability engage in regular physical activities . Evidence indicates an inherent benefit of moderate to vigorous physical activity on health outcomes . Social support from family and friends is positively related to regular physical activity . Research is needed to understand whether people with disability are disproportionately affected by lack of support . We hypothesized that living alone will more negatively affect engagement in physical activity among people with disability in Florida than among those without disability.\n\n【4】Methods\n-------\n\n【5】This cross-sectional study used 2009 Florida Behavioral Risk Factor Surveillance System (BRFSS) data. BRFSS is a random-digit–dial telephone household survey of adults aged 18 or older administered annually by state and territorial health agencies with support from the Centers for Disease Control and Prevention (CDC). BRFSS collects demographic and health-related data using core questions. In 2009, Florida added disability-related questions.\n\n【6】### Box. Disability definition and state-added questions used to develop the categories of disability, Florida Behavioral Risk Factor Surveillance System (BRFSS), 2009.\n\n【7】Questions used to define disability (BRFSS core questions)\n\n【8】*   Are you limited in any way in any activities because of physical, mental, or emotional problems?\n*   Do you now have any health problem that requires you to use special equipment, such as a cane, a wheelchair, a special bed, or a special telephone?\n\n【9】Questions used to create the severity levels of disability (state-added questions)\n\n【10】*   Because of any impairment or health problem, do you need the help of others in handling your routine needs, such as everyday household chores, doing necessary business, shopping, or getting around for other purposes?\n*   Because of any impairment or health problem, do you need the help of other persons with your personal care needs, such as eating, bathing, dressing, or getting around the house?\n*   How long have your activities been limited due to this condition or impairment?\n\n【11】Respondents were asked BRFSS questions used to define disability, as well as state-added questions (Box). The state-added questions were used to create a 4-level disability severity variable: no disability and 3 mutually exclusive classifications for activity limitations for 6 months or longer and needing assistance with 1) instrumental activities of daily living (IADL) only; 2) assistance with activities of daily living (ADL), with or without IADL assistance; and 3) neither IADL nor ADL assistance. ADL are the basic tasks of self-care (eg, feeding, toileting, grooming, bathing, walking, putting on clothes oneself), and IADL are elaborate skills needed for successful independent living (ie, finance managing, handling transportation, shopping, preparing meals, using a telephone or other communication device, managing medications, doing housework, and doing basic maintenance). We constructed a dichotomous variable, with participants “living alone” if they had no adults and no children younger than 18 living in their household. Then we assessed CDC-defined physical activity including any, moderate, and vigorous activity for at least 10 minutes each time in a usual week .\n\n【12】The response rate for the 2009 Florida BRFSS was 51% and included 12,055 respondents with landline telephones. Of these, 346 did not respond to questions that ascertained disability status; another 807 who reported activity limitations lasting less than 6 months were deemed to have a short-term disability and were excluded, yielding an analytic sample of 10,902.\n\n【13】We assessed demographic composition of the sample by age, sex, race, education, employment, income, marital status, and health behaviors (ie, smoking and drinking). Differences in physical activity measures for each disability classification were stratified by whether the respondent was living alone at the time of the survey and were assessed by using multivariate logistic regression analysis and considered significant at _P_ < .05. Interactions between disability status and living alone were tested. To give model estimates meaningful interpretation, adjusted average predicted probabilities of physical activity for each disability classification were calculated and adjusted to the overall distribution of covariates. All analyses were conducted using Stata/IC12.1 for Windows (StataCorp, LP) by using weighting procedures to account for the complex sampling design of the BRFSS.\n\n【14】Results\n-------\n\n【15】Overall, 51% of respondents were female, 61% were white, 60% were married, and 56% were employed. Mean age of respondents was 49 years, and 64% reported an annual household income below $50,000; 18% of the sample reported living alone. In total, 8,335 (%) respondents had no disability, and of the remaining 2,567 who reported having a disability, 235 (%) had ADL assistance needs, 713 (%) had IADL assistance needs, and 1,619 (%) had no needs. Respondents with a disability tended to be older, unmarried, and unemployed and had lower incomes than those with no disability.\n\n【16】We found a lower prevalence of participants engaging in each category of physical activity across all 3 levels of disability : any (running, gardening, or walking during the past month), moderate (bicycling, vacuuming, gardening, for at least 10 minutes per session in a usual week), and vigorous (running or heavy yard work, for at least 10 minutes per session in a usual week). One exception was found among people with ADL needs: a higher proportion engaged in vigorous activity than did people with activity limitations and IADL needs.\n\n【17】The adjusted average predicted probabilities indicated that respondents who lived alone were less likely to engage in all levels of physical activity than were respondents who did not live alone . Of respondents with activity limitations and ADL needs who did not live alone, the predicted probability of engaging in any physical activity was 52.3% (% confidence interval \\[CI\\], 34.3–70.4), 52.8% (.1–71.4) for moderate activity, and 23.8% (.9–40.8) for vigorous activity. For the same subsample who lived alone, the probability of engaging in any physical activity was 47.2% (.3–65.1), 41.6% (.8–61.3) for moderate activity, and 15.0% (.5–27.5) for vigorous activity. All comparisons were significant across regression models.\n\n【18】Discussion\n----------\n\n【19】Our findings suggest that living alone is a predictor of engaging in physical activity, among people both with and without disability. Adjusted average predicted probabilities of engaging in physical activity showed a consistent low prevalence across disability levels, regardless of household composition. However, people who lived alone were less likely to engage in any type of physical activity than were those who did not live alone across disability levels, and this had a disproportional impact on people with disability. Thus, living alone was negatively associated with engaging in all levels of physical activity (any activity, moderate, or vigorous) for people with disability. Our finding that people with ADL needs reported engaging in more vigorous activity than did those with IADL needs is counterintuitive. We think these results reflect the more integral cognitive functions of people with disability with ADL needs, which allow them to seek sources of support. We believe this finding warrants further exploration.\n\n【20】Social support is important for engaging in healthy behaviors, including physical activity . Although living alone does not equate to lack of social network or support , it is associated with early onset of disability and death . Our findings could be used to develop a set of health interventions to promote wellness and combat social isolation, among people both with and without disabilities. For example, online support sites and Internet-based communities increase social support and sense of community among this population. Interventions that use these technologies (eg, mHealth) might have positive results engaging people with disability in physical activity .\n\n【21】Our study had limitations. We used data from a representative survey with a large sample size. Although results are generalizable to households with landlines in Florida, a steady decrease in the use of landlines among young people may have caused underrepresentation of this demographic group. Survey data were self-reported and are subject to recall and social desirability biases.\n\n【22】Public health efforts should consider the role of household composition in increasing physical activity, among people both with and without disability. Moreover, efforts in improving physical activity among people with disability should take into account social support, specifically household composition.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71055fe3-ec65-4984-a5f8-0eeac8c92cf9", "title": "Clinical Malaria along the China–Myanmar Border, Yunnan Province, China, January 2011–August 2012", "text": "【0】Clinical Malaria along the China–Myanmar Border, Yunnan Province, China, January 2011–August 2012\nIncreased global efforts to control and eliminate malaria are leading to substantial declines in malaria-related illness and death . _Plasmodium vivax_ is the predominant malaria-causing species in China, followed by _P. falciparum_ . Cross-border migration from Myanmar is suspected to be the major source for the introduction of _P. falciparum_ malaria in southwestern China. During the past decade, the incidence of malaria in China has declined tremendously; the reduction in Myanmar has been less dramatic . To identify risk factors for clinical malaria and, in turn, to inform the ongoing malaria elimination programs in China, we conducted passive surveillance for malaria at health facilities along the China–Myanmar border in Yunnan Province, China, during January 2011–August 2012.\n\n【1】### The Study\n\n【2】The Southeast Asia Malaria Research Center , an International Center of Excellence for Malaria Research, in collaboration with the Chinese Center for Disease Control and Prevention, conducted passive malaria case detection along the China–Myanmar border. Surveillance was conducted at 60 hospitals and health care centers in Tengchong, Yingjiang, Longchuan, and Ruili Counties in Yunnan Province, China. According to the Sixth National Population Census of the People's Republic of China conducted in 2010 , the population of the 4 counties totaled ≈1.5 million. During 2010, Ruili and Yingjiang Counties reported the highest incidence of malaria in China .\n\n【3】Persons who sought care for febrile illnesses at 1 of the 60 surveillance site hospitals or health care centers were screened for clinical signs and symptoms of malaria. Case report forms were used to collect the following information from patients: demographic characteristics, occupation, education level, clinical symptoms, history of malaria in the preceding 12 months, history of travel within the 2 weeks preceding the clinic visit, history of fever, and use of measures to prevent malaria. For each suspected case-patient, thick and thin blood smears were prepared and examined by 3 experienced microscopists to provide a final diagnosis and parasite densities. Patients were considered to have clinical malaria if they had signs and symptoms consistent with malaria and a plasmodium-positive blood smear; severe malaria was defined according to World Health Organization criteria .\n\n【4】During January 2011–August 2012, a total of 8,296 Chinese and Myanmarese persons sought care for fever at the surveillance sites; 656 (.9%) of the patients had other signs and symptoms consistent with malaria. Blood smear examination by microscope confirmed malaria infection in 303 (.1%) of the 656 patients . Protozoa of all 4 _Plasmodium_ spp. that cause malaria in humans were detected; however, _P. vivax_ and _P. falciparum_ accounted for 69.0% and 27.7%, respectively, of the cases. Transmission peaked during April–July; cases of _P. falciparum_ infection were detected primarily during the peak season . Asexual parasite densities were 1,285 and 2,515 parasites/μL, for _P. vivax_ and _P. falciparum_ , respectively. Chinese patients had fever for a median of 3.0 days, and Myanmarese patients (>90% of whom lived in China) had fever for a median of 2.5 days (range 1–10 days; p>0.05) before seeking care at a surveillance site. A total of 4 (.9%) patients with _P. vivax_ malaria and 13 (.5%) patients with _P. falciparum_ malaria had severe symptoms at the first clinical visit and were treated as inpatients.\n\n【5】A total of 84.4% of suspected and confirmed malaria case-patients in our passive case surveillance were Chinese. However, among patients with suspected malaria, Myanmarese patients were 2.5 times more likely than Chinese patients to have malaria (odds ratio \\[OR\\] 2.5, 95% CI 1.5%–4.1%; p<0.0001) . Male patients were more likely than female patients to have malaria (OR 2.1, 95% CI 1.3%–3.5%; p<0.01), and most malaria case-patients were 18–60 years of age (OR 3.0, 95% CI 1.6%–5.3%; p<0.0001) . Compared with persons who worked indoors (e.g. students, office workers, and housewives), persons who worked outdoors (e.g. construction workers, traders, truck drivers who traveled frequently, and farmers) were at higher risk for malaria . Patients who reported using measures to prevent malaria (e.g. insecticide-treated nets and repellents) had a 14-fold lower odds of getting malaria than did patients who did not report using any preventive measures (OR 0.07, 95% CI 0.05%–0.10%; p<0.0001).\n\n【6】Among the 110 suspected malaria case-patients who reported travel during the 2 weeks before seeking care at a surveillance site, 54 were confirmed by blood-smear examination to have clinical malaria: 31 patients had _P. vivax_ infections, 21 had _P. falciparum_ infections, and 2 had mixed infections. After we adjusted for the confounding effects of age and sex, patients reporting travel across the border, >1 km into Myanmar, were 15 times more likely than nontravelers to have _P. falciparum_ malaria (adjusted OR 15.0, 95% CI 2.9%–175.0%; p<0.001); however, travel into Myanmar was not significantly associated with _P. vivax_ malaria (adjusted OR 1.9, 95% CI 0.8%–4.9%) .\n\n【7】### Conclusions\n\n【8】Most previous studies of malaria in China have analyzed case reports collected and reported by counties as a part of their routine health reporting system . Such information is prone to reporting bias and to underreporting . Furthermore, most publications implicating cross-border activity as a risk for malaria have not adequately delineated how migration and travel data were collected or how these variables were defined .\n\n【9】Our study has 2 major strengths: data were collected prospectively and the association with travel to Myanmar was determined on the basis of travel histories within the 2 weeks before study participants sought care at a surveillance site hospital or health center. Our observation that 44% of the febrile case-patients were female, although female patients comprised only 9% of the malaria case-patients, supports the association between occupation and cross-border travel and risk for malaria.\n\n【10】Despite recent reductions in the number of malaria cases in the border counties, our findings suggest that _P. vivax_ malaria persists in areas of Yunnan Province along the China–Myanmar border, whereas cases of _P. falciparum_ malaria are probably imported from Myanmar . Cross-border trade, logging, quarry and plantation activities, and construction in Myanmar may reintroduce _P. falciparum_ parasite to Yunnan Province. Whether the findings from this surveillance system, which focused on the China–Myanmar border areas, can be extrapolated to a larger geographic region needs further validation. Future elimination efforts should focus on the effects of cross-border activities on malaria parasite transmission, and elimination efforts should include more intensive surveillance so that prevention and control activities can be directed at hot-spot regions along the China–Myanmar border.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2e460b69-8405-4043-96af-b96576168531", "title": "High Tuberculosis and HIV Coinfection Rate, Johannesburg", "text": "【0】High Tuberculosis and HIV Coinfection Rate, Johannesburg\n**To the Editor:** Tuberculosis (TB) is the leading cause of illness and death among HIV-1–infected patients in sub-Saharan Africa , but valid data on the population-level interaction between the TB and HIV epidemics are scarce . Our objective was to determine the extent of this dual epidemic in our setting, a hospital in Johannesburg, South Africa. We did this by introducing bedside TB and HIV counseling. We also intended to increase the use of voluntary counseling and testing for our TB patients and facilitate referral to our antiretroviral clinic.\n\n【1】From February to April 2006, 2 volunteers from Community AIDS Response (CARE) counseled patients admitted to the medical wards of the Helen Joseph Hospital. This regional hospital serves a catchment population of >500,000 people, predominantly low-income black Africans. Counselors provided TB and HIV wellness and adherence information, HIV pretest counseling, and referral to the Themba Lethu Clinic for rapid testing that used standard CARE modules.\n\n【2】Basic demographic, TB, and HIV data from patient records were documented on standard data collection forms. Missing data were extracted from the hospital database and Therapy Edge-HIV, the data management system used by the HIV clinic. HIV testing was conducted with a fourth-generation ELISA or rapid finger prick antibody test, according to World Health Organization guidelines.\n\n【3】Most admissions were for pulmonary TB. A total of 467 patients receiving TB treatment were counseled; 8 of these patients refused the TB counseling service, and 2 refused voluntary counseling and testing for HIV. These 467 patients constituted 13% of medical admissions and excluded the 1,075 patients seen at the hospital’s outpatient clinic with suspected TB for this 3-month period. Our impression is that this figure constitutes an underrepresentation of the total TB admissions because TB counselors were not able to see every patient with TB.\n\n【4】Laboratory data were retrievable for 373 inpatients. For 301 (%) of the 373 patients, TB blood culture, smear, or culture results could be traced. Hence, 72 (%) of 373 patients who were receiving TB treatment had no record of a diagnostic effort to confirm TB. A total of 284 (%) HIV test results could be traced; 270 (%) of the 284 accessible TB patients had concurrent HIV infection .\n\n【5】Most ([89%\\]) documented HIV results were from ELISAs performed during admission. Rapid testing performed in the ward was unacceptable to patients because confidentiality was compromised in large, busy wards and patients were often too ill to move to a side room. The system of making an appointment with the HIV clinic at the time of discharge failed because few patients (%) actually had the rapid test after admission or began antiretroviral therapy. Those who began such therapy would have been captured on our database.\n\n【6】The level of concurrent TB and HIV coinfection at the hospital was 95%. To the best of our knowledge, this is the highest level ever described in the peer-reviewed English-language literature . This finding may reflect the selection bias for our inpatients, who generally would have more coexisting conditions than outpatients do. Also, HIV data were missing for 24% of the 373 patients, a fact that may also influence this finding.\n\n【7】The peak age incidence of TB in our population corresponds with previously published data and is similar to the peak age incidence of the HIV epidemic in South Africa . In one third of the admitted patients, no TB investigations were undertaken. This may be because patients provided a history of TB diagnosed elsewhere, or it may reflect the high rate of sputum smear negativity in the HIV-infected population, which lowers the clinician’s threshold for empiric TB treatment.\n\n【8】Mycobacteremia appeared to be less common (%) than reported in other African studies . However, we did not have a complete dataset—only 195 (%) of the 373 patients could be evaluated.\n\n【9】TB and HIV have reached unprecedented levels in our urban inpatient population. TB and HIV must be viewed as different sides of the same coin, and services and staff must change accordingly. We need to use the opportunity of hospital admission to educate patients on the interaction between these 2 epidemics and facilitate patient referral for long-term management. Such management would include voluntary counseling and testing, as well as antiretroviral medication. The latter is a recognized strategy of TB control because it reduces the risk for TB by 70%–90% .\n\n【10】In addition, all inpatient procedures in our TB/HIV control programs need to be strengthened. Infection control interventions to limit the high rates of nosocomial transmission of TB to other vulnerable patients and staff need to be instituted. At our hospital, we are committed to these approaches. To this end, we have secured a Presidents Emergency Plan for AIDS Relief Grant via the nongovernmental organization Right to Care, which shares our vision. Urgent and extraordinary measures are indeed required in our combined control programs to achieve the Millennium Development Goals for TB/HIV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3f59718f-b757-41f5-ba62-8d705bf9dc46", "title": "Seasonal Patterns of Buruli Ulcer Incidence, Central Africa, 2002–2012", "text": "【0】Seasonal Patterns of Buruli Ulcer Incidence, Central Africa, 2002–2012\nBuruli ulcer (BU) is a severe infection caused by _Mycobacterium ulcerans_ . Most affected are rural populations living in tropical areas with abundant wetlands . BU causes extensive, damaging skin lesions and often results in severe disabilities. Of the 4,000 cases reported to the World Health Organization by 14 countries in 2011, >95% originated in African countries around the Gulf of Guinea .\n\n【1】Much remains unknown about the mode of _M. ulcerans_ transmission and the epidemiology of BU . Specifically, although the spatial distribution of BU in several settings has been addressed , most studies have examined only temporal variations of BU incidence in terms of yearly trends . Several observational studies have reported seasonal changes in the monthly number of cases and have hypothesized that cases are linked with rainfall variation . One spatiotemporal study in Australia showed that BU incidence was associated with rainfall variability with a 5-month lag and with total rainfall with a 19-month lag . However, none of these studies provided quantitative evidence of seasonal changes in BU incidence and their relationship with seasonal environmental changes. Indeed, a formal demonstration of such evidence requires a sufficiently long time series, large numbers of cases from a defined source population, and use of signal analysis techniques adapted to the constraints of BU disease surveillance and environmental data  . Therefore, we investigated the seasonality of BU case incidence during 2002–2012 in Akonolinga District, located in the highly BU-endemic region of the Nyong River valley in Centre Region, Cameroon.\n\n【2】### The Study\n\n【3】Relying on previous spatial analysis of BU incidence in Akonolinga District, we analyzed a series of cases that occurred in the highest BU-risk area of the district, located along the Nyong River upstream of Akonolinga . This area includes 24,469 inhabitants of the town of Akonolinga and 24 surrounding villages. We analyzed 562 new cases of BU that originated in this area from January 2002 through May 2012, after aggregation by month of diagnosis. Biological confirmation was obtained from the National Reference Centre for Mycobacteria for 354 (%) cases. The BU incidence rate remained stable over the 10-year period at 2.2 cases/1,000 person-years.\n\n【4】Median BU incidence peaked in March, and a second peak occurred in September , but monthly medians did not differ significantly (Kruskal-Wallis test, p = 0.149). Given the specificities (nonstationarity) of the BU case series, wavelet analysis was the appropriate method for analysis . A 1-year periodic signal was identified in the BU-case time series from 2005 to 2011, and this periodicity was statistically significant from mid-2005 to the beginning of 2009 .\n\n【5】Next, we analyzed the links between BU and seasonal changes by using wavelet association and phase analyses between BU case incidence and total monthly rainfall (in mm) or mean Nyong River flow (in cubic meters per second). Strong seasonality was found in the series of monthly total rainfall and of monthly mean Nyong River flow; a 1-year period and a weaker 6-month period corresponded to the 2 rainy seasons separated by a period of lesser rainfall (the small dry season, mid-July to mid-August) . Because of its shape, the wavelet detected yearly rainfall oscillations between a minimum in December (dry season) and a maximum in July (the middle of the rainy period) instead of the maximal rainfall months of October and November .\n\n【6】We assessed the association of the incident case signal with environmental variables . The 1-year periodic signal of the BU case series was associated with Nyong River flow from the end of 2005 to the end of 2009  and with rainfall from the end of 2005 to the beginning of 2011 . Under the assumption that changes in the environment preceded changes in BU incidence, phase analysis indicated that cases lagged 6 months behind Nyong River flow oscillations . When the 2 signals were associated, a 9-month lag behind rainfall oscillations was observed .\n\n【7】### Conclusions\n\n【8】In the BU-endemic focus of Akonolinga, Cameroon, significant 1-year seasonal variations in BU incidence occur. The incubation period for BU has been estimated to be ≈4.5 months when data from Australia are used  and ≈3 months when data from Uganda are used . The median delay between symptom onset and health care seeking was reported to be 5 weeks in Akonolinga (interquartile range 3–12 weeks), yielding a delay between infection and diagnosis of 5–6 months. Given this delay and a finding of BU diagnosis peaks during March–April, the number of infections would therefore be highest from August through October . Such a pattern was observed in the 1970s in Uganda  and Cameroon  and more recently in Côte d’Ivoire . In low BU-endemicity French Guiana, an overseas territory located near Brazil at the same latitude as Cameroon, periodic peaks after the 2 rainy seasons have been reported .\n\n【9】Variations in BU incidence result from variations in population exposure  combined with variations in environmental presence of _M. ulcerans_ . We hypothesize that one of the main drivers of these variations is the seasonal flooding of the Nyong River, which rises 3–5 m from April through November, creating temporary bodies of water and swamps on a vast surface, deeply affecting the ecosystem.\n\n【10】Although _M_ . _ulcerans_ was identified year-round in specific environments such as permanent swamps, its presence and abundance were maximal during the rainy months, July–October  . Prevalence of _M. ulcerans_ in rivers was high at the beginning of the rainy season and was high in flooded areas during the following small dry season and high rainy season .\n\n【11】Human activity patterns follow these seasonal changes, resulting in seasonal variations in exposure to _M. ulcerans_ . In Uganda, the contribution of permanent swamps to BU risk and the increased risk associated with temporary swamps during the rainy season have been documented . According to residence, age, and/or sex, the inhabitants of the Akonolinga district face varying exposures to aquatic environments; during the period identified as high risk, populations frequent seasonally flooded environments for water collection, fishing, and harvest of dry season cultures  .\n\n【12】During the study period, the intensity of the association between BU incidence and rainfall or Nyong River flow varied. The seasonal signal was detected over 5 consecutive years and was strongest when yearly variations in the Nyong River flow were lower , which could indicate transient forcing of BU incidence by seasonal phenomena. Assessment of the effects of lower frequency climatic events, such as El Niño Southern Oscillation, is needed. In French Guiana, where BU endemicity is low, such events were shown to affect BU incidence dynamics .\n\n【13】We showed that BU incidence in this region varies significantly by season and linked these variations to the fluctuations of _M. ulcerans_ occurrence in the environment, which are probably driven by the dynamics of freshwater ecosystems of the Nyong River. In Akonolinga, during the high rainy season when risk for _M. ulcerans_ transmission seems to be highest, populations should increase their protective behaviors, and case detection efforts should be intensified in subsequent months to ensure early diagnosis and access to care.\n\n【14】Dr. Landier specializes in infectious diseases epidemiology. While working on his PhD degree at the Institut Pasteur in Paris, he focused on BU epidemiology and _M. ulcerans_ transmission in Central Africa.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "95410643-eaed-44e5-8972-88d1cf88f265", "title": "Coyotes as Reservoirs for Onchocerca lupi, United States, 2015–2018", "text": "【0】Coyotes as Reservoirs for Onchocerca lupi, United States, 2015–2018\n_Onchocerca lupi_ is a species of zoonotic, filarial nematode that causes onchocerciasis in dogs, cats, and humans. It was first described in 1967 in Georgia, then part of the USSR, in the periocular tissues of a wolf (Canis lupus lupus_ )  and has been reported in dogs (C. lupus domesticus_ ). Since 2013, increased detection of _O. lupi_ infections in dogs and humans in the United States and Europe has renewed interest in this parasite, its geographic distribution, and the range of its natural hosts .\n\n【1】The geographic distribution and prevalence of the _O. lupi_ nematode in the United States is unknown. US veterinarians are not required to report _O. lupi_ infections in canines, making it difficult to identify the parasite’s geographic distribution. The first documented case of _O. lupi_ infection in the United States affected a dog in California in 1991 ; since then, _O._ _lupi_ infections have been reported in dogs, cats, and humans in Arizona, California, Colorado, New Mexico, Texas, and Utah . This parasitic nematode is now endemic in domesticated canines in the southwestern United States . Reports of _O. lupi_ infection in Canada (Alberta and Prince Edward Island)  associated with dog importation from the southwestern United States and travel of US companion animals suggest an anthropogenic spread of the _O. lupi_ nematode. Whether wild canids, including coyotes (C. latrans_ ), might be reservoirs for the _O. lupi_ nematode is unknown.\n\n【2】Because of the growing number of _O. lupi_ infections in canines and humans, public health officials must understand the prevalence and distribution of this parasite in wildlife. Toward that goal, we investigated coyote populations in Arizona, New Mexico, and Nevada as potential primary hosts and natural reservoirs for the _O. lupi_ nematode.\n\n【3】### The Study\n\n【4】From December 2015 through July 2018, we collected skin tissue samples from coyotes harvested for predation management and from a hunt in Arizona, New Mexico, and Nevada conducted by the Arizona Game and Fish Department. We did not euthanize any coyotes for the specific purpose of this study. Skin tissue from the interocular frontal area of the animal’s head was removed and stored in 80% ethanol until we extracted the DNA. We screened 707 DNA sequences for an _O. lupi_ cytochrome c oxidase (COI) gene  using SYBR Green–real-time PCR on a QuantStudio 7 Flex Real Time PCR System . We used DNA from an adult worm from an infected dog in northern Arizona as a positive control . We included a no-template control in every real-time PCR reaction plate. We compared the product’s melting curve to the positive control using a dissociation curve. We prepared every sample that had a melting curve resembling that of the positive control for amplicon sequencing using neat DNA and Illumina  technologies.\n\n【5】Thirty-seven (.2%) samples from 8 counties in Arizona and New Mexico  had sequences that aligned with the reference gene. Of these samples, 36 were from Arizona and 1 was from Hildago County, New Mexico. In Arizona, the highest prevalences of _O. lupi_ infection were in Navajo County ([35.4%\\] positive coyotes) and Apache County ([9.4%\\]) . Coconino County had the third highest number  of coyotes that tested positive for the _O. lupi_ nematode but a lower positivity rate (.7%) than other counties. For example, in Hidalgo County we sampled only 2 coyotes, 1 of which tested positive.\n\n【6】We produced a phylogenetic tree of our 43 COI sequences (including 4 that were isolated from infected dogs and 2 from humans \\[GenBank accession nos. MT878134–9\\]) in addition to 30 _O. lupi_ COI genes on GenBank spanning 432 total bases containing 12 single-nucleotide polymorphisms (SNPs)  using IQTREE version 1.6.9  software with 1,000 bootstrap replicates. Examining only this region of the COI gene, we determined the US samples (from dogs, cats, coyotes, and humans) clustered within a single clade with dog samples from Germany, Romania, and Greece. Within this clade, we detected no SNP differences. This clade was separated from a sample from Hungary by 1 SNP and from a single clade containing a human isolate from Turkey and a dog sample from Greece by 1 SNP.\n\n【7】### Conclusions\n\n【8】_O. lupi_ infection has been reported mainly in domestic dogs and cats in the southwestern United States . However, international transportation (purchasing, adopting, and exporting) of dogs from that area has introduced this parasite into environments to which it is not endemic . We hypothesize coyotes are reservoirs for the _O. lupi_ nematode and could spread this parasite throughout the southwestern United States.\n\n【9】We consider the probable importance of coyotes as natural reservoirs and dispersal agents. In the United States, the average home territory covered by a resident coyote population (either a pack or lone coyote) is 5–41 km 2  , whereas solitary transient coyote territories are up to 155 km 2  . The large geographic range and widespread occurrence of not only coyotes, but also the putative black fly vector (Diptera: _Simuliidae_ ) , might facilitate the spread and establishment of the _O. lupi_ nematode in the southwestern United States. Furthermore, many North American wild canids, such as wolves and foxes, have never been assessed for the _O. lupi_ nematode but also should be considered as potential reservoirs. Although the _O. lupi_ nematode is only endemic to the southwestern United States, without appropriate surveillance and mitigation strategies it might spread across the United States and into Canada and Mexico. We are not aware of any reports of _O. lupi_ nematodes in Mexico; however, we identified a coyote that tested positive for _O. lupi_ infection in Hildago County, which borders Mexico. Increasing surveillance in nearby counties upon identification of _O. lupi_ nematode–positive coyotes would be prudent. Furthermore, the overlap of rural human residences with coyote and black fly populations probably increases the risk for human exposure.\n\n【10】In summary, canine onchocerciasis is an ongoing emerging infectious threat to wildlife, companion animals, and humans. The expanding range to which the _O. lupi_ nematode is endemic, coupled with increased incidence of onchocerciasis in humans and canines in the southwestern United States, reinforces the need to understand, respond to, and potentially mitigate this threat. This understanding will enable the development of surveillance and mitigation strategies, determine the risk of spread to nonendemic regions, and identify human populations at high risk of infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "da590a31-50a1-4143-90d5-cd14da739517", "title": "Liberia—Moving Beyond “Ebola Free”", "text": "【0】Liberia—Moving Beyond “Ebola Free”\nAlthough the ongoing Ebola epidemic has brought much attention to Liberia, diseases of poverty, such as malaria, tuberculosis (TB), and maternal–newborn complications, rarely make the headlines. Along with the other West African countries that bore the brunt of the epidemic, Liberia ranks near the bottom of the Human Development Index, a composite measure that assesses whether persons enjoy a long and healthy life, can acquire knowledge, and have an adequate standard of living . In Liberia, before the Ebola outbreak, ≈50 doctors attempted to care for ≈4 million persons . In an already fragile health-care setting, Ebola took a terrible toll: >8% of the health care workforce in Liberia died from the virus . The consequences of such a dramatic loss will be felt for years to come, especially in the areas of infectious disease and maternal and infant mortality . As we renew our commitment to make Liberia “Ebola free,” we should remind ourselves that in the 21st century, Liberians still die from 19th century diseases. The focus must go beyond “getting to zero.” As concerned clinicians, we argue that much more work needs to be done.\n\n【1】In early 2015, we went to Grand Gedeh County as short-term clinicians, working with a nongovernmental organization (NGO) to support Liberia’s existing health care infrastructure. Unlike emergency response NGOs, our NGO turned its attention to assisting local hospitals, clinics, and communities in their routine, day-to-day health care activities. Tucked away in Liberia’s remote southeastern corner, Grand Gedeh County had largely been spared from the epidemic; at that point, only 1 case had been reported since the epidemic’s onset. We would not wear the protective space suits so familiar in the public eye. Instead, we would work in surrounding communities, meeting with local health workers and psychosocial officers, or on the wards with Liberian nurses and doctors, tending to persons who suffered, and at times died, from easily preventable diseases.\n\n【2】The public hospital offered a glimpse into the state of Liberian health care facilities in the wake of Ebola. The building itself lacked electricity most hours of the day; it had no running water, and there was a severe shortage of medications and basic supplies. The only available pain medication at the hospital was oral acetaminophen tablets. We witnessed 5 neonatal deaths in 10 days. In the community, we listened to first-hand experiences about the peak of the epidemic from those who had relocated to Grand Gedeh County. They recounted how entire families died in the span of weeks and how fear and stigma rent communities apart. Nonetheless, working alongside and learning from our colleagues and friends filled us with deep admiration and humility. Despite their country’s history of war, poverty, and disease, the Liberians we met believe they can create something better.\n\n【3】Indeed, the inner strength and commitment of our Liberian colleagues prompted much self-reflection. In the throes of the epidemic, Liberian health care workers provided care at the expense of their own safety, and many died. Even before Ebola, however, these same health care workers were risking their own health and well-being, and that of their families, by treating patients with TB without respiratory masks or going without basic vaccinations—all fundamental, common-place measures taken in American hospitals. Our contribution as short-term health care providers seemed minuscule compared with the reality that our Liberian colleagues continue to face. This personal struggle was made all the more disturbing by knowing that an exit was already planned for us. Our Liberian colleagues and friends remain.\n\n【4】As we reflect on our brief time in Liberia, we revisit the central issue that compelled us to go: do we, in our position of comfort and relative ease, have a responsibility to help impoverished persons who have an exotic and frightening disease? We still emphatically believe that the answer is yes. But shouldn’t we go further and ask whether that responsibility includes diseases and deaths that have become routine, accepted, or rendered invisible?\n\n【5】The story of Ebola confirmed that we are all connected in a complex sphere of exchange—of resources, technologies, ideas—and that the arrangement benefits some more than others. It was not coincidental that so many persons in that part of the world fell ill from the virus. Decades of conflict, economic impoverishment, and a near-nonexistent health care system created a tangle of injustices that set the stage for the epidemic. We share the conviction that these injustices—unfair social, political, and economic arrangements—are the real culprits behind the Ebola epidemic and, in broader terms, today’s health disparities. Our participation reminds us that the Ebola experience is but one example of how politics and poverty cause death and suffering on a grand scale.\n\n【6】We should examine how the Ebola epidemic, and the global response to it, fit into this larger picture of health disparities and injustice. Ebola linked Monrovia, Liberia, to Dallas, Texas, USA, and in so doing exposed how interconnected our global society has become. While infected international health workers were evacuated to specialized centers with experimental drugs, infected Africans hoped for supportive care in crowded Ebola treatment units, circumstances that forced us to grapple with the unfairness of today’s health care inequities. Despite years of bureaucratic research, financing, and planning, the epidemic’s unprecedented scale demanded major reexamination of what is meant by the concept of global preparedness. Thus, in terms of understanding how health disparities should be addressed, Ebola overturned key assumptions: that rich countries can ensure the health of their populations in isolation, that fundamental ethical issues regarding the role of global health agencies and their actors are settled, and that a single, global authority can marshal and coordinate resources effectively .\n\n【7】Why don’t we apply the same lessons to diseases of poverty and other emerging infectious diseases? De Cock and colleagues note, “It is difficult to explain why investment in separating human drinking water from human feces, the basis of the nineteenth century public health revolution in Europe and North America, has not been a higher political or development priority in resource-poor settings” (p. 1195). If we acknowledge our interconnectedness through social, economic, and political dimensions, that there exist severe shortcomings in health equity both within and between countries, and that these challenges require cooperation beyond the traditional donor-recipient model, then perhaps we will come to terms with why deaths from malaria, TB, or diarrheal diseases implicate all of us, and matter . Engagement in global health is not just a humanitarian endeavor; it is a priority for our collective well-being.\n\n【8】As the Ebola crisis wanes and media attention shifts away from West Africa, the underlying determinants of health and disease are still in place. The reemergence of Ebola virus infection attests to this, as do the senseless deaths of those who die from easily preventable diseases. We must hold ourselves to a higher standard, one beyond a 42-day Ebola-free countdown. With self-reflection and in a spirit of solidarity, we must continue to articulate that standard and where our responsibility lies in meeting it.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a594599-192c-4554-88b8-24338d551ee6", "title": "Chikungunya Virus in Macaques, Malaysia", "text": "【0】Chikungunya Virus in Macaques, Malaysia\n**To the Editor:** In the past 10 years, chikungunya virus (CHIKV) has caused global epidemics of fever, rash, and arthralgia affecting millions of humans, most recently in the Americas . CHIKV is an alphavirus transmitted by _Aedes_ spp. mosquitoes. This virus has been isolated from wild vertebrates, particularly nonhuman primates (NHPs), in Africa . This sylvatic cycle might maintain the virus during interepidemic periods. The role of sylvatic cycles in Asia is less clear.\n\n【1】Encroachment of human settlements into forests has caused increased conflict between humans and macaques for space and resources in urban and rural areas. This interface exposes humans to zoonotic pathogens found in monkeys, such as CHIKV, dengue virus, and _Plasmodium knowlesi_ . The most common macaque species in Peninsular Malaysia is the long-tailed macaque (Macaca fascicularis_ ); an estimated population of >130,000 monkeys live in human-populated areas . We determined the potential role of long-tailed macaques in conflict with humans as a reservoir of CHIKV in Malaysia.\n\n【2】In response to reports of long-tailed macaques in human-populated areas, the Malaysian Department of Wildlife and National Parks traps monkeys in these areas and relocates them to forest areas. As part of the Wildlife Disease Surveillance Program conducted by Outbreak Response Team of this department, with assistance from the EcoHealth Alliance, serum samples were collected from 147 long-tailed macaques at >20 sites in the states of Selangor (monkeys), Negeri Sembilan , Perak , Pahang , and Penang  . Samples were collected in October–November 2009 and October 2010, just after a nationwide outbreak of CHIKV that affected >13,000 persons in 2008–2009 . These samples represent 0.05%–0.29% of estimated populations of long-tailed macaques in human-populated areas in these 5 states .\n\n【3】A seroneutralization assay was performed by using baby hamster kidney cells to screen for neutralizing antibodies against CHIKV in heat-inactivated monkey serum samples. Samples at a 1:20 dilution that neutralized CHIKV in < 2 days were confirmed as positive by using a described immunofluorescence-based cell infection assay  with modifications. Serially diluted serum samples were mixed with equal volumes of CHIKV suspensions at a multiplicity of infection of 10 and inoculated into baby hamster kidney cells. After incubation for 6 h at 37°C, cells were fixed, processed, and immunostained with a monoclonal antibody. Fluorescence was determined by using the Cellomics High Content Screening ArrayScan VTI imaging system (ThermoFisher Scientific, Waltham, MA, USA).\n\n【4】Despite the recent widespread CHIKV outbreak in humans and proximity of sampled macaques to humans in Malaysia, CHIKV neutralizing antibodies were detected in only 1 (.7%) of 147 macaques. This seropositive macaque was captured in Kampung Jeram Mengkuang (.06°N, 101.24°E) in Perak, one of the most affected states during the 2008–2009 outbreak . All serum samples tested showed negative PCR results for the CHIKV envelope 1 protein gene.\n\n【5】CHIKV neutralizing antibodies have also been detected in NHPs in Thailand  and Malaysia . In the study in Malaysia, 6 (.5%) of 393 long-tailed macaques were seropositive . A recent study in Mauritius reported neutralizing antibodies in just 1 (.7%) of 134 long-tailed macaques after a large human outbreak in 2006 . In another study in Malaysia, 105 wild long-tailed macaques were sampled from several sites in 3 states during 2007–2008; CHIKV was isolated from 4 (.8%) samples from 1 site (Kuala Lipis in Pahang) . This site is 90 km from the village where the 1 seropositive monkey was trapped in our study. In addition, a variety of domestic and wild vertebrates, including horses, cattle, pigs, rats, squirrels, bats, and chickens, have been reported to be seropositive for CHIKV .\n\n【6】These results indicate that CHIKV infects long-tailed macaques in Malaysia, but seroprevalence rates are low, and there is little evidence of viremia, except at the 1 specific site in Kuala Lipis. Although experimental infection of long-tailed macaques resulted in detectable CHIKV antigen in macrophages for \\> 3 months, infectious CHIKV is not detectable beyond 44 days , and long-term neutralizing immunity is present for \\> 180 days . However, there is no evidence for long-term active CHIKV infection and its recrudescence in macaques or humans.\n\n【7】A limitation of our study was the relatively small number of monkeys sampled. Although we found no overall significant correlation between incidence of human cases of infection with CHIKV and estimated number of long-tailed macaques per 100,000 persons in each state (r 2  \\= 0.05, p = 0.49), we cannot exclude the involvement of long-tailed macaques in a local outbreak at a specific site. Long-term dynamics of antibodies against CHIKV in long-tailed macaques are not known, which might affect sensitivity of detection assays.\n\n【8】We conclude that long-tailed macaques in conflict with humans in specific areas probably played a small part in transmission of CHIKV during recent large outbreaks in humans in Malaysia. Human–mosquito–human transmission and travel by infected humans were probably the major factors involved in spread of this virus. If a true sylvatic reservoir that effectively maintains CHIKV is present in Malaysia, long-tailed macaques might play only a minor role. In addition, involvement of other NHPs and mammals remains to be elucidated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c2301a9-d3a0-4b30-a055-21f62b21a8e2", "title": "Deaths Associated with Influenza Pandemic of 1918–19, Japan", "text": "【0】Deaths Associated with Influenza Pandemic of 1918–19, Japan\nThe influenza pandemic of 1918–19 caused unprecedented devastation ; worldwide, it is estimated to have taken 25–100 million lives , exceeding the combined death toll of both world wars. One of the strangest aspects of the currently held wisdom about the pandemic is the curiously low death rate attributed to Japan compared with other countries in Asia. Official records for Japan put the death toll at 257,363 persons , resulting in a crude influenza-attributable death rate of 0.47%. Patterson and Pyle  reported 350,000 deaths, and Johnson and Mueller  cited a figure from Palmer and Rice  of 388,000 deaths. Given Japan’s population of >54 million at the time , the influenza-attributable mortality rates (.64%–0.71%) are remarkably low by Asian standards, although they are similar to the rates calculated for the United States, Canada, and western Europe (.65%, 0.61%, and ≈0.48%, respectively) . Patterson and Pyle’s  conservative estimate of a global rate of 1.66% and Johnson and Mueller’s  substantial upward revision of that percentage to 2.77% suggest that the estimates for Japan, which are less than one quarter of the latter estimate, merit closer scrutiny. Although the epidemiologic approach used by Richard et al. which also uses death statistics reported by the Japanese health authorities, raises the estimate to 481,000 (or 0.88% of the population at the time) , even this estimate is extraordinarily lower than estimates from other parts of Asia.\n\n【1】As Taeuber argued in her classic book, _The Population of Japan_ , Japan occupies a special place in demography . Worldwide it remains one of the largest economies (third in 2011) and one of the most populous countries (tenth in 2011). Yet, surprisingly, substantial knowledge gaps remain with regard to the influenza pandemic of 1918–19 in Japan, rendering it “a strangely neglected episode in modern Japanese history” (p_ _. 389_ ). For example, a search of Taeuber’s work for the term “influenza” revealed only 1 mention of the influenza epidemic of 1918, in the context of speculation that it “may have led to reduced conceptions” (p_ _. 233_ ).\n\n【2】The few scholars who have studied the influenza pandemic in Japan have approached it from 1 of 3 broad perspectives: historical, epidemiologic, or demographic. The historical approach is exemplified by the works of Palmer and Rice, which provide a qualitative contextualization of aspects of the pandemic and its management in Japan . A second line of research is epidemiologic, within which 2 broad goals are pursued. The first goal is to produce estimates of major epidemiologic characteristics of the virus , and the second goal is to produce epidemiology-based estimates of mortality rates from the pandemic . The demographic approach is exemplified by Morita, Okazaki, Taeuber, and Yasukawa and Hirooka . Although these studies emphasize broader patterns of population growth in Japan, a few address the question of death rates during the pandemic. For example, Yasukawa and Hirooka  relied directly on official death statistics, including those from the pandemic, to produce estimates of the population in early 20th-century Japan. Unfortunately, the quantitative literature seems to have more or less accepted the official vital statistics on disease-specific deaths, feeding them (and therefore their inaccuracies) into otherwise technically refined estimates of population and population growth.\n\n【3】A common characteristic of the above studies is their heavy reliance on official vital and health statistics of the time. Such data are widely recognized by demographers as being plagued by the often-severe problem of underreporting. Indeed, according to Johnson and Mueller, “it is generally accepted that recorded statistics of influenza morbidity and mortality are likely to be a significant understatement” (p_ _. 108_ ). For India, Davis estimated that the “amount of underregistration certainly exceeds 30 per cent at all times, and is probably nearer 50 per cent” (p_ _. 34_ ). For Indonesia, Gooszen advised that such data “should be regarded with a good deal of caution” (p_ _. 32_ ), and Nitisastro opined that “for the system of registering deaths, the quality of the results was poor” (p_ _. 101_ ). Japan is no exception to this pattern. According to Mosk, “we do not have a trustworthy picture of what happened to vital rates in the Tokugawa period…. The same can be said for the Meiji period” (p_ _. 658_ ), and Taeuber’s assessment was that “the critical question is the accuracy of the records of vital events” (p_ _. 50_ ). The uncharacteristically low estimates of deaths from influenza in Japan provide a strong rationale for cross-checking the findings in the manner of Davis’ classic study of India .\n\n【4】We therefore used recently developed statistical methods to estimate the loss of population in Japan from the influenza pandemic of 1918–19. We adopted an approach that intentionally avoids heavy reliance on vital registration data and is based instead on population count data for Japan of that period. By applying data for multiple prefectures over time to prefecture-level population statistics, we estimated population loss from the pandemic to be the difference between expected population (using the prepandemic trajectory) and observed population (using the postpandemic trajectory) . The new estimates are appreciably higher than the earlier estimates, bringing Japan’s pandemic experience in line with that of other parts of Asia and resolving a major puzzle in the epidemiology of the 1918–19 pandemic.\n\n【5】### Methods\n\n【6】##### Data-associated Issues\n\n【7】With regard to data, 3 issues should be considered. The first is the coverage of the population count data for Japan in the late 19th and early 20th centuries, described by Matsuda  and Taeuber . In 1871, the Imperial Japanese government passed a law, the _koseki-ho_ , which required registration of households and persons in Japan. A major emphasis of the registrations was legal domicile, or _honseki_ status. The first set of summations of these registers was made in 1898, after which they were computed every 5 years until 1918, for a total of 5 nationwide population counts derived directly from the registers . The number of persons who physically resided in different parts of Japan (de facto A-type population) was computed by adjusting the numbers of persons with _honseki_ status downward to account for those who had _honseki_ status but lived in other locations. These numbers were further adjusted to account for the discrepancy between numbers of registered persons who immigrated into the various prefectures, which always exceeded the numbers of persons who migrated out (de facto B-type population ). Over time, through a process of learning by doing (habituation), the registration data became reasonably accurate .\n\n【8】For this study, we used data from the quinquennial (every 5 years) summations of 1898 and after. Given the de facto nature of the censuses of 1920 and after, we used the B-type population statistics for comparable data for 1918 and before . Because the population figures are based on repeated summations of records that were repeatedly updated, the count for a household was periodically revised upward or downward, and hitherto unreported births and deaths would have been more accurately captured by these revisions, even if they had not been reported in the annual vital registration records. Taeuber  provides evidence as follows: “Early publications of the Bureau of Statistics included a warning statement that the majority of the additions to the registers were the survivors of unrecorded births of earlier years,” and “Failures to report deaths during the earlier years are evident in the accumulations of the aged in the successive reports.” This phenomenon forms the basis for our reasoning that the quinquennial population count data are more accurate than the annual vital registration records.\n\n【9】The second data-associated issue is the change in the regime for population enumeration that began in 1920, when Japan conducted its first census of its de facto population. This census is widely regarded as having been accurate and yielded a population count of 55.96 million . After conducting the 1920 census, Japan conducted quinquennial censuses until the beginning of World War II. The problem with the timing of the change in the system is that quinquennial registration count totals are available up to 1918, and the quinquennial census counts started in 1920. Therefore, the break point in the system of population enumeration approximately coincides with the break point  for which population loss is to be estimated. Any statistical estimation of change across such a break point must satisfactorily address discontinuity in the data collection system. Fortunately, earlier demographers and statisticians went to great lengths to splice the data across this break point, producing similar estimates. Taeuber , for example, demonstrated that a backward projection of population from 1920 through 1898 produces a 1898 population estimate that is remarkably similar to a forward projection of population from 1871 through 1898. The theme of splicing is also covered in the works of Morita, Okazaki, and Yasukawa and Hirooka . Ohbuchi  compared the estimates of these authors, all of which are within 2% of each other for 1915 and 1920, and concluded that the estimates of Yasukawa and Hirooka, which are based on a reverse survival method, are the most reliable. Although the procedure used by Yasukawa and Hirooka is generally robust, its adjustment for the influenza pandemic fails because the official influenza death statistics of ≈178,000 for 1918–1920 were taken at face value and incorporated into estimates of life expectancy at birth . Therefore, inaccuracies in the official vital statistics of the period flow directly into the estimates of Yasukawa and Hirooka. When selecting the data for the analysis, therefore, we started with the observations of Yasukawa and Hirooka , who stated that by 1900, the most widely used population estimates of demographers  tend to converge and are close to the official population statistics. Next, because the official statistics are the only ones that contain published data at the prefectural level  (Morita, Okazaki, and Yasukawa and Hirooka  focus on producing Japan-wide data), we used the official statistics pertaining to the quinquennial population count (and before) and census years (and after).\n\n【10】The third data-related issue is the unreliability of the data before 1898 ; therefore, we used the 1898 population count as the starting point in our analysis. To maintain balance of the dataset across the break point of 1918, we also limited our data to the censuses including and before 1935. The full dataset consists of observations for each of 47 prefectures for the population count for the years 1898, 1903, 1908, 1913, and 1918, and the census data for 1920, 1925, 1930, and 1935, for a total of 423 observations.\n\n【11】##### Data Analyses\n\n【12】Although for decades scholars have been intrigued by the subject of low mortality rates from the pandemic in Japan, the currently circulating estimates were produced before the development and mainstreaming of panel data estimation methods. The studies described above based population estimates on annual or quinquennial observations for all of Japan and used datasets that were small in terms of numbers of observations. Given the existence of a panel of prefectural data on population for 47 prefectures and multiple time points straddling the pandemic years, more recently developed panel data methods can be used to estimate a standard population growth process that explicitly builds in a break point for the influenza pandemic . By treating these 47 prefectures of Japan as individual units, each with its own set of observations, the panel data method leverages the large amount of additional information available at the prefectural level to generate a more robust picture of population change and the effect of the influenza pandemic on that process. This method is also flexible enough, given the large sample size, to accommodate prefecture-specific variation. In this manner, the method enables estimation of prefecture-specific growth processes, each with a prefecture-specific estimate of population loss from the pandemic, while still leveraging the entire set of observations to create an aggregate estimate for Japan. This method is implemented by running a regression of the logarithm of population on a linear time trend while allowing for a 1-time (downward) shift in that time trend during 1918–19 to capture influenza-attributable population loss. Details of this method are provided in the Technical).\n\n【13】To examine the robustness of the estimates, we conducted a variety of sensitivity analyses. First, to control for the possible inaccuracy of the 1898 data and for the effects of outliers in time (the 1898 and 1935 data), we estimated models without these 2 time points. Second, we estimated models without the 4 prefectures that were most affected by the devastating Kanto earthquake of 1923: Chiba, Kanagawa, Shizuoka, and Tokyo. Third, because the 1918 population count was reported as of December of that year (i.e. the year of the pandemic), thereby introducing the possibility of contamination in the growth rate estimate for the prepandemic trajectory, we estimated models without the 1918 data. Finally, given the atypical population dynamic in Hokkaido, a frontier region in the early 20th century to which a large and prolonged wave of migration was in progress, we estimated models without data for Hokkaido. These sensitivity exercises yielded a total of 16 possible permutations of the model. Additional sensitivity analyses involved using the alternative A-type statistics  and dropping the data for 1898 (i.e. using 1903 as the earliest year) to account for the above-mentioned habituation process.\n\n【14】### Results\n\n【15】The only control that yielded distinct estimates conditional on its inclusion was the 1918 data control; the ranges of estimates for models that include the 1918 data (.38 to −1.61 million and −1.49 to −1.62 million) do not overlap with the ranges of estimates for models that exclude the 1918 data (.97 to −2.17 million and −1.98 to −2.05 million). Because other controls seem to have no material effect on the results, the final models selected are the ones in which the 1918 data are dropped but none of the other controls are implemented (i.e. the models in the second column of Tables 1 and 2 ). The estimated population loss is therefore 1.97 or 2.02 million persons, which translates to a drop in population of 3.62% or 3.71%.\n\n【16】### Discussion\n\n【17】For nearly a century, Japan’s experience during the influenza pandemic of 1918–19 has been viewed as an anomaly within the broader Asian experience. In stark contrast with significantly higher estimates for deaths in Asia and globally, which themselves are often conservative, the standing mortality rates for Japan, based heavily on vital registration data, are <1%. There is, however, substantial reason to believe that vital registration data for the early 20th century in the most densely populated parts of Asia, including British India , the Dutch East Indies , and Japan , are inaccurate, suggesting the need for verification of mortality rates by using the Davis method , which is based on population count or census data. The key result of this study is that when these alternative population counts and census data are used, the experience of Japan conforms more closely to that of the rest of Asia; in Japan, rates of population loss approach 4% and an actual loss of ≈2 million. These estimates are similar to those for India . This result has implications for the large bodies of work on the epidemiology of the influenza pandemic of 1918–19 and, more broadly, the demographic history of Japan. Even adjusting for the possibility that a brief decline in fertility partly explains the population loss estimated in this study, the number of deaths in Japan were in all probability much higher than previously believed.\n\n【18】The results of this study come from using an alternative data source rather than vital registration data. Although the alternative data source is vulnerable to any inaccuracies inherent in the population counts and censuses of Japan, it nevertheless provides a way to confirm or contradict prior results that were based on vital registration data in the manner of Davis  and Chandra et al. for India and Chandra  for Indonesia. Given the relatively reliable nature of population count and census data in comparison with vital registration data, however, the inaccuracies in the above analysis, in percentage terms, are probably smaller for population count and census data than for vital registration data.\n\n【19】A second possible limitation of the family of models estimated above is the implicit assumption of constant population growth rates for the periods before and after the pandemic. The analyses of Japanese demographers suggest some variation in birth and death rates during this period . Yet because we assumed stable population growth (derived from the differential between birth and death rates, with adjustment for migration), the models are tenable in view of the findings of these demographers of fairly stable population growth rates in Japan between 1900 and 1920 . The finding of population growth in the models  that lies within the range of earlier estimates is further cause for confidence in these models.\n\n【20】Statistical by-products of this study include the substantial upward revision of the toll of the pandemic and the information about annual population estimates for Japan. The higher number of deaths should affect worldwide estimates of deaths from the pandemic published in studies, such as those by Patterson and Pyle  and Johnson and Mueller , and estimates about the epidemiologic characteristics of the disease in Japan that depend on those data. The annual population estimates for Japan should advance the rich literature for Japan as a whole and for the 47 prefectures by generating new estimates that explicitly account for the effect of the pandemic. Although the estimates for years distant from the influenza pandemic are similar to those produced by demographers, including Morita, Okazaki, and Yasukawa and Hirooka, they depart substantially from these estimates for 1915–1920, with implications for the earlier published works that have used these data.\n\n【21】Given the virulence of the influenza A(H1N1) virus that caused the disease and the continued worry caused by the possibility of its reemergence , this study dispels the myth that Japan was spared the ravages of the influenza pandemic of 1918–19. Japan is not an exception to be studied for possible solutions or measures that might ameliorate the effects of such an epidemic in the future. Rather, its experience is typical of that of other Asian countries for which we have more reliable estimates.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71949e2b-95c0-45a1-bdee-1253be22d040", "title": "eHealth Literacy, Online Help-Seeking Behavior, and Willingness to Participate in mHealth Chronic Disease Research Among African Americans, Florida, 2014–2015", "text": "【0】eHealth Literacy, Online Help-Seeking Behavior, and Willingness to Participate in mHealth Chronic Disease Research Among African Americans, Florida, 2014–2015\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】The high rate of ownership of smartphones among African Americans provides researchers with opportunities to use digital technologies to reduce the prevalence of chronic diseases in this population. This study aimed to assess the association between eHealth literacy (EHL) and access to technology, health information–seeking behavior, and willingness to participate in mHealth (mobile health) research among African Americans.\n\n【3】**Methods**\n\n【4】A self-administered questionnaire was completed by 881 African American adults from April 2014 to January 2015 in north central Florida. EHL was assessed by using the eHealth Literacy Scale (eHEALS) with higher scores (range, 8–40) indicating greater perceived skills at using online health information to help solve health problems.\n\n【5】**Results**\n\n【6】Overall eHEALS scores ranged from 8 to 40, with a mean of 30.4 (standard deviation, 7.8). The highest score was for the item “I know how to find helpful health resources on the Internet,” and the lowest score was for “I can tell high quality from low quality health resources on the Internet.” Most respondents owned smartphones (%) and searched online for health information (%). Most were also willing to participate in health research that used text messages (%), smartwatches/health tracking devices (%), and health apps (%). We found significantly higher eHEALS scores among women, smartphone owners, those who use the Internet to seek health information, and those willing to participate in mHealth research (P_ < .01 for all).\n\n【7】**Conclusion**\n\n【8】Most participants owned smartphones, used the Internet as a source of information, and were willing to participate in mHealth research. Opportunities exist for improving EHL and conducting mHealth research among African Americans to reduce the prevalence of chronic diseases.\n\n【9】Introduction\n------------\n\n【10】Searching online for health information is an easy and affordable way for Americans to learn more about their health, self-diagnose an illness, and manage a health condition . Approximately 6 in 10 American adults search online for health information, and the trend is expected to increase as ownership of mobile devices grows and access to high-speed Internet expands . Knowing how to access and use credible online health information will allow patients to be more informed in medical decision making. This may ultimately impact health care costs, health outcomes, health care quality, and health equity .\n\n【11】Health literacy is a major public health concern and is one of 20 key areas identified to improve health outcomes and health care quality . African Americans have a disproportionately high prevalence of chronic diseases compared with other populations, and it is in the nation’s best interest to explore how new and emerging technologies can help to reduce these health disparities . One proposed way to reduce health disparities is to close the gap in health literacy and increase the use of health information technology to support patient self-management . Achievement of these objectives could also have an impact on eHealth literacy (EHL). EHL is “the ability to seek, find, understand, and appraise health information from electronic sources and apply the knowledge gained to addressing or solving a health problem” . The creation of an eHealth-literate population in an age of rapidly advancing technology should be a priority in American public health policy, research, practice, and education.\n\n【12】Despite persistent health disparities and the National Institutes of Health Revitalization Act of 1993, which mandated the inclusion of racial/ethnic minorities in all federally funded research , African Americans are underrepresented in eHealth research, clinical interventions, and clinical trials . Factors contributing to the underrepresentation of African Americans in research include institutional racism and historical mistrust of the health care system, research, and the government because of previous unethical research practices . Understanding EHL among African Americans and recruiting them into mobile health (mHealth) interventions are important goals for several reasons. African Americans 1) have a high prevalence of chronic diseases, 2) are the fastest adopters of home broadband Internet compared with other racial/ethnic groups, 3) have one of the highest rates of ownership of smartphones among racial/ethnic groups, and 4) have reported a willingness to participate in mHealth research . The objective of this study was to assess and examine the association between EHL and access to technology, health information–seeking behavior, and willingness to participate in mHealth research among African Americans.\n\n【13】Methods\n-------\n\n【14】A self-administered questionnaire was completed by 903 African Americans during a 9-month period from April 16, 2014, to January 15, 2015, in north central Florida. A convenience sample was recruited at various community events, and individuals were provided a $5 gift card for participation. This study was approved by the University of Florida institutional review board.\n\n【15】The questionnaire is described elsewhere . Questions were asked about sociodemographic characteristics (sex, age, marital status \\[married or not married\\], birthplace \\[in United States or not\\], education, employment status \\[employed or not\\], and home ownership \\[yes or no\\]), ownership and use of digital devices, health information–seeking behavior, willingness to participate in mHealth research, weight, and health status. Participants rated their overall health status on a scale from 1 to 5 (= excellent and 5 = poor). In addition, the questionnaire asked about sources of health information. EHL was assessed by using the eHealth Literacy Scale (eHEALS) , which is the most commonly used validated measure of EHL; eHEALS was validated with various population groups and translated into multiple languages . Each of the scale’s 8 items are rated on a 5-point Likert scale (= strongly disagree and 5 = strongly agree), and the overall score ranges from 8 to 40, with higher scores indicating greater perceived skills at finding, evaluating, and applying eHealth information to make health decisions. The internal consistency of the scale with our study sample was 0.96.\n\n【16】Of the 903 participants who completed the questionnaire, 881 (%) completed all 8 eHEALS items. These participants made up the final sample. Data were analyzed using JMP PRO version 12 (SAS Institute, Inc). Descriptive data (mean, standard deviation \\[SD\\], range, median, and interquartile range) were calculated to show the dispersion of the scores . Cutpoints have not been validated for the eHEALS, and scores cannot be categorized reliably . We used the Pearson χ 2  test, independent samples _t_ tests, and one-way analysis of variance (ANOVA) to analyze data. Posthoc comparisons of ANOVA were conducted by using the Tukey–Kramer honest significant difference test. Significance was established at _P_ ≤ .05 for all tests.\n\n【17】Results\n-------\n\n【18】The mean age of the study sample was 37.0 years (SD, 14.7 y), and ages ranged from 18 to 70 years. Of 881 participants, 579 (%) were female . Overall eHEALS scores ranged from 8 to 40, with a mean of 30.4 (SD, 7.8), median of 32, and interquartile range of 27 to 36. The mean scores for the 8 items ranged from 3.6 (SD, 1.2) to 4.0 (SD, 1.1). The highest mean score (.0) was for the items “I know how to find helpful health resources on the Internet” and “I know how to use the Internet to answer my health questions.” The lowest mean score (.6) was for the items “I can tell high quality from low quality health resources on the Internet” and “I feel confident using information from the Internet to make health decisions.” . With a total mean score of 30.8 (SD, 7.7), women had significantly higher scores than men (mean, 29.4; SD, 7.8; _t_ 877  \\= 7.00, _P_ \\= .008). Scores also varied significantly by age group, education level, and employment status (P_ < .001 for all) .\n\n【19】Most participants owned smartphones (%) or laptops (%). eHEALS scores were higher among device owners than among nonowners (P_ < .01 for both). Most (%) participants believed the Internet is useful for making health decisions. The Internet was accessed primarily from smartphones (%) and from computers at home (%) and at work or school (%). eHEALS scores were significantly higher among those who accessed the Internet from smartphones and from computers at home and at work or school than among those who did not (P_ < .01 for all).\n\n【20】Health was rated as excellent by 15% of study participants, very good by 34%, good by 35%, fair by 15%, and poor by 2%. eHEALS scores varied significantly by self-rated health status (F_ 4,872  \\= 11.49, _P_ < .001). In posthoc comparisons, those who rated their health as excellent or very good had significantly higher scores than those who rated their health as good, fair, or poor. Those who rated their health as very good or good had significantly higher scores than those who rated their health as fair or poor.\n\n| Self-Rated Health Status | eHEALS Score, Mean (SD) |\n| --- | --- |\n| Excellent | 31.4 (.6) |\n| Very good | 32.1 (.8) |\n| Good | 29.8 (.8) |\n| Fair | 27.3 (.8) |\n| Poor | 26.0 (.8) |\n\n【22】No other significant differences were found. Most (%) participants reported having had a physical examination by a physician within the previous 12 months: these participants had significantly higher eHEALS scores than those who had not had an examination (mean, 31.0; SD, 7.7 vs mean, 28.4; SD, 7.7; _t_ 879  \\= 17.47, _P_ < .001).\n\n【23】Health information was obtained from various sources: 62% of participants obtained health information from physicians, 60% from the Internet, and 40% from television . eHEALS scores were significantly higher among those who cited the Internet as a source of health information than among those who did not cite this source, among those who cited nurses than among those who did not cite this source, and those who cited books, radio, or news apps as sources of information than among those who did not cite those sources.\n\n【24】In the previous 12 months, participants reported Internet searches on the following topics: health and wellness (% of participants), nutrition/dieting (%), medication use (%), diabetes (%), stress/anxiety/depression (%), children’s health (%), heart disease (%), cancer (%), sexually transmitted infections (%), tobacco/alcohol/drugs (%), asthma (%), and human immunodeficiency virus/AIDS (%). Participants who searched online for these topics had significantly higher eHEALS scores than those who did not (P_ < .01 for all) .\n\n【25】Most (%) participants reported they would be willing to participate in mHealth research interventions that send educational text messages. Those who were willing to participate in mHealth research had significantly higher eHEALS scores than those who were not willing (P_ < .001). Women were significantly more willing to participate in mHealth research than men (odds ratio \\[OR\\], 1.4; 95% confidence interval \\[CI\\], 1.05–1.89; _P_ \\= .02).\n\n【26】Many (%) participants reported having downloaded a nutrition/health/fitness app in the previous 30 days. Those who had downloaded an app had significantly higher eHEALS scores than those who had not (mean score, 32.6 \\[SD, 6.3\\] vs mean score, 28.9 \\[SD, 8.2\\]; _t_ 879  \\= 50.73, _P_ < .001). Women were significantly more likely than men to have downloaded an app (OR, 1.61; 95% CI, 1.21–2.16; _P_ \\= .001). Participants reported willingness to participate in research that used smartwatches/health tracking devices (% of participants), health apps (%), online data entry (%), or online forums/support groups/counseling (%). Women were significantly more likely than men to report they would be willing to participate in research that asked them to wear a smartwatch/health tracking device (OR, 1.45; 95% CI, 1.09–1.93; _P_ \\= .01), enter data online (OR, 1.75; 95% CI, 1.31–2.33; _P_ < .001), or participate in online forums/support groups/counseling (OR, 1.69, 95%; CI, 1.22–2.33; _P_ \\= .002). eHEALS scores were significantly different between those who were willing to participate in research that asked them to wear a smartwatch/health tracking device (t_ 879  \\= 7.71; _P_ \\= .006), download a health app (t_ 879  \\= 24.29; _P_ < .001), or enter data online (t_ 879  \\=18.06; _P_ < .001) compared with those who were not willing. We found no significant differences in eHEALS scores for those willing to participate in research that used online forums/support groups/counseling and those not willing .\n\n【27】Discussion\n----------\n\n【28】Understanding the association between EHL, access to technology, health information–seeking behavior, and willingness to participate in mHealth research is an important step in creating mHealth messages, programs, and interventions to prevent and manage chronic diseases among African Americans. The percentage of study participants who owned a smartphone (%) was higher than the percentage of the general population (%) who own one . Study participants also used smartphones as their primary Internet access, which allows them to check signs or symptoms of health conditions, find free information on nutrition, and self-monitor weight, physical activity, and other health-related factors .\n\n【29】Most participants had eHEALS scores above the mean. However, the study found low scores on 2 items: the ability to differentiate high-quality from low-quality health resources on the Internet and confidence in using information from the Internet to make health decisions. These low scores are a key finding because participants reported searching online for information on many health topics. The inability to differentiate between credible and noncredible online health information and sites can make patients vulnerable to exploitation . Furthermore, low confidence in using online information to make health decisions suggests that credible Web portals and apps need to develop algorithms to help personalize information for users with various education and literacy levels .\n\n【30】Physicians and the Internet were the primary sources of health information used by participants with no significant differences in eHEALS scores, but surprisingly those who reported nurses as a source of health information had significantly higher eHEALS scores than those who did not. Nurses may be perceived as having more time to answer questions and as having better interpersonal communication skills than physicians . Because patients often use online information to manage their health condition , practitioners should consider providing patients with a list of credible sites that can answer questions and concerns. Such a list could also enhance patients’ confidence and improve face-to-face interaction with practitioners .\n\n【31】As expected, eHEALS scores varied by educational level. Although people with low income and low educational levels are less likely to use the Internet than people with high income and high educational levels, they are more likely to search for health information than any other topic when they do use the Internet . Women also had significantly higher eHEALS scores than men and expressed a greater willingness to participate in mHealth research in general and to participate in research that asked them to download health apps or wear tracking devices. These findings are consistent with the findings of other studies that show African American women express greater interest in health issues than African American men . Thus, as gatekeepers to the home and a major source of informal health information for the men in their lives, African American women should be considered as primary and secondary targets for mHealth interventions for various health conditions .\n\n【32】This study has several limitations. First, it used a convenience sample, which limits the generalizability of the findings to African Americans in north central Florida. Second, survey data are subject to social desirability bias. Third, only a few instruments exist to measure EHL, and a different survey instrument could have given different results. eHEALS is not a clinical diagnostic tool, and scores should be interpreted cautiously because appropriate cutpoints have not been validated . However, the scale is the most common measure of EHL, and it was validated among various population groups and translated into multiple languages . Furthermore, eHEALS has the potential to identify people who may benefit from using online resources and screening people who may benefit from mHealth interventions .\n\n【33】We found higher eHEALS scores among women, smartphone owners, those who used the Internet as a source of health information, and those willing to participate in research interventions that send educational text messages. These findings may be beneficial to practitioners, researchers, and program planners as they explore new strategies for developing, tailoring, and delivering online health information and interventions to prevent chronic diseases among African Americans. Further research is needed to develop new EHL assessment tools or improve existing ones for use in clinical settings and mHealth interventions. Additionally, research is needed in developing, implementing, and evaluating the content, format, and usefulness of online health information for low-income African American and those with low levels of literacy.\n\n【34】Tables\n------\n\n【35】#####  Table 1. Mean Scores for eHealth Literacy Scale a  Among African American Study Participants (N = 881), by Sociodemographic Characteristic, Florida, 2014–2015\n\n| Characteristic | No. (%) | Score, Mean (SD) | _t_ Statistic or _F_ Statistic (P_ Value) |\n| --- | --- | --- | --- |\n| **Sex** b | **Sex** b | **Sex** b | **Sex** b |\n| Male | 300  | 29.4 (.8) | 7.00 (.008) |\n| Female | 579  | 30.8 (.7) | 7.00 (.008) |\n| **Age group c** | **Age group c** | **Age group c** | **Age group c** |\n| 18–29 | 357  | 31.4 (.6) d | 28.06 (<.001) |\n| 30–50 | 316  | 31.4 (.2) d | 28.06 (<.001) |\n| ≥51 | 204  | 26.9 (.4) e | 28.06 (<.001) |\n| **Marital status** b | **Marital status** b | **Marital status** b | **Marital status** b |\n| Married | 283  | 29.6 (.6) | 1.92 (.06) |\n| Not married | 594  | 30.72 (.4) | 1.92 (.06) |\n| **Born in the United States** b | **Born in the United States** b | **Born in the United States** b | **Born in the United States** b |\n| Yes | 783  | 30.2 (.8) | 1.62 (.11) |\n| No | 96  | 31.6 (.8) | 1.62 (.11) |\n| **Education c** |  |  |  |\n| <High school | 76  | 25.0 (.4) f | 20.25 (<.001) |\n| High school | 221  | 28.4 (.8) e | 20.25 (<.001) |\n| College credits | 329  | 31.4 (.4) d | 20.25 (<.001) |\n| 4-year degree | 129  | 32.6 (.0) d | 20.25 (<.001) |\n| Graduate degree | 121  | 32.4 (.8) d | 20.25 (<.001) |\n| **Employment** b | **Employment** b | **Employment** b | **Employment** b |\n| Employed | 548  | 31.4 (.8) | 5.31 (<.001) |\n| Unemployed | 330  | 28.6 (.9) | 5.31 (<.001) |\n| **Home ownership** b | **Home ownership** b | **Home ownership** b | **Home ownership** b |\n| Yes | 255  | 30.5 (.8) | 0.15 (.88) |\n| No | 624  | 30.4 (.7) | 0.15 (.88) |\n\n【37】Abbreviation: SD, standard deviation.  \na  Each of the 8 items were rated on a 5-point Likert scale, with 1 = strongly disagree and 5 = strongly agree. Overall eHealth Literacy Scale score ranges from 8 to 40.  \nb  _t_ test performed.  \nc  _F_ test performed.  \nd,e,f  Posthoc comparisons were conducted to determine significant differences between categories; categories that do not have matching superscripted letters are significantly different.\n\n【38】#####  Table 2. Mean Scores for Survey Items in eHealth Literacy Scale a  , African American Study Participants (N = 881), Florida, 2014–2015\n\n| Survey Item | Mean Score (Standard Deviation) |\n| --- | --- |\n| I know how to find helpful health resources on the Internet. | 4.0 (.1) |\n| I know how to use the Internet to answer my health questions. | 4.0 (.1) |\n| I know what health resources are available on the Internet. | 3.7 (.1) |\n| I know where to find helpful health resources on the Internet. | 3.8 (.1) |\n| I know how to use the health information I find on the Internet to help me. | 3.9 (.1) |\n| I have the skills I need to evaluate the health resources I find on the Internet. | 3.7 (.0) |\n| I can tell high quality from low quality health resources on the Internet. | 3.6 (.2) |\n| I feel confident using information from the Internet to make health decisions. | 3.6 (.1) |\n| Mean overall score | 30.4 (.8) |\n\n【40】a  Each of the 8 items were rated on a 5-point Likert scale, with 1 = strongly disagree and 5 = strongly agree. Overall eHealth Literacy Scale score ranges from 8 to 40.\n\n【41】#####  Table 3. Mean Scores for eHealth Literacy Scale a  Among African American Study Participants (N = 881), by Source of Health Information Used, Florida, 2014–2015\n\n| Source of Health Information | No. (%) | Score, Mean (SD) | _t_ 879 (P_ Value) |\n| --- | --- | --- | --- |\n| **Physicians** | **Physicians** | **Physicians** | **Physicians** |\n| Yes | 542  | 30.5 (.5) | 0.59 (.44) |\n| No | 339  | 30.1 (.2) | 0.59 (.44) |\n| **Internet** | **Internet** | **Internet** | **Internet** |\n| Yes | 529  | 32.3 (.1) | 85.97 (<.001) |\n| No | 352  | 27.5 (.0) | 85.97 (<.001) |\n| **Television** | **Television** | **Television** | **Television** |\n| Yes | 353  | 30.1 (.6) | 0.80 (.37) |\n| No | 528  | 30.6 (.9) | 0.80 (.37) |\n| **Nurses** | **Nurses** | **Nurses** | **Nurses** |\n| Yes | 324  | 31.1 (.3) | 4.53 (.03) |\n| No | 557  | 29.9 (.0) | 4.53 (.03) |\n| **Books** | **Books** | **Books** | **Books** |\n| Yes | 290  | 31.3 (.0) | 6.09 (.01) |\n| No | 591  | 29.9 (.1) | 6.09 (.01) |\n| **Friends** | **Friends** | **Friends** | **Friends** |\n| Yes | 262  | 30.7 (.6) | 0.83 (.36) |\n| No | 619  | 30.2 (.2) | 0.83 (.36) |\n| **Magazines** | **Magazines** | **Magazines** | **Magazines** |\n| Yes | 199  | 31.0 (.9) | 1.58 (.21) |\n| No | 682  | 30.2 (.0) | 1.58 (.21) |\n| **Newspapers** | **Newspapers** | **Newspapers** | **Newspapers** |\n| Yes | 153  | 31.0 (.2) | 1.17 (.28) |\n| No | 728  | 30.2 (.9) | 1.17 (.28) |\n| **Radio** |  |  |  |\n| Yes | 126  | 31.9 (.0) | 5.42 (.02) |\n| No | 755  | 30.1 (.9) | 5.42 (.02) |\n| **News apps on smartphones** | **News apps on smartphones** | **News apps on smartphones** | **News apps on smartphones** |\n| Yes | 113  | 32.3 (.2) | 7.76 (.006) |\n| No | 768  | 30.1 (.0) | 7.76 (.006) |\n| **Spouse or partner** | **Spouse or partner** | **Spouse or partner** | **Spouse or partner** |\n| Yes | 87  | 31.0 (.5) | 0.68 (.41) |\n| No | 794  | 30.3 (.8) | 0.68 (.41) |\n\n【43】Abbreviation: SD, standard deviation.  \na  Each of the 8 items in eHealth Literacy Scale were rated on a 5-point Likert scale, with 1 = strongly disagree and 5 = strongly agree. Overall eHealth Literacy Scale score ranges from 8 to 40.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0323630e-91be-48d8-a4f8-4ace19d67f6b", "title": "Delays in Coccidioidomycosis Diagnosis and Relationship to Healthcare Utilization, Phoenix, Arizona, USA", "text": "【0】Delays in Coccidioidomycosis Diagnosis and Relationship to Healthcare Utilization, Phoenix, Arizona, USA\nCoccidioidomycosis (also known as Valley fever) is an endemic fungal infection . In Arizona, USA, it is responsible for one quarter of all community-acquired pneumonia (CAP) . Although accurate diagnosis requires specific laboratory tests , such testing was only ordered in < 13% of patients with CAP in Phoenix, Arizona , raising the possibility that delays in accurate diagnosis might be extensive. We report a retrospective analysis from the coccidioidomycosis-endemic region of Phoenix to estimate diagnostic delays and healthcare utilization before and after a coccidioidal diagnosis was confirmed.\n\n【1】### The Study\n\n【2】This study focused on cases recorded at clinics operated by Banner Health in the Phoenix area, using programmatic use of data from NextGen . This electronic medical record system was used during the study period by most of Banner Health’s clinics but not its hospitals. We programmatically searched the system for all patients \\> 18 years of age who were seen from January 1, 2011, through December 31, 2014, whose records showed codes from the International Classification of Diseases, 9th Revision (ICD-9), that indicated coccidioidomycosis (.\\*). The earliest diagnosis of coccidioidomycosis served as the diagnosis date for each patient. We excluded patients with a coccidioidomycosis diagnosis in 2011 who also had a coccidioidomycosis diagnosis in the prior 12 months. We corroborated all diagnoses by a programmatic identification of any positive coccidioidal serologic test result  <30 days before or >60 days after the date of diagnosis.\n\n【3】To estimate the date when patients first sought medical attention for their illness, we used a set of ICD-9 codes that represented an expansion of codes previously associated with early coccidioidal illnesses that were used in a Phase I independent chart review of 37 patients performed by 2 physicians in training and an internist . For our study, an internist and a medical student independently reviewed all ICD-9 codes occurring 6 months before the coccidioidomycosis diagnosis and included only those that they both judged to be similar to the original set from the previous Phase I study. An infectious disease specialist settled cases of nonconcordance. We removed 3 Phase I symptoms that were deemed erroneous. This process resulted in a total of 121 ICD-9 codes . We defined the date of first presentation (i.e. the date signs and symptoms were first clinically noted) as the earliest record in which \\> 1 of the expanded ICD-9 codes were recorded, within 6 months before the diagnosis date. We calculated the delay in diagnosis as the difference between the date of first presentation and the diagnosis date.\n\n【4】Healthcare charges represented total healthcare utilization. Billing data were available for ambulatory care from NextGen and for Banner Health’s hospitalizations from MedSeries4 . We defined healthcare utilization as the total charges, for any cause, from the date of first visit to 6 months after the diagnosis date. We distinguished total charges from actual reimbursement amount because the total is only 1 aspect of a complicated financial billing system.\n\n【5】We used the SAS Enterprise Guide Software Suite version 7.13  for data retrieval and analysis. The Banner Health Institutional Review Board approved this study.\n\n【6】We identified newly diagnosed coccidioidal infections in 139 patients. The mean patient age was 49.2 years (SD \\+ 15.3 years) , and 52% of patients were female.\n\n【7】The 3 most frequently occurring coccidioidal ICD-9 codes were unspecified (.9, 47%), primary pulmonary (.0, 37%), and pulmonary unspecified (.5, 8%) . Confirmatory serologic tests were recorded within 15 days before or after the date of diagnosis in 81% of patients .\n\n【8】The expanded set of ICD-9 codes failed to identify an initial presentation date for 20 patients. Of the 119 patients with an identified presentation date, 16 (%) received a diagnosis on the day of presentation, 24% within 1 week of first presentation, 46% within 1 month, and 54% from 30 days to 6 months after presentation .\n\n【9】We queried total charges for all 139 patients from the date of first visit (the coccidioidomycosis diagnosis date, if no initial presentation date was available) until 6 months after the diagnosis date. By cross-referencing patient identifiers, we found 66 (%) patients to have hospitalization billings, which were included in the total healthcare cost. Median healthcare charges seemed to increase as diagnosis delays increased . When we compared diagnosis delay to the log-normal of total charges, we found a significant, positive, linear relationship for overall charges (R 2  \\= 0.07, p = 0.003), attributable to those occurring before (R 2  \\= 0.16, p < 0.0001) but not to those on or after the diagnosis date (R 2  \\= 0.01, p = 0.2748). Although we saw issues with data normality because of our small sample size, we assumed normality for linear regression analysis .\n\n【10】### Conclusions\n\n【11】For this retrospective study, we developed a set of ICD-9 codes to use as a surrogate indicator for the date coccidioidomycosis patients were first seen within Banner Health clinics for signs or symptoms of their infection. This list expanded on an initial set of codes discovered through a manual chart review . Many of the codes are common and are unlikely themselves to be predictive of diagnosis. Previous studies have been unable to meaningfully distinguish presenting syndromes of coccidioidomycosis in patients from other overlapping etiologies . However, this code set was useful to computationally establish the date of first healthcare contact for these patients. If this analytic method is validated and perhaps improved in future studies, it could allow for automated monitoring of diagnostic delay as a metric of quality assessment, without the need for expert chart review.\n\n【12】Of the 139 patients analyzed in this study, 20 did not have an eligible ICD-9 code, and 16 others had their initial presentation code recorded on the same day as their coccidioidomycosis diagnosis. Factors leading to these problems might include failure of a clinician to record sufficient diagnostic information or use of codes that are not sufficiently comprehensive. Future studies could manually examine the full patient records to further expand the code set. It is also possible that patients entered the Banner Health electronic system with a diagnosis of coccidioidomycosis from elsewhere or that the coccidioidal infection was truly without symptoms.\n\n【13】This investigation identified a prolonged diagnosis delay for much of the population; 46% had a delay >1 month . With increased delays, total healthcare charges increased substantially. However, this study was not limited to coccidioidomycosis-related charges; any diagnosis delay could result in increased healthcare utilization, regardless of relationship to coccidioidomycosis. In addition, data gaps may have occurred because of difficulty in cross-referencing identifiers or visits by patients to a non-Banner facility. However, our findings are consistent with the simple concept that diagnostic delays in a protracted coccidioidal illness would result in additional testing, visits, and possible use of empiric antibacterial treatments until a precise diagnosis is achieved.\n\n【14】Our study investigates only patients with confirmed coccidioidomycosis and not patients who never received a correct diagnosis. Previous studies indicate that appropriate testing for coccidioidomycosis is often not done in Phoenix and Tucson . Thus, the costs of delayed coccidioidal diagnosis probably underestimates the situation for this clinical practice within a coccidioidomycosis-endemic region. A recent study of coccidioidomycosis diagnosed outside endemic regions also showed substantial delays , making our research relevant to those with a recent travel history to endemic regions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "76744747-6ce5-42f6-83a6-2abe76ee40f6", "title": "Effect of Culture-Independent Diagnostic Tests on Future Emerging Infections Program Surveillance", "text": "【0】Effect of Culture-Independent Diagnostic Tests on Future Emerging Infections Program Surveillance\nThe Centers for Disease Control and Prevention (CDC) Emerging Infections Program (EIP) network conducts population- and laboratory-based surveillance for foodborne, health care–associated, respiratory, and invasive bacterial pathogens of public health importance. The main objectives of surveillance are to 1) measure disease burden and monitor disease trends over time, 2) evaluate the impact of public health interventions, 3) track microbiological and molecular characteristics of pathogens, and 4) detect emerging infectious disease threats. EIP data are used for national projections of disease incidence and formulation of national public health policy for prevention and control of disease. Central to accomplishing these objectives is accurate laboratory detection of the pathogens under surveillance.\n\n【1】In the field of microbiology, culture remains the standard for detection of most organisms, but in clinical settings, detection of pathogens is increasingly reliant on culture-independent diagnostic tests (CIDTs). CIDTs include antigen-based tests and molecular tests. The most commonly used molecular tests are the nucleic acid amplification tests, which include PCR. In clinical settings, most CIDTs have several advantages over culture. Foremost, CIDT results can be obtained more rapidly than culture, a feature that can be critical for clinical decision-making. Additionally, CIDTs may require less technical expertise to perform. Although initial adoption of these newer technologies can be expensive, costs generally decline over time, particularly those associated with labor.\n\n【2】CIDTs have the potential to improve estimates of disease burden because 1) they may be more sensitive than culture, 2) their relative ease of use may increase the number of patients tested, 3) they may enable detection of organisms for which there are currently no practical laboratory tests, and 4) they may increase the ability to detect polymicrobial infections. However, incorporating CIDTs into public health surveillance presents several challenges. Interpreting trends in disease incidence can be difficult because of changes to testing practices and surveillance case definitions. Although also true for culture, detection of molecular material may not reflect the presence of a living microbe and true disease, especially when detected from nonsterile body sites. At least for now, it is generally more difficult to assess microbiological and molecular characteristics, such as pathogen subtypes and antimicrobial drug resistance and genotypes, without bacterial isolates. Addressing these and other factors that affect estimates of disease burden and the characterization of infectious pathogens is critical for public health surveillance systems and clinical decision-making. EIP sites have a long history of close collaboration between CDC, state and local public health departments, academia, and clinical laboratories, making them uniquely positioned to help chart the course in addressing these concerns. Because many infections are already being diagnosed by use of CIDTs and more CIDTs will probably be developed and used in the near future, a path for addressing these issues is urgently needed. This article provides an overview of current testing practices for pathogens under EIP surveillance and addresses how EIPs plan to advance their core objectives in the face of this dynamic diagnostic environment.\n\n【3】### Current Status of CIDTs in the EIP Network\n\n【4】CIDTs are either singleplex (i.e. they test for a single organism) or multiplex (i.e. they simultaneously test for multiple organisms). There has been rapid development of multiplex molecular tests that detect pathogens commonly associated with particular syndromes (e.g. respiratory, enteric, and bloodstream infections). CIDTs can be classified into commercial test kits that receive Food and Drug Administration (FDA) clearance or laboratory-developed tests (LDTs). FDA-cleared CIDTs undergo various levels of validation before they are made available for purchase in the United States, but postmarketing evaluations are generally not required . FDA defines LDTs as “in vitro diagnostic tests that are designed, manufactured, and used within a single laboratory.” Laboratories are required to establish test characteristics for LDTs, including accuracy and precision. Historically, FDA has not generally enforced premarket review and other applicable requirements because LDTs were relatively simple and available on a limited basis. Many LDTs are now more complex and are used nationwide. FDA guidance on additional oversight of LDTs is pending.\n\n【5】Many EIPs regularly conduct systematic surveys of clinical, commercial, and public health laboratories to monitor the use of CIDTs in laboratories that provide services to the population under surveillance. These surveys show that the availability and type of CIDTs used varies by pathogen . All or nearly all cases of influenza, _Clostridium difficile, Legionella_ spp. and _Bordetella pertussis_ infection reported through EIP are diagnosed by CIDTs. The percentage diagnosed by a particular type of CIDT has varied over the years. For instance, rapid antigen tests for influenza have been increasingly replaced by FDA-cleared molecular assays , including multiplex assays to detect viruses and bacteria from respiratory specimens  _._ Molecular tests are increasingly being used to detect _C. difficile_ infection. During 2011, ≈50% of _C. difficile_ infections were diagnosed by molecular assays performed at laboratories that serve the EIP population  _._ Also in 2011, for surveillance of _Legionella_ infections, 95% of cases were diagnosed by detection of urine antigen for _L. pneumophila_ serogroup 1 . During the early 1990s, culture and direct fluorescent antibody testing were the primary diagnostic methods used to identify _B. pertussis_ cases reported through the National Notifiable Disease Surveillance System . PCR, either alone or in combination with other diagnostic tests, diagnosed 89% of laboratory-confirmed pertussis cases reported through the EIP Enhanced Pertussis Surveillance system during 2011–2014 (CDC, unpub. data).\n\n【6】Culture remains the mainstay for diagnosis of invasive bacterial and fungal infections that cause predominantly bloodstream infections and meningitis, which are covered under EIP Active Bacterial Core surveillance (ABCs) and Healthcare-Associated Infections Community Interface programs . For these pathogens, fulfillment of the surveillance case definition still requires their isolation from a sterile site. Some FDA-cleared multiplex molecular tests for bacterial and fungal bloodstream pathogens are not truly culture independent because they require a positive blood culture from which an organism is identified by PCR  _._ In 2014, ≈10% of laboratories that participate in ABCs used one of these platforms to identify species from positive blood cultures (CDC, unpub. data). There are no FDA-cleared molecular tests for directly detecting bacteria from sterile site specimens (e.g. whole blood, cerebrospinal fluid \\[CSF\\]), but there are molecular LDTs that are used to directly detect bacterial pathogens from sterile sites. Less than 1% of ABCs laboratories offer these tests for at least 1 of the ABCs pathogens (CDC, unpub. data). There is an FDA-cleared molecular test to detect _Candida_ spp. directly from whole blood  _,_ but this test does not seem to be widely used by clinical laboratories . Nonetheless, multiplex PCR-based tests that detect organisms directly from blood and CSF are under development and may soon become more widely available in clinical settings .\n\n【7】For most pathogens covered under the surveillance system for foodborne pathogens , culture remains the primary means of diagnosis, but this predominance is changing . Antigen-based tests and molecular tests for _Campylobacter_ and Shiga toxin–producing _Escherichia coli_ have been increasingly adopted by EIP laboratories. Adding positive reports from CIDTs for _Campylobacter_ and Shiga-producing _E. coli_ results that are not culture confirmed could add an additional ≈13% and ≈8% cases to FoodNet surveillance, respectively . FDA recently cleared several molecular enteric syndrome panels , which are being rapidly adopted  _._\n\n【8】### Measuring Disease Burden Trends and Evaluating Public Health Interventions\n\n【9】To assess trends and the effect of population-based interventions over time, methods for measuring disease burden should remain relatively stable or adjustments should be made to account for changes in the use of diagnostic tests. The stability of disease burden estimates will be affected by differences in the performance characteristics of tests used, changes in clinical testing practices, and changes to case definitions.\n\n【10】##### Performance Characteristics and Use of Diagnostic Tests\n\n【11】Accurate tests give positive results when infection is present (i.e. the tests are sensitive) and negative results when infection is absent (i.e. the tests are specific). The predictive value of tests depends, in part, on the prevalence of infection in the population and on whether the organism may be present in the absence of disease (i.e. colonizing body sites). Molecular tests for influenza viruses, _C. difficile_ , and _B. pertussis_ have been found to be highly sensitive in clinical settings . The sensitivity of molecular tests for bacteria may be better than that for culture, particularly when antimicrobial drugs have been administered before specimen collection . Highly sensitive molecular tests may produce false-positive results, however, as has been shown in pseudo outbreaks of _B. pertussis_  _._ Molecular mutations in the organism may result in decreased sensitivity for antigen-based tests  and molecular tests . The specificity of CIDTs may be lower than that of culture because molecular targets may be nonspecific for the species of interest . The influenza and _C. difficile_ infection surveillance systems collect data on test method used and adjust national disease estimates on the basis of the sensitivity of the different test types .\n\n【12】The availability of tests; their speed, cost, and ease of use; and other factors (e.g. changes in testing guidelines) may result in changes to clinical testing practices, which may affect disease burden trends. These changes may especially be true for pathogens detected by multiplex panels for which clinical suspicion for an organism does not have to be as high as that for a specific organism. If more persons are tested for multiple organisms, more pathogens might be detected. To account for these potential changes, EIP influenza surveillance periodically collects data on the proportion of patients who are tested for influenza if they have an influenza-like illness and adjusts disease burden estimates on the basis of this information .\n\n【13】##### Case Definitions\n\n【14】The case definitions for EIP pathogens include specific requirements for the laboratory methods used and, for some pathogens, the site from which specimens were obtained . One consideration is whether clinical symptoms should be included in case definitions because detection of an organism may indicate asymptomatic carriage and not true disease . This consideration may especially be relevant for organisms that are detected after sample collection from nonsterile sites and that are known to colonize body sites. In EIP, the only activity that includes clinical symptoms as part of the surveillance case definition is Enhanced Pertussis Surveillance. Another consideration may be collection of specimens from negative controls to determine the likelihood of true infection.\n\n【15】In general, EIP case definitions have been characterized by high specificity and high positive clinical predictive value because most rely on culture from a normally sterile body site. Culturing of samples collected from nonsterile sites (e.g. stool samples) may also be more specific than testing for molecular material. EIP decisions about when and how to change case definitions will probably be specific for each pathogen. Advances in the quality of PCR diagnostics led the Council of State and Territorial Epidemiologists to include validated PCR results obtained from sterile site specimens in the Nationally Notifiable Disease Surveillance System for _Haemophilus influenzae_  and _Neisseria meningitidis_ starting in 2015 . Similarly, campylobacteriosis became nationally notifiable in 2015, and detection of _Campylobacter_ spp. by use of any CIDT would be classified as a “probable” case . Like the Council of State and Territorial Epidemiologists, EIP will need to consider what constitutes a valid test. FDA clearance may be a consideration, but FDA-cleared tests may not perform well in real-world clinical settings. LDTs may undergo rigorous validation that may justify including results from those tests. Presenting incidence data stratified by laboratory method (culture-confirmed and positive CIDT reports), as has been done for FoodNet, may be one way to highlight changes to case definitions .\n\n【16】### Detecting Other Emerging Pathogens\n\n【17】Increased availability and use of CIDTs may increase detection of certain pathogens that were previously hard to identify by culture (particularly those that are part of multiplex panels) or of bacterial pathogens that would otherwise be suppressed by antimicrobial drugs. This increased use may provide the opportunity to conduct surveillance for organisms for which the burden of disease may not have been measured or recognized as emerging infections (e.g. _Mycoplasma pneumoniae,_ respiratory syncytial virus, human metapneumovirus, enteroviruses, enterotoxigenic _E. coli_ ). Additionally, the detection of multiple infectious organisms by multiplex panels could provide insight into polymicrobial interactions and their effect on disease manifestations and severity.\n\n【18】### Analyzing Microbiological and Molecular Characteristics\n\n【19】One of the characteristics that has made EIP surveillance so useful for public health action has been the systematic collection of isolates that enable microbiological and molecular characterization. Serotyping and serogrouping data have been used for developing and evaluating vaccines and for measuring the effectiveness of prevention programs . Isolates collected through EIP have been used to identify outbreaks , monitor and raise awareness of the problem of antimicrobial drug resistance , identify mechanisms of resistance , detect the emergence of new strains  or mutations that may reduce vaccine effectiveness , and identify virulence factors . These isolates have been deposited in national repositories, and streptococcal isolates are widely available to the research communities .\n\n【20】Collection of isolates has been critical for strain characterization by serologic techniques and for in vitro determination of antimicrobial drug susceptibility in EIP reference laboratories. Over time, there has been a shift toward using molecular techniques for strain typing, but both typing and susceptibility testing still rely heavily on the availability of isolates. Although molecular techniques can identify genetic mutations that correlate with phenotypic antimicrobial drug resistance, new mutations may convey the emergence of phenotypes that are not apparent today. Through the CDC Advanced Molecular Detection initiative, EIPs have recently started whole-genome sequencing of EIP isolates . Whole-genome sequencing will be used for pathogen characterization for general surveillance and outbreak detection and for exploring genetic determinants of antimicrobial drug resistance, disease severity, and vaccine failure. Some molecular characterization has been performed directly for _N. meningitis_ in blood and CSF specimens and for _B. pertussis_ in respiratory tract specimens _._ For better characterization of strains without the use of culture, clinical specimens are now collected thorough EIP Enhanced Pertussis Surveillance, as will probably be done for other EIP pathogens in the future. However, much additional research is needed to determine whether and which molecular characteristics can be identified directly from clinical specimens. In the interim, collection of isolates remains essential, as demonstrated by the experience with the _C. difficile_ epidemic in the early 2000s, when CIDT use was widespread for this infection and the emergence of the NAP1 strain was not detected until 5 years after steady increases in _C. difficile_ incidence and severity .\n\n【21】### Future Considerations and Directions\n\n【22】To continue to impact public health programs and policies, EIPs will have to be forward-thinking in how disease burden trends are measured in light of the continued development and uptake of CIDTs . First, EIPs need to continue to systematically monitor the availability and use of these tests through periodic laboratory surveys, either within the EIP network or through coordination with outside organizations and to measure their use in clinical settings. Understanding the use of tests outside of EIP laboratories may also be relevant because some EIPs project estimates of national disease burden. EIPs will also need to develop and regularly evaluate criteria for incorporating CIDTs into case definitions, which will probably vary by pathogen. EIPs can and should contribute to the national discussion about changing case definitions for reportable diseases. Confirmatory testing at public health laboratories may also be necessary for pathogens detected by CIDTs that have questionable performance in real-world settings; however, doing so would require additional public health resources. Performance characteristics need to be determined on an ongoing basis because new variants of organisms that are not detected by the tests may arise. As is already being done for some EIP pathogens, data collection at EIP sites would need to expand to capture information on specific test types and to allow for the reporting of multiple test results. In the era of electronic laboratory reporting, the use of standard test codes that can be transmitted electronically will be essential, and data systems must be able to accommodate more complex data. It will also be critical to perform system checks to avoid counting cases multiple times because >1 testing method may be used for 1 patient. The type of test and the sensitivity and specificity of individual tests and adjustments for changes in testing practices could potentially be incorporated into incidence calculations. After CIDTs have been incorporated into case definitions, EIPs will need to highlight these changes and may consider reporting disease burden by testing method (e.g. cases by culture and molecular testing).\n\n【23】Because CIDTs may obviate the need for culture for making a clinical diagnosis, EIPs must consider short- and long-term strategies for assuring the continued availability of isolates. Isolates remain critical for molecular characterization and antimicrobial drug resistance testing. Resources or legal/regulatory approaches may be needed to give clinical or public health laboratories incentives to continue culturing specimens. It is unlikely that clinical laboratories will be paid by insurers for culture in addition to CIDTs. If providing resources to all laboratories is not possible, the EIP network may have a role in providing sentinel sites for collection of isolates. EIP may also have a role in the development and validation of culture-independent methods for serotyping, subtyping, virulence profiling, and antimicrobial drug resistance testing. EIPs have started using banked isolates for developing whole-genome sequence libraries, which will better characterize pathogens at the molecular level and may make characterization from patient specimens (e.g. whole blood, CSF) easier. In the clinical diagnostic setting, metagenomics (the study of genomes from mixed communities of organisms) may eventually replace organism identification, virulence profiling, and some resistance testing, and it may be possible to use this data stream for a variety of public health purposes, including surveillance. Although whole-genome sequencing and metagenomics hold great promise for characterizing pathogens for surveillance, outbreak detection, and detection of emergence of new pathogens, they also pose challenges for processing, analyzing, and interpreting large amounts of data. Resources are needed to develop and sustain the bioinformatics infrastructure and to make sequences available to genomics reference banks so that EIPs can play a broader role in advancing public health practice.\n\n【24】Perhaps the most challenges and opportunities for surveillance systems are presented by use of multiplex tests. They may enable better tracking organisms that are currently underrecognized because culturing is difficult or because they would not otherwise be considered in the differential diagnosis. It may also enable better tracking of polymicrobial infections. However, understanding when detection equates with true infection is a challenge. The EIP may play a unique role in helping to decipher true infections from mere detection of organisms and in describing true polymicrobial infections because laboratory results can be matched with epidemiologic and clinical data.\n\n【25】### Conclusions\n\n【26】The availability and use of CIDTs in clinical medicine present opportunities to rapidly characterize diseases currently covered under the EIP surveillance umbrella and to detect and monitor other emerging infectious diseases. Their use also presents challenges for maintaining the EIP ability to accurately describe disease burdens, the effect of interventions, and microbiological and molecular characteristics of pathogens over time. Because of the long-standing collaboration between the EIP, laboratories, and disease reporters and resources devoted to collecting highly detailed and comprehensive surveillance data, the EIP infrastructure lends itself to close examination of the effect of CIDTs. EIP hopes to work with other domestic and international public health entities, regulatory bodies, diagnostic manufacturers, and academic and clinical groups to chart an evidence-based course for continuing to incorporate CIDTs into public health surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0270e23e-45b4-4827-b5a7-8e011ea03cd7", "title": "Neutralizing Antibody against Enterovirus D68 in Children and Adults before 2014 Outbreak, Kansas City, Missouri, USA", "text": "【0】Neutralizing Antibody against Enterovirus D68 in Children and Adults before 2014 Outbreak, Kansas City, Missouri, USA\nThe first enterovirus D68 (EV-D68) isolate (Fermon) was identified in 1962 . Before a 2014 EV-D68 outbreak, US reports of EV-D68 were relatively sparse (<100 sporadic cases and periodic outbreaks in 50 years) . In autumn 2014, a total of 1,153 confirmed EV-D68 cases occurred in 49 states and the District of Columbia; EV-D68 patients mostly had respiratory symptoms consistent with those in previous EV-D68 outbreaks and cases . Nationwide, severe disease occurred mostly in school-aged children. Through September 2014, EV-D68 was detected in 338 of 551 children with rhinovirus/enterovirus-positive test results; most (.3%) were hospitalized at The Children’s Mercy Hospital (Kansas City, MO, USA). Hospitalized EV-D68 patients often had asthma or recurrent wheezing. Many of these EV-D68–infected children had unusually severe, refractory bronchospasms, which resulted in 100 intensive care unit stays .\n\n【1】### The Study\n\n【2】We assessed the prevalence of EV-D68 neutralizing antibody in the Kansas City population before the 2014 outbreak using deidentified banked serum samples (stored at The Children’s Mercy Hospital) collected in 2012 (n = 155) and 2013 (n = 281) from healthy persons >2 years of age during a poliovirus seroprevalence study . Age, sex, and race/ethnicity distributions matched 2010 Kansas City census data  .\n\n【3】We performed serology testing at the Centers for Disease Control and Prevention (Atlanta, Georgia, USA). We used an adapted poliovirus microneutralization assay to test samples for neutralizing antibodies  against 4 phylogenetically distinct EV-D68 isolates: Fermon ; the 2014 Missouri isolate 14-18949 ; and 2 related but non-Missouri 2014 isolates, 14-18952 (clade B2) and 14-18953 (clade A2; GenBank accession nos. KM851230–1) . The proportion of US patients from whom these three 2014 circulating strains were detected was >91% for 14-18949, 7.4% for 14-18952, and <2% for 14-18953 .\n\n【4】Besides age, sex, and race/ethnicity (Hispanic vs. non-Hispanic), population demographics are descriptive; small subset numbers precluded formal statistical analysis. We performed analyses using Sigmaplot version 12.2 , and we considered p values <0.05 significant.\n\n【5】Of 436 serum samples, 217 were from male donors; median age was 13 (range 2–81) years. All had neutralizing antibody (i.e. \\> 3 log 2  , \\> 1:8 titer) against Fermon and 14-18949 ; 97% of samples had neutralizing antibody to 14-18953 and 89% to 14-18952. Overall seropositivity for the 4 isolates was not different (p = 0.763).\n\n【6】In total, 50% (/48) of the 14-18952–seronegative samples and 57% (/42) of the 14-18953–seronegative samples (p<0.001) came from male donors. Our 2–5-year-old age group made up the largest proportions of these seronegative populations (% \\[32/48\\] of 14-18952 and 36% \\[15/42\\] of 14-18953; p = 0.003). The 21 persons seronegative for both 14–18952 and 14–18953 had a median age of 3  years. Donor sex and race/ethnicity and season of sample acquisition did not differ between samples seropositive and seronegative for 14-18952 or 14-18953 (data not shown).\n\n【7】Neutralizing antibody titers also did not differ by season of sample acquisition, donor race/ethnicity, or donor sex. Mean titers to 14-18949 were lower (p<0.05) among self-identifying Hispanics (n = 36; 7.1 log 2  , range 3.83–10.5 log 2  ) than among non-Hispanics (n = 400; 7.83 log 2  , range 3.17–10.5 log 2  ), but this difference might not be clinically significant. Median titers rose with each advancing age group, except against Fermon among 11–15-year-olds (p<0.001). The overall median titer was highest for 14-18952 (.34 log 2  , range 2.5–10.5 log 2  ; p<0.001), despite a comparatively lower seropositivity (%). The overall titer was lowest for 14-18953 (.83 log 2  , range 2.83–10.5 log 2  ; p<0.001), despite a relatively high seropositivity (%).\n\n【8】All serum samples had neutralizing antibody against the major EV-D68 isolates circulating in the United States in 2014, and most had antibody to the other 2 less frequently detected isolates that year . During the 2014 outbreak, 5–10-year-olds (who would have been 3–8-year-olds during the sampling time of our study) had the most severe disease. Severe EV-D68 disease occurred often in children with atopic disease, reactive airway disease, or asthma. In 2014, little EV-D68 disease was noted among adolescents, adults, or the elderly . The age-associated severe EV-D68 respiratory disease observed in 2014 parallels our finding of lower overall titers in 2–5-year-olds and 6–10-year-olds.\n\n【9】Although introduction of EV-D68 into naive populations could have explained the 2014 outbreak, universal detection of antibody against 14-18949 (dominant 2014 isolate) before 2014 indicates previous widespread exposure to 14-18949 or a related isolate. EV-D68 was also detected in Kansas City as early as 2009 . Thus, the Kansas City 2014 outbreak did not occur because of population naivete to a 14-18949–like isolate. Indeed, neutralizing antibodies to other EV-D68 isolates were also detected . That 2–5-year-olds in our study had lower titers to 14-18949  suggests that older persons had more experience with 14-18949 or confirms that antibody elicited by non–14-18949 isolates can also neutralize 14-18949. Severe disease during the 2014 outbreak occurred among children who, according to our results, were relatively experienced with this pathogen; they were positive for neutralizing antibodies against 14-18949 but had lower median titers and a reduced reverse cumulative distribution compared with other age groups.\n\n【10】Why this large outbreak was able to occur in a population with a high prevalence of neutralizing antibody against the outbreak isolates remains unclear. One possibility is that respiratory tract mucosal antibody (probably in the form of secretory IgA) is more relevant than serum antibody for protection against respiratory disease . In this study, we could not address this possibility because only serum samples were available for testing. Also, certain persons could be more susceptible to severe disease because of genetic factors, preexisting atopy or asthma, or differences in other parts of the immune response, including immunopathologic responses. The argument for multiple factors contributing to disease despite the presence of neutralizing antibody is bolstered by the predilection of persons with asthma or atopic disease to have severe disease ; further, asthma patients experience more tight junction injury than persons without asthma during rhinovirus infection .\n\n【11】The only demographic factor potentially affecting titers was Hispanic race/ethnicity. However, the ≈0.8 log 2  difference might not be clinically significant, considering the median titers in both groups were >5 log 2  ; thus, whether race/ethnicity is a factor is unclear.\n\n【12】Our data parallel another study with a similar 2011 sampling timeframe conducted in China . In that study, neutralizing titers against Beijing/2008/01 EV-D68 were low in serum samples collected in 2004 for all age groups, but their 2011 titers resembled our data, despite few reported EV-D68 illnesses in the sampled area during 2007–2011. Their lowest overall titers were also observed in persons of younger age groups.\n\n【13】EV-D68 has been proposed as a cause of acute flaccid myelitis. Increasing reports of this condition underscore the need to better understand EV-D68 seroprevalence and circulation .\n\n【14】Our study had several limitations . Because of the retrospective study design, our data and interpretations are limited regionally and temporally. We tested for only 4 select EV-D68 isolates, and antibody reactivity with other isolates might differ.\n\n【15】### Conclusions\n\n【16】We detected at least some neutralizing antibody to Fermon and the dominant 2014 isolate  in all 436 EV-D68 samples acquired during 2012–2013 in Kansas City. Prospective studies are warranted to define a protective threshold of serum neutralizing antibody (or a surrogate of protection), the distribution of titers in children <2 years of age, and whether antibody levels differ by race/ethnicity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ef54d272-2f30-4c29-9b84-eff66694efec", "title": "17DD Yellow Fever Revaccination and Heightened Long-Term Immunity in Populations of Disease-Endemic Areas, Brazil", "text": "【0】17DD Yellow Fever Revaccination and Heightened Long-Term Immunity in Populations of Disease-Endemic Areas, Brazil\nYellow fever (YF) vaccination is recommended for persons living in YF-endemic areas as the most effective strategy to reduce the risk for infection . The 17D and 17DD live attenuated vaccines are considered similarly safe and immunogenic, regardless of the minor differences in their nucleotide sequences . The progressive expansion of areas with YF viral circulation in YF-endemic countries has required extensive vaccination campaigns that reduced the international vaccine stockpile and brought to light the discussion about the need for booster doses to guarantee long-term cell memory in populations living in YF-endemic countries. Outbreaks of YF occur occasionally in areas of Africa and South America .\n\n【1】In 2013, the World Health Organization (WHO) stated that a single dose of YF vaccine sufficed to provide lifelong protection and that no booster dose was required to guarantee protection against the disease . However, time-dependent loss of protective immunity has been reported . The levels of YF-neutralizing antibodies decrease significantly 10 years after vaccination; ≈25%–30% of primary vaccinees lack protective antibodies . In addition, the polyfunctional cellular immune responses elicited by YF vaccination that contribute to protection also displayed a time-dependent decline following primary vaccination . In light of this information, the single-dose regimen for YF vaccine has been questioned, especially in YF-endemic countries where the proportion of persons exposed to potential risks should be considered against the primary-vaccine failure rate and time-dependent decline of protective immunity.\n\n【2】The goal of this study was to evaluate the proxies of protection elicited by primary, secondary, and multiple vaccinations and verify the duration of neutralizing antibodies and 17DD-specific T- and B-cell memory following these distinct vaccination regimens. We sought to clarify the importance of 17DD-YF booster vaccination to heighten the immune response of those primary vaccinees living in endemic areas whose immunity declines to nonprotective levels.\n\n【3】### Materials and Methods\n\n【4】##### Study Population\n\n【5】We conducted this investigation during May 12, 2014–December 16, 2016, simultaneously sampling from Rio de Janeiro and 2 municipalities of Minas Gerais state (Alfenas and Ribeirão das Neves), Brazil. We assigned participants to groups on the basis of official vaccination records. The study included 421 samples collected from 326 healthy adults 18–77 years of age, initially categorized into 3 arms: primary vaccination, secondary vaccination, and multiple vaccination . We designed the primary vaccination and secondary vaccination arms as 2 complementary independent approaches, each including a longitudinal (paired samples) and a cross-sectional investigation (unpaired samples). Study groups were coded to indicate participants’ vaccination status (NV for nonvaccinated persons, PV for those who had had primary YF vaccination only, RV for those who had been revaccinated) and time since last vaccination, given in days or years (e.g, d0 for day zero).\n\n【6】##### Samples and Tests\n\n【7】We collected whole blood samples from each participant. We used samples of 5 mL without anticoagulant for plaque-reduction neutralization test (PRNT) and samples of 20 mL in heparin for 17DD-YF phenotypic and functional analyses.\n\n【8】##### PRNT\n\n【9】We used serum samples to quantify the PRNT levels to the 17DD-YF virus by the micro-PRNT50 test, as described previously by Simões et al. We performed assays at Laboratório de Tecnologia Virológica (LATEV), Bio-Manguinhos, and expressed results from replicates as the reciprocal of sample dilution, considering seropositivity of PRNT titers >1:50 serum dilution.\n\n【10】##### Dengue IgG Indirect ELISA\n\n【11】We performed serologic tests for dengue virus (DENV) IgG using a Panbio dengue IgG indirect ELISA kit . Tests were performed at Laboratório de Flavivírus, Instituto Osvaldo Cruz, as previously reported .\n\n【12】##### Phenotypic and Functional Memory Biomarkers\n\n【13】We performed in vitro 17DD-YF–specific peripheral blood lymph proliferative assay as previously reported by Costa-Pereira et al. In brief, we incubated replicates of PBMC suspension (.0 × 10 6  /well) for 144 hours at 37°C in 5% CO 2  . We harvested cells from control (CC) and 17DD-YF antigen–stimulated (DD-YF Ag) cultures, labeled them with live/dead dye , and used a cocktail of monoclonal antibodies (mAbs) to quantify the phenotypic memory status of T cells and B cells. For T cells we used anti-CD4/(RPA-T4)/FITC, anti-CD8/(SK1)/PerCP-Cy5.5, anti-CD27/(M-T271)/PE, anti-CD45RO/(UCHL1)/PE-Cy7, and anti-CD3/(SK7)/APC-Cy7; for B cells, we used anti-CD19/(HIB19)/PerCP, anti-CD27/(M-T271)/PE, and anti-IgD/(IA6–2)/FITC. We obtained all mAbs from BD Pharmingen .\n\n【14】In parallel, we stained cultured PBMC aliquots with live/dead dye and a mix of mAbs to quantify the functional memory status of T and B cells: anti-CD3/(UCHT1)/Qdot605 ; anti-CD4/(GK1.5)/APCe-Fluor780 ; anti-CD8/(SK1)/PerCP ; and anti-CD19/(HIB19)/Alexa-Fluor700 (eBioscience). After a fix/permeabilize procedure, we incubated cells with a mAbs cocktail of anti-TNF-α/(clone MAb11)/PE-Cy7, anti-interferon (IFN)-γ/(clone B27)/Alexa-Fluor488), anti-interleukin (IL)-5/(JES1–39D10)/PE, and anti-IL-10/(JES3–19F1)/APC, all from BD Biosciences. We fixed the stained cells and stored them at 4°C for < 24 hours before acquisition on a BD LSR Fortessa Flow Cytometer (BD Biosciences).\n\n【15】We acquired a total of 100,000 lymphocytes from each sample. We used FlowJo version 9.3.2 software  to quantify the memory T-cell and B-cell subsets, as well as the percentage of cytokine-producing T and B cells. We expressed the results as 17DD-YF Ag/CC Index, calculated as the ratio of cells observed in the 17DD-YF Ag cultures divided by the respective control culture.\n\n【16】##### Data Analysis\n\n【17】This study was composed of 2 independent but complementary approaches: a longitudinal investigation and a cross-sectional investigation. We performed statistical analyses for the longitudinal investigation using paired t-test to compare NV(d0) versus PV(d30–45) groups for primary vaccination, as well as #PV(y \\> 10) (primary vaccination >10 years ago) versus RV(d30–45) for secondary vaccination. For the cross-sectional design, we made the transversal comparisons among groups using analysis of variance adjusted to multiple comparisons and set statistical significance at p<0.05. (We did not highlight nonsignificant differences in the figures.) We used the χ 2  test to compare seropositivity rates between NV and PV groups and also between #PV and RV groups.\n\n【18】We performed biomarker signature analysis as described previously by Luiza-Silva et al. In brief, we calculated the global median value of 17DD-YF Ag/CC Index for each phenotypic and functional biomarker and used that value as the cutoff to identify each biomarker as low index (below global median) or high index (above global median). We considered only biomarkers observed in >50% of study participants for comparative analysis among groups.\n\n【19】We conducted Venn diagram analysis  to select common biomarkers among subgroups. We overlaid biomarker signatures for comparative analysis of time-dependent changes of biomarker sets observed after vaccination.\n\n【20】### Results\n\n【21】##### Booster Vaccination and PRNT\n\n【22】Data analysis demonstrated that primary vaccination triggered significant levels of 17DD-YF–specific neutralizing antibodies (p<0.0001), reaching a seropositivity rate of 96% . Of note, we observed progressive decrease in PRNT levels (p<0.0001) and in the PRNT seropositivity rates along the time compared with PV(d30–45). The seropositivity rate declined to ≈71% by 10 years after primary vaccination . Booster vaccination significantly increased the PRNT levels (p<0.0001) and raised the seropositivity rate from 69% to 100% . The secondary vaccination was accompanied by higher seropositivity rate after 10 years upon booster dose, regardless of the decrease in PRNT levels (p<0.0001) observed over time compared with #PV(d30–45) .\n\n【23】##### Secondary Booster Vaccination and YF-specific Cell-Mediated Memory\n\n【24】Comparative analysis of NV(d0) versus PV(d30–45) after 1 or 2 doses of 17DD-YF vaccine demonstrated that primary vaccination is followed by an increase of memory T cells, including eEfCD4 (p<0.05), EMCD4 (p<0.05), and EMCD8 (p = 0.0006), and all B-cell subsets evaluated, NCD19 (p = 0.01), nCMCD19 (p = 0.001), and CMCD19 (p = 0.001). We also observed a decrease of NCD8 (p = 0.02), eEfCD8 (p = 0.02), and CMCD8 (p = 0.001). The results showed that cellular immunity, eEfCD4 (p = 0.01), EMCD4 (p = 0.01), and EMCD8 (p = 0.009); and all B-cell subsets, NCD19 (p = 0.003), nCMCD19 (p = 0.02), and CMCD19 (p = 0.0002), clearly wane over 10 years, compared with 30–45 days after primary vaccination .\n\n【25】Secondary vaccination with 17DD-YF was able not only to increase the level of memory T-cell subsets, eEfCD4 (p<0.05), EMCD4 (p<0.05), and EMCD8 (p = 0.04) at 30–45 days after the second dose but also to sustain the maintenance of eEfCD4 and EMCD8 levels even after \\> 10 years upon booster vaccination compared with 30–45 days after secondary vaccination. We observed no substantial changes in memory B-cell subsets upon secondary vaccination .\n\n【26】##### Revaccination and IFN-γ–Mediated T-Cell Memory\n\n【27】Data analysis for in vitro 17DD-YF antigen recall revealed that primary vaccination induced significant increases of functional memory biomarkers in CD4+ (tumor necrosis factor \\[TNF\\]–α/p = 0.04, IFN-γ/p = 0.04 and IL-5/p = 0.0001) and CD8+ T cells (TNF-α/p = 0.0003, IFN-γ/p = 0.008 and IL-5/p = 0.0002) as well as in B cells (TNF-α/p<0.05 and IL-5/p = 0.016) . We observed a decrease of IL-10+CD4+ T cells (p = 0.04) at 30–45 days after primary vaccination and a clear decrease of TNF-α, IFN-γ, and IL-5 produced by CD4+ and CD8+ T cells. We also saw a decrease in TNF-α and IL-5 from B cells over time, particularly at >10 years, compared with 30–45 days after primary vaccination (p<0.05 in all cases). Conversely, we detected an increase of IL-10+ B cells over time after primary vaccination .\n\n【28】Secondary vaccination was able to restore T cell functional memory mediated by IFN-γ and B-cell functional memory by TNF-α. We observed sustained production of IFN-γ by CD8+ T cells even \\> 10 years after booster vaccination .\n\n【29】##### Booster Vaccination and Long-lasting Persistence of Effector Memory\n\n【30】We constructed overlays of biomarker signatures of NV(d0) versus PV(d30–45) in the primary-vaccination arm of the study as well as #PV(y \\> 10) versus RV(d30–45) in the secondary-vaccination arm to select those attributes eligible as universal memory-related biomarkers . Venn diagram analysis revealed 3 common attributes (EMCD4, EMCD8, and IFNCD8) that we tagged for follow-up analysis after primary, secondary, or multiple 17DD-YF vaccination .\n\n【31】After we selected the universal set of follow-up attributes, we compared biomarker signatures over time after 17DD-YF primary or secondary vaccination . Data analysis demonstrated that all 3 biomarkers were observed in PV(d30–45) and PV(y1–5). EMCD8 was maintained in PV(y>5–9) but not in PV(y \\> 10). Comparative analysis between #PV(y \\> 10) and RV(d30–45) demonstrated restoration of universal memory-related biomarkers (EMCD4, EMCD8, and IFNCD8) upon revaccination. Moreover, we identified EMCD8 in a high proportion of secondary vaccinees across the time periods after booster dose .\n\n【32】##### Comparison of 17DD-YF Primary and Booster Vaccination Effects on Cell-Mediated Memory\n\n【33】We used individual PRNT and EMCD8 profiles to assemble a memory matrix and calculated the resultant YF-specific memory. We classified each volunteer by positive results above the cutoff threshold as EMCD8, PRNT, none, or both . Data analysis demonstrated that primary vaccination leads to a resultant memory in 97% of volunteers, with 3% of failure in the PV(d30–45) group. However, an increase in the proportion of primary vaccinees with “none” positive attributes, neither PRNT nor EMCD8 biomarkers, was observed, reaching a critical value of 30% at ≥10 years after primary vaccination .\n\n【34】The comparison between #PV(y \\> 10) and RV(d30–45) demonstrated that secondary vaccination restored the resultant memory in 100% of the volunteers, which was different from primary vaccination results. All secondary vaccines evaluated simultaneously for PRNT and EMCD8 profile presented a preserved resultant memory. In particular, at \\> 10 years after secondary vaccination, 100% of vaccinees had 1 or both biomarkers detectable .\n\n【35】##### Multiple Vaccination and the Overall Profile of Protection\n\n【36】We analyzed 17DD-YF memory status triggered by multiple vaccinations . The results demonstrated that all vaccinees who received multiple shots had PRNT levels above the cutoff limit . The proportion of vaccinees with EMCD8 above the cutoff limit was higher for RV(y1–5) and RV2+(y1–5) than for PV(y1–5) . Likewise, the overall resultant memory for RV(y1–5) and RV2+(y1–5) was higher than that for PV(y1–5) – .\n\n【37】##### PRNT Seronegativity before Revaccination and Cell-Mediated Memory Response\n\n【38】The volunteers from the #PV(y≥10) group were further categorized into subgroups, PRNT– and PRNT+, according to their PRNT results before revaccination. The levels of humoral and cellular biomarkers, as well as the magnitude of baseline fold changes in neutralizing antibodies, were higher in PRNT– than in PRNT+ vaccines . An increase in PRNT titer by a factor >4 at follow-up further demonstrated the relevance of booster doses to restore the immunological memory of these subjects . Furthermore, the booster dose had higher impact on cellular immunity and biomarker signature of PRNT– than PRNT+ primary vaccinees, and both groups restored the EMCD8 biomarker compared with the #PV(y \\> 10) group .\n\n【39】##### Dengue Virus Seropositivity\n\n【40】We analyzed the results to determine whether DENV seropositivity influences the humoral and cellular memory after secondary vaccination . We found DENV seropositivity in 28% of volunteers in the secondary vaccination arm. The results demonstrated that DENV seropositivity did not influence PRNT seropositivity over time after secondary vaccination . Moreover, DENV seropositivity did not affect cellular immunity (EMCD8), which remained above the cutoff limit in all subgroups of secondary vaccinees .\n\n【41】### Discussion\n\n【42】YF vaccination induces an efficient immunity and represents one of the most effective strategies to reduce the risk for infection in YF-endemic countries. The vaccine is highly immunogenic, eliciting a strong antibody response together with a broad and complex innate  and adaptive immunity . Although the YF vaccine has been considered a benchmark among vaccines because of its ability to induce long-lasting immune response, the duration of humoral and cellular immunity following YF vaccination is still controversial. Some studies demonstrate that neutralizing antibodies and YF-specific CD8+ T cells after primary vaccination are suggestive imprints compatible with long-lived memory ; other studies emphasize that the immunity to YF vaccine wanes over time, suggesting the need for booster doses to guarantee long-lasting, efficient immunity memory .\n\n【43】We could not assess protective immunity to YF in humans by challenge with live wild-type YF virus. Therefore, the protective or nonprotective immunity status to YF virus in humans is based on laboratory methods, known as correlates of protection; PRNT level has been considered the standard for measuring postvaccination immunity to YF. Pinpointing cellular immunity biomarkers is relevant in studies that pose the question of whether PRNT seronegativity necessarily means absence of protective immunity . Our investigation proposes that, in addition to PRNT seropositivity, YF-specific cellular immunity may be a useful tool for monitoring the duration of YF vaccine-induced memory over time. However, upon closer examination, our findings indicate the decline of YF-specific immune response over time, emphasizing that the YF-specific immunity wanes shortly after primary vaccination and that a substantial proportion of primary vaccines (%–30%) do not present sufficient levels of neutralizing antibodies or CD8+ T-cell memory within 5–10 years after primary vaccination.\n\n【44】Some studies have investigated the relevance of booster doses to heighten the YF-specific immune response in primary vaccinees whose immunity has declined to nonprotective levels. Weiten et al. postulated that booster vaccination did not increase the titers of YF-specific antibodies nor induce or alter the phenotypes of CD8+ T cells. Conversely, Kongsgaard et al. have demonstrated that, although most vaccinees responded to a booster vaccination, the antibody titers and the cellular immune responses observed following revaccination were lower than for primary vaccination responses. In this study, we have demonstrated that the booster dose was able to upregulate the levels of neutralizing antibodies and heighten the cellular immunity signature and restore the proportion of vaccinated participants with high levels of effector memory CD8+ T cells. We strongly believe that the differences we observed among the published studies could be attributed to differences in the populations under scrutiny.\n\n【45】The magnitude of increase in the neutralizing antibody titers and YF-specific CD8+ T-cell response achieved with a booster dose may be closely related to the baseline immune activation, suggesting that an activated immune microenvironment before revaccination impairs the response to the YF vaccine. Muyanja et al. showed that 17D-204 vaccinees in Africa displayed decreased levels of YF-specific immunity compared with vaccinees in Europe. It is possible that the immune microenvironment affects the quantitative and qualitative response, as well as the vaccine efficacy. Whereas vaccinees in Europe displayed persistent PRNT levels even at 10 years after vaccination, those in Africa exhibited reduced persistence of YF immunological memory. Constant exposure to infectious diseases, as well as diet and gut microbiota, may lead to a state of immune hyperactivation and exhaustion that can contribute to reduced magnitude of cellular and humoral responses and impair the YF vaccine memory. Of note, the impaired YF vaccine-induced memory was boosted by a second vaccination . Our results corroborate this hypothesis, showing the relevance of a booster dose to improve the immune response among primary vaccinees. This recommendation should be considered to restore the YF protective immunity in those primary vaccinees whose correlates of protection fade over time, reaching nonprotective levels.\n\n【46】The loss of immunity in a subpopulation of vaccine recipients should be taken into consideration, and a booster dose should be administered. Previous studies have demonstrated that, on average, approximately 1 in 5 persons from non–YF-endemic areas and 3 in 10 persons living in YF-endemic areas may lose measurable antibody responses within 10 years after primary vaccination. In this sense, it is true that the 17DD-YF vaccination elicits long-lasting immunity; however, lifelong immunity is not observed in every vaccinated individual. Although PRNT is the standard assay to monitor YF protective immunity, it is not a feasible method available in local laboratories to identify participants who should receive a booster vaccination.\n\n【47】The expansion of risk areas for YF worldwide has contributed to the depletion of YF vaccine stockpile and the need for new strategies to reverse the imminent shortage of vaccine. WHO has considered measures to improve YF vaccine supply, including a dose-sparing strategy as a short-term measure. However, the dose sparing is not proposed for routine immunization. WHO also considered advising a single lifetime dose of YF vaccine. In YF outbreaks or periods of limited vaccine production and reduced stockpiles, the YF primary vaccination should be the priority, as recommended by the Strategic Advisory Group of Experts on Immunization. However, once the YF vaccine supply has normalized and no outbreaks are reported, revaccination should be suggested. On the basis of our findings, \\> 1 booster dose at 10 years after primary vaccination is suggested for travelers entering higher-risk areas and required for residents of YF-endemic countries who are at risk for infection .\n\n【48】Altogether, our findings emphasize the relevance of booster doses to heighten the 17DD-YF specific immune response to achieve efficient immunity. Secondary vaccination improved the correlates of protection that had waned over time after 17DD-YF primary vaccination and lowered the loss of protection over time. However, multiple vaccination seems to better support long-lasting protection in all vaccinees, which suggests that \\> 3 booster doses at 10-year intervals should be recommended, especially in areas with high risk for YF transmission. Whether the waning of immune markers observed over time is associated with loss of YF vaccine effectiveness and increased risk for breakthrough YF infection remains to be seen.\n\n【49】In summary, we evaluated the evidence for benefits and risks associated with YF vaccine booster doses using the Grading of Recommendations, Assessment, Development, and Evaluation framework . A primary dose of YF vaccine is well known to be highly safe and effective, with few vaccine failures. However, a critical wane of correlates of protection has been documented by our group and others, particularly at ≥10 years postvaccination. Few reports of serious adverse events have been observed following booster doses of the vaccine. We recommend the booster dose to prevent this serious disease that has no treatment and substantial mortality rates.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9245dfb4-a498-488a-81c7-3dacfb2ca8f8", "title": "Buruli Ulcer in Long-Term Traveler to Senegal", "text": "【0】Buruli Ulcer in Long-Term Traveler to Senegal\n**To the Editor:** Buruli ulcer (BU) is caused by infection of subcutaneous fat with the environmental pathogen _Mycobacterium ulcerans_ . BU has been reported or suspected in more than 30 countries. It has never been reported in Senegal and Guinea-Bissau . We report a case of travel-associated BU in a French traveler to Senegal.\n\n【1】The patient was a 24-year-old Caucasian man who came to the University Hospital of Bordeaux, France, with a nonhealing lesion on the anterior left leg that had been present for ≈12 weeks. The patient had traveled in Senegal to the border of Guinea-Bissau from September 2006 through August 2007. His trip had begun in Dakar and proceeded south to the districts of Kaolack, Toubacouta, and Casamance. The patient stayed in Casamance during the rainy season from June 2007 through August 2007. He had been working on construction of wood dugouts, had been bare-legged regularly, and had been in contact with stagnant water.\n\n【2】He first noticed a lesion during June 2007, which had gradually increased to a small, centrally crusted ulcer. By the end of August 2007 (week 8 of the lesion), skin examination showed a 3 × 6-cm necrotizing ulcer with central crusting and an erythematous border . The lesion was not warm or tender but generated a seropurulent discharge. Concurrently, palpable left inguinal lymph nodes were observed.\n\n【3】Bacteriologic swabs identified _Staphylococcus aureus_ and group A _Streptoccocus pyogenes_ . Two punch-biopsy specimens were taken from the border of the lesion. Histologic analysis showed nonspecific acute and chronic dermal inflammation with necrotizing granulomas that extended into the subcutaneous tissues, suggestive of infection with atypical _Mycobacterium_ spp. Bacteriologic examination did not identify acid-fast bacilli (negative direct smear result after Ziehl-Neelsen staining) or other specific microorganisms (negative direct smear results after periodic acid–Schiff, Giemsa, and Gram staining). Tissue specimens were placed into BACTEC 12B broth (Becton Dickinson, Franklin Lakes, NJ, USA) (incubated at 35°C) and onto Löwenstein-Jensen slants (incubated at 30°C). No growth was detected after 42 days. On the basis of clinical findings, we suspected a diagnosis of BU.\n\n【4】Taq-Man real-time quantitative PCR that used primers for 2 _M_ . _ulcerans_ –specific genes (insertion sequence _2404_ and ketoreductase B gene)  and negative controls showed positive results for DNA from both biopsy specimens. A normalized standard curve was constructed, which indicated a bacterial load of ≈6 × 10 3  organisms/g of tissue.\n\n【5】Laboratory investigations indicated a total leukocyte count of 16,400 cells/μL (reference range 3,600–10,000 cells/μL) and a C-reactive protein level of 0.59 mg/mL (reference value <0.01 mg/mL). Results of radiologic investigations were normal. The patient was treated with rifampin (mg/day) and moxifloxacin (mg/day) for 12 weeks. Additional surgical excision was planned 4 weeks after treatment was begun. Unfortunately, 15 days later, the patient was lost to follow-up.\n\n【6】BU has been reported in many West African countries, with Guinea being the northern limit of reported cases. Detection of this case of BU suggests that the region in West Africa endemic for this disease has been underestimated or is expanding. Infection in the traveler may have occurred in Casamance, if one assumes an incubation period of 6 weeks to 3 months. Further cases should be actively sought in this region and adjoining districts visited to evaluate the geographic extent of the disease.\n\n【7】The environmental reservoir and mode of transmission of BU in our patient are unknown. Exposure of unprotected skin with stagnant or slow-flowing water is linked with BU. Our patient reported prolonged contact with water during his occupation. Recent studies implicating aquatic predator insects  and mosquitoes  in transmission of BU suggest that use of insect repellents and protective clothing may help prevent infection.\n\n【8】The diagnosis of BU in this patient relied on the PCR detection of 2 _M_ . _ulcerans_ –specific genes; this procedure is considered adequate . The relatively low number of organisms detected may explain the negative acid-fast bacilli smear and culture results . Our report of _M_ . _ulcerans_ infection from Senegal is not surprising because southern Senegal shares similar ecologic features with neighboring affected countries, especially during the heavy rainy season.\n\n【9】Although BU is a disease that affects mainly persons in recognized disease-endemic areas, this case emphasizes that tropical skin ulcers should be considered in differential diagnosis of BU in travelers returning from disease-endemic countries . Diagnostic delays can be avoided by use of _M_ . _ulcerans_ –specific PCR, a test available from World Health Organization collaborating laboratories, which enables rapid confirmation of diagnosis of BU.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "996f5410-067d-46b8-9010-2f236b8ba433", "title": "Spatial Analysis and Correlates of County-Level Diabetes Prevalence, 2009–2010", "text": "【0】Spatial Analysis and Correlates of County-Level Diabetes Prevalence, 2009–2010\nAbstract\n--------\n\n【1】**Introduction**  \nInformation on the relationship between diabetes prevalence and built environment attributes could allow public health programs to better target populations at risk for diabetes. This study sought to determine the spatial prevalence of diabetes in the United States and how this distribution is associated with the geography of common diabetes correlates.\n\n【2】**Methods**  \nData from the Centers for Disease Control and Prevention and the US Census Bureau were integrated to perform geographically weighted regression at the county level on the following variables: percentage nonwhite population, percentage Hispanic population, education level, percentage unemployed, percentage living below the federal poverty level, population density, percentage obese, percentage physically inactive, percentage population that cycles or walks to work, and percentage neighborhood food deserts.\n\n【3】**Results**  \nWe found significant spatial clustering of county-level diabetes prevalence in the United States; however, diabetes prevalence was inconsistently correlated with significant predictors. Percentage living below the federal poverty level and percentage nonwhite population were associated with diabetes in some regions. The percentage of population cycling or walking to work was the only significant built environment–related variable correlated with diabetes, and this association varied in magnitude across the nation.\n\n【4】**Conclusion**  \nSociodemographic and built environment–related variables correlated with diabetes prevalence in some regions of the United States. The variation in magnitude and direction of these relationships highlights the need to understand local context in the prevention and maintenance of diabetes. Geographically weighted regression shows promise for public health research in detecting variations in associations between health behaviors, outcomes, and predictors across geographic space.\n\n【5】Introduction\n------------\n\n【6】More than 25 million Americans have diabetes, and another 80 million have prediabetes; taken together, approximately 1 in 3 Americans have diabetes or prediabetes . Diabetes is associated with obesity and physical inactivity; many built environment factors — attributes of the proximate environment — such as access to healthy foods , crime level , the rural–urban matrix , and walking  are correlated with diabetes prevalence. One of the great challenges in understanding the associations between built environment attributes and diabetes is that both factors vary across the United States. Although studies of diabetes have found spatial variations in incidence and prevalence, there is a paucity of information on how the spatial prevalence of diabetes may or may not be associated with the spatial prevalence of built environment attributes. The importance of understanding the covariance of diabetes with its correlates was made salient by Siordia and colleagues , who found that the relationship between poverty and diabetes prevalence varied across the United States, with poverty highly correlated with diabetes in some regions but not in others . This finding provided the impetus for our hypothesis that the relationship between diabetes prevalence and county-level built environment attributes is nonstationary (ie, the relationship varies across space).\n\n【7】The objective of our study was to determine how and where diabetes prevalence is associated with built environment attributes at the county level in the contiguous United States. This information could allow programs and interventions to better target populations and attributes of the built environment associated with high diabetes prevalence.\n\n【8】Methods\n-------\n\n【9】Our study used geographically weighted regression (GWR), a tool that is increasingly used by public health researchers to understand the nuances of such issues as access to health care, disease distribution, and spatial variation in magnitude of health outcome predictors .\n\n【10】### Data sources\n\n【11】We used county-level cross-sectional secondary data from various publicly available sources. Geographic information systems (GIS) shapefiles of contiguous US counties were downloaded from the Topographically Integrated Geographic Encoding and Referencing (TIGER) files available from the US Census Bureau  and imported into the ArcGIS 10.2 software (ESRI). Data on diabetes prevalence, obesity rates, and physical inactivity were collected from the Centers for Disease Control and Prevention’s (CDC’s) Diabetes Interactive Atlas , which is based on data from the Behavioral Risk Factor Surveillance System (BRFSS). CDC defines diabetes prevalence as the estimated percentage of adults with diagnosed diabetes, after adjusting for age. BRFSS does not differentiate between type 1 and type 2 diabetes. CDC defines obesity prevalence as the estimated percentage of obese adults (body mass index ≥30) after adjusting for age. The prevalence of physical inactivity is an estimated percentage of adults who are physically inactive. Physically inactive adults are those who have not participated in any physical activity or exercise in the preceding 30 days . All BRFSS data are based on self-report. Data on walking or cycling to work were collected from the US Census; this variable was defined as the percentage of employed adults per county who stated they either walked or cycled to work in the previous week.\n\n【12】Data for the sociodemographic variables — percentage nonwhite population, percentage Hispanic population, percentage living below the federal poverty level, education level, population density, and percentage unemployed — were from the US Census Bureau’s American Community Survey 5-year estimates  . The variable for percentage nonwhite population refers to the percentage of people who did not identify themselves as white and does not include Hispanics who identify themselves as white. Percentage Hispanic population refers to the percentage of people who identified themselves as Hispanic (both white and nonwhite). The percentage of people living below the federal poverty level was determined according to income thresholds defined by the US Census Bureau, which differ by family composition. The education variable was defined as the percentage of people who reported having less than a high school diploma. Population density was defined as the number of people per square mile in a county. Unemployment was determined as the percentage of civilians aged 16 years or older that did not have work for the reference week. Data on food deserts were collected from the Department of Agriculture (USDA); the food desert variable was defined as the percentage of census tracts (per county) that are food deserts . USDA defines a census tract as a food desert if 33% of the population lives far (urban, >1 mile; rural, >10 miles) from a supermarket or a grocery store. All variables were determined at the county level. There were 3,109 counties included in the study. Counties in Alaska and Hawaii were excluded because we could not test the influence of proximity; these states do not border other US states, and in Hawaii, no county borders another.\n\n【13】### Geographically weighted regression\n\n【14】We used GWR in addition to ordinary least squares (OLS) regression because the spatial data used in our study violates 2 major assumptions of global regression. First, global OLS regression assumes observations are independent of each other. However, spatial data often are clustered, suggesting stronger relationships between proximate observations . Clustering can result in correlation among regression residuals across space, or spatial autocorrelation, and biased parameter estimates . Second, OLS regression assumes spatial stationarity of the relationship between independent and dependent variables . In other words, it assumes coefficients will be constant across a sample area. However, the context of a particular area can influence the magnitude and direction of the relationship and produce a range of coefficients . GWR relaxes these assumptions and enables the analysis of spatially relevant data. Unlike OLS regression models, which produce global models across space, GWR produces numerous local models. It simultaneously conducts multiple regressions so that there is one regression model per spatial data point (eg, a county). Observations closer to a particular data point will have more weight in the estimation than observations farther away.\n\n【15】### Methodological steps in model building\n\n【16】The first step in the model building process is to map the dependent variable and explore spatial heterogeneity. If the dependent variable is not clustered, there is no need to build a spatially explicit model. Without clustering, the global model will be similar to the local model . We used the Moran’s Index (I_ ) in ArcGIS to map the clustering of diabetes prevalence across counties in the United States. Moran’s _I_ ranges from −1.0, perfectly dispersed (eg, a checkerboard pattern), to a +1.0, perfectly clustered. A _z_ score and _P_ value are generated as outputs along with Moran’s _I_ .\n\n【17】Initial data exploration and model specification using OLS was completed using SPSS 22 software (IBM Corporation). Three factors motivated the decision to first specify the OLS model: 1) we wished to identify variables significantly correlated with the dependent variable before specifying the regression model; 2) the GWR software used for spatial analysis does not provide a variance inflation factor (VIF) to assess multicollinearity; and 3) the GWR software does not enable the researcher to extract regression residuals to assess spatial autocorrelation for the global model.\n\n【18】In the OLS regression we included only variables significantly correlated with the dependent variable, diabetes prevalence. Residuals from the global OLS model were mapped and analyzed for spatial autocorrelation using Moran’s _I_ . The same set of variables was then used to specify a GWR model using the GWR4 software . While conducting GWR, we used the adaptive kernel, which was produced using the bi-square weighting function. The adaptive kernel uses varying spatial areas but a fixed number of observations for each estimation, a method most appropriate when the distribution of observations varies across space. In our case, observations (counties) are much smaller and closer together in the Northeast and Southeast than they are in the Midwest and West Coast. Finally, a process that minimizes the Akaike Information Criteria (AIC) was used to determine the best kernel size. The parameter estimates and _t_ values produced by the software were exported and mapped using ArcGIS 10.2 (ESRI).\n\n【19】The residuals of GWR models are assumed to be normally distributed; a further assumption is that they are not spatially autocorrelated or clustered across space. Such clustering suggests that the local model underestimates or overestimates diabetes prevalence in particular areas. The residuals from the GWR model were analyzed using Moran’s _I_ to assess spatial autocorrelation. The clustering of residuals for OLS and GWR models were compared to assess the value of using GWR.\n\n【20】### Comparison of OLS and GWR model performance\n\n【21】We used 3 tools to compare the OLS and GWR models. First, we compared the adjusted R 2  of the basic OLS model and the GWR model. A higher adjusted R 2  in the GWR model than in the OLS model for the same set of variables suggests that location plays an important role in explaining the variance of diabetes prevalence. Second, we compared the corrected AIC (AICc) for both models. AICc is a widely used measure of goodness-of-fit that adjusts for degrees of freedom . It can be used to compare models with the same dependent variable but different independent variables. AICc can also be used to compare a global model with local models  because AICc does not assume models must be nested . The values of AICc are not absolute, but relative, so that they are meaningful only when compared between models. The model with a smaller AICc is deemed a better fit. The final analytical step was to compare residuals of both models for their distribution and spatial autocorrelation.\n\n【22】Results\n-------\n\n【23】Diabetes prevalence in the United States at the county level ranged from 3.8% to 17.8% and was significantly clustered (Moran’s _I_ \\= 0.35; _z_ \\= 540.2; _P_ < .001). We found clusters of high diabetes prevalence in the Southeast and clusters of low diabetes prevalence in Colorado. Diabetes prevalence was significantly correlated with numerous independent variables. Because the percentage of neighborhood food deserts was not significantly correlated at the county level, it was not included in the OLS model. The following 9 variables were included in the OLS model: population density, percentage nonwhite, percentage Hispanic, percentage living below the federal poverty level, percentage with less than high school education, percentage unemployed, percentage obese, percentage physically inactive, and percentage that walked or cycled to work. The OLS model was significant (F_ 9,3099  \\= 495.87, _P_ < .001). The model explained 58.8% of the variance in county-level diabetes prevalence. The VIF for all variables was less than 4.0, a commonly used cutoff point, suggesting no multicollinearity .\n\n【24】The residuals of the OLS model were spatially autocorrelated (Moran’s _I_ \\= 0.13; z = 26.4; _P_ < .001). The OLS model overestimated diabetes prevalence for Colorado and New Mexico counties. Similarly, it underestimated the outcomes for Alabama and West Virginia counties.\n\n【25】The GWR model produced coefficients for each county . The change in both magnitude and direction of the coefficients suggests spatial nonstationarity of the relationship between the predictors and diabetes prevalence. The direction of the relationship in most counties was as expected. Only a few counties had opposite relationships for the predictors in the GWR model. In most counties, walking or cycling to work was associated with lower diabetes prevalence. However, a few clusters of rural counties in Minnesota, North Dakota, and South Dakota show an association between walking or cycling to work and higher diabetes prevalence. Such nonstationarity demands a more nuanced analysis with a contextual focus. For example, high rates of walking or cycling to work are often associated with multimodal transportation that also includes public transit, which is less likely to be available in rural communities .\n\n【26】The adjusted R 2  for the local GWR model ranged from 0.06 to 0.94; the adjusted R 2  in the OLS model was 0.58. Explicitly, the global OLS R 2  of 0.58 masks a wide distribution of local associations between the predictors and diabetes prevalence. Without GWR, we would have been unable to estimate local models. In counties in North Dakota, South Dakota, and Montana, the GWR model explained up to 94% of the variance in diabetes prevalence. However, in Washington and Oregon, the model did not explain much of the variance (%–37%), a spatial variation that would have been missed with the OLS model alone. Residuals for the GWR model, although significant, were less spatially autocorrelated than residuals for the OLS model (Moran’s _I_ \\= 0.01; _z_ \\= 3.74; _P_ < .001). Compared with OLS, the GWR model greatly improved model fit. The GWR model explained more variance in diabetes prevalence and reduced the AICc (ΔR 2  \\= 0.22; ΔAICc = 2,008.4).\n\n【27】Discussion\n----------\n\n【28】Poverty level, physical inactivity, and walking or cycling to work were each significantly associated with county-level diabetes prevalence, relationships that were spatially nonstationary across the United States. The variation in parameter estimates from GWR suggests the need to apply this spatial analysis tool to other diabetes studies that have been restricted to global models . In the global OLS model, 58.8% of county-level diabetes prevalence was explained by race, poverty, obesity, physical inactivity, and walking or cycling to work. However, at an individual county level, the explanatory percentage ranged from 6% to 94%, and the individual county-level models were significantly clustered. This clustering suggests that local contexts, policies, programs, and built environment attributes are associated with diabetes prevalence and that the amplitude of such contexts, policies, programs, and environments varies across the nation.\n\n【29】The dissimilarity in variable coefficients was not a factor of one county alone but was a factor of multiple proximal counties, perhaps because of policy and programmatic spillover from neighboring counties and diffusion of innovation . The percentage of nonwhite population in a county had the greatest effect in the Southeast and the Rockies, from Arizona and New Mexico to Idaho and Montana. States in these regions have a high proportion of African Americans, Hispanics, or Native Americans, races/ethnicities with disproportionately high rates of diabetes . In several regions (including the Midwest, the Ohio Valley, and New England), poverty had a greater association with diabetes prevalence than any other variable. Physical inactivity had the greatest effect in the Southeast and the Southwest, a pattern similar to that of obesity prevalence . Walking or cycling to work was most associated with diabetes prevalence in the Mississippi Valley, the panhandles of Texas and Oklahoma, and south Florida, areas not generally associated with walking or cycling because of their hot summers.\n\n【30】The relationships among nonwhite populations, poverty, physical inactivity, and diabetes are not new . Others found these relationships have a spatial component . With the exception of recent work by Siordia and colleagues , there has been no investigation into the nonstationarity of these relationships. Similarly, the strong association between walking or cycling to work and diabetes is consistent with findings of other studies , but it has not been investigated for spatial heterogeneity or nonstationarity. That there is a significant association between nonwhite populations, poverty, physical inactivity, and diabetes and that this relationship has a spatial but nonstationary association highlights the need for local, context-specific diabetes prevention programs.\n\n【31】There are limitations to GWR and our analyses. GWR equates the local regression coefficients based on those geographic areas (eg, counties) most proximate to the area of interest. That is, the regression equation and coefficients for a county in Missouri are most influenced by bordering counties and other nearby counties, but not influenced by counties in Colorado or North Carolina. This concept is essential for local planning and related to Tobler’s first law of geography, that “everything is related to everything else, but near things are more related than distant things” . However, the distance of influence (of predictors or potential interventions) is theoretically unknown and perhaps inconsistent across a geographic area (eg, the continental United States). We chose to use an adaptive kernel bandwidth, which accounted for differences in the size of counties and therefore the distance of influence. This choice should have helped adjust for the fact that, for example, North Carolina has 100 small counties and California has 58 larger counties spread over 3 times the landmass of North Carolina. Because of this discrepancy, the data point (county) was an estimate based on proximate counties as defined by the kernel type. GWR is also limited by the edge effect, whereby counties located on the edges of the United States (ie, coastal regions and the borders with Canada and Mexico) do not have the 360° influence of counties in the nation’s interior.\n\n【32】There are also limitations to our findings. The local R 2  s accounted for 6% to 94% of county-level diabetes prevalence. In large geographic areas in the Mid-Atlantic, upper Midwest, and Northwest, the 9 variables included in the model explained less than one-third of the variance in diabetes prevalence, which means that most factors associated with county-level diabetes prevalence in these geographic areas must have been missing from our model.\n\n【33】The primary strength of this study is the use of GWR in the analysis of the spatial distribution and correlates of diabetes prevalence. Siordia and colleagues  introduced the concept of spatial nonstationarity to the relationship between poverty and diabetes. Here, we extend their work by incorporating additional socioeconomic variables and built environment correlates with diabetes. GWR adds value to public health research and practice by emphasizing location-specific theories of health outcomes and tailored policies for intervention. It scrutinizes the assumption of global relationships between various predictors and health outcomes. Using GWR, public health researchers and practitioners can gain a nuanced understanding of health-related issues and respond to the notion that “all health is local” . In doing so, they can provide clarity for designing and funding context-specific public health programs and policies, especially for national programs that have local reach, including those of the CDC and the American Diabetes Association (ADA). Our analyses could also be used by local public health departments and ADA offices for resources such as MIYO  to tailor messages and materials for their target audiences. The use of GWR is a key advancement in public health research and practice because many health behaviors and outcomes vary spatially (eg, obesity) as do many common predictors (eg, race/ethnicity) .\n\n【34】Shedding light on spatial variations can provide new insights into well-established relationships. The methodology of GWR needs to be expanded to additional public health efforts to understand the impact of environment and place on health and how these relationships may vary across space. For diabetes prevalence, we presented an initial step in this direction, but much work remains before we understand why these variations exist and why race/ethnicity, poverty, physical inactivity, and active commuting have little explanatory effect in some regions but explain up to 94% of diabetes prevalence in other regions.\n\n【35】Tables\n------\n\n【36】#####  Table 1. Results From Ordinary Least Square Model of US County-Level Diabetes Prevalence, 2009–2010\n\n| Characteristic | β | SE | _t_ Value | _P_ Value | Variance Inflation Factor |\n| --- | --- | --- | --- | --- | --- |\n| Intercept | 4.80 | 0.190 | 24.94 | <.001 | — |\n| Population density | 0.000192 | 0 | 8.93 | <.001 | 1.45 |\n| Percentage nonwhite population | 0.043 | 0.002 | 26.22 | <.001 | 1.42 |\n| Percentage Hispanic population | −0.03 | 0.002 | −18.32 | <.001 | 1.11 |\n| Percentage living below federal poverty level | 0.10 | 0.004 | 23.65 | <.001 | 1.46 |\n| Percentage unemployed | −0.89 | 0.360 | −2.48 | .01 | 2.95 |\n| Percentage with less than a high school education | 0.44 | 0.110 | 3.97 | <.001 | 3.00 |\n| Percentage obese | 0.063 | 0.009 | 6.99 | <.001 | 2.22 |\n| Percentage physically inactive | 0.03 | 0.007 | 5.86 | <.001 | 2.15 |\n| Percentage that walks or cycles to work | −12.46 | 0.580 | −21.38 | <.001 | 1.47 |\n\n【38】Abbreviation: SE, standard error.\n\n【39】#####  Table 2. Results From Geographically Weighted Regression Model of US County-Level Diabetes Prevalence, 2009–2010\n\n| Characteristic | β | Percentage of Counties by 95% of _t_ Statistic |\n| --- | --- | --- |\n| Min | Max | _t_ ≤ −1.96 | −1.96 < _t_ < 1.96 | _t_ ≥ 1.96 |\n| --- | --- | --- | --- | --- |\n| Intercept | 1.60 | 10.7 | 0 | 0 | 100 |\n| Population density | −0.003 | 0.01 | 13.2 | 86.2 | 0.70 |\n| Percentage nonwhite population | −0.04 | 0.09 | 1.00 | 21.6 | 77.4 |\n| Percentage Hispanic population | −0.22 | 0.16 | 30.4 | 68.0 | 1.50 |\n| Percentage living below federal poverty level | −0.02 | 0.14 | 0.00 | 60.2 | 39.8 |\n| Percentage unemployed | −45.4 | 23.4 | 21.4 | 78.1 | 0.60 |\n| Percentage with less than a high school education | −6.66 | 15.0 | 0.10 | 76.3 | 23.6 |\n| Percentage obese | −0.07 | 0.16 | 0.00 | 71.0 | 29.0 |\n| Percentage physically inactive | −0.08 | 0.11 | 5.00 | 81.9 | 13.1 |\n| Percentage that walks or cycles to work | −32.1 | 6.69 | 29.2 | 70.7 | 0.20 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "67593107-8178-4bcd-9eb8-7cda0978912b", "title": "Pandrug-Resistant Acinetobacter baumannii Causing Nosocomial Infections in a University Hospital, Taiwan", "text": "【0】Pandrug-Resistant Acinetobacter baumannii Causing Nosocomial Infections in a University Hospital, Taiwan\n##### Background\n\n【1】National Taiwan University Hospital (NTUH) is a 2,000-bed hospital located in northern Taiwan. The Nosocomial Infection Control Committee of the hospital was established in 1980. Since then, identification of pathogens that cause nosocomial infections and collection and analysis of antimicrobial susceptibility results of these pathogens from the hospital’s clinical microbiology laboratory have been performed  . Definitions for nosocomial infection followed the guidelines of the National Nosocomial Infections Surveillance system .\n\n【2】To determine the secular trend of CRAB, we analyzed data on the disk-diffusion susceptibilities to imipenem of this organism recovered in the period 1993–2000 in NTUH. Organisms were categorized as susceptible, intermediate, or resistant to the antimicrobial agents tested on the basis of guidelines provided by the National Committee for Clinical Laboratory Standards (NCCLS)  . PDRAB described isolates resistant to almost all commercially available antibiotics tested (i.e. ceftazidime, cefepime, ticarcillin-clavulanate, piperacillin-tazobactam, aztreonam, imipenem, meropenem, gentamicin, amikacin, ofloxacin, and ciprofloxacin). Isolates of CRAB, which did not belong to PDRAB, were usually susceptible to ciprofloxacin, ofloxacin, gentamicin, or amikacin.\n\n【3】The annual use of carbapenems (imipenem and meropenem), extended-spectrum cephalosporins (cefotaxime, ceftriaxone, ceftazidime, cefepime), aminoglycosides (gentamicin, tobramycin, netilmicin, and amikacin), and ciprofloxacin, expressed as grams per 1,000 patient-days from 1993 to 2000, was also analyzed. Imipenem was introduced in the hospital in 1990, and cefepime and meropenem have been available since 1997 and 1998, respectively.\n\n【4】##### Bacterial Isolates\n\n【5】We collected 199 consecutive isolates of PDRAB recovered from 72 patients colonized or infected by these organisms from January 1999 to April 2000 and from one patient with bacteremia in May in 1998. Multiple isolates from a single patient were included only if they were recovered from different body sites or recovered from the same body site more than 7 days apart. These isolates were recovered from sputum (isolates), wound pus (isolates), blood (isolates), bronchial washing (isolates), central venous catheter tips (isolates), pleural fluid (isolates), and urine (isolates). Thirty-three of these patients had more than one isolate (range 2 to 13 isolates) collected for this study. Four environmental isolates were recovered from a ventilator monitor board (two isolates) and tips of feeding syringe (two isolates) from an ICU. The isolates were stored at –70°C in trypticase soy broth (Difco Laboratories, Detroit, MI) supplemented with 15% glycerol before being tested.\n\n【6】##### Antimicrobial Susceptibility Testing\n\n【7】MICs of antimicrobial agents for the isolates were determined by means of the agar dilution method, according to guidelines established by NCCLS  . The following antimicrobial agents were provided by their manufacturers for use in this study: ceftazidime (GlaxoSmithKline, Greenford, UK), cefepime and amikacin (Bristol-Myers Squibb Company, Princeton, NJ), flomoxef (Shionogi & Co. Ltd. Osaka, Japan), imipenem (Merck & Co. Inc. Rahway, NJ), meropenem (Sumitomo Pharmaceuticals Co. Ltd. Osaka, Japan), ampicillin-sulbactam and trovafloxacin (Pfizer Inc. New York, NY), and ciprofloxacin and moxifloxacin (Bayer Corporation, West Haven, CN). The isolates were grown overnight on trypticase soy agar plates supplemented with 5% sheep blood (BBL Microbiology Systems, Cockeysville, MD) at 37°C. Bacterial inocula were prepared by suspending the freshly grown bacteria in sterile normal saline and adjusted to a 0.5 McFarland standard. With the use of a Steers replicator, an organism density of 10 4  CFU/spot was spread onto the unsupplemented Mueller-Hinton agar (BBL Microbiology Systems) with various concentrations of antimicrobial agents and incubated at 35°C in ambient air.\n\n【8】##### Time-Kill Determination\n\n【9】Two PDRAB isolates were tested according to methods described previously . Antibiotic combinations tested included imipenem plus amikacin, imipenem plus ciprofloxacin, imipenem plus ampicillin-sulbactam, and ciprofloxacin plus ampicillin-sulbactam. In each case, concentration of MIC and one to eight twofold dilutions lower than the MICs were tested. Viability counts were performed at 0, 2, 4, 8, and 24 hours. Synergy was defined as a decrease of \\> 2 µg/mL in viability count of the combination at 24 h compared to that with the more active of the two agents used alone  .\n\n【10】##### Molecular Typing\n\n【11】Genotyping was determined by the random amplified polymorphic DNA (RAPD) patterns generated by APPCR and by the pulsotypes generated by PFGE. APPCR was performed with two random oligonucleotide primers: OPA-05 and OPA-02 (Operon Technologies, Inc. Alameda, CA) under conditions described previously  . For PFGE, DNA extraction and purification were also carried out as described previously  . DNA was digested by the restriction enzyme _Sma_ I, and the restriction fragments were separated in a CHEF-DRIII unit (Bio-Rad Laboratories, Hercules, CA) at 200 V for 27 h. Interpretation of the PFGE profiles followed the description by Tenover et al. PFGE profiles of the isolates were considered derived from a common ancestor (closely related isolates), if the numbers of fragment differences were three or less  .\n\n【12】### Results\n\n【13】##### Trend of CRAB and PDRAB\n\n【14】The rapidly increasing incidence of CRAB (from 5.88% in 1993 to 21.5% in 2000) and PDRAB (% before 1998 to 6.5% in 2000)as causes of nosocomial infection is shown in Figure 1 . This trend correlates with the increasing use of carbapenem and ciprofloxacin but not with the use of extended-spectrum cephalosporins and aminoglycosides.\n\n【15】##### Antimicrobial Susceptibilities\n\n【16】All PDRAB isolates were also nonsusceptible to all of the antibiotics tested by the agar dilution method . Most (%) of the isolates were intermediate to imipenem, although only 8% of these isolates were intermediate to meropenem. Only 3% of these isolates were susceptible to ampicillin-sulbactam. The MIC 90  of trovafloxacin and moxifloxacin was 16 µg/mL for each.\n\n【17】##### Synergy Tests\n\n【18】The results of the time-kill study of the two isolates tested—one (isolate I) that belonged to clone 5 and the other (isolate II) that belonged to clone 6 —are shown . Only imipenem plus amikacin and imipenem plus ampicillin-sulbactam showed synergy against isolate I; synergy was detected for all four combinations for isolate II. The MICs of the two combinations with synergistic activity for isolate I remained in the resistant ranges. On the other hand, the MICs of two (imipenem plus amikacin and imipenem plus ciprofloxacin) of the four combinations for isolate II were within the susceptible ranges.\n\n【19】##### PFGE and APPCR Analysis\n\n【20】A total of 10 PFGE profiles, pulsotypes A to J, were identified among the isolates recovered from 73 patients . Pulsotype E isolates were further separated into 10 subtypes, subtypes E1 to E10 . Most (.9%) of the subtypes among the strain isolates of pulsotype E were subtype E2, followed by E3 (.2%) and E6 (.5%). For RAPD analysis using the two primers (OPA-02 and OPA-05), 10 RAPD patterns, patterns 1 to 10, were recognized . The 10 patterns correlated well with the 10 pulsotypes. Isolates recovered from various body sites of the same patient had identical pulsotypes and RAPD patterns.\n\n【21】Among the isolates, one clone (clone 5) belonging to pulsotype E and RAPD pattern 5 predominated (isolates) and most isolates (.2%) of this strain were recovered from ICU patients, particularly from November 1999 to April 2000 . The first PDRAB isolate in 1998 belonged to pulsotype A. The first clone 5 isolate was subtype E1, identified in April in an ICU (ICU-2); subtype E2 was found in another unit (ICU-7) in May. All four isolates from the equipment in ICU-4 in February 2000 belonged to clone 5 (subtype E2).\n\n【22】### Discussion\n\n【23】This report describes the trends of nosocomial infections caused by PDRAB in a university hospital and characterizes a hospitalwide epidemic due to these organisms during a 16-month period. Our results suggest three important facets. First, the upward trend in CRAB in the past 8 years and rapid emergence of PDRAB in the last 3 years are impressive. This phenomenon correlated with the level of annual use of ciprofloxacin and carbapenems in the hospital. However, risk factors for acquiring PDRAB should be studied before attributing the emergence of PDRAB clones to carbapenem and other antibiotic consumption, and before implementing a antibiotic- (particularly carbapenem) restriction program as an infection-control measure to eradicate the outbreak. Second, by using PFGE and APPCR, we demonstrated the spread of one epidemic clone in 64 of 73 patients, and nine other genotypes were observed in the outbreak PDRAB isolates. Widespread dissemination of the major clone (clone 5) in all ICUs and in most general wards of the hospital contributed to the rapid emergence of PDRAB in the hospital. Third, contrary to the findings by other investigators , imipenem plus amikacin, ciprofloxacin, and ampicillin-sulbactam, in the combinations tested, exhibited weak activity against the major clone (clone 5) of PDRAB. Newer fluorquinolones (trovafloxacin and moxifloxacin) also had limited potency against these PDRAB isolates.\n\n【24】Isolates of _A. baumannii_ , particularly those recovered from patients with nosocomial infections, are frequently resistant to multiple antimicrobial agents, including cephalosporins, aminoglycosides, and quinolones . Imipenem is the most effective agent against this organism. However, with the increasing use of carbapenems and other antibiotics (such as ciprofloxacin and amikacin), particularly in institutions that have an increasing incidence of extended-spectrum β-lactamase–producing _Enterobacteiraceae_ or those with hyperproduction of AmpC enzymes, the rapid and progressive emergence of CRAB and PRRAB is unavoidable . This phenomenon was illustrated in many countries as well as in numerous major teaching hospitals in Taiwan, including NTUH .\n\n【25】Different mechanisms have been involved in _A. baumannii_ isolates resistant to cephalosporins and carbapenems: the altered penicillin-binding proteins, the presence of various types of β-lactamases, and the loss of porins . Investigation of these resistance mechanisms in our PDRAB isolates is ongoing. Although resistance emerged after considerable pressure from carbapenem use in our hospital, molecular typing approaches demonstrated that the rapid emergence of PDRAB was less likely caused by the acquisition of different resistant mechanisms by preexisting multiple clones than by the introduction of a new clone (clone 5). After detection of the first clone 5 isolate, this strain was found in many infected or colonized ICU patients as well as in patients admitted in the general wards—despite the implementation of isolation precaution and environmental surveillance. At the end of this study, the epidemic is still occurring. Further control measures such as restriction of carbapenem use (particularly in ICUs), intensification and modification of cleaning procedures for contaminated equipment, and cohorting the patients infected or colonized with CRAB or PDRAB are now being undertaken.\n\n【26】Previous studies of gram-negative bacilli, such as combinations of a β-lactam with amikacin, which were synergistic in vitro, have been associated with better outcomes than those achieved with nonsynergistic regimens, particularly in debilitated patients with severe infections  . In recent reports on multidrug-resistant _A. baumannii_ isolates, combinations of imipenem plus amikacin or tobramycin had better bactericidal activity against these isolates than imipenem plus sulbactam  . Moreover, for isolates with a high MIC of amikacin (> 32 µg/mL) and ciprofloxacin (> 4 µg/mL), amikacin, or ciprofloxacin MICs in combination with in vitro synergy were not achievable clinically  . Our study partly supports these findings. Although synergy was detected for combinations of imipenem plus amikacin and imipenem plus ampicillin-sulbactam, MICs of these agents exceeded the levels achievable in plasma, suggesting their limited potential as treatment regimens. Some of our patients with bacteremia due to clone 5 _A. baumannii_ could be treated successfully with a higher dose of imipenem (g/day) plus amikacin. However, this regimen could not eradicate the organisms from respiratory secretions and wound pus (data not shown). Further in vitro and in vivo studies should be conducted to establish the treatment guidelines for CRAB or PDRAB infections.\n\n【27】In summary, we report a nosocomial outbreak due to a major clone of PDRAB in a hospital with widespread carbapenem use. This new emerging PDRAB can be considered a harbinger of the so-called post-antibiotic era. To confront the imminent threat of untreatable infection caused by this organism, a correct antibiotic strategy should be addressed, and strict compliance with basic and potential control measures for the containment of infection should be instituted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e90ab441-a505-44f8-a0c1-f28f4434b74e", "title": "Replicative Capacity of MERS Coronavirus in Livestock Cell Lines", "text": "【0】Replicative Capacity of MERS Coronavirus in Livestock Cell Lines\nCoronaviruses (CoV) in the genera _Alphacoronavirus_ and _Betacoronavirus_ (order _Nidovirales_ , family _Coronaviridae_ , subfamily _Coronavirinae_ ) infect a broad range of mammals, including humans . The human CoVs (HCoVs) HCoV-HKU1, HCoV-229E, HCoV-NL63, and HCoV-OC43 typically cause mild to moderate respiratory tract infection; however, the disease course can be more severe in a minority of patients. In 2002–2003, an epidemic of severe lower respiratory tract infection with a case-fatality rate of ≈10% was caused by severe acute respiratory syndrome (SARS)–CoV . In 2012, another CoV associated with severe respiratory disease emerged on the Arabian Peninsula and was termed Middle East respiratory syndrome (MERS)–CoV .\n\n【1】Both SARS-CoV and MERS-CoV are zoonotic viruses, and their presumed origin is in bats. SARS-related CoVs were identified in _Rhinolophus_ spp. bats in China and Europe , and MERS-related CoVs were found in _Pipistrellus_ bats in Europe and in _Neoromicia_ bats in South Africa . As with SARS-CoV, it is expected that MERS-CoV might be transmitted to humans by an intermediate animal host, and neutralizing antibodies against MERS-CoV have been found in Arabian camels originating from Oman, Spain, and Egypt .\n\n【2】We investigated replication of MERS-CoV in cell lines of the most abundant mammalian livestock species and representative peridomestic small mammals on the Arabian Peninsula. To estimate MERS-CoV permissiveness of cell cultures derived from these animals, we compared MERS-CoV replication and infectious virus production with that in bat- and primate-derived cells known to be permissive for MERS-CoV. The MERS-CoV receptor dipeptidyl peptidase 4 (DPP-4) is expressed in epithelial cells of the lung and kidney, and patients with MERS-CoV consistently show severe involvement of both organs; thus, we focused on lung and kidney cells in potential animal hosts .\n\n【3】### The Study\n\n【4】Using enhanced respiratory personal protection equipment in a Biosafety Level 3 facility, we cultivated, in parallel, cell lines from goats, sheep, cattle, camelids (dromedary and alpaca), rodents, insectivores, bats, and human and nonhuman primates . Cells were checked to ensure the absence of mycoplasma contamination and genotyped for their species of origin by sequencing of the mitochondrial cytochrome c subunit oxidase I gene . All cells expressed DPP-4, as determined by immunofluorescence staining  and Western blot analysis (data not shown). Because several of the ungulate cell lines had not previously been used for viral infection experiments, we determined permissiveness of all cell lines for Rift Valley fever virus (RVFV) clone 13, a virus mutant known to be attenuated yet broadly infectious for ungulate cell lines . Triplicate infections with multiplicities of infection (MOIs) of 0.5 infectious units/cell resulted in highly consistent levels between cells (maximal variation 3.2-fold) . In addition, to demonstrate the ability of the cells to support CoV replication, we infected all cell lines in triplicate (MOIs of 0.5) with bovine CoV strain Nebraska. The strain replicated to high levels in all cell lines; replication varied by < 52.9-fold, which constitutes small relative variations in light of the overall levels of replication . We conducted MERS-CoV infections under the same conditions and with MOIs of 0.5.\n\n【5】In addition to livestock cell lines, we used rodent, insectivore, bat, and primate cell lines in the experiments . Bat and primate cells known to be permissive for MERS-CoV served as controls. MERS-CoV–inoculated cells were incubated for 1 h and then washed twice before supernatant was harvested (h after incubation) . We quantified virus replication by using the _upE_ assay , a MERS-CoV real-time reverse transcription PCR that screens upstream of the _E_ gene. Replication was seen in lung and kidney cell lines derived from goats (Capra hircus_ ) and in umbilical cord and kidney cells from camelids (Camelus dromedarius_ and _Llama pacos_ ) . Efficient replication (> 9.3 log 10  virus RNA genome equivalents/mL of cell culture supernatant) was seen in the goat kidney cells; this replication level was similar to that in Vero E6 cells (interferon-deficient primate kidney cells). Goat lung cells, alpaca kidney cells, and dromedary umbilical cord cells also showed strong replication, but virus did not replicate in sheep, cattle, rodent, or insectivore cells. In addition, expression of the receptor, DPP-4, was confirmed in all cells, including the nonpermissive sheep, cattle, rodent, and insectivore cells.\n\n【6】After following virus growth in all permissive cells for another 20 h, we harvested supernatants and, to confirm the production of infectious virus particles, we titrated the supernatants by using a plaque assay in Vero cells. MERS-CoV replication was seen in all permissive cells except TT-R.B , and all permissive cells showed cytopathic effects. The highest production of virus particles was in goat lung and kidney cells (.0 × 10 7  and 2.7 × 10 6  PFU/mL, respectively). This level of replication was comparable to that in human lung cells (A549) and Vero E6 .\n\n【7】### Conclusions\n\n【8】Transmission of MERS-CoV between humans is still limited, and the identification of an intermediate animal host could enable the development of public health measures to prevent future spread of the virus among humans. Although MERS-CoV neutralizing antibodies have been detected in camels from Oman, Spain, and Egypt, the virus has not previously been detected in camels . An informed focusing of investigations on a select group of species, such as camels, could benefit epidemiologic investigations. To identify potential intermediate host species of MERS-CoV, we used in vitro testing to determine virus permissiveness in select cell culture models. In general, cell lines cannot depict the full pathogenicity of in vivo infection because infection is influenced by epithelium-specific differentiation of target cells and the presence of immune cells. However, for viruses such as CoVs, whose tropism is believed to be determined mainly by the availability of an appropriate entry receptor , epithelial cell cultures could indeed constitute valid surrogates of virus permissiveness in vivo. With these limitations in mind, our results are in concordance with the findings of MERS-CoV neutralizing antibodies in camels and with information regarding patient contact with animals in reports of 2 human cases of MERS-CoV infection . One of the patients owned a farm on which camels and goats were kept. Before onset of his own illness, the patient reported illness in several goats on his farm. The patient did not have direct contact with animals, but he reported having eaten goat meat and having had contact with one of the animal caretakers, who suffered from respiratory disease . The second patient reported direct contact with a diseased camel shortly before onset of his symptoms .\n\n【9】In our study, production of infectious virus particles was seen in goat lung and kidney cells and in camelid kidney cells. Excretion patterns indicative of kidney infection should be investigated once further clues to the identity of the MERS-CoV animal reservoir become available. Our preliminary findings suggest that ungulates, such as goats and camels, are a possible intermediate host of MERS-CoV; thus, exposure to urine and feces from these animals might constitute a source of human infection. Moreover, food products derived from these animals (e.g. meat and milk) should be tested for their potential to transmit MERS-CoV. The results of our study suggest that investigations into the MERS-CoV animal reservoir and intermediate host should focus on caprid (e.g. goats) and camelid hosts, and we identified several new cell lines for use in virus isolation studies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a1d1b4df-558b-41b8-a968-4d7a200b6f45", "title": "Varicella in Adult Foreigners at a Referral Hospital, Central Tokyo, Japan, 2012–2016", "text": "【0】Varicella in Adult Foreigners at a Referral Hospital, Central Tokyo, Japan, 2012–2016\nVaricella is a benign disease with fever and rash caused by primary infection with varicella zoster virus (VZV) . Varicella is usually self-limiting but can sometimes be life-threatening, especially in adults . The World Health Organization estimates that 140 million varicella cases and 4,200 related deaths occur each year . In Japan, 1 million persons, primarily young children, have varicella every year. Of these patients, 4,000 require hospitalization, and adults make up ≈60% of this population .\n\n【1】Varicella vaccine is a live attenuated vaccine and is effective against varicella in healthy children . One dose of varicella vaccine reduces the number of varicella cases but can also reduce opportunities for exposure to the virus in the community and result in breakthrough varicella. Therefore, many industrialized countries now recommend 2 doses of varicella vaccine . In Japan, the 2-dose varicella vaccination became part of the childhood immunization schedule in October 2014; the number of varicella cases has drastically decreased since implementation .\n\n【2】We report cases of varicella in adult foreigners who were given a diagnosis at the National Center for Global Health and Medicine (NCGM) Medical Center in central Tokyo, Japan. These cases highlight major health issues, including differences in varicella vaccination programs in countries and VZV epidemiology in temperate and tropical regions, and identify immigrants and international students as high-risk populations for varicella.\n\n【3】### The Study\n\n【4】We reviewed 22 varicella cases in adult foreigners diagnosed during January 2012–December 2016 at the NCGM. During the same period, 7 cases of varicella were diagnosed in Japanese adults at the NCGM. We compiled basic demographic information for the patients . Eleven case-patients were from Vietnam, 5 from China, and 1 each from 6 other countries (Cambodia, New Zealand, Belgium, Italy, Uzbekistan, and Nepal). Among 21 patients who resided in central Tokyo, 18 were international students. Fourteen patients were male, and the median age was 19 years (range 18–35 years).\n\n【5】The patients sought medical care on median day 2 of illness (range days 1–5). Most diagnoses of varicella were made clinically; for some case-patients, varicella was confirmed by serologic testing or PCR or antigen detection testing of skin lesions. Twenty patients received antiviral therapy (acyclovir or valacyclovir). Five case-patients were hospitalized because of severe malaise and for infection control to prevent secondary cases.\n\n【6】Nine patients from Vietnam and 1 from China (case-patients 3–12) were students at the same language school who lived together in the school dormitories. During this outbreak, varicella developed in 9 students during a 2-month period after the index case (case 3). Information about previous vaccination and past infection could not be obtained for most patients in this outbreak because of their inability to communicate in Japanese or English and our inability to organize translation in the first language of the patients immediately at their visits. Overall, we found that there were 81 international students in 3 dormitories: 30 (.0%) reported previous varicella and 6 (.4%) claimed to have been vaccinated at least once. Although we contacted the school to provide postexposure prophylaxis, none of 41 students without previous VZV infection had received it.\n\n【7】### Conclusions\n\n【8】Japan has >2 million foreign residents; most are from China (%), South Korea (%), and the Philippines (%) . However, Japan does not have any vaccination requirements for foreign residents. Because varicella is not a target of the World Health Organization Expanded Program on Immunization, the vaccine is not included in the national immunization schedules of many countries.\n\n【9】In China, from which Japan receives most of its immigrants and tourists, varicella vaccine was approved in the late 1990s and has been given on a voluntary basis . Although 80% of children in kindergartens and primary schools in Beijing were reported to have received the varicella vaccine, most had only received 1 dose, and breakthrough varicella has become a public health concern . In South Korea, the single-dose varicella vaccination was included in the national immunization program in 2005 and is recommended for 12–15-month-old children . Despite high use of the vaccine (>95%), varicella outbreaks continuously occur . The varicella vaccine has not been introduced into the national vaccination program of the Philippines .\n\n【10】The epidemiology of varicella differs between temperate and tropical regions. In temperate areas, most of the population contracts VZV before adolescence. In tropical areas, childhood varicella is much less common, and many persons stay susceptible to VZV until adulthood . This difference is attributed to changes in VZV transmissibility by temperature and humidity, population density, and sociologic factors, such as nursery school attendance .\n\n【11】Previous research has shown immigrants and refugees to be at high risk for varicella. A study in Denmark during the 1980s investigated an epidemic of varicella among refugees during a preasylum phase . The study showed that 44% of Tamil refugees from Sri Lanka had varicella infections within 1 week to 5 months after arrival in Denmark. Considering that many immigrants contracted varicella shortly after their arrival, it was deemed necessary to check VZV immunity for immigrants before they entered that country, rather than after entry.\n\n【12】In an outbreak study of 18 varicella cases among Mexican-born workers in Alabama, USA , Mexican-born workers were 5 times more likely to be susceptible to VZV than US-born workers. Although postexposure prophylaxis vaccine was provided to susceptible workers, a second dose was not administered to many of those who needed it. This finding shows the difficulty in reaching these populations because of their lack of stable access to healthcare. Faced with a rapidly aging population, Japan has also been accepting nurses and caregivers from countries in Southeast Asia as medical personnel in healthcare facilities and nursing homes . Thus, nosocomial infection with VZV is also a matter of concern.\n\n【13】Japan had 208,000 international students during 2015 . More than 90% of these students were from countries in Asia; most were from China (%), Vietnam (%), and Nepal (%). The demographics of the case-patients in our study were consistent with these data. In general, colleges are suitable environments for communicable disease outbreaks because of highly dense interactions among students and staff, crowded living conditions, and an influx of students from many countries . Because 10%–20% of persons in Japan 15–30 years of age are susceptible to VZV , varicella outbreaks among international students might spread into the local student population. Despite these facts, Japan does not have any regulations or recommendations regarding the health of international students. In the United States, the American College Health Association strongly recommends institutional prematriculation immunizations . These guidelines recommend that all college students should have 2 doses of varicella vaccine unless they have other evidence of VZV immunity.\n\n【14】This varicella case series occurred in multifactorial contexts. However, little is known about the immunity against VZV among foreign residents in Japan. Educational institutions that receive international students need to consider varicella as a major preventable health issue among their students. Healthcare providers for immigrants and international students should also be aware of the risk for varicella and should verify VZV immunity or varicella vaccination status for students.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7dc6818f-b24a-4557-8d5a-0d0c22b71074", "title": "Cluster Analysis and Cluster Ranking for Asthma Inpatient Hospitalizations Among Children, Adolescents, and Adults Aged 0 to 19 Years in Cook County, Illinois, 2011–2014", "text": "【0】Cluster Analysis and Cluster Ranking for Asthma Inpatient Hospitalizations Among Children, Adolescents, and Adults Aged 0 to 19 Years in Cook County, Illinois, 2011–2014\nBackground\n----------\n\n【1】Asthma is one of the top 5 principal diagnoses for inpatient hospitalizations among children and adolescents aged 1 to 17 years in the United States . Inpatient hospitalizations are used as a marker for severe asthma symptoms, suggesting poor management of disease and limited access to routine care . Preventing adverse childhood asthma outcomes is a priority of the Illinois Department of Public Health in collaboration with community and university partners . To understand the burden of asthma among young people in the greater Chicago (Cook County) area, we mapped crude rates of asthma inpatient hospitalizations among children, adolescents, and young adults aged 0 to 19 years, by zip code, in Cook County, Illinois, during 2011–2014. Additionally, we performed a cluster analysis to identify neighborhoods with high rates of asthma inpatient hospitalizations.\n\n【2】Data Sources and Map Logistics\n------------------------------\n\n【3】We obtained inpatient hospitalization records from the Illinois Department of Public Health Division of Patient Safety and Quality. To be included in our analysis, a patient record had to indicate the following: residence in Cook County, Illinois; age 0 to 19 years at hospital admission; discharge from January 1, 2011, through December 31, 2014; and a principal _International Classification of Diseases, Ninth Revision, Clinical Modification_ (ICD-9-CM) diagnosis code of 493.XX (asthma) . We aggregated each record by zip code of residence after assessing that ZIP Code Tabulation Areas  and zip codes were well aligned for our data set. We obtained aggregated data for Cook County residents hospitalized out of state from the bordering states of Indiana, Iowa, Missouri, and Wisconsin, and we added these data to the final residential zip code counts.\n\n【4】We calculated the crude rate of annual asthma inpatient hospitalizations for each Cook County zip code by dividing the number of asthma inpatient hospitalizations during 2011–2014 by 4 times the 2014 American Community Survey population estimates for each zip code (ie, 4 years “at risk,” assuming a steady population size during 2011–2014) . We expressed rates as the number of asthma inpatient hospitalizations per 10,000 residents aged 0 to 19 years. We did not age-standardize rates because the proportion of the population aged 0 to 4 years and the proportion of the population aged 5 to 19 did not differ significantly by zip code. We created a map to illustrate the geographic distribution of crude rates by using Esri’s ArcMap version 10.4. For this map, we suppressed zip codes with fewer than 10 asthma inpatient hospitalizations during the 4-year study period. To inform the reference cutpoints in mapping the geographic distribution of hospitalization rates, we used Healthy People 2020 objective RD-2.2: 8.7 inpatient hospitalizations per 10,000 persons aged 5 to 64 . We created 4 categories of crude rate: 0 to 8.7, 8.8 to 17.4, 17.5 to 26.1, and more than 26.1 per 10,000.\n\n【5】We used SaTScan version 9.4.4, a free spatial scan statistics software , to identify clusters of zip codes with high rates of asthma inpatient hospitalizations. Briefly, scan statistics assess a constant rate hypothesis by using a scanning “window” in varying sizes, which moves across the study region . Of interest is whether the rate for the area inside the window is equal to the rate for the area outside the window. In our analysis, we looked for high-rate clusters, areas in which the number of asthma inpatient hospitalizations in a group of one or more zip codes (inside the window) was significantly greater than the number of asthma inpatient hospitalizations that would be expected if the hospitalization rates inside and outside the zip code group were equivalent. We used a Poisson distribution model and the Gini coefficient to identify the optimal cluster reporting size . After setting the maximum scanning window size at 50%, SaTScan found 3% to be the optimal maximum (Gini coefficient = 0.21) . We specified 999 Monte Carlo replications for the analysis. Output specification included all significant clusters (P_ < .05) ordered by log likelihood ratio (a measure of whether the rate inside the scanning window is higher than expected). We mapped all high-rate clusters in Esri’s ArcMap version 10.4. To maximize the utility of our study data for public health decision making, we ranked the clusters in 3 ways: by the observed number of inpatient hospitalizations; by the size of the population aged 0 to 19 years in each cluster; and by the relative excess rate of asthma inpatient hospitalizations, calculated as the observed number of inpatient hospitalizations divided by the expected number of inpatient hospitalizations.\n\n【6】Highlights\n----------\n\n【7】We found 11,456 asthma inpatient hospitalizations among children, adolescents, and young adults aged 0 to 19 years in Cook County, Illinois, during 2011–2014. Zip code–specific rates ranged from 4.0 to 65.8 hospitalizations per 10,000 persons aged 0 to 19 years with the highest rates 2 to 3 times greater than the Cook County rate of 20.0. Of 174 zip codes in the study area, 31 had a crude hospitalization rate of more than 26.1 per 10,000 persons aged 0 to 19 years (Map A).\n\n【8】We identified 13 high-rate clusters of asthma inpatient hospitalizations (Map B and Table 1 ). These clusters were located primarily in the western and southern areas of Chicago and southern Cook County. Together, the zip codes in these 13 high-rate clusters comprised 44.9% (of 11,456) of asthma inpatient hospitalizations and 25.4% (of 5,741,061) of young people aged 0 to 19 years in Cook County. The degree of precision in relative excess rate estimates varied because of differences in the population size and the number of asthma inpatient hospitalizations in each cluster. During the 4-year study period, cluster A had the largest number of inpatient hospitalizations (n = 704), cluster M had the smallest total population (n = 18,317), and cluster D had the highest relative excess rate (2.78 \\[95% confidence interval, 2.38–3.18\\] .\n\n【9】Action\n------\n\n【10】We identified substantial variation in the number of asthma inpatient hospitalizations by zip code in Cook County. The grouping of zip code clusters by neighborhood will be useful to organizations working in these neighborhoods. Our ranking of asthma clusters can be used to examine zip codes in which interventions can be introduced or accelerated. The ranking by relative excess rate may be most useful to organizations concerned with geographic disparities in asthma inpatient hospitalization rates, whereas data on total number of asthma inpatient hospitalizations may be most useful to organizations interested in programs or policies to reduce the absolute numbers of young people with asthma. Alternatively, the ranking by total size of the population aged 0 to 19 years may be most relevant when resources are limited or when introducing resource-intensive care coordination programs. These priorities can be combined to best address the needs and interests of the community.\n\n【11】One limitation of this ranking framework is the inability to assess the unique drivers of risk in each cluster; each set of unique drivers may require its own uniquely designed intervention. Assessment of the modifiable characteristics of each cluster may lead to targeted health interventions. Other limitations were an inability to identify hospital readmissions and an inability to detect false-positive clusters caused by repeated significance testing. False positive concerns are often addressed by considering only the 2 most likely clusters. However, excluding information on all potential clusters may not align with public health priorities.\n\n【12】Our study adds a framework for combining public health decision-making considerations and exploratory spatial statistical tools. We will continue to investigate the application and value of cluster analysis ranking to inform asthma programs for young people in Illinois.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6aeea0bc-cee2-4995-ad1f-9b7718d165e4", "title": "Menu-Labeling Usage and Its Association with Diet and Exercise: 2011 BRFSS Sugar Sweetened Beverage and Menu Labeling Module", "text": "【0】Menu-Labeling Usage and Its Association with Diet and Exercise: 2011 BRFSS Sugar Sweetened Beverage and Menu Labeling Module\nAbstract\n--------\n\n【1】**Introduction**  \nThe primary objective of our study was to investigate the association between menu-labeling usage and healthy behaviors pertaining to diet (consumption of fruits, vegetables, sodas, and sugar-sweetened beverages) and exercise.\n\n【2】**Methods**  \nData from the 2011 Behavioral Risk Factor Surveillance System, Sugar Sweetened Beverage and Menu Labeling module, were used. Logistic regression was used to determine the association between menu-labeling usage and explanatory variables that included fruit, vegetable, soda, and sugar-sweetened beverage consumption as well as exercise.\n\n【3】**Results**  \nNearly half (%) of the sample indicated that they used menu labeling. People who used menu labeling were more likely to be female (odds ratio \\[OR\\], 2.29; 95% confidence interval \\[CI\\], 2.04–2.58), overweight (OR, 1.13; 95% CI, 1.00–1.29) or obese (OR, 1.29; 95% CI, 1.12–1.50), obtain adequate weekly aerobic exercise (OR, 1.18; 95% CI, 1.06–1.32), eat fruits (OR, 1.20; 95% CI, 1.12–1.29) and vegetables (OR, 1.12; 95% CI, 1.05–1.20), and drink less soda (OR, 0.76; 95% CI, 0.69–0.83).\n\n【4】**Conclusion**  \nAlthough obese and overweight people were more likely to use menu labeling, they were also adequately exercising, eating more fruits and vegetables, and drinking less soda. Menu labeling is intended to combat the obesity epidemic; however, the results indicate an association between menu-labeling usage and certain healthy behaviors. Thus, efforts may be necessary to increase menu-labeling usage among people who are not partaking in such behaviors.\n\n【5】Introduction\n------------\n\n【6】Poor nutrition and obesity are major public health concerns . In 2011, approximately 69% of adults in the United States were classified as overweight or obese ; if the trend continues, by 2020, an estimated 80% of Americans will be overweight or obese . Chronic diseases such as cardiovascular disease, type 2 diabetes, osteoarthritis, and psychological illness are associated with obesity and contribute to increased risk of early death and poor quality of life . In addition, these illnesses are associated with billions of dollars in annual health care costs . Although a 2006 meta-analytic review of 64 obesity prevention programs and interventions showed that most have been unsuccessful , it is important to continue to address the issue because of the growing burden of disease and disability related to obesity.\n\n【7】Obesity is primarily a lifestyle-related condition; it is most often related to a person’s physical activity and nutritional habits. Specifically, it has been linked to consuming few servings of fruits and vegetables and a high level of sweetened beverage consumption . Although the cause of obesity is complex, frequently dining outside the home is a risk factor for significant weight gain . Foods prepared in restaurants are generally more caloric and higher in fat, sodium, and sugar than foods prepared in the home . The percentage of food expenditures outside the home has increased from 32% in 1980 to 44% in 2010 , and more than 50% of Americans eat at fast-food restaurants and other commercially prepared meal establishments approximately 3 times a week .\n\n【8】The increase in eating outside the home prompted legislation encouraging healthy eating choices in restaurants. One such legislation model recommends or mandates listing nutritional information next to menu items to increase awareness of the nutritional content of restaurant items and influence healthier food choices . Since 2006, several cities and states in the United States have passed laws requiring menu labeling in restaurants. In 2010, as a part of the Patient Protection and Affordable Care Act (PPACA) , a law was passed requiring restaurants with 20 or more locations to list caloric information and other nutritional facts about menu items near the point of purchase. Although the implementation of this law has been delayed, many restaurants have already begun displaying calorie and nutritional information.\n\n【9】Although the purpose of displaying such information is clear, the effects are not. Past studies have reported either limited or no differences in nutritional choices when people were provided point-of-purchase menu labeling while dining at a restaurant . However, it has also been reported that people who used menu labeling to determine calorie content consumed significantly fewer calories during a meal compared with people who did not use menu labeling . Additionally, most studies on the effects of menu labeling have been limited to a student population  or were restricted to samples from only 1 city (eg, New York, New York ; Philadelphia, Pennsylvania ; Stillwater, Oklahoma ). There is a paucity of research on menu labeling on a larger scale. Moreover, little research has been done comparing the differences between people who use menu labeling and those who do not. Many studies considered only the effects that menu labeling have on calorie intake  while failing to investigate the differences in characteristics between users of menu labeling and nonusers. For this study, we conjectured that users of menu labeling tend to live a healthier lifestyle that includes healthy eating habits and regular exercise while nonusers are people living lifestyles more conducive to becoming overweight or obese. If this were the case, menu labeling (intended to increase awareness of nutritional content and influence healthier food choices) may not help the people who would benefit the most. Thus, the primary objective of our study was to investigate the association between menu-labeling usage and healthy behaviors pertaining to exercise and diet (consumption of fruits, vegetables, sodas, and other sweetened sugary beverages).\n\n【10】Methods\n-------\n\n【11】Data from the 2011 Behavioral Risk Factor Surveillance System (BRFSS)  were used. In 2011, the Sugar Sweetened Beverages and Menu Labeling module (Module 4)  was administered for the first time, with 3 states, Hawaii, Minnesota, and Wisconsin, implementing the module. The module consisted of 3 questions: the first pertaining to soda consumption, the second to sweetened fruit drinks, and the third to menu-labeling usage at fast-food and chain restaurants. Only participants who responded to questions in Module 4 were included in our study sample. In addition, participants were excluded if they indicated that they “do not eat at fast food or chain restaurants” (n = 1,788), “didn’t know” if they used menu labeling (n = 85), “usually could not find the menu labeling” (n = 97), or refused to answer the question (n = 12). The final sample consisted of 23,951 participants.\n\n【12】### Outcome variable\n\n【13】Menu-labeling usage was dichotomized (use vs nonuse) based on the response to the survey item that read, “The next question is about eating out at fast food and chain restaurants. When calorie information is available in the restaurant, how often does this information help you decide what to order?” Participants who answered either “always,” “most of the time,” “about half the time,” or “sometimes” were collapsed into users (n = 12,587). Participants who answered either “never” or “never noticed or never looked for calorie information” were collapsed into nonusers (n = 11,364).\n\n【14】With no menu-labeling laws enacted in these states at the time of the survey, it is difficult to ascertain what information was available to the subjects. However, because the question in the module specifically asks about calorie information, which is commonly displayed on most menu labels, we assumed that the subjects would be classified as menu-labeling users if they had access to the calorie information.\n\n【15】### Explanatory variables\n\n【16】The primary explanatory variables in regard to diet were consumption of fruits, vegetables, sugar-sweetened beverages, and sodas. Fruit and vegetable consumption information were extracted from BRFSS 2011 Core Section 9: Fruit and Vegetables . The information was converted into fruit servings per day and vegetable servings per day. Information on soda and other sugar-sweetened beverage consumption was obtained from Module 4, questions 1 and 2: “About how often do you drink regular soda or pop that contains sugar? Do not include diet soda or diet pop.” and “About how often do you drink sweetened fruit drinks, such as Kool-aid, cranberry, and lemonade? Include fruit drinks you made at home and added sugar to.” The information was converted into soda consumption per day and sweetened fruit beverage consumption per day. In addition, information from BRFSS 2011 Core Section 10: Exercise  was used to create the primary explanatory variable for exercise. Whether a participant had attained the recommended aerobic guidelines was measured via a variable provided among the 2011 BRFSS calculated variables, specifically, “\\_PAINDEX”. This variable indicated whether a subject met daily guidelines for daily aerobic exercise . Other covariates included body mass index (BMI), self-perception of general health, age, sex, education level, and annual household income. We did not include race/ethnicity as a covariate because the 3 states used for the analysis did not have enough diversity for a meaningful comparison; however, state was included as a covariate in the full model.\n\n【17】### Statistical analysis\n\n【18】The data were analyzed using SAS 9.3 (SAS Institute Inc, Cary, North Carolina) adjusting for the complex structure of the BRFSS. Univariate analyses were performed using a simple logistic regression model or a χ 2  test to examine the association between menu labeling usage and various factors. Multivariable logistic regression was used to control for any potential confounders. Statistical tests were determined to be significant for _P_ values < .05, but _P_ values < .10 were also reported as being marginally significant.\n\n【19】Results\n-------\n\n【20】Nearly half of the sample indicated using menu labeling (%) . Menu-labeling users were more likely to be female (% vs 39%), have an annual household income of $50,000 or more (% vs 43%), and exercise according to the aerobic exercise guidelines (% vs 53%). Similar distribution of BMI categories were found for both users and nonusers. People were more likely to use menu labeling, on average, if they consumed more fruits (OR, 1.43; 95% CI, 1.34–1.54), more vegetables (OR, 1.32; 95% CI, 1.25–1.40), fewer sugar-sweetened beverages (OR, 0.88; 95% CI, 0.80–0.97), and fewer sodas (OR, 0.70; 95% CI, 0.64–0.76).\n\n【21】Multivariable logistic regression was used to further examine the association between menu-labeling usage and the adjusted effects of diet and exercise while controlling for potential confounders . The results were mostly consistent with the univariate analyses, which showed that people were more likely to use menu labeling if they met aerobic exercise guidelines (OR, 1.18; 95% CI, 1.06–1.32), consumed more fruits (OR, 1.20; 95% CI, 1.12–1.29), consumed more vegetables (OR, 1.12; 95% CI, 1.05–1.20), and consumed less soda (OR, 0.76; 95% CI, 0.69–0.83). The consumption of sweetened fruit beverages was not significant in the multivariable analysis. The initial multivariable analysis also indicated that, compared with those who are in the underweight or normal weight BMI categories, those who are overweight (OR, 1.13; 95% CI, 1.00–1.29, _P_ \\= .06) and obese (OR, 1.29; 95% CI, 1.12–1.50, _P_ < .001) were more likely to use menu labeling.\n\n【22】Stratified analyses were conducted for each BMI category to determine why menu-labeling users tend to have healthier dietary and exercise habits than do nonusers of menu labeling yet at the same time tend to be overweight or obese . People in the obese category were more likely to use menu labeling if they met aerobic exercise guidelines (OR, 1.27; 95% CI, 1.03–1.56), consume less soda (OR, 0.84; 95% CI, 0.73–0.96), and consume more fruits (OR, 1.24; 95% CI, 1.08–1.42). Vegetable consumption was marginally significant (OR, 1.11; 95% CI, 0.99–1.25, _P_ \\= .09) within the obese category. Similarly, within the overweight category, those who consumed less soda (OR, 0.72; 95% CI, 0.64–0.82) and consumed more fruits (OR, 1.16; 95% CI, 1.04–1.30) tended to used menu labeling more. Meeting aerobic exercise guidelines was marginally significant (OR, 1.16; 95% CI, 0.97–1.38, _P_ \\= .09) within the overweight category.\n\n【23】Discussion\n----------\n\n【24】Menu labeling was used by nearly 50% of the study population; this percentage was considerably higher than past estimates (%–33%) . Our finding may be because these previous studies used a small sample size or a sample restricted to a single city. Although our study includes only 3 states, the sample size is much larger and the sample is representative of the entire state . Our results indicated that people were more likely to use menu labeling if they were adequately exercising, eating more fruits and vegetables, and drinking less soda. Past studies demonstrated that menu labeling may contribute to a decrease in calories consumed . Because we have no measures of actual calorie consumption, we cannot conclude whether menu-labeling usage is the major determinant in decreasing calorie intake or whether people using menu labeling are already participating in healthy behaviors, and hence would naturally consume fewer calories regardless of menu-labeling usage. More research on this matter is needed, because if the former is true, simply increasing menu-labeling usage among the public would lead to intake of fewer calories overall, whereas if the latter is true, then further efforts to increase menu-labeling usage among people who are not participating in such healthy behaviors would be needed.\n\n【25】Harnack and colleagues  reported no significant differences with respect to BMI and menu-labeling usage. Although the unadjusted univariate results from our study showed similar findings, the results of the adjusted analysis showed that those who are overweight and obese were more likely to use menu labeling than those who were underweight or normal weight. Hence, at first glance it may seem that menu labeling is being used by the groups most at risk. However, these results are somewhat misleading. Stratified analyses of the data indicate that even among those who are overweight and obese, the people who are using menu-labeling are those participating in healthy behaviors. These people tended to meet aerobic exercise guidelines, consumed more fruits, and consumed less soda than those who did not use menu labeling, suggesting that menu labeling may not be benefiting those who do not partake in such behaviors.\n\n【26】Among the other covariates, menu labeling was more than twice as likely to be used if a person was female. This finding is probably because women are more likely than men to engage in nutrition and exercise activities . Women are more likely than men to read the nutrition labels on food items, and when a woman reads nutritional labels, she is more likely to focus on the total amount of calories . Efforts may be warranted to increase the usage of menu labeling among men.\n\n【27】To our knowledge, this is the first large study on menu labeling; however, several limitations are noted. First, all measures on the BRFSS are self-reported by the participants, which may result in bias. Second, although the BRFSS is structured to provide nationally representative estimates, Module 4 that pertains to menu-labeling usage was administered in only 3 states (Minnesota, Wisconsin, and Hawaii). State was not a significant factor in the model, indicating that combining data from these states was not an issue. However, the results of this study may not be truly representative of the national population. Even so, because Module 4 was implemented for the first time in the 2011 BRFSS, this study may serve as the baseline study for future years when more states implement the module. Third, because of the cross-sectional nature of the BRFSS, it is unknown whether the people using menu labeling were participating in healthy behaviors before they began using menu labeling. Moreover, because no measures of calorie consumption are collected, direct comparisons to other studies with such measures are not possible. Future studies could investigate the lifestyle habits before and after menu-labeling usage while controlling for calorie intake.\n\n【28】Menu labeling is intended to foster a nutritional and health behavior change; however, our study results show an association between menu-labeling usage and participation in healthy behaviors. People using menu labeling reported meeting recommended exercise guidelines, consuming more fruits, and consuming less soda than those who did not use menu labeling. More research is needed to add to the knowledge of menu labeling and its effects on behavior choices.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2bc9332b-9536-40c8-b141-af9a85fbf5ad", "title": "Comparative Aerosol and Surface Stability of SARS-CoV-2 Variants of Concern", "text": "【0】Comparative Aerosol and Surface Stability of SARS-CoV-2 Variants of Concern\nSince the initial emergence of SARS-CoV-2 (lineage A), new lineages and variants have emerged , typically replacing previously circulating lineages. The World Health Organization has designated 5 virus variants as variants of concern (VOCs) . To assess whether the transmission advantage of new VOCs might have arisen partly from changes in aerosol and surface stability, we compared them directly with a lineage A ancestral virus (WA1 isolate).\n\n【1】### The Study\n\n【2】We evaluated the stability of SARS-CoV-2 variants in aerosols and on high-density polyethylene (to represent a common surface) and estimated their decay rates by using a Bayesian regression model . We generated aerosols (<5 μm) containing SARS-CoV-2 with a 3-jet Collison nebulizer and fed them into a Goldberg drum to create an aerosolized environment , using an initial virus stock of 10 5.75  –10 6  50% tissue-culture infectious dose (TCID 50  ) per mL. To measure surface stability, we deposited 50 μL containing 10 5  TCID 50  of virus onto polypropylene.\n\n【3】For aerosol stability, we directly compared the exponential decay rate of different SARS-CoV-2 isolates  by measuring virus titer at 0, 3, and 8 hours; the 8-hour time point was chosen through modeling to maximize information on decay rate given the observed 3-hour decay. We performed experiments as single runs (to-3 or 0-to-8 hours) and collected samples at start and finish to minimize virus loss and humidity changes from repeat sampling. We conducted all runs in triplicate. To estimate quantities of sampled virus, we analyzed air samples collected at 0, 3, or 8 hours postaerosolization by quantitative reverse transcription PCR for the SARS-CoV-2 envelope (E) gene to quantify the genome copies within the samples. To determine the remaining concentration of infectious SARS-CoV-2 virions, we titrated samples on standard Vero E6 cells. To check robustness, we also titrated the samples on 2 Vero E6 TMPRSS2-expressing lines, yielding similar results . We estimated exponential decay of infectious virus relative to the amount of remaining genome copies to account for particle settling and other physical loss of viruses, although we also estimated decay rates from uncorrected titration data as a robustness check, which yielded similar results .\n\n【4】We recovered viable SARS-CoV-2 virus from the drum for all VOCs . The quantity of viable virus decayed exponentially over time . The half-life of the ancestral lineage WA1 in aerosols (posterior median value \\[2.5%–97.5% posterior quantiles\\]) was 3.20 (.33–4.98) hours. The B.1, Alpha, and Beta viruses appeared to have longer half-lives than WA1: 3.99 (.73–7.20) hours for B.1, 6.13 (.14–27.5) hours for Alpha, and 5.13 (.16–12.3) hours for Beta. The half-life of Delta was similar to that of WA1: 3.12 (.29–4.73) hours. The Omicron (BA.1) variant displayed a similar or decreased half-life compared with WA1: 2.15 (.35–4.04) hours . To better quantify the magnitude and certainty of the change, we computed the posterior of the ratio for variant half-life to WA1 half-life for each variant . Estimated ratios were 1.25 (.701–2.48) for B.1, 1.88 (.859–8.75) for Alpha, 1.6 (.838–4.01) for Beta, 0.978 (.571–1.63) for Delta, and 0.659 (.35–1.37) for Omicron. That is, initial spike protein divergence from WA1 (heuristically quantified by the number of amino acid substitutions) appeared to produce increased relative stability, but further evolutionary divergence reverted stability back to that of WA1, or even below it .\n\n【5】Next, we investigated surface stability of VOCs compared with the ancestral variant on polyethylene. Again, all variants exhibited exponential decay . We found a half-life of 4.82 (.23–5.49) hours for WA1, similar to our previous estimates  . Early VOCs had slightly longer half-lives: 5.16 (.48–5.96) hours for B.1, 5.13 (.59–5.74) hours for Alpha, and 5.73 (.01–6.72) hours for Beta . As with aerosols, Delta had a half-life similar to WA1 of 4.38 (.48–5.65) hours and Omicron had a somewhat shorter half-life of 3.58 (.88–4.47) hours . We again calculated posterior probabilities for the half-life ratios relative to WA1 . B.1 had a half-life ratio to WA1 of 1.07 (.876–1.32), Alpha a half-life ratio of 1.07 (.896–1.27), and Beta a half-life ratio of 1.19 (.988–1.46). The ratios for Delta and Omicron were 0.912 (.694–1.21) and 0.744 (.578–0.965).\n\n【6】In both aerosol and surface results, the posterior 95% credible intervals for most ratios overlap 1. Experimental noise could possibly explain the apparent trend toward increased stability for B.1, Alpha, and Beta, although the clear bulk of posterior probability mass indicates greater half-lives. Conversely, the posterior ratios indicate clearly that Delta and Omicron are not markedly more stable than WA1 and might be less stable (particularly Omicron and particularly on surfaces).\n\n【7】### Conclusions\n\n【8】Several studies have analyzed the stability of SARS-CoV-2 on surfaces or in aerosols in a Goldberg rotating drum . Most have focused on the duration over which infectious virus could be detected. In this study, we paired a model-optimized experimental design with Bayesian hierarchical analysis to systematically measure virus half-life across 6 SARS-CoV-2 variants and directly estimate relative half-lives with full error propagation. We found a small initial increase in aerosol stability from ancestral WA1 to the B.1, Alpha, and Beta variants, with some statistical uncertainty. However, we found that Delta has a half-life similar to that of WA1 and that Omicron likely has a shorter one. In surface measurements, the VOCs followed the same pattern of relative stability, confirming that the overall stability of SARS-CoV-2 variants is determined by similar factors in aerosols and on surfaces . Divergent results on the aerosol and surface stability of VOCs have been reported .\n\n【9】Our study suggests that aerosol stability is likely not a major factor driving the increase in transmissibility observed with several VOCs . The early rise in stability for B.1 and its descendants Alpha and Beta might have arisen incidentally from selection for other viral traits that favored higher transmission. Epidemiologic and experimental studies suggest that the window for transmission is typically relatively short (<1 hour), and thus a modest change in aerosol half-life would not have discernible epidemiologic effects . However, in specific contexts of enclosed spaces, it will remain vital to understand the temporal profile of transmission risks after the release of aerosols containing SARS-CoV-2 from an infected person. We conducted our experiments under laboratory conditions using tissue culture media, so biological factors potentially affecting decay (e.g. airway mucins and other components of airway-lining fluids) were not considered. Novel approaches studying aerosol microenvironments have reported initial rapid loss of SARS-CoV-2 infectiousness in the seconds after aerosolization ; our work only addresses SARS-CoV-2 decay and stability over longer timescales, after the initial deposition loss has occurred.\n\n【10】Whereas evolutionary selection for previous variants favored high transmission among immunologically naive humans , since late 2021, global population-level selection has favored antigenic change  and the consequent ability to transmit among nonnaive persons. Our findings suggest that increased transmissibility through antigenic evolution might come at a tolerable cost to the virus in environmental stability. Overall, the differences in environmental stability among different VOCs, in aerosols or on surfaces, are unlikely to be driving variant population-level epidemiology.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4e8ce2bc-1c81-463a-8682-176dac5bfd83", "title": "Detection of Endosymbiont Candidatus Midichloria mitochondrii and Tickborne Pathogens in Humans Exposed to Tick Bites, Italy", "text": "【0】Detection of Endosymbiont Candidatus Midichloria mitochondrii and Tickborne Pathogens in Humans Exposed to Tick Bites, Italy\nHard ticks (Acarina: Ixodidae) represent a major public health issue worldwide and are vectors of a broad range of viruses, bacteria, and parasites of human and veterinary concern . Tickborne diseases are increasing globally , and several zoonotic tickborne pathogens, such as _Coxiella burnetii_ , _Rickettsia_ spp. and _Borrelia burgdorferi_ sensu lato complex, are emerging or re-emerging in animals and humans .\n\n【1】Wild animals are increasingly becoming synanthropic, and urbanization of wildlife could bring them into contact with humans, as recently observed during the COVID-19 lockdown in Chile, when sightings of several animal species not previously seen in cities were reported in urban settlements . Wildlife movement into areas of human habitation could contribute to higher circulation of ticks, tickborne pathogens, and other zoonotic agents in suburban and rural areas . However, because of the scant notifications of tickborne diseases, limited data are available on ticks and tickborne pathogens circulating in human populations .\n\n【2】Because Italy is highly suited to outdoor activities such as camping, gardening, hiking, farming, and forestry work, the risk for tick bites among humans is likely high in suburban and rural environments, especially in southern regions where several species of _Ixodes_ ticks have been recorded . Indeed, high seroprevalences for _Rickettsia_ spp. (.0%) and _B. burgdorferi_ s.l. complex (.5%) have been reported in forestry workers, farmers, and livestock breeders in the central-southern regions of Italy . Studies focused on humans exposed to tick bites in southern Italy also reported high seroprevalence for _C. burnetii_ (.0%) and spotted fever group rickettsiae (.8%) that paralleled molecular detection of the same pathogens in ticks collected from the environment and infested reptiles . Those data emphasize the importance of wild animals as reservoirs of tickborne pathogens. Among categories of persons at risk for tickborne pathogen infections, hunters are more likely to be infected than hikers or forestry workers ; high seroprevalence for _Rickettsia_ spp. was noted in hunters from Germany (.1%)  and Brazil (.7%) . Also, hunting dogs are considered sentinels and reservoirs of several infectious and parasitic agents, including ticks and tickborne pathogens that can be transmitted to animals and humans . Although several epidemiologic studies have reported circulation of tickborne pathogens in animals and humans worldwide, limited data are available on tick endosymbionts and their role in the transmission of pathogens or clinical onset of tickborne diseases in vertebrate hosts.\n\n【3】For instance, _Candidatus_ Midichloria mitochondrii is an intracellular bacterial endosymbiont of ticks, which is detected inside mitochondria and has high prevalence in the sheep tick, _Ixodes ricinus_ . This bacterium is vertically transmitted through generations of _I. ricinus_ ticks, but strong evidence supports its transmission to vertebrate hosts via a blood meal, which has been experimentally reported in rabbits . Indeed, high _Candidatus_ M. mitochondrii seroprevalence (%) has been reported in human patients exposed to _I. ricinus_ tick bites  and those affected by Lyme disease . Although these results support the idea that _Candidatus_ M. mitochondrii is inoculated into the vertebrate host during the blood meal of the tick, whether this bacterium is involved in tickborne pathogen transmission remains unclear. However, screening studies for _Candidatus_ M. mitochondrii DNA in human blood samples are lacking, even though DNA has been detected in several anthropophilic tick species, such as _Rhipicephalus turanicus_ , _Rhipicephalus bursa_ , and _Haemaphysalis punctata_ ; in domestic dogs, sheep, and horses ; in wild roe deer (Capreolus capreolus_ ) ; and in laboratory rabbits .\n\n【4】We aimed to assess the occurrence of zoonotic tickborne pathogens in persons exposed to tick bites and living in suburban and rural areas of southern Italy. In addition, we investigated the presence and the potential involvement of _Candidatus_ M. mitochondrii in tickborne pathogen infections.\n\n【5】### Methods\n\n【6】##### Study Area\n\n【7】The study was conducted in 3 regions of southern Italy, Apulia, Basilicata, and Campania, which are surrounded by the Adriatic, Ionian, and Tyrrhenian Seas. These regions are characterized by a typical Mediterranean temperate climate and have progressively continental features including inland and mountainous landscapes.\n\n【8】##### Sampling\n\n【9】During February–December 2021, we collected blood and serum samples from 135 persons exposed to tick bites by using 10 mL BD Vacutainer EDTA Blood Collection Tubes and 5 mL BD Vacutainer Serum-separating Tubes . For each participant, we used a detailed anamnestic form to collect data on patient age, sex, participation in outdoor activities (i.e. hiking, hunting, camping, gardening, farming, or forestry work), geographic origin of tick bite, and post-bite clinical findings ascribable to tickborne diseases. All samples were kept at \\+ 4°C in portable refrigerators and delivered to the Departments of Biomedical Science and Human Oncology and Veterinary Medicine, University of Bari (Bari, Italy), for serologic and molecular analyses.\n\n【10】This study was conducted in accordance with ethical principles of the Declaration of Helsinki. Patients provided written informed consent after they were fully informed about the research aims and features. The research was approved by the ethics committee of the University Hospital of Bari Aldo Moro (Italy) .\n\n【11】##### Serologic Assays\n\n【12】We screened all serum samples for pathogen-specific IgG by using indirect chemiluminescent immunoassays, _Coxiella burnetii_ VirCLIA IgG, _Rickettsia_ _conorii_ VirCLIA IgG, and _Borrelia_ VirCLIA IgG . Results were expressed by antibody concentration index, which we calculated as the ratio between the sample and calibrator relative light units and interpreted according to the manufacturer’s instructions.\n\n【13】The _C. burnetii_ IgG test showed a sensitivity of 90%, specificity of 100%, positive predictive value of 100%, and negative predictive value of 96%. The _R. conorii_ IgG test showed sensitivity of 100%, specificity of 100%, positive predictive value of 100%, and negative predictive value of 100%. The _Borrelia_ IgG test showed sensitivity of 89%, specificity of 98%, positive predictive value of 97%, and negative predictive value of 92%.\n\n【14】##### DNA Extraction, PCR, and Phylogenetic Analysis\n\n【15】We used the QIAampDNA Blood & Tissue Kit  to extract DNA from blood samples, according to the manufacturer’s instructions, and tested for tickborne pathogens and _Candidatus_ M. mitochondrii. In all PCR runs, we used sterile water as the negative control and used the following as positive controls: _Anaplasma phagocytophilum_ and _Candidatus_ M. mitochondrii for PCR targeting 16S rRNA gene; _C. burnetii_ for IS1111a and _Borrelia lusitaniae_ for Flagellin from _I. ricinus_ ticks; and _Rickettsia raoultii_ for _gltA_ from a _Dermacentor marginatus_ tick specimen  . All ticks used were previously collected from animals or humans in the same study area.\n\n【16】We placed PCR products on 2% agarose gel stained with GelRed Nucleic Acid Gel Stain  and visualized products on a Gel Logic 100 Digital Imaging System . We then purified amplicons and sequenced in both directions by using the same primers we used for PCR and by using the BigDye Terminator v3.1 Cycle Sequencing Kit  in a 3130 Genetic Analyzer (Applied Biosystems/Thermo Fisher Scientific). We edited and analyzed sequences by using Geneious version 9.0  and compared the obtained sequences with available sequences in the GenBank database by using BLAST .\n\n【17】We used MAFFT version 7.490  to align the 16S rRNA gene sequences obtained from human blood samples against 123 available _Midichloria_ genus entries in GenBank, including a sequence type from _I. ricinus_  previously found in the study area . We then trimmed sequences by using TrimAL version 1.4 revision 15 . We split the multisequence alignment (MSA) file into 2 partitions: a reference sequence alignment (RefMSA) containing 1 reference sequence for each sequence type (sequences with 202 nucleotide sites) and a query sequence alignment containing the remaining 115 sequences. We created a maximum-likelihood phylogeny on the RefMSA by using IQ-TREE multicore version 1.6.12  under 1,000 ultrafast bootstrap replications . We selected the Kimura 3-parameter model with empirical frequencies plus regression (TPM3+F+R2) model in ModelFinder , which we implemented as function in the IQ-TREE computation. We used the DNA sequence of _Lariskella_ endosymbiont of _Curculio okumai_ beetles  as an outgroup. To assess the species diversity among the genus _Midichloria_ , we performed a multi-rate Poisson tree processes on the maximum-likelihood tree by using mPTP version 0.2.4\\_osx\\_x86\\_64 . We used an accurate phylogenetic placement based on the least squares method  to place the query sequences on the maximum-likelihood tree. In brief, we subjected the maximum-likelihood tree generated by IQ-TREE to branch length correction using FastTree 2 software . We then used the corrected tree backbone, the RefMSA, and query sequence alignments to generate the phylogenetic placement by using APPLES version 2.0.6 . Finally, we used the result of species delimitation and informative alignment to annotate the final tree by using iTOL version 5 . We performed Pearson correlation to assess the relationship between phylogenetic clades of _Midichloria_ sequences and their host or isolation source and geographic origin. We performed all analyses on RStudio software .\n\n【18】##### Statistical Analysis\n\n【19】We established exact binomial 95% CI for the prevalence values found. We used a χ _2 _ test to assess statistical differences of prevalence of tickborne pathogens with age, sex, outdoor activity, and geographic origin of tick bite of the enrolled patients. We considered p<0.05 statistically significant. We calculated the odds ratio (OR) to assess the infection risk according to several participant variables. We used a κ value formula to compare serologic and molecular methods and considered risk agreement poor at 0–20%, fair at 21%–40%, moderate at 41%–60%, strong at 61%–80%, and perfect at 81%–100%.\n\n【20】We used Epitools Epidemiologic Calculators software  to calculate 95% CI, χ 2  , p value, OR, and κ values. We used QGIS3 version 3.10.2 with GRASS 7.8.2  to map the distribution of participants who were positive for tickborne pathogens and _Candidatus_ M. mitochondrii in the administrative boundaries of the regional study area.\n\n【21】### Results\n\n【22】Among 135 participants, 47.4% (/135) reported having been bitten by ticks in the last year, and 62 (.9%, 95% CI 37.7%–54.3%) tested positive for \\> 1 tickborne pathogen, 54 (.0%, 95% CI 32.1%–48.4%) by serologic methods and 20 (.8%, 95% CI 9.8%–21.8%) by molecular methods. We identified _C. burnetii_ from 37 (.4%) participants, _Rickettsia_ spp. from 29 (.5%), and _Borrelia_ spp. from 14 (.4%). We serologically detected tickborne pathogen co-infections in 14 (.4%) participants, most of whom had _C. burnetii_ – _Rickettsia_ spp. coinfection. We detected co-infection by molecular methods in 1 (.7%) case-patient who had _R. raoultii–B. lusitaniae_ coinfection.\n\n【23】Nine persons were positive for the same pathogen by both diagnostic tools; 4 were positive for _C. burnetii_ , 4 for _R. raoultii_ , and 1 for _B. lusitaniae_ . We noted strong agreement between serologic and molecular results (κ = 63.0%) .\n\n【24】Among the 62 participants who tested positive for tickborne pathogens, 31 (.0%) reported a localized reaction (i.e. edema and redness) at the tick bite site, among whom 13 (.6%) displayed \\> 1 clinical sign ascribable to tickborne pathogen infections . We noted a statistically significant association between the occurrence of tickborne pathogens and administrative region among patients living in Basilicata, suggesting this area could have higher levels of tickborne pathogens .\n\n【25】We detected _Candidatus_ M. mitochondrii DNA in 46 (.1%, 95% CI 26.6%–42.4%) participants, among whom 11 tested negative for tickborne pathogens and 35 tested positive for \\> 1 tickborne pathogen. The 35 participants who tested positive for tickborne infections and _Candidatus_ M. mitochondrii DNA represented 18/54 (.3%, 95% CI 22.2%–46.6%) of participants whose infections were detected serologically and 17/20 (.0%, 95% CI 63.9%–94.7%) of participants whose infections were detected molecularly. _Candidatus_ M. mitochondrii was statistically associated with each pathogen found (i.e. _C. burnetii_ , _R. raoultii_ , and _B. lusitaniae_ ) . No clinical signs were reported in persons infected by _Candidatus_ M. mitochondrii and we saw no statistically significant difference in the clinical signs between subjects who tested positive for tickborne pathogens and those who were positive for both tickborne pathogens and _Candidatus_ M. mitochondrii (χ _2 _ \\= 0.9, p = 0.342). A single case of _Candidatus_ Wolbachia inokumae was molecularly detected in a participant infected by _B. lusitaniae_ .\n\n【26】Consensus sequences of all bacteria species we detected displayed 99%–100% nucleotide identity with sequences available in the literature. We submitted sequences to GenBank under the following accession numbers: ON227500 for _C. burnetii_ , ON228179 for _R. raoultii_ , ON237925 for _B. lusitaniae_ , OM982495–2502 for _M. mitochondrii_ , and OM983334 for _W. inokumae_ .\n\n【27】Phylogenetic analysis of the partial 16S rRNA gene revealed 8 sequence types and the existence of 5 distinct clades (clades A–E); the _Candidatus_ M. mitochondrii sequence types we obtained belonged clade A . Pearson correlation (r_ ) showed that _Candidatus_ M. mitochondrii clade A was significantly correlated with vertebrate hosts (r_ \\= 0.32), _Ixodes_ spp. ticks (r_ \\= 0.21), and countries in Europe (r_ \\= 0.46); clade B with _Hyalomma_ ticks (r_ \\= 0.4) and Argasidae ticks (Ornithodoros_ spp.) (r_ \\= 0.36) and Africa (r_ \\= 0.48); clade C with _Ixodes_ spp. ticks (r_ \\= 0.46); and clades D and E with marine sources (r_ \\= 0.57) .\n\n【28】### Discussion\n\n【29】We report a high serologic and molecular prevalence of infection by zoonotic tickborne bacteria in humans exposed to tick bites in rural areas of southern Italy. In addition, we noted an association between _Candidatus_ M. mitochondrii and tickborne pathogens in humans, suggesting that this tick endosymbiont might be involved in these infections.\n\n【30】The finding of _C. burnetii_ as the most representative tickborne pathogen in humans from southern Italy is a public health concern because prevalence of this bacterium in hard ticks is much higher in the Mediterranean Basin than in other areas of Europe . However, the high (.4%) overall serologic and molecular prevalence we obtained contrasts with the low (.5%) rate of molecular detection of the bacterium in ticks collected on citizens in the same study area . This finding suggests inhalation of contaminated aerosols, not tick bites, might be the main route of pathogen transmission among the study participants .\n\n【31】Detection of _R_ . _raoultii_ is relevant because this pathogen is considered an emerging spotted fever group rickettsiae among humans . Although no patient in our study showed the scalp eschar and neck lymphadenopathy that are typical clinical signs for differential diagnosis , the difference in clinical features might be dependent on the site of the tick bite on a person’s body .\n\n【32】Detection of _B. lusitaniae_ in persons frequenting rural environments of southern Italy confirms the circulation of this zoonotic genospecies, which previously was reported in different hosts, such as the Italian wall lizard (Podarcis siculus_ )  and red foxes (Vulpes vulpes_ ) , and in _I. ricinus_ ticks collected on humans from the same study area . The human pathogenic role of _B. lusitaniae_ has not been completely clarified, but the finding of erythematous macules from positive patients examined in this study is of clinical relevance because these skin lesions could cause chronic or long-lasting injuries associated with infiltration of the local subcutaneous tissues . In addition, co-infections in this human population and in ticks previously collected from citizens of the same study area , indicate that multiple pathogens could be transmitted by the same tick specimen and develop in the same host, complicating the clinical manifestation .\n\n【33】Despite the strong agreement (κ = 63.0%) in the comparison between serology and PCR, use of both serologic and molecular methods is crucial for identifying all cases of infection . Indeed, combining PCR and serology can be useful for detecting recent and old infections because PCR is sensitive < 2 weeks of infection and serology is sensitive \\> 2 weeks after illness onset .\n\n【34】The high molecular prevalence (.1%) of _Candidatus_ M. mitochondrii in participants exposed to tick bites is in accordance with the serologic rate (.3%) previously outlined for this endosymbiont in humans , and confirms its role as a candidate tick-bite marker . Thus, because _Candidatus_ M. mitochondrii is known to subsist in several hard tick species , its detection in human blood could be a useful tool for determining exposure to tick bites and to define tick populations circulating in a certain area. The statistical association between molecular positivity for tickborne pathogens and _Candidatus_ M. mitochondrii demonstrated here suggests a link between endosymbiont and tickborne pathogen infections in humans, which was previously established through serologic methods . However, absence of clinical signs or symptoms in humans testing positive for _Candidatus_ M. mitochondrii and the statistically significant difference in the clinical picture between participants who tested positive only for tickborne pathogens and those who tested positive for both tickborne pathogens and _Candidatus_ M. mitochondrii, suggests that this bacterium might be not involved in the onset or overt pathogenesis of primary infections. Conversely, the hypothesis of a potential _Candidatus_ M. mitochondrii–tickborne pathogen interaction within tick hosts needs further investigation, as previously demonstrated for this endosymbiont in regulating the growth of _R. parkeri_ in its competent vector _Amblyomma maculatum_ ticks .\n\n【35】Our results also highlight the occurrence of 8 _Candidatus_ M. mitochondrii sequence types, suggesting patients were exposed to different tick species, which reinforces the hypothesis that _Candidatus_ M. mitochondrii could be used as candidate marker to define the composition of the tick population biting humans in a certain area . In addition, the clustering of all sequence types into clade A, as we noted, is relevant for public health because this clade encompasses endosymbionts associated with vertebrate hosts, rather than to _Ixodes_ spp. ticks, and geographic areas of Europe. On the other hand, the presence of the same _Midichloria_ genotype within several tick species indicates that vertebrates might act as ecological arenas for intraspecies and interspecies–specific transmission of endosymbionts among hard ticks, which was previously suggested by similar 16S rRNA sequences in genetically distant tick species . Another phylogenetic study analyzed concatenated loci of 16S rRNA, and _groEL_ and _dnaK_ gene sequences, and reported 3 distinct clades (I, II, and III) of _Candidatus_ M. mitochondrii associated with ticks, indicating the existence of 2 lineages including different tick species and 1 _Ixodes_ genus .\n\n【36】Detection of _W. inokumae_ DNA in humans might be a consequence of exposure to a blood meal by selected phlebotomine sandflies. In fact, _W. inokumae_ has only been identified in _Phlebotomus perniciosus_ sand fly specimens from France and Tunisia , where it has been indicated as endosymbiont of this arthropod species in the Mediterranean Basin.\n\n【37】Our data demonstrate a high circulation of tickborne pathogens and _Candidatus_ M. mitochondrii in persons frequenting suburban and rural areas of southern Italy. Further experimental studies could clarify the biologic interaction between tickborne pathogens and _Candidatus_ M. mitochondrii and the potential role of _Candidatus_ M. mitochondrii in tickborne pathogen transmission to mammalian hosts. The set of 16S rRNA sequences and their cladogenetic classification we generated could be a useful template for future studies, especially for further description of new _Candidatus_ M. mitochondrii clades.\n\n【38】In conclusion, the survey we describe relies on cooperation between different stakeholders, including patients, clinicians, and veterinarians. This type of surveillance could be a small step toward a One Health approach for monitoring and controlling the circulation of tickborne pathogens in suburban and rural areas .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4d1bca26-d62f-47d7-aa74-510f5ee546f4", "title": "Detail of Sonoran Desert", "text": "【0】— US Postal Service\n\n【1】Stamp Design ©1999 U.S. Postal Service. Detail of Sonoran Desert. First in the Nature in America series. Reproduced with permission. All rights reserved\n\n【2】This issue was originally published without an accompanying cover story.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f1e34072-7713-4590-8ab6-d5f023fd0c1b", "title": "Novel Prion Strain as Cause of Chronic Wasting Disease in a Moose, Finland", "text": "【0】Novel Prion Strain as Cause of Chronic Wasting Disease in a Moose, Finland\nPrions are infectious proteins that cause fatal, incurable neurodegenerative diseases of humans and animals, which include Creutzfeldt-Jakob disease (CJD), sheep scrapie, bovine spongiform encephalopathy, and chronic wasting disease (CWD) of cervids. The extraordinary biology and transmissibility of these disorders stems from the protean conformational properties of the prion protein (PrP). Although the secondary structure of host-encoded cellular PrP (PrP C  ) is predominantly α-helical, during disease its relatively underglycosylated infectious counterpart (PrP Sc  ) assembles into amyloid fibrils with parallel, in-register, intermolecular β-sheets . The replicative properties of prions stem from the capacity of PrP Sc  to template its conformation on PrP C  in a cyclical process resulting in exponential accumulation of prion infectivity .\n\n【1】Although they lack nucleic acids, prions exhibit heritable strain properties that influence disease outcomes, including the time between infection and disease onset (incubation period), clinical signs, patterns of neuronal degeneration and PrP Sc  deposition in the central nervous system (CNS), and the ability to replicate in non-CNS tissues such as the lymphoreticular system and musculature . Strain properties also influence the capacity of prions from one species to cause disease in a different species . Heritable strain information appears to be enciphered by distinct PrP Sc  conformations that are faithfully propagated during prion replication .\n\n【2】The food-chain transmission of bovine spongiform encephalopathy prions that resulted in a variant form of CJD illustrates the unpredictable potential of emergent strains for zoonotic transmission . Although novel prion diseases and strain variants continue to arise in increasing numbers of animal species, CWD elicits particular concern . After its initial description in a captive deer facility , uncontrolled contagious transmission has resulted in growing numbers of CWD-affected cervids in at least 30 US states and 3 Canada provinces . Inadvertent importation of subclinically diseased animals from North America led to the establishment of CWD in South Korea . CWD was also diagnosed in free-ranging Norwegian reindeer in 2016 , and additional cases were subsequently identified in growing numbers of moose, red deer, and reindeer from Norway, Sweden, and Finland .\n\n【3】The development and application of genetically modified, CWD-susceptible mice expressing cervid PrP C  (CerPrP C  ) has provided insights into multiple aspects of pathogenesis, including the impact of naturally occurring _PRNP_ coding sequence variations . Whereas North America deer and moose encode glutamine (Q) at codon 226 (CerPrP-Q226), North America elk encode glutamate (E) at this position (CerPrP-E226). Early studies in transgenic (Tg) mice suggested a role for this variation in the selection and propagation of CWD strains . To precisely assess the effects of this primary structural difference, we created CWD-susceptible gene-targeted (Gt) mice in which the PrP coding sequence was replaced with CerPrP-Q226 or CerPrP-E226 . Because the resulting mice, referred to as GtQ and GtE mice, express equivalent, physiologically controlled levels of CerPrP C  and are otherwise syngeneic, we were able to ascribe distinct disease outcomes in each line to the effects of these amino acids . Furthermore, in contrast to previous CWD-susceptible Tg mice, Gt mice recapitulated the lymphotropic properties of CWD strains . By using Gt mice, we showed that emergent strains causing CWD in Norway reindeer and moose were unrelated to established North America forms of CWD and that they responded differently to variation at residue 226 . These findings underscored the utility of Gt mice for accurately defining the strain properties of emergent CWD prions . The goal of this study was to characterize a newly emergent form of CWD in Finland moose.\n\n【4】### Materials and Methods\n\n【5】##### Ethics Statement\n\n【6】We performed all animal work in an Association for Assessment for Accreditation of Laboratory Animal Care–accredited facility in accordance with the Guide for the Care and Use of Laboratory Animals . All procedures used in this study were performed in compliance with and were approved by the Colorado State University Institutional Animal Care and Use Committee.\n\n【7】##### Isolates\n\n【8】The Finland moose (M-F1) CWD isolate was from a free-ranging female moose identified to be sick and later found dead in 2018. The North America moose (M-US1) CWD isolate was from a captive moose orally inoculated with a pooled preparation of deer CWD. North America elk CWD isolate (E-US1), and Norway moose CWD isolates (M-NO1, M-NO2, and M-NO3) have been described previously .\n\n【9】##### Mouse Models and Incubation Time Assay\n\n【10】Development and characterization of Tg mice expressing cervid PrP with glutamine at residue 226 (TgQ), Tg mice expressing cervid PrP with glutamate at residue 226 (TgE), GtQ and GtE mice (expressing the same thing as Tg mice), and the prion incubation time assay have been described previously . We used equal numbers of male and female mice in inoculation studies. We confirmed clinical diagnoses of prion disease by PrP Sc  detection in the CNS and neuropathologic assessments.\n\n【11】##### Analysis of PrP Sc  by Western Blotting\n\n【12】We performed analysis of PrP Sc  by western blotting as described previously . Equivalent amounts of protein were treated with 50 μg/mL of proteinase K  in the presence of 2% sarkosyl for 1 hour at 37°C. We performed sodium dodecyl sulfate–polyacrylamide gel electrophoresis by using precast 12% discontinuous Bis-Tris gels . We detected proteins transferred to Immobilon-FL PVDF membranes  with monoclonal antibody (mAb) PRC5 at a dilution of 1:5,000 and mAb PRC1 at a dilution of 1:2,500 , followed by horseradish peroxidase–conjugated anti-mouse IgG secondary antibody. We developed membranes by using ECL 2 Western Blotting Substrate .\n\n【13】##### Conformational Stability Assay\n\n【14】We conducted a conformational stability assay as described previously . We detected PrP Sc  on membranes with mAb PRC5  at a dilution of 1:5,000, followed by horseradish peroxidase–conjugated goat anti-mouse IgG secondary antibody. We developed membranes with ECL 2 Western Blotting Substrate and scanned them with an ImageQuant LAS 4000 digital camera system  and analyzed signals with ImageQuant TL 7.0 software (GE Healthcare).\n\n【15】##### Histoblot Analysis\n\n【16】We performed histoblot analysis as described previously . We detected PrP Sc  on membranes by using mAb PRC5 at a dilution of 1:5,000, followed by alkaline phosphatase conjugated goat anti-mouse IgG  at a dilution of 1:5,000. We developed membranes by using 5-bromo-4-chloro-3-indolyl phosphate/nitro blue tetrazolium  and captured images with a Nikon SMZ1000 microscope .\n\n【17】##### Neuropathologic and Immunohistochemical Analyses\n\n【18】We dissected brains rapidly after euthanizing the animal and immersion fixed tissues in 10% buffered formalin. We embedded tissues in paraffin and mounted 8 μm–thick coronal microtome sections onto positively charged glass slides. We performed immunohistochemical analyses of PrP in brain sections by using mAb D18 as described previously .\n\n【19】##### Neuropathologic Lesion Profiling\n\n【20】We sectioned paraffin-embedded mouse brains coronally to areas corresponding to 5 levels of the brain containing the 9 mouse brain regions of interest. We captured images of hematoxylin and eosin–stained sections by using a Photometrics CoolSNAP digital camera  and an Olympus BX51 fluorescence microscope  and analyzed them by using Olympus cellSens imaging software (standard version). We assessed the severity of vacuolar degeneration in cerebral gray matter from 9 regions, as described previously , by using a team of 4 investigators, who scored the images on scale of 0 to 4, with 4 corresponding to severe vacuolation.\n\n【21】##### Statistical Analyses\n\n【22】We performed statistical analyses by using GraphPad Prism 8.0 software . We assessed statistical significance between survival curves of inoculated groups by comparing median times of survival of various inoculated groups using the log-rank (Mantel-Cox) test. We assessed statistical significance between denaturation curves of PrP Sc  by comparing log half maximal effective concentration values.\n\n【23】### Results\n\n【24】We registered disease after ≈260 days in all but 1 of 11 GtQ mice inoculated with a homogenate of CNS from the index case of Finland CWD, M-F1 . Brain homogenates from 2 diseased GtQ mice, referred to as GtQ (M-F1) brain 1 and brain 2 , produced disease after ≈225 days following intracerebral inoculation of additional GtQ mice . By contrast, all GtE mice inoculated with M-F1 brain homogenate remained free of disease for up to 545 days , and GtQ (M-F1) brain 1 produced disease in only 4 of 7 inoculated GtE mice after ≈470 days , whereas GtQ (MF-1) brain 2 failed to produced disease in GtE mice after >510 days . We conclude that M-F1 prion replication is supported when Q is encoded at residue 226 of host-encoded PrP (CerPrP C  \\-Q226) and relatively restricted when E is present at that position. In support of this conclusion, whereas CNS overexpression of CerPrP C  \\-Q226 produced faster times to disease in TgQ compared with GtQ mice, only a single inoculated TgE mouse had disease after ≈390 days .\n\n【25】We confirmed clinical diagnoses of prion disease by using western blotting of PK-resistant CerPrP Sc  in brain homogenates , histoblotting of CerPrP Sc  in coronal sections , and microscopic analyses of neuronal vacuolation  and CerPrP Sc  deposition in the CNS . Although spleen homogenates of GtQ mice infected with North America CWD and Norway reindeer CWD prions contained CerPrP Sc  , there was no evidence of prion replication in the spleens of GtQ mice after primary or secondary passage of the M-F1 isolate, or, as observed previously , in the spleens of GtQ mice infected with Norway moose CWD isolate M-NO1 . We conclude that whereas North America and Norway reindeer CWD prions are lymphotropic, Finland and Norway moose CWD prions are nonlymphotropic. In support of this conclusion, neither Gt nor Tg mice had disease after intracerebral inoculation with lymphoid tissue homogenates from M-F1 .\n\n【26】We questioned whether the inefficient transmission of M-F1 prions in GtE and TgE mice could be overcome by adaptation in a CerPrP-E226 background. Serial passage of M-F1 prions from the diseased TgE mouse, referred to as TgE (M-F1) , produced disease in GtQ mice after ≈230 days but failed to produce disease in GtE mice after ≈500 days . Because serial passage of M-F1 prions from diseased TgQ mice also produced disease in GtQ mice after ≈230 days and failed to produce disease in GtE mice after ≈500 days , we conclude that the M-F1 strain favors conformational conversion of CerPrP C  \\-Q226 over CerPrP C  \\-E226 regardless of whether prions are composed of CerPrP Sc  \\-E226 or CerPrP Sc  \\-Q226.\n\n【27】Our previous studies showed that propagation of prions from 3 Norway moose CWD isolates, referred to as M-NO1, M-NO2, and M-NO3, was also supported by expression of CerPrP C  \\-Q226 and relatively restricted by CerPrP C  \\-E226 . Despite this shared feature, comparisons of primary transmissions to GtQ mice showed that the mean incubation time of M-F1 was faster than those of M-NO1, M-NO2, or M-NO3 (p<0.0001) . Primary transmission of M-F1 was also faster than Norway moose CWD isolates in TgQ mice (p>0.0001) . We also observed faster disease onsets when serial transmissions of GtQ (M-F1) brains 1 and 2 were compared with serially transmitted M-NO1 and M-NO2 in GtQ mice  and when M-F1 and Norway moose CWD prions were serially propagated from TgQ to GtQ mice . Those consistently different disease kinetics during primary and secondary transmissions are indicative of strain differences between Finland and Norway moose CWD prions.\n\n【28】To substantiate that conclusion, we assessed additional strain features in infected GtQ mice. Semiquantitative assessment by brain lesion profiling  showed that the extent and regional distribution of spongiform degeneration varied depending on whether GtQ mice were infected with Finland or Norway moose CWD prions . Differences between GtQ mice infected with M-F1 and M-NO1 were particularly evident in the septum, retrosplenial and adjacent motor cortex, and cingulate and adjacent motor cortex. In these areas the intensity of spongiform degeneration was more pronounced in GtQ mice infected with M-F1 . The distinctive lesion profile of M-F1 was sustained after serial passage to additional GtQ mice .\n\n【29】We assessed the responses of CerPrP Sc  produced in the brains of mice infected with Finland, Norway, and North America moose CWD prions to denaturation with increasing concentrations of guanidine hydrochloride (GdnHCl) . This measure of PrP Sc  stability is associated with conformational variation among prion strains . Denaturation profiles and concentrations of GdnHCl producing half-maximal denaturation indicated that the stability of CerPrP Sc  in GtQ mice infected with M-F1 was equivalent to CerPrP Sc  in the brains of GtQ mice infected with North America moose CWD and lower than that of CerPrP Sc  in the brains of GtQ mice infected with M-NO1 (p<0.0001) . The conformational stability of CerPrP Sc  produced on primary passage of M-F1 was maintained upon serial transmissions of GtQ (M-F1) brains 1 and 2 to GtQ mice . We also observed overlapping denaturation curves and comparable half-maximal denaturation values in the range of 2.59 to 2.95 upon infection of TgQ mice with M-F1 prions and M-F1 prions passaged in TgE or TgQ mice . We conclude that conformational properties of M-F1 prions are distinct from those of M-NO1 prions, they are stable during iterative passages, and they remain unchanged after passage in mice expressing CerPrP C  \\-E226.\n\n【30】Despite their different transmission properties, neuropathologic profiles, and conformations, Finland and Norway CWD prions shared certain features. Western blotting showed that levels of CerPrP Sc  in the brains of GtQ mice infected with M-F1 and M-NO1 were lower than the levels of CerPrP Sc  in the brains of GtQ mice infected with North America CWD . This finding is consistent with previous studies showing consistently reduced CerPrP Sc  accumulation after Norway moose CWD prion infection . The electrophoretic migration profiles of CerPrP Sc  resulting from infection of GtQ mice with M-F1 and M-NO1 were both more rapid than CerPrP Sc  produced by infection with North America CWD . This finding also is consistent with our previous descriptions of Norway moose CerPrP Sc  . Finally, also in accordance with previous findings , CerPrP Sc  in the brains of M-F1 and M-NO1 infected mice was relatively refractory to detection by mAb PRC1 compared with CerPrP Sc  generated by infection with North America CWD . The faster migration of CerPrP Sc  generated by infection with M-F1 and M-NO1 and its comparative resistance to detection by PRC1 results from PK cleavage downstream from the PRC1 epitope at residue 90 in both strains . Serial transmissions of M-F1, M-NO1, and North America CWD prions produced CerPrP Sc  immunoblotting profiles consistent with primary passages . We speculate that the low levels of PRC1-reactive CerPrP Sc  in brain extracts of mice infected with M-F1 and M-NO1 CWD prions  correspond to minor strain components that remain obscured when probing with nondiscriminatory mAbs. Using discriminatory mAbs revealed similar coexistence of multiple PrP Sc  types in persons with CJD .\n\n【31】Analyses of PK-resistant CerPrP Sc  in histoblotted coronal brain sections revealed a diffuse, symmetric deposition pattern in GtQ mice infected with M-F1  that was indistinguishable from the pattern in GtQ mice infected with M-NO1 CWD , and consistent with previous analyses of mice infected with Norway moose CWD . This pattern was recapitulated after iterative passage of GtQ (M-F1) prions to GtQ mice  and after infection of TgQ mice with M-F1 . This diffuse pattern differed from the disorganized, asymmetrically distributed amalgamations of CerPrP Sc  produced by infection of GtQ mice with North America CWD prions  . Microscopic analysis of immunohistochemically stained CNS sections from GtQ mice infected with M-F1 CWD revealed small punctate accumulations of disease-associated CerPrP set against a background of diffuse staining . We observed a similar pattern in brain sections of TgE mice infected with M-F1 . We also detected small punctate accumulations in GtQ mice infected with M-NO1 . By contrast, and consistent with previous findings , GtQ mice infected with North America CWD contained intensely staining, amorphous aggregates of disease-associated CerPrP .\n\n【32】### Discussion\n\n【33】Our previous studies showed that although Gt and Tg mice expressing either CerPrP C  \\-E226 or CerPrP C  \\-Q226 were susceptible to North America CWD, times to disease onset were more rapid in mice expressing CerPrP C  \\-E226 . By comparison, although efficient transmission of Norway moose CWD prions was registered in Gt and Tg mice expressing CerPrP C  \\-Q226, their counterparts expressing CerPrP C  \\-E226 were resistant to infection or had prolonged incubation times and incomplete attack rates . Those studies also revealed strain variation among Norway moose CWD isolates, the properties of which were different from Norway reindeer CWD . The different properties of emergent Norway strains compared with North America CWD made it unlikely that they were causally related. Parallel experiments in bank voles supported this interpretation . Differences in the strain properties of M-F1 and North America CWD enable us to build on our previous conclusions  and to state more broadly that the etiology of Nordic CWD is distinct from North America CWD.\n\n【34】Our analyses reveal that certain characteristics of M-F1 CWD prions overlap with those of Norway moose CWD. M-F1 and Norway moose CWD both propagated more efficiently in mice expressing CerPrP C  \\-Q226. Western and histoblot profiles of M-F1 and M-NO1 CerPrP Sc  are also indistinguishable. Our findings in this study show that M-F1 prions, like Norway moose CWD prions, are nonlymphotropic. Despite these broadly similar characteristics, other measures, including consistently different disease kinetics, distinct neuropathologic lesion profiles, and variable responses to GdnHCl denaturation, indicate that the strain properties of Finland and Norway moose CWD prions are different. This finding adds to an increasing body of evidence for a surprising variety of strains among Nordic cervids, which stands in contrast to the relatively consistent CWD strain profile among North America deer, elk, and moose. As yet, there appears to be no clear explanation for this diversity of Nordic CWD strains or insights into their origins. Although efficient prion dissemination to peripheral tissues of affected cervids has been cited as a reason for the contagious properties of North America CWD , the nonlymphotropic properties of Finland and Norway moose CWD strains suggests a decreased potential for contagious propagation. The emergence of increasing numbers of CWD strains in a confined geographic location during a short timeframe presents challenges when considering a sporadic etiology; however, our findings showing dissimilar CWD profiles in Nordic moose can be explained by divergent strains arising from unrelated, stochastic conversion events in different animals. CWD has also been diagnosed in moose in Sweden . Analyses of the strain properties of those additional emergent CWD cases will be of considerable interest.\n\n【35】Additional findings from this study build on our previous observations and warrant discussion. Seminal studies indicating that related PrP Sc  and PrP C  primary structures supported optimal prion propagation laid the foundation for the development of Tg mice with susceptibility to human and animal prions . Because moose express CerPrP C  \\-Q226, we reasoned that the relatively inefficient transmission of M-F1 prions in GtE and TgE mice might be overcome by acclimatization in mice expressing CerPrP C  \\-E226, resulting in adapted M-F1 prions composed of CerPrP Sc  \\-E226. However, our findings showing comparable transmission profiles of TgE (M-F1) and TgQ (M-F1) prions indicate that M-F1 favors conformational conversion of CerPrP C  \\-Q226 over CerPrP C  \\-E226 regardless of whether prions are composed of CerPrP Sc  \\-E226 or CerPrP Sc  \\-Q226. Resistance of M-F1 prions to adapt to a host expressing a single amino acid difference is reminiscent of our description of nonadaptive prion amplification (NAPA) . Those studies showed that prions produced during particular interspecies transmissions are not adapted for sustained propagation in that new host. Instead, prions resulting from NAPA retain atavistic preferences for their species of origin . In this context, although M-F1 prions are capable of suboptimal propagation by NAPA using CerPrP C  \\-E226 as template for replication, the resulting TgE (M-F1) prions are not adapted for further propagation by CerPrP C  \\-E226 but instead retain a preference for conversion of CerPrP C  \\-Q226.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5537b3c9-d0fc-4c40-90fb-04371e16167d", "title": "Effectiveness of Abbott BinaxNOW Rapid Antigen Test for Detection of SARS-CoV-2 Infections in Outbreak among Horse Racetrack Workers, California, USA", "text": "【0】Effectiveness of Abbott BinaxNOW Rapid Antigen Test for Detection of SARS-CoV-2 Infections in Outbreak among Horse Racetrack Workers, California, USA\nRapid antigen tests, such as Abbott BinaxNOW  test kits, offer a less expensive and faster alternative to nucleic acid amplification tests, such as real-time reverse transcription PCR (rRT-PCR), in the diagnosis of coronavirus disease (COVID-19) . Previous studies of BinaxNOW compared with rRT-PCR have demonstrated a high negative percent agreement (NPA) (.4%–100%) but variable positive percent agreement (PPA) (.5%–89.0%). Performance was better among symptomatic persons, specimens with cycle threshold (C t  ) <30 (suggestive of higher viral loads), and specimens with positive viral cultures . These reports have focused on community testing sites and outbreaks in healthcare facilities.\n\n【1】Throughout the pandemic, certain nonhealthcare occupational groups (e.g. meat and poultry processing workers) have experienced higher risk of contracting COVID-19; this higher risk is attributable to workplace hazards, such as lack of appropriate personal protective equipment, densely populated work areas, poorly ventilated workspaces, and prolonged close contact . These workplaces might benefit from effective rapid antigen tests that enable employers to quickly identify persons infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) for isolation and to guide contact tracing, thereby reducing workplace transmission. Despite the need for research on this topic, information on the performance of BinaxNOW in the setting of nonhealthcare workplace outbreaks is lacking.\n\n【2】During October 20, 2020–January 15, 2021, a horse racetrack (the facility) in California, USA, experienced a COVID-19 outbreak among its 563 employees and independent contractor workers (hereafter collectively called facility staff). Nearly half (n = 278; 49.4%) of the staff lived onsite in facility-provided housing, and many performed essential duties (e.g. grooming, feeding) related to the basic care of the >1,100 horses stabled there.\n\n【3】The outbreak was discovered by the contact tracing efforts of the local health department (LHD), the City of Berkeley Public Health Officer Unit. In response, the LHD ordered that all nonessential work activities (e.g. horse racing) be stopped until mass testing of all staff demonstrated no further transmission. The initial round of rRT-PCR testing (round 0) occurred on November 14–15, 2020, and identified 169 SARS-CoV-2–positive staff who were subsequently isolated. At this time, all staff were assumed to have been exposed. Those living onsite were moved to hotel rooms to quarantine, and those living offsite quarantined in their homes. No staff were permitted to return to onsite residence until the outbreak had ended. However, some quarantined employees were permitted to return to work if they were needed to perform duties related to essential care of the horses. Additional rounds of testing were needed to monitor ongoing transmission and determine when the outbreak had ended. The LHD decided to use BinaxNOW as a supplement to rRT-PCR to more quickly identify SARS-CoV-2–positive employees for isolation. This use provided an opportunity to assess the effectiveness of the BinaxNOW rapid antigen test in detecting SARS-CoV-2 infection in a nonhealthcare workplace outbreak. The purpose of this analysis is to compare BinaxNOW with rRT-PCR in paired specimens from persons during a COVID-19 outbreak among horse racetrack workers. These findings could inform testing protocols used to contain future outbreaks of COVID-19 in nonhealthcare workplaces.\n\n【4】### Methods\n\n【5】The facility, in collaboration with the LHD and the California Department of Public Health (CDPH) laboratory, conducted 6 rounds of serial testing of its staff with paired BinaxNOW rapid antigen and rRT-PCR tests during November 25–December 22 (rounds 1–6). Testing frequency was determined by the LHD and changed as the outbreak progressed. Each round was intended to test all staff who had not yet tested positive by BinaxNOW or rRT-PCR to continue identifying potentially infectious persons. Staff who tested positive by either BinaxNOW or rRT-PCR were isolated and excluded from further testing.\n\n【6】All specimen collection and antigen testing occurred outdoors in the parking lot of the facility. On the day of testing, a facility administrative employee conducted registration and collected demographic data, including self-reported race and ethnicity. Symptom information was elicited by asking staff if they were experiencing any COVID symptoms, such as fever, headache, or loss of taste. Bilateral anterior nasal swab specimens were collected by either the racetrack physician or one of the racetrack veterinarians trained in collection procedures. A first swab specimen was used for onsite BinaxNOW testing; a second swab specimen was placed in viral transport medium and chilled on ice packs before transport to the CDPH laboratory for rRT-PCR testing 24–72 hours after collection. All specimens in viral transport medium were frozen at –70°C within 12 hours of delivery to the laboratory. BinaxNOW test results were interpreted immediately at the 15-minute read time by the racetrack physician in accordance with the test kit instructions, along with the updated scoring criteria described by Pilarowski et al. which indicates that bands are scored as positive only if they extend across the full width of the strip, irrespective of the intensity of the band. Because BinaxNOW testing was not performed for round 0, those 169 rRT-PCR–positive specimens were not included in this analysis.\n\n【7】For rRT-PCR, we isolated and purified viral nucleic acid (NA) from the swab specimens by using the KingFisher Flex Purification System and the MagMAX Viral/Pathogen Nucleic Acid Isolation Kit . We performed rRT-PCR by using the ThermoFisher TaqPath COVID-19 Combo Kit, which targets 3 SARS-CoV-2 viral regions (nucleocapsid protein gene, spike protein gene, and open reading frame 1ab), and the Applied Biosystems 7500 Fast Dx Real-Time PCR Instrument (ThermoFisher Scientific), according to the manufacturer’s instructions. Real-time RT-PCR–positive specimens with C t  <30 were also cultured for SARS-CoV-2 at CDPH in a Biosafety Level 3 laboratory. For cultures, 200 µL of patient specimen was diluted 1:1 with diluent containing 0.75% bovine serum albumin, and 50 µL was added to 8 replicate wells in a 96-well plate containing confluent Vero-81 cells at 37°C with 5% CO 2  . After 1 h, the inoculum was removed and 200 µL of minimum essential medium containing 5% fetal bovine serum and antibiotics was added to each well. Cells were monitored for cytopathic effect. Cells with positive cytopathic effect were tested by rRT-PCR to confirm presence of SARS-CoV-2. Cells with no cytopathic effect or negative rRT-PCR results were passaged after 7 d onto fresh confluent Vero-81 and monitored for an additional 7 d before performing rRT-PCR again. Viral replication in these specimens was defined as a decrease in C t  over the culture period.\n\n【8】We used the paired BinaxNOW and rRT-PCR results to calculate the BinaxNOW PPA, NPA, negative predictive value (NPV), and positive predictive value (PPV), using C t  < 37 to define rRT-PCR–positive specimens. As described in Pilarowski et al. we also calculated performance by using C t  <30 to define rRT-PCR–positive specimens. The exact binomial method was used to calculate 95% CIs. Comparison of mean C t  was performed using the Welch t-test. We performed statistical analyses using R version 4.0.1 .\n\n【9】This activity was reviewed by the Centers for Disease Control and Prevention (CDC) and was conducted consistent with applicable federal law and CDC policy (45 C.F.R. part 46, 21 C.F.R. part 56; 42 U.S.C. §241(d); 5 U.S.C. §552a; 44 U.S.C. §3501 et seq.). In addition, this activity was conducted as part of a COVID-19 project determined to be nonresearch by the California Health and Human Services Agency’s Committee for the Protection of Human Subjects.\n\n【10】### Results\n\n【11】Including testing performed in round 0 and results reported by outside laboratories from staff seeking testing on their own, the cumulative incidence over the course of the outbreak in the entire staff was 62.3% (/563). A total of 342 different staff participated in testing rounds 1 through 6. These persons ranged in age from 18 to 92 years (median 52 years). Self-reported race and ethnicity produced cell sizes that are too small to report, so only Hispanic ethnicity is presented in this study. Most staff identified as Hispanic (.0%) . Symptoms were reported by 11 different persons at the time of testing, which accounted for 11/769 (.4%) of collected paired specimens. A total of 6 persons were hospitalized, and 1 of those patients died. The number of staff tested in each round, which varied because of attrition and exclusion of SARS-CoV-2–positive staff from further testing, ranged from 333 persons (round 1) to 57 persons (round 4). The number of rRT-PCR–positive results in each round ranged from 98 (round 1) to 0 (round 4) .\n\n【12】In total, 769 valid, paired rRT-PCR and BinaxNOW antigen test results were reported and analyzed. Among all paired testing rounds with rRT-PCR, BinaxNOW produced these results when rRT-PCR tests with C t  < 37 were considered positive: PPA, 43.3% (% CI 34.6%–52.4%); NPA, 100% (% CI 99.4%–100.0%); PPV, 100.0% (% CI 93.5%–100.0%); and NPV, 89.9% (% CI 87.5%–92.0%). When only rRT-PCR tests with C t  <30 were considered positive, BinaxNOW produced these results: PPA, 55.6% (% CI 45.2%–65.6%); NPA, 100% (% CI 99.5%–100%), PPV, 100.0% (% CI 93.5%–100%); and NPV, 93.8% (% CI 91.8%–95.5%) .\n\n【13】Of 127 rRT-PCR–positive specimens, BinaxNOW detected 55, did not detect 72 (specimens with C t  <30, 5 specimens with C t  < 20, and 6 specimens with positive viral cultures), and produced no false-positive results . Among rRT-PCR–positive specimens, those with paired BinaxNOW-positive results had a lower mean C t  (.8) than those with paired BinaxNOW-negative results (.5) (p < 0.001). No rRT-PCR–positive results with a C t  \\>29.4 were detected by BinaxNOW .\n\n【14】In dual-positive pairs, the median time between rRT-PCR specimen collection date and results reported date was 4 days (range 1–6 days). For BinaxNOW false-negative pairs, the median time between rRT-PCR specimen collection date and results reported date was 5 days (range 1–7 days). In contrast, the 15-minute read time of the BinaxNOW antigen test kit provided results to the facility and LHD the same day as testing.\n\n【15】Of the 127 rRT-PCR–positive specimens, we attempted virus isolation and culture for all 100 specimens with C t  <30. Of those specimens, 51 resulted in positive virus isolation. Of those culture-positive specimens, 45 (.2%) were BinaxNOW-positive . The mean C t  of culture-positive specimens (.4) was significantly lower than culture-negative specimens (.5) (p<0.001).\n\n【16】### Discussion\n\n【17】In the setting of a nonhealthcare workplace outbreak of COVID-19 with high attack rate (.3%), we found that BinaxNOW was a useful adjunct to rRT-PCR testing. BinaxNOW showed NPA and PPV of 100%. A total of 55 participants were concordantly identified as positive by BinaxNOW and rRT-PCR, and no false-positive BinaxNOW results were noted. This low false-positive rate is consistent with results from Pilarowski et al. that established the updated BinaxNOW card-reading technique used by the racetrack physician in this outbreak. Results of BinaxNOW testing were available the same day, which enabled more rapid identification of infected workers for isolation than reliance on rRT-PCR alone.\n\n【18】Negative BinaxNOW results were less concordant with rRT-PCR results. The PPA of BinaxNOW was 43.0% and the NPV was 89.9%. Real-time RT-PCR confirmation of BinaxNOW negative results identified 72 additional positive specimens. The median time between rRT-PCR specimen collection date and results reported date for these BinaxNOW false-negative specimens was 5 days (range 1–7 days).\n\n【19】Although C t  cannot be used to define viral load or infectivity of a given person, C t  is inversely related to the amount of target genetic material present in the specimen . Therefore, the significantly lower mean C t  for true-positive BinaxNOW specimens (.8) compared with false-negative BinaxNOW specimens (.5) indicates that more viral genetic material was present in those specimens. BinaxNOW demonstrated better concordance with positive viral culture results (.2%) than with positive rRT-PCR results (.3%). Positive viral culture is further evidence of the presence of infectious virus, so these findings might indicate that some BinaxNOW false-negative participants were not infectious at the time of specimen collection (i.e. they had low viral RNA load at the beginning or end of their infection trajectory) . Numerous factors can affect the outcome of a viral culture; therefore, negative culture results do not necessarily mean that no viable virus was present in those specimens, nor that the participants from whom those specimens were collected were not infectious at the time of specimen collection.\n\n【20】With serial BinaxNOW testing, some of the persons with discordant paired results could have tested positive with subsequent BinaxNOW testing. Further studies are needed to determine whether serial rapid antigen testing alone can identify infectious persons as efficiently as rRT-PCR alone or a combination of rRT-PCR and rapid antigen testing .\n\n【21】The first limitation of our study is that, although other studies have demonstrated differential BinaxNOW test performance in symptomatic and asymptomatic persons , we were unable to examine test performance by symptom status, because symptom reporting might not have been reliable. At the time of specimen collection, only 11 persons reported symptoms to the facility administrative employee registering them for testing. This number conflicts with data previously collected from the racetrack physician as part of a prospective cohort drug trial on this same population which, out of an enrolled cohort of 113 BinaxNOW-positive staff, identified 60 (%) persons who were symptomatic at the time of testing . This discrepancy might have resulted from staff feeling less comfortable discussing symptoms with the administrative employee versus the racetrack physician or it could be associated with the incomplete list of COVID-19 symptoms in the administrative employee’s question. It might also reflect a language barrier, because the question about symptoms was asked only in English by the administrative employee. According to onsite interactions with staff and reports from racetrack leadership, many staff were native Spanish speakers, although this language difference was not quantified.\n\n【22】Second, the BinaxNOW tests may have been performed in ambient temperatures below the manufacturer’s recommended range. The BinaxNOW test kit instructions recommend that all test components be at room temperature (°C–30°C) before use; the mean daily minimum and maximum air temperature recordings from a nearby National Oceanic and Atmospheric Administration weather station in Richmond, CA, on testing days were 7.9°C and 15.1°C . Performing BinaxNOW tests in the recommended temperature range might have improved performance.\n\n【23】Third, some missing data limit this analysis from encompassing the entire outbreak. The first mass testing dates (round 0) only used rRT-PCR testing, so no comparison with BinaxNOW was possible. Furthermore, each round of testing was intended to capture all staff who had not yet tested positive; however, participant attrition occurred between testing rounds. We attribute this attrition to the logistical obstacles of staff getting to the testing site or to staff leaving their jobs during the outbreak. More complete paired-testing data could have provided better insight as to the usefulness of rapid antigen testing when used for the entire duration of an outbreak.\n\n【24】Our results support considering BinaxNOW-positive employees as infectious without waiting for rRT-PCR confirmation. The rapid turnaround time and high PPV of BinaxNOW enabled some SARS-CoV-2–positive employees to be identified and isolated faster than if rRT-PCR had been used alone. In outbreak situations in which access to laboratory rRT-PCR services is limited, it might be reasonable to act on BinaxNOW-positive results and forgo rRT-PCR confirmation. In contrast, our findings suggest that BinaxNOW negative results in an outbreak investigation should be confirmed with rRT-PCR, because false negatives do occur.\n\n【25】Our results indicate that BinaxNOW performs better at identifying rRT-PCR–positive specimens with lower C t  (suggestive of higher viral loads) and positive viral cultures, although these factors are not precise proxies for infectiousness. Real-time RT-PCR remains a more sensitive test for identifying persons that might be infectious, and our results support the current recommendation that rRT-PCR (or another nucleic acid amplification test) should be used in outbreak situations to confirm BinaxNOW-negative results . Clinical discretion informed by COVID-19 incidence in the relevant population, as well as individual exposure history and symptoms, should be used to determine whether to quarantine persons who test negative for SARS-CoV-2 by BinaxNOW but are awaiting results of rRT-PCR testing .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "315b4321-057f-41b3-94a5-5d82c83ed937", "title": "Working to Increase Vaccination for Human Papillomavirus: A Survey of Wisconsin Stakeholders, 2015", "text": "【0】Working to Increase Vaccination for Human Papillomavirus: A Survey of Wisconsin Stakeholders, 2015\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Infection with human papillomavirus (HPV) is common and can progress to various types of cancer. HPV infection can be prevented through vaccination; however, vaccination rates among adolescents are low. The objective of this study was to assess efforts among Wisconsin stakeholders in HPV vaccination and organizational capacity for future collaborative work.\n\n【3】**Methods**\n\n【4】We conducted a cross-sectional online survey of 277 stakeholders in HPV vaccination activities, from April 30, 2015, through June 30, 2015. Stakeholders were public health professionals, health care providers, educators, quality improvement professionals, researchers, and advocates identified as engaged in HPV vaccination work.\n\n【5】**Results**\n\n【6】Of the 277 invited stakeholders, 117 (%) responded to the survey. Findings showed that most current HPV vaccination activities targeted 3 groups: adolescents and parents, clinical and health professionals, and communities and health systems. The main activities directed at these groups were providing printed educational materials, professional education, and media campaigns to raise awareness. Common barriers reported were lack of understanding about the link between HPV and cancer, requests to delay vaccination, difficulty completing the 3-dose vaccine series, and reluctance to discuss sexuality.\n\n【7】**Conclusion**\n\n【8】HPV vaccination rates are far below those of other vaccinations administered to adolescents in Wisconsin. Our study showed that various local efforts were being made to increase HPV vaccination uptake; however, many barriers exist to initiation and completion of the vaccine series. Future interventions should address barriers and employ evidence-based strategies for increasing HPV vaccination rates.\n\n【9】Introduction\n------------\n\n【10】Human papillomavirus (HPV) infection is common, especially among young adults . Almost all cervical and anal cancers and most oropharyngeal cancers are caused by persistent infection with high-risk HPV, types 16 and 18 . HPV vaccines offer the best protection to women and men who receive all 3 doses in the series and have time to develop an immune response before becoming sexually active. Initiation of HPV vaccination is recommended for girls and boys beginning when they are aged 11 to 12 years . It is also recommended for girls aged 13 to 26 and for boys aged 13 to 21 who have not been vaccinated . However, since licensure of the first HPV vaccine in 2006, vaccination rates have remained low in the United States compared with rates for other adolescent vaccinations. In Wisconsin, data from the Wisconsin Immunization Registry indicate that HPV vaccination coverage is far lower than for other vaccines administered to adolescents. In 2013, an estimated 55% of girls and 29% of boys aged 13 to 17 had initiated the HPV vaccine series, and only 34% of girls and 11% of boys aged 13 to 17 had completed the series. By comparison, 92% of adolescents in this age group have received the tetanus, diphtheria, pertussis (Tdap) vaccine, and 75% had received the meningococcal conjugate vaccine .\n\n【11】The Wisconsin Comprehensive Cancer Control Program, housed at the University of Wisconsin Carbone Cancer Center, was among 18 sites funded by the National Cancer Institute to conduct an environmental scan of HPV vaccination activities to enhance collaborations for developing research and interventions to increase HPV vaccination rates in Wisconsin . The scan involved 4 components: 1) examination of vaccination data from the Wisconsin Immunization Registry, 2) a survey of stakeholders’ HPV-related projects, 3) assessment of current University of Wisconsin–Madison research on HPV, and 4) evaluation of a quality improvement education series for health care providers. The objective of this study was to assess local interventions to increase HPV vaccination and organizational capacity for future collaborative work in Wisconsin.\n\n【12】Methods\n-------\n\n【13】### Study population\n\n【14】We conducted a cross-sectional online survey of 277 Wisconsin stakeholders in HPV vaccination activities from April 30, 2015, through June 30, 2015 . Stakeholders were public health professionals, health care providers, educators, quality improvement professionals, researchers, and advocates from the state who attended or were invited to the Wisconsin HPV Vaccine Summit in June of 2014. The summit was a conference designed to mobilize vaccine advocates by providing data on state-specific HPV disease epidemiology and immunization rates, updates on the diseases caused by HPV, vaccine safety information, and discussions on how to address vaccination hesitancy and strengthen vaccination recommendations. We also surveyed members of the 15 local or regional immunization coalitions in Wisconsin. Stakeholders received an email inviting them to participate in the online survey. Four follow-up email reminders were sent to incomplete survey respondents and nonrespondents at intervals of every 2 weeks to encourage participation.\n\n【15】### Measures\n\n【16】Study participants reported on their role in their organization and answered other organization-related questions, including type of organization, whether the organization belonged to a local or regional immunization coalition, whether the organization accessed the Wisconsin Immunization Registry, and whether the organization used the Registry for Effectively Communicating Immunization Needs and an electronic health record to track immunizations. Respondents were also asked about their organization’s interest in conducting new activities and their capacity to take on new activities to promote HPV vaccination.\n\n【17】Respondents were asked whether their organizations conducted any activities in Wisconsin in 2013, 2014, or 2015 related to HPV vaccination. The activities were classified into 4 categories:1) adolescents and their parents, 2) clinical and health care professionals, 3) communities and health systems, and 4) advocacy and public policy. If respondents reported activities in any year in any of these categories, they were asked to provide information about how often they encountered certain barriers (never, rarely, sometimes, often, or always). The selections of sometimes, often, and always were defined as barriers. In addition, respondents provided information on their funding sources and collaborating organizations for each category of activities.\n\n【18】### Statistical analysis\n\n【19】We calculated simple descriptive statistics to examine the prevalence of HPV vaccination-related activities that stakeholders conducted from 2013 through 2015 and the barriers they encountered. We then tested for trends in different activities over time. All analyses were performed with Stata/MP version 13.1 (StataCorp LP). Significance was set at _P_ less than .05.\n\n【20】Results\n-------\n\n【21】Of the 277 stakeholders we invited to participate, 117 replied, a response rate of 42.2%. Of the 117 stakeholders who participated in the survey, 37 (.6%) classified themselves as public health professionals, 21 (.9%) as health care providers, and 9 (.7%) as educators . Thirty (.6%) of the respondents worked for local public health departments, 8 (.8%) worked for state health departments, and 22 (.8%) for health care providers/health systems (categories were not mutually exclusive). Fifty-six (.9%) indicated that they or their organization belonged to a local or regional immunization coalition, 71 (.7%) reported that their organizations currently accessed the Wisconsin Immunization Registry, 6 (.1%) used the Regional Early Childhood Immunization Network, and 32 (.4%) used an electronic health record to track immunizations. Over half of respondents 67 (.3%) expressed a medium to high level of interest in conducting new activities to promote HPV vaccination. However, only 46 (.3%) rated their capacity to take on new activities as adequate (medium or high capacity).\n\n【22】Stakeholders reported on activities that their organizations conducted in recent years to increase HPV vaccination. Ninety-four (.3%) reported that their organizations were involved in activities focusing on adolescents (girls or boys aged 11–18 y) and their parents. In 2015, the most common activities involving adolescents and their parents were providing printed educational materials (.2%), one-on-one consultations to adolescents and parents on HPV vaccination (.7%), reminders to adolescents and parents of when adolescents were due for HPV vaccination (.9%), and referrals to HPV vaccination services (.2%). The pattern was similar for 2013 and 2014, and the prevalence of these 4 types of activities increased significantly over time (P_ < .05) .  \n\n【23】Percentage of Wisconsin stakeholder organizations (N = 117) reporting activities to increase human papillomavirus vaccination in Wisconsin, 2013–2015. Figure 1a shows stakeholder organizations with activities focused on adolescents and parents, 1b shows stakeholder organizations with activities focused on clinical and health professionals, 1c shows stakeholder organizations with activities focused on communities and health systems, and 1d shows stakeholder organizations with activities focused on advocacy and public policy. The rates of advocating for public policy change and advocating for increased public funding increased over time (P_ < .05). \n\n【24】Overall, 75 respondents reported barriers to activities focusing on adolescents and parents. The most common barriers were lack of education or understanding about HPV infection, including its link to cancer (.6%); logistical or other barriers to returning for the full series of 3 shots (.7%); requests that HPV vaccination be deferred (.7%); belief that the adolescent is not at risk for HPV infection (.6%); and the parent’s belief that child is too young for the HPV vaccine (.5%).  \n\n【25】Percentage of Wisconsin stakeholder organizations (N = 117) reporting barriers to human papillomavirus (HPV) vaccination, from 2013 through 2015. Figure 2a shows percentage of stakeholder organizations reporting barriers related to adolescents and parents (n = 75), 2b shows the  \npercentage of stakeholder organizations reporting barriers related to clinical and health professionals (n = 54), 2c shows the percentage of stakeholder organizations reporting barriers related to communities and health systems (n = 43), and 2d shows the percentage of stakeholder organizations reporting barriers related to advocacy and public policy (n = 10). Abbreviation: STI, sexually transmitted infection. \n\n【26】Seventy-six of the 117 respondents (.0%) indicated that their organizations were conducting activities focusing on clinical and health professionals to increase HPV vaccination. The most commonly reported activities in 2015 were providing printed educational materials (.0% of stakeholders), professional education on HPV vaccination (.0%), protocols to track and report on HPV vaccination rates (.0%), and websites focused on health professionals (.1%) . The increase in all activity rates from 2013 to 2015 was significant (P_ < .05). Barriers most often encountered by these organizations (N = 54) were difficulty ensuring that patients completed the 3-dose HPV vaccination series (.0%), concern that parents would decline HPV vaccination despite appropriate counseling (.7%), reluctance to discuss sexuality or sexually transmitted infections (.3%), concern about the time it takes to discuss HPV vaccination with patients or parents (.5%), and HPV vaccination not being required by schools (.1%) .\n\n【27】Sixty-one stakeholders (.1%) identified their organization’s activities as projects focusing on communities and health systems. The most commonly reported activity in this category in 2015 was media campaigns to raise awareness about the need for HPV vaccination of adolescents (.3%,), followed by community committee or work groups that focus on HPV vaccination (.8%), support for community groups that fund and promote the vaccine (.1%), and community events or health fairs to promote HPV vaccination (.4%) . The increase from 2013 through 2015 in these activities was significant (P_ < .05). Barriers experienced in implementing community- and health-system–focused activities (N = 43) were lack of education or understanding about HPV infection, including its link to cancer (.1%); difficulty ensuring that patients will complete the 3-dose HPV vaccination series (.9%); belief that younger adolescents are too young for the HPV vaccinations (.8%); and reluctance to discuss sexuality or sexually transmitted infections (.7%) .\n\n【28】Lastly, 18 respondents (.4%) indicated that they were involved in activities focused on advocacy and public policy with the most commonly reported activity in 2015 being efforts to increase HPV vaccination rates through advocating public policy change (respondents), followed by advocating for increasing public funding for HPV vaccination (respondents) and advocating for increased public funding for HPV research (respondents) (categories were not mutually exclusive). The rates of the former 2 activities increased over time (P_ < .05) . Barriers to these activities that respondents (N = 10) reported were described as lack of provider recommendations for HPV vaccination (respondents), concerns about vaccine safety (respondents), and reluctance to discuss sexuality or sexually transmitted infections (respondents) \n\n【29】Most organizations reported working with partners on activities promoting HPV vaccination: activities that focused on adolescents and parents (of 94 respondents), activities that focused on clinical and health professionals (of 76 respondents), activities that focused on communities and health systems (of 61 respondents), and activities that focused on advocacy and public policy (of 18 respondents). Commonly reported collaborators were the Centers for Disease Control and Prevention (CDC), the Wisconsin Immunization Program, local health departments, local clinics and hospitals, schools, local immunization coalitions, the Wisconsin Comprehensive Cancer Control Program, and the Wisconsin chapter of the American Academy of Pediatrics. Primary sources of funding for HPV vaccination activities were CDC, the Wisconsin chapter of the American Academy of Pediatrics, the Wisconsin Department of Health Services, the University of Wisconsin Partnership Program, the Wisconsin Immunization Program, immunization coalitions, the Wisconsin Comprehensive Cancer Control Program, and tax levy funds.\n\n【30】Discussion\n----------\n\n【31】Persistent HPV infection can lead to cancer of the cervix, vulva, vagina, penis, anus, and oropharynx . The HPV vaccine is safe and effective and has been recommended since 2006 . However, HPV vaccination coverage has remained far below that of other vaccinations administered to adolescents, in Wisconsin and throughout the United States. To assess efforts to increase HPV vaccination in Wisconsin and barriers encountered, we conducted a survey of stakeholders involved in the effort. Findings from the survey illustrate the HPV vaccination-related projects being conducted in Wisconsin and provide a roadmap for future collaboration and activities to increase HPV vaccination.\n\n【32】Our survey of stakeholder activities found that ongoing work is focused on 4 main groups: adolescents and parents, clinical and health professionals, communities and health systems, and stakeholders involved with advocacy and public policy. HPV vaccination promotion activities increased across all these groups from 2013 through 2015. Many survey respondents reported conducting activities that were focused on adolescents and parents, the most common of which were the provision of educational materials, one-on-one consultations and referrals, and reminders for vaccination. The main activities used to influence clinical and health professionals were provision of educational materials, professional education, and protocols to track and report vaccination rates. These activities are in line with research, which indicates that educational campaigns directed at health care professionals and parents are key facilitators for promoting HPV vaccination . Examples of educational efforts to raise awareness about HPV and HPV vaccination are media campaigns, printed materials, online information, webinars, conferences, and in-person training sessions. Of the activities designed to affect communities and health systems, media campaigns to raise awareness were used most often, followed by health fairs and community group collaborations. We identified fewer activities in the area of advocacy and public policy, but among the activities reported were advocating for public policy change and advocating for increased public funding for vaccination. HPV vaccination promotion activities in Wisconsin are similar to efforts conducted at the health care practice or community level in other parts of the United States. The effectiveness of interventions to increase HPV vaccination has varied, and further research is needed .\n\n【33】As in other research, Wisconsin stakeholders reported several main barriers to vaccination across groups: lack of understanding about HPV and its link to cancer, reluctance to discuss sexuality, parental resistance to the vaccination, belief that the adolescent is too young or is not at risk, difficulty with completion of the 3-dose vaccine series, safety concerns, and concern about promoting risky sexual behavior . Health care providers have been identified as uniquely positioned to address barriers perceived by parents. They can initiate conversations to address concerns and misunderstandings and recommend the vaccine . Studies have shown that parents and patients want additional information about HPV and the HPV vaccine and that they trust the information provided by health care providers . Providers need additional training on how to give a strong, clear recommendation and communicate the urgency and importance of HPV vaccination . Other potential strategies for providers and health care systems to increase HPV vaccination rates are reducing missed clinical opportunities and administering the HPV vaccine at the same time as other adolescent vaccines . To reduce the number of HPV-related cancer deaths in Wisconsin and reach the Healthy People 2020 goal of 80% HPV vaccination coverage, continued efforts are needed to cultivate provider champions for the vaccine, educate the public about its safety and efficacy, increase access to vaccination, and reduce missed clinical opportunities to vaccinate . The consistency of barriers cited across target groups related to the lack of understanding about HPV, the HPV vaccine, and HPV related cancers indicates that future efforts should focus on enhanced education and communication strategies for providers, parents, adolescents, and communities.\n\n【34】The Advisory Committee on Immunization Practices recommends that young adolescents (aged 9–14 y) move from a 3-dose to a 2-dose HPV vaccine schedule . This change presents new opportunities for increasing adherence. The change, also recommended by CDC, was based on a review of clinical trials showing that 2 doses of the vaccine among young adolescents produced an immune response similar to, or higher than, the response in young adults (aged 16–26 y) who received 3 doses . Because survey respondents cited logistical challenges to returning for the series of 3 shots as a barrier, the move to a 2-dose schedule — which still provides safe, long lasting, effective protection — may increase acceptance and improve vaccination rates among young adolescents . More than half of survey respondents expressed interest in conducting new activities to promote HPV vaccination. However, because less than 40% said they had the organizational capacity to do so, policy efforts should focus on increasing funding for HPV vaccination education and promotion. HPV vaccination rates in Wisconsin have increased in recent years. In 2015, the overall vaccination rate for adolescents aged 13 to 18 years was 44%, and in 2016 the rate was 49% .\n\n【35】A strength of our study is that it is the first statewide, comprehensive review of HPV vaccination activities in Wisconsin. A limitation is that our list of Wisconsin HPV vaccination stakeholders may be incomplete, and we do not have information about HPV vaccination-related activities conducted by stakeholders who were not identified. However, the participant list was reviewed by representatives of the Wisconsin Immunization Program, the Wisconsin chapter of the American Academy of Pediatrics, and the University of Wisconsin Immunization Taskforce to identify missing stakeholders engaged in current or recent HPV vaccination work who were then added to the group. Another limitation is that the survey response rate was moderate (.2%), but it was within the range (%–60%) recommended for surveys to inform key policies and resource allocation . A comparison between survey respondents and nonrespondents indicated that the geographic distribution was similar between the 2 groups, suggesting that the survey findings were representative of the state.\n\n【36】The Wisconsin HPV stakeholder survey found that activities promoting HPV vaccination are wide-ranging and have increased over time across the state. Future interventions should address identified barriers and employ evidence-based strategies for increasing HPV vaccination rates.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a01577c3-0ce4-4046-86df-cb857e07615e", "title": "Isolation of Porcine Epidemic Diarrhea Virus during Outbreaks in South Korea, 2013–2014", "text": "【0】Isolation of Porcine Epidemic Diarrhea Virus during Outbreaks in South Korea, 2013–2014\n**To the Editor:** Porcine epidemic diarrhea (PED) is an acute infectious diarrhea caused by the PED virus (PEDV), which belongs to the order _Nidovirales_ , family _Coronaviridae_ , genus _Alphacoronavirus_ . The virus is transmitted mainly through fecal–oral routes and infects all age groups of pigs; the most severe form of disease occurs in suckling piglets . PEDV was first reported in South Korea in 1992 , with the occurrence of an outbreak, and has since circulated with considerable genetic diversity . During 2013, PED outbreaks reoccurred in South Korea; however, the emerging PEDVs in these outbreaks were not variants of previous Korean isolates or attenuated vaccine strains . We report on a field isolate of a novel emerging PEDV and the isolate’s genetic relationship with other PEDV strains.\n\n【1】During October 2013–June 2014, dead piglets and fecal swabs from 9 provinces of South Korea were sent to the Department of Veterinary Medicine Virology Laboratory at Seoul National University to confirm diagnoses of enteric viral diseases. All samples (intestine samples of dead piglets and 16 fecal swabs) were found to be PEDV positive. Attempts to isolate the field strains of PEDV on Vero cell lines followed a previously described protocol with modifications . An overnight monolayer of Vero cells (%–100% confluence) was washed twice with 1× phosphate-buffered saline before homogenized samples (.02 µm filtered) were inoculated with 10% suspension. After 30 min absorption at 37°C with 5% CO 2  , maintenance medium (Dulbecco’s Modified Eagle Medium supplemented with trypsin \\[10 µg/mL\\]), yeast extract (.04%), tryptose phosphate broth (.6%), and Antibiotic-Antimycotic 100× (µl/mL; Gibco, Thermo Fisher Scientific, Grand Island, NY, USA) were added at a ratio of 1:10. The inoculated cells were cultured for 3–4 days at 37°C in 5% CO 2  atmosphere and were blindly passaged 5 times. One field strain of PEDV (named BM1) was successfully adapted for growth on Vero cells. This virus was isolated from a 60-sow farm (identified as BM farm) that had not vaccinated its animals against PEDV. Pigs of all ages from the farm showed clinical symptoms of diarrhea, and death occurred for 100% of suckling piglets and 10% of sows. Examination at necropsy revealed that the dead piglets from BM farm were covered with brown blotches of dried diarrheal feces and their stomachs were filled with undigested milk. Thin, translucent small intestines that contained yellow fluid were also observed . The BM1 PEDV field isolate induced cytopathic effects of rounded shape  within 48 hours at passage 10. The presence of PEDV in the cell culture was confirmed by immunofluorescence assay (VDPro PEDV FA Reagent kit, MEDIAN Diagnostics, Gangwon-do, South Korea), which showed the specific fluorescence signal . In addition to evidence by microscopic observation, real-time reverse transcription PCR showed that the quantity of viral RNA increased incrementally as the number of passages increased: from 30,325 copies/μL (cycle threshold 16.11) at passage 2 to 418,000 copies/μL (cycle threshold 13.77) at passage 10. Infective titers of the BM1 isolate increased from 10 4.7  50% tissue culture infectious doses/mL at passage 2 to 10 7.9  50% tissue culture infectious doses/mL at passage 10 .\n\n【2】The complete S gene of BM1  was sequenced for genetic characterization; the gene was 4,161-nt long and encoded 1,386 aa. The spike protein of the BM1 isolate showed substitutions at neutralizing SS6 epitope from LQDGQVKI  to SQSGQVKI but identity at the SS2  and 2C10  neutralizing epitopes. The genetic relationship of the BM1 isolate with other PEDVs in the world was inferred from a codon-based alignment of 409 sequences of the complete S gene . The maximum-likelihood phylogenetic tree was constructed by using the FastTree program , with the general time reversible nucleotide substitution model. The phylogeny constructed on the basis of the complete S gene  showed that the BM1 isolate belongs to subgroup 2a, genogroup 2 of PEDV. This isolate clustered closely with emergent PEDV strains in the United States , showing 99.2%–99.7% identity with PEDVs of North American strains . This observation was repeated by the phylogenetic inference of the complete N gene . The branching pattern  clearly showed that BM1 is genetically less related (.9–93.4% identity) to the live vaccine strains that are derived from genogroup 1 and used currently to prevent PEDV infections in South Korea.\n\n【3】In summary, we isolated the BM1 strain  in South Korea from a sample from a suckling pig with severe diarrhea; the pig came from a farm that had not vaccinated its pigs against PEDV. The strain was adapted and grew to high titers on Vero cells. The isolate belongs to genogroup 2 and genetically clustered with emerging PEDVs of North American strains but was loosely related to genogroup 1, the basis of the vaccine used for inoculation against Korean PEDV strains. This isolate may need further evaluation as a candidate for a vaccine to prevent reemerging PEDVs in South Korea.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b31c1acc-dd0d-46e5-9280-c54328f490bf", "title": "Severe Ehrlichia chaffeensis Infection in a Lung Transplant Recipient: A Review of Ehrlichiosis in the Immunocompromised Patient", "text": "【0】Severe Ehrlichia chaffeensis Infection in a Lung Transplant Recipient: A Review of Ehrlichiosis in the Immunocompromised Patient\nSince the discovery in 1987 of _Ehrlichia_ as a cause of tick-borne disease in humans  , ehrlichiosis has been recognized as an increasingly important cause of acute febrile illness . The two main pathogenic species are _Ehrlichia chaffeensis_ , which causes human monocytic ehrlichiosis (HME), and the as-yet-unnamed agent of human granulocytic ehrlichiosis (HGE)  . A third species, _E. ewingii,_ which has been recently described, causes clinical disease indistinguishable from infection caused by _E. chaffeensis_ or the agent of human granulocytic ehrlichiosis  .\n\n【1】Delineation of the epidemiology of human ehrlichiosis has greatly enhanced our understanding of this emerging infection. However, information on the manifestations of ehrlichiosis in immunocompromised patients is limited. We report a case of severe monocytic ehrlichiosis in a lung transplant recipient who had pancytopenia, acute renal failure, and encephalopathy. Despite early diagnosis and treatment with doxycycline, his illness progressed and took on features of thrombotic thrombocytopenic purpura (TTP). A review of reported cases of _Ehrlichia_ infection in immunocompromised patients shows that the infection is far more severe in this population and is often fatal.\n\n【2】### Case Report\n\n【3】A 38-year-old man with cystic fibrosis had undergone bilateral lung transplantation in 1998 and had been well. In September 2000, he visited a physician with a 3-day history of fever as high as 38.3°C, myalgias, and headache. A resident of Columbia, Missouri, the patient had spent much time outdoors but did not recall tick infestation or recent tick bite. His medications included cyclosporine, mycophenolate, prednisone, diltiazem, trimethoprim-sulfamethoxazole, and valacyclovir.\n\n【4】On physical examination, the patient appeared acutely ill with temperature 38.3° C, blood pressure 140/64, heart rate 110 per minute, and respiratory rate 20 per minute. He was lethargic but could follow commands, and his neurologic exam was unremarkable. Fine bibasilar crackles were present bilaterally, but heart sounds were normal. Examination of the abdomen was negative. Synovitis was not evident, and no cutaneous lesions were found.\n\n【5】The leukoctye count was 3.7x10 9  per L with 68% neutrophils, hemoglobin was 64 g/L, and platelet count was 23,000/L. Serum creatinine was 4.6 mg/dL, aspartate aminotransferase 420 U/L, alanine aminotransferase 96 U/L, and bilirubin 3.2 mg/dL. International normalized prothrombin time ratio (INR) was 1.4. Examination of a peripheral blood smear showed schistocytes and other microangiopathic changes.\n\n【6】Multiple blood cultures were negative. Cytomegalovirus DNA was not detected in peripheral blood. Noncontrast computed tomography of the brain was normal. Chest radiograph showed bilateral infiltrates.\n\n【7】The patient was treated initially with intravenous piperacillin-tazobactam and vancomycin. Cyclosporine and trimethoprim-sulfamethoxazole were discontinued. The next day, his mental status continued to deteriorate. Lumbar puncture was deferred because of thrombocytopenia. Antibiotic therapy was changed to intravenous meropenem. Four days after admission, the bone marrow was examined because of worsening pancytopenia; intracytoplasmic morulae were seen in monocytoid cells, characteristic of monocytic ehrlichiosis . Leukocytes in a peripheral blood smear also contained morulae. Intravenous doxycycline was begun for treatment of presumed _Ehrlichia_ infection. Whole-blood polymerase chain reaction (PCR) (Viromed, Minneapolis, MN) in the first week of illness was subsequently reported positive for _E. chaffeensis_ DNA. Serology by immunofluorescence antibody testing for both _E. equi_ and _E. chaffeensis_ performed 2 weeks after onset of illness was negative, with titers <1:40.\n\n【8】Despite treatment with doxycycline, the patient’s confusion, thrombocytopenia, and microangiopathic anemia did not improve, and on the fifth hospital day he was transferred to the University of Wisconsin Hospital and Clinics. Physical examination showed blood pressure 144/94 mmHg, heart rate 77/minute, temperature 36.5ΕC, and respiratory rate 24/minute. Multiple ecchymoses were present on the torso and extremities. Neurologic examination was nonfocal. There were coarse bibasilar crackles in the lungs bilaterally. Examination of the heart and abdomen was unremarkable.\n\n【9】Leukocytes were 2.9 x 10 9  /L, hemoglobin 86 g/L, and platelets 30,000/L. Serum creatinine was 6.2 mg/dL (mol/L), total bilirubin 2.0 mg/dL, aspartate aminotransferase 105 U/L, and alanine aminotransferase 55 U/L. INR was 1.1, and the activated partial thromboplastin time was 26 seconds. A peripheral blood smear showed numerous fragmented red blood cells. Chest radiograph showed persistence of bilateral infiltrates.\n\n【10】The patient’s fever resolved 2 days after doxcycline was started; however, oliguric renal failure necessitated hemodialysis. Hematologic studies showed progressive microangiopathic anemia and thrombocytopenia with a normal INR, suggestive of TTP, presumably secondary to _Ehrlichia_ infection. Daily plasmaphereses were begun and continued for 8 weeks. Gradually the hematologic abnormalities and renal function improved, and the patient’s mental status returned to normal. Doxycycline was given for 2 weeks.\n\n【11】The patient ultimately made a full recovery with no apparent sequelae. Cyclosporine was not resumed, and he was maintained on sirolimus and prednisone to prevent transplant rejection. No rejection occurred, despite a reduction in immunosuppressive therapy during the treatment of _Ehrlichia_ infection.\n\n【12】### Review of Published Reports\n\n【13】Ehrlichiosis is a zoonotic illness caused by _Ehrlichia_ species, which are pleomorphic, intracellular, rickettsia-like organisms . The clinical spectrum of ehrlichiois varies from a mild, influenzalike illness to a fulminant sepsis syndrome, but in most patients is self-limiting and not fatal. Death rates of documented ehrlichial infection in large, unselected series have been 1% to 8% . This low rate contrasts sharply with the high death rate of ehrliochiosis in immunocompromised patients .\n\n【14】Cellular immunity represents the most important host defense against rickettsial infection  . Acute-phase sera of patients with HGE contain elevated levels of interferon gamma, which is associated with the clearance of _Ehrlichia_ from peripheral blood  . In a mouse model of ehrlichiosis, immunocompromised mice have persistent infection, and most eventually die  . Impairment of cellular immunity, whether from immunosuppresssive therapies or underlying disease, retards recovery, leading to more severe disease and higher death rates.\n\n【15】The population of immunocompromised patients is large and growing; many have asplenia or solid organ or bone marrow transplants  . An analysis of the published reports of ehrlichial infection shows that the disease in immunocompromised patients is far more severe and prolonged and more likely to be fatal  . Virtually all these patients had signs of organ dysfunction, including pancytopenia (%), renal failure (%), respiratory distress (%), shock (%), and neurologic dysfunction (%). Six (%) of 23 patients died; 4 of the 6 deaths were in HIV-infected patients. Two patients died within 24 hours after coming to medical attention, despite initiation of appropriate antimicrobial therapy; in the third, the diagnosis was not considered until late in the hospital course; and in the fourth, the diagnosis was made postmortem. Two deaths occurred in asplenic patients; in both, _Ehrlichia_ infection was not suspected until 1 week after onset of illness.\n\n【16】In a recent series of ehrlichial infection in 21 HIV-infected patients, 6 of which are included in our review, Paddock et al. reported a high frequency (%) of moderate to severe disease in HIV-infected patients, particularly with _E_ . _chaffeensis_  . Low CD4 counts were associated with a poor outcome.\n\n【17】### Discussion\n\n【18】To our knowledge, this is the first reported case of acute ehrlichiosis in a lung transplant recipient. Our patient had laboratory features typical of _Ehrlichia_ infection (thrombocytopenia, leukopenia, and transaminase elevation). However, he also had microangiopathic anemia, renal failure, and neurologic dysfunction characteristic of TTP. Ehrlichiosis with features of TTP has been described in two reports , one case each of HME and HGE. Both cases were in immunocompetent persons: one was treated with doxycycline and plasmapheresis; in the other, the diagnosis was made postmortem.\n\n【19】Our patient's multiorgan failure and hematologic aberrations persisted, despite doxycycline therapy, until he underwent plasmapheresis. He was receiving cyclosporine, which is a well-known cause of a rare hemolytic uremic syndrome-TTP-like condition that does not respond to plasmapheresis and nearly always proves fatal  . That our patient’s TTP-like illness coincided with _Ehrlichia_ infection and responded to doxycycline and plasmapheresis makes it most likely that it was a consequence of acute ehrlichiosis, not cyclosporine.\n\n【20】Neurologic manifestations, ranging from confusion to frank meningitis, have been reported in up to 20% of patients with erhlichiosis  . Our patient had obtundation and delirium that persisted after doxycycline therapy was initiated and his fever had resolved. The presence of headache and confusion in conjunction with pancytopenia and transaminase elevation should raise suspicion of _Ehrlichia_ infection, especially if the patient has had potential tick exposures.\n\n【21】The diagnosis of ehrlichiosis is often delayed because of its nonspecific clinical and laboratory manifestations. In the immunocompromised person, the search for opportunistic infections may further preclude consideration of _Ehrlichia_ infection. The empiric antimicrobial regimens used in immunocompromised patients for suspected cryptogenic bacterial and fungal sepsis rarely include a drug or drugs effective against _Ehrlichia_ . PCR to detect _Ehrlichia_ DNA is invaluable for the diagnosis and has >90% sensitivity and even better specificity  . This technique is particularly useful in the immunocompromised host in whom rapid diagnosis is of utmost importance. Peripheral blood and bone marrow examinations show intracellular morulae in HME in only 1% to 5% of cases and cannot be relied on diagnostically, unless positive. Serologic testing does not allow rapid diagnosis and may be negative in the immunocompromised patient  , as was the case with our patient.\n\n【22】The diagnosis of ehrlichiosis should be considered in any patient with fever, transaminase elevations, and new-onset thrombocytopenia or leukopenia who has had potential tick exposures in an endemic area. In the immunocompromised host, clinical manifestations are more severe and can include neurologic deterioration, multiorgan failure, and even a TTP-like illness. Response to appropriate therapy with doxycycline may be delayed. A high index of suspicion, the use of PCR for confirmatory diagnosis and early empiric therapy can be life-saving.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d4d7375-7144-4293-9c63-f56e8b6dd117", "title": "Decline of Tuberculosis Burden in Vietnam Measured by Consecutive National Surveys, 2007–2017", "text": "【0】Decline of Tuberculosis Burden in Vietnam Measured by Consecutive National Surveys, 2007–2017\nDespite global progress in preventing and controlling tuberculosis (TB), reducing worldwide burden has fallen short of the World Health Organization’s (WHO) End-TB elimination target . However, in most high-burden countries, estimates of burden and its trend over time are derived indirectly . One main factor that impedes global efforts to estimate the effects of and eliminate TB is the gap in case detection. In most high-burden lower-middle income countries, an unknown proportion of incident TB remains undiagnosed and unreported . To estimate TB incidence, WHO has applied the results of nationwide prevalence surveys to a model of disease duration distribution, among other estimation methods . These surveys were administered to determine a country’s gaps in detecting TB cases, help plan interventions, and estimate the resources required . Prevalence surveys measure TB burden directly; surveys that are repeated have particular strategic importance. Comparing TB prevalence over time enables public health authorities to assess the trend in burden and therefore to evaluate the effect of TB control interventions between surveys and develop policies to guide future actions .\n\n【1】Vietnam is among the 30 countries with the highest burden of TB in the world . In 2006–2007, the Vietnam National TB Program (NTP) conducted the first national TB prevalence survey, which identified 307 (% CI 249–366) pulmonary TB cases/100,000 adult participants . Since then, to reduce the TB burden in Vietnam NTP has applied a broad package of TB control interventions . In addition to strengthening routine TB care and treatment, NTP has introduced new TB diagnostics, TB drugs, and treatment regimens for multidrug-resistant TB patients, preventive treatment for children ≤5 years of age living with ≥1 additional TB patients, household contact tracing, and active case finding . Although after 2007 the TB notification rate in Vietnam declined , it remained unknown to what extent this represented a decrease in TB burden.\n\n【2】In 2017–2018, NTP administered a second national TB prevalence survey in Vietnam, which showed 322  TB cases/100,000 adults . These findings suggested no change in burden over the 10-year period. However, to be in line with WHO recommendations, the second survey used TB screening procedures and diagnostics that were state-of-the-art in 2017–2018 but not available in 2006–2007 . These methods differed from those used in the first survey, potentially affecting estimates of trends in prevalence. To enable direct comparisons, we also analyzed data from the second survey using screening and diagnostic methods similar to those used in the first survey. To directly measure the change in TB burden over time and the effect of control interventions in Vietnam, as much as possible we compared the results of the 2 surveys based on the same methods with regard to sampling, screening, laboratory testing, case definitions, and statistical adjustments. Our primary comparison considered the adjusted prevalence of culture-positive TB; secondary comparisons included adjusted and unadjusted prevalence of smear-positive TB and smear-negative TB.\n\n【3】This study was given scientific and ethics approval by the Institutional Review Board of the Vietnam National Lung Hospital, under approval letter number 62/17/CTHĐKH-ĐĐ. The risks and benefits of the study were explained to participants and each signed a written informed consent. All of those with \\> 1 positive test result for TB were referred for TB treatment; we followed up to verify that they received adequate health care and treatment.\n\n【4】### Methods\n\n【5】##### Study Design and Population\n\n【6】Both the first and second surveys were cross-sectional studies using multistage cluster sampling. The sample size of both surveys was based on the estimated prevalence of TB in the country at the time of study design . Also, a frequentist approach was applied in the second survey  to ensure a large enough sample for finding a difference in TB prevalence between the 2 surveys of \\> 25%, with a power of 80%.\n\n【7】Sampling methods were similar between the 2 surveys, except for small details in the sampling frames, enumeration, and inclusion criteria . The sampling frame used in the second survey did not include stratification, whereas the sampling frame in the first survey was stratified by urban, rural, and remote areas. Enumeration criteria differed slightly; the first survey included all adult residents (≥15 years old) who had lived in households of the selected clusters for \\> 3 months, but the duration used in the second survey was ≥2 weeks. All enumerated persons who met the inclusion criteria were considered eligible for the second survey, but for the first survey, they also had to indicate that they planned to attend the survey site for screening.\n\n【8】##### Screening and Diagnostic Procedures\n\n【9】For both surveys, all participants were screened for TB based on self-reported symptoms and chest radiograph results. Those who reported TB symptoms, had TB treatment history < 2 years before the respective survey, or had chest radiograph abnormalities consistent with TB were considered screen-positive and were eligible for sputum collection and examination. For the second survey, all chest radiographs were digital, whereas in the first survey, only two thirds of the clusters used digital chest radiographs . The criteria for defining screen-positive participants based on symptoms differed slightly between the 2 surveys. In the first survey only those reporting productive cough for \\> 2 weeks were considered positive on their symptom screens. In the second survey, all participants reporting any cough lasting \\> 2 weeks, as well as pregnant women reporting any cough of any duration, were considered to have a positive symptom screen. To compare the 2 surveys, we only considered participants screen-positive if they reported productive cough for \\> 2 weeks or had a history of TB treatment <2 years before the survey .\n\n【10】Recommended laboratory methods for prevalence survey estimates were followed in each survey, but guidelines have changed over time . The first survey used sputum smear microscopy and Löwenstein-Jensen (LJ) solid culture assays; the second survey used the molecular assay Xpert MTB/RIF  and BD MGIT BACTEC 960 liquid culture . To ensure comparable laboratory results between the surveys, for the second survey we also conducted sputum smear microscopy and LJ solid culture on sputum samples . Sputum sample processing was slightly different between surveys. For the first survey we used 4% NALC-NaOH to decontaminate the specimen before LJ culture; for the second survey we used the modified Petrov method with 3% NALC-NaOH for decontamination.\n\n【11】##### Case Definition\n\n【12】TB case definitions used for comparison purposes were based solely on laboratory results in which each screen-positive participant had 1 early morning sputum sample tested for TB by sputum smear microscopy and LJ culture in designated survey laboratories. Screen-positive participants whose result from sputum smear was positive for acid-fast bacilli and whose LJ culture grew _Mycobacterium tuberculosis_ were defined as smear-positive TB cases, whereas those who had a negative sputum smear result and LJ culture grew _Mycobacterium tuberculosis_ were defined as smear-negative. Culture-positive TB cases had either smear-negative or smear-positive TB.\n\n【13】##### Data Analysis\n\n【14】We combined data from the 2 surveys into 1 database, assigning a unique personal identification code for each observation, and adapted the variables needed to compare the 2 surveys. Similar to what we reported for the second survey, data analysis was conducted on the combined database, with updated inclusion criteria and case definitions, as stated . The adjusted analysis involved multiple imputation by chained equation for missing data, including sputum smear and LJ culture results, and inverse probability weighting as recommended by WHO . To ensure the comparability of the 2 surveys and adjust for the relative sampling probability of each participant in both surveys, we applied poststratification using population data for Vietnam estimated by the General Statistics Office of Vietnam in 2007 and 2017 .\n\n【15】We assessed statistical differences in characteristics between the 2 surveys by χ 2  test and logistic regression. We calculated point estimates and 95% CIs for TB prevalence in Stata version 14.0  using the _mim_ and _svy_ commands with _pweights_ specified to adjust for design effect. We derived 95% CIs and p values using Rubin’s rules . To assess whether TB prevalence had changed between the 2 surveys, we calculated a prevalence difference on the combined dataset using Stata’s _epitab_ module (StataCorp), including the outcome “TB case Yes/No” as the dependent variable and first or second survey as the explanatory variable.\n\n【16】### Results\n\n【17】There were 94,156 participants (.7% of the eligible survey population) in the first survey and 61,763 (.8% of the eligible survey population) in the second. Among participants in the first survey, 99.9% were screened for TB symptoms and had chest radiographs; in the second survey, 93.1% were screened and had radiographs. Among all survey participants, in the first survey 7,529/94,156 (.0%) tested screen-positive; in the second, 4,595/61,763 (.3%) tested screen-positive .\n\n【18】Among screen-positive participants in the first survey, 6,780/7,529 (.1%) had available sputum smear microscopy results; 121 (.13% of 94,156 total participants) were smear-positive . This proportion was similar in the second survey with positive sputum smear results in 77 (.12% of 61,763 total participants; p = 0.835). There were 218 (.23%) participants culture-positive for _Mycobacterium tuberculosis_ in the first survey and 125 (.20%) in the second (p = 0.230). The proportion of LJ cultures growing nontuberculous mycobacteria or being contaminated was significantly higher in the second survey than in the first survey (p<0.001).\n\n【19】Overall, the crude prevalence of culture-positive TB decreased 26.1% (% CI 7.5%–44.7%) between the 2 surveys (p = 0.007), from 191 (% CI 167–218) cases/100,000 adults in the first survey to 142 cases/100,000 adults (% CI 113–169) in the second survey. The crude prevalence of smear-positive TB decreased by 53.0% (% CI 28.7%–77.2%), from 92 (% CI 78–125) cases/100,000 adults in 2006–2007 to 43 (% CI 31–59) cases/100,000 adults in 2017–2018. The crude prevalence of smear-negative TB was 99 (% CI 82–119) cases/100,000 adults in the first survey and 99 (% CI 80–122) cases/100,000 adults (p = 0.938) in the second survey .\n\n【20】Sputum smear results, LJ culture results, or both, were missing for 805/7,529 (.7%) screen-positive participants in the first survey and for 881/4,595 (.2%) in the second survey. Imputation and adjustment by inverse probability weighting and poststratification resulted in increased adjusted prevalence rates for culture- or smear-positive TB and for smear-negative TB by an average of 11.3% for the first survey and by an average of 20.0% for the second survey. The adjusted prevalence of culture-positive TB declined by 37.1% (% CI 11.5%–55.4%), from 199 (% CI 160–248) cases/100,000 adults in 2006–2007 to 125 (% CI 98–159) cases/100,000 adults in 2017–2018 (p = 0.008). This decline was 53.1% (% CI 27.0–69.7) for smear-positive, from 99 cases/100,000 adults (% CI 78–125) to 46 cases/100,000 adults (% CI 32–68) (p = 0.001). We observed no significant reduction in smear-negative TB prevalence (.3%, 95% CI: −22.0% to 49.7%; p = 0.679) .\n\n【21】When we stratified results by age and sex, we found a significant reduction (.0%, 95% CI 40.1%–78.4%; p<0.001) in the prevalence of smear-positive TB among men: 71% (% CI 32.9%–87.6%; p = 0.026) among men in the 35–44 year age group and 73.8% (% CI 22.5%–91.1%; p = 0.019) among men ≥65 years of age . The adjusted prevalence of smear-positive TB significantly decreased from the first survey to the second in rural areas (.5%, 95% CI 35.1%–89.2%; p = 0.017), and in the southern (.5%, 95% CI 8.3%–76.5%; p = 0.047) and northern (.2%, 95% CI 20.6%–84.7%; p = 0.022) regions of Vietnam ; a significant decline in culture-positive TB (.3%, 95% CI 7.4%–65.0%; p = 0.023) occurred only in rural areas . In-depth interviews in the 2 surveys showed no difference between the symptoms reported by the screen-positive participants and TB cases found in the 2 surveys, except for self-reported weight loss, which was lower in the second survey .\n\n【22】### Discussion\n\n【23】Our analyses show a 37% decline in the adjusted prevalence of culture-positive TB in Vietnam over a 10-year period between 2006–2007 and 2017–2018, equating to an average annual decline of 4.5%. This decline was mainly due to the 53% reduction in smear-positive TB, particularly because of substantial reductions among men, persons living in rural areas, and persons in the northern and southern parts of the country. We found no reduction in the adjusted prevalence of smear-negative TB. These results are in line with a 57% decline in smear-positive TB prevalence observed from 2000 to 2010 in China  and declines in culture-positive TB of 45% and smear-positive TB of 38% in Cambodia from 2002 to 2011 .\n\n【24】The second survey’s participation rate was much lower than that for the first survey , which could be explained by the difference between the household enumeration inclusion criteria of the 2 surveys; in the first survey, only those who confirmed availability to participate in the survey during fieldwork days were recorded as eligible. Also, recent economic growth in Vietnam has enabled more people to access out-of-pocket healthcare services, making attending a TB prevalence survey to receive free health checkups less attractive. The types of symptoms and proportion of participants with any symptoms suggestive of TB among screen-positive participants and culture-positive TB cases in the 2 surveys were similar, except that weight loss was more frequently reported in the first survey. This difference could be attributable to differences in training interviewers; for the first survey the interviewers were explicitly trained to ask carefully about weight loss to help ascertain the prevalence of chronic obstructive pulmonary disease, a secondary objective in the first survey only (H.B. Nguyen, unpub. data).\n\n【25】Over the decade between surveys, smear-positive TB prevalence declined, but smear-negative TB prevalence remained static. Vietnam, as a low-middle income country, has used sputum smear microscopy as a key method to diagnose TB nationwide . Microscopy cannot detect smear-negative TB, which might explain why the recorded prevalence of smear-negative TB did not change after the first survey. Of note, whereas the prevalence of smear-positive TB decreased by 64% among men, it did not change among women. This difference may be because for decades women in Vietnam have sought healthcare more often than men and experienced shorter delays . TB case finding among women, with the support of the Women’s Union, has been instrumental in reducing TB prevalence . Thus, it may be that, compared with men, women already had relatively shorter TB duration, and so prevalence was less affected by TB control interventions during the decade. In addition, the Global Adult Tobacco Survey conducted in Vietnam in 2015 found that smoking, a well-known risk factor for TB, was more frequent among men (.4%) than among women (.4%), although the smoking prevalence among men had declined slightly compared with a similar survey in 2010 .\n\n【26】The difference in TB prevalence between men and women was also reflected in the decline of the male-to-female ratio in the prevalence of culture-positive TB found in the 2 surveys by using similar methods, from 4.0:1 in 2006–2007 to 3.0:1 in 2017–2018. This ratio is still high compared with the average found in 56 previous TB prevalence surveys in low-middle income countries (.2:1) . The consistently high male-to-female ratios in both studies suggest that the difference by sex in recorded TB prevalence reflects an actual difference in disease occurrence, warranting further research.\n\n【27】We cannot be conclusive about the causes of the reduction in TB prevalence in our study because confounding factors may have affected the trend in TB burden in Vietnam. One factor is economic growth in Vietnam during 2007–2017; steady annual GDP growth rates ranged from 5.2% to 7.1% . When the economy grows, TB burden tends to decline because nutritional, housing, and working conditions improve . A decline solely due to economic growth would likely have affected smear-positive and smear-negative TB equally. However, many interventions employed by NTP in the 10 years between surveys relied on sputum smear microscopy, which cannot detect smear-negative TB, possibly explaining these divergent trends. The decline in smear-positive TB suggests that the interventions were effective and are at least partially responsible for the observed decline. Nevertheless, estimates of the burden of TB in Vietnam remained high when newer, more advanced, recommended diagnostics were applied to measure prevalence. Data based only on sputum smear microscopy and LJ culture underestimated prevalence by 60%–62% compared with prevalence estimated by using Xpert MTB/RIF rapid molecular assays (Cepheid) and the more sensitive MGIT BACTEC960 liquid culture method (BD) . It is reasonable to believe that the first survey underestimated the TB prevalence to a similar extent, which may be true for many prevalence surveys using less sensitive methods. This likelihood also underscores the need to replace microscopy with Xpert MTB/RIF as the primary TB diagnostic method for TB care nationwide.\n\n【28】Our study’s first limitation was that we could not address all of the analytic differences in screening and diagnostic technologies used in the 2 surveys. This difference may have affected the comparability of screening and diagnostic results, contributing to uncertainties surrounding the true TB burden in both surveys. For screening, digital chest radiograph was applied in all clusters in the second survey, whereas participants in only two thirds of the clusters in the first survey had digital chest radiograph, which probably resulted in lower sensitivity of TB screening in the first survey . For diagnosis, sputum processing using harsher decontamination methods during the first survey might have resulted in more false-negative culture results compared with the second survey. The difference in sputum processing could also explain the higher rate of contamination in the second survey. Second, the comparison of TB prevalence between surveys among participants in the youngest age group (years of age) was not available because no culture-positive TB case was found in the second survey among this age group. This lack of results might be due to undersampling in this age group because the time they needed to go to school or work interfered with the timing of fieldwork, as described elsewhere . We attempted to correct for this lack of data using multiple imputation and poststratification as recommended .\n\n【29】In summary, our study shows a statistically significant reduction in prevalence of smear-positive TB in Vietnam between 2007 and 2017 but no statistically significant reduction in prevalence of smear-negative TB. These results should be used by the NTP to direct efforts and resources to support TB prevention and control nationwide. In particular, our results suggest that replacing microscopy with rapid molecular diagnostic methods, such as Xpert MTB/RIF, and screening for TB using digital chest radiograph to enhance rapid case finding and treatment could further reduce the TB burden in Vietnam.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5a591dab-b16f-46c1-9506-22b909d28bfa", "title": "RSS Feeds", "text": "【0】### RSS Feeds\n\n【1】#### What is RSS?\n\n【2】RSS, which stands for _Really Simple Syndication_ , is an easy way to receive automatic updates from the Centers for Disease Control and Prevention right on your desktop or browser.\n\n【3】More information on RSS\n\n【4】#### How do you add an RSS feed from this site?\n\n【5】To view this site's feeds in your RSS news reader:\n\n【6】1.  Copy the URL of the feed below.\n2.  Following the instructions of your particular news reader, paste the URL into your reader.\n\n【7】#### Feeds\n\n【8】*   Travel Notices  \n    Be notified whenever any new or updated travel notices are posted anywhere on the Travelers' Health website .  \n\n【9】*   Updates to Yellow Book  \n    Be notified whenever any new or updated content is posted in the Yellow Book (Health Information for International Travel_ ).  ", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "89d8dcc5-6300-4def-85e5-c94c0e13739d", "title": "Underreported Threat of Multidrug-Resistant Tuberculosis in Africa", "text": "【0】Underreported Threat of Multidrug-Resistant Tuberculosis in Africa\nGlobal control of tuberculosis (TB) has been jeopardized by 2 major threats: HIV/AIDS and multidrug-resistant TB (MDR TB). MDR TB is defined as strains of _Mycobacterium tuberculosis_ that are resistant to at least isoniazid and rifampin . Drug resistance has reached alarming levels with the emergence of strains that are virtually untreatable with existing drugs. The recent report of an outbreak of extensively drug-resistant TB (XDR TB) in South Africa , with its extremely high case-fatality rate, has drawn wide attention. However, the more general problem of MDR TB, with an estimated 450,000 cases worldwide annually, has been recognized since the first World Health Organization (WHO) global survey on drug resistance in the late 1990s .\n\n【1】According to the WHO Global Report on Anti-tuberculosis Drug Resistance in the World , MDR TB strains have emerged in all regions of the world. However, despite the dramatic increase in TB rates in Africa, MDR TB appears nearly absent from this continent, which, until recently, reported the lowest median levels of drug resistance.\n\n【2】Two explanations have been most commonly put forward to explain these reported low levels of MDR TB in Africa. The first explanation is the presence of well-functioning control programs in Africa. Eighty-nine percent of the population in the WHO-defined region of Africa is covered by directly observed therapy, short course (DOTS), which is similar to the global average . However, there is discordance between purported DOTS coverage and national TB program (NTP) efficacy. Each year, countries with the lowest case detection and cure rates are clustered in the WHO-defined Regional Office for Africa (AFRO) region . This suggests that NTPs in Africa are not performing better than their Eastern European or South American counterparts, where MDR TB rates have already reached alarming levels. The functional status of many programs in Africa is difficult to assess with certainty, and the high incidence rates on that continent indicate that programs may not be functioning well. Low case-detection rates alone may not lead to development of MDR TB. Other factors that might favor development of MDR TB include the availability of drugs on the open market and a private sector that delivers drugs to the population in an unregulated fashion.\n\n【3】The second explanation is the recent introduction of rifampin in Africa. It is often stated that because rifampin was only recently introduced in Africa on a large scale, there has been relatively little time for resistance to develop. However, development of rifampin resistance may be spurred by HIV infection, and such resistance appears to develop rapidly .\n\n【4】Our research described in this report indicates the possibility of a third and straightforward explanation: field results for Africa are still incomplete, despite the recent publication of the WHO Fourth Global Report . This report is the result of independent drug resistance surveys (DRSs) conducted throughout the world following strict guidelines. The number of countries participating in the overall survey was low, especially in the Africa region. Furthermore, one might speculate that those countries capable of providing DRS data may have been the ones most likely to have a well-functioning NTP, laboratory structures, and transport networks, which would bias overall reporting. However, even if this is not the case, overall low levels of reporting make findings of the report questionable.\n\n【5】Consequently, the low level of reported drug resistance in Africa may not be an accurate reflection of reality, and the limited evaluation may be responsible for this skewed portrayal. Lack of comprehensive national DRS data from all countries in Africa is a barrier to understanding the magnitude of prevalence and incidence of MDR. In light of the added threat of XDR TB within the African context of high HIV prevalence, a thorough assessment of TB drug resistance and evaluation of data-specific deficiencies is urgently needed.\n\n【6】Our report aims to provide a more accurate assessment of MDR TB in Africa by incorporating all the latest available data and published estimates. In addition to suggesting that the effect of MDR TB in Africa is higher than previously thought, our report explores trends and relationships among case detection, treatment success, retreatment failure, and HIV rates relative to MDR TB. We attempted to identify population-based factors associated with MDR TB. Given the limited healthcare funding and substantial incidence of HIV in Africa, even a relatively low but increasing tide of MDR TB can lead to disastrous consequences for this continent.\n\n【7】### Methods\n\n【8】##### Search Strategy\n\n【9】PubMed and Medline databases were searched for reviews and original reports based on primary studies by using the keywords tuberculosis, _Mycobacterium tuberculosis_ , multidrug-resistant, and Africa. English and French language articles were reviewed.\n\n【10】##### Study Selection\n\n【11】Data were assembled into a comprehensive composite on the basis of several criteria. Published MDR TB rates from WHO were prioritized because of their reliability and because they are the result of DRSs conducted throughout the world following strict guidelines: new patients were clearly distinguished from those with previous TB treatment, and optimal laboratory performance was ensured and maintained through links with supranational reference laboratories.\n\n【12】Second, data on MDR TB were collected from peer-reviewed journals. Articles were considered only if specific criteria were met: 1) new patients needed to be distinguished from patients with prior TB treatment; 2) the number of confirmed TB cases needed to be statistically significant; and 3) the study needed to be nationwide or the population covered needed to be greater than one fourth of the total population.\n\n【13】For countries where data were unavailable through WHO reports or recent peer-reviewed journals, published formulaic estimates by Zignol et al. were used . The formula considers 9 independent variables considered likely to be associated with drug resistance and used in regression analyses. Resulting estimates compare favorably with known MDR TB values.\n\n【14】##### Data Extraction\n\n【15】The following data were tabulated for all forms of TB for all countries under scrutiny. Presurvey and postsurvey variables were annual country-specific indicators averaged from 1995 to the year of the drug resistance survey (presurvey) and from year after drug resistance survey to 2005 (postsurvey). These variables included TB incidence rate, case detection rate, treatment rates (category I: cured, completed, failure, default and success), and retreatment rates (category II: cured, completed, failure, default, and success). Year 2005 variables were published MDR TB estimates for new cases of pulmonary TB, HIV prevalence among adults 15–49 years of age, TB-related death rate, HIV/TB co-infection rate, male-to-female ratio of case notification rate, and health funding per capita (US dollars) in 2002 obtained from the United Nations Development Program .\n\n【16】Combined MDR TB rates were compiled for 39 of the 46 countries in the AFRO region. For 21 of those 39 countries, MDR TB rates were gathered from either WHO reports or peer-reviewed articles. For the remaining 18 countries, MDR TB rates were reported by using a formula described by Zignol et al. No DRS data or formulas were available for 6 countries, Cape Verde, Comoros, Liberia, Mauritania, Sao Tome and Principe, and the Seychelles, which account for only 20,475 of the 2,528,915 TB cases in Africa . Additionally, the accuracy of annual data from Mauritius was deemed questionable and thus excluded from further analysis.\n\n【17】##### Regression Analysis of Continuous Variables\n\n【18】The unadjusted associations between continuous independent variables and MDR TB rates were evaluated by using a linear correlation matrix. Independent variables highly (p<0.05) or marginally (p<0.10) predictive of the outcome in unadjusted models were included in further linear multivariable modeling. In addition, interactions between case detection, treatment failure, and retreatment failure rates were tested by using univariate linear regression analyses. All analyses were conducted in Stata Statistics version 9.2 (StataCorp, College Station, TX, USA).\n\n【19】##### Analysis of Presurvey and Postsurvey Continuous Variables\n\n【20】The change between presurvey and postsurvey averages (average value from 1995 to year of drug resistance survey, and year of drug resistance survey to 2005, respectively) for each continuous independent variable was evaluated by using the nonparametric Wilcoxon signed rank test. Given the small sample size, these variables were not assumed to be normal. A change between presurvey and postsurvey was deemed statistically significant if the p value was <0.05 and marginally significant if <0.10.\n\n【21】##### Categorical Trend Analysis\n\n【22】In addition to relationships between continuous MDR TB rates and independent factors, a categorical analysis of MDR TB was conducted. Countries were divided into 3 categories (tertiles) on the basis of MDR TB rates: (.0–1.7, 1.8–2.1, and \\> 2.2 defined as low, middle, and high MDR rates categories, respectively). For each TB-related indicator, a univariate trend across the 3 MDR TB categories (low, middle, and high) was assessed. Trend analysis was conducted by using the nptrend function in Stata Statistics version 9.2 (StataCorp), which performs a nonparametric trend test of rank sums across ordered groups. A trend was deemed statistically significant if the p value was <0.05 and marginally significant if <0.10.\n\n【23】### Results\n\n【24】##### MDR TB Rates in Africa\n\n【25】Data on prevalence of MDR TB among all TB cases collected from various WHO publications and published peer-reviewed articles are shown in Table 1 . The 39 countries considered in this study encompass ≈99% of total estimated incident or prevalent TB cases (all forms) in the AFRO region. The proportion of MDR TB among all TB cases varies from 5.8% in the Democratic Republic of Congo to virtually 0% in Kenya. The median MDR TB rate was ≈1.9% .\n\n【26】The situation in Africa for MDR TB among all combined TB cases is depicted in a series of related maps . The first map is reproduced and translated from the WHO Third Global Report published in 2004  . The coloring scheme was translated into the 3 categories used in this report, and the continent appears particularly devoid of MDR TB.\n\n【27】Panel B of the Figure includes data from various recent WHO publications, peer-reviewed journal articles, and the Fourth Global Report published in 2008 . Indications of geographic clustering are apparent; countries with a high prevalence of MDR TB are clustered in the southeastern and central regions of Africa. Panel B of the Figure also shows the 11 countries that have MDR TB rates >2.0 of all combined TB cases, including Democratic Republic of Congo, Rwanda, Equatorial Guinea, and Senegal, which were not included the WHO Third Global Report.\n\n【28】Panel C of the Figure includes formulaic estimates. On the basis of these estimates, high levels of MDR TB are not only located in the southeastern and central regions, but also in West Africa, which differs from results of the WHO Third Global Report.\n\n【29】##### Effect of Year of Introduction of Rifampin\n\n【30】Dates of rifampin introduction were available for 11 countries . Dates of introduction are clustered around the mid-1980s. The earliest and latest known introductions of rifampin were 1979 in South Africa and 1989 in Zambia. Zambia has a middle-level rate of MDR TB, South Africa has a high rate, and other countries have low rates. No clear association between date of rifampin introduction and MDR rates could be discerned.\n\n【31】##### Indicators for Continuous MDR Rates\n\n【32】Presurvey independent variables were evaluated through 3 types of linear regression analysis: correlation , univariate, and multivariate regression (data not shown). Regarding associations with MDR rates, correlation analysis showed a significant association with retreatment failure rate (p = 0.043, r 2  \\= 0.119). Several other univariate and multivariate linear models, such as treatment failure × retreatment failure and case detection × treatment failure × retreatment failure, were marginally significant. However, no other models that used presurvey data were more predictive than retreatment failure rate alone. For postsurvey variables, univariate analysis  also showed a statistically significant association between MDR rates and retreatment failure rates.\n\n【33】##### Comparison of Presurvey and Postsurvey Indicator Averages\n\n【34】Using the Wilcoxon signed rank test, we determined that 5 of 14 indicators showed a significant (p<0.05) change between presurvey and postsurvey years. Treatment default showed a decrease; incidence rate, treatment cured, treatment success, and retreatment death rate increased between time intervals.\n\n【35】##### Indicators for MDR TB Categories\n\n【36】When considered through nonparametric trend analysis, there was a marginally significant positive association between MDR TB categories and retreatment failure rates (data not shown). Median retreatment failure rates were higher within relatively higher categorized MDR TB countries.\n\n【37】### Discussion\n\n【38】Our visual mapping indicates that MDR TB is likely to be more prevalent in Africa than previous reports indicated. The latest WHO Global Report on Anti-tuberculosis Drug Resistance in the World published in March 2008  indicated rates of drug-resistant TB in Africa to be among the lowest worldwide. However, as of April 2008, 25 of the 46 countries in the AFRO region still had not completed a national study to investigate levels of MDR TB. By incorporating the latest national studies on MDR TB in Africa, and classifying countries on the basis of MDR TB rates in combined cases of TB, we found worrisome trends: 6 countries that were not included in the WHO Third Global Report published in 2004 have MDR TB rates >2.0% of all combined TB cases. This finding suggests that completing DRSs for all or most countries in the AFRO region is urgently needed and that the MDR TB threat in Africa could be much higher than originally assessed by WHO in its previous report in 2004. Drug-resistant strains, along with HIV/AIDS, are causing the biggest challenge to efficient management and control of TB.\n\n【39】The lower rates of MDR TB in Africa, when compared with rates in Eastern Europe or South America, could be related to the fact that for many years Africa was neglected and TB was not treated. Alternatively, later introduction of rifampin in drug regimens is often cited as an explanation for these low rates. However, for the few countries for which data were available, we did not find a statistically significant relationship between year of rifampin introduction and level of MDR TB.\n\n【40】We subsequently set out to investigate linear associations between country-specific factors as well as categorical trends in TB management to discriminate levels of MDR TB in Africa. Retreatment failure rate was the most predictive indicator of MDR TB rates; other variables such as average case detection rate , average TB incidence rate , TB prevalence , and HIV/TB co-infection  did not show a linear relationship. Categorical analysis showed similar results.\n\n【41】One can speculate about the reasons that retreatment failure rates may be associated with higher rates of MDR TB. The relationship may simply be an association without causation, i.e. that retreatment fails because a given patient may already have MDR TB. In that sense, retreatment failure is simply a marker for preexisting MDR TB. However, it is also possible that retreatment may be a cause of MDR TB. Current WHO recommendations for retreatment regimens could lead to development of MDR TB or XDR TB in many instances because suggested retreatment regimens potentially amount to addition of 1 drug to a failing regimen, thereby intensifying the level of drug resistance.\n\n【42】We subsequently investigated whether combining the retreatment failure rate with other indicators in our model could generate a predictive combination for MDR TB rates. Whereas the univariate linear and the multivariate regression model showed a marginally significant association with either treatment failure, case detection rate, or both, retreatment failure as a sole indicator remained the most significant. This finding may indicate that, regardless of the successful record of a given NTP in detecting cases and subsequently successfully treating them with a category I regimen, the highest MDR TB rates were found in the countries with the highest retreatment failure rates (category II).\n\n【43】We also investigated levels of variables reported in years after a DRS to determine whether the result of the survey affected these variables. Five of the 14 variables measured showed a statistically significant change over time: 4 variables (case detection rates, treatment cure rates, treatment success rates, and retreatment death rates) increased after the survey, and treatment defaulters decreased. Unfortunately, we cannot determine the effect of those 5 indicators on MDR TB rates because most countries surveyed reported only 1 DRS. Retreatment failure, which was shown to be the most predictive indicator of MDR TB rates, did not change over time. This finding suggests that MDR TB rates have increased in the years after the survey. To draw a parallel with physics, if the acceleration of an object remains constant over time, its speed increases. Therefore, it seems critical for countries who have already reported DRS results to undergo a second DRS to monitor the evolution of the MDR TB rate originally reported and help determine the effect of newly installed healthcare and TB treatment systems on drug resistance. For example, the only DRS reported from Kenya (conducted in 1995) reported an MDR TB rate of 0.0%. In 2008, the validity of this result is highly questionable given the recently published high MDR TB rates of neighboring countries .\n\n【44】This research was limited by the differing country-level data estimates and the ecologic study design. Twenty one of the 39 countries analyzed have TB estimates from WHO or peer-reviewed journals. However, the other 19 countries required formulaic estimates of MDR TB rates. The effect of these differing estimation methods depends on the congruity of future country-level estimates relative to the figures accepted in this study. Therefore, analyses should be repeated as more definitive estimates become available. Second, the ecologic study design, which analyzes these TB-related factors on a population level, limits the transference of any apparent relationships to the individual level. However, evidence shows that an effective DOTS program can limit the development of drug resistance on the individual TB patient level . Lastly, the small sample size of 39 countries limits the power to statistically determine differences. As a final note, this report hoped to realize 2 goals: to provide a composite of MDR TB in Africa and to identify relationships and trends.\n\n【45】Trends and relationships aside, MDR TB in Africa is a burgeoning obstacle that shows no signs of regression. The prevalence of MDR TB remains below the levels seen in Central Europe and parts of Latin America. However, in Africa, the tragically high HIV prevalence and limited funds and infrastructure dedicated to healthcare are serious factors. As the outbreak of MDR TB/HIV co-infection in New York City in the mid-1990s taught the medical community, this co-infection is pernicious, difficult, and overwhelmingly expensive to treat. This situation begs the question: if the continent is finding difficulty addressing TB, a well-defined disease caused by a well-defined agent, which is fully treatable with effective and affordable drugs through internationally recommended guidelines, then how will the continent fare against MDR TB and XDR TB?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6e2698b1-f706-4a3d-aae9-f357e9a75d89", "title": "Respiratory Tract Reinfections by the New Human Metapneumovirus in an Immunocompromised Child", "text": "【0】Respiratory Tract Reinfections by the New Human Metapneumovirus in an Immunocompromised Child\nA variety of viruses,—such as the influenza viruses A and B, the Human respiratory syncytial virus (HRSV), the parainfluenza viruses, and the adenoviruses—cause seasonal respiratory tract infections in young children. Symptoms range from influenzalike illnesses to lower respiratory tract syndromes, such as bronchiolitis, croup, and pneumonitis . These viruses may also cause severe respiratory infections in immunocompromised patients . However, the etiology of many cases of bronchiolitis and pneumonitis remains unknown despite the extensive use of sensitive diagnostic techniques.\n\n【1】The human metapneumovirus (HMPV) has been recently classified as a new member of the Paramyxoviridae family based on nucleic acid sequence, gene organization, and electron microscopy findings . This virus has been reported to cause respiratory tract infections in children <5 years of age from the Netherlands  , as well as in elderly patients from North America  . We describe a case of recurrent HMPV respiratory tract infections, associated with genetically distinct viral strains, in an immunocompromised child.\n\n【2】### Case Report\n\n【3】A 7-month-old girl, who had acute lymphoblastic leukemia with meningeal involvement (treated with intravenous vincristine and doxorubicin, as well as intrathecal methotrexate), was brought to the hospital on March 23, 1998, for recent onset of nasal congestion and nonproductive cough. Her medical history showed that she had recent contact with family members who had a common cold. On initial physical examination, her temperature was 38.9°C with a respiratory rate of 40 per min, with no rales at lung auscultation. Initial laboratory tests showed a leukocyte count of 1,800 X 109 cells/L with 25% neutrophils and 3% band forms. The child was hospitalized and treated empirically with intravenous ceftazidime. Urine and blood cultures were negative for bacterial pathogens. The patient rapidly improved and was discharged from the hospital on day 3 after admission; no antibiotics were given, and the presumptive diagnosis was viral upper respiratory tract infection. The next day, the patient was brought to the same hospital for an exacerbation of coughing and sneezing. On physical examination, she had a temperature of 38.5°C, with bilateral wheezing at lung auscultation. The respiratory rate was 36 per min, and a clear nasal discharge was noted. The leukocyte count was 1,600 x 109 cells/L, with 33% neutrophils and 0% band forms. Blood was drawn for cultures, and a nasopharyngeal aspirate (NPA) sample was obtained for HRSV antigenic testing (Test Pack, Abbott Laboratories, Abbott Park, IL) and viral culture on Madin Dorby Kidney cells (MDKC), tertiary monkey kidney cells (LLC-MK2), Hep-2, human foreskin fibroblast, Vero, Mink lung, human lung adenocarcinoma (A-549), human rhabdomyosarcoma (RD), transformed human kidney 293, and human colon adenocarcinoma (HT-29) cells. The patient was again treated empirically with intravenous ceftazidime. Three days after admission her condition had improved, and she was discharged from the hospital. The final diagnosis was bronchiolitis, and antibiotics were not given. Bacterial blood cultures and the rapid antigenic test for HRSV were negative. However, after 17 days of incubation, the viral culture from the NPA showed a nonhemagglutinating virus (isolate 1) growing in LLC-MK2 cells. Immunofluorescence assays with antibodies against common respiratory pathogens (influenza viruses A and B, HRSV, adenoviruses, and parainfluenza viruses 1–4) were all negative.\n\n【4】The next year, on January 18, 1999, during a scheduled medical appointment for administration of intravenous (daunorubicin and vincristine) and intrathecal (methotrexate, cytarabine, and hydroxyvrea) chemotherapy in addition to oral dexamethasone, the now 17-month-old girl again had nasal congestion and nonproductive cough. Her father mentioned that he had an upper respiratory tract infection 2 weeks earlier. An NPA sample was obtained for viral culture and HRSV antigenic detection. The patient did not appear ill and was allowed to go home the same day after chemotherapy. The HRSV antigenic test came back negative, but the viral culture showed an unidentified cytopathic effect only apparent in LLC-MK2 cells after 14 days of incubation (isolate 2). Immunofluorescence assays were again negative for all common respiratory viruses. The patient received another course of intravenous chemotherapy (daunorubicin and vincristine) on January 25 at the outpatient clinic, and her upper respiratory tract symptoms were then treated with an oral antibiotic (axetil cefuroxime).\n\n【5】Five days later, on January 30, 1999, the child was admitted to the hospital with a fever (.9°C), persistent dry cough, and clear nasal discharge while on oral antibiotic. On physical examination, her respiratory rate was 28 per min with fine bilateral crackles at lung auscultation. An otoscopic exam showed bilateral otitis. The leukocyte count showed neutropenia (.400 x 109 cells/L), and ceftazidime plus vancomycin were initiated. The blood cultures taken on the first day of hospitalization indicated a coagulase-negative staphylococcus (of 2 bottles were positive). On the fourth day of hospitalization, the patient’s clinical condition deteriorated with desaturation (pO2, 0.88) and tachypnea (respiratory rate, 60–70 per min). A chest x-ray showed bilateral infiltrates compatible with pneumonitis. The next day, an NPA sample was obtained for a viral culture and direct immunofluorescent assays against typical respiratory viruses. The results were negative. Intravenous erythromycin was administered for empirical coverage of atypical pathogens. On day 10 of hospitalization, the patient was transferred to the pediatric intensive-care unit and was intubated. A bronchoscopy was performed, and a bronchoalveolar lavage (BAL) sample was obtained for cultures of fungal, mycobacterial, viral, and bacterial pathogens, as well as for specific staining procedures for Pneumocystis carinii, fungi, bacteria, and mycobacteria. At that point, ceftazidime was replaced by trimethoprim/sulfamethoxazole, and an amphotericin B lipidic complex formulation (Abelcet, Liposome, Canada) was added. All cultures and specific staining procedures performed with the BAL sample were negative. Direct immunofluorescence assays on the BAL sample also were negative for influenza A and B, parainfluenza 1–4, and HRSV. On day 13 of hospitalization, the patient’s respiratory signs deteriorated markedly, with bilateral pulmonary infiltrates compatible with acute respiratory distress syndrome. Four days later, because of free air in the pericardium and peritoneum (the patient was on high ventilatory pressure), drainage tubes were placed, resulting in a temporary improvement of pulmonary gas exchange. New blood cultures drawn on day 27 were negative. The next day, shock developed, and vasopressive drugs were started. Because of her irreversible condition, all treatment was stopped on day 30, and the patient died shortly thereafter. No autopsy was performed.\n\n【6】Two years after the child’s death, reverse transcription-polymerase chain reaction assays for HMPV with infected LLC-MK2 cell culture supernatants of the two unidentified isolates were performed and found to be positive. On the basis of partial published sequences, a set of primers was designed for amplification of the F gene of HMPV . The forward primer sequence was 5´-ATGTCTTGGAAAGTGGTG-3´, and the reverse primer sequence was 5´-TCTTCTTACCATTGCAC-3´. Amplified products were sequenced by an automated DNA sequencer (ABI 377A \\[Perkin-Elmer Applied Biosystems, Foster City, CA\\]) and the 759-bp nucleotide sequences of the two viral isolates from consecutive seasons were aligned with the Clustal W software  . The two viral isolates differed by 114 nt in the F gene sequence, which resulted in a change of 10 amino acids .\n\n【7】### Conclusions\n\n【8】We describe the case of a young immunocompromised child who had respiratory tract infections during two consecutive seasons, probably due to HMPV, a newly discovered paramyxovirus. On the basis of sequence analysis of the viral F gene, we found that the child was infected by two genetically distinct HMPV strains.\n\n【9】In the first clinical episode (occurring at the age of 7 months), the initial symptoms resembled a common cold and progressed to bronchiolitis, followed by a complete resolution of symptoms within a few days. HMPV was the only organism isolated in culture from an NPA sample, and the HRSV rapid antigenic test was negative. Ten months later, the child had a second respiratory tract infection, again with common cold symptoms. HMPV was again the sole pathogen isolated in an NPA sample. In the next weeks, however, the patient’s clinical condition deteriorated, with development of bilateral pneumonitis and respiratory failure and ultimately death. The BAL sample obtained showed no bacterial, viral, or fungal pathogens by culture, antigenic testing, or special staining procedures. The exact cause of respiratory failure in this immunocompromised child remains uncertain, because autopsy was not performed. An opportunistic infection or a neoplastic infiltrate was the most probable cause. HMPV could have led to the acute respiratory distress syndrome in this immunocompromised patient. We suggest that the initial upper respiratory tract infection may have progressed to pneumonitis after the patient received intensive chemotherapy. The fact that HMPV was not recovered in the last BAL sample does not rule out such viral infection since paramyxoviruses are very labile  , and the culture may not have been incubated long enough for a cytopathic effect to be observed.\n\n【10】The two HMPV isolates from this study had many differences at the nucleotide level (similarity 85.0%) but much less at the amino acid level (similarity 96.0%) in the gene coding for the fusion (F) protein. Isolate 2, recovered in 1999, had greater amino acid similarity with the prototype sequence from the Netherlands  compared with isolate 1 (.0% vs. 96.0% similarity). Furthermore, the two strains seem to belong to the two different HMPV lineages previously reported by Dutch and North American groups . We suggest that many HMPV strains may cocirculate in a specific population, and such viral diversity, coupled with waning immunity, as found in elderly and immunocompromised patients, may lead to multiple reinfections similar to HRSV .\n\n【11】Future studies are needed to evaluate the full clinical spectrum associated with HMPV infection, the groups at risk for severe complications, and the potential therapeutic options. Our data suggest that HMPV should be added to the list of pathogens associated with severe respiratory tract infections in immunocompromised patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "83cf0013-722f-4d39-83ae-309c3d67a1bc", "title": "Association of Healthcare and Aesthetic Procedures with Infections Caused by Nontuberculous Mycobacteria, France, 2012‒2020", "text": "【0】Association of Healthcare and Aesthetic Procedures with Infections Caused by Nontuberculous Mycobacteria, France, 2012‒2020\nNontuberculous mycobacteria (NTM) are ubiquitous bacteria found in soil, water, and other environments . More than 200 NTM species have been described to date. NTM are classified according to their speed of growth in vitro, specifically rapidly growing mycobacteria (such as _Mycobacterium chelonae_ , _M. fortuitum_ complex, and _M. abscessus_ ) and slowly growing mycobacteria (such as _M. avium_ complex, _M. marinum_ , and _M. kansasii_ ).\n\n【1】NTM infections are usually not transmissible between humans, although outbreaks linked to the same contamination event or from a common water reservoir have been reported. This finding was especially observed for extrapulmonary NTM infections after invasive procedures because of a common source, such as healthcare-associated infections (HAIs) and those related to medical, aesthetic, or cosmetic procedures . In particular, NTM HAIs were observed after heart surgery: >100 cases of endocarditis caused by a single clone of _M. chimaera_ were found in water tanks of heater-cooler units used for cardiac bypass . In France, previously reported outbreaks of NTM HAI cases have involved _M. xenopi_ in bone and joint infections after orthopedic surgery  and _M. chelonae_ in skin infections after mesotherapy cosmetic procedures  or in hematopoietic stem cell transplantation . Endocarditis on bioprosthetic heart valves were also reported to contain _M. wolinskyi_ and _M. chelonae_ .\n\n【2】HAI reporting has been mandatory in France since 2001, and reports are collected at the French Public Health Agency. In addition, the National Reference Centre for Mycobacteria and Resistance of Mycobacteria to Anti-Tuberculosis Agents (CNR-MyRMA) regularly receives NTM isolates, including NTM HAI isolates, from human infections for diagnosis and treatment purposes. A previous case series described an initial cross-database evaluation focusing on NTM infections associated with cosmetic procedures during 2001–2010 . We describe episodes of extrapulmonary NTM infections associated with surgical, medical, or aesthetic procedures, including cosmetic care, reported in France during 2012‒2020, in and outside healthcare facilities (HCFs).\n\n【3】### Materials and Methods\n\n【4】##### Data Sources\n\n【5】We used 2 data sources to include reported cases of NTM infections associated with surgical, medical, or aesthetic procedures during January 2012‒June 2020. The first data source was the national early warning response system (EWRS) for HAI diagnosed in HCF, using an electronic reporting process implemented in 2012 (e-SIN), and slightly modified in 2017 . The second data source was the NTM isolate database of the CNR-MyRMA, which includes microbiological results of clinical isolates, as well as environmental isolates found after epidemiologic investigations. The Regional Support Centre for the Prevention of Healthcare-Associated Infections conducted epidemiologic investigations. We contacted health professionals who reported cases to the national EWRS for HAI and send isolates to CNR-MyRMA to associate isolates with cases. We used this procedure to set up a single database containing epidemiologic and microbiological data.\n\n【6】##### Case Definition\n\n【7】We included extrapulmonary NTM infections defined as a person who had clinical symptoms compatible with an NTM infection and \\> 1 NTM-positive microbiological sample (cases considered as NTM colonization by physicians were excluded); and specific surgical, medical, or aesthetic procedures, including cosmetic care, potentially at the origin of the NTM infection. Pulmonary NTM infections, even hospital-acquired, were excluded. We usually consolidated epidemiologically related cases into a single report.\n\n【8】##### Data Collection and Analysis\n\n【9】We collected the following information from the 2 data sources: 1) the report itself (the HCF or the laboratory which made the report, date of report); 2) data for infection (date of onset of initial symptoms, symptoms, infection type); 3) the context and suspected cause of the infection (procedure at the origin of the infection, date of contamination or invasive procedure, equipment implicated); 4) epidemic context (number of cases, distribution over time); 5) characteristics of case-patients (age, sex, and immune status); 6) investigation characteristics performed after the NTM infection diagnosis (environmental and professional practices investigations, corrective measures implemented); and 7) microbiological results (name of species and subspecies, whole-genome sequencing \\[WGS\\] comparison). We performed a descriptive data analysis by using STATA version 14.2 . We analyzed the rate of NTM infection cases over the study period by using a Poisson regression model.\n\n【10】##### Genomic Comparison\n\n【11】We performed genotypic analysis by using WGS to compare isolates found in environmental and clinical samples. We extracted DNA by using the DNA Ultraclean Microbial Kit . We prepared DNA libraries by using the Nextera XT Kit  and sequenced them by using the MiSeq System (Illumina) and MiSeq Reagent V2 (× 150) Kits (Illumina). We performed WGS comparison by aligning sequencing reads of the isolates to a reference genome. We analyzed sequencing data by using Bionumerics version 7.6 . We trimmed reads to exclude base calls with a Phred score <15 and then aligned them by using the Trimming and Resequencing analysis options. The single-nucleotide polymorphism (SNP) signature was built by using the Strict filtering (closed SNP set) option, retaining all SNP with a minimum coverage of 5×, at least covered once in both forward and reverse direction and a minimum distance between retained SNP position of 12 bases, removing the nondiscriminatory position. We used the SNP matrix to build a maximum parsimony tree. We defined a cluster in the WGS analysis by isolates sharing < 10 or fewer SNPs. WGS data are available from the National Center for Biotechnology Information .\n\n【12】### Results\n\n【13】For the study period, 71 reports of extrapulmonary NTM infections related to HAI surgical, medical, or aesthetic procedures were included to give a total of 85 original cases, a mean of 10 cases/year for complete years (i.e. 2012–2019) . The regression identified an increasing trend of cases per year of onset of clinical signs during the study period after excluding incomplete years (p<0.01). This increase was observed particularly during the period 2016–2019 (i.e. after the _M. chimaera_ heater–cooler unit \\[HCU\\] outbreak) but was caused by addition of these cases because regression analysis without them showed a similar significant trend (p<0.01). Among the 85 cases, 36 (%) were found in the CNR-MyRMA strain database and the e-SIN database. The CNR-MyRMA database contained 30 additional cases, and the e-SIN database contained 19 additional cases.\n\n【14】The reports were received from 46 HCFs throughout France. Twenty-nine of the HCFs sent only 1 report during the study period, and 16 HCFs sent >1 (HCFs made 2 reports, 3 HCFs made 3 reports, 2 HCFs made 4 reports, and 1 HCF made 5 reports). Most (%, 64/71) of the reports concerned an individual case, and 7 reports concerned clusters (cases in 4 reports, 3 cases in 1 report, and 5 cases in 2 reports).\n\n【15】For most (%, 59/85) of cases, the infection was acquired inside the HCF, whereas 26 cases (%) were acquired outside the reporting HCF. These infections acquired outside the HCF were imported either from another HCF (n = 14) or from a non–hospital-based medical or aesthetic practice (n = 12).\n\n【16】More women than men had cases reported (M:F = 31:46); sex was not reported for 8 case-patients. The median case age was 54 years (range 4–86 years; age was not reported for 12 cases). One third of the cases concerned immunosuppressed patients (%, 23/70 cases; 15 cases did not report this information). NTM infections resulted from surgical procedures (%, 48/85 cases), other invasive procedures (%, 28/85 cases), and noninvasive procedures (%, 9/85 cases). Cardiovascular surgery (n = 14), orthopedic surgery (n = 11), plastic surgery (e.g. breast surgery, (n = 13), face-lift (n = 2), abdominoplasty (n = 1), or capillary implant (n = 1), and catheter-associated infections (n = 17) comprised most of the surgical and other invasive procedure cases reported. The NTM infections concerned mainly skin and soft tissues (%, 31/85), intravascular catheters (%, 17/85), bones and joints (%, 15/85), and arterial/cardiac (%, 13/85) .\n\n【17】Overall, 14 NTM species were isolated: 10 rapidly growing NTM species and 4 slowly growing NTM species. Rapidly growing NTM were _M. chelonae_ (n = 30), _M. fortuitum complex_ (n = 24) (cases with _M. fortuitum_ , 2 with _M. mageritense_ , 2 with _M. porcinum_ , 1 with _M. senegalense_ , and 2 strains for which the exact species could not be determined), _M. abscessus_ (n = 14), _M. mucogenicum_ (n = 5), _M. neoaurum_ (n = 2), _M. fuerthensis_ (n = 1), and _M. wolinskyi_ (n = 1). Slowly growing NTM species were _M. chimaera_ (n = 4), _M. avium_ (n = 2), _M. lentiflavum_ (n = 1), and _M. marinum_ (n = 1). For 2 cases of endocarditis, direct examination of the valve samples identified acid-fast bacilli after Ziehl-Neelsen staining, but culture results were negative.\n\n【18】We determined the hypothesized incubation time (i.e. time between the onset of clinical signs and most probable contamination date, which is most often the date of the procedure) for 50 cases . Incubation time was shorter for rapidly growing NTM (median time 34 days, n = 40) than for slowly growing NTM (median time 549 days, n = 5).\n\n【19】We suspected that medical devices were related to the infection for 80% of cases (/85), but a medical device vigilance report was performed for only 21% of those (/68 cases). Medical devices comprised implantable devices (e.g. breast prosthesis, artificial heart valve and vascular prosthesis, knee or hip prosthesis) for 50% of case-patients that had a medical device (/68 cases), invasive devices (e.g. catheter and implantable port, dialysis device, endoscopy device, infiltration device in orthopedic surgery, liposuction cannula, mesotherapy, and tattoo injection equipment) (%, 25/68 cases), and noninvasive devices (e.g. cardiopulmonary bypass HCU, contact lens) (%, 9/68 cases).\n\n【20】For nearly half of all reported cases (%, 40/85 cases; 30 cases did not report this information), there was a specific investigation of professional practices after the NTM infection was diagnosed to assess the level of compliance with hygiene guidelines, sterilization procedures, and treatment procedures. For more than one fourth of these cases (%, 11/40), the investigations found failure to comply with infection risk prevention recommendations. Corrective measures were implemented for 45% of the reports (/38 reports; 33 did not report this information), most often involving increased hygiene vigilance and recommendations to improve practices (n = 7). For 32 reports, there was no active case finding; most of them were isolated cases. For 30 reports, this information was not available. Active case finding was conducted after 9 reports (did not report this information) for procedures such as breast reconstruction, heart surgery, gastrointestinal endoscopy, mesotherapy, orthopedic surgery, and tattooing sessions.\n\n【21】Environmental investigations were undertaken for 42% (/71) of the reports. Most (/30) reports involved water sampling from potential sources of the contamination, such as water supply networks (n = 21), HCU (n = 4), dialysis water (n = 1), and swimming pool water (n = 1). Perioperative surfaces (n = 2) and air samples (n = 1) were rarely sampled. Among the 45% of cases (/85) involving environmental investigations, mycobacteria samples were positive for 18 (were not reported). The same NTM species as in the clinical isolate was found for 12 cases (%, 12/38) after environmental investigations. For 10 cases, the clinical isolate could not be distinguished from the environmental isolate : _M. chimaera_ isolates from HCU and heart surgery infection (patient A3) , isolates from hospital water supply network, _M. fortuitum_ breast infection (patient C1) , _M. chelonae_ skin and soft tissue infection (patients D1, E1-E2, E3, and F1) , _M. marinum_ isolates from pool balneotherapy and skin and soft tissue infection (patient I1) , and _M. mucogenicum_ catheter-associated infection (patients J1, J2, and J3) .\n\n【22】We also performed genomic comparisons for case-patients suspected of being contaminated by a common source. For 8/10 case-patients, studied isolates had the same pattern . In report A, 3 clinical isolates of _M. chimaera_ endocarditis from 2 patients were clustered (A1-A2 and A3) . These 2 patients were linked to a worldwide outbreak of HCU contamination, as shown in the section comparing clinical (A3) and environmental (A4-A8) isolates. In report H, which concerned _M. chelonae_ catheter-associated infections diagnosed in the same institution (n = 5 cases), clinical isolates were clustered into 2 distinct groups; the first cluster grouped 3 isolates, H1, H4, and H5, from 3 patients, and the second cluster grouped 2 isolates, H2 and H3, from 2 patients . The presence of 2 clusters in the same HCF suggested that there were 2 sources of contamination, neither of which were found. In report K, the same genotype was found when comparing _M. neoaurum_ isolates from blood cultures of 1 patient with the isolate found during microbiological control testing after a peripheral autologous stem cell transplant (n = 1 case) . The contamination of the stem cell transplant was attributed to colonization of the catheter used for the cell sampling.\n\n【23】### Discussion\n\n【24】We describe extrapulmonary NTM infections diagnosed after surgical, medical, or aesthetic procedures in France over an 8-year period . To broaden the spontaneous reporting from medical professionals, we sought 2 information sources: the national EWRS for HAI and the national reference NTM strain database. Because only 85 cases were described in 71 reports over 8 years, we might consider that such NTM infections remain rare. However, most of the cases were related to a medical device, a specific procedure, or lack of hygiene practices and might have been preventable.\n\n【25】Our study highlights a slight increase in reported annual numbers of cases during the study period. This increase, particularly during 2016 , could be explained by greater global awareness in public health community after invasive infections with _M. chimaera_ associated with HCUs used during cardiac surgery  and other published outbreaks .\n\n【26】Both infection sites and NTM species isolated from these cases of infections were diverse, emphasizing the opportunistic nature of these pathogens. The most commonly reported infections were skin and soft tissue infections, catheter-related infections, infective endocarditis, and bone and joint infections. When we compared our findings with the major proportion of extrapulmonary NTM infections described in the literature, but not limited to healthcare-associated and aesthetic procedure‒associated infections, we found that skin and soft tissue infections were the most commonly reported infection site . However, we identified catheter-associated infections, bone and joint infections, or infective endocarditis, which are less described, except for the HCU _M._ _chimaera_ outbreak .\n\n【27】A wide variety of NTM species were responsible for the infections reported in our study; _M. chelonae_ and _M. fortuitum_ were the most common. Isolates of slowly growing NTM species were rare (%) compared with those found in a review in the United States, in which 50% of all species were in the _M. avium_ complex . One possible explanation for this difference is that rapidly growing NTM cultivated on standard bacteriology diagnostic media might constitute an unexpected etiology diagnosis. We should also consider that these rapidly growing species were regularly found in France in the water networks, one of the sources of infection .\n\n【28】Most of the cases in our study were linked to surgical, invasive, and noninvasive procedures. In 5 cases, no specific procedures were identified as the cause of infection, even though all patients underwent healthcare procedures such as previous intravenous catheter, or no procedures were identified at all, such as the case of bone and joint infection, which could not have originated from spontaneous infection. As observed in previous reviews, the most commonly reported infections were those associated with aesthetic care , particularly breast prostheses .\n\n【29】The source of infection was not determined in 9/10 case-patients, despite environmental investigations conducted for ≈50% of the reports. Even when environmental investigations were performed, they occurred months after the suspected contamination because of long incubation times, as in reports B and G .\n\n【30】Because a medical device was implicated in most of the NTM infection cases we reported, we believe that these infections can be prevented. The medical device could be contaminated by NTM before its use or can lead to contamination from environmental NTM . When an environmental cause is identified, the water system is the major environmental source most frequently considered responsible. Water systems, particularly in hospitals, are frequently identified as NTM reservoirs .\n\n【31】Genomic comparison of NTM isolates can be used to rule out or confirm any hypothesis concerning the origin of the contamination. However, careful analysis of genomic sequence comparisons should be conducted because several factors, such as the reference sequence on which the reads are mapped (epidemic strain or unrelated strain), quality of the sequenced data, coverage of the mapping assembly, number of sequences included in the comparison, and use of de novo assembly, influence SNP analysis. WGS appears to be a suitable tool for the molecular investigation of NTM infections, but might need expert rules and standardization to be used further.\n\n【32】The major limitations of this study concern the lack of completeness of the reported data. There is no specific surveillance system for NTM infection in France, and the 2 databases used for this case series are not exhaustive. The purpose of the EWRS for HAI platform is to improve the management of HAI reporting by HCF, and the CNR-MyRMA receives NTM isolates for patient diagnosis and treatment and genotypic comparison in epidemiologically related cases with environmental analyses when necessary. Therefore, underdeclaration of NTM infection cases in France is probable . However, when combined, the 2 databases provide a useful inventory of extrapulmonary NTM infection cases related to surgical, medical, and aesthetic procedures. The genomic comparison of NTM isolates performed by CNR-MyRMA was able to demonstrate the source when an environmental investigation was conducted and clinical and environmental isolates were available. This comparison provides valuable pointers for the future implementation, improvement, and follow-up of certain preventive measures.\n\n【33】Although data in the current study were not exhaustive, reports of NTM infection cases and subsequent microbiological and workplace practice investigations showed that considerable progress has been made in understanding contamination mechanisms during healthcare treatment. Water used in the procedures appeared to be the infection source for 10 cases. This finding is particularly true for heart surgery after the alert issued concerning the global outbreak of _M. chimaera_ endocarditis as a result of contaminated HCUs .\n\n【34】Our observations should prompt more stringent recommendations for prompt reporting of NTM infections and provision of clinical and environmental samples for analysis of strains. Better application of these recommendations should improve methods to identify causes of NTM infections and enable their prevention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0c766214-4c90-4d23-aaa3-75647c9c626e", "title": "Respiratory Infection with Enterovirus Genotype C117, China and Mongolia", "text": "【0】Respiratory Infection with Enterovirus Genotype C117, China and Mongolia\n**To the Editor:** Enteroviruses (EVs) are small, nonenveloped viruses of the family _Picornaviridae_ . EVs are classified into 12 species according to the molecular and antigenic properties of their viral capsid protein (VP1). To date, 7 species are known to infect humans, including EV-A to EV-D and rhinovirus A, B, and C \n\n【1】EV-C117 was a newly found EV-C genotype. It was identified in a nasopharyngeal sample from a hospitalized child, 3 years and 9 months of age, with community-acquired pneumonia in Lithuania in 2012 . However, aside from this case, little is known about the prevalence and clinical significance of EV-C117. Here, we report the detection of EV-C117 in children in China and Mongolia with respiratory tract infections (RTIs).\n\n【2】During March 2007–March 2013, we screened for EV-C117 in respiratory samples from patients with RTIs in China and Mongolia, including nasopharyngeal aspirates collected from 3,108 children in China who had lower respiratory tract infections when they were admitted to Beijing Children’s Hospital  and swab samples from 2,516 patients in Mongolia with influenza-like illness . Respiratory viruses in samples from China were screened by using multiplex PCR and single PCR assays as described . Samples from Mongolia were screened by using the FTD Respiratory Pathogens Multiplex Assay Kit (Fast-track Diagnostics, Luxembourg City, Luxembourg). EV-positive samples were further genotyped by using reverse transcription PCR (RT-PCR) and primers sequentially targeting the VP1 region , the 5′-untranslated region (′-UTR)/VP4/VP2 region  and the 5′-UTR . A 394-nt amplicon corresponding to the 5′-UTR of EVs was obtained from 10 children in China; a 598-nt amplicon corresponding to the 5′-UTR/VP4/VP2 region was obtained by RT-PCR from 5 children in Mongolia. Blastn analysis  of PCR amplicons showed that only amplicons detected in 2 children from China (patients BCH096A and BCH104A) and 2 children from Mongolia (patients MGL126 and MGL208) had the highest similarity (%–98%) to the EV-C117 prototype strain LIT22.\n\n【3】To further confirm that these 4 strains belong to EV-C117, we attempted to amplify the full-length viral genome sequences. However, we only obtained full-length viral genome sequences from the 2 strains found in patients from China (GenBank accession nos. JX560527 \\[patient BCH096A\\], and JX560528 \\[patient BCH104A\\], respectively). For the remaining 2 strains from Mongolia, MGL126 (′UTR/VP4/VP2: KF726102; VP1: KF726100) and MGL208 (′UTR/VP4/VP2: KF726103; VP1: KF726101), we obtained the sequence of the 5′-UTR/VP4/VP2 region and VP1 gene. Phylogenetic analysis of these sequences showed that they all belonged to genotype EV-C117 .\n\n【4】Virus isolation for EV-C117 by using Vero and H1-HeLa cells was unsuccessful. Through blastn and phylogenetic analyses, we also found that the previously identified EV-C strain HC90835 (EU697831, from Nepal) , EV-C104 strain CL-C22 (EU840734, EU840744, and EU840749, from Switzerland)  and a rhinovirus strain SE-10–028 (JQ417886, from South Korea), also belong to EV-C117 , indicating that EV-C117 is widely distributed geographically. Because a large proportion of EV infections are subclinical or mild , the prevalence of EV-C117 should be further estimated by using serologic investigations in general populations.\n\n【5】The VP1 sequences of the EV-C117 strains isolated in China and Mongolia were 89.9%–95.6% (nt) and 95.2%–98.3% (aa) identical to the EV-C117 prototype strain LIT22 (patient JX262382). Alignment analysis of amino acid sequences showed differences between strains isolated in this study and LIT22, i.e. Ser 15  (strains in this study) versus Asn 15  (LIT22). In addition, we found that the strains from patients in China contain Lys 63  and Ala 90  , and those from Mongolia have Thr 93  , Asn 97  , and Ser 276  . The biological significance of these mutations is unknown.\n\n【6】Of these 4 EV-C117–positive children, 3 were hospitalized with respiratory disease ; the nonhospitalized child (MGL208) had a sore throat, but no other signs or symptoms. The viral loads of EV-C117 and co-detected viruses were quantified by using real-time PCR (methods available upon request), with a median EV-C117 load of 2.9 × 10 5  RNA copies/mL (range 1.1–4.8 × 10 5  RNA copies/mL \\[ Technical Appendix Table 2\\]). EV-C117 was the only virus detected in patients BCH104A and MGL126. Respiratory syncytial virus (.0 × 10 6  copies/mL) and rhinovirus (.5 × 10 5  copies/mL) were detected in patient BCH096A, and influenza virus A (IFVA, H3N2; 5.1 × 10 10  copies/mL) and human bocavirus (.7 × 10 2  copies/mL) were detected in patient MGL208.\n\n【7】The co-detection of viruses in 2 of the EV-C117–positive patients raises the question of what role EV-C117 plays in RTIs. However, it is notable that EV-C117 was the only virus detected in the other 2 patients. This finding indicates that, at least in patients with low resistance (patient BCH104A had severe bacterial infection before EV-C117 was detected and patient MGL126 had congenital heart disease), EV-C117 might be associated with RTIs. In addition, the strain isolated in Nepal and the strain isolated in Switzerland, EV-C117, were both detected in specimens collected from patients with RTIs . Collectively, these data indicate the respiratory tropism of EV-C117. Additional epidemiologic and virologic studies on EV-C117 may be warranted to establish its role in RTIs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "882026d0-d8dd-4231-9fc7-d8453166650b", "title": "Norovirus Outbreak Surveillance, China, 2016–2018", "text": "【0】Norovirus Outbreak Surveillance, China, 2016–2018\nHuman noroviruses are the leading cause of outbreaks of acute gastroenteritis, associated with ≈50% of all outbreaks worldwide . Norovirus outbreaks are frequently reported in semiclosed institutions, such as hospitals, nursing homes, schools, and childcare centers . The virus is primarily transmitted directly from person to person or indirectly through contaminated surfaces, food, or water . The relative stability of noroviruses on environmental surfaces makes infection control challenging . Several candidate norovirus vaccines are in clinical trials .\n\n【1】Noroviruses are single-stranded RNA viruses that belong to the genus _Norovirus_ , family _Caliciviridae_ . The genome is organized into 3 open reading frames (ORFs): ORF1 encodes polyprotein, ORF2 encodes the major capsid protein (VP1), and ORF3 encodes the minor (VP2) capsid protein. The viruses are classified into at least 7 genogroups (G), of which viruses from GI, GII, and GIV infect humans . On the basis of the diversity of VP1, these genogroups can be further divided into at least 33 genotypes: 9 GI, 22 GII, and 2 GIV . In addition, on the basis of the diversity of the polymerase region of ORF1, >14 GI polymerase (GI.P) types and 27 GII.P types have been described . Because of the frequent recombination at the ORF1/ORF2 junction region, a dual-typing system has been proposed for GI and GII noroviruses . Since 2002, genogroup II, genotype 4 (GII.4), noroviruses have been associated with most norovirus outbreaks globally, and new GII.4 variants have emerged every 2–3 years . Monitoring the trends in the distribution of the various genotypes and possible association of certain strains with a more severe disease outcome is important for understanding and controlling norovirus epidemics .\n\n【2】Several norovirus outbreak surveillance networks, including NoroNet  and CaliciNet , have been developed during the past decade. NoroNet captures molecular and epidemiologic data on norovirus outbreaks and sporadic cases submitted by 19 participating countries across Europe and Asia and by Australia. CaliciNet is a norovirus outbreak surveillance network in the United States in which state and local public health laboratories electronically submit laboratory data, including sequences from norovirus outbreaks, to a central database . CaliciNet data are integrated with epidemiologic data from the National Outbreak Reporting System, yielding comprehensive surveillance in the United States that enables multifactorial characterization of norovirus outbreaks .\n\n【3】Since 2004, provincial and local Centers for Disease Control and Prevention (CDCs) in China have been required to report all acute gastroenteritis outbreaks of \\> 20 cases, including those caused by noroviruses, to a passive national outbreak surveillance system called Public Health Emergency Event Surveillance System (PHEESS) . Although PHEESS provides useful information, detailed epidemiologic and molecular data from norovirus outbreaks are typically not included. In October 2016, the National Institute for Viral Disease Control and Prevention at the China CDC launched CaliciNet China as a surveillance network to monitor the epidemiology and molecular characteristics of norovirus outbreaks. We describe this new network and report data from the first 2 years of surveillance.\n\n【4】### Materials and Methods\n\n【5】##### CaliciNet China\n\n【6】For the surveillance of norovirus outbreaks, CaliciNet China relies on contributions from laboratory and epidemiologic staff at county, city, and provincial CDCs. County CDCs are responsible for acute gastroenteritis outbreak investigations and collection of clinical specimens. Laboratories of city and provincial CDCs perform norovirus detection and genotyping on the specimens. The national laboratory at China CDC receives and aggregates findings and provides overall quality control. Laboratory staff at these CDCs received training on using the standardized methods of norovirus detection and typing . Each laboratory performs proficiency testing once a year for quality assurance.\n\n【7】##### Outbreak Reporting\n\n【8】CaliciNet China defines norovirus outbreaks as \\> 5 acute gastroenteritis cases within 3 days after exposure in a common setting where \\> 2 samples (whole fecal, rectal swab, or vomitus) had been laboratory confirmed as norovirus. Acute gastroenteritis was defined as \\> 3 events involving loose feces, vomiting, or both within a 24-hour period. The data were collected in collaboration with the local epidemiologists investigating the outbreaks using national guidelines . Local CDCs aggregated epidemiologic data for each outbreak, including date of first illness onset, setting, transmission route, number of cases, and type of specimens collected. All data were entered into Microsoft Excel . City, county, and provincial CDCs also submitted laboratory results from real-time reverse transcription PCR (rRT-PCR), conventional RT-PCR, and sequences into a BioNumerics version 6.6 database (Applied Maths **,** _https://www._ **applied-maths** **.** _com_ ) using CaliciNet scripts provided by CaliciNet USA . Each month, data were sent electronically to China CDC.\n\n【9】##### rRT-PCR and Genotyping\n\n【10】For fecal samples, laboratories prepared a 10% fecal suspension by mixing 0.1 g feces with 1.0 mL phosphate-buffered saline (pH 7.2). For swab or vomitus samples, nucleic acid was extracted directly. Laboratories tested viral RNA for GI and GII norovirus using the Ag-Path kit   in a duplex rRT-PCR with primers (Cog1F, Cog1R, Cog2F, and Cog2R)  and TaqMan probe (Ring 1E and Ring 2) . The cycling conditions were described previously . Some laboratories also used commercial norovirus rRT-PCR kits . Norovirus-positive samples were then amplified by conventional RT-PCR, which amplifies a partial region of ORF1 and a partial region of ORF2 . The RT-PCR conditions and the primers used (MON432, G1SKR, MON431, and G2SKR) have been described previously .\n\n【11】##### Data Management and Analysis\n\n【12】We merged epidemiologic and genotype outbreak data at China CDC and then imported them into SPSS Statistics 25 software  for data cleaning and analysis. We generated descriptive statistics on reported outbreaks by setting, transmission route, outbreak size, and province and examined differences between genotypes and mode of transmission, outbreak size, setting, and season using χ 2  tests. We used an α value of p<0.05 to assess statistical significance. Genotypes were assigned by phylogenetic analysis using the UPGMA method with reference sequences used by CaliciNet for capsid typing.\n\n【13】##### Ethics Review\n\n【14】The China CDC Ethical Review Committee approved CaliciNet China as routine surveillance for norovirus. Because CaliciNet China collects only aggregate data on norovirus outbreaks, we did not analyze any personal identifying information as part of this project. Linkage of specimens to patient name was maintained by provincial-level CDCs and not submitted into the CaliciNet China database.\n\n【15】### Results\n\n【16】##### Epidemiologic Characteristics of Norovirus Outbreaks\n\n【17】In 2016, CaliciNet China started with 9 laboratories in 3 provinces (Beijing, Guangdong, and Jiangsu) and by 2018 increased to 17 laboratories in 6 provinces (adding Shanghai, Hunan, and Liaoning) distributed across eastern, central, and western China . During October 2016–September 2018, a total of 556 norovirus outbreaks were reported to CaliciNet China. Of these, 320 (.6%) were reported from Guangdong Province, 101 (.2%) from Jiangsu Province, 79 (.2%) from Beijing, 38 (.8%) from Hunan Province, 12 (.2%) from Liaoning Province, and 6 (.1%) from Shanghai. Most outbreaks occurred during the winter season (November–March) . Information about the outbreak size was reported for 427 (.7%) outbreaks, among which the median size was 15 persons (interquartile range 12–81.5) per outbreak. Five outbreaks each had >200 cases. Three of those occurred in universities and 2 in vocational schools; 4 were caused by person-to person transmission and 1 by foodborne transmission . Of the 556 outbreaks, 280 (.4%) occurred in childcare centers; 155 (.9%) occurred in primary schools, 61 (.0%) in middle schools, and 24 (.3%) in universities . Of 452 (.3%) outbreaks in which transmission mode was identified, person-to-person transmission predominated ([95.1%\\]), followed by foodborne transmission ([2.9%\\]) and waterborne transmission ([1.3%\\]) .\n\n【18】##### Genotypes\n\n【19】We obtained genotype information for 470 (.5%) of the 556 outbreaks. Of the 86 (.5%) outbreaks with no genotyping results, 72 were not further genotyped by network laboratories and 14 were positive by rRT-PCR but negative by conventional RT-PCR and thus could not be genotyped. Of the typed outbreaks, 430 (.5%) were GII, 26 (.5%) were GI, and 14 (.5%) comprised both GI- and GII-positive samples. Overall, 5 GI genotypes and 12 GII genotypes were detected . Of 470 genotyped outbreaks, GI genotypes included GI.2\\[P2\\] ([2.3%\\]), GI.6\\[P11\\] ([1.3%\\]), and GI.3\\[P13\\] ([1.1%\\]). Among GII outbreaks, 349 (.3%) were typed as GII.2\\[P16\\], which was detected throughout the study period and peaked during winter 2016–17 . Other GII genotypes were GII.3\\[P12\\] ([5.3%\\]), GII.17\\[P17\\] ([3.8%\\]), GII.6\\[P7\\] ([3.4%\\]), and GII.4 Sydney\\[P31\\] ([2.3%\\]) . Genotypes detected in <1% of the outbreaks were GI.1\\[P1\\], GI.3\\[P13\\], GI.3\\[P13\\], GI.5\\[P12\\], and GI.6\\[P11\\] among GI viruses and GII.1\\[P33\\], GII.2\\[P2\\], GII.8\\[P8\\], GII.13\\[P21\\], GII.14\\[P7\\], GIX.1\\[P15\\], and GII.17\\[P31\\] among GII viruses. In the GII.2\\[P16\\] epidemic during winter 2016–17, GI viruses were rarely detected ([0.3%\\] of 298), whereas during the following seasons , GI viruses were detected in 24 (.9%) of 172 outbreaks and peaked in March 2018 ([40.0%\\] of 25). Outbreaks caused by multiple genotypes were mainly detected in March 2018 ([64.3%\\] of 14); GII.17\\[P17\\] viruses were mainly detected during January–April 2018 ([64.3%\\] of 14); and GII.6\\[P7\\] viruses were detected primarily in the 2017–18 winter season ([81.3%\\] of 16).\n\n【20】##### Genotypes Associated with Epidemiologic Characteristics\n\n【21】Among the GII.2\\[P16\\] outbreaks, foodborne transmission was reported for 10 (.6%) of 276 and person-to-person transmission for 266 (.4%) of 276; no waterborne outbreaks were reported (χ 2  10.5; p = 0.003) . Outbreaks with multiple genotypes were more often associated with waterborne and foodborne transmission than were outbreaks with a single genotype.\n\n【22】GII.2\\[P16\\] outbreaks predominated across all childcare centers and school settings . GII.3\\[P12\\] outbreaks occurred in childcare centers ([6.8%\\] of 280), primary schools ([1.9%\\] of 155), and middle schools ([1.6%\\] of 61). GII.17\\[P17\\] outbreaks occurred in universities ([8.3%\\] of 24), middle schools ([6.6%\\] of 61), primary schools ([4.5%\\] of 155), and childcare centers ([0.7%\\] of 280). GI.2\\[P2\\] outbreaks were most commonly detected in middle schools ([8.2%\\] of /61) (χ 2  12.907; p = 0.002) . Of the 14 outbreaks with samples containing multiple genotypes, 4 occurred in universities, 6 in middle schools, 1 in a primary school, and 3 in childcare centers .\n\n【23】### Discussion\n\n【24】CaliciNet China was launched in October 2016 for the surveillance of norovirus outbreaks in China. By using more sensitive inclusion criteria for reporting acute gastroenteritis outbreaks than PHEESS, CaliciNet China captured more norovirus outbreaks, especially in the catchment area where the network laboratories are located. In addition, the participating laboratories implemented standardized genotyping protocols, which enabled comparison across jurisdictions of norovirus sequences, including emerging strains. The sequence data are accessible only within CaliciNet China.\n\n【25】As found in studies in other countries in the Northern Hemisphere, we found the number of norovirus outbreaks was highest during October–March . Person-to-person transmission was the dominant route (.1%), even higher than has been reported in other countries . Almost all (.7%) outbreaks in our study occurred in childcare centers and schools (primary schools, middle schools, and universities). These outbreak settings have likewise been identified in other Asia countries, such as Japan and South Korea , as the most common settings for norovirus outbreaks. In contrast, in the United States and Europe, healthcare facilities (primarily nursing homes and hospitals) are the most commonly reported setting for norovirus outbreaks . The high proportion of norovirus outbreaks in childcare centers and schools seems unique to China and might be associated with the high population density in these settings  and the enhanced monitoring and reporting of any outbreaks in schools in China. In 2006, the national government started to require school officials to check and screen children attending kindergarten, primary school, and middle schools each morning for fever, vomiting, or diarrhea . This program has helped facilitate the detection and reporting of infectious diseases, including norovirus, at kindergartens and schools. However, we received few norovirus outbreak reports from other settings involving more adults and elderly persons (such as companies, restaurants, and nursing homes). This difference in the collection of norovirus outbreaks between school and other settings may cause potential bias of the strain patterns of norovirus.\n\n【26】Previous data from CaliciNet and NoroNet have suggested a correlation of certain genotypes with different transmission modes and outbreak settings. Specifically, GII.4 viruses have been associated with person-to-person transmission, whereas non-GII.4 viruses, such as GI.3, GI.6, GI.7, GII.3, GII.6, and GII.12, are more often associated with foodborne transmission . Other studies reported that GI genotypes more frequently caused waterborne transmission than GII genotypes . In our study, all outbreaks caused by GII.4 Sydney\\[P31\\] were transmitted from person to person; however, GII.2\\[P16\\] viruses predominated among both person-to-person and foodborne outbreaks. A recent study also reported that GII.2 outbreaks were transmitted from person to person (.8%) and food (.5%) . The outbreaks caused by multiple genotypes were more often associated with waterborne transmission than foodborne or person-to-person transmission.\n\n【27】The dominant genotype in our study, GII.2\\[P16\\], was mainly associated with outbreaks in childcare centers and schools, consistent with a study in Japan, which found GII.2 was the most prevalent genotype in childcare facilities and schools for multiple years . In other parts of the world, GII.4 viruses have been reported as the dominant genotype among adults and elderly persons, especially in outbreaks in long-term care facilities . Our study had only 11 GII.4 outbreaks, and they mainly occurred in childcare centers and primary schools. GII.3\\[P12\\] mainly occurred among children in this study, similar to other studies where GII.3 was one of the most common genotypes associated with sporadic norovirus infection, particularly among children . We also found that GII.17 outbreaks tended to infect older children and adults rather than younger children. Previous reports indicated that the median age of persons infected by GII.17 (range 9–75 years) was significantly higher than that of GII.4 cases (range 1–8 years) .\n\n【28】During winter 2014–15, GII.17 was reported as the predominant norovirus strain in China, accounting for 67.2% of outbreaks, whereas during the following winter , the proportion of GII.17 outbreaks decreased to 25.0% . We found that GII.2\\[P16\\] viruses caused an increase in the number of norovirus outbreaks during winter 2016–17. The first GII.2\\[P16\\]–positive sample was detected in August 2016 in Guangdong Province , and the number subsequently increased, with GII.2\\[P16\\] accounting for 70%–100% of norovirus outbreaks during winter 2016–17 in different provinces . Previous studies suggested that reemerging GII.2\\[P16\\] virus most likely evolved from strains emerging during 2012–2013 . Although the antigenicity and histo-blood group antigen binding profile of the early 2016–2017 and pre-2016 GII.2 noroviruses were similar, 1 GII.2\\[P16\\] strain with single Val256Ile mutation and the conventionally orientated Asp382 in VP1 showed an expanded histo-blood group antigen binding spectrum in China . Unlike previously emergent GII.4 viruses, GII.2\\[P16\\] did not become a globally predominant genotype; however, this emerging virus was also detected in other countries, such as Germany, Italy, Japan, France, and the United States .\n\n【29】Since 2002, GII.4 has been the predominant norovirus genotype in outbreaks in many countries, and every 2–4 years, new GII.4 variants have emerged, displacing previous variants, several of which have been associated with increased norovirus outbreaks worldwide . Previous studies indicated that global epidemics, including in China, were caused by GII.4 Den Haag (b) variant in 2006–2007, New Orleans variant in 2009–2010, and GII.4 Sydney variant in 2012–2013. Although GII.4 Sydney has predominated globally, non-GII.4 strains (GII.17\\[P17\\], and GII.2\\[P16\\]) have caused 2 recent norovirus epidemics in China . These observations highlight the need for enhanced global surveillance for potential epidemics of emerging norovirus genotypes, which may have different regional impacts.\n\n【30】CaliciNet China uses database scripts provided by CaliciNet USA  and updated detection and dual genotyping protocols . The dual-typing method is helpful in clarifying the molecular epidemiology of noroviruses, including the identification of newly emerging recombinant viruses. For example, GII.4 Sydney\\[P31\\] predominated since it was first recognized in 2012 and subsequently became one of the most successful genotypes causing epidemics globally . In November 2015, this virus was gradually, although not completely, replaced by a new recombinant GII.4 virus, GII.P16-GII.4 Sydney , and has since caused ≈50% of all norovirus outbreaks in the United States .\n\n【31】Our study has several limitations. First, participation in the network was voluntary, so the distribution of the network does not represent the entire country. Currently, network laboratories are located primarily in eastern and southern China, which generally have better infrastructure. Second, the epidemiologic information for each outbreak collected by CaliciNet China is still limited; however, China CDC is making efforts to include more epidemiologic staff in the network and collect more complete and accurate epidemiologic data. Third, the timeliness of the data reporting in CaliciNet needs to be improved. For a variety of reasons, not all network laboratories were able to submit data to China CDC on a monthly basis. In the future, submission of epidemiologic and laboratory data through a Web-based information system will enable uploading of data in near real time.\n\n【32】CaliciNet China’s use of the same protocols as other norovirus surveillance networks, such as CaliciNet USA, enables comparison of data internationally and potentially provides an early warning when new strains emerge that have the potential to cause global epidemics. In addition, monitoring changes in the distribution of genotypes can help inform the development and assessment of norovirus vaccines, several of which are in clinical trials .\n\n【33】In conclusion, we collected information about 556 norovirus outbreaks using standardized epidemiologic definitions and laboratory testing procedures in 6 provinces in China. Person-to-person was the predominant transmission route, and childcare centers and schools were the most common settings for reported norovirus outbreaks. The large number of outbreaks during winter 2016–17 was attributable at least in part to the emergence of the new recombinant genotype GII.2\\[P16\\]. CaliciNet China provides essential information about the evolving strain distribution and epidemiologic characteristics of norovirus outbreaks, which can contribute to the development of effective vaccines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1617cd9e-aa69-4820-b865-3e51d55faca4", "title": "NSAID-Avoidance Education in Community Pharmacies for Patients at High Risk for Acute Kidney Injury, Upstate New York, 2011", "text": "【0】NSAID-Avoidance Education in Community Pharmacies for Patients at High Risk for Acute Kidney Injury, Upstate New York, 2011\nAbstract\n--------\n\n【1】**Introduction**  \nNonsteroidal anti-inflammatory drugs (NSAIDs) are frequently associated with community-acquired acute kidney injury (AKI), a strong risk factor for development and progression of chronic kidney disease. Using access to prescription medication profiles, pharmacists can identify patients at high risk for NSAID-induced AKI. The primary objective of this analysis was to evaluate the effectiveness of a community pharmacy–based patient education program on patient knowledge of NSAID-associated renal safety concerns.\n\n【2】**Methods**  \nPatients receiving prescription medications for hypertension or diabetes mellitus were invited to participate in an educational program on the risks of NSAID use. A patient knowledge questionnaire (PKQ) consisting of 5 questions scored from 1 to 5 was completed before and after the intervention. Information was collected on age, race, sex, and frequency of NSAID use.\n\n【3】**Results**  \nA total of 152 participants (% women) completed both the pre- and post-intervention questionnaire; average age was 54.6 (standard deviation \\[SD\\], 17.5). Mean pre-intervention PKQ score was 3.3 (SD, 1.4), and post-intervention score was 4.6 (SD, 0.9) (P_ \\= .002). Participants rated program usefulness (= not useful to 5 = extremely useful) as 4.2 (SD, 1.0). In addition, 48% reported current NSAID use and 67% reported that the program encouraged them to limit their use.\n\n【4】**Conclusion**  \nNSAID use was common among patients at high risk for AKI. A brief educational intervention in a community pharmacy improved patient knowledge on NSAID-associated risks. Pharmacists practicing in the community can partner with primary care providers in the medical home model to educate patients at risk for AKI.\n\n【5】Introduction\n------------\n\n【6】More than 98 million nonsteroidal anti-inflammatory drug (NSAID) prescriptions were filled in 2012 . NSAIDs have accounted for more than 70 million prescriptions and 30 billion over-the-counter purchases . NSAIDs are also among the most common medications prescribed inappropriately to older Americans . Among a cohort of 12,065 participants in the cross-sectional National Health and Nutrition Examination Survey who had an estimated glomerular filtration rate (eGFR) between 15 and 50 mL/min/1.73m 2  , 5% reported using over-the-counter NSAIDs regularly and 66.1% had used these agents for 1 year or longer .\n\n【7】Frequent, unmonitored use of NSAIDs among high-risk patients is associated with the development of acute and chronic kidney injury . NSAID use is a common inciting factor for community-acquired acute kidney injury (AKI) . NSAID-induced AKI abruptly alters renal hemodynamics, lowering effective perfusion of the glomerulus .\n\n【8】Interruption of this regulatory pathway increases the risk for hemodynamically mediated AKI, especially in patients who depend on vasodilatory prostaglandins to maintain kidney perfusion . Concomitant use of antihypertensive drugs and NSAIDs has been associated with a 5-fold increase in AKI risk . The relative risk for AKI among concurrent users of NSAIDs and diuretics is 3-fold higher than the risk among nonconcurrent users, likely because of decreased intravascular volume and renal perfusion . Angiotensin-converting enzyme inhibitors (ACEIs) dilate efferent arterioles and reduce glomerular capillary pressure, inhibiting the ability of the efferent arteriole to constrict when the renin–angiotensin–aldosterone system is activated or afferent arteriole vasodilatation is insufficient . Both current and recent use of ACEIs has been associated with as much as a 3-fold increase in the risk for AKI . Differences in pharmacologic selectivity and potential to cause intrarenal hemodynamic changes exist among NSAIDs; however, NSAID-induced AKI depends also on patient factors, which limits the ability to predict outcomes according to each NSAID .\n\n【9】The implications of an episode of AKI are relevant to chronic kidney disease (CKD). After an episode of AKI, kidney function is presumed to be fully recovered if serum creatinine levels return to baseline. However, recent data showed that up to 70% of elderly patients were predisposed to progression and development of de novo CKD within 2 years of an episode of AKI . NSAIDs are an important contributor to risk for AKI and a more rapid progression of CKD. In a cohort analysis of more than 10,000 patients aged 66 years or older, a high dose of NSAIDs was associated with a 26% increase in the risk for a decline in eGFR of more than 15 mL/min/1.73 m 2  within 2 years .\n\n【10】This increased risk for adverse kidney events related to NSAIDs prompted the National Kidney Foundation to recommend displaying a clear warning on over-the-counter NSAID labels in 1985 . The NSAID Patient Safety Study collected data on NSAID use in primary care practices in Alabama . Patients who were identified as current NSAID users were contacted by telephone to participate in a survey. Among the survey participants, 63% used both over-the-counter and prescription NSAIDs, and only 13.7% patients recalled discussing NSAID use with a pharmacist. The authors concluded that pharmacists and pharmacy staff are missing an opportunity to provide counseling to high-risk patients to avoid inappropriate and unsafe NSAID use. The patient surveys indicated that a community pharmacy intervention could be valuable in increasing awareness of the risks of NSAID-induced AKI.\n\n【11】Provision of NSAID avoidance education to patients at risk for AKI is an important but underappreciated prevention strategy. Community pharmacists are readily accessible to these high-risk patients as they visit the pharmacy for prescription refills and over-the-counter purchases . The primary objective of this pilot project was to design and evaluate the effectiveness of a community pharmacy–based patient education program to increase awareness of the safety issues associated with NSAID use among patients at high risk for AKI. The primary outcome measure was patient knowledge questionnaire (PKQ) scores before and after the intervention. Secondary objectives were to quantify current use of NSAIDs and to determine whether the intervention encouraged patients to reduce their NSAID use.\n\n【12】Methods\n-------\n\n【13】### Structured patient education intervention\n\n【14】Pharmacy interns (n = 24) in their last year of the doctor of pharmacy program at Albany College of Pharmacy and Health Sciences delivered the NSAID avoidance education program. The program was a requirement for interns who were completing a 6-week experiential rotation in a Community Pharmacy Advanced Pharmacy Practice Experience site during 3 consecutive 6-week rotations from January through April 2011 (a total of 18 weeks). The program was part of a larger educational initiative, the Wellness Targeted Intervention Program , which requires interns to deliver structured educational interventions during their community pharmacy rotations.\n\n【15】Pharmacy interns were educated on CKD and NSAID-induced hemodynamically mediated AKI by nephrology-trained doctors of pharmacy faculty (A.B.P. and D.W.G.) on the first day of rotation. The education module for interns and pharmacists was delivered in 90 minutes and included an interactive lecture-based format and self-assessment questions. The material on renal hemodynamics and the effects of medications, including NSAIDs, is covered topically in the intern’s core curriculum. As part of the rotation, the topic was reviewed more in depth, and the interns’ knowledge was assessed before delivery of the patient education program. The program provided to the interns and pharmacists included information on how the benefits of low-dose aspirin outweigh the risks of its use for primary or secondary prevention of cardiovascular disease. Because the primary purpose of the intervention was to increase awareness of the safety issues associated with NSAIDs, interns advised program participants to discuss NSAID use further with their primary care provider and offered counseling on topical agents and acetaminophen as alternatives under the direction of their supervising pharmacist.\n\n【16】### Box 1. Text of Patient Education Handout Given to 182 Patients in Community Pharmacies in the Capital Region of Upstate New York,\n\n【17】*   Nonsteroidal anti-inflammatory drugs, or NSAIDs, are over-the-counter pain relievers. Common types of NSAIDs are ibuprofen (Motrin, Advil) and naproxen (Aleve). There are more examples on the back of this sheet.\n*   People with diabetes and high blood pressure are at risk for kidney disease.\n*   NSAIDs may not be good for people at risk for kidney disease because they may cause bad kidney effects — such as lowering blood supply to the kidney.\n*   Adding NSAIDs to some blood pressure medicines can increase the possibility of having bad kidney effects, including lack of good blood flow to the kidneys.\n*   People with high blood pressure and/or diabetes should avoid NSAIDs even short term and should always contact their pharmacist or other care provider before they consider using an NSAID.\n\n【18】### Study population\n\n【19】Our target population for the education program was adults (≥18 y) who were at high risk for NSAID-induced AKI. We reviewed the prescription-dispensing–system medication profiles of adults receiving a new or refilled prescription in a convenience sample of 17 pharmacies (chain \\[n = 5\\], mass merchandiser \\[n = 4\\], supermarket \\[n = 5\\], independent \\[n = 3\\]) in the Community Pharmacy Advanced Pharmacy Practice Experience site network in the capital region of upstate New York. We identified potential participants as those taking antihypertensive medications (ACEIs, angiotensin-receptor blockers \\[ARBs\\], aliskiren, or diuretics), diabetes medications (oral or injectable), or digoxin. No patient identifiers were collected. Pre-intervention and post-intervention PKQs and prescription-dispensing–system medication profiles were numbered by investigators; these numbers were used as identifiers for pharmacy sites and participants. We excluded patients who declined to participate or were unable to read or understand English. Interns recruited participants during prescription pick-up or while patients were waiting for their prescriptions to be filled. Brightly colored notes placed on prescription bags of eligible patients alerted pharmacy staff and interns to engage those patients. Interns also recruited patients while conducting nondispensing activities, such as those during over-the-counter consultations, blood pressure clinics, or NSAID avoidance outreach programs. Patients who declined to participate were given a handout that covered the intervention’s key messages (Box 1).\n\n【20】Patients willing to participate in the program were asked to complete a baseline PKQ before engaging in the intervention. The intervention consisted of a review of the handout and an intern–participant discussion to address each educational objective. After the intervention, the participant completed the post-intervention PKQ and a short questionnaire. The average time for the intervention, including completion of both PKQs, was 10.3 minutes (SD, 3.5 min), according to completed documentation for 53 participant encounters. The interns collected information on medication history (from the patient and the prescription-dispensing–system medication profile).\n\n【21】### Development of the PKQ\n\n【22】For the primary outcome measure, 5 patient-knowledge questions (Box 2) were developed by 2 nephrology-trained pharmacists (A.B.P. and D.W.G.) and an advanced community-practice pharmacist (J.C.). These questions were designed to test the knowledge of participants on 3 learning objectives: 1) The patient will be able to identify common NSAID products by generic/brand, 2) The patient will be able to understand the risks associated with NSAID use, and 3) The patient will be able to understand a course of action they should take to prevent adverse effects from NSAIDs.\n\n【23】### Box 2. Text of 5 Patient Knowledge Questions on NSAIDs Administered to 182 Patients in Community Pharmacies in the Capital Region of Upstate New York,\n\n【24】1.  Which of the following medications is a nonsteroidal anti-inflammatory drug, or NSAID?\n\n【25】    a. Acetaminophen (Tylenol)  \n    b. Ibuprofen (Motrin)  \n    c. Pseudoephedrine (Sudafed)  \n    d. Menthol arthritis rubs (Mineral Ice, Icy Hot)  \n\n【26】2.  People at risk for kidney disease include people with which of the following medical conditions?\n\n【27】    a. high blood pressure  \n    b. diabetes  \n    c. asthma  \n    d. a and b only  \n\n【28】3.  NSAIDs may not be safe to use in patients with kidney disease or at risk for kidney disease.\n\n【29】    a. True  \n    b. False  \n\n【30】4.  Adding NSAIDs to prescription medications for high blood pressure can increase the risk of having bad kidney effects.\n\n【31】    a. True  \n    b. False  \n\n【32】5.  If someone has high blood pressure or diabetes, what should they do if they are in need of a pain reliever?\n\n【33】    a. They should avoid NSAIDs even short term.  \n    b. NSAIDs are not a problem in patients with high blood pressure or diabetes.  \n    c. They should contact their pharmacist or other care provider before they consider using an NSAID.  \n    d. a and c only  \n\n【34】The questions were peer reviewed and piloted with 7 lay people for readability and clarity before being used in the study. Questions on age, race, and sex were included in the pre-intervention PKQ, and questions on current NSAID use and frequency of use were included in the post-intervention PKQ. Also, the post-intervention PKQ asked participants to rate the usefulness of the program on a 5-point Likert scale (= not useful to 5 = extremely useful) and to indicate whether the program encouraged them to avoid NSAID use in the future. If participants indicated the program did not encourage them to avoid NSAIDs, they were asked to explain why not in an open-ended response. The project was approved by the Albany College of Pharmacy and Health Sciences institutional review board.\n\n【35】### Analysis\n\n【36】We performed a descriptive analysis (mean, standard deviation \\[SD\\], range, or frequency) of all data. Only data on participants who completed both the pre-intervention and post-intervention PKQ were included in analysis. PKQs were scored according to the number of correct answers (range, 0 to 5). Pre-intervention and post-intervention PKQ scores were compared using a paired Student _t_ test. Differences in characteristics (age, sex, and race) associated with poor health literacy among certain populations with kidney disease were compared using the unpaired Student _t_ test . Comparisons of categorical data were analyzed by χ 2  test. One-way analysis of variance (ANOVA) with Bonferroni correction was performed to evaluate percentage changes in pre-intervention and post-intervention PKQ scores by pharmacy type and pharmacy intern. All analysis was performed in STATA/SE 13.1 (StataCorp LP).\n\n【37】Results\n-------\n\n【38】During the 18-week intervention, 182 patients participated in the education program, and 152 participants were included in the analysis. Of the 152 participants, 55 (%) were recruited at supermarket pharmacies, 42 (%) at chain pharmacies, 31 (%) at mass merchandiser pharmacies, and 24 (%) at independent pharmacies.\n\n【39】Average age was 54.6 (SD, 17.5); 76% were white; 60% were women . The mean (SD) score on the pre-intervention question on avoidance was 3.3 (.4), and the mean post-intervention score was 4.6 (.9) (P_ < .001). The scores of white participants and black participants significantly increased after the intervention . Women had slightly lower scores than men before the intervention (women, 3.2 \\[1.5\\]; men, 3.4 \\[1.4\\], _P_ \\= .35); however, both women and men had significantly higher scores after the intervention (women, 4.4 \\[0.9\\]; men, 4.7 \\[0.7\\]; _P_ < .02 for pre–post comparison). Participants aged 65 or older had lower scores (.9 \\[1.3\\]) than participants younger than 65 years (.4 \\[1.5\\]) (P_ \\= .049). Participants who scored less than 100% on the post-intervention PKQ were older (mean age, 60 y \\[SD, 15 y\\]) than those who scored 100% (y \\[17 y\\]) _P_ \\= .02). Although differences were not significant, those who scored less than 100% tended to be women (% vs 48% of men) and had a lower rate of NSAID use (% of participants who scored less than 100% vs 12% of participants who scored 100% reported current use of NSAIDs).\n\n【40】The use of ACEIs, ARBs, and diuretics was similar for participants who scored less than 100% and for those who scored 100% on the post-intervention PKQ. By type of pharmacy, participants at chain pharmacies showed the least improvement in score (P_ < .01 in comparison of chain pharmacies with all other types of pharmacy). By intern, we found no significant differences in score changes. Three-quarters of participants rated the intervention as very useful (n = 46) or extremely useful (n = 68); program usefulness was scored as 4.2 (SD, 1.0) overall.\n\n【41】Overall, 48% reported current NSAID use. Among NSAID users, 45% used NSAIDs 1 or 2 times per month, 29% used them 1 or 2 times per week, and 26% used them more than 1 or 2 times per week. The most common concomitant medications among participants with current prescription (n = 19) or over-the-counter NSAID use (n = 54) were ACEIs or ARBs (%), oral antidiabetic agents (%), β blockers (%), and diuretics (%). The combination of an ACEI or ARB with a diuretic was reported by 7% of NSAID users. Concomitant use of an ACEI with an ARB or an ARB with aliskiren, which is associated with labeled warnings and contraindications for participants with or at risk for kidney disease, was recorded for 2 participants. Sixty-seven percent reported that the program encouraged them to limit their use; 72% reported that this program encouraged them not to use NSAIDs for pain control. We found no significant correlation between pre-intervention PKQ score and perceived value of the program.\n\n【42】Discussion\n----------\n\n【43】Patient education about kidney disease and kidney-related risks is complex and requires multidisciplinary patient education interventions . An intensive education program focused on NSAID-induced gastrointestinal bleeding delivered by nurse practitioners in a primary care setting reduced dosage and overall use of prescription NSAIDs at 6 months among participants to a greater extent than did delivery of written materials ; however, the study’s randomized design could have caused the Hawthorne effect, whereby study participants modified their behavior simply because they were being observed by study investigators. One drawback of primary care office–delivered education programs is the length of time between follow-up appointments. Our community-pharmacy–based intervention, in which counseling and education are initiated when prescriptions are picked up or over-the-counter NSAID purchases are made, allows for more frequent reinforcement of patient knowledge. This type of education program can complement and augment chronic disease education provided by primary care practitioners, promoting and improving the concept of the medical neighborhood for the patient-centered medical home .\n\n【44】Administration of a pre-intervention and post-intervention PKQ is a validated approach to assessing the effectiveness of an education program. Previous studies using this approach concluded that pre-intervention and post-intervention PKQs were useful to assess patients’ knowledge and were reliable, valid, and reproducible . Pharmacist-delivered education programs using PKQs successfully addressed unsafe use of other medications .\n\n【45】In our high-risk patient population, 48% reported current use of NSAIDs despite high risk for development of AKI. This rate is higher than some previously reported rates, perhaps because we collected data on over-the-counter use of NSAIDs and included patients who may have been at risk for CKD but did not have it . Given the extensive use of both prescription and over-the-counter NSAIDs and the prevalence of people at high risk for AKI in the U.S. population, awareness of the risk for AKI among health care professionals needs more attention. Robust data corroborate the idea that an episode of AKI can lead to development of CKD or accelerate the progression of existing CKD . The clinical practice guidelines of the organization Kidney Disease: Improving Global Outcomes state “all people with CKD are considered to be at increased risk for AKI” . In a nested-case–control study of nearly 400,000 patients aged 50 to 84 years, current NSAID use was associated with a 3-fold higher risk for AKI . The risk for AKI declined after NSAID treatment was documented to be discontinued by refill history. The impact of NSAID discontinuation on AKI underscores the importance of education on the appropriate use of NSAIDs. An analysis of 450 patients (% of whom were 56 years or older) receiving NSAID prescriptions from a primary care center showed that only 14% of patients had their kidney function determined before initiating therapy. This study illustrates that risk for CKD and AKI is not routinely assessed among those for whom NSAIDs are prescribed.\n\n【46】Health literacy is an important consideration in patient education programs and success of educational interventions. We found that older participants had lower baseline PKQ scores than younger participants. This finding is consistent with research that identified older age as a risk factor for low health literacy . Our data can guide future education programs to focus on older patients, who are at especially high risk for AKI.\n\n【47】Success of any patient education program is due in part to perception of the intervention among participants. Importantly, 75% of participants in our program found it “very useful” or “extremely useful.” Community-based pharmacy education programs have been shown to be successful and sustainable with other targeted intervention programs .\n\n【48】This study has several limitations. Retention and application of knowledge over time was not assessed after the post-intervention PKQ was administered; the lack of a second post-intervention PKQ is one limitation. Performing any test of knowledge immediately after training does not indicate how long the individual will retain the information . In addition, this study used a convenience sample drawn from largely affluent suburban communities and thus limits the applicability to other populations. Deficits in health information technology and pharmacy-dispensing software interfaces limited access to comorbidity data. Although the intervention was generally well accepted by participants, 30 participants did not complete the post-intervention PKQ. Others refused to participate in the program for various reasons, such as lack of interest, too much paperwork, or intimidation by the quiz-like set-up. Chain pharmacies were associated with the least improvement in post-intervention scores. Although the potential variables associated with this result need further exploration, this result suggests a possible need to tailor intervention implementation by location in future scaled-up initiatives. Feedback from pharmacy interns suggested that omitting the PKQ would increase engagement of participants in the program. After receipt of feedback from the first 6-week rotation of interns, additional tips were provided to interns to increase patient participation. One tip was to include a return-reply envelope for patients who declined to complete the post-intervention PKQ.\n\n【49】Patient knowledge of NSAID kidney-safety issues improved as a result of this community pharmacy–based education program. Pharmacists are easily accessible health care providers and can augment primary care office- based education programs. Based on the observed need for patient education on NSAID avoidance, we augmented our core curriculum on the topic of medications and renal hemodynamics for doctor of pharmacy candidates. The success of this project led to collaboration with the National Kidney Disease Education Program to develop an innovative, interactive continuing education program on NSAID avoidance for pharmacists and other health care providers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aae312da-94a5-4405-8d1c-234a4c128207", "title": "Deforestation and Vectorial Capacity of Anopheles gambiae Giles Mosquitoes in Malaria Transmission, Kenya", "text": "【0】Deforestation and Vectorial Capacity of Anopheles gambiae Giles Mosquitoes in Malaria Transmission, Kenya\nLand use changes in the form of deforestation and swamp cultivation have been occurring rapidly in the western Kenyan highlands because of unprecedented human demand for forest products and land for agricultural cultivation . In the East African highlands, 2.9 million hectares of forest were cleared between 1981 and 1990, representing an 8% reduction in forest cover in 1 decade . As has been observed in the Usambara Mountains of Tanzania  and in the southwestern highlands of Uganda , land use change in Kenya may have exacerbated malaria epidemics caused by _Plasmodium falciparum_ parasites and its mosquito vectors _Anopheles gambiae_ and _An_ . _funestus_ , although other factors may have also contributed to the surge in epidemics, including global warming , climate variability , and drug resistance . Land use change can influence malaria transmission by increasing the temperature and decreasing the humidity of vector mosquito habitats. This in turn affects biting, survival, and reproductive rates of vectors .\n\n【1】Temperature changes will also shorten the development time of _P_ . _falciparum_ in mosquitoes . Development of malaria parasites in mosquitoes (sporogony) involves a sequential process of developmental steps. Gametocytes ingested by anopheline mosquitoes develop into zygotes, ookinetes, oocysts, and eventually sporozoites in the salivary glands, ready for transmission to the next human host. Generally, the extrinsic incubation period of _P_ . _falciparum_ is inversely correlated with temperature. At 27°C, the extrinsic incubation period of _P_ . _falciparum_ from zygotes to sporozoites was found to be 12 days . The relationship between temperature and extrinsic incubation period is not linear because small changes at low temperatures can have proportionally large effects on parasites’ extrinsic incubation period. Thus, geographic distribution of malaria is confined within climates favoring its extrinsic cycle, provided that other conditions do not limit mosquito survival . In turn, duration of the sporogonic development of malaria parasites in mosquitoes is an important component of vectorial capacity. Vectorial capacity measures the potential rate of contact between infectious vectors and susceptible hosts.\n\n【2】The objective of this study was to assess the effect of deforestation on the microclimate (temperature and humidity) of houses where mosquitoes normally reside with their parasites and to investigate how these microclimate changes affect the sporogonic development of the malaria parasite within _An_ . _gambiae_ . Humidity may not have a direct effect on the sporogonic development of malaria parasites, but it may affect mosquito survival through the combined effect of temperature and humidity (saturation deficit). We hypothesized that land use changes in the highlands alter the microclimatic conditions of vector mosquitoes and their parasites, subsequently enhancing malaria risk in the area.\n\n【3】### Materials and Methods\n\n【4】##### Study Sites\n\n【5】This study was conducted at a highland site in Iguhu village (°45′E, 0°10′N, 1,430–1,580 m above sea level) in Kakamega district in western Kenya. A lowland site in Kisian village (°75′E, 0°10′S, 1,190 m above sea level) in Kisumu district in western Kenya was studied for comparison. The highland area of Kakamega district has 2 rainy seasons and ≈1,800 mm of rainfall per year. The long rainy season usually occurs between mid-March and May, with an average monthly rainfall 150–260 mm, and the short rainy season usually occurs between September and October, with an average monthly rainfall of 165 mm. Malaria prevalence peaks usually lag 1–2 months after the rain . The mean annual daily temperature is 20.8°C. A natural forest is located east of the study area, constituting ≈15% of the total area. The rest of the area is a continuum of forest in different states of deforestation. A forested area was defined as an area having a tree canopy cover >60%. The deforested area has canopy cover <10%. The deforested area was once forested but the forest has been cut down. The predominant malaria vector species in the area is _An_ . _gambiae_ s.s .\n\n【6】We also studied 1 lowland site (Kisian) as a comparison site for parasite development. Kisian (m above sea level) is flat land located on the shores of Lake Victoria where malaria transmission is perennial and malaria vectors include _An_ . _gambiae_ s. s. _An_ . _arabiensis_ , and _An_ . _funestus_ . The land cover type in Kisian is primarily farmland with little tree canopy coverage; thus, Kisian is classified as a deforested area. The average minimum and maximum temperatures during 1970–2000 were 15.0°C and 28.4°C, respectively. The average annual rainfall within this same period was ≈1,400 mm.\n\n【7】##### Selection of _P_ . _falciparum_ Gametocyte Carriers\n\n【8】The study population was primary school students in Iguhu. Children 5–14 years of age were recruited for this study and were screened monthly for _P_ . _falciparum_ gametocytes in their blood. The ethical review boards of the Kenya Medical Research Institute, Kenya, and the University of California, Irvine, reviewed and approved the protocol for screening of _P_ . _falciparum_ gametocyte carriers and subsequent mosquito infections. Parents or guardians, as well as the children involved in the study, signed a consent or assent form to participate in this study.\n\n【9】##### Mosquito Infection\n\n【10】Students (gametocyte carriers) who had >40 gametocytes/μL of blood and who consented to participate in the study were asked to donate 10 mL of blood, which was obtained by a clinician. Butterfly needles were used in drawing the blood into a heparinized tube. This blood was immediately centrifuged at 2,000 rpm for 5 min. The supernatant (serum) was discarded and replaced with human AB serum (Cambrex Bio Science, Walkersville, MD, USA). All equipment used was warmed to 37°C. After replacement of the serum, the blood was placed in warmed artificial membrane feeders. _An_ . _gambiae_ mosquitoes were placed in paper cups at a density of 60/cup and allowed to feed on the infected blood for 30 min. Fed mosquitoes were divided and placed in 3 cages (cm 3  ), which were hung in bedrooms of houses in deforested and forested areas in Iguhu and in the lowland site. Four houses were used at each site. At each mosquito feeding, 600 mosquitoes were provided an opportunity to blood feed and an average of 60–80% of mosquitoes took a blood meal. Fed mosquitoes were distributed equally in cages at the 3 sites. _An_ . _gambiae_ mosquitoes used in this experiment were obtained from the village of Iguhu but had been bred in an insectary and adapted to feed from a membrane feeder. All gametocyte donors were treated with amodiaquine by the clinicians in Iguhu Health Centre. Feeding of mosquitoes was conducted in a secure, insect-proof room at the Iguhu Health Centre. Fed mosquitoes were transported to different sites in paper cups placed in cool boxes.\n\n【11】##### Dissection for Oocysts and Sporozoites\n\n【12】Starting at 5 days after mosquitoes were exposed to different house environments, 30 mosquitoes from each cage at each site were dissected daily in 2% mercurochrome and examined for oocysts. The gut of each mosquito was carefully drawn out of the abdomen and observed under a light microscope. Infected mosquitoes were counted and the number was recorded; the oocyst load was expressed as number of oocysts per infected mosquito. Dissection for sporozoites started on day 9 postinfection. Salivary glands of each mosquito were taken from the thorax and crushed under a coverslip. This material was then observed under a microscope for sporozoites. This part of the study was conducted during April–November 2005.\n\n【13】##### Climate Data Collection\n\n【14】HOBO data loggers (Onset Computer Corporation, Bourne, MA, USA), which are devices for measuring temperature and humidity, were placed inside all experimental houses where the duration of sporogony of _P_ . _falciparum_ was measured to record temperature and relative humidity hourly. Data loggers were suspended from the roof 2 m above the ground. Outdoor temperature was recorded by using these data loggers placed in standard meteorologic boxes 2 m above the ground. All selected houses had a data logger for indoor and outdoor microclimate measurements. These data were offloaded from the data loggers by using a HOBO shuttle data transporter (Shuttle; Onset Computer Corporation) and downloaded to a computer by using BoxCar Pro version 4.0 (Onset Computer Corporation).\n\n【15】##### Statistical Analysis\n\n【16】Daily mean maximum, mean, and minimum temperatures were calculated from hourly temperature and humidity data recorded throughout the experimental period. Analysis of variance with repeated measures was used to compare monthly mean (based on the average per house in each site) temperatures throughout the entire study period. Analysis of variance was also used to test the effect of site (a fixed effect with 3 levels: forested, deforested, or lowland) and donor child (a random effect) on the average (per cage) time to sporozoite detection, the average (per cage) oocyst load, or the infection rate. Time to sporozoite infection was defined as the time postinfection needed for sporozoites to appear in dissected mosquitoes. Infection rate was defined as the proportion of mosquitoes containing oocysts or sporozoites per total number of mosquitoes dissected in each cage. This proportion was arcsin square root transformed before analysis. Oocyst load was defined as the average of oocyst counts in all the positive mosquito midguts. A total of 34 membrane feedings (gametocyte donors) were conducted; 29 feedings yielded infection rates >10% and were used in the data analysis. Analyses were performed with the JMP statistical package .\n\n【17】To evaluate how the time for sporozoite development affects vectorial capacity, we used the formula of MacDonald . For this analysis, vectorial capacity = _ma_ 2  _p_ n  /(–ln(p_ )), in which _m_ is the relative density of vectors in relation to human density, _a_ is the average number of persons bitten by 1 mosquito in 1 day, _p_ is the proportion of vectors surviving per day, and n is the duration of sporogony in days. Therefore, vectorial capacity is the number of future infections that will arise from 1 current infective case. Vector abundance was calculated from the population dynamics data from the study area during June 2003–June 2004 , which was stratified by land use type. Parameter _a_ was calculated as 2/first gonotrophic cycle duration that we estimated from the same study site , which was also stratified by land use type, assuming that double feeding is required for 1 gonotrophic cycle. Parameter _p_ was estimated from results of our previous study on adult survival in the same setting, which is an approximation to observed age-dependent survival . Duration of sporogony was calculated on the basis of data from the present study.\n\n【18】### Results\n\n【19】##### Indoor Environment\n\n【20】Mean indoor temperatures in experimental houses during the entire experimental period differed significantly among the 3 sites (forested area 21.5°C, deforested area 22.4°C, lowland 23.9°C; F 2,8  \\= 4.5, p<0.05) . Post hoc contrast showed a difference between the lowland and either highland site. Indoor maximum and minimum temperatures mirrored this difference. Mean indoor humidity within experimental houses also differed significantly among the 3 sites (forested area 75.9%, deforested area 60.6%, lowland 64.7%; F 2,8  \\= 8.1, p<0.05) . Post hoc contrast showed a significant difference between the lowland and both highland sites and between the forested and deforested areas in the highland (F 1,7  \\= 14.5, p<0.001).\n\n【21】##### Outdoor Environment\n\n【22】Mean outdoor temperatures within the experimental houses during the entire experimental period differed significantly among the 3 sites (forested 19.0°C, deforested 19.9°C, lowland 22.4°C; F 2,8  \\= 58.1, p<0.0001). Post hoc contrast showed a significant difference between the lowland and either highland site, and also between the forested and deforested areas in the highland (F 1,8  \\= 82.8, p<0.05). Indoor maximum and minimum temperatures mirrored this difference. Mean outdoor humidity did not differ among the 3 sites (F 2,8  \\= 2.6, p>0.05); this was true also for maximum and minimum humidity values.\n\n【23】##### Parasite Infection and Development\n\n【24】The number of oocysts in a mosquito ranged from 1 to 98. The proportion of mosquitoes that carried _P_ . _falciparum_ infection (either oocysts or sporozoites) differed significantly among the sites (F 2,30  \\= 12.1, p<0.0001) , and post hoc contrast indicated a significant forested–deforested difference within the highland site (F 1,30  \\= 16.5, p<0.0001). Mean oocyst intensity was affected by land use type (F 2,30  \\= 6.5, p<0.01) . Time for sporozoite development differed between sites (F 2,13  \\= 9.1, p<0.01), and post hoc contrasts indicated a significant difference between forested and deforested areas in the highland site (F 1,13  \\= 6.9, p<0.05) , and the lowland site exhibited the shortest sporozoite development time. Post hoc contrasts did not result in different conclusions from the alternative approach of reducing the number of factor levels in the dataset (e.g. comparing forested with deforested sites in the absence of lowland data).\n\n【25】##### Effects of Deforestation on Vectorial Capacity\n\n【26】Vectorial capacity at the 3 study sites was estimated by using the formula of MacDonald . Female _A_ . _gambiae_ mosquito density (m_ ) was estimated to be 3.05 mosquitoes/person/day in the highland forested area, ≈4.64 in the highland deforested area, and ≈8 in the lowland site . We estimated vectorial capacity to be 77.7% higher in the deforested area than in the forested area in the highlands .\n\n【27】### Discussion\n\n【28】Environmental changes, whether natural phenomena or the result of human intervention, alter the ecologic context within which vectors and their parasites breed, develop, and transmit disease . Large-scale conversion of tropical forests for agricultural purposes can change surface properties (e.g. soil wetness, reflectivity, and evaporation rates) of an area, leading to changes in local climate . Bounoua et al. reported that in the tropics and subtropics, conversion warms canopy temperature by 0.8°C year round. Our study established the role of deforestation on local microclimate and on the sporogonic development of _P_ . _falciparum_ in _An_ . _gambiae_ mosquitoes from the western Kenyan highlands. Deforestation appears to alter the climate in highlands, in particular, making it warmer and less humid. Ingestion of _P_ . _falciparum_ gametocytes by _An_ . _gambiae_ mosquitoes is more likely to result in infection, and the development time for oocyts and sporozoites is shorter in deforested sites than in forested sites. Given the established effects of temperature on _P_ . _falciparum_ development, we conclude that changes in climatic conditions caused by deforestation have caused changes in parasite infection rates and development. Okech et al. showed that in different microhabitats with different temperatures in a lowland site in western Kenya, increasing temperatures led to an increase in infection rates of _An_ . _gambiae_ mosquitoes fed on blood infected with _P_ . _falciparum_ gametocytes. However, as with our study, in the study of Okech et al. oocyte densities did not differ with increasing temperatures. Noden et al. examined the effect of temperature on development of _P_ . _falciparum_ in _An_ . _stephensi_ mosquitoes (normally a vector of rodent malaria) and found that the rate of ookinate development was lengthened as temperature decreased from 27°C to 21°C. They concluded that temperature affects sporogonic development of _P_ . _falciparum_ in anophelines by altering kinetics of ookinete maturation.\n\n【29】Our study has implications for understanding the effects of deforestation on malaria risk in the western Kenyan and other Africa highlands. Force of malaria transmission may be measured by using vectorial capacity. Duration of sporogony of _P_ . _falciparum_ in mosquitoes is exponentially related to vectorial capacity . If daily survival and biting frequency of a vector are assumed to be constant, decreasing the duration of sporogony leads to an increase in vectorial capacity. In our study, deforestation led to a decrease in duration of sporogony of _P_ . _falciparum_ by 1.1 days. Together with other factors that were also influenced by deforestation, such as increased mosquito density, biting frequency, and enhanced survivorship, This decrease translates into an increase in vectorial capacity of _An_ . _gambiae_ mosquitoes by 77.7%. The implication of this finding is that deforestation in the western Kenyan highlands could potentially increase malaria risk. In African highlands where temperature is an important driving factor for malaria and the human population generally has little functional immunity , relationships between land use, microclimate, and malaria should be carefully considered during economic development planning to mitigate the effects of malaria on human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "83f199b1-ac1e-4ede-80e7-f3afa475e297", "title": "Zika Virus Outbreak, Bahia, Brazil", "text": "【0】Zika Virus Outbreak, Bahia, Brazil\n**To the Editor:** Zika virus (ZIKV) is a mosquito-borne flavivirus related to yellow fever virus, dengue virus (DENV), and West Nile virus (WNV). It is a single-stranded positive RNA virus (nt genome) that is closely related to the Spondweni virus and is transmitted by many _Aedes_ spp. mosquitoes, including _Ae. africanus_ , _Ae. luteocephalus_ , _Ae. hensilli_ , and _Ae. aegypti_ . The virus was identified in rhesus monkeys during sylvatic yellow fever surveillance in the Zika Forest in Uganda in 1947 and was reported in humans in 1952 .\n\n【1】In 2007, an outbreak of ZIKV was reported in Yap Island, Federated States of Micronesia . ZIKV also caused a major epidemic in the French Polynesia in 2013–2014 , and New Caledonia reported imported cases from French Polynesia in 2013 and reported an outbreak in 2014 .\n\n【2】A new challenge has arisen in Brazil with the emergence of ZIKV and co-circulation with others arboviruses (i.e. DENV and chikungunya virus \\[CHIKV\\]). We report ZIKV infection in Brazil associated with a recent ongoing outbreak in Camaçari, Bahia, Brazil, of an illness characterized by maculopapular rash, fever, myalgias/arthralgia, and conjunctivitis.\n\n【3】On March 26, 2015, serum samples were obtained from 24 patients  at Santa Helena Hospital in Camaçari who were given a presumptive diagnosis of an acute viral illness by emergency department physicians. These patients were given treatment for a dengue-like illness, and blood samples were obtained for complete blood counts and serologic testing by using an ELISA specific for IgG and IgM against DENV.\n\n【4】Serum samples were analyzed at the Federal University of Bahia by reverse transcription PCR (RT-PCR) to detect DENV, CHIKV, WNV, Mayaro virus, and ZIKV. In brief, serum samples were subjected to RNA extraction by using the QIAamp Viral RNA Mini Kit (QIAGEN, Hilden, Germany). RNA was reverse transcribed by using the SuperScript II Reverse Transcription Kit (Invitrogen, Carlsbad, CA, USA) and subjected to PCRs specific for DENV  CHIKV , WNV  and Mayaro virus . A positive RT-PCR for a partial region of the envelope gene with primers ZIKVENF and ZIKVENVR (positions 1538–1558 and 1902–1883, respectively)  was considered indicative of ZIKV infection. PCR products (bp) were sequenced at the ACTGene Analises Moleculares, Alvorada, Rio Grande do Sul (Porto Allegre, Brazil), and sequences were deposited in GenBank under accession nos. KR816333–KR816336.\n\n【5】All patients were negative by RT-PCR for DENV, Mayaro virus, and WNV. Samples from 7 (.2%) patients were positive by RT-PCR for ZIKV (bp fragment) and from 3 (.5%) patients for CHIKV (bp fragment). There was no simultaneous detection of ZIKV and CHIKV. Most (.7%) patients positive for ZIKV were women; they had a median age of 28 years and no history of international travel. Patients positive for ZIKV sought medical care after a 4-day (range 1–5 days) history of rash, myalgias, arthralgias, or fever. Three patients had IgG against DENV, which is consistent with a previous DENV infection, and none of the 7 ZIKV-positive patients had a positive response for DENV.\n\n【6】Mean laboratory findings for patients with acute ZIKV infection were a leukocyte count of 3,750 cells/mm 3  (range 2,790 cells/mm 3  –6,150 cells/mm 3  ) and a platelet count of 180,000 platelets/mm 3  (range 151,000 platelets/mm 3  –274,000 platelets/mm 3  ). The mean C-reactive protein level was 16.3 mg/L (range 0.9 mg/L–19.7 mg/L). Sign and symptom duration was 1–5 days, and most patients had a maculopapular rash, myalgias, fever, and headache. Arthralgia was seen less frequently.\n\n【7】ZIKV infections were assessed by sequencing partial ZIKV envelope gene regions of isolates. Phylogenetic analysis rooted with Spondwei virus showed that ZIKV sequences obtained belonged to the Asian lineage and showed 99% identity with a sequence from a ZIKV isolate from French Polynesia (KJ776791) .\n\n【8】We report ZIKV infection in Brazil in association with an ongoing outbreak of an acute maculoexantematic illness. Although the patient population samples were not randomly selected, 42% (/24) of the patients were positive for ZIKV (n = 7) or CHIKV (n = 3) and had maculopapular rash, fever, myalgias and headache. After detection of ZIKV in Bahia, many cases have been identified in other states .\n\n【9】Cases of infection with DENV, CHIKV, and ZIKV in Brazil and elsewhere will make diagnosis based on clinical and epidemiologic grounds unreliable. These issues show the need for laboratory confirmation of these arboviral infections. More studies are needed to address the effects of these concurrent arboviruses infections in Brazil.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ff2d8489-dbed-4519-8454-e805ef9fab28", "title": "Recurrent Lymphocytic Meningitis Positive for Herpes Simplex Virus Type 2", "text": "【0】Recurrent Lymphocytic Meningitis Positive for Herpes Simplex Virus Type 2\nRecurrent lymphocytic meningitis (RLM) is a rare disease, characterized by attacks of sudden onset aseptic meningitis followed by complete recovery and unpredictable recurrences. The disease is most often caused by herpes simplex virus type 2 (HSV-2) and less frequently by other viruses, autoimmune disorders, or medication. Symptomatic episodes of RLM usually subside within 5 years, but the total number of episodes may reach 30. Patients are typically middle-aged, and women are more often affected than men . In addition to symptoms typical of meningitis, ≈50% of patients have transient hallucinations, seizures, cranial nerve palsies, or an altered level of consciousness .\n\n【1】### The Study\n\n【2】This study was conducted at Helsinki University Central Hospital, Finland, which serves a population of 1.4 million. The prevalence study covered January 1996 through December 2006. This period differed from that of the patient study because the World Health Organization’s coding system changed in 1996 to the International Classification of Diseases, 10th Revision. Diagnostic codes A87, B00.3 + G02.0, B01.0 + G02.0, B02.1, G02\\*, G03.0, G03.1 and G03.2 were used to identify study cases.\n\n【3】The patient study was conducted from January 1994 through December 2003. All patients with RLM (> 2 clinical episodes, lymphocytic predominance and negative bacterial culture from cerebrospinal fluid \\[CSF\\], and HSV-2 DNA in at least 1 CSF sample) were recruited. A structured questionnaire was used to interview patients about symptoms during and after meningitis episodes. Antibodies against HSV types 1 and 2 were tested on the study entry day, which was at least 1 month after the most recent RLM episode. Sixty-two age- and sex-matched healthy participants served as controls in the laboratory analysis.\n\n【4】Type-specific HSV-1 and -2 immunoglobulin (Ig) G and IgM were measured by enzyme immunoassay (HerpeSelect 1&2 ELISA IgG; Focus Diagnostics, Cypress, CA, USA; and EIAgen HSV IgM; Adaltis, Bologna, Italy). The detection of HSV DNA in CSF samples was performed as described .\n\n【5】Statistical comparisons between groups were made by using a permutation test for titiers of antibodies against HSV-2, and Fisher exact test for HSV seropositivity. Kaplan-Meier estimate was used to illustrate information on the cumulative proportions of the second meningitis episode.\n\n【6】During the prevalence study, from January 1996 through December 2006, a total of 665 patients were treated at the Helsinki University Central Hospital for lymphocytic meningitis. Meningitis was recurrent in 37 patients (.6%). Twenty-eight patients with RLM had HSV-2 DNA in CSF. In addition, 3 patients had recurring genital herpes and elevated HSV-2 serum titers. Thus, the minimum 11-year period prevalence of RLM was 2.7/100,000 population (% confidence interval \\[CI\\] 1.9–3.7) and that of HSV-2 associated RLM 2.2/100,000 population. HSV-2 was the likely etiologic agent in 84% of all RLM cases. Six patients (%) had no herpetic etiology. One had systemic lupus and 1 had Sjögren syndrome; in 4 patients, etiology remained unknown.\n\n【7】During the patient study, from January 1994 through December 2003, 86 patients had a CSF sample positive for HSV DNA. Of these patients, 23 (%) were diagnosed with RLM; 22 case-patients (age: mean 40 years, range 25–55 years; 18 females, 4 males) were enrolled in the study.\n\n【8】HSV-1 seropositivity was less common in case-patients than in controls (% vs. 52%; p = 0.043). All case-patients and 19% of the controls were seropositive for HSV-2 (p = 0.003). IgG antibody titers against HSV-2 were higher in case-patients than in seropositive controls (median 118 vs. 79; p = 0.034). IgM against HSV was not detected in 96% of the episodes.\n\n【9】The 22 case-patients had a combined 95 episodes (mean 4.3) of meningitis. The presence of HSV DNA in CSF had been analyzed during 48 episodes . HSV-2 DNA was present in 82% of the samples taken during the first 2–5 days, and in 46% of samples obtained 24–48 hours after the first symptoms. If the sample was obtained either earlier or later, no HSV-2 DNA was detected, despite previous HSV-2 DNA-positive episodes. The median leukocyte count during the first HSV-2 PCR positive episode was 350 cells/mm 3  (range 44–1,410 cells/mm 3  ). In PCR negative cases, the leukocyte counts were lower.\n\n【10】The median patient follow-up time was 16.2 years (range 4–32 years). The number of meningitis episodes per case-patient varied from 2 to 13 , and the number of meningitis episodes per follow-up year was 0.28 (% CI 0.22–0.35). The time between the first and the second episode of meningitis ranged from 1 to 216 months (median 47 months).\n\n【11】Eight case-patients (%) reported paresthesias and 7 (%) had neuropathic pain during or after meningitis . The pain typically radiated to the extremities, followed a dermatome pattern, and could last up to several years after meningitis. Arthralgias (n = 6) and urinary dysfunction (n = 5) were common.\n\n【12】In 56 (%) of the 95 episodes, case-patients received antiviral medication to alleviate symptoms. Seven case-patients (%) used daily antiviral medication to prevent new episodes; 2 experienced new episodes despite the medication.\n\n【13】Five case-patients (%) experienced their first episode of meningitis after symptomatic HSV-2 genital herpes; 4 of these case-patients were HSV-1 seronegative at that time. In 4 case-patients (%), genital herpes recurred frequently (more than 6 episodes per year). Four case-patients (%) had a history of labial herpes; 7 (%) had no history of herpetic infections.\n\n【14】### Conclusions\n\n【15】The prevalence of HSV-2–associated RLM has been estimated to be 1/100,000 population . According to our study, the 11-year prevalence of HSV-2–associated RLM was higher, 2.2/100,000 population. Because the disease is only periodically active with long asymptomatic periods, its accurate prevalence in the population is difficult to define.\n\n【16】Paresthesias, neuropathic pain, arthralgias, and urinary dysfunction were common during and after meningitis. Case reports have described paresthesias associated with meningitis . Radiculomyelitis, urinary retention, and neuralgia associated with HSV-2 genital herpes have been reported, but rarely in connection with RLM .\n\n【17】The case-patients in our study were less often seropositive for HSV-1, and their IgG titers against HSV-2 were higher than in controls. Lately, the seroprevalence of HSV-1 has declined in developed countries, while that of HSV-2 has increased. Complications after the disease may be more frequent in HSV-2 infections without preceding HSV-1 infection .\n\n【18】In an earlier study, during a 1-year follow-up, recurrent disease developed in 18% of patients with HSV-2 DNA in CSF . In our study, during the 10-year follow-up, the percentage of case-patients with recurrent disease was 27% of all HSV-2 DNA-positive cases.\n\n【19】In RLM, all episodes are most likely caused by the same etiologic agent. However, HSV-2 DNA is not always found in CSF, despite earlier HSV-2 DNA–positive episodes. The viral load and leukocyte counts reach higher levels during the first episode of meningitis compared with recurrent cases . False-negative results may be due to a lower viral load or to earlier timing of the CSF sample in recurrent episodes . In our study, most PCR-positive samples were taken 2–5 days after the onset of acute symptoms, which is considered to be optimal timing. Clinical symptoms were alleviated in recurrent episodes, probably because of milder inflammatory changes in the CNS and lower viral load, which may result in PCR becoming less sensitive in diagnosis.\n\n【20】HSV-2–associated RLM is more common than previously reported. Prophylactic antiviral therapy may have decreased the incidence of recurrences but was not universally effective. In case-patients with frequent relapses, the recovery was prolonged, and residual symptoms were common.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "62dcbac1-6d94-432c-93ba-bc8d90475f3f", "title": "Possible Role of Rickettsia felis in Acute Febrile Illness among Children in Gabon", "text": "【0】Possible Role of Rickettsia felis in Acute Febrile Illness among Children in Gabon\nOver the past decade, reported cases of malaria and associated deaths have declined in Africa . This decrease has led to a search for other causes of fever in Africa, where unexplained febrile illnesses are one of the major health problems. In some sub-Saharan Africa countries, malaria treatments are still administered without a biologic diagnosis. For example, an assessment of complicated malaria and other severe febrile illness cases in a pediatric ward in Libreville, Gabon, showed that 43.5% of the children who received an antimalarial treatment had microscopy test results negative for malaria .\n\n【1】Other studies have shown that, in addition to malaria, other bacterial infections are a major cause of fever in Africa . _Staphylococcus aureus_ , _Streptococcus pneumoniae_ , nontyphoidal _Salmonella_ spp. _Klebsiella pneumoniae_ , and _Escherichia coli_ are the bacteria most often detected in sub-Saharan Africa by the culture method . The use of molecular tools has enabled the identification of the following fastidious bacteria as a cause of unexplained fevers in Africa: _Rickettsia_ spp. including _R. felis_ ; _Coxiella burnetii_ ; _Tropheryma whipplei_ ; and _Borrelia_ spp. However, the epidemiology of many fastidious bacteria, such as _R. felis_ , remains poorly understood. In rural areas of Senegal, the prevalence of _R. felis_ was generally higher (%–24%) than that in urban areas of sub-Saharan African, such as Franceville, Gabon (%) .\n\n【2】_R. felis_ is a gram-negative bacterium belonging to the spotted fever group of _Rickettsia_ spp. In Gabon, the bacterium has been reported in arthropods, including _Ctenocephalides felis_ cat fleas  and _Aedes albopictus_ mosquitoes , and in humans . Similar to many African countries, Gabon has a strong disparity between health care in urban and rural areas; in rural areas, little is known about the epidemiology of infectious diseases. The aim of our study was to evaluate the prevalence of _R. felis_ infection among febrile and afebrile children in rural and urban areas of Gabon and the possible role of _R. felis_ in acute febrile illness.\n\n【3】### Materials and Methods\n\n【4】##### Study Area\n\n【5】Gabon is a central African country located on the equator along the Atlantic Coast . The country has a low coastal plain and hilly inland areas and savannas to the east and south; 80% of Gabon is covered by forest. The tropical climate is hot and humid, and the seasons alternate in precipitation and length: short dry season, long rainy season, long dry season, short rainy season.\n\n【6】##### Study Design and Participants\n\n【7】Patients were recruited at 4 health centers  located in 3 Gabon provinces. One center, the Regional Hospital Center Amissa Bongo of Franceville, is in an urban area of Haut-Ogooué Province. Two centers, the Regional Hospital Center Paul Moukambi of Koulamoutou and the Medical Center of Lastourville, are in semiurban and rural areas, respectively, of Ogooué Lolo Province. The fourth center, the Medical Research Unit of Ngounie in Fougamou, is in a rural area of Ngounié Province.\n\n【8】The National Ethics Committee of Gabon approved this prospective study . Written informed consent forms and questionnaires were completed by parents or legal guardians upon a child’s enrollment in the study.\n\n【9】During April 2013–January 2014, a total of 525 children <15 years of age were recruited for the study; 465 of the children were febrile (axillary temperature >37.5°C), and 60 were afebrile (controls). Febrile children were recruited from the pediatric outpatient clinics at the 4 health care centers. The control group was recruited from children who had accompanied their sick parents to the health care centers. Children in the control group had to be free of fever for at least 1 week before study inclusion.\n\n【10】##### Sample Collection and Molecular Analysis\n\n【11】Molecular analyses were performed on DNA extracts from blood samples from each child; blood smears, serologic testing, and culture were not done. After a child’s parent or legal guardian was interviewed, a blood sample was collected into an EDTA tube. World Health Organization guidelines for blood collection were followed, including guidelines for hand hygiene, use of sterile tubes, and skin disinfection with 70% alcohol. The International Center of Medical Research of Franceville, which has a well-trained staff with expertise in infectious diseases, performed DNA extraction by using the E.Z.N.A. Blood DNA Maxi Kit (Omega Bio-tek, Norcross, GA, USA) according to the manufacturer’s protocol . A total of 150 μL of DNA extract was obtained from each sample. The extracts were stored at −20°C before being sent on ice to URMITE (Unité de Recherche sur les Maladies Infectieuses et Tropicales, Marseille, France) for molecular analyses.\n\n【12】Specific quantitative PCR (qPCR) was performed by using a CFX96 Touch Real-Time PCR Detection System (Bio-Rad Laboratories, Marnes-la-Coquette, France). qPCR Master Mix (Eurogentec, Liege, Belgium) was prepared according to the manufacturer’s instructions; for each reaction, 15 μL of Master Mix was added to 5 μL of DNA. The quality of extracted DNA and the lack of PCR inhibitors were systematically checked by targeting a housekeeping gene, human β-actin . Positive (R. felis_ DNA) and negative (mix alone) controls were also systematically used for each PCR assay. All samples were screened by _Rickettsia_ spp.–specific qPCR targeting the _gltA_ gene and by _R. felis_ –specific qPCR targeting the _bioB_ , _orfB_ , and _vapB1_ genes  . Samples positive for at least 2 genes were considered positive.\n\n【13】##### Statistical Analysis\n\n【14】Data were analyzed by using Epi Info software version 7.0.8.0 (Centers for Disease Control and Prevention, Atlanta, GA, USA). Mantel-Haenszel χ 2  and Fisher exact tests were used to compare the prevalence between groups (i.e. febrile and afebrile children), seasons, and geographic areas and by the participants’ sex and age. A 2-tailed p value <0.05 was considered statistically significant.\n\n【15】### Results\n\n【16】##### Recruitment\n\n【17】A total of 465 febrile children were recruited from Franceville (n = 80), Koulamoutou (n = 167), Lastourville (n = 155), and Fougamou (n = 63) . However, 55 of these children were excluded from the statistical analysis because their sex and age data were unavailable; 3 of the excluded children were from Franceville, 26 from Koulamoutou, 21 from Lastourville, and 2 from Fougamou. Of the 410 children included in the statistical analysis, 52.7% (/410) were recruited during a rainy season, and 47.3% (/410) were recruited during a dry season. A total of 60 afebrile children were recruited and enrolled in parallel from Franceville (n = 24), Fougamou (n = 20), and Lastourville (n = 16); no afebrile children were enrolled from Koulamoutou. The 410 febrile patients included in the statistical analysis consisted of 212 boys and 198 girls (sex ratio 1.07).\n\n【18】##### _R. felis_ in Febrile and Afebrile Children in Gabon\n\n【19】_R. felis_ DNA was detected in 42 (.2%) of 410 analyzed samples from febrile children . The bacterium was detected significantly more frequently during the rainy season (.3% \\[33/216 samples\\]) than the dry season (.6% \\[9/194 samples\\]; p<0.001). The prevalence among boys (.8% \\[23/212\\]) and girls (.6% \\[19/198\\]) did not differ significantly (p = 0.74). Among febrile children, _R. felis_ prevalence varied by age group: 8.5% (/129 children) among children 0–1 year of age, 15.2% (/105) among children >1– 3 years of age, 11.5% (/87) children >3–5 years of age, 7.7% (/39) among children >5–7 years of age, 6.7% (/30) among children >7–9 years of age, and 0 (/20) among children >9–15 years of age . The prevalence of _R. felis_ among febrile children did not differ substantially by age, but the prevalence did increase progressively from Franceville (.3% \\[1/77\\]) to Koulamoutou (.1% \\[3/141\\]) to Lastourville (.2% \\[15/134\\]) to Fougamou (.7% \\[23/58\\]); however, no adjustments were made when testing these pairs . The prevalence was statistically lower in Franceville than in Lastourville (odds ratio \\[OR\\] 0.1, 95% CI 2.5 × 10 −3  –0.7; p = 0.006), in Franceville than in Fougamou (OR 0.02, 95% CI 5 × 10 −4  –0.1, p<0.001), in Koulamoutou than in Lastourville (OR 0.17, 95% CI 0.03–0.63; p = 0.002), in Koulamoutou than in Fougamou (OR 0.03, 95% CI 6 × 10 −3  –0.12; p<0.001), and in Lastourville than in Fougamou (OR 0.19, 95% CI 0.08–0.43; p<0.001). Overall, the prevalence of _R. felis_ among febrile children was significantly higher in the rural areas (Lastourville and Fougamou; 19.8% \\[38/192 children\\]) than in the urban area (Franceville; 1.3% \\[1/77 children\\]; p<0.001). Among the 60 afebrile children, only 2 (.3%; both girls) were positive for _R. felis_ ; the girls were 1 and 3 years of age and were from Fougamou and Franceville, respectively. In all, _R. felis_ DNA was detected in 10.2% (/410) of febrile children and in 3.3% (/60) of afebrile children; this difference was not significant (p = 0.09).\n\n【20】##### _R. felis_ at Each Sampling Location\n\n【21】##### Fougamou, Ngounié Province\n\n【22】_R. felis_ DNA was detected in 23 (.7%) of 58 febrile children at this rural location . Prevalence during the rainy season (.7% \\[19/39 children\\]) was higher than that during the dry season (.1% \\[4/19 children\\]; p = 0.05). _R. felis_ prevalence among boys (% \\[15/30\\]) and girls (.6% \\[8/28\\]) did not differ significantly (p = 0.11). _R. felis_ prevalence also did not differ significantly by age, even though prevalence was higher among 1- to 3-year-old children . Overall, _R. felis_ DNA was detected significantly more frequently in febrile (.7% \\[23/58\\]) than afebrile (.0% \\[1/20\\]) children (p = 0.004).\n\n【23】##### Lastourville, Ogooué Lolo Province\n\n【24】_R. felis_ DNA was detected in 15 (.2%) of 134 febrile children in this rural location . _R. felis_ prevalence during the rainy season (.8% \\[12/76 children\\]) was higher than that during the dry season (.2% \\[3/58 children\\]), but the difference was not statistically significant (p = 0.05). Prevalence among boys (.6% \\[6/79\\]) and girls (.4% \\[9/55\\]) did not differ significantly (p = 0.17). Prevalence also did not differ significantly by age . _R. felis_ DNA was detected in 15 (.2%) of 134 febrile children and in 0 of 16 afebrile children; this difference was not statistically significant (p = 0.3).\n\n【25】##### Koulamoutou, Ogooué Lolo Province\n\n【26】_R. felis_ DNA was detected in 3 (.1%) of 141 febrile children in this semiurban location . Prevalence during the rainy season (.9% \\[2/68 children\\]) was higher than that during the dry season (.4 \\[1/73 children\\]; p = 0.6). _R. felis_ DNA was detected in 1 (.6%) of 63 boys and in 2 (.6%) of 78 girls (p < 1); 2 of these children were 4 years of age, and 1 was 5 years of age. As stated above, no afebrile children were enrolled from Koulamoutou.\n\n【27】##### Franceville, Haut-Ogooué Province\n\n【28】_R. felis_ DNA was detected in 1 (.3%) of 77 febrile children and in 1 (.2%) of 24 afebrile children (p = 0.3) in this urban area . The infected febrile child was a 2-year-old boy, and the infected afebrile child was a 3-year-old girl; both children became infected during the dry season.\n\n【29】### Discussion\n\n【30】The lack of molecular tools in many health centers in countries in sub-Saharan Africa limits the management of all febrile illnesses in these areas. In this study, we used molecular tools (PCR assays) to assess the prevalence of _R. felis_ in blood specimens from febrile and afebrile children from rural, semiurban, and urban areas of Gabon. One of the most frequent pitfalls of PCR assays is the contamination of samples, which can occur any time during or after collection of the samples, including during their use in the laboratory. Over the years, the URMITE laboratory has developed strategies and applied rigorous procedures to prevent and detect contamination . For example, we require that 2 different PCR assays show positive results before we conclude that a sample is positive .\n\n【31】In this study, we used a _Rickettsia_ spp.–specific qPCR targeting the highly conservative _gltA_ gene and an _R. felis_ –specific qPCR targeting the _bioB_ , _orfB_ , and v _apB1_ genes. The _Rickettsia_ spp.–specific assay can amplify almost all _Rickettsia_ spp. including _R. conorii_ and _R. rickettsii_ , but it is less sensitive than the _R. felis_ –specific assay. When a sample in our study was positive by the _Rickettsia_ spp.–specific qPCR, it was also positive by the _R. felis_ qPCR. Several samples had positive results for only 2 of the 3 genes targeted by the _R. felis_ –specific qPCR. This discrepancy might be explained by possible genetic diversity of the _R. felis_ strains. The 3 genes used for the _R. felis_ –specific assay are not all conservative, so it is possible that some strains have a certain degree of genetic diversity that may occasionally cause false-negative PCR results. Our results were also systematically validated by using rigorous criteria. To check the quality of each PCR run, we used negative controls (i.e. PCR mix without template) with every tenth sample, and we used 2 positive controls (R. felis_ DNA) per run. In addition, we required that all negative and positive controls be systematically correct (i.e. negative and positive, respectively) before validating each PCR run, and a sample was not considered _R. felis_ –positive unless confirmed by at least 2 of the 4 sequences of targeted DNA. Thus, we consider our results to be valid. The fact that _R. felis_ DNA has not been detected in samples (from febrile patients in France and Tunisia) previously analyzed in our laboratory  provides further support of our team’s ability to prevent and detect contamination during PCR runs. Furthermore, it has been reported that the prevalence of _R. felis_ is low in northern Africa countries (France, Algeria, Morocco, Tunisia) and increases in southern Africa countries (Mali, Senegal, Gabon) .\n\n【32】Consistent with our previous findings from Franceville in Haut-Ogooué Province , our findings from this study confirmed the presence of _R. felis_ bacteremia in febrile children in Gabon and showed that the prevalence of infection was higher in rural than urban and semiurban areas. This fastidious bacterium was previously found in arthropods in Franceville , including the cat flea, _C. felis_ . The bacterium was also detected in _A. albopictus_ mosquitoes in Libreville in Estuaire Province . Data from our study also confirm the presence of _R. felis_ in children in Ogooué lolo and Ngounié Provinces. Therefore, _R. felis_ is widespread in Gabon, and its prevalence should be assessed in other areas of the country.\n\n【33】Common microorganisms involved in bacteremia (S. aureus_ , _Streptococcus pyogenes_ , _E. coli_ , _K. pneumoniae_ , _Salmonella_ spp. and _S. pneumoniae_ ) were previously assessed in Gabon by using standard culture methods , but the prevalence of fastidious bacteria, which are mainly detected by using molecular techniques, was not studied. There is a need to include these sensitive methods in diagnostic determinations. Our findings, plus those from studies in Senegal , Mali , Kenya , Ethiopia , Cameroon , Democratic Republic of the Congo , Ivory Coast (Côte d’Ivoire) , and Zimbabwe , show that _R. felis_ is widespread in sub-Saharan Africa countries . However, its prevalence changes according to the season, year, area (rural, urban, and semiurban), country, and age of those infected.\n\n【34】A comparison of our findings with previously reported data showed that _R. felis_ prevalence among febrile children in Franceville decreased from 10% in 2012  to 1.3% in 2014. In addition, the variation in the _R. felis_ prevalence between urban (.3%) and rural (.7%) areas of Gabon showed that _R. felis_ is unequally distributed in the country. Differences in the prevalence of a possible vector or reservoir, or both, and disparate health care–associated conditions, including environmental conditions, poverty, and the availability and quality of health care facilities, between rural and urban areas may influence the distribution of _R. felis_ in Gabon; however, these factors have not been determined. An increased prevalence of _R. felis_ was observed during the rainy season. This same finding was described in Senegal, where the prevalence of infection in the rural areas was 24 times higher than that in urban areas of Algeria . Together, the finding suggests that _R. felis_ is more prevalent in rural areas and during the rainy season in sub-Saharan Africa.\n\n【35】_R. felis_ was previously found in _C. felis_ fleas from a pet monkey in Gabon, but the data concerned only 1 region, Franceville . _R. felis_ has been reported to be absent from _C. felis_ fleas in rural areas of Senegal, where _R. felis_ is common . The factors explaining the spread of _R. felis_ in Gabon should be evaluated in further studies. Although _R. felis_ is widespread in Africa and unequally distributed in Gabon and Senegal , its prevalence varies by country: 15.0% in Senegal , 3.0% in Mali , and 7.2% in Kenya . In our study, _R. felis_ was mainly detected in young febrile children <5 years of age (primarily in those 1–3 years of age). In a rural area of Senegal (Dielmo and Ndiop), the incidence of _R. felis_ infection has also been reported to be higher (reaching 36%) among febrile children 1–3 years of age, but the incidence was lower (.1%) in persons >15 years of age . It has been reported that the seroprevalence of _R. felis_ increases with age in areas where the bacterium is endemic . This increase is probably due to exposure to the bacterium during the course of a lifetime, leading to protection by a progressive development of immunity against this bacterium in adults.\n\n【36】Most of the fever-associated studies conducted in Africa failed to use a control group of afebrile persons. Consequently, when a pathogen was detected in febrile patients, it was systematically and automatically considered as the cause of fever . In some cases, we have also observed a mistake in methodology: data comparisons were performed between samples from afebrile persons in an occidental area and from febrile persons in Africa . The epidemiology of microorganisms depends on the studied areas, and the positive predictive value of a disease depends on its symptoms and epidemiology. For example, _Plasmodium falciparum_ , the primary agent of malaria, is commonly detected in blood specimens from apparently healthy, afebrile persons in Sub-Saharan Africa; prevalence can reach 20% in Ethiopia and 32% in Senegal . Respiratory viruses, including influenza virus, have also been found in 12% of nasopharyngeal samples of asymptomatic Hajj pilgrims . More recently in Tanzania, the prevalence of _S. pneumoniae_ DNA was less frequently detected in febrile (.1%) than afebrile (.3%) persons . Thus, these examples show that even well-known pathogens may be detected in blood or respiratory secretions of afebrile persons. The inclusion of control groups of participants in studies is indispensable to a better understanding of infectious diseases; the use of controls has shown the existence of carriers of well-known pathogens, emphasizing that it is not easy to interpret data about the potential pathogenic role of a microorganism. The finding of _R. felis_ in febrile versus afebrile persons is not well characterized. The overall prevalence of _R. felis_ in Gabon was higher in febrile (.2%) than afebrile (.3%) children, but the difference was not statistically significant (p = 0.09). Of more interest, in rural Fougamou, the difference in prevalence was significantly higher in similarly aged children with and without fever (.7% vs. 5.0%, respectively; p = 0.004). Therefore, the presence of _R. felis_ in febrile and afebrile persons should not exclude that this bacterium is a cause of fever in sub-Saharan Africa.\n\n【37】In 2010, independent research teams detected _R. felis_ in blood specimens from febrile patients in 2 different areas of Africa (eastern and western) . In other studies, these teams confirmed and extended the preliminary data: 1 team showed that the presence of _R. felis_ was 2.2 times higher in blood specimens from febrile persons compared with afebrile persons in Kenya , and the other team showed that the prevalence of _R. felis_ was significantly higher in febrile (.0%) than afebrile (.0%) persons in Senegal . In Senegal, an 8-month-old febrile girl was cured of _R. felis_ infection after treatment with doxycycline . The presence of _R. felis_ has also been observed in blood specimens from febrile patients in Asia . The higher prevalence of _R. felis_ among febrile persons compared with healthy persons in our study led us to suspect that this microorganism plays the role of pathogen. However, the presence of the microorganism may be a cofactor or the cause of a previous event not yet determined. Another hypothesis would be that blood specimens may be contaminated by surface bacteria, including _R. felis_ , which has been detected on the skin of healthy persons in Senegal .\n\n【38】In summary, the _R. felis_ bacterium is widespread in Gabon, but it primarily occurs in rural areas and is most prominent during the rainy season. _R. felis_ is also more prevalent among febrile than afebrile children in rural areas of Gabon. More studies will help to better understand the pathogenic role of _R. felis_ in this part of the world.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e92f0853-877c-4b23-905d-10be60f3b069", "title": "Health Promotion Interventions for Low-Income Californians Through Medi-Cal Managed Care Plans, 2012", "text": "【0】Health Promotion Interventions for Low-Income Californians Through Medi-Cal Managed Care Plans, 2012\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Prevention is the most cost-effective approach to promote population health, yet little is known about the delivery of health promotion interventions in the nation’s largest Medicaid program, Medi-Cal. The purpose of this study was to inventory health promotion interventions delivered through Medi-Cal Managed Care Plans; identify attributes of the interventions that plans judged to have the greatest impact on their members; and determine the extent to which the plans refer members to community assistance programs and sponsor health-promoting community activities.\n\n【3】**Methods**\n\n【4】The lead health educator from each managed care plan was asked to complete a 190-item online survey in January 2013; 20 of 21 managed care plans responded. Survey data on the health promotion interventions with the greatest impact were grouped according to intervention attributes and measures of effectiveness; quantitative data were analyzed using descriptive statistics.\n\n【5】**Results**\n\n【6】Health promotion interventions judged to have the greatest impact on Medi-Cal members were delivered in various ways; educational materials, one-on-one education, and group classes were delivered most frequently. Behavior change, knowledge gain, and improved disease management were cited most often as measures of effectiveness. Across all interventions, median educational hours were limited (.4 h), and median Medi-Cal member participation was low (members per intervention). Most interventions with greatest impact (of 137 \\[88%\\]) focused on tertiary prevention. There were mixed results in referring members to community assistance programs and investing in community activities.\n\n【7】**Conclusion**\n\n【8】Managed care plans have many opportunities to more effectively deliver health promotion interventions. Establishing measurable, evidence-based, consensus standards for such programs could facilitate improved delivery of these services.\n\n【9】Introduction\n------------\n\n【10】Four modifiable behaviors — lack of physical activity, poor nutrition, tobacco use, and alcohol abuse — are the major risk factors for a substantial portion of chronic disease morbidity and mortality . Preventing disease or its progression is the most cost-effective and practical way to promote population health . Studies, however, indicate variable delivery and suboptimal use of preventive services, particularly among low-income populations, including those enrolled in Medicaid and the Children’s Health Insurance Program (CHIP) .\n\n【11】The California Department of Health Care Services (DHCS) administers Medi-Cal, which is the largest Medicaid program in the United States. Eighty percent of the approximately 12 million members are enrolled in some form of Medi-Cal Managed Care Plan (MCP). The MCPs are contracted by DHCS to provide a range of medically necessary diagnostic and treatment services. These include clinical preventive services that are informed by the United States Preventive Services Task Force (USPSTF) and health promotion interventions (HPIs) designed to achieve behavior change and positive health outcomes . Despite contract requirements, little is known about the characteristics and effectiveness of HPIs delivered to Medi-Cal members.\n\n【12】As part of a departmentwide quality improvement initiative, the objectives of this study were to 1) inventory HPIs offered to Medi-Cal members in the areas of healthful eating, physical activity, alcohol and drug abuse prevention, breastfeeding, asthma management, and prevention and management of cardiovascular disease (CVD), type 2 diabetes, and overweight/obesity; 2) identify attributes of HPIs that MCPs judged to have the greatest impact on their Medi-Cal members; and 3) determine the extent to which MCPs referred Medi-Cal members to community assistance programs and sponsored health-promoting community activities.\n\n【13】Methods\n-------\n\n【14】This was a cross-sectional, descriptive study consisting of a survey in January 2013 of the lead health educator from each of the 21 contracted MCPs in the Medi-Cal system. Lead health educators were identified through a DHCS database and asked via email and telephone to complete the survey. In alignment with DHCS contract requirements, lead health educators have authority and oversight for the implementation of HPIs in their MCPs. Lead health educators also have primary responsibility for the full portfolio of health promotion programs delivered to their MCP populations, and those participating in this study had at least a master’s degree in public health, health education, or a related field.\n\n【15】### Survey development\n\n【16】A 190-item survey instrument was developed from October through December 2012. Each health behavior (healthful eating, physical activity, alcohol abuse prevention, and breastfeeding) was selected for the survey because it is an important contributor to reducing the risk of morbidity and mortality . Each disease (obesity, CVD, and type 2 diabetes) was included because it is a common, costly, and largely preventable chronic condition . Two other topics — drug abuse prevention and asthma management — were included because the MCP medical directors and health educators requested their inclusion during survey development. In addition, MCP contracts require the implementation of risk reduction, healthful lifestyle, health condition management, and self-care interventions covering all topics in this study as well as others, such as injury prevention, prevention of sexually transmitted diseases, prenatal care, and more .\n\n【17】Originally, we wanted to include smoking cessation in the assessment, but it was the subject of a survey in January 2012 , and plans were under way to conduct other smoking cessation–related surveys of MCPs. Thus, we excluded smoking cessation from the survey to enable the assessment of other health topics.\n\n【18】A draft survey was reviewed by 4 population health experts, who compared the questions with the study aims to establish face validity. The survey instrument was also reviewed by MCP medical directors, who suggested no major changes, and by the 21 lead health educators to assess ease of use, comprehension, readability, and inclusion of major health topics. Minor semantic and formatting changes were made on the basis of their feedback.\n\n【19】### Survey measures\n\n【20】The survey contained detailed instructions and a mix of open- and close-ended questions with multiple response categories. An “other, please specify” response was included in all questions with multiple responses (survey available upon request).\n\n【21】An initial set of questions asked respondents to describe the administrative oversight of their MCP health education system. The lead health educators were asked to provide their contact information, title, and credentials, and a brief description of their health education systems, and to specify the number of full-time equivalents dedicated to health education. The next section asked for general information about the MCPs’ interventions, including information on how content is delivered and how DHCS could complement their efforts. They were also asked to provide the name of each HPI offered to Medi-Cal members by behavior and disease category. The bulk of the survey, organized by behavior and disease, then focused on interventions that the MCPs judged to have the greatest positive impact on their Medi-Cal members (hereinafter referred to as greatest impact health promotion interventions \\[GIHPIs\\]). For each GIHPI from January through December 2012, the survey respondents were asked to state the intervention’s goals, describe the intervention, specify the number of hours of education provided and the number of Medi-Cal members reached, explain how effectiveness was assessed and measured, and document outcomes among participants. The intent of these questions was to capture data on what the MCPs thought were the most effective HPIs being delivered to Medi-Cal members during the study period. No information of this kind was previously available.\n\n【22】The final set of questions asked whether the MCPs referred Medi-Cal members to community assistance programs, such as California’s Supplemental Nutrition Assistance Program (known as CalFresh), the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC), Temporary Assistance for Needy Families, housing or utility assistance, or education and job training programs, among others, to address some of the social determinants of health . Respondents were also asked whether their health plan sponsored activities to foster healthy communities, such as physical activity events, food pantries, farmers markets, or community gardens, among others.\n\n【23】Health educators were given 4 weeks to complete the survey, and 20 of the 21 MCPs submitted survey responses. Repeated attempts to obtain the one unanswered survey were unsuccessful.\n\n【24】### Statistical analysis\n\n【25】Online survey responses were downloaded into an Excel file and exported into SAS/STAT, version 9.3 (SAS Institute Inc). To protect privacy, all lead health educator and MCP names were removed from the data set before analysis and only aggregate analyses were conducted.\n\n【26】Data analysis, conducted from August 2013 through June 2014, consisted of 2 components. First, open-ended questions on program descriptions and measures of effectiveness of the GIHPIs were grouped into a list of common attributes by 2 reviewers (D.R.B. and N.D.K.). One MCP provided program descriptions that were unclear and could not be interpreted across all behavior and disease categories; these descriptions were excluded from the analysis. Seven descriptions of GIHPIs were excluded because of insufficient data. The reviewers compared their lists and agreed to a final list of attributes. Each reviewer independently assigned selected attributes to each program description and measure of effectiveness, and then the reviewers compared and discussed their final assignments. Kappa statistics were used to measure interrater reliability (range, 0.93–0.99). Second, the number of MCPs reporting each attribute by behavior and disease category was tallied. Descriptive statistics, which included frequencies, proportions, medians, and ranges, were used to analyze all remaining quantitative data. The data set did not include missing values after the exclusions were applied.\n\n【27】Results\n-------\n\n【28】The 20 MCPs varied by number of counties served (range, 1–11; median, 1); number of total patients enrolled (median, 197,500); number (range, 30,415–1,004,062; median, 141,782) and percentage (range, <1%–100%; median, 93%) of Medi-Cal members enrolled; and number of full-time equivalent health educators employed (range, 1–45; median, 2).\n\n【29】The number of HPIs (n = 194) and GIHPIs (n = 137) delivered by MCPs varied by behavior and disease category . All MCPs provided healthful eating and physical activity interventions, and only 11 offered CVD management programs or alcohol abuse prevention programs. Eighteen MCPs offered high-impact healthful eating interventions, and only 4 offered high-impact alcohol abuse prevention programs.\n\n【30】Educational materials, one-on-one education, and group classes were the 3 most frequently cited delivery methods for GIHPIs ; 99 of 137 (%) GIHPIs had 2 or more attributes.\n\n【31】Most GIHPIs were aimed at tertiary prevention (of 137 \\[88%\\]), followed by primary prevention (of 137 \\[32%\\]) and secondary prevention (of 137 \\[26%\\]). The number of minutes or hours of education provided by the interventions varied (range, 15 min–48 h); the median was 2.4 hours. The number of Medi-Cal members who participated in the interventions also varied (range, 0–44,289); the median number of members reached by an intervention was 265.\n\n【32】Of the 137 GIHPIs, 86 (%) described how effectiveness was measured . The 3 most frequently cited measures across all categories were measures of behavior change (eg, changes in dietary intake or physical activity); knowledge; and improved disease management, demonstrated by changes in clinical measures or laboratory values. According to respondents who provided further details about their method of measurement for these 3 measures, 35 of 45 (%) GIPHIs were evaluated using pre-assessments and post-assessments involving surveys, laboratory tests, or other clinical metrics.\n\n【33】When asked how DHCS could complement the MCPs’ health promotion efforts, 11 of 20 (%) MCPs suggested that DHCS provide best practice guidelines, share effective interventions, and specify the level of interventions required to meet contractual requirements. In addition, 5 of 20 (%) MCPs recommended the provision of materials that are culturally, linguistically, and educationally appropriate for Medi-Cal members.\n\n【34】Fourteen or more MCPs reported referring Medi-Cal members to food and nutrition assistance, shelter, utilities, or financial support services . Seven or fewer MCPs cited referrals to education, employment, childcare assistance, or the 211 telephone line.\n\n【35】Twelve MCPs reported sponsoring physical activity events, such as cycling, walking, and running events. Five sponsored health fairs and food pantries, 4 invested in farmers markets, 2 supported community and school gardens, and 3 did not sponsor community activities.\n\n【36】Discussion\n----------\n\n【37】The delivery of HPIs and GIHPIs across behavior and disease categories varied in our study. The variability of HPIs was surprising, given that each MCP contract calls for the implementation of educational interventions in risk reduction, healthful lifestyle, health condition management, and self-care and also specifies the risk factors and diseases covered by our investigation . Several factors may explain the differences between contract expectations and practice. First, the MCPs may have emphasized certain behavior and disease topics because of local needs and the special characteristics of the populations served, although these factors do not fully explain the degree of variation found. Second, use of a standardized, valid health risk appraisal has not been required of MCPs, making it difficult to judge risk and tailor intervention delivery accordingly. Third, the lack of a performance monitoring and evaluation system to track HPIs confounds accountability. To advance systemwide health promotion, monitoring and evaluation are essential. It was also surprising that some plans did not report GIHPIs, possibly indicating that these MCPs did not judge any of their interventions to be worthy of greatest impact status.\n\n【38】Multiple attributes were identified for the GIHPIs. Educational materials, one-on-one education, and group classes were the top 3 delivery modes. Although helpful in improving knowledge, these approaches may not address the complex interplay of determinants that shape near- and long-term health behaviors, including self-efficacy, social support, organizational policies, and the environment in which people live . Of the 137 GIHPIs, 86 (%) described how effectiveness was measured; behavior change, knowledge gain, and improved disease management were the most commonly used measures. Most GIHPIs focused on tertiary prevention, median intervention hours were limited, and median Medi-Cal member participation was low. These findings are consistent with those of another assessment of coverage, utilization, and evaluation of health-promoting programs among California’s commercial health plans . The study found that most health maintenance organizations (HMOs) used brochures to address health issues and offered free educational classes. Outcome measures used by HMOs to evaluate the impact of their programs included member satisfaction surveys, participation rates, behavior change, and changes in health status. They also found low patient participation rates in health plan–sponsored health promotion programs.\n\n【39】Although health education programs are important, they often have limited impact on health behaviors or long-term health status by themselves. They are more effective when coupled with interventions that address the many determinants of individual and population health . A growing body of research shows that the health care delivery system’s focus on treating medical conditions typically overshadows and neglects the significant role that social needs — such as food security, safe housing, and employment assistance — play in health, especially among vulnerable populations . Our study found mixed results in MCP efforts to support or improve selected determinants of health and invest in health-promoting community activities. Most MCPs referred Medi-Cal members to food assistance, shelter, utilities, and financial support services; referrals to education and job-related resources were limited. Most MCPs invested in some type of physical activity event, and a few sponsored health fairs, food pantries, farmers markets, or community and school gardens. Several MCPs did not invest in any community activities. The MCPs could benefit from greater application of the USPSTF guidelines, _The Guide to Community Preventive Services_ , and other sources of information on effective HPIs to improve the delivery of whole-person care .\n\n【40】To our knowledge, ours is the first study to describe the characteristics of HPIs conducted in a state Medicaid program. This work is timely, given that the Affordable Care Act offers opportunities to increase access to preventive services through the expansion of Medicaid and through several provisions in the law that provide incentives to states to increase access to Medicaid- and CHIP-covered preventive services . The results of this study are important for population health in California; nearly 10 million Californians are receiving full-scope Medi-Cal services from MCPs. Because federal law and policy determined by the Centers for Medicare and Medicaid Services provide oversight for all Medicaid MCPs, our results suggest areas in which health promotion and disease prevention and management activities could be enhanced in Medicaid plans outside of California.\n\n【41】Our study has several limitations. First, responses to the questions were self-reported and subject to possible comprehension, memory, and other reporting errors. Second, the MCPs are under contract with DHCS; therefore, responses may reflect a social desirability bias. Third, given the exploratory nature of this study, the GIHPIs were identified independently by each MCP without the use of standardized criteria. Fourth, some behavior and disease categories, such as cancer, were excluded from the assessment to maintain a reasonable number of survey questions. Despite these limitations, the overall patterns of practice in health promotion were striking and consistent with a similarly designed study of commercial health plans in California in the late 1990s . The results also provide a real-world snapshot of health promotion services provided to a large, well-defined, low-income population.\n\n【42】The findings of this study indicate that substantial and immediate opportunities exist to improve the delivery and effectiveness of health promotion and disease prevention and management services for Medi-Cal members. Improvements are critical, given that populations of low socioeconomic status have higher morbidity and mortality rates than the general population . Establishing an evidence-based, measurable, consensus standard of HPIs and leveraging partnerships with state and local health departments with expertise in health promotion may materially improve service delivery among those most in need of sound preventive services. This call to action is consistent with 11 of 20 (%) MCPs, which recommended that DHCS provide best practice guidelines, share effective interventions, and specify the level of health promotion and disease prevention and management services required to meet contractual agreements.\n\n【43】To advance health promotion in Medi-Cal, achieve outcomes consistent with high-performing systems, such as Kaiser Permanente and the reengineered Veterans Affairs (VA) Health Care System , and inform the broader dialogue about preventive care improvements in Medicaid, DHCS plans to determine 1) how the MCPs assess health risks among Medi-Cal members and how risk-related data are used to inform intervention delivery; 2) the best approach to set quality improvement targets and accountability systems, starting with the leading causes of preventable mortality and illness, to ensure that evidence-based interventions are delivered to Medi-Cal members in a timely, prudent, and effective manner; 3) methods to optimize the delivery of the USPSTF A and B recommendations and other evidence-informed best practice interventions ; 4) opportunities to ensure that health care and community prevention efforts are available, integrated, mutually reinforcing, and address multiple determinants of health; and 5) methods to implement a monitoring system for tracking the delivery and performance of HPIs. Such a system could help decision makers deploy resources to the most effective programs while curtailing ineffective programs.\n\n【44】Studies are needed to learn how methods associated with the consistent delivery of preventive services in highly organized delivery systems (eg, the VA Health Care System, Kaiser Permanente, Group Health Cooperative) can be applied more effectively in Medicaid health plans, which typically contract with a diverse network that might include federally qualified health centers, independent practice associations, large group practices, and specialty groups. On a broader scale, to achieve the prevention and health promotion and disease prevention and management targets outlined in the National Strategy for Quality Improvement in Health Care and the National Prevention Strategy, additional applied research is needed to understand how to effect systemwide changes that advance population health in Medicaid .\n\n【45】Tables\n------\n\n【46】#####  Table 1. Attributes of Health Promotion Interventions With the Greatest Impact, by Behavior and Disease Category, Medi-Cal Managed Care Plans (n = 20), California, 2012 a,b  \n\n| Behavior or Disease Category  | Attribute |\n| --- | --- |\n| Educational Materials | One-on-One Education | Group Classes | Referral to Clinical Resources | Health Risk Appraisal and Screening | Disease Management and Self-Management Tools | Incentives for Members | Referral to Community Resources | Health Coaching | Case Management | Resources for Providers | Incentives for Providers |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Healthful eating (n = 18) | 6 | 5 | 9 | 7 | 7 | 2 | 3 | 1 | 2 | 0 | 1 | 0 |\n| Physical activity (n = 15) | 6 | 5 | 3 | 5 | 3 | 2 | 5 | 3 | 4 | 0 | 0 | 0 |\n| Alcohol abuse prevention (n = 4) | 3 | 1 | 1 | 0 | 0 | 0 | 0 | 2 | 0 | 1 | 0 | 0 |\n| Drug abuse prevention (n = 5) | 3 | 1 | 1 | 0 | 1 | 1 | 0 | 3 | 0 | 0 | 0 | 0 |\n| Breastfeeding (n = 12) | 8 | 7 | 1 | 4 | 0 | 1 | 1 | 2 | 0 | 2 | 2 | 0 |\n| Overweight/obesity prevention (n = 17) | 6 | 5 | 10 | 8 | 6 | 2 | 4 | 3 | 3 | 0 | 0 | 0 |\n| CVD prevention (n = 9) | 3 | 3 | 3 | 2 | 3 | 1 | 1 | 0 | 1 | 2 | 0 | 1 |\n| Type 2 diabetes prevention (n = 12) | 4 | 6 | 6 | 1 | 4 | 6 | 1 | 0 | 0 | 1 | 0 | 1 |\n| Weight management (n = 14) | 5 | 4 | 5 | 7 | 4 | 5 | 3 | 3 | 3 | 0 | 1 | 0 |\n| CVD management (n = 5) | 5 | 1 | 2 | 2 | 2 | 0 | 1 | 0 | 1 | 1 | 0 | 0 |\n| Diabetes management (n = 14) | 6 | 4 | 2 | 4 | 5 | 7 | 2 | 0 | 0 | 1 | 1 | 0 |\n| Asthma management (n = 12) | 3 | 6 | 1 | 1 | 5 | 4 | 1 | 0 | 0 | 5 | 2 | 0 |\n| **Total** | 58 | 48 | 44 | 41 | 40 | 31 | 22 | 17 | 14 | 13 | 7 | 2 |\n\n【48】Abbreviation: CVD, cardiovascular disease.  \na  Source of data: survey of the lead health educator from 20 of 21 contracted managed care plans in the Medi-Cal system.  \nb  When a health promotion intervention applied to more than one behavior and disease category, it was counted and analyzed as a unique entry. For example, if the same intervention was noted in the category of healthful eating and the category of overweight/obesity prevention, it was analyzed and counted in both categories.\n\n【49】#####  Table 2. Measures of Effectiveness Used by Managed Care Plans for Health Promotion Interventions With the Greatest Impact, by Behavior and Disease Category, Medi-Cal Managed Care Plans (n = 20), California, 2012 a  \n\n| Behavior or Disease Category c | Measure b |\n| --- | --- |\n| Behavior Change | Knowledge Gain | Improved Disease Management d | Patient Satisfaction | Participation | No. of Members Receiving Coaching/ Counseling | Health Care Effectiveness Data and Information Set | Attitudes and Self-Efficacy | Hospital Utilization | Change in Body Mass Index | No. of People Screened | Frequency of Clinical Visits and Access to Clinical Services | No. of Materials Requested or Distributed |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Healthful eating (n = 14) | 7 | 5 | 0 | 5 | 1 | 3 | 0 | 1 | 0 | 3 | 0 | 1 | 0 |\n| Physical activity (n = 9) | 5 | 2 | 0 | 4 | 4 | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n| Alcohol abuse prevention e (n = 1) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| Drug abuse prevention (n = 2) | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n| Breastfeeding (n = 8) | 1 | 1 | 0 | 0 | 2 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 2 |\n| Overweight/ obesity prevention (n = 8) | 4 | 2 | 0 | 0 | 3 | 4 | 0 | 3 | 0 | 2 | 1 | 1 | 0 |\n| CVD prevention (n = 6) | 0 | 2 | 2 | 1 | 0 | 0 | 2 | 1 | 2 | 0 | 1 | 0 | 0 |\n| Type 2 diabetes prevention (n = 8) | 1 | 2 | 3 | 1 | 1 | 0 | 3 | 1 | 2 | 0 | 1 | 0 | 0 |\n| Weight management (n = 3) | 1 | 2 | 0 | 0 | 1 | 2 | 0 | 1 | 0 | 1 | 1 | 1 | 0 |\n| CVD management (n = 3) | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 |\n| Diabetes management (n = 12) | 1 | 2 | 6 | 2 | 0 | 0 | 3 | 1 | 2 | 0 | 2 | 1 | 1 |\n| Asthma management (n = 12) | 1 | 1 | 5 | 1 | 1 | 0 | 2 | 1 | 1 | 0 | 0 | 0 | 0 |\n| **Total** | 21 | 19 | 16 | 15 | 14 | 11 | 11 | 10 | 9 | 7 | 6 | 5 | 4 |\n\n【51】Abbreviation: CVD, cardiovascular disease.  \na  Source of data: survey of the lead health educator from 20 of 21 contracted managed care plans in the Medi-Cal system. Not all plans reported using measures of effectiveness.  \nb  The following measures were excluded from the table because of low frequencies: self-reported health status/disease management, medication use/compliance, number of incentives distributed, timeliness of educational material mailings, and educational material comprehension.  \nc  Some managed care plans described multiple measures of effectiveness; therefore, the number of measures may be greater than the number of managed care plans in each behavior or disease category.  \nd  Measured as changes in clinical measures or laboratory values.  \ne  Timeliness of educational material mailings and educational material comprehension were measures of effectiveness for alcohol abuse prevention.\n\n【52】#####  Table 3. Medi-Cal Managed Care Plans (n = 20) Referring Medi-Cal Members to Community Assistance Programs, California, 2012 a  \n\n| Assistance Program | Number (N = 20) |\n| --- | --- |\n| Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) | 18 |\n| Food banks | 17 |\n| Domestic violence shelters | 16 |\n| CalFresh (Supplemental Nutrition Assistance Program) | 16 |\n| Housing assistance | 15 |\n| Homeless shelters | 14 |\n| Temporary Assistance for Needy Families (TANF) | 14 |\n| Utilities assistance (eg, electricity, home heating, telephone service) | 14 |\n| English proficiency programs | 7 |\n| Job training and placement | 6 |\n| Childcare assistance | 6 |\n| Adult education/General Educational Development (GED) test preparation | 4 |\n| Vocational education programs | 4 |\n| 211 Telephone line | 1 |\n| Do not refer to community assistance programs | 1 |\n| No response | 1 |\n\n【54】a  Source of data: survey of the lead health educator from 20 of 21 contracted managed care plans in the Medi-Cal system.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "54b89ec9-5147-4eca-bba1-2311d442e5d4", "title": "New Rural Focus of Plague, Algeria", "text": "【0】New Rural Focus of Plague, Algeria\n**To the Editor:** Plague is a deadly rodent-associated flea-borne zoonosis caused by the bacterium _Yersinia pestis_ . Human plague periodically reemerges in so-called plague foci, as illustrated by the 2003 reemergence of human plague in the Oran area, Algeria . We report emergence of a new plague focus in a remote region of Algeria.\n\n【1】In July 2008, three patients came to Laghouat University Hospital with signs of severe infection and painful, inflamed, enlarged lymph nodes suggestive of buboes. One additional patient became ill with pneumonia and coma after a bubo appeared. The patients were nomads living in a 24-person camp in Thait El Maa in the Laghouat area, 550 km southwest of Algiers . Plague was confirmed by culturing _Y. pestis_ from 1 bubo aspirate. Ten days of oral doxycycline (mg/kg/d) combined with oral rifampin (mg/kg/d) and intramuscular gentamicin (mg/kg/d) cured the patients with bubonic plague, but the patient with pneumonic plague died.\n\n【2】In January 2009, eight individuals of the rodent species _Meriones shawii_ (Shaw’s jird) and 2 _Psamommys obesus_ (fat sand rats) were trapped inside nomads’ tents (H.P. Sherman Traps, Tallahassee, FL, USA). At time of capture, there was a cold wind with blowing sand, and, after visual inspection of the rodents, efforts to recover fleas failed. DNA from the rodents’ spleens was extracted by using the QIAamp Tissue Kit (QIAGEN, Hilden, Germany) at the Medical Entomology Unit Laboratory, Pasteur Institute, Algiers, and subjected to PCR amplification of the plasminogen activator gene (pla_ ) from 6 _M. shawii_ jirds. Negative controls (DNA extracted from uninfected fleas maintained as colonies in Medical Entomology Unit Laboratory was used in the absence of negative animal tissue) remained negative.\n\n【3】After sequencing, the PCR amplicons showed 100% sequence identity with _Y. pestis_ reference sequences. Identification was further confirmed in Marseille, France, by culturing 2 rodent glycerol-negative _Y. pestis_ isolates (Algeria 1 and Algeria 2) and sequencing _pla_ , _caf_ , and _glp_ D genes. The latter sequence was identical to the reference _Y. pestis_ CO92, an Orientalis biotype. Multispacer sequence typing found the following combination: spacer Yp3, type 5; Yp4, 1; Yp5, 1; Yp7, 8; Yp8, 2; Yp9, 2; and Yp10, 1, a pattern that is typical for all Orientalis isolates investigated by this method but does not match the combinations observed for other genotypes. The original spacer Yp7 type 8 ruled out contamination .\n\n【4】National health records indicate that plague foci have been known for decades in Algiers, Kahelia, Aumale, Philippeville, and Oran, where plague reemerged in 2003 after its abence for >50 years . In the Oran outbreak, it was not clear if reemergence resulted from importation through the international port of Oran or from a previously unknown rural focus . The Laghouat area was not previously known as a plague focus, and plague must therefore be regarded as an emerging disease in this region.\n\n【5】No patients with plague reported handling sick animals. Thus, the patients likely acquired plague from rodent flea bites. Because human ectoparasites were not found on the nomads, rodent ectoparasites must have transmitted the disease. It is unlikely plague had been imported into this region; the 2 rodent species from which _Y. pestis_ was recovered are present in the area. To the best of the nomads’ knowledge, there have been no reports of movements of commensal rats or other plague-susceptible rodents into the area near the sites where the patients acquired their illnesses.\n\n【6】We found _Y. pestis_ in _M. shawii_ jirds, a native rodent species living in close contact with human populations. _M. shawii_ jirds have been shown to be a plague-resistant species  and thus are an efficient reservoir for _Y. pestis._ These data verify the presence of a new, rural zoonotic focus of plague. This situation is worrisome because nomads remain in close contact with rodents and fleas and the risk of further outbreaks remains high. In Oran and Laghouat, an Orientalis biotype sharing the same Yp8 and Yp9 spacer sequences was found, but limited multiple spacer typing of Oran strains hampered further comparisons beyond the biotype level .\n\n【7】A plague focus has been recently detected in a neighboring Libyan focus located at the same latitude as the Laghouat area . Our report suggests that extending surveillance to adjacent Libya and Mauritania, which also have natural foci of plague, is necessary. The reasons for emergence of plague in these regions are unknown, but _Y. pestis_ can survive in the soil under laboratory conditions, possibly providing the opportunity for rodents to be infected and promoting reemergence of the disease .\n\n【8】Emergence of plague in an area of Algeria where it had never been reported illustrates the necessity to reinforce surveillance of plague in possible rodent hosts and their ectoparasites, which are in contact with humans, to prevent emergence and reemergence of this deadly infection. Surveillance should be maintained to monitor this natural focus and potential spread of plague that might occur because of climatic or habitat influences .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "83fbd803-efbd-4aab-858d-b4a56d4e3bed", "title": "Urban–Rural Disparities in Access to Low-Dose Computed Tomography Lung Cancer Screening in Missouri and Illinois", "text": "【0】Urban–Rural Disparities in Access to Low-Dose Computed Tomography Lung Cancer Screening in Missouri and Illinois\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Low-dose computed tomography (LDCT) lung cancer screening is recommended for current and former smokers who meet eligibility criteria. Few studies have quantitatively examined disparities in access to LDCT screening. The objective of this study was to examine relationships between 1) rurality, sociodemographic characteristics, and access to LDCT lung cancer screening and 2) screening access and lung cancer mortality.\n\n【3】**Methods**\n\n【4】We used census block group and county-level data from Missouri and Illinois. We defined access to screening as presence of an accredited screening center within 30 miles of residence as of May 2019. We used mixed-effects logistic models for screening access and county-level multiple linear regression models for lung cancer mortality.\n\n【5】**Results**\n\n【6】Approximately 97.6% of metropolitan residents had access to screening, compared with 41.0% of nonmetropolitan residents. After controlling for sociodemographic characteristics, the odds of having access to screening in rural areas were 17% of the odds in metropolitan areas (% CI, 12%–26%). We observed no association between screening access and lung cancer mortality. Southeastern Missouri, a rural and impoverished area, had low levels of screening access, high smoking prevalence, and high lung cancer mortality.\n\n【7】**Conclusion**\n\n【8】Although access to LDCT is lower in rural areas than in urban areas, lung cancer mortality in rural residents is multifactorial and cannot be explained by access alone. Targeted efforts to implement rural LDCT screening could reduce geographic disparities in access, although further research is needed to understand how increased access to screening could affect uptake and rural disparities in lung cancer mortality.\n\n【9】Introduction\n------------\n\n【10】Low-dose computed tomography (LDCT) screening has increased the ability to detect early-stage lung cancer in recent years . The National Lung Screening Trial showed that LDCT screening reduces risk of lung cancer death by up to 20%, compared with chest x-ray . In light of this evidence, the US Preventive Services Task Force (USPSTF) issued a recommendation to provide annual LDCT screening to adults aged 55 to 80 who have at least a 30 pack-year smoking history, currently smoke or quit in the past 15 years, and have no lung cancer symptoms . Medicare subsequently began reimbursing screening of adults aged 55 to 77 . Unique among cancer screenings, LDCT reimbursement is contingent on provision of smoking cessation counseling and shared decision making, both of which are also billable services .\n\n【11】The burden of these requirements on physician practices, along with the high rate (>95%) of false-positive test results , may explain why screening rates are low. Although the number of accredited LDCT centers nationwide increased from an estimated 203 in 2014 to 1,748 in early 2017 , a study of 10 geographically diverse US states found that 12.7% of adults aged 55 to 80 met USPSTF criteria for LDCT screening in 2017, but of these adults, only 12.5% reported receiving screening in the previous year .\n\n【12】Barriers to LDCT screening persist — rural residents nationwide have less access, defined as distance and driving time, to LDCT screening than their urban counterparts . Although more than 95% of adults aged 55 to 79 in 8 northeastern states (Connecticut, Delaware, Maryland, Massachusetts, New Jersey, New York, Pennsylvania, Rhode Island) have access to a screening center within 30 miles (Euclidean distance), the proportion in the Midwest (Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, Wisconsin) is lower and highly variable (%–93%) .\n\n【13】Our investigation focused on Missouri and Illinois, both Midwestern states in the upper Mississippi Delta, a region marked by high cancer mortality . Missouri and Illinois are home to 6.1% of the US population and contain a heterogeneous mix of geographies, from densely populated cities to rural farmland. Both states reflect the nationwide pattern of higher smoking prevalence in rural areas than in urban areas .\n\n【14】The 2 states have significantly different policies on health care and tobacco. Illinois was an early expander of Medicaid under the Affordable Care Act, whereas Missouri was not. The state cigarette tax is more than 15 times higher in Illinois ($2.98/pack) than in Missouri ($0.17/pack) . Demographically, Missouri has a higher proportion of rural residents than the United States as a whole (.6% for Missouri vs 19.3% nationwide), whereas Illinois, at 11.5%, has a lower proportion . A study published in 2018 identified Missouri as a state with moderate access to LDCT screening and high lung cancer mortality and Illinois as a state with high access to screening and moderate mortality .\n\n【15】Given rural–urban differences and the importance of using precise and localized estimates to drive public health priorities , we performed a detailed analysis of screening access in Missouri and Illinois. Efforts to reduce rural–urban disparities in LDCT screening and lung cancer mortality require county-specific information on screening “deserts” and mortality hotspots . As such, the primary objective of this study was to identify locations in Missouri and Illinois that have high lung cancer mortality and/or cigarette smoking rates but low levels of access to LDCT screening; these locations are priority areas for intervention. We built on previous work  by using multilevel, mixed-effects modeling to quantify the association between rurality, sociodemographic characteristics, and access to screening at the census block group level. Additionally, a secondary objective was to conduct an exploratory analysis of the relationship between access to screening and lung cancer mortality.\n\n【16】Methods\n-------\n\n【17】### Data management\n\n【18】We collected and organized data by using methods similar to those of Eberth et al . In May 2019, we obtained addresses of screening centers accredited by the American College of Radiology  and Lung Cancer Alliance (now GO 2  Foundation for Lung Cancer) Screening Centers of Excellence . We compiled addresses for 356 centers in Missouri, Illinois, and all neighboring states (Arkansas, Indiana, Iowa, Kansas, Kentucky, Michigan, Nebraska, Oklahoma, Tennessee, Wisconsin). We collected addresses from neighboring states because patients may cross state lines to reach the nearest center. When multiple screening centers were located on a single hospital campus, we randomly chose 1 center. Additionally, we removed from analysis 1 center in Indiana that was closed. We performed automatic geocoding in ArcGIS Desktop version 10.6 using the USA Geocoding Service (Esri). We used interactive rematch for screening centers that matched equally well to multiple street addresses.\n\n【19】We manually rematched all unmatched centers and centers matched to a zip code rather than a street address (n = 56 centers) by using a Google Maps API . Consistent with the methods of Eberth et al, we constructed a 30-mile planar buffer around each screening center to represent the area in which that center was deemed accessible . A nationwide study comparing driving distance and straight-line distance from all census tracts to the closest hospital found that the 2 measures are highly correlated in the absence of shorelines, mountains, or other physical barriers . Missouri and Illinois contain few such barriers; thus, we felt justified in using a 30-mile straight-line buffer. Hospital “deserts” are defined by the lack of a hospital within a 30-mile radius . Consistent with Eberth et al, we considered a center accessible to residents of census block groups whose centroids lay inside the buffer .\n\n【20】We used these data to calculate the county-wide percentage of residents aged 55 to 79 who have access to LDCT screening within 30 miles. We obtained census block group–level data on age from American Community Survey (ACS) 2013–2017 five-year estimates . Of the available categories, the age group 55 to 79 was the closest option to the recommended screening age range of 55 to 80 .\n\n【21】### Measures\n\n【22】**Screening access measure.** We dichotomized access to LDCT screening at the census block group–level as presence or absence of at least 1 center within 30 miles of the centroid. At the county level, we quantified access by the proportion of adults aged 55 to 79 who lived in a census block group and met this criterion. Because appropriate data on smoking status were unavailable, we assumed that the ratio of adults aged 55 to 79 to LDCT-eligible adults was roughly constant across all census block groups in a county.\n\n【23】**Rurality measures.** We used census tract–level rural–urban commuting area (RUCA) codes to measure rurality . For modeling purposes, we grouped codes 1 to 3 as metropolitan, codes 4 to 6 as micropolitan, and codes 7 to 10 as small town/rural areas. However, because lung cancer mortality data were available only at the county level, we used the National Center for Health Statistics (NCHS) county-level classification  for our exploratory mortality model. NCHS codes range from 1 (large central metro) to 6 (noncore). We used RUCA codes for our main access model because they provide more fine-grained information than NCHS codes on rurality in a census tract and its census block groups.\n\n【24】**Sociodemographic characteristics.** We obtained demographic census block group–level data from ACS 2013–2017 five-year estimates . We defined income as median annual household income (in thousands of dollars), education as percentage of residents aged 25 or older with at least a college degree, and race as the percentage of White residents and the percentage of African American residents.\n\n【25】**Lung cancer and smoking measures.** We obtained county-level, age-adjusted lung and bronchus cancer mortality rates during 2013–2017 from the National Cancer Institute’s Surveillance, Epidemiology, and End Results (SEER) program via SEER\\*Stat software version 8.3.6 . We used mortality rates (per 100,000) for people aged 60 or older. Given the lead-time bias and additional survival time after lung cancer diagnosis, we believed mortality in this age range was most likely to be affected by a screening program for people aged 55 to 80. We suppressed data from 1 county in Missouri because of a small number (<10) of deaths. We obtained data on 2019 adult smoking prevalence from County Health Rankings . We classified adults as smokers if they reported currently smoking every day or most days and having smoked at least 100 cigarettes in their lifetime.\n\n【26】**Map development.** We obtained census block group shapefiles from the Census Bureau  and state-level and county-level shapefiles from Esri . We created categories by rounding quintiles to the nearest 10% for access to screening, nearest 10 per 100,000 for lung cancer mortality, and nearest 0.5% for smoking prevalence. Mortality and smoking quintiles were based on national (rather than bi-state) data, to emphasize how Illinois and Missouri compare with other states. We created maps in ArcGIS Desktop version 10.6 (Esri).\n\n【27】### Statistical analysis\n\n【28】For the first analysis, our outcome of interest was access to screening within 30 miles of the census block group centroid (binary). Predictor variables were rurality as quantified by RUCA codes (main predictor; categorical), income (continuous), education (continuous), and race (continuous). We used multilevel, mixed-effects logistic regression modeling to determine the association between outcome and predictor variables. In this model, the census block group was the unit of analysis. We defined RUCA codes at the census tract level; all other variables were defined at the census block group level.\n\n【29】Our modeling procedure was as follows: first, we considered bivariate logistic models to examine crude associations between screening access and each predictor. We then used the full additive model with all predictor variables (fixed effects) and random intercepts for each state and county. Counties were nested within states. Census tract was not considered a random effect because of the small number of census block groups in some tracts. We then tested models involving interaction terms and random slopes for various predictors. These terms were all nonsignificant and thus not included in the final model. We calculated the odds ratio (OR), 95% CI, and _P_ value associated with each fixed-effect parameter.\n\n【30】Our second, exploratory model used the county as the unit of analysis. We sought to determine the association between access to LDCT screening, defined as the proportion of residents aged 55 to 79 whose census block group of residence is located within 30 miles of a screening center (main predictor), and lung cancer mortality rate in adults aged 60 or older (outcome). Other covariates included adult smoking prevalence, rurality (NCHS code), income, education, race, and state in which the county is located. We used multiple linear regression modeling for this county-level analysis. We defined all variables at the county level, and all variables except NCHS code were continuous. Because only 3 counties in the study area were designated as large central metro (level 1), we performed a sensitivity analysis using a dichotomized rurality variable (levels 1–4 \\[all metro area counties\\] vs levels 5–6 \\[micropolitan and noncore\\]).\n\n【31】For both analyses, all tests were 2-sided and _P_ < .05 was considered significant. We calculated variance inflation factors to assess evidence of multicollinearity. For the main mixed-effects model, we assessed county-level random intercepts for normality. For the multiple regression model, we checked residual plots for normality and constant variance. We performed statistical analyses in R version 3.6.1 (The R Project for Statistical Computing).\n\n【32】Results\n-------\n\n【33】Overall, 91.2% of Illinois residents aged 55 to 79 and 78.3% of their Missouri counterparts were within 30 miles of an LDCT screening center. Areas with low access to screening corresponded roughly to the states’ most rural regions . These areas of low access included central northern Missouri, the Bootheel region in southeastern Missouri, and southern Illinois . LDCT screening centers in Illinois and Missouri were located in census block groups whose residents were more likely than residents in the 2-state region as a whole to identify as White (.6% vs 67.6%) and have at least a college degree (.1% vs 31.8%). Similarly, weighted median income in census block groups containing screening centers was $72,222, compared with $57,750 across all census block groups.  \n\n【34】**  \nMeasures of rurality in Missouri and Illinois and location of low-dose computed tomography screening centers. A, Rural–urban commuting area (RUCA) categories at the census tract level, determined by US Department of Agriculture Economic Research Service . B, National Center for Health Statistics (NCHS) rural–urban classification codes at the county level . Data on screening centers obtained from American College of Radiology  and GO 2  Foundation for Lung Cancer . Shapefiles obtained from ESRI . \n\n【35】**  \nAccess to LDCT lung cancer screening, lung cancer mortality, and smoking prevalence in Missouri and Illinois. A, Percentage of residents aged 55–79 with access to an LDCT lung cancer screening center within 30 miles. B, Lung cancer mortality (deaths per 100,000) among adults aged ≥60. C, Adult smoking prevalence. All maps are at the county level, and categories are based on rounded quintiles. Data obtained from American College of Radiology , GO 2  Foundation for Lung Cancer , Surveillance, Epidemiology, and End Results program , and County Health Rankings . Shapefiles from ESRI . Abbreviation: LDCT, low-dose computed tomography .\n\n【36】Both states had pockets of high lung cancer mortality, although smoking rates were consistently higher in Missouri than in Illinois . Southeastern Missouri had the highest concentration of both lung cancer mortality and adult smokers.\n\n【37】In metropolitan area cores or nearby commuting areas (RUCA codes 1–3), 97.6% of residents had access to LDCT screening, compared with 41.0% of residents in micropolitan or small town/rural areas (codes 4–10). This difference in access was similar across NCHS county-level codes . Furthermore, as rurality increased, we observed higher rates of adult smoking and lung cancer mortality among adults aged 60 or older.\n\n【38】The mixed-effects logistic regression model of access to LDCT screening within a 30-mile radius achieved convergence, and a likelihood ratio test showed that inclusion of random effects significantly improved fit (χ 2  \\= 3417.6; _df_ \\= 2; _P_ < .001). Small town and rural census block groups had significantly lower adjusted odds than metropolitan census block groups of access to screening within a 30-mile radius (OR = 0.17; 95% CI, 0.12–0.26) . Screening access in micropolitan areas was similarly lower than in metropolitan areas (OR = 0.17; 95% CI, 0.10–0.27).\n\n【39】In the county-level models, we found no significant relationship between access to LDCT screening and lung cancer mortality after adjusting for smoking prevalence, rurality, and demographic characteristics (P_ \\= .68) . The variables most strongly associated with lung cancer mortality per 100,000 residents were smoking prevalence (β = 9.7; 95% CI, 4.6 to 14.9), percentage of population aged 25 or older with a college degree (β = −2.7; 95% CI, −1.5 to −3.9), and residence in Missouri (β = −41.2; 95% CI, −68.2 to −14.2). Thus, a 1 percentage-point increase in smoking prevalence was associated with a mortality increase of 9.7 per 100,000 residents, and a 1 percentage-point increase in the fraction of individuals aged 25 or older with a college degree was associated with a decrease of 2.7 per 100,000. Rurality and other variables showed no association, and use of a binary rurality variable (all metropolitan vs micropolitan/noncore) yielded nearly identical results.\n\n【40】Discussion\n----------\n\n【41】Our study examined access to LDCT screening across diverse urban and rural areas, and in communities of varying sociodemographics. The odds of urban populations having access to screening were more than 5 times greater than those of micropolitan or rural counterparts. After adjusting for smoking prevalence and demographic characteristics, we found no evidence that greater access to screening or greater urbanization is associated with lower county-level lung cancer mortality. However, counties with a larger proportion of college-educated residents or lower smoking prevalence tended to have lower lung cancer mortality.\n\n【42】Several studies reported that rural residents have lower access to LDCT screening , and our study confirms those findings. Our study also found that micropolitan areas have no better access than rural areas. Findings from our study reveal a negligible association between access to LDCT screening and lung cancer mortality rates.\n\n【43】Most likely, the observed lack of association between access to screening and mortality was due to the nascent state of LDCT screening and low uptake during the years of mortality data used in our study  . Screening can detect early-stage and slow-growing cancers that would not have otherwise been diagnosed for quite some time. Because lung cancer tends to be diagnosed at late stages with poor survival rates, several years of higher rates of screening may be needed before reduced mortality is seen. The overall delay from screening implementation to decrease in mortality roughly equals the sum of lead-time bias (approximately 1–3 years for LDCT)  and the traditional (without screening) survival time. Other variables may have affected our mortality analysis. In Illinois, a major coal-producing state, residential proximity to coal mines is associated with increased lung cancer incidence and mortality . Regardless, our analysis represents valuable baseline research and demonstrates the importance of attending to county-level disparities. An increase in LDCT screening uptake would likely reduce lung cancer mortality at the population level. On the basis of colorectal cancer screening research, we believe that greater geographic access to LDCT screening could effectively increase uptake . Improving geographic access to a service with low uptake is still worthwhile, because poor access may be contributing to low uptake.\n\n【44】Although rural areas are associated with poorer health outcomes than urban areas , we must also consider the urban–rural paradox, which suggests that among urban residents, greater distance to health care facilities is inversely associated with receiving care, but among rural residents, greater distance is positively associated with receiving care . Using 2015 data, Odahowski et al found that LDCT screening uptake was similar across metropolitan and nonmetropolitan counties, although low rates in both areas (<4%) make it difficult to understand why uptake is similar and whether the similarity will be maintained over time . The similarity in screening uptake rates may result from selection bias: the few people who completed screening may be the most enthusiastic and well-resourced patients in both urban and rural areas. Increased geographic access to LDCT screening may be needed to further increase uptake in rural areas. Further studies using discriminate, comprehensive measures of access and uptake are needed to explore whether geographic availability of screening has a different effect on mortality in urban and rural areas.\n\n【45】Previous research on geographic access to LDCT screening is minimal. To our knowledge, ours is the first study to assess access to LDCT screening, associated demographic determinants, and implications for mortality at a local population level. However, our study has several limitations. First, limited availability of public data necessitated the use of variables from 2 different periods. Demographic and lung cancer mortality data were from 2013–2017, whereas data on smoking prevalence and access to screening were from 2019. Second, we used data from multiple sources, including telephone surveys, online surveys, and government registries. Each source has its own limitations and can contribute to biased model estimates. Third, the ecological study design based on census block group–level and county-level data precludes extensive application of our conclusions about the relationships between rurality, access, and mortality to any 1 person. Fourth, in our exploratory analysis, county-level rates of access to LDCT screening were based on all residents aged 55 to 79, regardless of smoking status or other screening eligibility criteria. By taking this approach, we assumed that the percentage of residents aged 55 to 79 who meet eligibility criteria was roughly constant within a county; we made no assumptions about differences between counties. Finally, we included in our analyses only GO 2  Foundation Screening Centers of Excellence and American College of Radiology accredited centers. Thus, our analyses may have underestimated the proportion of residents, especially in rural areas, who had access to some form of screening. However, accredited LDCT programs may deliver a better level of care than nonaccredited programs .\n\n【46】This study underscores the need for further research and creative solutions for increasing LDCT screening in rural areas, especially in the Mississippi Delta region, where significant cancer disparities exist. Not doing so may propagate the urban–rural disparities that exist in other cancer screening programs, such as mammography . Further research may be especially important in areas with high rates of smoking and lung cancer mortality, such as southeastern Missouri. In the past few years, mobile LDCT screening has been introduced in dozens of rural communities in Georgia and Tennessee . Incorporation of telemedicine could also circumvent the difficulty of finding qualified on-site specialists to interpret LDCT scans and recommend treatment in rural areas. Teleradiology is now a ubiquitous practice, allowing radiologists to bill for LDCT and other interpretations furnished off-site. Some teleoncology programs offer remote interpretation of biopsy specimens , which is occasionally required as a follow-up to LDCT screening. Additionally, screening must be coupled with effective smoking cessation interventions to maximize reductions in mortality.\n\n【47】Finally, our results emphasize the need for data-driven, locally targeted programs to increase screening and decrease mortality. In Missouri and Illinois, many areas with high rates of smoking and lung cancer mortality have low access to screening. However, some areas with high rates of smoking and lung cancer mortality, such as the rural counties north of Kansas City, have good access to screening. State or national one-size-fits-all programs to simply add more screening centers may not be helpful in these communities.\n\n【48】Our study adds to the growing body of evidence on urban–rural disparities in access to screening, while exploring the effects of access to LDCT screening on lung cancer mortality. County-specific approaches are needed to increase access to screening in rural areas with high mortality. At the same time, further implementation research is needed to understand how to effectively minimize individual and system-level barriers to rural screening.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9dd65b2f-0644-40dd-b45e-50f0bd336857", "title": "Shiga Toxin–producing Escherichia coli, New Mexico, USA, 2004–2007", "text": "【0】Shiga Toxin–producing Escherichia coli, New Mexico, USA, 2004–2007\nThe epidemiology of infections and hemolytic uremic syndrome (HUS) caused by Shiga toxin–producing _Escherichia coli_ (STEC) O157:H7 are well described . Non-O157 STEC infection also is associated with severe illness and HUS but often is underdiagnosed and less well understood . Studies in Europe indicate that non-O157 STEC infections occur more frequently than do STEC O157 infections . STEC O157 infection has been a notifiable disease in the United States since 1994, but non-O157 STEC infection became reportable only in 2000 . To understand trends in STEC O157 and non-O157 infections and their epidemiology in New Mexico, we analyzed population-based data from the active surveillance of clinical laboratories. This surveillance was performed as part of the Centers for Disease Control and Prevention’s (CDC’s) Foodborne Diseases Active Surveillance Network (FoodNet), to which New Mexico began contributing data in 2004.\n\n【1】### The Study\n\n【2】All STEC isolates or broths were sent for confirmation to the Scientific Laboratory Division of the New Mexico Department of Health, where Shiga toxin expression was confirmed by enzyme immunoassay (EIA); the broth was then cultured and preliminarily serotyped. Pulsed-field gel electrophoresis was performed on all isolates. The laboratory submitted non-O157 STEC isolates to CDC for additional serotyping and PCR testing for toxin genes.\n\n【3】This analysis comprised only sporadic cases of STEC. Negative binomial regression models were used to calculate incidence rates and assess differences in risks for STEC O157 and non-O157 infections. Variables were reporting year, patient age, race/ethnicity, sex, and rural versus urban residence ; p < 0.05 was considered statistically significant.\n\n【4】During 2004–2007, New Mexico FoodNet identified 111 cases of laboratory-confirmed sporadic STEC infection; 40 (%) were STEC O157, and 71 (%) were non-O157 STEC . Six additional cases were outbreak associated. Incidence increased from 0.93 cases per 100,000 population (% confidence interval \\[CI\\] 0.5–1.36) in 2004 to 1.07 per 100,000 (% CI 0.61–1.52) in 2005 and 1.84 per 100,000 (% CI 1.25–2.44) in 2006. The rate fell slightly in 2007, to 1.70 per 100,000 (% CI 1.14–2.26) population, resulting in a test of trend that approached statistical significance (p = 0.09). From 2004 through 2007, sporadic STEC infections increased 94%. A total of 18 STEC serotypes were identified during this time. The primary serotypes responsible for the increase were STEC O157, O26, O111, and O103, constituting 75% of all STEC cases reported. Incidences of STEC serotypes O26, O111, and O103 combined increased 300% from 2004 through 2007. The proportion of non-O157 STEC ranged from 52% in 2005 to 74% in 2007.\n\n【5】Although STEC O157 was the 1 serotype most often identified, infections caused by non-O157 STEC (all serotypes) were diagnosed more frequently. STEC O26 (%) and O111 (%) were the most commonly identified non-O157 STEC serotypes. Other STEC O serotypes (O103, O121, O46, O177, and O91) were responsible for 33% of all STEC infections. Most isolates were positive for toxin gene _stx 1 _ (%), intimin (%), and enterohemolysin A (%).\n\n【6】STEC O157 infection was significantly more likely to be diagnosed in adults (half of all cases, one third of non-O157 STEC cases) than in children (<18 years of age) (p = 0.01). Non-O157 STEC serotypes were most commonly identified in children 1–4 years of age. STEC O157 infections occurred most commonly in children 5–10 years of age, followed by adults >60 years. Sex distributions were similar for patients with STEC O157 and non-O157 infections (% female vs. 45% male and 52% vs. 48%, respectively). White non-Hispanics, which constitute 43% of New Mexico’s population, made up 31% of all confirmed STEC infections in New Mexico; white Hispanics (%) made up 21% of confirmed STEC cases; and Native Americans (%) made up 12% of cases .\n\n【7】More laboratory-confirmed STEC infections were diagnosed during the summer months than during the rest of the year. STEC O157 cases were diagnosed more frequently in September (cases); non-O157 STEC cases were most frequently diagnosed in June and July (cases each).\n\n【8】Although STEC O157 infections was diagnosed in more persons than were non-O157 STEC infections (% vs. 16%), the difference was not significant. Patients with STEC O157 infections stayed in the hospital a mean of 6 days (median 4), compared with a mean of 4.5 days (median 3) for patients with non-O157 STEC infections, also not significant. All 5 reported cases of HUS were caused by STEC O157.\n\n【9】International travel was related to STEC infection for 12 (%) patients. Two (%) STEC O157 cases were travel related, as were 4 (%) cases each of STEC O111 and O26; and 1 (%) case each of STEC O128 and O103. Travel to Mexico was documented for 10 of the 12 travel-associated cases.\n\n【10】Most ([89%\\]) persons with STEC were from New Mexico’s urban counties. We calculated incidence rate ratios between STEC O157 and non-O157 infections during 2004–2007 , while adjusting for variables that were significant in negative binomial models, including race (white non-Hispanic vs. nonwhite), age (<5 years vs. \\> 5 years), and county (urban vs. rural). More patients with non-O157 STEC infection during this time were nonwhite, <5 years of age, and residents of urban counties. Patients infected with STEC O157 were more likely to be white non-Hispanic, \\> 5 years of age, and residents of rural counties.\n\n【11】### Conclusions\n\n【12】The data collected by New Mexico’s FoodNet surveillance network indicate that sporadic STEC cases increased substantially from 2004 through 2007. Reports of STEC O157 infection doubled from 7 to 14 from 2004 to 2006 but dropped to 9 in 2007. However, the number of non-O157 STEC cases continued to climb and accounted for most of the increase in overall STEC rates in New Mexico during this time, similar to rates in Connecticut and other FoodNet sites .\n\n【13】Non-O157 STEC infections ranged from 52% to 74% of all Shiga toxin–positive cases diagnosed each year and 64% of all identified STEC cases. New Mexico was second only to Colorado (.12 cases per 100,000 population) among FoodNet sites for non-O157 STEC incidence in 2007 . Similar to serotypes reported from other locations, STEC serotypes O26, O111, and O103 made up most non-O157 infections in New Mexico , especially O26 and O111 (% and 13% of all cases, respectively).\n\n【14】As previously reported , non-O157 STEC infections occurred commonly in young children in this study. They also occurred at a higher rate for non-white New Mexico residents. Another recent study similarly found higher shigellosis rates in counties with higher proportions of Hispanics . This study concurs with others that have reported finding the highest rates of STEC O157 rates in rural communities (with increased opportunities for animal contact) and in the West .\n\n【15】Year-to-year increases in numbers of non-O157 STEC infections, both nationally and in New Mexico, must be interpreted with caution because of changes in laboratory testing practices. Although the state’s largest clinical laboratory performed EIAs before active surveillance was implemented in 2004, other laboratories throughout the state might have changed their testing practices.\n\n【16】Risk factors for non-O157 STEC infections may be similar to those for STEC O157 infections , but additional studies are needed to elucidate differences and similarities between them, as well as among non-O157 STEC serotypes . Clinical laboratories should simultaneously screen for Shiga toxin and culture all positive isolates to determine the true incidence of non-O157 STEC infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "384537cd-11ae-4d65-95a3-293fec9599f1", "title": "Cockroaches (Ectobius vittiventris) in an Intensive Care Unit, Switzerland", "text": "【0】Cockroaches (Ectobius vittiventris) in an Intensive Care Unit, Switzerland\n**To the Editor:** _Ectobius vittiventris_ (Costa) is a field-dwelling cockroach and 1 of 4,000 cockroach species worldwide . We describe a cockroach infestation of an intensive care unit (ICU). Successful management required knowledge of the ecology of cockroaches and highlighted the need for species-level identification to tailor control strategies.\n\n【1】The University of Geneva Hospitals are a 2,200-bed tertiary healthcare center. The 18-bed medical ICU is located on the ground floor next to an outdoor recreational area and admits ≈1,400 patients/year. Smoking inside hospital buildings by patients and healthcare workers (HCWs) is strictly prohibited. On August 25, 2006, ≈30 cockroaches were observed in the ICU hiding inside oxygen masks, moving around on the light panels below the ceilings, or dropping onto intubated patients during the night.\n\n【2】An outbreak investigation was initiated. All work areas, including sinks and material stock areas, were thoroughly searched for cockroaches. External pest control experts identified only 1 species, _E_ . _vittiventris_ , which had presumably entered the ICU through windows facing the outdoor recreational area. The investigation showed that despite verbal recommendations and being repeatedly forbidden to do so, HCWs had opened the windows secretly with screwdrivers so that they could smoke during night shifts. The infestation was halted within 3 days after information regarding the infestation was provided to HCWs and all windows were bolted shut. In contrast to measures required to deal with a reported infestation in a neonatal ICU , no other measures such as use of insecticides, review of the air circulation system, or changes in architectural structures were necessary to stop the infestation reported here.\n\n【3】Cockroaches can cause 2 potentially serious health problems. First, they may provoke allergic reactions . Second, they have been suggested as possible vectors of multidrug-resistant pathogens. In particular, cockroaches that live and breed in hospitals have higher bacterial loads than cockroaches in the community . Up to 98% of “nosocomial” cockroaches may carry medically important microorganisms on their external surfaces or in their alimentary tracts  and may disseminate these microorganisms by fecal–oral transmission.\n\n【4】Cockroaches are capable of harboring _Escherichia coli_ , _Enterobacter_ spp. _Klebsiella_ spp. _Pseudomonas aeruginosa_ , _Acinetobacter baumannii_ , other nonfermentative bacteria , _Serratia marcescens_ , _Shigella_ spp. _Staphylococcus aureus_ , group A streptococci , _Enterococcus_ spp. _Bacillus_ spp. various fungi , and parasites and their cysts . An outbreak of extended-spectrum β-lactamase–producing _Klebsiella pneumoniae_ in a neonatal unit was attributed to cockroaches . Pulsed-field gel electrophoresis did not distinguish organisms from the insects from those colonizing infants or causing clinical disease . Unlike other investigators, we did not cultivate the cockroaches .\n\n【5】_E_ . _vittiventris_ cockroaches are easily confused with _Blattella germanica_ (Linnaeus) (the German or croton cockroach), which is probably the most important cockroach pest worldwide . In contrast to _B_ . _germanica_  and other species , _E_ . _vittiventris_ cockroaches are considered to be harmless and have not been associated with human disease or transmission of pathogens. We did not observe any allergic reactions or an increase in colonization or infection rates of multidrug-resistant organisms. _B_ . _germanica_ cockroaches are nocturnal, cannot fly, are always encountered within human habitations, and require specialized measures for eradication .\n\n【6】_E_ . _vittiventris_ cockroaches live in outdoor areas, do not avoid light, and are active during daytime. Buildings are not a natural habitat. In summer, adult insects can fly inside at night, but because these cockroaches are unable to reproduce inside buildings , stopping entry from outside halts the infestation. Entry can be stopped by closing windows or using mosquito nets. There is no existing insecticide for eradication of _E_ . _vittiventris_ cockroaches , and even if there were, it would not be effective because insects from untreated areas outside would enter continuously .\n\n【7】_E_ . _vittiventris_ cockroaches have been recently discovered in Geneva  and have become the most frequently encountered cockroaches in urban areas of Switzerland for several years . The reason for this finding remains unknown. The summer of 2003 was remarkably hot and dry in central Europe, thus representing a subtropical climate that usually favors the growth and development of cockroach populations . If this warming trend persists, populations of _E_ . _vittiventris_ cockroaches may continue to expand and similar infestations may occur.\n\n【8】In conclusion, effective control strategies for cockroach infestations depend on identification of cockroach species. In this report, permanent closure of all windows was sufficient to stop the infestation. However, to ensure compliance, it was critical to discuss the purposes of the intervention with HCWs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c842ffe-1360-4ad6-a5b6-5ef616ac6f6a", "title": "Overweight, Obesity, and Late-Life Sarcopenia Among Men With Cardiovascular Disease, Israel", "text": "【0】Overweight, Obesity, and Late-Life Sarcopenia Among Men With Cardiovascular Disease, Israel\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Little is known about the association between obesity and sarcopenia — age-related loss of muscle mass and function — among patients with cardiovascular disease. We investigated the association between overweight, obesity, and sarcopenia among community-dwelling men in Israel with cardiovascular disease.\n\n【3】**Methods**\n\n【4】A subset of 337 men (mean age at baseline 56.7 \\[SD, 6.5\\]) who previously  participated in the Bezafibrate Infarction Prevention trial underwent a neurovascular evaluation as part of the Bezafibrate Infarction Prevention Neurocognitive Study 15.0 (SD, 3.0) years after baseline and a sarcopenia evaluation 19.9 (SD, 1.0) years after baseline. We applied a multinomial logistic model to estimate odds ratios and 95% CIs for 3 categories of sarcopenia: no evidence of sarcopenia (ie, robust), probable sarcopenia, and sarcopenia.\n\n【5】**Results**\n\n【6】We found sarcopenia among 54.3% of participants with obesity (body mass index \\[BMI, in kg/m 2  \\] ≥30.0), 37.0% of participants who were overweight (.0 ≤ BMI ≤29.9), and 24.8% of participants with normal weight (BMI 18.5 to 24.9). In a comparison of BMI ≥25.0 and BMI <25.0, adjusting for covariates, the odds ratio of having probable sarcopenia was 3.27 (% CI, 1.68–6.36) and having sarcopenia was 5.31 (% CI, 2.50–11.27).\n\n【7】**Conclusion**\n\n【8】We found a positive association between obesity and late-life sarcopenia and suggest that obesity might be an important modifiable risk factor related to sarcopenia among men with cardiovascular disease.\n\n【9】Introduction\n------------\n\n【10】Sarcopenia, from the Greek “poverty of flesh,” is a highly prevalent geriatric syndrome first described by Rosenberg in 1989 as the age-related loss of muscle mass and function . Accumulating evidence suggests that sarcopenia is associated with adverse health outcomes such as frailty, falls, disability, admission to nursing homes, and mortality . Several underlying mechanisms are linked with the development of sarcopenia, including impaired neuromuscular function, hormonal changes, increased inflammation, changes in body-fat distribution, poor nutritional status, and various chronic conditions, yet not all have been fully elucidated . The most studied approach in modifying risk factors for sarcopenia is resistance exercise. Numerous treatments of sarcopenia, including protein supplementation and pharmacological interventions, have limited value .\n\n【11】Obesity-mediated factors may aggravate sarcopenia in older people and maximize its effects on physical disability, morbidity, and mortality . Several studies investigated the association of obesity with sarcopenia . Findings on the association between overweight and sarcopenia are controversial . The prevalence of cardiovascular disease (CVD) in middle and old age is increasing, partly as a result of increases in the prevalence of obesity . Furthermore, CVD might accelerate the development of sarcopenia, and both have been strongly tied to chronic low-grade inflammation, insulin resistance, and obesity . However, little is known about the association between obesity and sarcopenia in patients with CVD. The aim of this study was to describe the association between overweight, obesity, and late-life sarcopenia among community-dwelling men aged 64 or older with CVD.\n\n【12】Methods\n-------\n\n【13】Our study sample consisted of a subset of patients from 8 hospitals who resided in the central region of Israel and who previously participated in the Bezafibrate Infarction Prevention (BIP) clinical trial of lipid modification during 1990–1997 (N = 1,232) and then in the BIP Neurocognitive Study during 2004–2013. The study design and procedures of the BIP trial are detailed elsewhere . In brief, the BIP study was a placebo-controlled randomized clinical trial investigating the efficacy of a 400-mg daily dose of bezafibrate, a fibric derivative, in secondary prevention among patients with established stable coronary heart disease. To be included in our study, the lipid profile of patients had to fall within these parameters: serum total cholesterol 180 to 250 mg/dL, low-density lipoprotein cholesterol ≤180 mg/dL (≤160 mg/dL for people aged <50), high-density lipoprotein cholesterol ≤45 mg/dL, and triglycerides ≤300 mg/dL. Other exclusion criteria were renal failure, defined as a serum creatinine level ≥1.5 mg/dl or nephrotic syndrome; liver failure, defined as serum glutamate pyruvatetransominase >60 U/L; and stroke, assessed by reviewing records from hospital or emergency department discharge, a primary care physician, or a neurologist .\n\n【14】The BIP Neurocognitive Study consisted of 2 follow-up evaluations . The first follow-up evaluation (time 1; n = 546) was performed during 2004–2009, an average of 15.0 (SD, 3.0) years after recruitment to BIP; it assessed neurovascular and cognitive function. Patients were re-examined during 2011–2013 (time 2; n = 351), 19.9 (SD, 1.0) years after recruitment; this examination assessed sarcopenia and re-assessed cognitive function. The mean interval between time 1 and time 2 was 4.8 (SD, 1.3) years. Patients were assessed at a central research center (the Sagol Neuroscience Center, Sheba Medical Center, Ramar Gan, Israel), or if a patient was unable or unwilling to attend the medical center, the assessment occurred at their residence.\n\n【15】The institutional review boards of the Sheba Medical Center Ethics Committee approved the study and informed consent was obtained from all participants.\n\n【16】### Baseline measurements \n\n【17】Methods for baseline assessment of the BIP study are described elsewhere . Height and weight (without shoes) were measured at baseline  using a standard stadiometer and an electronic weighing scale. Body mass index (BMI) was calculated as body weight in kilograms divided by height in meters squared (kg/m 2  ). Weight was categorized as normal (BMI 18.5–24.9, hereinafter referred to as BMI <25.0), overweight (BMI 25.0–29.9), or obese (BMI ≥30.0). Blood samples were drawn from each study patient at baseline of the BIP trial. Samples were collected after at least 12 hours of fasting, with the use of standardized equipment and procedures and transferred to a central study laboratory. Insulin resistance was defined as the upper quartile of the homeostatic model assessment of insulin resistance, calculated as fasting insulin (μU/ml) × fasting glucose in mg/dL/405. C-reactive protein concentrations were measured using standard automated procedures with commercially available kits (Roche Diagnostics). Angina severity at baseline of the BIP study was classified according to the Canadian Cardiovascular Society angina classification, the standard measure for grading the severity of effort-induced angina . Participants were categorized into 2 groups: 1) no angina pectoris or angina pectoris class 1, and 2) angina pectoris class 2 and above. Physical activity was assessed by asking participants, “Do you participate in one or more of the following activities such as walking, swimming, jogging, gymnastics, biking, dancing, tennis, gardening, gym and rehabilitation activities?” Responses were categorized as any physical activity or sedentary physical activity. Data on education, place of birth, smoking, and comorbidity were collected through a questionnaire at baseline.\n\n【18】### Sarcopenia evaluation (time 2)\n\n【19】Sarcopenia was defined by using the European Working Group on Sarcopenia in Older People definitions of low muscle strength, low muscle mass, and low physical performance . Probable sarcopenia was defined as low muscle strength or low muscle mass. Sarcopenia was defined as low muscle strength and low muscle mass and/or low physical performance.\n\n【20】Muscle strength was defined as isometric dominant handgrip strength, which was assessed by using a Jamar hydraulic hand dynamometer (Sammons Preston). Muscle strength was categorized as low if ≤29 kg (for patients with BMI ≤24.0), ≤30 kg (for patients with BMI 24.1–28.0), and ≤32 kg (for patients with BMI >28.0). The test was carried out twice and the higher score was used.\n\n【21】Muscle mass was assessed by bioelectric impedance analysis using a Tanita BC-545 8-contact electrode body composition analyzer. To calculate body composition, the computer software in the bioelectric impedance analysis system uses the measured impedance, the programmed person’s sex and height, and the measured weight. The device measures fat-free mass in kilograms and the percentage of total body fat according to the equation provided by the device’s software. Skeletal muscle mass was calculated by using the following equation: skeletal muscle mass = 0.566 × fat-free mass. Skeletal muscle mass index, adjusted for weight, was calculated as (skeletal muscle mass × 100)/weight . Low skeletal muscle mass index was defined as 37.4% or lower .\n\n【22】We used gait speed to determine physical performance. Gait speed was measured by gait time in seconds using a 5-meter timed walk test. Usual gait speed of less than 1 meter per second signifies a high risk of health-related outcomes in well-functioning older people . The test instructions were as follows: “On the word ‘go’ start walking at your regular pace to the line on the floor.” We used height-adjusted time as the cutoff. Low physical performance was denoted as ≥6 seconds (for height ≤173 cm \\[68 inches\\]) and ≥5 seconds (for height >173 cm) .\n\n【23】### Additional assessments\n\n【24】In both evaluations (time 1 and time 2), data on comorbidities and hospitalizations, medication use, smoking status, physical activity, and anthropometric measurements were collected systematically. In addition, systolic blood pressure, diastolic blood pressure, and weight and height were measured.\n\n【25】Incident stroke during follow-up was assessed by reviewing records from hospital or emergency department discharge, primary care physicians, or neurologists. We used a score of 5 or more on the 15-item version of the Geriatric Depression Scale  to indicate clinically significant depressive symptoms. Patients completed the NeuroTrax computerized cognitive test (NeuroTrax Corporation). A description of this test is available elsewhere . All NeuroTrax scores are normalized according to age- and education-specific normative data and scaled to an IQ-style scale with a mean of 100 and SD of 15. Dementia and incident stroke during follow-up were determined by an adjudication committee composed of 3 investigators, 2 of whom were experienced board certified neurologists. A diagnosis of dementia was based on a cognitive evaluation, a clinical interview, and data collected and was in accordance with criteria of the _Diagnostic and Statistical Manual of Mental Disorders, 4th Edition_ . An occurrence of stroke was determined on the basis of World Health Organization criteria .\n\n【26】Cerebrovascular reactivity, a marker of cerebral microvascular function, was evaluated by transcranial Doppler (Trans-Link 9900 \\[Rimed\\]), which measures the breath-holding index . Participants were categorized into normal (≥0.69) or impaired (<0.69) cerebrovascular reactivity on the basis of the mean breath-holding index of both middle cerebral arteries according to previously established standard parameters . Carotid intima-media thickness (cIMT), a measure of cerebral large-vessel atherosclerosis, and plaque presence were measured at the far wall of both common carotid arteries using high-resolution B-mode ultrasound. Participants were classified into the following 2 categories according to previously established standard parameters : cIMT ≥0.93 mm (elevated) and/or bilateral carotid plaques or cIMT <0.93 (normal) and without bilateral carotid plaques.\n\n【27】### Statistical analysis\n\n【28】We summarized data on the clinical characteristics of patients as percentage and mean (SD), unless the distribution was strongly skewed, in which case we summarized data as median and interquartile range (IQR). We compared variables between BMI and sarcopenia groups by using analysis of variance or the Kruskal–Wallis test for continuous variables and the χ 2  test for categorical variables. We used multinomial logistic regression to estimate odds ratios (ORs) and 95% CIs for sarcopenia. The categories of the outcome variable were sarcopenia, probable sarcopenia, and robust (ie, no evidence of sarcopenia); we used robust as the reference category in comparisons. We first adjusted for age, education, and birthplace, then additionally for systolic blood pressure, physical activity, diabetes, insulin resistance, C-reactive protein, high-density lipoprotein cholesterol, and triglycerides. Subsequently, we further adjusted for depressive symptoms (Geriatric Depression Scale score ≥5 vs <5), global cognitive score, and Doppler ultrasound indices of cerebrovascular disease at time 1.\n\n【29】Because of loss to follow-up of eligible patients who had either died or refused participation, we estimated the probability of every person to reach the sarcopenia assessment and calculated the inverse probability weights . We compared the results of weighted analysis with nonweighted analysis. Data were analyzed using SPSS version 21 (IBM Corporation).\n\n【30】Results\n-------\n\n【31】Of the 1,232 patients eligible for the evaluation at time 1, 214 had died; 259 refused to participate; 102 could not be contacted; 45 were unable to participate because of dementia, language incompatibility, vision or hearing defects, or physical disability; and 54 were excluded because of missing data. Twelve women were not included because of the small sample. This process of exclusion resulted in 546 men available for evaluation at time 1 . A total of 351 patients were re-assessed at time 2; of these, 337 had BMI measurements and a sarcopenia evaluation. The attrition from time 1 and time 2 was mainly a result of interim death (n = 114); in addition, 58 refused to participate, 4 could not be contacted, and 19 were unable to participate. The mean (SD) age of the study sample was 56.7 (.5) at baseline, 71.8 (.5) at time 1, and 77.1 (.4) at time 2 .\n\n【32】**  \nStudy flowchart. Participants from 8 hospitals in central Israel were initially recruited for the Bezafibrate Infarction Prevention (BIP) clinical trial of lipid modification during 1990–1997 and were also in the BIP Neurocognitive Study. The BIP Neurocognitive Study consisted of 2 follow-up evaluations: time 1  and time 2 . \n\n【33】Of the 337 patients, 109 (.3%) were classified as robust, 112 (.2%) as probable sarcopenia, and 116 (.4%) as sarcopenia . Sarcopenia was significantly related to older age, less education, more severe angina pectoris (angina pectoris class ≥2), and a higher systolic blood pressure at baseline. In addition, participants with sarcopenia had poorer global cognitive function, more cerebrovascular diseases, and a higher level of depression at time 1 compared with their counterparts.\n\n【34】In general, patients with obesity, compared with patients without obesity, more often had diabetes and insulin resistance, a blood glucose level ≥100 mg/dL, and higher levels of C-reactive protein, and they were less physically active at baseline. We found sarcopenia among 54.3% of patients with obesity, 37.0% of patients who were overweight, and 24.8% of patients with normal weight (P_ for trend <.001).\n\n【35】Adjusting for age, education, birthplace, systolic blood pressure, physical activity, insulin resistance, C-reactive protein, high-density lipid cholesterol, and triglycerides, the estimated adjusted OR (% CI) for probable sarcopenia for patients with BMI ≥25.0, compared with patients with BMI <25.0, was 2.88 (.54–5.36) and for sarcopenia was 5.04 (.51–10.15). Additional adjustment for global cognitive score, Doppler ultrasound indices of cerebrovascular disease, and depressive symptoms did not materially alter the results . An increment of 1.0 BMI unit was associated with an adjusted OR (% CI) of 1.33 (.16–1.53) for probable sarcopenia and of 1.38 (.19–1.59) for sarcopenia . The adjusted OR (% CI) for probable sarcopenia for patients with obesity was 6.51 (.56–27.13) and for patients who were overweight (but not obese) was 3.03 (.54–5.98). The adjusted OR (% CI) for sarcopenia for patients with obesity was 13.49 (.12–58.20) and for patients who were overweight was 4.65 (.16–10.04).\n\n【36】Discussion\n----------\n\n【37】In this cross-sectional study of men with CVD, being overweight and obese in their fifties was related to late-life sarcopenia. The observed relationship was independent of traditional cardiovascular risk factors and global cognitive score and did not change after exclusion of participants with a history of stroke or dementia at the sarcopenia assessment. To the best of our knowledge, our study is the first to estimate the association between obesity and overweight and late-life sarcopenia among men with CVD. In our sample, the prevalence of probable sarcopenia (.2%) and sarcopenia (.4%) was high compared with the prevalence in the general population (%–13%) measured by using the criteria of the European Working Group on Sarcopenia in Older People . A retrospective study of Japanese men with CVD and an average age of 72 (SD, 12) found sarcopenia in 29.5% of the study population . The prevalence of sarcopenia is higher among people with CVD than among the general population, but comparisons of sarcopenia rates between studies are difficult because of differences in study population, age ranges, sarcopenia definition, and the use of various thresholds for defining low muscle mass and low muscle performance .\n\n【38】Some researchers have suggested that being overweight in midlife may be associated with lower, rather than higher, sarcopenia rates . In a 4-year follow-up study among community-dwelling Chinese men and women aged 72.5 (SD, 5.2) on average found that higher BMI was inversely associated with the development of sarcopenia . On the other hand, in the Collaborative Research on Ageing in Europe survey conducted among people aged 65 or older, higher percentage body fat and lower levels of physical activity were associated with low muscle mass and sarcopenia in almost all the countries studied .\n\n【39】Obesity is associated with increased risk of glucose intolerance, chronic inflammation, hypertension, dyslipidemia, and CVD. Sarcopenia shares many pathological mechanisms with obesity, including insulin resistance and low-grade chronic inflammation . Aging induces changes in body composition, such as increases in visceral fat and decreases in skeletal muscle mass . Fat mass induces inflammation, which may contribute to the development of sarcopenia . A vicious circle between muscle loss and fat gain may lead to more sarcopenia and then to further weight gain, inflammation, and impaired glucose tolerance . However, these changes in body composition, such as increases in fat and decreases in muscle mass, are potentially reversible by modifying lifestyle behaviors . Resistance exercise and nutrition support, which increases muscle mass and muscle strength, are key lifestyle strategies that can prevent sarcopenia or reverse it .\n\n【40】Strengths of this study include a unique data set of extensively studied men with CVD who underwent comprehensive evaluations. Skeletal muscle mass was assessed by bioimpedance method, which is considered simple and inexpensive and has a high correlation with magnetic resonance imaging and dual-energy x-ray absorptiometry . However, some important issues need to be considered. First, we could not evaluate the causal effect of obesity on sarcopenia because we did not assess sarcopenia at baseline. Second, the generalizability of the study is limited to men with CVD who had the specific clinical characteristics required for eligibility in the BIP study. Finally, despite an acceptable response rate of 82% among survivors, a substantial proportion of patients did not participate in the late-life assessment because of the long period between baseline and the late-life evaluations.\n\n【41】In summary, we observed a positive association between overweight and obese men with CVD and late-life sarcopenia. Given the growth of the population aged 65 or older and the high prevalence of obesity and CVD , it is important to identify people with CVD and potentially preventable risk factors for sarcopenia. However, these findings need to be confirmed in larger studies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eef52041-4ae2-4aea-b2f4-58033a88373a", "title": "Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa 1850 A.D.)", "text": "【0】Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa 1850 A.D.)\nDr. Ryusai Kuwata (Bunka 8/1811-Keio 4/1868) . Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa Kaei 3/1850 A.D.). Reproduced with permission from the Nihon University Medical Library, Iidamachi, Tokyo.\n\n【1】In 1849, 18 years before the Meiji Revolution, cowpox vaccination was first successful in Nagasaki, which was the only city open to foreign visitors during the Tokunaga Era. This method became widespread throughout Japan. Dr. Ryusai Kuwata (Bunka 8/1811-Keio 4/1868) from Edo (modern Tokyo) made this color woodcut print to advertise the effectiveness of the vaccination to protect against smallpox; he used this picture at the Osaka Vaccination Clinic. The white cow in the print represents vaccination.\n\n【2】It was known that among persons and families who tended cows, the clinical signs of smallpox were never serious. Following this clue, Dr. Kuwata tried vaccinating children with the contents of eruptions from cowpox. As the result, the children's illness was mild and not transmitted to others. Dr. Kuwata, who was the pioneer of vaccination in Japan, vaccinated more than 70,000 people. He died with a vaccination needle in his hand in 1868, when he was 58 years old.\n\n【3】\\* \">\n\n【4】### Front cover and text translation \\*\n\n【5】##### Use of Vaccine Has Spread from Osaka to Edo (Tokyo) for the Benefit of Mankind\n\n【6】I wonder who in the world named Smallpox a God? It is no more than a demon that deviated from the path of mankind.\n\n【7】There are approximately five ways of treating smallpox; one is called the vaccinia method. In the 1790s, a Dutchman \\[sic\\] named Edward Jenner announced this method. It is said that when this method was first used, a vaccine made from cow's blood was directly injected into patients. It was quite astonishing that this vaccine, which was made from blood drawn from cow udders and was used on children, cured smallpox.\n\n【8】The first form of this vaccine was the Dutch form, but the Chinese form was used after 1806. The Chinese form is currently being used in Japan, but many forms and methods are used throughout the world. In this manner, vaccination has become a method of treating smallpox.\n\n【9】We began using the Dutch form of the vaccine in the latter half of the 1800s. Thanks to the patronage of our loyal customers, the use of our vaccine has spread widely.\n\n【10】\\[Translator's note: At the end of this advertisement, there is a poem, the gist of which is, \"Parents should do whatever they can to ensure the well-being of their children.\"\\]\n\n【11】\\* \">\n\n【12】### Back cover and text translation \\*\n\n【13】Translation of the image that appears on back cover.\n\n【14】##### What You Should Know about Vaccinations\n\n【15】*   Babies should be vaccinated on the 70th day after birth, regardless of the season. However, babies may be vaccinated sooner if an outbreak of smallpox has already occurred nearby.\n\n【16】*   After receiving the vaccine, patients should not become infected with smallpox, but patients who come into contact with the smallpox virus before being vaccinated should provide a vaccination document that states \"smallpox\" and \"vaccine\" to the physician within 10 days after the vaccination.\n\n【17】*   If patients become infected with another disease during the vaccination period, they should, at their own responsibility, request to be examined by other physicians and receive medicine for the disease.\n\n【18】*   During the vaccination period, a patient may bathe every day in a bath of warm water up to the hips.\n\n【19】*   Patients must return to the original medical institution and undergo another examination 8 to 10 days after receiving the vaccination. If a patient fails to receive the second examination, the \"vaccination complete\" certificate will not be issued.\n\n【20】*   A person who has received the \"vaccination complete\" certificate must be vaccinated again 6 years later and must receive a third vaccination 6 years after that. In either case, the patient must have a physician confirm that the vaccination was successful; provided, however, that if an outbreak of smallpox occurs nearby during that period, another vaccination may be administered before the end of the 6-year period. \\*\n\n【21】    Cover text translation by Bonnie Mikami and Masahito Mikami", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "46bb64aa-00ee-4a9f-b125-62e323b39f07", "title": "Influenza Virus A (H1N1) in Giant Anteaters (Myrmecophaga tridactyla)", "text": "【0】Influenza Virus A (H1N1) in Giant Anteaters (Myrmecophaga tridactyla)\nEmergence of viruses in new hosts is a continuing concern in public health surveillance and thus is the focus of intense research. The ability to move among different species may enable mutations and changes in phenotypes as the virus adapts to a new host . Influenza virus is a prime example of a pathogen with the ability to infect not only its avian reservoirs but also mammalian species such as swine, horses, dogs, cats, ferrets, whales, and humans.\n\n【1】In March 2007, we documented the occurrence of influenza in giant anteaters (Myrmecophaga tridactyla_ ). Giant anteaters are indigenous to neotropical regions of Central and South America, and, although they are extinct or endangered in some regions, overall their status is listed as near threatened by the International Union for Conservation of Nature .\n\n【2】### The Study\n\n【3】A colony of 11 adult anteaters (males, 4 females) and 1 neonate was housed at the Nashville Zoo at Grassmere, Nashville, Tennessee, USA. The colony experienced an outbreak of respiratory disease beginning in February 2007. The anteaters were housed separately in stalls in the same building with shared ventilation, with the exception of the nursing neonate who was housed with his dam. There was no contact with animals outside the colony. The primary caretaker of the colony had no contact with other animals housed at the zoo. No other species experienced respiratory disease at the zoo during the outbreak. Only the primary caretaker had sustained direct contact with any members of the colony.\n\n【4】The index case occurred on February 8 in 1 animal that was being treated twice a day for a superficial wound. The respiratory disease was characterized clinically by severe nasal discharge and congestion, inappetence, and lethargy. Within several days, all adult animals in the colony were affected. Only the neonate appeared to remain unaffected. The caretakers overseeing the colony, with the exception of the attending veterinarian, were also ill with respiratory disease, including the primary caretaker. The onset of the caretakers’ illness coincided with the illness in the anteaters. No diagnostic testing was conducted for the caretakers during the outbreak.\n\n【5】Nasal discharge samples were collected on February 15 for virus isolation from 3 animals, including the index case. These samples were shipped overnight on cold packs to the Clinical Virology Laboratory at the University of Tennessee College of Veterinary Medicine. These samples were prepared for virus isolation in Dulbecco minimal essential medium supplemented with antimicrobial drugs, antifungals, and 5% fetal bovine serum (Atlanta Biologicals, Norcross, GA, USA; and Cambrex Bioscience Walkersville, Walkersville, MD, USA). Cell lines injected included Vero, rabbit kidney, MDCK, and Crandell-Reese feline kidney (American Type Culture Collection, Manassas, VA, USA). Cytopathic effects were noted in the MDCK cell lines on first passage in 2 of the 3 isolations; no cytopathic effects were noted in the isolation from the third sample.\n\n【6】Cell culture supernatant from the infected flasks was tested for hemagglutination by using 10% guinea pig erythrocytes, and agglutination was observed. Virus-infected cell pellets were prepared for electron microscopic examination . Examination showed an enveloped virus of 100–120 nm diameter . Slides of infected cells were also prepared and examined by direct immunofluorescent antibody (IFA) assay by using influenza A- and B-specific antiserum (Diagnostic Hybrids, Athens, OH, USA); positive fluorescence was observed only with type A-specific antiserum. Based on the morphology and IFA results identifying a type A influenza virus, reverse transcription and PCR were conducted on the isolate from one of the animals by using type A-specific primers encompassing the entire hemagglutinin (HA), neuraminidase (NA), nonstructural protein 1 (NS1), and RNA polymerase 2 (PB2) gene segments as previously described . Nucleotide sequencing of HA, NA, PB2, and NS1 gene amplification products were conducted by Molecular Biology Resources (University of Tennessee, Knoxville, TN, USA) using an ABI prism dye terminator cycle sequencing reaction kit and an ABI 373 DNA sequencer (Perkin Elmer, Foster City, CA, USA) (Genbank accession nos. EU543278, EU543279, FJ478393, and FJ785200, respectively). High nucleotide identity (> 99%) was found between the anteater isolate and human influenza virus isolate type A strain Tennessee/UR06-0119/2007(H1N1) and others from the United States isolated in 2006 and 2007 (data not shown). Phylogenetic analysis (MegAlign program with ClustalW align, Lasergene package; DNAStar, Madison, WI, USA) of the HA and NA amplification products indicated a close relationship among these isolates .\n\n【7】Serologic analysis was not feasible at the time of the outbreak due to the need to use anesthesia to immobilize the animals for blood collection. However, samples collected from 3 animals in months following the outbreak were made available for serologic testing, as well as samples from these same animals collected at various times prior to the outbreak. The samples had been stored at –20°C. Testing was conducted by hemagglutination inhibition using the anteater influenza isolate . The serologic results are shown in the Table . Evidence of seroconversion was observed in 2 of the animals, and 1 animal had evidence of infection prior to the 2007 outbreak.\n\n【8】### Conclusions\n\n【9】This respiratory disease outbreak among members of an anteater colony was caused by an influenza virus isolate closely related to the human influenza virus (H1N1) strains circulating in the concurrent year. Serologic analysis of samples from animals involved in the outbreak confirmed infection; seroconversion was documented in 2 animals, and 1 animal appeared to have been exposed to and infected with influenza virus a year before the described outbreak. This animal had been imported as a juvenile in 2003 and was ill at that time with a respiratory disease. It is not known what the cause of this earlier illness was or if exposure to influenza in the intervening years had occurred. Analysis of 4 viral genes sequenced indicated high homology with the human influenza virus (H1N1) isolates.\n\n【10】The differences between the anteater isolate and circulating human strains did not occur at sites known to be antigenically or functionally important; thus, these minor changes do not appear to alter the antigenicity or the function of the encoded proteins . We concluded that, based on the genetic sequence of the virus isolated from the anteaters and on the fact that the colony was not exposed to animals other than the human caretakers, the caretakers were the most likely source of the virus affecting the anteater colony. Further genetic sequencing will be required to determine if this interspecies transmission arose as a result of mutations of the virus, including reassortment.\n\n【11】Influenza is known to cross species lines into mammalian species. Birds, in particular waterfowl (Family Anatidae), are the natural reservoir for influenza viruses. Reassortment between avian and mammalian strains may occur in swine because they express receptors for avian and mammalian influenza . An entire influenza virus may undergo interspecies transmission without reassortment. Two recent notable incidences of this are the transmission of equine influenza virus (H3N8) to dogs  and the transmission of avian influenza virus (H5N1) to humans as well as to various carnivore species . With the first mentioned, efficient dog-to-dog transmission was documented. For avian influenza virus (H5N1), sustained human-to-human transmission has not occurred. It is not clear whether animal-to-animal spread of this influenza virus occurred within the anteater colony or whether all affected animals were infected directly from the caretakers.\n\n【12】The isolation of influenza virus in giant anteaters highlights the difficulty of influenza surveillance as the full spectrum of mammalian hosts for influenza virus remains unknown. This host variability could potentially impact human populations as possible sources of zoonotic spread of influenza.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b7190f51-a733-4764-bbb9-4f6bfbf18b48", "title": "Phylogeny and Disease Association of Shiga Toxin–producing Escherichia coli O91", "text": "【0】Phylogeny and Disease Association of Shiga Toxin–producing Escherichia coli O91\nShiga toxin–producing _Escherichia coli_ (STEC) infections are public health concerns because of the severe illnesses they cause, such as hemorrhagic colitis and hemolytic uremic syndrome (HUS) . STEC constitute a heterogeneous group of bacteria abundant in the reservoir and in the environment . Transmission routes for human STEC infection are numerous and include contact with animal excreta, person-to-person transmission, and inadvertent ingestion of contaminated food and water. Many STEC serotypes have been recovered from humans . Among them, STEC O91 is the most common serogroup isolated from adult patients in Germany . The strains within this serogroup appear to be transmitted predominantly by food, because 1) food vehicles have been identified as the only risk factors for adults with sporadic STEC O91 infection in Germany ; 2) O91 is the second most frequently isolated STEC serogroup in routine food samples ; and 3) O91 is the only major STEC serogroup with no association between incidence of human infection and cattle density .\n\n【1】Whereas most human disease STEC serogroups possess, in addition to Shiga toxin, the _eae_ gene encoding the adhesin intimin , STEC O91 consistently lack this virulence determinant . Despite frequent isolation of STEC O91 from humans, the clonal relatedness of the serotypes of this serogoup is poorly understood. Therefore, we investigated 100 human STEC O91 isolates to determine the clonal structure of STEC O91 and its association with disease.\n\n【2】### The Study\n\n【3】A total of 100 STEC O91 isolates were obtained from 1997 through 2007 from patients with HUS (n = 4), bloody diarrhea (n = 8), watery diarrhea without visible blood (n = 79), abdominal cramps without diarrhea (n = 1), or from asymptomatic carriers (n = 8); samples were from Germany (n = 96), Austria (n = 2; Austrian Reference Library, Innsbruck, Austria), Finland (n = 1; The National Public Health Institute, Helsinki, Finland), and Canada (n = 1; Public Health Agency of Canada, Guelph, Ontario, Canada). The 96 German O91 strains were recovered at the Institute of Hygiene, University of Münster, Münster, and the Robert Koch Institute, Wernigerode, Germany. The strains included all human isolates of this serogroup that were recovered during the study period in Germany and for which complete clinical information was available. The strains correspond to all O91 serotypes associated with human diseases from sporadic cases in Germany in that interval. Thirty-five strains have been described previously .\n\n【4】The age of patients from whom the STEC O91strains originated ranged from 4 months to 89 years (median 28 years, interquartile range 12–38 years). The most severe symptom was recorded for each patient. Diarrhea was defined as ≥3 semisolid or liquid stools per day. Bloody diarrhea was defined as diarrheal stools containing blood visible to the naked eye. HUS was defined as a case of microangiopathic hemolytic anemia (hematocrit <30% with peripheral evidence of intravascular hemolysis), thrombocytopenia (platelet count <150,000/mm 3  ), and renal insufficiency (serum creatinine concentration greater than the upper limit of normal for age) . Asymptomatic carriers were apparently healthy persons without diarrhea; their stools were submitted as noted above.\n\n【5】Strains were isolated using Shiga toxin–encoding genes as diagnostic targets  and then serotyped phenotypically . All strains were verified as O91 by using PCR targeting _wzy_ O91  , a component of the _rfb_ gene cluster that synthesizes the O91 antigen . Multilocus sequence typing (MLST) and phylogenetic analysis were performed as described . All allelic sequences were deposited in the _E. coli_ MLST database . The minimum spanning tree was generated from all 100 O91 sequence types (STs) and compared with the HUS-associated enterohemorrhagic _E. coli_ (HUSEC) collection  to display the distribution of the STs compared with all known STs associated with HUS.\n\n【6】We tabulated disease severity according to STs. To study the relationship between ST442 and disease severity, patients were categorized into those with HUS, those with bloody diarrhea, and, serving as reference group, those with nonbloody diarrhea or those that asymptomatically excreted these organisms. Univariate associations were computed by using exact logistic regression. p values <0.05 were considered statistically significant. STATA release 10.0 (StataCorp LP, College Station, TX, USA) was used for statistical analysis.\n\n【7】The 100 STEC O91 strains resulted in 10 different STs. Of these, STs 33 and 442 were most common (and 20 isolates, respectively). Six additional STs  were single-locus variants of ST33, indicating their close relationship. The 2 remaining STs (and 1020) were not closely related to any other ST of the serogroup O91 strains. Detailed analysis of the 7 housekeeping genes used for MLST demonstrated the _fumC_ gene sequence alone could differentiate 5 STs of the O91 strains. The comparison of all O91 STs with all STEC STs and serotypes associated with HUS (HUSEC collection) is displayed in the Figure .\n\n【8】H antigens associated with O91 were H8 (n = 1), H10 (n = 2), H14 (n = 52), H21 (n = 20), Hnt (H-antigen nontypeable) (n = 10), and H –  (H-antigen nonmotile) (n = 15). In serotype O91:H8, O91:H10, and O91:H21 strains, the STs were serotype-specific. However, ST33 and its single-locus variants represented all strains of serotypes O91:H14, O91: H –  , and O91:Hnt.\n\n【9】A single sequence type, ST442, accounting for 20% of all strains, was found among each of the four O91 isolates from patients with HUS , a highly significant association (odds ratio \\[OR\\] 27.8, 95% confidence interval \\[CI\\] 3.3–∞, p<0.01). ST442 strains were also more frequently isolated from patients with bloody diarrhea than were strains belonging to other STs (/20 \\[15.0%\\] vs. 5/80 \\[6.3%\\] respectively), but this difference was not statistically significant . The overall association with severe disease, defined as either HUS or bloody diarrhea, was strong (OR 7.8, 95% CI 1.83–36.6, p<0.01). Severe illness was noted for 7 (.0%) of 20 patients infected by ST442 strains, but only for 5 (.3%) of 80 patients infected by STEC O91 of other STs . Patients with bloody diarrhea were younger (median age 12 years) than patients who had mild or no symptoms (median age 20 years). However, this difference was not observed for the 4 patients with HUS (median age 21 years); in this instance, 2 were adults, 1 was 39 months old, and 1 was unknown.\n\n【10】### Conclusions\n\n【11】To gain insight into the clonal structure of STEC O91, we determined the relatedness of 100 strains isolated from patients and correlated the clonal lineage to the clinical outcome of the infection. MLST analysis divided the O91 isolates into 10 different STs, whereas classical serotyping identified only 4 complete serotypes (O- and H-antigen). Moreover, MLST was able to type all 25 nonmotile (H –  ) or nontypeable (Hnt) O91 strains. The analysis demonstrated that the _fumC_ gene from the 7 genes used for MLST was the most heterogeneous and enabled strain differentiation into 5 different STs, among these ST442. It might therefore be a candidate for first-line single-locus sequence typing.\n\n【12】HUS or bloody diarrhea without HUS was significantly associated with ST442, which was represented by serotype O91:H21 only. However, Pradel et al. also reported a case of HUS associated with an O91:H10 isolate that could be differentiated from O91:H21 by using ribotyping . In our study, known virulence determinants such as cytolethal distending toxin V or Shiga toxin 2d activatable by elastase in O91:H21 strains  might contribute to the higher virulence of O91:H21 (ST442). However, further studies of the mechanisms behind the emergence of ST442 in Germany and additional analysis of global O91 isolates are needed. With the MLST approach described, trends and changes in STEC O91 epidemiology and human infections can be carefully surveyed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6db33ad1-295a-48ed-8717-39dc15b9769e", "title": "The Access Principle: The Case for Open Access to Research and Scholarship", "text": "【0】The Access Principle: The Case for Open Access to Research and Scholarship\nEmerging Infectious Diseases helped pioneer open-access publishing by launching free online and print editions simultaneously in 1995. A decade later, perhaps half of the 50,000 scholarly journals are available online; however, access to the contents generally requires a subscription. Although many journals are experimenting with enhanced-access models, such as offering open access to a small selection of articles (e.g. Lancet) or making archived articles freely available 6–12 months after publication (e.g. New England Journal of Medicine), only ≈20% of all research articles are open access, and many of these are available only as self-archived manuscripts on authors' personal websites. Meanwhile, journal subscription rates continue to escalate, strapping library budgets and restricting circulation.\n\n【1】In this book, John Willinsky, professor of literacy and technology at the University of British Columbia, argues that access to the results of research and scholarship are a public good: information shared is not diminished; in fact, only when shared does it become knowledge. The access principle states that a commitment to research entails a responsibility to circulate the results as widely as possible.\n\n【2】Each chapter in the book presents this principle from a different perspective, making a case for open access on philosophical, ethical, practical, economic, and technical grounds. In each instance, the contentions of open-access critics are carefully dissected, exposed, and refuted with timely and relevant data. Individual researchers concerned about losing prestige and officers of professional societies concerned about losing subscription revenue might find these arguments particularly interesting. For example, the evidence from physics, a field with well-established open-access publishing conventions, suggests that open-access articles are cited more often (i.e. higher impact factor) than those available only to subscribers. Analyses of scholarly association budgets and journal management economics, which appear among the useful appendices at the end of the book, suggest that alternative publishing models could be more cost-effective than the status quo.\n\n【3】The author does not overlook the irony of publishing this work as a book that costs $34.95, but I tend to agree that the book is still the best medium for a \"thoroughgoing treatment of an issue in a single sustained piece of writing.\" This book is for scholars and professionals who are interested in the idea of open access but are not yet convinced. Those who read it are likely to be surprised, engrossed, informed, and perhaps persuaded.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "794b0ef0-b01f-4fc2-aab9-a2b3e01c23c6", "title": "Norovirus Infection in Harbor Porpoises", "text": "【0】Norovirus Infection in Harbor Porpoises\nNoroviruses have been detected in humans, cats, dogs, pigs, sheep, cattle, sea lions, rodents, and bats . In humans, norovirus is a leading cause of gastroenteritis . Seven different norovirus genogroups have been described for the norovirus genus  that can be further subdivided in ≈30 genotypes. Noroviruses comprise a single-strand positive-sense RNA genome that is divided into 3 open reading frames (ORFs). Recombination among different genotypes is frequently observed for human noroviruses, most commonly near the junction of ORF1 and ORF2, leading to a recommendation for multilocus genotyping . Surprisingly, recombinant human noroviruses regularly contain previously undetected ORF1 sequences, which raises questions about the reservoirs of these viruses.\n\n【1】Noroviruses can spread through the fecal–oral route, and sewage contamination in coastal environments can result in contamination of shellfish, such as oysters. Oysters filter several liters of seawater daily and contain histo-blood group antigens resembling those of humans. These antigens can be specifically bound by noroviruses, resulting in bioaccumulation ; as a result, eating oysters is linked to foodborne norovirus outbreaks in humans . This mode of transmission, however, could expose humans to viruses from other animal reservoirs, such as marine mammals, and vice versa.\n\n【2】### The Study\n\n【3】A juvenile male harbor porpoise (Phocoena phocoena_ ) ≈10.5 months of age was found alive on the coast of the Netherlands on May 10, 2012, and was transported to the SOS Dolphin Foundation (Harderwijk, the Netherlands) for rehabilitation . Important clinical signs at the rehabilitation center were anorexia, labored breathing, and disorientation. The animal showed no evidence of gastrointestinal disease, such as vomiting or diarrhea. Eight days after arrival in the rehabilitation center, the animal was euthanized because of the severity of clinical signs, and necropsy was performed according to standard procedures . The main pathology findings were bronchopneumonia associated with lungworm infection and encephalitis and hepatitis of unknown cause. The intestine did not show significant lesions macroscopically; microscopically, the enterocytes at the luminal surface of the intestine had sloughed into the lumen as a result of freeze–thaw artifact. The cells lining the intestinal crypts consisted of a mixture of enterocytes and mucus cells. The proportion of mucus cells increased progressively toward the end of the intestine. The lamina propria was infiltrated diffusely with a moderate number of lymphocytes, plasma cells, and eosinophils. This infiltrate was considered normal for this species.\n\n【4】In the frame of a research program focusing on the identification of new viruses in possible reservoir hosts, we collected fecal material and performed random PCR in combination with 454-sequencing as described previously . This analysis resulted in 5,774 reads, of which 88 reads were most closely related to the norovirus genus, as determined by blastn and blastx analysis . Other reads that were most similar to viral genomes were most closely related to a coronavirus (reads \\[35%–94% nt identity\\]), salmon calicivirus (reads \\[11%–69% nt identity\\]), and porcine anello virus (read \\[91% nt identity\\], 2 reads \\[36% amino acid identity\\]). The harbor porpoise norovirus (HPNV) sequence was confirmed by Sanger sequencing  by using specific primers and comprising 3 ORFs, the partial ORF1 encoding the putative polyprotein, ORF2 encoding viral protein (VP) 1, and a partial ORF3 encoding VP2 . Phylogenetic analysis revealed that the HPNV RNA-dependent RNA-polymerase encoded by ORF1 clustered together with human genogroup I(GI) sequences , whereas VP1 clustered near strains belonging to human GI and bovine GIII .\n\n【5】To determine the tissue tropism of HPNV, we extracted RNA from formalin-fixed paraffin-embedded (FFPE) tissues collected from the HPNV-positive animal for histopathology. Tissues from all main organs and lymph nodes were tested for HPNV RNA by real-time PCR . Only the intestinal tissue and the FFPE material for immunohistochemical analysis (containing a mixture of tissues) were positive for norovirus with cycle threshold values of 30.5 and 37.4, respectively.\n\n【6】We conducted in situ hybridization to determine the cellular tropism of HPNV, as described previously  . HPNV-specific transcripts were detected in the cells in the intestine, indicating that this virus replicates in the intestinal tract. Sequential slides stained with hematoxylin and eosin or pankeratin showed that positive cells corresponded with enterocytes that had sloughed into the intestinal lumen because of freeze–thaw artifact, although we cannot exclude the possibility that other cell types were present .\n\n【7】To estimate the percentage of norovirus-infected harbor porpoises in our dataset, we extracted RNA from FFPE porpoise intestinal tissues collected from 48 animals during a 10-year period. Including the animal in which we detected HPNV, 10% (/49) of the animals were positive for norovirus (cycle threshold <37) , with primers designed to detect HPNV. Macroscopic and microscopic examination of the intestine of these animals showed no pathologic differences from harbor porpoises without norovirus infection.\n\n【8】To detect norovirus-specific antibodies in porpoise serum, we developed an ELISA based on HPNV VP1 and subsequently screened 34 harbor porpoise serum samples collected during 2006–2015 in the Netherlands . Samples from 8 (%) harbor porpoises were positive for norovirus antibodies . This dataset included samples from 2 harbor porpoises that were positive for norovirus RNA; however, their serum samples were negative for norovirus-specific antibodies.\n\n【9】### Conclusions\n\n【10】Similar to human noroviruses, HPNV replicates in the intestine. B cells and enterocytes support human norovirus replication in vitro . We detected HPNV in cells corresponding to enterocytes, and it will be interesting to determine whether these viruses share receptor use with other noroviruses. In humans, norovirus infections are self-limiting in healthy persons but can result in illness and death in high-risk groups . Because the harbor porpoise in which we detected HPNV did not exhibit clinical signs of gastrointestinal disease, norovirus infection probably was not a major factor in the death of this animal.\n\n【11】Remarkably, the HPNV displayed 99% sequence homology to a short (nt) norovirus VP1 sequence detected in oysters . These oysters had been sampled because they were associated with a foodborne gastroenteritis outbreak in Ireland in 2012 . Oyster samples were collected from the restaurant where the outbreak occurred and from their harvesting area. Strains belonging to genotypes GI.1, GI.4, GII.4, GII.3, GII.1, GII.6, GI.2, GII.7, GI.11, and the strain that was homologous to HPNV were detected, although the HPNV-like strain was detected only in oysters from the harvesting area. In the patients, only GI.4, GI.2, GI.6, GII.1, and GII.7 strains were detected, but the fact that noroviruses infecting marine mammals closely related to human noroviruses have been found infecting harbor porpoises and contaminating oysters raises the question of whether HPNV could infect humans through contamination of oysters or other shellfish.\n\n【12】On the basis of our findings that norovirus infections might be a common infection in harbor porpoises from the southern North Sea and the detection of a norovirus in a sea lion , it is not unlikely that noroviruses are common in other marine mammals as well. The high genetic diversity within this genus complicates detection of new noroviruses. The discovery of HPNV and the recent discovery of noroviruses in bats highlight that much still remains to be discovered about animal reservoirs of noroviruses and triggers questions about the zoonotic potential of these viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "73c5b3d9-5449-4b54-bc86-e81226f10e32", "title": "Avian Influenza A(H5N1) Virus in Egypt", "text": "【0】Avian Influenza A(H5N1) Virus in Egypt\nAn unprecedented increase in the number of human infections with the highly pathogenic avian influenza A(H5N1) virus was observed in Egypt during the 2014–15 winter season. The World Health Organization reported that 31 cases were confirmed in 2014, of which 27 were in persons infected as of September . The Ministry of Health and Population in Egypt confirmed 31 cases in 2014 and 88 in January and February 2015. Thus, the official number of cases during September 2014–February 2015 was 114, including 36 deaths. Furthermore, in February 2015, the first human case of subtype H9N2 virus infection in Egypt was reported. These events compelled national and international authorities to examine the reasons behind the increase in human infections and implement control measures.\n\n【1】In Egypt, highly pathogenic avian influenza subtype H5N1 virus was first reported in poultry in 2006 and was declared to be enzootic in 2008 . As an initial response, the government of Egypt devised a comprehensive response plan that included increasing awareness, culling infected poultry, zoning and movement restrictions, and emergency vaccination of parent flocks . However, the virus continued to circulate, and infections were reported in more governorates. The authorities then decided to increase vaccination to cover all commercial flocks and backyard poultry . Eventually, vaccination became the only tool used to control H5N1 virus in Egypt, as other aspects of the control plan became neglected. This strategy failed to control the spread of H5N1 virus, given that outbreaks in poultry continued to occur.\n\n【2】The inadequate control measures enabled H5N1 viruses to mutate. Genetic drift in the hemagglutinin (HA) gene was observed each year and was more profound after 2008, when the virus was declared enzootic . Two subclades of H5N1 viruses, 2.2.1 and 2.2.1.1, co-circulated in poultry from late 2009 through 2011 . Subclade 2.2.1.1 viruses are thought to have emerged as escape mutants because of vaccine pressure . As of 2012, subclade 2.2.1.1 viruses were rarely detected, but subclade 2.2.1 viruses continued to evolve to form a new phylogenetic cluster . Subclade 2.2.1 and 2.2.1.1 viruses were also antigenically distinct .\n\n【3】Most human cases of H5N1 infection in Egypt were caused by infection with subclade 2.2.1 H5N1 viruses, which are abundant in backyard poultry . Epidemiologic analysis of human H5N1 cases reported during 2006–2010 showed that the case-fatality rate was 34% and differed significantly by sex (higher among female patients), age (increased with age), and time to hospitalization (decreased with faster hospitalization) . By 2015, most of the reported H5N1 human cases worldwide were in Egypt (%, 292/784) .\n\n【4】This increase in human infections and the continuous circulation of H5N1 and H9N2 viruses in poultry in Egypt have raised concerns for public health and animal health. Here we analyze the current situation of H5N1 viruses in Egypt. We discuss the evolution of the viruses in poultry, describe the epidemiology of human infection, analyze the effect of poultry vaccination, and provide insight on how to move forward for controlling H5N1 virus circulation in poultry.\n\n【5】### Avian Influenza in Poultry in Egypt\n\n【6】##### Surveillance\n\n【7】In Egypt, a systematic surveillance program for avian influenza viruses has been in place since 2009. The program, a collaborative effort between the Center of Scientific Excellence for Influenza Viruses in Egypt and the St. Jude Center for Excellence for Influenza Research and Surveillance in the United States, provided an important tool for understanding the ecology of avian influenza viruses in poultry in Egypt. Commercial and semicommercial farms, abattoirs, backyard flocks, and live bird markets located in different governorates are sampled on a monthly basis regardless of the presence of disease symptoms in poultry. In each governorate, the same locations are continuously sampled by a poultry veterinarian, who collects 1 swab sample per bird; depending on the size of the poultry population, as many as 5 birds are sampled per flock. Birds are not randomly selected, and samples also are collected from sick or dead birds found onsite. Samples are mostly from chickens, but ducks, geese, turkeys, pigeons, and quails also are sampled. This program enabled detection of genetic and antigenic changes in H5N1 viruses in Egypt. It provided important epizootiologic data and described the poultry sectors acting as reservoirs of H5N1 viruses.\n\n【8】The rate of avian influenza infection during August 2009–July 2010 was 5%, was exclusively attributable to H5N1 infection, and was more concentrated in the commercial production sector . From August 2010 through January 2013, the positivity rate increased to 10%, and the infection was detected in all poultry production sectors . In 2011, H9N2 viruses emerged and were detected by this program and other surveillance activities in Egypt . The monthly positivity rate of avian influenza infection from August 2009 through December 2014  showed that avian influenza infection in poultry follows a seasonal pattern, with sharp increases during the colder months (November through March). After the detection of H9N2 virus in 2011, the positivity rate during colder months was higher than that in the period when H5N1 was the only virus infecting poultry, exceeding 20% in some months. Furthermore, co-infection with H5N1 and H9N2 viruses was frequently detected .\n\n【9】We further analyzed data collected through the surveillance program during February 2013–December 2014, when 4,858 cloacal and 3,049 oropharyngeal samples were collected (range 120–700 samples monthly). The positivity rate for any avian influenza infection was 4.7%. A higher rate of infection was observed in oropharyngeal swab samples. Detection rates differed significantly by governorate, species, and poultry production sector. No significant differences by the birds’ health status or age were observed . The same seasonal pattern observed during August 2009–July 2010  was observed during this period . H9N2 virus was more frequently detected as a single virus causing infection or as co-infecting the same bird with H5N1 virus . Both H5N1 and H9N2 viruses were circulating in the more recent surveillance months, when more human cases were reported.\n\n【10】##### Genetic Analysis\n\n【11】Phylogenetic analysis of all the HA genes of H5N1 viruses in Egypt available in GenBank shows considerable evolution over time . H5N1 was first detected in 2006 and remained relatively stable until 2008. In 2008, subclade 2.2.1.1 emerged and continued in circulation until early 2011. At the same time, clade 2.2.1 viruses also were in circulation. Within this group, further drift was observed, and as of 2011, viruses grouped together and formed a new cluster characterized by a set of mutations . Viruses from late 2013 and 2014 branched together within this new cluster. Some avian H5N1 viruses in 2014 possessed mutations R140K in antigenic site A and A86V in antigenic site E of the HA gene, similar to other viruses in the new cluster. The new cluster was classified as clade 2.2.1.2 .\n\n【12】Phylogenetic analysis of the neuraminidase (NA) gene and the internal genes showed a similar pattern of evolution as that for the HA gene . Currently circulating viruses belong to the new cluster. No major mutations were observed in viruses circulating during 2013–2014.\n\n【13】Genetic analysis of the H9N2 viruses in poultry in Egypt showed that those viruses belonged to the G1 lineage. Also, they possessed several genetic markers of increased transmission to mammalian hosts .\n\n【14】##### Antigenic Analysis\n\n【15】The genetic drift of H5N1 viruses in Egypt led to antigenic variability. When tested against a panel of H5N1 virus monoclonal antibodies, subclades 2.2.1 and 2.2.1.1 are antigenically distinct . A more recent analysis revealed that several strains from 2013 and 2014, especially those with the R140K mutation in the HA gene, had a distinct antigenic composition compared with other viruses of the new cluster .\n\n【16】### Human Infection with H5N1 Viruses\n\n【17】##### Human Cases\n\n【18】During 2006–2015, the estimated number of confirmed human cases of H5N1 infection in Egypt was 292, with a 34% case-fatality rate. The number of reported cases by year for 2006–2015  shows that, before the 2014–15 winter season, the annual number of cases never exceeded 40. However, in just 2 months, January and February 2015, a total of 88 cases were reported. The most recent cases occurred in persons who reported exposure to backyard poultry (%), bred domestic birds (%), slaughtered poultry (%), or were exposed to dead birds (%). The main clinical signs and symptoms were fever (%), sore throat (%), and cough (%). Overall, the case-fatality rate was lower than the 67% calculated for human cases globally, excluding those from Egypt. Within Egypt, the case-fatality rate annually ranged from 10% to 75%.\n\n【19】##### Characterization of Recent Human Viruses\n\n【20】We sequenced the full genomes of 2 H5N1 viruses isolated from infected humans in Egypt during November 2014 (A/Egypt/MOH-NRC-7271/2014 and A/Egypt/MOH-NRC-7305/2014; GenBank accession nos. KP7022162–KP7022177). We also sequenced the HA segment of a third virus, A/Egypt/MOH-NRC-8434/2014 , isolated from a human in December 2014. In addition, we sequenced 2 poultry viruses (A/duck/Egypt/A10353A/2014 and A/chicken/Egypt/A10351A/2014) obtained from the same places around the same time the human cases were detected . The poultry and human viruses branched together. The strains of human influenza viruses from Egypt carried several mutations that were novel or rare in previously circulating strains and have not been characterized before. Two viruses had mutations M66I, I529V, and E249K in the polymerase basic 2 gene. These mutations were present in 1 virus isolated from a chicken in November 2014. Mutations M66I and I529V were previously seen in chicken viruses from January 2014. Mutation I529V was previously seen in a single chicken isolate from 2013.\n\n【21】Sequencing results for the 2 human viruses isolated in November 2014 for which the full genome sequences were obtained showed the presence of mutation G22E in the polymerase basic 1–frameshift 2 (PB1-F2) gene; the mutation was not found in other viruses. Sequencing results also showed 3 unique mutations in the polymerase protein of the 2 viruses: L342M, E351D, and F708L. These mutations were also seen in a chicken virus obtained during the same month that the humans became ill, and mutations L342M and E351D were previously seen in chicken viruses isolated in January 2014. Mutation K373R was common in the HA protein of all 3 human viruses and was previously observed in a chicken virus from the same period and in 2 human viruses isolated in 2009. The NA protein of the 2 fully sequenced human viruses had novel mutations V43I, I94V, V264I, and V304I, which were also present in the chicken isolate of the same period. One novel mutation, R452K, was observed in the nucleoprotein gene of the 2 fully sequenced viruses; this mutation was also seen in the chicken isolate of the same period, 2 chicken isolates from January 2014, and 1 chicken isolate from 2013.\n\n【22】The results of our genetic analysis indicate that the viruses infecting humans in November and December 2014 had a genetic composition almost exactly the same as that of the avian viruses circulating at that time. These human and avian viruses had a set of mutations throughout the genome that places the viruses on the same phylogenetic branch . The role of these mutations, whether individually or in combination, is not known. Determining whether the mutations we found contributed to the recent increase in human H5N1 infections in Egypt would require basic science research that might fall under the “gain of function” category, in which viruses are genetically manipulated under laboratory conditions to study their effects on mammalian hosts.\n\n【23】##### Extent of Avian Influenza Infection in Egypt\n\n【24】In Egypt, the number of reported human cases of avian influenza infection appears to be underestimated. An underestimation might result in an overestimation of the case-fatality rate, but it would certainly underestimate the extent of human infection with avian influenza viruses. Results from a controlled, serologic cohort study of persons in Egypt exposed and not exposed to poultry estimated the seroprevalence of antibodies against H5N1 (titers >80) at 2% . If this seroprevalence were to be extrapolated to the entire poultry-exposed population in Egypt, the true number of infections would amount to several hundred thousand. These figures are even more striking when it comes to human infection with H9N2 viruses. The seroprevalence of H9N2 antibodies detected in the same cohort study  ranged from 5.6% to 7.5%, whereas just 1 case of H9N2 infection was reported.\n\n【25】H5N1 viruses elicit a poor humoral immune response, providing low antibody titers that typically fade over a short period . Thus, relying on serologic testing to detect prevalence or incidence of infection can yield underestimated results. This outcome was evident when we used a microneutralization assay to test serum samples from 38 contacts of persons with confirmed H5N1 infection; no antibodies against H5N1 virus were found, but 5 contacts had low levels (<1:20) of antibodies against H9N2 virus. This finding suggests that the extent of avian influenza infection in humans is even higher than what is currently thought. Human genetic predisposition to infection with avian influenza viruses is an important epidemiologic question that is not well studied, although some reports suggest that genetics play a role in susceptibility to infection . Hence, estimating the true incidence of human infection with avian influenza viruses and determining the accompanying risk factors need further study.\n\n【26】### H5 Influenza Vaccines for Poultry\n\n【27】As of 2006, at least 24 commercial inactivated avian influenza H5 vaccines were licensed for use at poultry farms in Egypt . Different viruses were used as vaccine seed strains, including classical H5 lineage viruses and reverse genetics–engineered reassortant viruses containing H5N1 virus HA and NA genes and the remaining genes from A/Puerto Rico/8/1934(H1N1). Farm owners decide which vaccine to use, if any. Amino acid sequence similarities between vaccine strains and the consensus sequence of H5N1 isolates circulating in Egypt during 2013–2014 ranged from 84.0% to 99.6% of A/chicken/Mexico/232/94 (H5N2) and reverse genetics–engineered A/chicken/Egypt/M2583D/2010 (H5N1), respectively. Serum samples obtained from chickens vaccinated with commercial vaccines or an experimental vaccine based on clade 2.2.1 A/chicken/Egypt/M7217B/2013(H5N1) were tested against H5N1 viruses isolated in Egypt during 2006–2014 . Commercial vaccines showed variable reactivity against earlier antigens, but reactivity declined as the virus mutated. The experimental vaccine was highly reactive with all antigens, especially for more recent viruses. The genetic dissimilarity and poor reactivity between commercial vaccines and currently circulating viruses indicate that the vaccines are not efficacious in the field. These vaccines confer partial protection and thus might lead to vaccine-induced escape mutants, thereby complicating, rather than solving, the problem of H5N1 virus circulation in Egypt. Previous reports have indicated that improper antigenic matching between vaccines and circulating viruses might reduce vaccine efficacy .\n\n【28】All vaccines used in Egypt are licensed by the Ministry of Agriculture and Land Reclamation on the basis of laboratory evaluation results. For influenza virus vaccines, this evaluation involves vaccinating poultry and challenging them with an H5N1 virus. Until recently, the challenge virus was a 2008 H5N1 virus isolate, A/chicken/Egypt/1709-6/2008(H5N1) . Against the 2008 isolate, those vaccines were efficacious in the laboratory setting but not in the field because the circulating viruses were not antigenically matched to the vaccine seed strains, highlighting the failure of the only tool used to control avian influenza among poultry in Egypt.\n\n【29】### Proposed Avian Influenza Control Plan\n\n【30】The avian influenza situation in Egypt is deteriorating, evident in the fact that H5N1 and H9N2 viruses are enzootic in poultry and that incidence of H5N1 and H9N2 infection is increasing. The problem is also evident in the sharp increase in human H5N1 cases and the detection of the first human H9N2 case. Thus, it is imperative that authorities in Egypt devise and implement an aggressive control plan to curb the spread of disease in human and animal populations. The control plan must include the following elements: 1) mapping of the unlicensed, small-scale poultry farms that have become abundant in rural areas; 2) increasing the biosecurity levels of these small farms by using inexpensive tools; 3) revamping veterinary and public health surveillance and conducting joint human–animal interface surveillance and risk-assessment exercises; 4) encouraging poultry owners to report outbreaks and providing them appropriate compensation; 5) intervening, when poultry outbreaks are reported, by culling infected poultry and setting monitoring zones around each focus point; 6) properly decontaminating infected farms; 7) encouraging the use of disinfectants in backyards where poultry are raised; 8) increasing awareness about the effects of avian influenza; 9) testing patients with suspected influenza forH5N1 and H9N2 virus; and 10) reevaluating the vaccination strategy, including that for H9N2 virus. If vaccination is to remain an important tool in the control plan, then the following aspects should be considered: 1) matching vaccine strains to currently circulating strains; 2) matching challenge strains to currently circulating strains; 3) maintaining high vaccination coverage; 4) ensuring vaccine efficacy not only in a laboratory setting but also in the field; and 5) evaluating vaccine efficacy on an annual basis.\n\n【31】### Conclusions\n\n【32】Egypt is one of the few countries where H5N1 virus has become enzootic and is the only country with a high number of H5N1 outbreaks among poultry and cases among human. During the 2014–15 winter season, a sudden and substantial increase in human infection with H5N1 viruses was observed. There is no obvious or confirmed reason for this increase, but data indicate the following: 1) H9N2 virus is co-circulating and co-infecting with H5N1 viruses, 2) H5N1 viruses causing the infections possess some mutations that were rarely seen in the past, and 3) the poultry vaccination program is failing. However, our perspective was limited to the data available through our surveillance program, which might not be representative of the epizootiology of avian influenza virus in Egypt. Regardless of the causes of the recent increase in human H5N1 cases, this situation evolved because of the ineffective control strategy that was implemented. Controlling the situation requires a One Health approach, but certainly the greater share of responsibility now lies with the veterinary side.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c6904f00-1d43-49dc-b6aa-ae8c565fb17f", "title": "Hepatitis E Virus in Pork Liver Sausage, France", "text": "【0】Hepatitis E Virus in Pork Liver Sausage, France\nFoodborne transmission of hepatitis E virus (HEV) to humans from consumption of undercooked pig liver and deer meat has been reported in Japan . In addition, commercial pig livers purchased from local grocery stores in Japan, the United States, and Europe were contaminated with HEV . Epidemiologic and PCR results linked a cluster of autochthonous acute hepatitis E cases to ingestion of raw figatelli, which is a dried, cold-smoked sausage containing ≈30% pig liver . We investigated the viability of HEV in pork liver sausages produced in France.\n\n【1】### The Study\n\n【2】Four samples of pork liver sausage (designated sausages A–D) collected at the final production stage from 4 independent manufacturers in 3 locations in southern France were found to be HEV positive by using real-time reverse transcription PCR (RT-PCR). Samples were tested in 2 institutes, the Animal Health and Veterinary Laboratories Agency, United Kingdom, and Wageningen University and Research Centre, the Netherlands. Three-dimensional (D) cell culture propagation was performed to investigate the presence of infectious HEV particles. PLC/PRF/5 hepatocarcinoma cells (American Type Culture Collection 8024) were cultured as a monolayer (D) at 37°C in GTSF-2 medium  in a 5% CO 2  environment. Cells were trypsinized at 95% confluence and resuspended in fresh medium to a density of 2 × 10 5  cells/mL. Fifty milliliters of cell suspension was then introduced into a rotating wall vessel with 10 mg/mL of porous Cytodex-3 microcarrier beads (collagen type I–coated porous microspheres, average diameter 175 μm (Sigma, Dorset, UK and Zwijndrecht, the Netherlands) and incubated at 37°C in 5% CO 2  . The cells were incubated for \\> 28 d before inoculation to enable complete differentiation.\n\n【3】The inoculum was prepared by homogenizing a 2.5-mg fragment of each sample with mortar and pestle in 5 mL of culture medium. The homogenate was centrifuged at 8,000 × _g_ for 3 min, and the supernatant was filtered sequentially through 1.2-µm, 0.45-µm, and 0.2-µm filters to reduce the risk for bacterial contamination. The medium was removed from the rotating wall vessel, and 2.5 mL of inoculum was incubated with the cells for 2 h at 35.5°C; at that point, 47.5 mL of fresh medium was added. Subsamples of medium (µL) were collected in duplicate, added to 560 µL of lysis buffer (QIAamp Viral RNA Mini Kit; QIAGEN, Crawley, UK, and Venlo, the Netherlands), and stored at −20°C until RNA extraction was performed.\n\n【4】Both institutes performed real-time RT-PCR by using primers and probe as described , using the Superscript III Platinum One-Step Quantitative RT-PCR System (Invitrogen, Paisley, UK, and Bleiswijk, the Netherlands). Negative (water) and positive (extract from positive fecal sample, genotype 3) controls were included.\n\n【5】HEV RNA was detected in the 3D cell culture supernatants of all 4 sausage samples up to 8 d postinfection (dpi). Thereafter, HEV RNA was detected only in the cells inoculated with the sample A homogenate ; it was assumed that the signals from the other 3 samples represented residual inoculum or an abortive infection. The sample A culture showed a cycle threshold (C t  ) value of 27 on the day of inoculation (dpi 0) that increased to 35 on dpi 5, continued to increase until dpi 11, when it peaked at 38 and then began to decrease. A low C t  value of 29 was observed on dpi 44. Similar results were obtained in the laboratory in the Netherlands .\n\n【6】To evaluate the infectivity of progeny viruses from the primary inoculation, supernatant positive for HEV by RT-PCR from dpi 16 was used to infect fresh 3D PLC/PRF/5 cultures according to the protocol described for the primary inoculation. Cells infected with the supernatant from the original sample A homogenate cultures had positive HEV RNA test results on most days after inoculation; C t  values remained relatively constant at an average of 37 from immediately after inoculation (dpi 0) to the end of the experiment at dpi 35.\n\n【7】To compare the cultured virus to the inoculum, a partial fragment of the open reading frame 2 of HEV extracted from culture subsamples (dpi 16, dpi 55; progeny subsample at dpi 35) was sequenced as described . HEV RNA sequences detected in the culture subsamples were characterized and demonstrated 100% identity with that of the inoculum (bp of open reading frame 2) but differed from the control strain used. These sequences were confirmed as HEV genotype 3.\n\n【8】Further confirmation of the presence of viable virus particles in cell culture was sought by using electron microscopy. Supernatants of the cell cultures were applied to Pioloform/carbon-coated, 400-mesh copper grids (Plano GmbH, Wetzlar, Germany) for 10 min, fixed with 2.5% aqueous glutaraldehyde solution for 1 min, and stained with 2% aqueous uranyl acetate solution for 1 min. The specimens were examined through transmission electron microscopy by using a JEM-1010 microscope (JEOL, Tokyo, Japan) at an 80-kV accelerated voltage. HEV particles were observed in the supernatant of the sample A culture collected at dpi 33 . For additional proof by immunoelectron microscopy, an _Escherichia coli_ –expressed, His-tagged HEV genotype 3 capsid protein derivative harboring amino acid residues 326–608  was used to generate HEV-positive serum in a rabbit by 3 subcutaneous inoculations at 4-week intervals (P. Dremsek et al. unpub. data). The immunoelectron microscopy examination of this serum and goat anti-rabbit IgG linked with 5-nm gold particles (BBInternational, Oconomowoc, WI, USA) confirmed the presence of HEV particles .\n\n【9】### Conclusions\n\n【10】We confirmed that 1 sample of pork liver sausage that had positive test results for HEV RNA by real-time RT-PCR contained viable HEV. We cultured 4 sausage samples in 2 different institutes by performing 3D cell-culture propagation of HEV; HEV replication was detected in the same sample independently in both institutes, and the replicated virus was shown to be infectious. We observed entire, cell-free virus particles by transmission electron microscopy at dpi 33, providing further proof of in vitro replication of the virus that contaminated the pork liver sausage.\n\n【11】We conclude that pork liver sausages can contain infectious HEV and that consuming these products should be regarded as a risk factor for HEV infection. Furthermore, we have shown the potential of the 3D culture system to correlate the presence of HEV RNA with the presence of infectious HEV particles.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fa31e0f4-8543-4604-995e-c59ab3001b93", "title": "Recombinant GII.P16-GII.2 Norovirus, Taiwan, 2016", "text": "【0】Recombinant GII.P16-GII.2 Norovirus, Taiwan, 2016\nNorovirus, the leading global cause of epidemic gastroenteritis, is responsible for >90% of all viral gastroenteritis and ≈50% of gastroenteritis outbreaks worldwide . In Taiwan, variants of the genotype GII.4 had been the most prevalent genotype in norovirus-associated acute gastroenteritis outbreaks until mid-2014 . Since winter 2014–15, a previously uncommon genotype, GII.17, became the predominant genotype in Taiwan. This GII.17 strain also caused many outbreaks in Japan and was reported sporadically in other parts of the world . Since September 2016, we have identified a new GII.P16-GII.2 strain that has replaced GII.17 as the predominant strain in norovirus-associated outbreaks in Taiwan, concurrent with a sharp increase in GII.P16-GII.2 norovirus outbreaks and sporadic cases during the 2016–17 season in Germany .\n\n【1】### The Study\n\n【2】During January 2015–December 2016, a total of 876 acute gastroenteritis outbreaks were reported to the Centers for Disease Control in Taiwan. A total of 576 (.8%) outbreaks were identified as norovirus by reverse transcription PCR with primer pairs G1SKF/G1SKR and G2SKF/G2SKR for GI and GII genogroup detection, respectively; all positive samples underwent sequence-based genotyping by using the Norovirus Genotyping Tool  . Before August 2016, GII.2 strains had been detected only sporadically . By September 2016, GII.2 accounted for 60% of norovirus-associated acute gastroenteritis outbreaks that month and, in December 2016, for 86%. Globally, GII.2 is an uncommon genotype that accounted for ≈1.2% of sporadic norovirus infections in children during 2004–2012; by contrast, GII.4 was responsible for 67.2% of norovirus infections during the same period . In 2016, the settings of outbreaks involving GII.2 differed from those of non-GII.2; most (.0%) GII.2 outbreaks occurred in schools, whereas most non-GII.2 outbreaks occurred in restaurants (.4%) and fewer occurred in schools (.0%). Within schools, half of GII.2 outbreaks occurred in kindergartens, 33.3% in elementary schools, and 14.9% in junior/senior high schools and colleges/universities .\n\n【3】Complete or near full-length open reading frame (ORF) 2 (viral protein \\[VP\\] 1) and partial ORF1 (RNA-dependent RNA polymerase \\[RdRp\\]) sequences were further obtained from 11 randomly selected Taiwan GII.2 2016 strains by different outbreak and month, as previously described . However, only 10 and 8 of the RdRp and VP1 sequences, respectively, were of sufficient quality and length for use in analysis. We identified the RdRp region of all GII.2 Taiwan strains as genotype GII.P16 . Phylogenetic trees were inferred from GII.P16 RdRp and GII.2 VP1 sequences, and in both trees, the GII.2 Taiwan strains form 2 major clusters: 2016 GII.2 Taiwan strains and pre-2016 Taiwan GII.2 strains . Before GII.P16-GII.2 strains appeared, GII.P16 polymerases were found in combination with other strains, such as GII.17 , GII.4 Sydney , GII.10 , GII.3 , GII.16  in Japan, Korea, and Europe, and GII.13  in Taiwan . In 2014, GII.P16-GII.2 recombinant strains also were detected sporadically, accounting for only 2 (.1%) of 170 GII strains in South Korea . However, these South Korean GII.P16-GII.2 recombinants (GenBank accession nos. KC110856, KC110857) are more similar to the pre-2016 Taiwan GII.P16-GII.2 strains than to the 2016 recombinants in ORF1 phylogenetic tree . Phylogenetic analysis of VP1 genes of South Korean strains also grouped them with pre-2016 Taiwan GII.P16-GII.2 strains (data not shown). During the 2016–17 norovirus season in Germany, GII.P16-GII.2 strains also were found in nearly half of outbreaks (/65) and sporadic cases (/65) . For the _RdRp_ gene, the closest strains to 2016 Taiwan GII.P16-GII.2 strains are the 2016 Germany GII.P16-GII.2 strains (GenBank accession nos. KY357454, KY357459) and a 2016 Japan GII.P16-GII.4 recombinant strain , which had 98.6%–99.4% identity  . In Japan, during the 2004 and 2007–2010 norovirus outbreaks, GII.2 strains mostly were associated with GII.P2 polymerase, and few that had GII.P16 polymerase were classified as recombinants. These recombinants are more closely related to pre-2016 Taiwan strains than to 2016 strains  . For the VP1 phylogenetic tree, 2016 Taiwan strains were most closely related to the Germany GII.P16-GII.2 strains (GenBank accession nos. KY357459, KY357454) and a Hong Kong GII.2 strain also sampled in 2016 , which had 97.8%–99.0% identity, and more distantly to a 2011 US strain  and a 2012 Japan  strain (% identity for each). The pre-2016 Taiwan strains sampled from 2011–2015 outbreaks that were more closely related to Japan chimeric GII.P16-GII.2 strains that were identified during the 2009–10 and 2012–2014 outbreak seasons in Japan  . In both trees, all Taiwan GII.P16-GII.2 strains from 2016 form a cluster that shares close genetic distance (>99% similarity), regardless of the type of transmission or setting of the outbreaks, and the 2016 Taiwan and Germany GII.P16-GII.2 strains are too closely related to discern the difference, suggesting that these 2016 GII.P16-GII.2 strains descended from a recent ancestor that might have been imported and spread within a short time. Furthermore, the close relation of pre-2016 Taiwan GII.P16-GII.2 strains to Japan GII.P16-GII.2 strains and 2016 Taiwan GII.P16-GII.2 strains to 2016 Germany and Hong Kong GII.P16-GII.2 strains could indicate that the pre-2016 and 2016 waves of outbreaks each derived independently from GII.2 strains that were circulating elsewhere globally.\n\n【4】Analysis revealed that RdRp and VP1 of 2016 and pre-2016 Taiwan strains share higher degree of amino acid identity (>98%) than nucleotide identity (%–93%), indicating more synonymous mutations between the 2 groups. Sites in VP1 that can be used to differentiate between pre-2016 and 2016 Taiwan GII.P16-GII.2 strains were identified as S 71  A, V 335  I, T 344  A, A 354  G, and D 400  E, of which 3 of the sites shared the same residue between the 2016 strains and the 1976 Snow Mountain strain . However, previous studies have shown that for GII.2 strains, antibodies against the 1976 Snow Mountain strain can block and recognize strains as recent as 2010; thus the GII.2 strains have had only limited evolution in antigenic sites . Recent 2016 GII.P16-GII.2 strains became the predominant strains of norovirus-associated acute gastroenteritis in Taiwan , although whether they were involved in outbreaks before 2004 is unknown because norovirus monitoring in Taiwan did not begin until that year. The recent outbreaks illustrated the potential of GII.2 to turn from a sporadically detected strain into an unprecedented dominating strain.\n\n【5】### Conclusions\n\n【6】We report the previously uncommon norovirus genotype GII.P16-GII.2 causing an epidemic in Taiwan in late 2016. Previously, GII.2 strains were infrequently reported among both sporadic cases and outbreaks in Taiwan and globally. Before 2014, variants of GII.4, such as New Orleans 2009 and Sydney 2012, followed this paradigm of emergence and rapid replacement every few years, but no non-GII.4 genotypes predominated until the 2014–15 season, when GII.17 emerged in Asia . The simultaneous detection and close relation of 2016 Taiwan GII.P16-GII.2 strains with 2016 Germany strains is of importance; given that the previous GII.17 epidemic was limited to Asia, the prevalence of GII.P16-GII.2 should be monitored to assess whether this could become the new predominant strains in other parts of the world. Continued surveillance and unified systems for norovirus typing are critical to monitor the emergence and impact of these and other new norovirus strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "61642d20-c17b-4e9b-82ca-394f575d67c8", "title": "Coxiella burnetii in 3 Species of Turtles in the Upper Midwest, United States", "text": "【0】Coxiella burnetii in 3 Species of Turtles in the Upper Midwest, United States\nTwo studies have identified _Coxiella burnetii_ in poikilotherms (vertebrates that cannot regulate body temperature physiologically); both studies originated in India. Two tortoises had antibodies to _C. burnetii_ by capillary agglutination testing of their serum samples in Uttar Pradesh . Additional reptiles, including snakes and skinks, had serum samples positive for _C. burnetii_ in a separate study in Karnataka . Although both studies are useful in clarifying how this bacterium might interface with reptiles, there is no other evidence to support the role played by this large class of vertebrates . Furthermore, serologic assays applied to species that they were not designed for are difficult to interpret .\n\n【1】Serologic testing, typically using indirect immunofluorescence assay, is the primary method used to diagnose _C. burnetii_ infection, which causes Q fever in humans and coxiellosis in domestic ruminants . Additional serologic testing includes complement fixation and ELISA . Serologic assay benefits include commercial availability and insights into acute, treated, and chronic patients, depending on titers . Several PCR-based assays have been developed for detection of _C. burnetii_ in samples from nontraditional mammals, birds, and arthropods . PCR provides a simple and reliable method for detection of the bacterium even retrospectively from tissues . Therefore, we tested turtles from multiple locations in Illinois and Wisconsin, USA, for _C. burnetii_ .\n\n【2】This study was approved by the institutional animal care and use committees of the University of Illinois , Northern Illinois University (LA16–0016), and University of Wisconsin‒Whitewater (K145011020Q). The Wildlife Epidemiology Laboratory, based at the University of Illinois College of Veterinary Medicine, continually conducts long-term, prospective health assessments of several turtle species across Illinois and neighboring states in natural habitats. Reptiles can be an excellent proxy for the health of environments, and many turtle species have small home ranges with diverse diets reflecting local conditions .\n\n【3】As part of these annual surveys, turtle species collected have various morphometric data, blood samples, or oral and cloacal swab specimens obtained before being released. Several diagnostic tests are performed with these samples, such as PCR screening for several pathogens, including _C. burnetii_ . Other pathogenic organisms include _Ambystoma tigrinum_ virus, Bohle iridovirus, Terrapene herpesvirus 1, Terrapene herpesvirus 2, epizootic hematopoietic necrosis virus, _Emydomyces testavorans_ , frog virus 3, Emydid herpesvirus 1, Emydoidea herpesvirus 1 (in Blanding’s turtles), _Mycoplasma agassizii_ , _M. testudineum_ , _Salmonella_ spp. and Testudinid herpesvirus 2 .\n\n【4】We extracted DNA from frozen, combined oral/cloacal swab specimens from each turtle by using the DNA Blood Mini Kit . We assessed spectrophotometrically DNA concentration and purity by using NanoDrop 1000 . We performed quantitative PCR by using a QuantStudio3 Real Time PCR System  and a TaqMan primer‒probe assay targeting the _C. burnetti icd_ gene as described .\n\n【5】We assayed all samples, standards, and nontemplate controls in triplicate and quantified positive samples by using a 7-point standard curve (target copies). Samples were considered positive if all 3 replicates had a lower cycle threshold value than the lowest detected standard dilution. We used a highly sensitive and specific quantitative PCR for _C. burnetti._\n\n【6】During 2019, samples from 5/605 turtles encountered across 8 counties showed positive results for quantitative PCRs, indicating presence of _C. burnetii_ . We collected positive samples from 3 Blanding’s turtles (Emydoidea blandingii_ ), 1 painted turtle (Chrysemys picta_ ), and 1 ornate box turtle (Terrapene ornata_ ). These positive turtles were found in Kane and Lee Counties in Illinois and Sauk County in Wisconsin. We did not perform serologic analysis for these animals. One Blanding’s turtle had a microchip and transmitter, was sampled again during 2020, and showed a negative PCR result. All of these turtles were found within a 1-hour drive to the Illinois‒Wisconsin state border within protected preserves. However, the 3 locations in which the 5 turtles varied in proximity to farms, livestock, industry, residential areas, and major highways; we found no geographic associations. All other screening tests showed negative results for pathogenic organisms for these 5 animals.\n\n【7】_C. burnetii_ is a ubiquitous bacterium that has been found in many different species, often without pathogenicity . A variety of species of turtles are sampled annually in Illinois and surrounding areas through the Wildlife Epidemiology Laboratory. Over time, the testing for various organisms has expanded, especially as additional tests are validated. Screening for the bacterium that causes Q fever has been conducted for many species but infrequently in poikilotherms. These results show that the bacteria can be detected in these species and should be further researched to understand additional sources of this reportable disease, including potential management or regulatory decisions.\n\n【8】Continued investigation and screening in poikilotherms for zoonotic pathogens should be prioritized to understand the potential risk from additional hosts. The pet trade is a potential avenue of risk for exposure between humans and turtles. As these pathogens of concern are better characterized, the implications of different and varied hosts will drive the need for continued One Health research and dialogue between environmental, animal, and human health professionals.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "97830085-35b1-4386-a952-acff90f42449", "title": "Serial Interval of COVID-19 among Publicly Reported Confirmed Cases", "text": "【0】Serial Interval of COVID-19 among Publicly Reported Confirmed Cases\nKey aspects of the transmission dynamics of coronavirus disease (COVID-19) remain unclear . The serial interval of COVID-19 is defined as the time duration between a primary case-patient (infector) having symptom onset and a secondary case-patient (infectee) having symptom onset . The distribution of COVID-19 serial intervals is a critical input for determining the basic reproduction number (R 0  ) and the extent of interventions required to control an epidemic .\n\n【1】To obtain reliable estimates of the serial interval, we obtained data on 468 COVID-19 transmission events reported in mainland China outside of Hubei Province during January 21–February 8, 2020. Each report consists of a probable date of symptom onset for both the infector and infectee, as well as the probable locations of infection for both case-patients. The data include only confirmed cases compiled from online reports from 18 provincial centers for disease control and prevention .\n\n【2】Fifty-nine of the 468 reports indicate that the infectee had symptoms earlier than the infector. Thus, presymptomatic transmission might be occurring. Given these negative-valued serial intervals, COVID-19 serial intervals seem to resemble a normal distribution more than the commonly assumed gamma or Weibull distributions , which are limited to positive values . We estimate a mean serial interval for COVID-19 of 3.96 (% CI 3.53–4.39) days, with an SD of 4.75 (% CI 4.46–5.07) days, which is considerably lower than reported mean serial intervals of 8.4 days for severe acute respiratory syndrome  to 14.6 days  for Middle East respiratory syndrome. The mean serial interval is slightly but not significantly longer when the index case is imported (.06 \\[95% CI 3.55–4.57\\] days) versus locally infected (.66 \\[95% CI 2.84–4.47\\] days), but slightly shorter when the secondary transmission occurs within the household (.03 \\[95% CI 3.12–4.94\\] days) versus outside the household (.56 \\[95% CI 3.85–5.27\\] days) . Combining these findings with published estimates for the early exponential growth rate COVID-19 in Wuhan , we estimate an R 0  of 1.32 (% CI 1.16–1.48) , which is lower than published estimates that assume a mean serial interval exceeding 7 days .\n\n【3】These estimates reflect reported symptom onset dates for 752 case-patients from 93 cities in China, who range in age from 1 to 90 years (mean 45.2 years, SD 17.21 years). Recent analyses of putative COVID-19 infector–infectee pairs from several countries have indicated average serial intervals of 4.0 days , 4.4 days (95% CI 2.9–6.7 days, n = 21; S. Zhao et al. unpub.5 days (% CI 5.3–19, n = 6; _8_ ). Whereas none of these studies report negative serial intervals in which the infectee had symptoms before the infector, 12.6% of the serial intervals in our sample were negative.\n\n【4】We note 4 potential sources of bias. First, the data are restricted to online reports of confirmed cases and therefore might be biased toward more severe cases in areas with a high-functioning healthcare and public health infrastructure. The rapid isolation of such case-patients might have prevented longer serial intervals, potentially shifting our estimate downward compared with serial intervals that might be observed in an uncontrolled epidemic. Second, the distribution of serial intervals varies throughout an epidemic; the time between successive cases contracts around the epidemic peak . A susceptible person is likely to become infected more quickly if they are surrounded by 2 infected persons instead of 1. Because our estimates are based primarily on transmission events reported during the early stages of outbreaks, we do not explicitly account for such compression and interpret the estimates as basic serial intervals at the outset of an epidemic. However, if some of the reported infections occurred amid growing clusters of cases, then our estimates might reflect effective (compressed) serial intervals that would be expected during a period of epidemic growth. Third, the identity of each infector and the timing of symptom onset were presumably based on individual recollection of past events. If recall accuracy is impeded by time or trauma, case-patients might be more likely to attribute infection to recent encounters (short serial intervals) over past encounters (longer serial intervals). In contrast, the reported serial intervals might be biased upward by travel-related delays in transmission from primary case-patients that were infected in Wuhan or another city before returning home. If their infectious period started during travel, then we might be unlikely to observe early transmission events with shorter serial intervals. The mean serial interval is slightly higher for the 218 of 301 unique infectors reported to have imported cases.\n\n【5】Given the heterogeneity in type and reliability of these sources, we caution that our findings should be interpreted as working hypotheses regarding the infectiousness of COVID-19, requiring further validation. The potential implications for COVID-19 control are mixed. Although our lower estimates for R 0  suggest easier containment, the large number of reported asymptomatic transmission events is concerning.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8ed255b7-e68f-42ca-844f-61bbd1addc76", "title": "Haemophilus aphrophilus Endocarditis after Tongue Piercing", "text": "【0】Haemophilus aphrophilus Endocarditis after Tongue Piercing\nBody piercing poses a risk for serious disease. Because it invades subcutaneous areas, piercing has a high potential for infectious complications. Such complications result from introduction of skin or mucous membrane microflora into subcutaneous tissue or from the ongoing presence of colonies of these microflora at the piercing site. Pain, edema, and prolonged bleeding may occur immediately after piercing  , and a cyst, scar, or keloid may form at the piercing site. In various surveys, the rate of earlobe piercing infections alone has been estimated at 11% to 24%. Skin lesions or anatomic abnormalities at the site of piercing, as well as valvular heart disease, are risk factors for complications  . Staphylococcal endocarditis of the mitral valve after nasal piercing  , _Neisseria_ endocarditis after tongue piercing  , and _Staphylococcus epidermidis_ endocarditis and mastitis following nipple piercing have been reported  . Even though a consistent correlation is not known between piercing and endocarditis, the number of case reports is increasing, and a correlation may well exist.\n\n【1】Persons at high risk for complications should be treated with preventive antibiotics, just as persons at high risk for complications receive antibiotic treatment before dental procedures. The correlation between dental procedures and endocarditis has been reviewed by Van der Meer et al. who prospectively examined all cases of infective endocarditis in the Netherlands over a 2-year period  . Of 427 patients who had been hospitalized, 64 had previous dental or other procedures in the preceding 3 months. Using 48 of these 438 patients as study cases (only 48 patients met the qualification of having native-valve and cardiovascular anomalies that increased their risk of getting endocarditis, these researches found no significant difference in presence of dental procedures between patients and matched controls without endocarditis (odds ratio 1.2, 95% confidence interval 0.03 to 2.3). Two other studies  reported similar results. No study has examined the correlation between piercing and endocarditis.\n\n【2】In the United States, body piercing, which is becoming increasingly common, is mainly performed by unlicenced practitioners. Only 26% of states have regulatory authority over tattooing establishments, and only six of these states exercise authority over body-piercing establishments. Piercing occurs in regulated and unregulated shops, department stores, jewelry shops, homes, or physicians’ offices. Generally no antibiotic is used, and sterilization methods vary. Studies show that ear piercing can cause cephalic tetanus (a local form of tetanus caused by wounds or other head and neck infections)  , _Pseudomonas_ infections, or perichondrial auricular abscesses, especially with _Pseudomonas aeruginosa._ Tongue or oral piercing can cause Ludwig’s angina  or may be complicated by normal oral flora, such as _Haemophilus aphrophilus_ , as in this case. Genital piercing may result in _Escherichia coli_ infection and may increase the risk for sexually transmitted diseases through tissue damage and exposure and unwanted pregnancy because of condom rupture  . Systemic infections, such as toxic shock syndrome or sepsis, have also been reported  . Among noninfectious cases, granulomatous perichondritis of the nasal ala, sarcoidlike foreign body reaction from multiple piercing, paraphimosis from a distal penis pierce, and speech impairment, together with difficulty in chewing and swallowing from oral jewelry, have been reported . Metal-associated problems include allergy (especially to nickel), eczematous rash, and lymphocytoma . We describe an incidence of _H. aphrophilus_ endocarditis following tongue piercing.\n\n【3】### Case Report\n\n【4】A 25-year-old man arrived at Memorial Health University Medical Center with fever, chills, rigors, and shortness of breath of 6 days duration. He had a history of aortic valvuloplasty at 8 years of age for correction of congenital aortic stenosis. At admission, the patient had fever of 38.9°C and a grade III/VI ejection systolic murmur accompanied by a grade II/VI diastolic blowing murmur best heard in the left sternal border area. The oral cavity was pink, and no inflammation or exudates were noticed on the pharynx. The middle portion of the tongue had been pierced, and a bispherical stud was in place . The piercing was performed 2 months before onset of illness. Extensive tattoos on the shoulders, arms, and upper torso dated back 3 years. The patient had previous dental work done but always with antibiotic prophylaxis.\n\n【5】Laboratory tests showed erythrocyte sedimentation rate of 41 mm/hr (normal rate, 0–15 mm/hr) and elevated C-reactive protein of 5.1 mg/dL (normal level 0–1). Transthoracic echocardiography was not conclusive; a transesophageal echocardiogram showed remnants of a bicuspid and deformed aortic valve with multiple vegetative lesions. Blood cultures were obtained, and the patient was started on triple antibiotics (ampicillin, nafcillin, and gentamycin). Wet preparation and acridine orange stain of the blood specimen showed gram-negative pleomorphic rods. Two of the conventional chocolate-agar cultures turned positive approximately 4 days after incubation and were consistent with _H. aphrophilus_ (β_ \\-lactamase negative, lactose fermenting, and Mannose fermenting). The stud culture was also positive for _H. aphrophilus_ . Antibiotics were modified because of sensitivity to ceftriaxone and gentamycin, and the patient was discharged to complete the 6-week course through a peripherally inserted central catheter line at home. Aortic valve replacement was recommended after completion of antibiotic therapy, but the patient did not return for treatment.\n\n【6】### Conclusions\n\n【7】Our case demonstrates _H. aphrophilus_ endocarditis possibly caused by tongue piercing (or as a complication of the ongoing presence of the stud) in a patient with congenital heart disease. Colonization around the stud likely caused bacteremia and endocarditis. _H. aphrophilus_ is commonly isolated from the upper respiratory tracts of humans and animals; however, its prevalence is unknown. In a previous study of piercing complications in patients with congenital heart disease  , 43% of the study population had earlobe piercing; of these, 6% took antibiotics before piercing. Twenty-three percent of patients had piercing-related infections 1 week to 3 years after piercing. Most infections were local skin infections; no endocarditis was reported in that study.\n\n【8】Until prospective randomized studies shed light on the relationship between piercing and endocarditis, prophylactic measures are indicated and should be formulated, particularly for persons at high risk, e.g. those with structural heart diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a0b74c4-2b4e-4ec4-9977-80e0d3721ae3", "title": "Culex torrentium Mosquito Role as Major Enzootic Vector Defined by Rate of Sindbis Virus Infection, Sweden, 2009", "text": "【0】Culex torrentium Mosquito Role as Major Enzootic Vector Defined by Rate of Sindbis Virus Infection, Sweden, 2009\nIn Sweden, Finland, Russia, and South Africa, Sindbis virus (SINV; family _Togaviridae_ , genus _Alphavirus_ ) is an etiologic agent for outbreaks of rash and long-lasting polyarthritis . Ecologically, SINV is a zoonotic mosquitoborne virus that naturally circulates in bird populations but only incidentally infects humans . Previous detections and isolations of SINV from field-collected mosquitoes identified the ornithophilic mosquitoes _Culex pipiens_ / _Cx_ . _torrentium_ and _Culiseta morsitans_ as possible enzootic vectors of SINV and the generalist mosquitoes _Aedes cinereus_ and _Ae. rossicus_ , which feed on birds and humans, as potential bridge vectors for transmission of the virus from viremic birds to humans (J.C. Hesson, J.O. Lundström, unpub. data). However, female _Cx. torrentium_ and _Cx. pipiens_ mosquitoes are morphologically indistinguishable, so all previous virus isolates from these species were from pools that may have contained both species. The distinction between _Cx. torrentium_ and _Cx. pipiens_ is necessary because vector competence experiments show great differences between the capacities of the 2 species to become infected with and to transmit SINV . _Cx. torrentium_ is highly superior to _Cx. pipiens_ as a vector of SINV in the laboratory , but the extent to which the 2 species are infected in nature is unclear.\n\n【1】We determined the natural SINV infection rates (IRs) in _Culex_ mosquitoes, which were identified by using a newly developed molecular method for reliable identification of _Cx. torrentium_ and _Cx. pipiens_ mosquitoes . We also studied the simultaneous occurrence of SINV in _Cs. morsitans_ mosquitoes.\n\n【2】### The Study\n\n【3】Every 2 weeks during July 13–September 13, 2009, we collected adult female mosquitoes by using 35 miniature light traps from the US Centers for Disease Control and Prevention (Atlanta, GA, USA) baited with carbon dioxide; the traps were set within the regular mosquito surveillance area of the River Dalälven floodplains in central Sweden . Mosquitoes were kept cold on a chilled table during morphologic identification and stored at −80°C. Legs were removed from mosquitoes morphologically identified as _Cx. pipiens_ / _Cx. torrentium_ and used for DNA extraction, enabling identification of the individual specimens to species by using a previously described molecular method .\n\n【4】RNA was extracted from 668 mosquito bodies without legs (Cx. torrentium_ , 367 _Cx. pipiens_ ) and from 74 pools of mosquitoes pooled by collection trap and week (Cs. morsitans_ ); pool sizes ranged from 1 to 19 mosquitoes. The mosquitoes were processed for RNA extraction, real-time reverse transcription PCR (rRT-PCR), and virus isolation on Vero cells as previously described .\n\n【5】First, all samples were screened by SINV rRT-PCR by pooling 2–10 RNA extractions by species and collection week. Of 81 total pools, 14 were positive for SINV RNA. Nine positive pools were from _Cx. torrentium_ mosquitoes, and 3 and 2 pools, respectively, were from _Cx. pipiens_ and _Cs. morsitans_ mosquitoes. Second, all individual samples from the SINV-positive pools from the first screening were subjected to another rRT-PCR, so that individual (Culex_ ) or smaller pools of (Culiseta_ ) mosquitoes were ultimately tested for SINV RNA. The second rRT-PCR showed 16 samples positive for SINV RNA. One of the positive _Cx. torrentium_ pools, which contained samples from 10 mosquitoes, included samples from 3 SINV RNA–positive mosquitoes. Thus, 11 of 301 individual _Cx. torrentium_ and 3 of 367 individual _Cx. pipiens_ mosquitoes were positive for SINV RNA. For _Cs. morsitans_ mosquitoes, 2 of 74 pools were positive for SINV RNA .\n\n【6】Because individual _Cx. torrentium_ and _Cx. pipiens_ mosquitoes were tested, the IRs were calculated as  × 1,000; differences between the species were tested for significance by using a χ 2  test. The actual species-specific IR differed significantly between species (p = 0.01): 36.5, 8.2, and 21 infections/1,000 mosquitoes for _Cx. torrentium_ , _Cx. pipiens_ , and mixed species (Cx. pipiens and Cx. torrentium_ ), respectively. _Cs. morsitans_ mosquitoes were tested in pools, so we calculated the minimum IR (MIR) as  × 1,000; the calculated MIR was 6.9 infections/1,000 _Cs. morsitans_ mosquitoes. A comparison of MIR and maximum-likelihood estimates of infection gave similar estimates.\n\n【7】SINV was successfully isolated from all 16 rRT-PCR–positive mosquito samples . Part of the SINV E2 envelope glycoprotein gene was sequenced as previously described . The phylogenetic analysis included all genomic sequences obtained in this study together with previously published sequences for 63 other virus strains . The results showed that all new strains belonged to the SINV-I genotype and have close relationships with strains from Europe, the Middle East, and South Africa .\n\n【8】### Conclusions\n\n【9】We describe information on the actual occurrence and IR of SINV-I in the enzootic mosquito vectors _Cx. torrentium_ and _Cx. pipiens_ , reliably identified to species level. The significantly higher SINV-I IR observed for field-caught _Cx. torrentium_ than _Cx. pipiens_ mosquitoes is a key addition to the previous findings of the extreme susceptibility of _Cx. torrentium_ mosquitoes to SINV-I . Experimental transmission studies showed that all infected _Cx. torrentium_ mosquitoes could transmit the virus upon refeeding on a susceptible animal . Thus, the observed natural IR of 36.5 infections/1,000 _Cx. torrentium_ mosquitoes translates to 36.0 mosquitoes/1,000 being able to transmit SINV-I. In contrast, because only one third of infected _Cx. pipiens_ mosquitoes can transmit SINV-I upon refeeding, the observed natural IR of 8.2 infections/1,000 _Cx. pipiens_ mosquitoes translates to only 2.0 mosquitoes/1,000 being able to transmit the virus .\n\n【10】The observed SINV-I IRs are very high for both species, and the IR for _Cx. torrentium_ is among the highest ever reported for mosquitoes. This could partly be attributed to the fact that single mosquitoes were analyzed, as compared with the more common technique of pooling. This higher IR would have remained undetected if only pooled mosquitoes were analyzed, even though our original pools consisted of only 10 individual mosquitoes. _Cx. pipiens_ mosquitoes are generally considered a secondary enzootic vector of SINV because, as in Sweden, they are less frequently found infected in nature in South Africa, Israel, and Saudi Arabia, where _Cx. univittatus_ mosquitoes are the main SINV vector .\n\n【11】Because of the exceptionally high SINV IR for field-caught _Cx. torrentium_ mosquitoes in Sweden, the outstanding vector competence results for the species in the laboratory , and its status as the dominating _Culex_ species in SINV-endemic areas of Europe , _Cx. torrentium_ can now be identified as the main enzootic vector of SINV-I in Scandinavia. In areas of Sweden and Finland where clinical SINV-I infections are most prevalent, _Cx. torrentium_ mosquitoes account for >90% of the _Cx. pipiens/Cx. torrentium_ population; in central Europe, where the virus is more uncommon in mosquitoes and no human cases have been observed, both species are equally common . Thus, a large population of _Cx. torrentium_ mosquitoes may be a prerequisite for the intense enzootic transmission of SINV-I that is needed to increase the risk for spillover infections in humans. Whether the _Cx. torrentium_ mosquitoes are also to be considered a vector of other mosquitoborne bird viruses remains to be investigated. In continental Europe, West Nile virus and Usutu virus are emerging, and it is unknown if _Cx. torrentium_ has a vector role for these viruses is unknown because of the lack of transmission experiments and isolation attempts from reliably identified female _Cx. pipiens/Cx. torrentium_ mosquitoes. Knowledge from such experiments and isolation attempts would be especially valuable for northern and central Europe where _Cx. torrentium_ is the dominating candidate enzootic vector species for bird-associated viruses .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f714b2d3-da9c-440b-9798-7e0d71872855", "title": "Modulatory Effect of Cattle on Risk for Lyme Disease", "text": "【0】Modulatory Effect of Cattle on Risk for Lyme Disease\nThe tick Ixodes ricinus, which transmits the spirochete agent of Lyme disease, is abundant in the ecotone at a forest's edge. The brushy vegetation there provides a suitable microclimate for these vector ticks as well as food and cover for the animals that serve as hosts for the ticks' subadult stages. Of those, rodents and particular passerine birds serve as reservoirs for the agent of Lyme disease . Other animals, such as domestic ungulates, serve as hosts for the tick but fail to support the pathogen. These animals exert a zooprophylactic effect on the transmission cycle by diverting ticks from feeding on reservoir-competent hosts. Any tick feeding on these hosts fails to acquire spirochetes and, thereby, fails to contribute to transmission . In addition, infected ticks may lose the pathogen when feeding on such incompetent hosts . The ecotone constitutes the focus of transmission, and persons may acquire numerous vector ticks when passing through such a site. The local composition of vertebrate animals serving as hosts to vector ticks determines the risk for infection at that site.\n\n【1】Recently, the European Union and the United Nations Educational, Scientific and Cultural Organization (UNESCO) instituted programs to limit the growth of brush and subsequent reforestation of uncultivated land. These programs specifically target the ecotonal habitat that developed ubiquitously in Europe when cultivating marginal land was considered too labor-intensive and costly. Toward this end, traditional farming methods are being reimplemented. Domestic ungulates, for example, are being introduced into the ecotone for landscape management. By grazing or browsing on scrub brush and saplings, these animals help maintain open landscapes and simultaneously enhance biodiversity. Although domestic animals may alter the contours of the ecotone, the effect of their presence on the risk for human infection by the agent of Lyme disease has not been defined.\n\n【2】To determine whether the presence of domestic ungulates may reduce risk for human Lyme disease, we compared the prevalence of spirochetal infection in ticks taken from an ecotonal cattle pasture to that in a site in which no cattle were permitted to graze. In addition, we determined the prevalence of the different Lyme disease genospecies in these ticks and that of a distantly related spirochete.\n\n【3】### Materials and Methods\n\n【4】The study site, ≈2 km 2  , is located near the town of Lembach, in the northern Vosges region of France, at the floodplain of the Sauer River. It is part of the Pfälzer Wald–Vosges du Nord biosphere reserve of UNESCO. Scottish Highland cattle were introduced there in 1999 for low-intensity grazing. About 250 cattle are allowed to graze in the floodplain year-round and are rotated on grazing patches enclosed by electric fencing on a total area of ≈200 km 2  . The fence does not constitute a barrier to other vertebrates, such as hedgehogs, foxes, or deer.\n\n【5】On the cattle pasture and at a site 10 m outside the pasture, questing ticks were sampled by passing a flannel flag over the vegetation; ticks were preserved in 80% ethanol. We sampled the sites twice, in early May and early June 2005. Ticks were collected from 3 distinct areas within and 2 areas outside the pasture site. Both sites are separated by a path ≈4 m wide. Microscopy was used to identify stage and species of all ticks.\n\n【6】To detect and identify the various spirochetes that may be present in these ticks, the opisthosoma of each was opened, the contained mass of soft tissue was dissected out and placed in physiologic saline, and DNA was extracted as described previously . Borrelia genospecies were detected and characterized by amplifying and sequencing a 600-nt fragment of the gene encoding the 16S rRNA by nested PCR . Each resulting sequence was compared with sequences of the homologous gene fragment representing each of the validated spirochete genospecies. The following sequences served for comparison: GenBank accession nos. X85196 and X85203 for B. burgdorferi s.s. X85190, X85192, and X85194 for B. afzelii; X85193, X85199, and M64311 for B. garinii; X98228 and X98229 for B. lusitaniae; X98232 and X98233 for B. valaisiana; AY147008 for B. spielmanii; and AY253149 for B. miyamotoi. A complete match, permitting no more than 2 nt changes, was required.\n\n【7】### Results\n\n【8】To determine the effect of the presence of cattle on the prevalence of spirochetal infection in vector ticks, we sampled questing ticks within a fenced site where cattle grazed, examined them individually for spirochetal DNA by PCR and compared the resulting prevalence to that in ticks sampled from at least 10 m beyond the fence. Spirochetes infected ≈4× as many nymphs and ≈6× as many adults outside the enclosure as within  (Fisher exact test, p<0.0001 for either developmental stage). The variance between different samples taken within the pasture site and the variance between samples taken outside the pasture were considerably smaller than the variance between the pasture and the nonpasture site. Fewer questing ticks harbored spirochetes where cattle were present than where they were absent.\n\n【9】We then determined whether the presence of cattle affects a spirochetal genospecies more than others. Amplified spirochetal DNA derived from each infected tick was sequenced and the genospecies determined by comparison with published sequences. Where cattle were present, virtually no nymphal or adult ticks were infected by B. valaisiana (Fisher exact test, p = 0.006, p = 0.05, respectively). The prevalence of B. lusitaniae in ticks sampled on the cattle pasture was about one tenth the prevalence in ticks sampled outside the pasture (Fisher exact test, p = 0.025 in nymphs, p = 0.0025 in adults). B. garinii infected about one twentieth as many nymphal ticks and about one eighth as many adult ticks where cattle were present compared with the nonpasture site (Fisher exact test, p<0.0001 for either developmental stage). Whereas the prevalence of B. afzelii in nymphal ticks appeared to be not significantly affected by the presence of cattle, its prevalence in adult ticks was reduced to approximately one quarter (Fisher exact test, p = 0.0019). Too few ticks infected by B. burgdorferi s.s. or B. spielmanii were sampled to permit comparison. In contrast to the prevalence of Lyme disease spirochetes, the prevalence of B. miyamotoi, an ixodid-borne member of relapsing fever spirochetes, was similar, regardless of the presence of cattle. We conclude that the presence of cattle appears to reduce the prevalence of Lyme disease spirochetes in vector ticks but appears not to influence that of a distantly related spirochete.\n\n【10】### Discussion\n\n【11】A vertebrate animal originally was said to be zooprophylactic if its presence in an endemic site serves to divert invertebrate vectors of a human pathogen from feeding on people . The concept was extended for Lyme disease to include more specific diversionary effects on the force of transmission of the pathogen, including diversion of the vector tick from feeding on potential reservoir hosts, transportation of vector ticks away from the venue of transmission, or elimination of infection from a previously infected tick . Lizards, for example, fail to support Lyme disease spirochetes, and the prevalence of infection in questing vector ticks appears reduced where such hosts are abundant . Ticks may detach from birds in nonsuitable habitats. Zooprophylaxis may affect the prevalence of infection in vector ticks by various factors.\n\n【12】Wild ungulates are generally thought to contribute to risk for Lyme disease, mainly by supporting the population of vector ticks. Indeed, adult I. dammini (or scapularis) ticks in the northeastern United States appear to preferably feed on white-tailed deer . Because they serve as definitive host for the tick, density of deer correlates with tick abundance . Reduction or elimination of deer on islands or their exclusion by electric fences has resulted in fewer subadult ticks questing in subsequent years; the density of adult ticks, however, has declined only slowly . In contrast, a simulation model indicates that the density of ticks may be more sensitive to the availability of hosts for the subadult stages than to that of hosts for adult ticks . In Europe, diverse medium-sized and large mammals, such as hares, hedgehogs, and foxes, constitute alternative hosts for adult ticks . As a result, any association between the abundance of vector ticks and the density of deer may be less profound in Europe than in eastern North America.\n\n【13】Domestic as well as wild ungulates exert a powerful zooprophylactic effect on Lyme disease spirochetes because they are incompetent for these pathogens. Not only do noninfected ticks that feed on these mammals fail to acquire spirochetes, but also infected ticks lose their infection during the course of such a bloodmeal . The sensitivity to the bacteriolytic activity of the complement pathway may determine the incompetence of a host . Cattle support the feeding of numerous ticks of all stages  and thereby divert subadult vector ticks from feeding on reservoir-competent hosts. A simulation model indicated that the availability of incompetent hosts for subadult tick stages would reduce prevalence of infection . Our observation that questing nymphal and adult ticks collected on a cattle pasture were less likely to be infected by Lyme disease spirochetes than were vector ticks collected outside the pasture supports this model. In addition, cattle and other domestic ungulates may modify the ecotonal vegetation by their grazing. This would render the microclimate more arid and, thereby, less suitable for the survival of ticks. Indeed, during equal time periods, we collected approximately half as many questing ticks where cattle were present as we did where cattle were absent (tick every 4 minutes vs. 2 minutes, respectively). Grazing cattle may also deplete reservoir-competent rodents and birds foraging in this ecotone of their food source and of cover from predators. By reducing the availability of potential reservoir hosts, additional subadult stages are diverted to feed on incompetent domestic ungulates. Whether the effect of cattle on the prevalence of spirochete infection in questing vector ticks results from the inability of cattle to support the pathogen or from their effect on tick habitat and reservoir rodents remains to be determined. In addition to their parasitologic effects, certain zooprophylactic animals may modify the ecotonal landscape in ways that affect transmission of the agent of Lyme disease.\n\n【14】Although the presence of cattle appears to suppress the prevalence of the various genospecies of Lyme disease spirochetes, that of B. miyamotoi is not affected. It is more closely related to spirochetes that cause relapsing fever than to spirochetes that cause Lyme disease . No disease relationship has yet been demonstrated for this microbe. B. miyamotoi spirochetes infect ≈2%–5% of questing nymphal or adult I. ricinus–like ticks, regardless of location . The prevalence of Lyme disease spirochetes, on the other hand, varies from zero to ≈50%, depending on the local composition of reservoir populations. Our observation that the prevalence of B. miyamotoi is insensitive to the presence of cattle corroborates reports on transovarial transmission  and suggests that this spirochete may cycle solely within the vector population.\n\n【15】The European Union and UNESCO support various policies for the deintensification of agriculture. The European Union's Biodiversity Action Plan for Agriculture, for example, recognizes that traditional nonintensive agriculture, as it was practiced for centuries until only a few decades ago, maintained an enormous species variety in wild and domestic flora and fauna . By encouraging extensive agricultural practices, such programs aim to prevent further abandonment of marginal farmland and the subsequent development of scrub brush and forests that support relatively few species. Management of previous fallow land as low-intensity pastures for robust varieties of cattle, sheep, goats, horses, or pigs helps maintain or reestablish not only open landscapes but also high nature value habitats (high biodiversity achieved by extensive farming). Our finding that the prevalence of Lyme disease spirochetes in questing vector ticks is significantly reduced on such pastures suggests that enhanced biodiversity may also contribute directly to human health. Conclusions derived from a mathematical model similarly suggest that a species-rich composition of vertebrates that serve as hosts for vector ticks would decrease the prevalence of spirochete-infected ticks by diluting the effect of reservoir-competent hosts . Efforts aimed at extensive landscape management may help to reduce risk for Lyme disease in central Europe.\n\n【16】We conclude that cattle appear to modulate risk for Lyme disease by reducing the prevalence of Lyme disease spirochetes in vector ticks; they do not, however, influence prevalence of a distantly related spirochete. Our observations were conducted in a high nature-value habitat that included an extensively managed cattle pasture. In addition to the direct parasitologic effects, certain zooprophylactic animals may modify the landscape in ways that affect the transmission of Lyme disease spirochetes. Removal of ground-covering vegetation may reduce the force of transmission of this pathogen by reducing local rodent density, thereby diverting vector ticks to reservoir-incompetent cattle. Zooprophylaxis may include environmental modification. Our finding that the prevalence of B. miyamotoi is insensitive to the presence of cattle suggests that this spirochete may be perpetuated differently than are Lyme disease spirochetes. Extensive landscape management in central Europe may help reduce risk for Lyme disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0da09b5e-d106-4cf6-9f7d-3b773606c4ef", "title": "Extensively Drug-Resistant New Delhi Metallo-β-Lactamase–Encoding Bacteria in the Environment, Dhaka, Bangladesh, 2012", "text": "【0】Extensively Drug-Resistant New Delhi Metallo-β-Lactamase–Encoding Bacteria in the Environment, Dhaka, Bangladesh, 2012\nCarbapenemases, bacterial enzymes that typically inactivate most of the β-lactam class of antimicrobial drugs, have emerged rapidly over the past decade . These resistance mechanisms are often accompanied by other resistance alleles, and together they can confer extensive drug resistance, leaving minimal treatment options . The New Delhi metallo-β-lactamase variant 1 (NDM-1), a chimera formed by the fusion of 2 resistance genes, is unique among the carbapenemases . Since its description in 2009, NDM-1 has spread rapidly to many countries worldwide and appears to be endemic in South Asia . A study of the environment in New Delhi, India, showed that ≈30% of surface waters and sewage was contaminated with NDM-1; the enzyme was also detected in drinking water . In addition, high rates of NDM-1 gut carriage have been found in the community and in hospitals in Pakistan . High rates of gut carriage can lead to contamination of drinking water and food through inadequate sewage treatment. Furthermore, gut carriage of NDM-1–encoding _Escherichia coli_ can lead to common community-acquired infections (e.g. urinary tract infections), which often require hospitalization  and enable resistance mechanisms to move between community and hospital sectors. Indirect studies in 2009 and 2010 showed that NDM-1 was not present in the Bangladesh environment . To determine whether NDM-1 is now present in Bangladesh, we surveyed the environmental waters of Dhaka.\n\n【1】### The Study\n\n【2】During October 19–27, 2012, we collected environmental water/sewage samples from 7 regions (sites) in Dhaka, Bangladesh . Control samples were from the United Kingdom. Each sample was investigated for bacterial growth on UTI brilliance agar plates (Thermo Fischer Scientific, Basingstoke, UK) containing vancomycin (mg/L) plus meropenem (.5 mg/L). The species of individual colonies of different colors and morphologies were determined by using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry. Bacteria were genetically characterized by _bla_ NDM-  specific PCR. Genetic location of the _bla_ NDM-1  gene was determined by probing S1 nuclease pulsed-field gels. A subset of isolates of each species was further investigated for MICs of relevant antimicrobial drugs. All _E. coli_ isolates were genotyped to determine multilocus sequence typing group; examples of each group were characterized for additional relevant resistance mechanisms. Details are provided in the Technical Appendix .\n\n【3】The carbapenemase and extended-spectrum β-lactamase genes _bla_ NDM-1  and _bla_ CTX-M-15  were detected by PCR in 36 (%) and 41 (%), respectively, of the 58 water samples. Both genes were found at all 7 sample region sites in Dhaka. Gene _bla_ CTX-M-15  , but not _bla_ NDM-1  , was detected in sewage samples from the United Kingdom; neither was detected in UK water samples from the River Thames.\n\n【4】We identified 226 gram-negative NDM-1–producing isolates to the species level ; 15 isolates harboring _bla_ NDM-1  could not be identified and were not investigated further. The most widely disseminated bacteria in samples from Dhaka were pseudomonads (/7 regions) and _Klebsiella pneumoniae_ (/7 regions). Nine different species of _Pseudomonas_ spp. and 5 _Acinetobacter_ spp. mostly belonging to nonpathogenic strains, were among the nonfermentative bacteria . Carbapenem resistance in the _Pseudomonas_ spp. isolates was unstable; all strains lost the _bla_ NDM-1  gene after 2 days’ growth or when frozen for storage.\n\n【5】With the exception of 4 isolates, all bacterial isolates contained the original _bla_ NDM-1  allele; 3 _E. coli_ sequence type (ST) 101 isolates carried the _bla_ NDM-3  variant, and 1 ST648 isolate carried the _bla_ NDM-4  variant . S1 nuclease pulsed-field gel electrophoresis combined with _bla_ NDM-1  probes detected _bla_ NDM-1  on plasmids of limited size diversity in _E. coli_ (ST101, 160 kb; ST405, 100 kb; ST648, 150 kb); however, other species included _bla_ NDM-1  –positive plasmids in a wide diversity of sizes (kb–450 kb); some of these species had multiple positive plasmids, and _bla_ NDM-1  was also found on the chromosome .\n\n【6】The _E. coli_ isolates were further analyzed by PCR to identify additional resistance mechanisms often associated with _bla_ NDM-1  . _bla_ CTX-M-15  and 16s ribosomal methylase genes (armA_ or _rmtB_ ) were associated with most _E. coli_ strains, which explains the extensively drug-resistant phenotype of the _E. coli_ isolates . Plasmids of plasmid incompatibility groups _inc_ FII (ST101, ST405, ST648) and _inc_ X (ST405, ST648) were also closely associated with _E. coli_ strains . _E. coli_ harboring _bla_ NDM-1  were isolated from 10 sampling sites . _The E. coli_ isolates belonged to 3 different multilocus sequence typing groups: ST101 (phylogroup B1, 20/53 samples); ST405 (phylogroup D, 5/53 samples); and ST648 (phylogroup D, 28/53 samples) . ST101, which was found in samples from 6 (.3%) of the 58 sites, was the most prevalent NDM-1–encoding _E. coli_ genotype. ST648 represented an intermediate prevalence (/58 \\[8.6%\\] sites), and ST405 was the least prevalent (/58 \\[1.7%\\] sites) .\n\n【7】### Conclusions\n\n【8】Our findings indicate that NDM-1 is widespread in the Dhaka environment. We detected 241 NDM-1–encoding bacterial isolates; they were found in all 7 sampled regions and at 36 (%) of the 58 sampling sites. This high level of environmental _bla_ NDM-1  contamination is of concern, especially because drinking water in Bangladesh usually carries high levels of sewage-derived bacteria . It is therefore likely that _bla_ NDM-1  carriage rates will rise rapidly. Future environmental studies could provide indicators of epidemics of emerging resistant bacteria before they are realized in hospitals.\n\n【9】Despite the widespread presence of NDM-1 in Dhaka, it appears that this carbapenemase has recently emerged in the Bangladesh environment. Studies in northern Bangladesh did not find NDM-1 in wild ducks and poultry in 2009  or in crow and gull feces in 2010 . Similarly, NDM-1 was not detected in drinking water in Dhaka during 2008–2009  even though all samples had high levels of fecal and _bla_ CTX-M-15  contamination. Furthermore, a study of 1,879 clinical _E. coli_ and _Shigella_ spp. isolates collected during 2009–2010 in Bangladesh did not detect _bla_ NDM-1  . The first known clinical isolates date from 2008 , and the first evidence of human gut carriage of _bla_ NDM-1  was found in samples collected in Dhaka  a month before our study.\n\n【10】Because _E. coli_ is the leading cause of human urinary tract infections, bloodstream infections, and neonatal meningitis, the ability of NDM-1 to give this bacterium clinical resistance to carbapenems is of concern . _E. coli_ is also universally carried in the human gut. Therefore, we focused on this species because it is likely to be the greatest threat to human health. _E. coli_ encoding NDM-1 were found in 3 of the 7 sampled regions, and genotyping showed they belonged to only 3 STs: ST648, ST101, and ST405. These same 3 _E. coli_ genotypes are responsible for 80% of clinical NDM-1–encoding _E. coli_ isolates in the United Kingdom . Furthermore, ST101 is the most common _E. coli_ genotype in the Bangladesh environment (.3% prevalence) and in clinical isolates from the United Kingdom (%). Results of a literature search for NDM-1–encoding _E. coli_ belonging to ST101 showed that this genotype has been detected in 15 nations . Thus, _E. coli_ ST101 appears to be a successful global genotype that is often associated with NDM-1. This association with a single global genotype is analogous to the association between _E. coli_ ST131 and the cephalosporinase CTX-M-15. Because of the critical nature of extensively drug-resistant bacteria, we are investigating the underlying factors responsible for the success of these particular antimicrobial drug–resistant strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "11635515-35c6-4172-8b1b-cb80c027f65a", "title": "Fatal Case of Pneumonia Associated with Pandemic (H1N1) 2009 in HIV-Positive Patient", "text": "【0】Fatal Case of Pneumonia Associated with Pandemic (H1N1) 2009 in HIV-Positive Patient\n**To the Editor:** Pandemic (H1N1) 2009 virus first appeared in March 2009 in Mexico. In June 2009, a pandemic was declared by the World Health Organization . Influenza A virus (H1N1) caused a pandemic in 1918–1919; estimated deaths were ≈100 million worldwide . Symptoms of pandemic (H1N1) 2009 are similar to those of seasonal influenza (fever, cough, sore throat, body aches, headache, chills, and fatigue) . Pandemic (H1N1) 2009 should be considered in the differential diagnosis of patients with acute febrile respiratory illness who have been in contact with persons with confirmed influenza or reside in areas where influenza has been reported .\n\n【1】Although most cases of pandemic (H1N1) 2009 in the United States have been mild, 2%–5% of infected persons have required hospitalization . Immunosuppressed persons, the elderly, persons with underlying lung or cardiac disease, pregnant women, persons with diabetes, obese persons, and children <5 years of age are at increased risk for this disease .\n\n【2】We report pneumonia associated with pandemic (H1N1) 2009, which resulted in respiratory and renal failure and death, in a 39-year-old HIV-positive woman. She had type 1 diabetes and a diagnosis of AIDS 7 years ago and had received highly active antiretroviral therapy. She also had an ill child at home with an influenza-like illness.\n\n【3】Her medical history included pleuropericardial _Nocardia_ spp. infection, recurrent pleural effusions requiring thoracentesis, and hepatomegaly of unknown cause. Her most recent CD4 cell count was 166 cells/μL with undetectable viral load 1 month before admission. Medications prescribed included combivir, efavirenz, and trimethoprim/sulfamethoxazole but she was noncompliant. She had received the 2008–09 seasonal influenza vaccine and pneumococcal vaccine.\n\n【4】The patient was admitted to Winthrop-University Hospital (Mineola, NY, USA) on June 5, 2009, for community-acquired pneumonia. She received empiric moxifloxacin and atovaquone. Because of concern for persistent _Nocardia_ spp. infection, she was also treated with doxycycline. The result of a rapid influenza test (QuickVue; Quidel, San Diego, CA, USA) was negative for a nasal swab specimen on day 1 of hospitalization. Over the next 48 hours, her clinical status deteriorated, and she experienced worsening hypotension and respiratory distress.\n\n【5】On admission, she had fever (°F) for 3 days, pulse rate of 109 beats/min, blood pressure of 86/52 mm Hg, respiratory rate of 22 breaths/min (oxygen saturation of 88% on room air), generalized weakness, nonproductive cough, and increasing shortness of breath. She was alert and oriented. Physical examination showed decreased breath sounds at bases, hepatomegaly, and bilateral edema in the lower extremities. Laboratory tests showed 3,000 leukocytes/mm 3  (% neutrophils, 2% bands, and 3% lymphocytes), hemoglobin level of 12.7 g/dL, and 118,000 platelets/mm 3  . Other laboratory values were blood urea nitrogen 66 mg/dL, creatinine 2.9 mg/dL, creatinine phosphokinase 2,276 IU/L, and lactic acid 3.6 mmol/L (anion gap 13). A chest radiograph showed moderate pleural effusion in the right lung and retrocardiac air space disease. Test results for influenza virus, respiratory fluorescent antibodies (D3 Ultra DFA Respiratory Virus Screening and Infectious Disease Kit; Diagnostic Hybrids, Inc. Athens, OH, USA), and virus culture were negative.\n\n【6】The patient was transferred to the intensive care unit and required intubation, pressor support, and continuous venovenous hemofiltration for fluid removal. Empiric oseltamivir (mg 2×/d) was started on hospital day 3; moxifloxicin was discontinued, and meropenem was given for pneumonia . Thoracentesis showed transudative fluid negative for acid-fast bacilli, bacteria, and fungi.\n\n【7】Results of blood cultures and urine analysis for _Legionella_ spp. antigen were negative. Repeat chest radiography showed a right-sided pneumothorax and worsening bilateral airspace disease. A chest tube was inserted in the right lung, and bronchoscopy was performed on hospital day 5. Results of bronchoalveolar lavage were negative for _Pneumocystis jiroveci_ , virus inclusions, fungi, acid-fast bacilli, bacteria, and mycobacteria. However, clusters of filamentous organisms were seen. On hospital day 5, results of a second rapid influenza test, respiratory fluorescent antibody test, and nasopharyngeal virus culture were negative. Diagnosis was based on a positive result for pandemic (H1N1) 2009 by real-time reverse transcription–PCR (RT-PCR) for a nasopharyngeal swab specimen (New York State Department of Health). Despite empiric treatment with oseltamivir, the patient died on June 15, 2009 (day 11 of hospitalization).\n\n【8】Symptoms of pandemic (H1N1) 2009 in HIV-infected persons are not known. However, these persons have a higher risk for complications. In previous seasonal influenza outbreaks, HIV-infected persons had more severe infections and increased hospitalization and mortality rates .\n\n【9】Although a diagnosis of pandemic (H1N1) 2009 was first considered for our patient because of her ill child, she was not initially treated with oseltamivir because of the negative influenza test result and concern for opportunistic infections. Only the result of an RT-PCR for pandemic (H1N1) 2009 was positive. No other pathogens were detected in her blood, urine, sputum, bronchoalveolar lavage, or thoracenthesis fluid.\n\n【10】Empiric treatment in patients with pandemic (H1N1) 2009 should be considered in those seeking treatment for influenza-like symptoms, especially in the setting of sick contacts with respiratory illnesses. Rapid influenza tests, respiratory fluorescent antibody tests, and viral cultures may not provide a diagnosis. An RT-PCR for pandemic (H1N1) 2009 may be needed to provide a diagnosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f27ef74f-23ba-4f3b-a44e-166ecbe6a54a", "title": "Carbapenemase-producing Acinetobacter spp. in Cattle, France", "text": "【0】Carbapenemase-producing Acinetobacter spp. in Cattle, France\n**To the Editor:** Multidrug resistance in bacteria isolated from animals is an emerging phenomenon, mirroring what is happening among humans. During the past decade, expanded-spectrum β-lactamases in _Enterobacteriaceae_ from humans  and animals  worldwide have been reported. Among humans, as a consequence of this high rate, use of carbepenems is increasing selection pressure; carbapenem-resistant gram-negative organisms are increasingly reported, including carbapenemase-producing _Enterobacteriaceae_ and _Acinetobacter_ spp .\n\n【1】The most commonly acquired carbapenemases identified in _Acinetobacter_ spp. correspond to carbapenem-hydrolyzing class D β-lactamases . In particular, the worldwide spread of OXA-23–producing _A. baumannii_ is considered a serious threat; those strains are frequently involved in nosocomial outbreaks for which therapeutic options are extremely limited . Our study objective was to evaluate the possible occurrence of carbapenemase-producing gram-negative bacteria in dairy cattle in France.\n\n【2】In August 2010, at a dairy farm 30 km from Paris, France, rectal swabs were collected from 50 cows. Samples were precultured in buffered peptone water and incubated for 18 h at 37°C. Cultures were inoculated by streaking 100 μL of the suspensions onto Drigalski agar plates (bioMérieux, Balmes-les-Grottes, France) containing 1 μg/mL of imipenem to select for carbapenem-resistant gram-negative isolates. Of the 50 samples, 9 produced growth on imipenem-containing plates. All colonies tested (colonies/sample) by using the API 20 NE (bioMérieux) system were first identified as _A. lwoffii_ . Molecular techniques based on sequencing of the _gyrA_ , _gyrB_ , and _rpoB_ genes  enabled more precise identification and indicated that all isolates belonged to the _Acinetobacter_ genomospecies (DNA group) 15TU, which is known to be phylogenetically related to _A. lwoffii_ and which has been reportedly isolated from sewage, freshwater aquaculture habitats, trout intestines, and frozen shrimp .\n\n【3】One colony per sample was retained for further investigation (isolates BY1 to BY9). Susceptibility testing and MIC determinations were performed by disk-diffusion assay (Sanofi-Diagnostic Pasteur, Marnes-la-Coquette, France) and Etest (AB bioMérieux, Solna, Sweden) . All isolates except 1 were resistant to penicillins, combinations of penicillins and β-lactamase inhibitors, and carbapenems but susceptible to cefotaxime and of reduced susceptibility to ceftazidime. Isolate BY1 showed higher MICs for carbapenems . In addition, all isolates were resistant to tetracycline, kanamycin, and fosfomycin and remained susceptible to fluoroquinolones, chloramphenicol, gentamicin, amikacin, tobramycin, and sulfonamides. Susceptibility profiles of 3 _Acinetobacter_ genomospecies 15TU reference strains showed that they were fully susceptible to penicillins, carbapenems, tetracycline, and kanamycin.\n\n【4】Clonal diversity between the isolates was assessed by pulsed-field gel electrophoresis , which showed 6 distinct genotypes. Isolate BY1 corresponded to a single clone (data not shown), which indicated that the occurrence of _Acinetobacter_ genomospecies 15TU strains among these animals was not the result of dissemination of a single clone.\n\n【5】PCR detection and sequencing of genes that encode carbapenem-hydrolyzing class D β-lactamases  showed that the 9 _Acinetobacter_ genomospecies 15TU isolates harbored a _bla_ OXA-23  gene, whereas the 3 reference strains remained negative. Sequencing confirmed that all isolates expressed β-lactamase OXA-23, which is known to be widespread in _A. baumannii_ .\n\n【6】Mating-out assays and plasmid electroporation assays were performed by using _bla_ OXA-23  –positive _Acinetobacter_ spp. isolates as donors and rifampin-resistant _A. baumannii_ BM4547 isolates as a recipient strain ; however, these assays were unsuccessful. Plasmid DNA analysis  gave uninterpretable results, with DNA degradations.\n\n【7】The genetic structures surrounding the _bla_ OXA-23  gene were investigated by PCR mapping , which identified transposon Tn _2008_ in isolate BY2 only. Tn _2008_ is a major vehicle for the spread of the _bla_ OXA-23  gene in _A. baumannii_ in the People’s Republic of China  and the United States . In the other isolates, the IS _Aba1_ element of Tn _2008_ had been truncated by a novel insertion sequence termed IS _Acsp2_ .\n\n【8】The dairy farmer indicated that most animals from which OXA-23 producers had been identified had received antimicrobial drugs in the previous weeks. Although 1 animal had received amoxicillin-clavulanate, most of the others had been given oxytetracycline and neomycin to treat mastitis.\n\n【9】β-lactamase OXA-23 is a common source of carbapenem resistance in _A. baumannii _ . Infections with multidrug-resistant OXA-23–producing _A. baumannii_ or _A. junii_ have been reported from hospitals but not from the community. Our study showed that OXA-23–producers in particular, and carbapenemase producers in general, may be isolated from animals. Among the hypotheses that could explain the selection of this carbapenemase, use of penicillins or penicillin–β-lactamase inhibitor combinations could create selective pressure for β-lactamases because OXA-23 does confer, in addition to decreased susceptibility to carbapenems, a high level of resistance to those compounds. We have previously shown that _A. radioresistens_ , an environmental species, was the progenitor of the _bla_ OXA-23  gene . Studies are needed to determine to what extent and at which locations _Acinetobacter_ genomospecies 15TU and _A. radioresistens_ might co-reside and therefore where the _bla_ OXA-23  gene exchange might have occurred.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e77fa3dc-798b-4143-a11e-bcf6ba90f6a7", "title": "Malaria in Children Adopted from the Democratic Republic of the Congo", "text": "【0】Malaria in Children Adopted from the Democratic Republic of the Congo\nThe Democratic Republic of the Congo (DRC) banned adoption of children by parents from other countries in 2013. In February 2016, the ban was removed, and several hundred children were allowed to join families in Europe and in the United States. The first group of children from DRC arrived in Italy in April 2016, and 20 children were referred to the Center for Internationally Adopted Children at Meyer University Hospital, Florence, Italy. All children underwent the standard infectious disease screening tests recommended by the American Academy of Pediatrics, including those for tuberculosis (by tuberculin skin test and interferon-γ release assay) and intestinal parasites (by fecal testing for ova, parasites, and antigen test for _Giardia_ spp.) and serologic tests for _Toxocara canis, Strongyloides_ spp. hepatitis B and C viruses, HIV-1/2 viruses, and _Treponema pallidum_ (syphilis) .\n\n【1】Eight children who were exhibiting fever were admitted to the hospital and received a diagnosis of malaria soon after their arrival in Italy. For malaria testing, PCR and microscopy were performed on thin and thick smears. Parasitemia level was determined by counting the parasitized erythrocytes among the 500–2,000 erythrocytes on the thin smear and calculating the percentage.\n\n【2】The remaining children from DRC were screened, and another 8 children were found to be infected. Thus, malaria was diagnosed in 16 children (were boys; median age 7 years \\[range 4–10 years\\]), and malaria prevalence was 80% (/20). _Plasmodium falciparum_ infection was documented in 15 cases, whereas a mixed infection (P._ _falciparum_ and _P. ovale_ ) was observed in 1 child. All children underwent treatment with intravenous quinine plus artesunate or oral dihydroartemisinin/piperaquine . Intravenous treatment was administered to 1 child who had severe malaria (generalized seizures) and to 5 children with a parasitemia level \\> 2% or who exhibited vomiting and therefore were unable to take oral medications reliably . Because intravenous artesunate is unlicensed in Europe but is available in our center, we obtained written informed consent for its use from the patients’ parents before administration. The study received approval by Meyer University Hospital Ethics Committee.\n\n【3】Other studies have assessed the prevalence of malaria in internationally adopted children. Among a population of 182 children in France, when Blanchi et al. screened for the children originating from malaria-endemic zones who were exhibiting fever, splenomegaly, or both for malaria, they found 2 infected children . More recently, Adebo et al. screened 52 children arriving in the United States from Ethiopia for malaria and reported 7 (.5%) children with asymptomatic malaria . These authors suggested that screening be conducted of children coming from malaria-endemic areas with noted risk factors, such as splenomegaly . Anemia (hemoglobin levels <11 g/dL) could be another risk factor for malaria but, because it is common in this population, it was not considered in the study by Adebo et al. However, in our population, we found that tests for parasitemia were positive for 6 children without splenomegaly or hepatomegaly and for 1 child who had neither anemia (<11 g/dL), hepatomegaly, nor splenomegaly. Therefore, using clinical features to select children who should undergo the screening may be challenging.\n\n【4】Given the paucity of literature data regarding malaria prevalence in internationally adopted children, testing by PCR, microscopy, or both, followed by treatment of infected children, would be preferable to the empiric treatment, considering the costs and possible adverse effects of antimarial drugs. Moreover, the preferable screening strategy is not apparent. We did not observe any discrepancy between microscopy and PCR results; however, a higher sensitivity by PCR has been reported . In contrast, some experts prefer testing by microscopy examination because PCR techniques are not sufficiently standardized or validated to be used for routine clinical diagnosis .\n\n【5】In our dataset, malaria prevalence was substantially higher than that previously reported . This finding may be due to the particular situation of these children and to orphanage conditions (i.e. lack of mosquito nets). Moreover, it should be noted that, to date, 3 countries— DRC, Nigeria, and India—account for 40% of all estimated malaria cases in the world . Also, a high prevalence of asymptomatic malaria in DRC has been reported, in ≈15% of children .\n\n【6】Our results should be interpreted with caution, given the small dataset, but they should alert pediatricians regarding the importance of assessing malaria risk in children who have been adopted internationally. The degree of malaria endemicity in the child’s area of origin may be considered in the decision to screen asymptomatic children adopted in non–malaria-endemic countries. In particular, children who come from areas of high malaria endemicity, such as DRC, deserve a careful screening, even in the absence of any sign or symptom.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d210bdb5-b25d-416f-8793-0273e4cfd4d7", "title": "The Quality of School Wellness Policies and Energy-Balance Behaviors of Adolescent Mothers", "text": "【0】The Quality of School Wellness Policies and Energy-Balance Behaviors of Adolescent Mothers\nAbstract\n--------\n\n【1】**Introduction**In this study, we 1) compared the quality of school wellness policies among schools participating in Moms for a Healthy Balance (BALANCE), a school- and home-based weight loss study conducted with postpartum adolescents in 27 states; and 2) assessed the relationship between policy quality with energy-balance behaviors and body mass index _z_ scores of postpartum adolescents. **Methods**As a part of BALANCE, we collected data on high-calorie food and beverage consumption, minutes spent walking, and height and weight for 647 participants. The School Wellness Policy Coding Tool was used to assess the strength and comprehensiveness of school district wellness policies from 251 schools attended by participating adolescent mothers. **Results**Schools averaged low scores for wellness policy comprehensiveness and strength. When compared with participants in schools with the lowest policy comprehensiveness scores, adolescent mothers in schools with the highest scores reported consuming significantly fewer daily calories from sweetened beverages while reporting higher consumption of water (P_ \\= .04 and _P_ \\= .01, respectively). School wellness policy strength was associated with lower BMI _z_ scores among adolescent mothers (P_ \\= .01). **Conclusion**School wellness policies associated with BALANCE may be limited in their ability to promote a healthy school environment. Future studies are needed to evaluate the effect of the strength and comprehensiveness of policy language on energy balance in high-risk postpartum adolescents. Evidence from this work can provide additional guidance to federal or state government in mandating not only policy content, but also systematic evaluation.  \n\n【2】Introduction\n------------\n\n【3】Approximately 18% of adolescents aged 12-19 years or 9 million youth in the United States are overweight . The risk of overweight is significantly heightened for the approximately 500,000 adolescents who become pregnant each year . Postpartum weight retention exacerbates the risk of development of overweight, impaired glucose tolerance, type 2 diabetes, and other diseases . Strategies addressing high-risk patterns among adolescent mothers may have important public health implications, as postpartum weight retention may compound with future pregnancies and timely interventions may mitigate the intergenerational transfer of high-risk behaviors . Environmental and policy interventions for food and activity environments may be effective strategies for preventing childhood obesity . Policy interventions create population access to environments that promote healthy options . Some policy initiatives have targeted schools . Children may spend up to 10 hours per day at school, which accounts for much of their physical activity and as many as 2 meals and 2 snacks per day. The Child Nutrition and WIC (Women, Infants, and Children) Reauthorization Act of 2004 (Public Law 108-265), which went into effect in 2006-2007, required all local education agencies participating in the National School Lunch Program to create a school wellness policy that included goals for achieving energy balance through healthy dietary intake and physical activity behaviors . To date, preliminary data have shown mixed results regarding the quality of school wellness policies . Variations in measures used in evaluating policies make interpretation of findings challenging and limit the opportunity for comparative analyses of school wellness policies across communities and states . Schwartz and colleagues  developed a measure to evaluate the quality of school wellness policies across common criteria for comprehensiveness (ie, breadth of areas covered) and strength (ie, degree to which policies included specific and firm language). In this study, we 1) compared the quality of school wellness policies of schools participating in Moms for a Healthy Balance (BALANCE) , a school- and home-based weight loss study conducted with postpartum adolescents across 27 states; and 2) assessed the relationship between policy quality with energy-balance behaviors and body mass index (BMI) _z_ scores of postpartum adolescents.  \n\n【4】Methods\n-------\n\n【5】### Design and sample\n\n【6】BALANCE was a group-randomized, nested-cohort study developed and designed in partnership with Parents As Teachers (PAT), a national parenting and child development program . We recruited postpartum adolescents who retained their pregnancy weight to participate in the BALANCE weight-reduction protocol. We used data from BALANCE baseline assessments that participants completed between January 2007 and April 2008. As part of our BALANCE study during 2008-2009, we collected school wellness policies from the websites of schools or school districts attended by our participants. If the policy was unavailable on the website, we contacted the school and requested a copy. We also verified that collected policies were in effect in 2006-2007. We recruited 1,330 ethnically diverse participants into BALANCE who were enrolled in PAT Teen Parent Programs from 27 states . In addition to enrollment in the PAT Teen Parent Program (for ages 13-19 y), eligibility criteria included 1) a willingness to participate throughout the study period, 2) being less than 1 year postpartum, and 3) not being pregnant or planning to become pregnant during the study period. For our analysis, we further excluded participants who had either graduated or withdrawn from school (n = 275), were currently breastfeeding (n = 109), or were missing residential zip code and school information (n = 299). In total, 647 postpartum adolescents located in 251 schools from 203 school districts in 27 states, contributed to our findings. The institutional review board of Washington University in St. Louis approved this study. Our sample had a mean age of 17.2 (standard deviation 1.1 y). Forty-eight percent were white, 30% were black, and 22% were other; most were receiving some form of aid from either WIC (%) or the federally sponsored free or reduced-lunch program (%), and they were approximately 6 months postpartum (days). Approximately half of participants were at a normal weight and half were overweight or obese. \n\n【7】### Measures\n\n【8】Participants’ height and weight were collected by trained PAT staff to determine BMI _z_ score classification according to criteria specified for adolescents by National Health and Nutrition Examination Survey procedures . Adolescents then completed the online Snack and Beverage Food Frequency Questionnaire (SBFFQ), which was used to measure specific high-calorie snack and beverage consumption patterns of participants. Following a similar format to that of the Diet History Questionnaire , the SBFFQ examined each participant’s intake of 31 items during the previous 7 days by asking on how many days, how many times per day, and how much of the item the participant consumed. Food items were assessed by subgroups: sweetened beverages (eg, soda and fruit juice), salty snacks (eg, potato chips), sweet snacks (eg, hard candy), meal-type snacks (eg, french fries), fruits and vegetables, and water consumption. Intake was converted into the total calories consumed for each individual food item and summed to obtain the daily calorie total. The test–retest reliability for the separate measures ranged from moderate to substantial with the following intraclass correlation coefficients: water (.71), sweetened beverages (.68), salty snacks (.43), meal-type snacks (.64), and fruits and vegetables (.46) . The testretest reliability for the composite measure of total calories was acceptable (.63). Physical activity was measured with 3 items asking participants how many minutes they spent walking at a slow, brisk, or very brisk pace on the 2 weekdays preceding completion of the measure, and on 1 weekend day . Participants reported their age, race/ethnicity, education level, breastfeeding status, and postpartum status. They also reported their participation in aid programs (WIC and the National School Lunch Program), which we used as indicators of socioeconomic status. We used the 96-item School Wellness Policy Coding Tool developed by Schwartz and colleagues  to assess the strength and comprehensiveness of the school wellness policies in each school district . Each of the 96 content items was coded with a score of 0, if the item was not mentioned; 1, if the item was a “weak” statement making it hard to enforce because of vague, unclear, or confusing language; or 2, meaning the item meets or exceeds expectations since it was mentioned in a specific and directive manner suggesting commitment to enforcement . \n\n【9】### Data analysis\n\n【10】Data analyses were conducted in 2 stages. First, we sought to determine the comprehensiveness and strength scores of school wellness policy language for school districts attended by our participants. Second, we sought to relate the overall comprehensiveness and strength of school wellness policy language to the measured energy-balance behaviors and BMI _z_ scores of BALANCE participants. All analyses were conducted by using SPSS version 17.0 (SPSS, Inc, Chicago, Illinois). We evaluated the language quality for each policy item of the coding tool by the percentage of school districts with a rating of “meets or exceeds expectations.” For assessing the language quality of the 7 policy sections and the overall district policy score, we computed the sample mean and standard deviation for both comprehensiveness and strength with methods suggested by Schwartz and colleagues . The school wellness policy language scores for both comprehensiveness and strength were split into low, middle, and upper tertiles. We compared demographic characteristics of BALANCE participants among school wellness policy language tertiles with χ 2 , Kruskal-Wallis, or 1-way analysis of variance tests, as indicated by measurement level. Univariate, general linear models were constructed to assess the relationship between school wellness policy comprehensiveness and strength tertiles and measured energy-balance behaviors. We explored the possibility that relationships between policy language quality and energy-balance behaviors may vary by either race/ethnicity or BMI, by testing the race/ethnicity policy score tertile and BMI policy score tertile cross-product terms. Final models were adjusted for race/ethnicity, as both the scoring of policy quality and energy consumption of snacks appeared to vary by race/ethnicity in our sample. The statistical assumptions underlying each test were checked for violations (eg, homogeneity of variances and outlying and influential cases). Given that our sample had little variation regarding school wellness policy comprehensiveness or strength scores, we selected the 40 highest and 40 lowest scoring districts for further analysis.  \n\n【11】Results\n-------\n\n【12】### District school wellness policies\n\n【13】### Relationship of policy quality to dietary intake, physical activity, and BMI _z_ score\n\n【14】When assessed for group differences across tertiles of school wellness policy comprehensiveness and strength scores, race/ethnicity and BMI _z_ score were unbalanced . Specifically, white mothers were more commonly found in districts with the highest policy rating, while black mothers were more commonly found in districts with the lowest policy rating. Additionally, the lower tertiles of both comprehensiveness and strength scores included adolescents with higher BMI _z_ scores, though the group comparison was not significant for policy strength. We found no evidence of effect modification for either race/ethnicity or BMI when considering the relationship between school wellness policy quality and energy-balance behavior outcomes. In our initial adjusted models assessing snack and physical activity behaviors of participants, we found no significant relationships between policy comprehensiveness or strength tertile and energy-balance behaviors. In the 40 school districts that had the highest scores for policy comprehensiveness, adolescent mothers reported consuming fewer daily calories from sweetened beverages and more water . There was an inverse relationship between policy comprehensiveness and strength and salty, sweet, and meal-type snacks and total snack calories. Policy strength was significantly associated with a lower BMI _z_ score and was also inversely related to sweetened beverage consumption.  \n\n【15】Discussion\n----------\n\n【16】Four findings from this study can expand research related to policy initiatives associated with promoting energy-balance behaviors among adolescent mothers. First, our study suggests that items that are mandated in school wellness policies are most likely to meet or exceed expectations for quality language when compared with nonmandated items. This study also supports previous studies that have found that strong mandatory language, as opposed to recommended language, has the greatest effect on food access . Clarification of school wellness policy language by the federal government to address both strength and scope of content may further enhance the effect of school wellness policies for adolescent mothers. State governments have the best knowledge of needs, possible incentive programs, and the financial situation of their state when crafting the model policies for school districts. Second, our study suggests there are differences in the quality of policies that have an educational focus compared with those focused on behavioral outcomes. Previous studies have reported variations in the extent to which nutrition or physical activity topics are included in school wellness policies . We were able to expand on this work and systematically measure and compare both the comprehensiveness and strength of nutrition and physical activity focused topics in policies among multiple states and school districts . Of particular note was that 2 of the sections (establishing nutrition standards for competitive and other foods and beverages, physical education) requiring language for policy actions directly related to regulating food access and time to be physically active scored the lowest for comprehensiveness and strength. In contrast, sections scoring the highest included evaluation and nutrition education, which each focused on establishing goals or documenting a plan for implementation as opposed to mandating immediate changes in the environment . Further study is warranted to describe reasons for these differences, barriers to the development and implementation of strong and comprehensive policies, and the extent to which they may affect behavior . Third, our study found that schools associated with PAT programs for adolescent parents have generally weak wellness policies in place. Additionally, there appeared to be a relationship between the presence of weak policies and energy-balance behaviors of adolescent mothers. For example, the most comprehensive policies were associated with adolescent mothers consuming 136 fewer calories from sweetened drinks per day and by 17 ounces more water per day. Indeed, substantial literature suggests a relationship of sweetened beverages to obesity . Others have also found sweetened beverage intake was altered by school environmental changes . Our study contributes to this literature by further suggesting the value of policy quality in addressing beverage intake in schools as a possible mechanism for preventing obesity. Finally, from a translational perspective, our findings suggest the importance of defining the model content of quality school wellness policies, and effectively communicating to parents as to whether this content is present in school policies . Currently, adolescents or their parents have no way of adequately judging the quality of the wellness policy that directly influences the school environment. The overwhelming presence of weak school wellness policies might mislead parents or adolescents into thinking their educational environment practices and reinforces positive eating and activity behaviors. Our results, consistent with those of other studies, suggest that in fact this may be the case . Wellness report cards or other strategies for communicating the strength and comprehensiveness of school policies to parents in easily understandable ways are needed . \n\n【17】### Limitations\n\n【18】Our study had several limitations. First, this is a cross-sectional study that does not allow for assessment of temporal relationships. We did not assess policy effect or implementation which may vary by school district. We had limited information on the school districts in our sample, so were unable to address heterogeneity and generalizability issues. Many of the policies under observation were not required until the start of the 2006-2007 academic year, which may not provide enough time to see the full effect of the policies on measured behaviors. We also present information on a group that may not be generalizable to broader school-district populations. Finally, interpretation of our findings should be considered within the limitations of self-report measures. \n\n【19】### Conclusion\n\n【20】School wellness policies associated with PAT programs for adolescent mothers in multiple states may be limited in their ability to promote a healthy school environment. Improvements in the quality of school wellness policies may help to enhance the school environment and, in turn, energy-balance behaviors of adolescents. Future studies, reflecting naturalistic or prospective designs, are needed to evaluate the effect of the strength and comprehensiveness of policy language on energy balance in high-risk postpartum teenagers. Evidence from this work can provide additional guidance to federal or state government in mandating not only policy content, but systematic evaluation. To be active advocates for their adolescent, parents need to be accurately informed about the quality of the wellness policies in their adolescent’s school. Quality assurances are needed so that school wellness policies are not missed opportunities for encouraging energy-balance behaviors and preventing obesity among adolescent mothers.  \n\n【21】Tables\n------\n\n【22】#####  Table 1. School Wellness Policy Comprehensiveness and Strength Scores, 2007-2009\n\n【23】School Wellness Policy Coding Tool aMean Score (SD)Maximum bSection 1: Nutrition educationComprehensiveness score c49.0 (.8)100.0Strength score d31.3 (.2)77.8Section 2: Standards for USDA child nutrition programs and school mealsComprehensiveness score c39.0 (.3)92.3Strength score d24.3 (.5)69.2Section 3: Nutrition standards for competitive and other foods and beveragesComprehensiveness score c37.7 (.1)72.4Strength score d9.3 (.4)58.6Section 4: Physical educationComprehensiveness score c32.5 (.7)88.2Strength score d17.8 (.0)52.9Section 5: Physical activityComprehensiveness score c37.9 (.3)100.0Strength score d21.3 (.3)70.0Section 6: Communication and promotionComprehensiveness score c39.5 (.7)100.0Strength score d21.0 (.9)66.7Section 7: EvaluationComprehensiveness score c59.0 (.5)100.0Strength score d31.6 (.9)83.3OverallComprehensiveness score e39.6 (.1)72.9Strength score f19.0 (.6)51.0Abbreviations: SD, standard deviation; USDA, US Department of Agriculture.a School Wellness Policy Coding Tool consists of 96 items split among 7 sections. Each item is rated 0 if the policy item was not mentioned, 1 if the policy item was written in vague or confusing language, or 2 if the policy item used specific and directive language.b The minimum score for each tool was 0, and the maximum score was the highest score received.c Comprehensiveness scores represent items rated either 1 or 2 within a section divided by the number of items in that section, indicating the policy addressed the section.d Strength scores represent items rated a 2 within a section divided by the number of items in that section, indicating the policy addressed the section with specific and directive language.e Overall comprehensiveness score is the average of the 7 comprehensiveness section scores.f Overall strength score is the average of the 7 strength section scores. \n\n【24】#####  Table 2. Characteristics of 647 Postpartum Adolescents, by School Wellness Policy Score Tertiles, 2007-2009\n\n【25】MeasureSchool Wellness Policy Score TertilesComprehensiveness Score, Mean (SD) aP ValueStrength Score,Mean (SD) bP ValueLowerMiddleUpperLowerMiddleUpperSchool level (n = 251)National School Lunch Program participants, %43.5 (.2)44.2 (.2)39.4 (.4).11 c45.8 (.8)42.5 (.0)39.6 (.5).02 cIndividual level (n = 647)Age, y (SD)17.2 (.1)17.2 (.1)17.2 (.2).90 d17.2 (.0)17.3 (.1)17.1 (.2).19 dRace/ethnicity, %White44.544.357.1&lt;001 e43.851.050.2.004 eBlack41.627.122.240.426.325.1Other13.928.520.715.822.721.2BMI z score (SD)0.51 (.1)0.36 (.0)0.20 (.2).03 d0.51 (.1)0.27 (.1)0.30 (.2).08 dPostpartum duration, d (SD)189.8 (.4)187.7 (.7)167.8 (.6).08 d187.5 (.3)186.1 (.4)173.8 (.7).35 dWIC participants, %94.488.889.1.08 e93.891.187.6.08 eAbbreviations: SD, standard deviation; BMI, body mass index; WIC, Child Nutrition and Women, Infants, and Children Reauthorization Act of 2004.a Comprehensiveness scores indicate common criteria for the breadth of areas covered by school wellness policies. Strength scores indicate the degree to which policies included specific and firm language. Minimum and maximum scores: lower, minimum = 0, maximum = 25; middle, minimum = 26.04, maximum = 47.92; upper, minimum = 48.96, maximum = 72.92.b Strength scores indicate the degree to which policies included specific and firm language. Minimum and maximum scores: lower, minimum = 0, maximum = 10.42; middle, minimum = 26.04, maximum = 47.92; upper, minimum = 48.96, maximum = 72.92.c Calculated by using the Kruskal-Wallis test.d Calculated by using 1-way analysis of variance.e Calculated by using the _χ 2_ test. \n\n【26】#####  Table 3. Energy-Balance Behaviors of 647 Postpartum Adolescents, Bottom 40 and Top 40 School Wellness Policy Scores, 2007-2009\n\n【27】CharacteristicDistrict RatingComprehensiveness Score, Mean (% CI) aStrength Score, Mean (% CI) bBottom 40Top 40P V alueBottom 40Top 40P ValueBMI z score c0.51 (.23-0.79)0.40 (.14-0.66).560.55 (.32-0.76)0.14 (.09-0.37).01Water consumption, oz c24 41 .0128 36 .07Sweetened beverage, kcal c508 372 .04421 372 .36Salty snack, kcal c369 376 .93317 306 .86Sweet snack, kcal c268 277 .83283 255 .46Meal-type snack, kcal c237 276 44277 254 .56Fruit and vegetable snack, kcal c51 46 .6939 47 .45Total snack, kcal c1,786 1,707 .641,699 1,568 .32Walking, min14 17 .2516 17 .90Abbreviations: CI, confidence interval; BMI, body mass index.a Minimum and maximum scores: bottom 40 districts, minimum = 0, maximum = 19.79; top 40 districts, minimum = 61.46, maximum = 79.92.b Minimum and maximum scores: bottom 40 districts, minimum = 0, maximum = 6.25; top 40 districts, minimum = 33.33, maximum = 51.04.c General linear models adjusted for race/ethnicity.  \n\n【28】Appendices\n----------\n\n【29】RegionStateSouthAlabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, TexasMidwestIllinois, Indiana, Iowa, Kansas, Michigan, Missouri, Ohio, South Dakota, WisconsinNortheastNew York, Pennsylvania, Rhode IslandWestArizona, California\n\nItem%Nutrition education1. Includes goals for nutrition education that are designed to promote student wellness in a manner that the local education agency determines is appropriate (federal requirement )76.82. Nutrition curriculum provided for each grade level46.33. Coordinates nutrition education with the larger school community19.24. Nutrition education extends beyond the school environment14.35. District provides nutrition education training for all teachers7.96. Nutrition education is integrated into other subjects beyond health education23.27. Nutrition education teaches skills that are behavior-focused and/or interactive and/or participatory35.08. Specifies number of nutrition education courses or contact hours1.59. Nutrition education quality is addressed57.6Standards for US Department of Agriculture (USDA) child nutrition programs and school meals10. Assures guidelines for reimbursable school meals shall not be less restrictive than USDA school meal regulations (federal requirement )89.711. Addresses access to and/or promotion of the USDA School Breakfast Program6.912. Addresses access to and/or promotion of the Summer Food Service Program3.413. Addresses nutrition standards for school meals beyond USDA (National School Lunch Program/School Breakfast) minimum standards23.614. Specifies use of low-fat versions of foods and/or low-fat methods for preparing foods8.915. Specifies strategies to increase participation in school meal programs10.816. Optimizes scheduling of meals to improve student nutrition11.817. Ensures adequate time to eat18.718. Addresses access to hand washing before meals25.619. Requires nutrition qualifications of school food service staff23.220. Ensures training or professional development for food service staff29.121. Addresses school meal environment52.722. Nutrition information for school meals (eg, calories, saturated fat, sugar) is available10.8Nutrition standards for competitive and other foods and beverages23. Includes nutrition guidelines for ALL foods available on school campus during the school day with the objective of promoting student health and reducing childhood obesity (federal requirement )35.524. Regulates vending machines21.725. Regulates school stores21.226. Regulates food service la carte22.227. Regulates food served at class parties and other school celebrations2.028. Regulates food from home for the whole class5.429. Regulates food sold before school2.030. Regulates food sold after school that is not part of a district-run after-school program1.031. Regulates food sold at evening and community events on school grounds9.432. Regulates food sold for fundraising17.233. Addresses limiting sugar content of foods7.434. Addresses limiting fat content of foods14.335. Addresses limiting sodium content of foods3.936. Addresses limiting calorie content per serving size of foods3.437. Addresses limiting serving size of foods11.838. Addresses increasing whole foods (eg, whole grains, unprocessed foods, fresh produce)1.539. Addresses limiting the use of ingredients with questionable health effects in food or beverages (eg, artificial sweeteners, processed or artificial foods, trans fats, high fructose corn syrup)1.040. Addresses food not being used as a reward and/or withheld as a punishment24.641. Nutrition information (eg, calories, saturated fat, sugar) available for foods other than school meals2.042. Addresses limiting sugar content of beverages13.343. Addresses limiting fat content of drinks (other than milk)0.544. Addresses limiting calorie content per serving size of beverages2.545. Addresses limiting regular (sugar-sweetened) soda16.746. Addresses limiting beverages other than soda containing added caloric sweeteners such as sweetened teas, juice drinks, energy drinks, and sports drinks1.547. Addresses limiting sugar/calorie content of flavored milk15.348. Addresses limiting fat content of milk1.049. Addresses serving size limits for beverages3.950. Addresses limiting caffeine content of beverages (with exception of trace amounts of naturally occurring caffeine substances)2.051. Addresses access to free drinking water15.3Physical education (PE)52. Addresses PE curriculum for each grade level51.753. Addresses time per week of PE for elementary school students15.854. Addresses time per week of PE for middle school students12.355. Addresses time per week of PE for high school students3.956. PE promotes a physically active lifestyle45.857. Specifies competency assessment (ie, knowledge, skills, and practice)6.958. Addresses PE quality63.159. PE promotes inclusive play4.460. Addresses PE classes or credits7.461. Addresses frequency of required PE (daily)1.562. Addresses teacher–student ratio for PE2.063. Addresses safe and adequate equipment and facilities for PE16.364. Addresses amount of time devoted to moderate to vigorous activity in PE5.965. Addresses qualifications for PE instructors32.066. District provides PE training provided for teachers24.167. Addresses PE waiver requirements (eg, substituting PE requirement with other activities)8.968. Requires students to participate in an annual health assessment (eg, fitness or body mass index)1.0Physical activity (PA)69. Includes goals for PA that are designed to promote student wellness in a manner that the local education agency determines is appropriate (federal requirement)60.170. PA provided for every grade level33.071. Includes PA opportunities for school staff8.472. Regular PA opportunities are provided throughout the school day (not including recess)8.473. Addresses PA through intramurals or interscholastic activities26.674. Addresses community use of school facilities for PA outside of the school day13.375. Addresses safe active routes to school9.476. Addresses not using PA (extra or restricted) as punishment25.677. Addresses recess frequency or amount in elementary school23.678. Addresses recess quality to promote PA4.9Communication and promotion79. Involves parents, students, and representatives of the school food authority, the school board, school administrators, and the public in the development of the school wellness policy (federal requirement)43.880. Includes staff wellness programs specifically addressing the health of staff17.781. Addresses consistency of nutrition messages25.182. Encourages staff to role model healthy behaviors30.083. Specifies who in the district is responsible for wellness/health communication beyond required policy implementation reporting7.984. Specifies district use of the Centers for Disease Control and Prevention’s Coordinated School Health model or other coordinated/comprehensive method1.585. Addresses methods to solicit or encourage input from stakeholder groups (eg, 2-way sharing)17.786. Specifies how district will engage parents or community to meet district wellness goals21.787. Specifies what content/information district communicates to parents25.688. Specifies marketing to promote healthful choices23.289. Specifies restricting marketing of unhealthful choices11.890. Establishes a health advisory committee or school health council that is ongoing beyond policy development26.1Evaluation91. Establish a plan for measuring implementation of the local wellness policy, including designation of 1 or more persons within the local educational agency or at each school, as appropriate, charged with operational responsibility for ensuring that the school meets the local wellness policy (federal requirement)65.092. Addresses a plan for policy implementation, including a person or group responsible (initial or ongoing)36.593. Addresses a plan for policy evaluation, including a person/group responsible for tracking outcomes16.794. Addresses the audience and frequency of a report on compliance and/or evaluation40.495. Identifies funding support for wellness activities or policy evaluation1.096. Identifies a plan for revising the policy30.0  |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "63007c20-e3ba-45cb-a4fb-d6f1f3e23080", "title": "VIM-2–producing Pseudomonas putida, Buenos Aires", "text": "【0】VIM-2–producing Pseudomonas putida, Buenos Aires\n**To the Editor:** _Pseudomonas putida_  infections are mainly reported in immunocompromised patients, such as newborns, neutropenic patients, and cancer patients. They are usually susceptible to extended-spectrum cephalosporins, aminoglycosides, fluoroquinolones, and carbapenems. However, isolates have been identified that produce acquired metallo-β-lactamases (MBLs) and are resistant to most β-lactams, including carbapenems.\n\n【1】Two multidrug-resistant _P_ . _putida_ isolates were obtained from clinical samples at the Sanatorio Mater Dei in Buenos Aires. One isolate was obtained in March 2005 from a urine specimen of a 76-year-old woman with a urinary tract infection who was using a urethral catheter. The second isolate was obtained in May 2005 from a tracheal aspirate of a 67-year-old man with nosocomial pneumonia.\n\n【2】\\[\\[AA:T1:PREVIEWHTML\\]\\]\n\n【3】Bacteria were identified by using conventional biochemical tests and the API 20NE System (API, bioMérieux, Lyon, France). Susceptibility tests were performed according to standard procedures. Both isolates were resistant to imipenem and meropenem (MICs >32 μg/mL) but were susceptible to amikacin and colistin. Susceptibility data are shown in the Table .\n\n【4】Screening for MBLs was performed by using a double-disk diffusion method. Disks containing 1 μmol/L EDTA (metal chelator) were placed on Mueller-Hinton agar plates containing the 2 isolates. Disks containing carbapenem were placed 15 mm from disks containing EDTA. An increase in the inhibition zone of the disk containing drug near the disk containing EDTA was observed for both isolates, which suggested the presence of MBLs.\n\n【5】PCR amplification of _imp_ and _vim_ genes was conducted by using primers based on conserved regions of the _imp_ and _vim_ genes (bla_ IMP-F: 5′-GAAGGCGTTTATGTTCATACTT-3′, _bla_ IMP-R: 5′-GTTTGCCTTACCATATTTGGA-3′, _bla_ VIMG-F: 5′-GGTGTTTGGTCGCATATC-3′, and _bla_ VIMG-R 5′-TGGGCCATTCAGCCAGATC-3′) and heat-extracted DNA as template. Reactions were performed in a T-gradient instrument (Biometra, Göttingen, Germany) with the following reaction conditions: 1 cycle at 95°C for 5 min, 52°C for 15 min, and 72°C for 6 min, followed by 30 cycles at 95°C for 1 min, 52°C for 1 min, and 72°C for 1 min, and a final reaction at 72°C for 20 min. Amplified fragments were sequenced on both strands by using an ABI Prism DNA 3700 (Applied Biosystems, Foster City, CA, USA), and nucleotide sequences were compared by using BLAST . Nucleotide sequences were completely homologous to the _vim-2_ coding gene.\n\n【6】Two repetitive-element–based PCR (rep-PCR) assays (ERIC-PCR and REP-PCR) with primers REP-1 (′-IGCGCCGICATCAGGC-3′), REP-2 (′-CGTCTTATCAGGCCTAC-3′), ERIC-1 (′-CACTTAGGGGTCCTCAATGTA-3′), and ERIC-2 (′-AAGTAAGTGACTGGGGTGAGCG-3′) were used to characterize isolates. PCR conditions were 94°C for 2 min, 30 cycles at 94°C for 30 s, 50°C for 1 min, and 72°C for 4 min, and a final reaction at 72°C for 7 min. Banding patterns were visually analyzed after electrophoresis of samples. Variations in band intensity were not considered to indicate genetic differences. Banding patterns obtained by REP-PCR and ERIC-PCR assays were identical in both isolates (data not shown).\n\n【7】Among the MBLs acquired by _P_ . _putida_ , IMP-1 was reported by Senda et al. in Japan in 1996  and later reported in Taiwan and Japan . IMP-12 was the first IMP MBL described in _P_ . _putida_ in Europe . VIM-1 in _P_ . _putida_ was first reported in Europe , and VIM-2 in _P_ . _putida_ was first reported in Taiwan, Republic of Korea, Japan, and France . Our isolates were resistant to aztreonam (MIC 64 μg/mL). However, carbapenem-susceptible _P_ . _putida_ had low levels of susceptibility because the MIC 50  was only 1 dilution below the current breakpoint . Aztreonam resistance could not be transferred by conjugation between IMP-1–producing (aztreonam-resistant) _P_ . _putida_ and _P_ . _aeruginosa_  and is not associated with a transposon carrying blaVIM-2 . No evidence of extended-spectrum β-lactamases was detected in our isolates by classic synergy assays with clavulanate plus aztreonam, ceftazidime, or cefotaxime. VIM-6–producing _P_ . _putida_ isolates from Singapore  were more resistant to aztreonam (MIC >128 μg/mL), ceftazidime, and cefepime (MIC >256 μg/mL).\n\n【8】Detection of _bla_ VIM-2  in _Pseudomonas_ in South America was initially reported by the SENTRY Antimicrobial Surveillance Program  and included 1 _P_ . _fluorescens_ isolate in Chile and 3 _P_ . _aeruginosa_ isolates in Venezuela. To the best of our knowledge, our report is the first of VIM-2 in _P_ . _putida_ in Latin America. VIM-2–producing _P_ . _putida_ , which were originally restricted to East Asia and only very recently found in France, may represent an emerging pathogen or function as reservoirs for resistance because of their widespread presence in the hospital environment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db50220b-95dc-4cfc-9e39-3d4391806104", "title": "Changes in Local School Policies and Practices in Washington State After an Unfunded Physical Activity and Nutrition Mandate", "text": "【0】Changes in Local School Policies and Practices in Washington State After an Unfunded Physical Activity and Nutrition Mandate\nAbstract\n--------\n\n【1】**Introduction**Policies and practices in schools may create environments that encourage and reinforce healthy behaviors and are thus a means for stemming the rising rates of childhood obesity. We assessed the effect of a 2005 statewide school physical activity and nutrition mandate on policies and practices in middle and high schools in Washington State. **Methods**We used 2002, 2004, and 2006 statewide School Health Profiles survey data from Washington, with Oregon as a comparison group, to create longitudinal linear regression models to describe changes in relevant school policies after the Washington statewide mandate. Policy area composite measures were generated by principal component factor analysis from survey questions about multiple binary measure policy and practice. **Results**Relative to expected trends without the mandate, we found significant percentage-point increases in various policies, including restricted access to competitive foods in middle and high schools (increased by 18.8-20.0 percentage points); school food practices (increased by 10.4 percentage points in middle schools); and eliminating exemptions from physical education (PE) for sports (.6 percentage-point increase for middle schools), exemptions from PE for community activities (.8 and 14.4 percentage-point increases for middle and high schools, respectively) and exemptions from PE for academics (.1 percentage-point increase for middle schools). **Conclusion**Our results suggest that a statewide mandate had a modest effect on increasing physical activity and nutrition policies and practices in schools. Government policy is potentially an effective tool for addressing the childhood obesity epidemic through improvements in school physical activity and nutrition environments.  \n\n【2】Introduction\n------------\n\n【3】Since 1980, the prevalence of obesity among children and adolescents in the United States has tripled . From 1980 to 2008, the prevalence of obesity among children aged 6 to 11 years increased from 6.5% to 19.6%, and among adolescents aged 12 to 19 years obesity increased from 5.0% to 18.1% . Policies and practices in schools may create environments that encourage and reinforce healthy eating and regular physical activity and thus are promising means to stem the rising rates of childhood obesity . To address the youth obesity problem, state and federal authorities have adopted obesity-prevention strategies such as legal mandates to improve school physical activity and nutrition policies. The federal Child Nutrition and WIC Reauthorization Act of 2004 mandated that all US school districts participating in the federally reimbursed school meal programs develop a local school wellness policy by the beginning of the 2006-2007 school year . Washington’s legislature adopted Washington Senate Bill 5436, which was similar to the federal mandate and required each of Washington’s 296 school districts to establish a nutrition and physical fitness policy by August 1, 2005, one year before the federal deadline. No funding was authorized for the implementation of SB 5436. To assist school districts, the Washington State School Directors’ Association developed a model policy regarding access to nutritious foods, opportunities for exercise, and classroom instruction related to nutrition and physical activity . The model policy required that middle school students have an average of 100 minutes per week (minutes per day) of aerobic education activity. High school students were required to complete 2 credits of health and fitness. It further recommended that districts adopt policies to hire certified physical education teachers; provide after-hours access to school facilities for physical activity, fitness, sports, and recreation programs; and identify safe routes to school for walking and biking. For nutrition, the model policy required that school breakfasts and lunches meet the nutritional standards of state and federal school breakfast and lunch programs . In addition, the model policy contained provisions for the availability of fresh fruit and safe drinking water, use of nonfood alternatives for rewards, competitive pricing for healthy food options, adequate time and places to eat lunch, and a nutrition education curriculum focused on knowledge, skills, and assessment of personal eating habits. Limited research has examined the relationship between state and federal legislation related to physical activity, nutrition, and school wellness and local physical activity, nutrition, and wellness policies. Research on the effect of the federal wellness policy mandate on local wellness policies in a nationally representative sample of schools in the 2006-2007 or 2007-2008 school years found that policies were weak overall and varied greatly from district to district . Among the few studies that specifically evaluated changes in local policies before and after the federal wellness mandate, 1 found that overall time available for physical activity did not change in a random sample of 45 rural elementary schools in Colorado after the policy went into effect . Another study of 847 medium and larger schools in the United States found that nutrition components increased significantly and were reported as the components most frequently implemented . Previous studies examining the effect of a policy mandate on school policies have been limited by lack of both a comparison group and strong longitudinal data on policy trends. The primary objective of our study was to assess the effect of Washington’s statewide mandate on physical activity and nutrition (PAN) policies and practices in schools relative to historical trends in Washington and to compare these with those in Oregon, a geographically and demographically similar state without a statewide PAN mandate. We hypothesized that an unfunded policy mandate may not lead to successful policy adoption and that its success depends on both components of PAN policies and characteristics of the school districts. Our secondary objective was to investigate whether the law’s effect was associated with geographic area (urban or rural), school-level socioeconomic status (SES), or the average standardized test scores of schools. Washington and Oregon’s demographic similarities make them ideal for examining how statewide laws lead to local policy changes.  \n\n【4】Methods\n-------\n\n【5】We used public health surveillance data to conduct a secondary data analysis. We performed hierarchical longitudinal linear regression with schools nested in time to test whether the proportion of schools in Washington with PAN policies and practices changed after the implementation of the statewide PAN mandate compared with schools in Oregon where there was no mandate. \n\n【6】### Data and sample\n\n【7】The School Health Profiles survey (Profiles) is conducted biennially by the Centers for Disease Control and Prevention (CDC) in collaboration with state and local education and health agencies. It is a self-administered survey of public secondary school principals and lead health education teachers and is designed to assess school health programs, policies, and activities . CDC uses a random, systematic, equal-probability sampling strategy to produce representative samples of public schools that serve students in grades 6 through 12. In 2004 and 2006, Washington modified this sampling procedure and invited all secondary schools, rather than just a sample, to participate. We obtained identical 2002, 2004, and 2006 data from principal and health educator surveys from the Oregon Public Health Division and the Washington State Department of Health. Study schools had an enrollment of at least 15 students per grade, had a standardized health and physical education curriculum, and were not alternative schools or combined middle/high schools . \n\n【8】### Measures\n\n【9】We used the 30 nutrition-related questions and 26 physical activity-related questions in Profiles. Most item responses in Profiles were binary, indicating the presence or absence of a particular policy, practice, activity, or attribute at the school. We recoded items so that positive responses always indicated a better or desired condition, such as presence of a policy. To create a manageable set of PAN outcome measures we performed a preliminary analysis of principal component factors to identify subsets of associated items within physical activity-related and nutrition-related domains. We began by grouping items in conceptual domains based on item content (eg, nutrition policy, nutrition curriculum). We then extracted the empirical principal components and used structural equation modeling to create item groupings  . In the nutrition domain, the factor analysis procedure confirmed 7 independent sets of items 3 factors relating to classroom educational content and 4 factors relating to nutritional policy. In the physical activity domain, we found 10 independent factors 3 educational content factors and 7 policy-related factors . We combined items within a factor into composite outcome measures by summing. Measures with larger scores reflected a greater number of positive responses to component variables. Once combined, we recoded these measures to a percentage scale (ie, 0-100). We used indicator variables for year, state, and introduction of the law (= presence of policy in Washington schools in year 2006, 0 = otherwise). We considered schools with students in grades 6 to 8 as middle schools and schools with students in grades 9 through 12 as high schools. We examined middle and high schools separately because Washington’s model policy gave different recommendations for each and because of other differences such as availability of competitive foods. As school descriptors, we used publicly available school-level data from the 2 states’ departments of education as covariates in the models. These data included SES (measured as the average percentage of students eligible for the free and reduced-price lunch program during study years; schools with more than one-third of students enrolled in the lunch program were considered low-SES schools); geographic area (urban and not urban using Rural Area Commuting Area codes; schools located in urban and suburban areas were classified as urban; schools located in small towns and frontier areas were classified as not urban) ; and average percentage of students meeting state standards on statewide achievement tests (high performing schools were those with more than half of students meeting state standards). \n\n【10】### Data Analysis\n\n【11】To determine whether the Washington State mandate had an effect on the presence of PAN policies and practices in 2006, we created hierarchical longitudinal linear regression models with schools nested in time by using the GLMMIX procedure in SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina). In the base model, we tested whether the proportion of schools in Washington reported having changed each of the 17 PAN policy and practice composite outcome measures from 2002 and 2004 (before the statewide mandate) to 2006 (after the statewide mandate). We used Oregon as an implicit control to strengthen these estimates. We included indicator variables “state,” “year,” and “law” in models for each PAN outcome and stratified by school type (middle school vs high school): \n\n【12】> Base Model: policy jst \\= _β_ 0 \\+ _β_ 1 state s \\+ _β_ 2 year t \\+ _β_ 3 law st \\+ _ε_ jst \n> \n> > with _j_ \\= 1 to number of schools; \n> > \n> > > _s_ \\= 0 (Oregon), 1 (Washington); and _t_ \\= 0 , 2 , 4 . \n\n【13】The coefficient _β_ 0 indicated the baseline  percentage value of policy in Oregon; the coefficient _β_ 1 for the variable “state” measured the baseline difference between Washington and Oregon; the coefficient _β_ 2 for the variable “year” assessed secular annual trends across the study years; the coefficient _β_ 3 for the variable “law” determined the deviation from the expected trend in Washington in 2006; and _ε_ indicated random error. We stratified the base models by grade and tested the model coefficients for consistency across school type using contrast statements in SAS. To determine whether the Washington State mandate had similar effects across various school subgroups, we added terms to adjust for the covariates SES, geographic area, and school test score and incorporated terms for interactions of these variables with the enactment of the law. We used a logistic link function in all models to test for statistical significance. Model coefficient estimates are presented in percentage-point scale values in our discussion and in Tables 3 through 5 in this article. We considered _P_ values of less than or equal to .05 (sided test) as significant and estimates of 5 percentage points or more as substantively meaningful. The Washington State and Oregon Public Health institutional review boards declared this study to be exempt under 45 CFR 46.101(b).  \n\n【14】Results\n-------\n\n【15】Approximately half of all Washington State schools had high SES or high test scores  At the 2002 baseline, only 31.6% to 41.6% of middle schools and 14.9% to 23.1% of high schools had policies in place that restricted access to competitive foods (what types of foods or times of day for access), and only 9.0% of middle schools and 17.7% of high schools had favorable school food practices (adequate time for lunch and availability of fruits and vegetables at school events) . In the absence of the Washington PAN mandate, the expected trends (based on 2002-2006 Oregon data and 2002-2004 Washington data) for the percentage of Washington schools with nutrition policies and practices generally did not change; however, the percentage of schools with restricted access to competitive foods (ie, what foods and when accessible) increased significantly, and healthy food options (ie, low-fat snacks, fruits, and vegetables) decreased significantly. The expected trends in physical activity policies and practices did not change significantly, except for a decline in the percentage of schools requiring certification for middle school physical education (PE) teachers. Both middle schools and high schools showed a significant (.8-20.0 percentage-point) increase in the number of schools with restricted access to type of competitive foods . For restricted access to competitive foods (time of day), high schools increased by 19.2 percentage points, which is significantly higher than in middle schools  where the increase was not statistically significant. Unexpectedly, healthy food options for middle and high schools declined significantly, by 5.9 and 2.0 percentage points, respectively. Middle schools showed a significant (.4 percentage-point) increase in school food practices (ie, adequate time for lunch and availability of fruits and vegetables at school events). There was no significant increase for high schools. There was a significant increase in the percentage of middle schools that did not allow exemptions from PE for sports, community activities, or academics, and a significant increase in high schools that did not allow such exemptions for community activities . Profiles found no other significant increases for other physical activity policies and practices. When we examined the interactions of each of the 17 policy measures with urban/not urban, high/low SES, and high/low academic performance (middle and high schools combined), we found only 3 significant interactions: restricted access to competitive foods (type of food), which was present in 14.0% more higher-performing than lower-performing schools; restricted access to competitive foods (time of day), which was present in 11.3% fewer urban vs not-urban schools; and facilitators for PA (ie, safe routes to schools, community programs), which were available in 0.3% low- vs high-SES schools.  \n\n【16】Discussion\n----------\n\n【17】We found significant increases in the percentage of middle and high schools reporting the presence of certain PAN policies and practices after the implementation of the Washington State PAN mandate. This is the first published study to present a longitudinal analysis of changes in PAN policies and practices in 1 state by using measures of PAN policies and practices in another state for comparison to control for secular trend. In 2002, few schools in Washington State had restricted access to competitive foods or nutrition-related policies in place, although most schools did have healthy food options available. It is not surprising that restricted access to competitive foods and school food practices were areas for growth, even in absence of the Washington mandate. We were surprised, however, to find a substantial decline in the percentage of middle and high schools offering healthy food options. Because the Profiles questions about healthy food options focused on the availability of healthy foods in vending machines and school stores, these schools may have been eliminating these venues for food purchases rather than reducing the availability of healthier food types in vending machines or school stores. Another explanation for the decline may be changing perceptions of school principals about what constitutes a “healthy” option. Because so many Washington State schools already had physical activity policies and practices in place before the statewide mandate, opportunity for growth in this area was limited. The only area that changed after the implementation of the law was an increase in the number of schools that were not allowing student exemptions from PE because of participation in school sports and other school or community activities. Rather than trying to fund new provisions and programs for physical activity, the elimination of exemptions from PE may have been a budget-neutral way for schools and districts to respond to the mandate. Our study’s findings of a significant increase in nutrition policies and practices and only small improvement in physical activity policies are consistent with a recent study of trends in state-level school nutrition and physical activity policy environments . That study found that schools adopted more food service and nutrition policies than physical activity, education, or weight assessment policies. Similarly, a study of the effects of federal wellness legislation in school districts throughout the United States found that nutrition components were the most frequently implemented . However, even in the presence of nutrition policies, improvements in school food environments are modest , and foods of minimal nutritional value remain available . In contrast to most nutrition policies, many physical activity policies have a direct effect on instruction time (eg, requiring more PE classes or longer classes), and this may be a barrier to school districts adopting such changes. We saw very few differences in change associated with various school-level factors, such as urban/not urban setting, SES, and academic performance. This suggests that the effect of a policy mandate was similar among different school types. Washington State did not allocate funding to support schools in implementing their PAN policies, nor was there meaningful quality assurance of adopted policies or clear punitive measures in place for school districts that failed to effectively implement a policy. Addition of any of these supportive measures might have resulted in different — perhaps greater — policy and practice improvements. This study has several limitations. First, we did not see appreciable increases in physical activity policies and practices associated with the Washington State mandate. This may indicate that Profiles did not ask about the features of the model policy that were most emphasized and acted on by local school districts. A substantial portion of the language in the model policy described the amount of instructional time for physical education, which was not asked about in the Profiles questionnaire. Second, we examined the changes in school policies and practices only 1 year after the Washington statewide PAN mandate went into effect. Arguably, 1 year is too short a time for schools to mobilize their efforts for PAN-related policy changes. However, we did see changes in restricted access to competitive foods, nutrition policy, and reduced exemptions from PE relative to trend, which could confirm that PAN effected changes. Conversely, a federal wellness policy requirement similar to the Washington policy requirement was scheduled for implementation in both Washington and Oregon in the fall of 2006, and the Profiles survey was administered in spring of that year. Thus, some Oregon schools may have already responded to the federal requirement. If this was the case, our analysis of the Washington trends may have underestimated the real effect of the Washington mandate. Finally, this study describes the policy environment in the Pacific Northwest, which may differ in other regions. Future studies should examine the relationship between state and federal laws and the quality of PAN policy implementation in schools and the association of PAN policy with youth PAN outcomes. \n\n【18】### Conclusion\n\n【19】Our results suggest that a statewide mandate had a modest positive effect on PAN policies and practices in schools. Government policy is potentially an effective tool for addressing the childhood obesity epidemic through improvements in PAN environments in schools.  \n\n【20】Tables\n------\n\n【21】#####  Table 1. Characteristics, Middle and High Schools, Washington State and Oregon, School Health Profiles Survey, 2002, 2004, and\n\n【22】YearWashington, n (%)Oregon, n (%)200220042006200220042006Total middle and high schools1,5421,5251,4898329201,038Participated in survey a296 (.2)537 (.2)599 (.2)192 (.1)262 (.5)277 (.7)Did not participate in survey1,246 (.8)988 (.8)890 (.8)640 (.9)658 (.5)761 (.3)Excluded schools b,c57 (.3)86 (.0)92 (.4)29 (.1)46 (.6)50 (.1)Study sample b239 (.4)451 (.0)507 (.6)163 (.9)216 (.4)227 (.9)Middle schools in study sample120 (.7)224 (.6)260 (.6)86 (.3)119 (.9)127 (.2)High schools in study sample d119 (.7)227 (.8)247 (.5)77 (.2)97 (.5)100 (.6)Abbreviations: CI, confidence interval; PA, physical activity; PE, physical education.a Percentage of all schools.b Percentage of all schools participating in School Health Profiles survey.c Schools excluded from the study were alternative schools, combined middle and high schools, and schools with an enrollment of fewer than 15 students per grade or no standardized health and physical education curriculum.d Percentage of schools in study sample. \n\n【23】#####  Table 2. Demographics of Schools Participating in the School Health Profiles Survey in Washington State and Oregon, 20022006 a\n\n【24】Characteristic bWashingtonn (%)(n = 1,197)Oregonn (%)(n = 606)High socioeconomic status c561 (.9)220 (.3)Urban area d796 (.5)319 (.6)High score e631 (.7)385 (.5)Abbreviations: CI, confidence interval; PA, physical activity; PE, physical education.a Data are from Washington State Department of Education  and Oregon Department of Education  and include schools with enrollment of at least 15 students per grade and a standardized health and physical education curriculum; excludes alternative schools, combined middle/high schools, and schools with other or unknown grade combinations. Some schools are represented in more than 1 year.b Averaged over the 2002, 2004, and 2006  study years.c Schools with fewer than one-third of students enrolled in the free and reduced-price lunch program.d Schools in urban areas defined by Rural Area Commuting Area codes .e Schools with more than half of students meeting state standards on statewide achievement tests. \n\n【25】#####  Table 3. Prevalence of Washington State Middle and High Schools With Physical Activity and Nutrition Policy Measures in 2002, 2004, and 2006 and Estimated Annual Trend\n\n【26】Policy Outcome Measure2002,Mean, % (% CI)2004 Mean,&nbsp;% (% CI)2006 Mean, % (% CI)Estimated Annual Trend a , (% CI)P Value bNutritionFood-choice educationMiddle school75.4 (.3-85.6)79.9 (.8-87.1)88.5 (.0-93.0)1.6 (.6 to 1.6).10High school87.1 (.8-94.5)89.1 (.9-94.2)92.5 (.2-95.8)Healthy-weight educationMiddle school77.3 (.1-87.5)88.1 (.6-93.5)89.7 (.6-93.8)1.0 (.0 to 1.1).13High school89.2 (.9-95.5)91.4 (.7-96.2)92.7 (.2-96.2)General nutrition educationMiddle school73.6 (.5-82.7)79.3 (.1-85.6)82.5 (.8-87.1)0.4 (.4 to 0.5).37High school86.3 (.0-92.5)86.5 (.7-91.4)86.6 (.0-90.2)Restricted access competitive foods (type of food)Middle school31.6 (.7-38.5)35.4 (.3-41.5)63.6 (.9-68.4)3.2 (.2 to 3.2)&lt;001High school14.9 (.6-21.3)8.8 (.1-12.5)37.8 (.4-43.3)Restricted access competitive food (time of day)Middle school41.6 (.0-48.2)49.5 (.8-55.2)68.3 (.8-72.7)3.9(.9 to 4.0)&lt;001High school23.1 (.4-28.8)17.9 (.3-22.4)48.0 (.6-53.3)Healthy food optionsMiddle school66.0 (.9-72.0)73.0 (.3-77.6)64.1 (.6-67.7)−0.2 (.3 to −0.2).03High school78.3 (.4-83.3)76.5 (.2-77.9)75.0 (.1-77.9)School food practicesMiddle school9.0 (.3-12.6)10.6 (.9-14.3)22.2 (.6-25.9)0.3 (.3 to 0.4).55High school17.7 (.7-23.7)11.6 (.7-15.5)18.6 (.0-22.1)Physical activityLife fitness knowledgeMiddle school69.6 (.2-79.1)79.8 (.8-85.8)75.9 (.5-81.3)0.4 (.4 to 0.4).59High school77.9 (.7-85.1)78.5 (.5-84.5)80.5 (.8-85.3)Skills knowledgeMiddle school60.1 (.5-70.7)74.2 (.3-82.0)68.5 (.8-75.2)−0.1 (.1 to −0.1).35High school71.5 (.4-80.6)73.1 (.6-80.7)73.8 (.0-79.6)Safety knowledgeMiddle school64.9 (.3-74.5)75.1 (.8-82.4)73.8 (.8-79.8)−0.2 (.2 to −0.2).62High school75.8 (.7-83.8)77.7 (.1-84.4)76.2 (.6-81.7)Facilitators for PAMiddle school77.4 (.4-82.3)71.8 (.9-76.7)70.5 (.3-73.7)−0.7 (.7 to −0.7).06High school56.7 (.7-62.8)53.7 (.3-59.1)50.3 (.9-53.7)No PE exemptions for sportsMiddle school76.3 (.0-83.6)74.8 (.3-81.3)87.4 (.3-90.5)−0.4 (.4 to −0.4).38High school58.7 (.6-66.8)61.8 (.1-68.5)70.7 (.6-74.8)No PE exemptions for community activitiesMiddle school84.2 (.8-91.6)91.0 (.2-95.9)91.5 (.8-94.1)−1.0 (.0 to −0.9).15High school69.6 (.1-79.0)58.0 (.1-66.9)81.3 (.3-85.3)No PE exemptions for academicsMiddle school75.8 (.3-84.2)71.5 (.8-79.3)82.0 (.4-85.6)−1.1 (.1 to −1.1).17High school69.6 (.1-79.0)63.3 (.7-72.0)73.2 (.7-77.7)PE teacher certificationMiddle school90.5 (.8-96.1)87.9 (.3-93.5)84.5 (.9-89.2)−1.5 (.5 to −1.5).01High school91.7 (.1-97.2)92.7 (.2-97.3)94.4 (.3-97.5)PE completion requirementMiddle school16.8 (.5-24.2)13.7 (.8-19.7)12.9 (.5-17.3)−0.8 (.8 to −0.7).18High school92.3 (.8-97.8)91.9 (.0-96.7)90.9 (.0-94.8)PE requirementMiddle school100.0 (.0-100.0)97.8 (.3-100.3)99.6 (.7-100.4)−0.2 (.2 to −0.2).48High school94.8 (.4-99.3)96.8 (.7-99.9)96.8 (.5-99.1)Abbreviations: CI, confidence interval; PA, physical activity; PE, physical education.a Estimated annual trend based on Washington and Oregon data as percentage point increase or decrease in absence of 2005 Washington law, middle and high schools combined.b _P_ values from logit model indicate the significance of the increase or decrease in the annual trend. \n\n【27】#####  Table 4. Effect of Washington State Physical Activity and Nutrition Mandate on School Nutrition Policies and Practices,\n\n【28】Policy Outcome MeasurePercentage Point Change in Schools With Policy (% CI) aP Value bStrata Contrast: P Value, Comparison of Middle Schools to High Schools cFood-choice educationMiddle school0.7 (.2 to 8.6).62.88High school0.3 (.9 to 6.4).64Healthy-weight educationMiddle school−1.3 (.7 to 6.1).81.89High school-4.0 (.7 to 1.6).64General nutrition educationMiddle school−1.4 (.0 to 6.2).80.46High school−4.3 (.0 to 2.4).64Restricted access competitive foods (type of food)Middle school20.0 (.5 to 28.5)&lt;001.44High school18.8 (.4 to 27.2)&lt;001Restricted access competitive food (time of day)Middle school10.7 (.5 to 18.9).06.03High school19.2 (.1 to 27.3)&lt;001Healthy food optionsMiddle school−5.9 (.3 to 0.5).002.27High school−2.0 (.6 to 3.5)&lt;001School food practicesMiddle school10.4 (.4 to 16.4).001.06High school4.6 (.5 to 10.6).14Abbreviation: CI, confidence interval.a Effect of the statewide law on selected physical activity and nutrition policies and practices in percentage point terms as measured by the magnitude of the difference from the expected trend in absence of the law, taking into account the trend in Oregon.b Significance of the difference from the expected trend.c _P_ values from logit model \n\n【29】#####  Table 5. Effect of Washington State Physical Activity and Nutrition Mandate on School Physical Activity Policies and Practices,\n\n【30】Policy Outcome MeasurePercentage Point Change in Schools With Policy (% CI) aP Value bStrata Contrast: P Value, Comparison of Middle Schools to High Schools cLife fitness knowledgeMiddle school−3.8 (.4 to 4.0).97.31High school−0.6 (.9 to 6.7).32Skills knowledgeMiddle School−0.4 (.8 to 10.0).33.73High school−1.6 (.6 to 7.4).48Safety knowledgeMiddle school0.6 (.2 to 10.4).68.88High school−1.3 (.5 to 6.9).53Facilitators for PAMiddle school−1.1 (.0 to 4.8).08.29High school−5.9 (.0 to 0.2).43No PE exemptions for sportsMiddle school16.6 (.5 to 23.7)&lt;001.004High school8.7 (.2 to 15.2).24No PE exemptions for community activitiesMiddle school12.8 (.1 to 19.5)&lt;001.63High school14.4 (.8 to 22.0)&lt;001No PE exemptions for academicsMiddle school18.1 (.7 to 26.5)&lt;001.02High School9.0 (.2 to 18.2).06PE teacher certificationMiddle school−2.3 (.9 to 5.3).56.16High school2.3 (.7 to 7.1).39PE completion requirementMiddle school−5.3 (.2 to 1.6).13.10High school0.8 (.5 to 8.1).84PE requirementMiddle school1.7 (.3 to 3.7).09.01High school−1.8 (.2 to 0.6).15a Effect of the statewide law on selected physical activity and nutrition policies and practices in percentage terms as measured by the magnitude of the difference from the expected trend in absence of the law.b Significance of the difference from the expected trend.c _P_ values from logit model; test used  \n\n【31】 Appendices\n-----------\n\n【32】### Table 1. Nutrition Policy Outcome Measures: Nutrition Factors Generated From Factor Analysis and Component Variables From Individual Questions in School Health Profiles, 2002-2006\n\n【33】Policy Outcome Measure: FactorComponent Variables From Individual Questions in ProfilesFood-choice educationTeach to choose foods that are low in fat, saturated fat, and cholesterolTeach to eat more fruits, vegetables, and grainsTeach to use sugars in moderationTeach to use salt and sodium in moderationTeach to eat more calcium-rich foodHealthy-weight educationTeach the risks of unhealthy weight control practicesTeach to accept body size differencesTeach to balance food intake and physical activityTeach about eating disordersGeneral nutrition educationTeach food guidance using MyPyramidTeach benefits of healthy eatingTeach to prepare healthy meals and snacksTeach food safetyTeach use of food labelsRestricted access to competitive foods (type of food)Report not able to purchase chocolate candy from school vending machines or storeReport not able to purchase other kinds of candy from school vending machines or storeReport not able to purchase salty snacks not low in fat (eg, potato chips from school vending machines or storeReport not able to purchase soft drinks or fruit drinks that are not 100% juice from school vending machines or storeRestricted access to competitive foods (time of day)Report not able to purchase snack food or beverages before classes in the morningReport not able to purchase snack food or beverages during lunch periodReport not able to purchase snack food or beverages during any school hours when meals are not servedHealthy food optionsReport able to purchase 1% or skim milk from school vending machines or storeReport able to purchase 2% or whole milk (plain or flavored)Report able to purchase low-fat cookies, crackers, cakes, pastries, baked goods from school vending machines or storeReport able to purchase low-fat salty snacks from (eg, pretzels, baked chips) from school vending machines or storeReport able to purchase fruit or vegetables from school vending machines or storeReport able to purchase bottled water from school vending machines or storeReport able to purchase sports drinks from school vending machines or storeSchool food practicesReport students usually have 20 or more minutes to eat lunch once seatedReport school/district policy that fruits/vegetables served at parties, after-school programs, staff/parent meetings, concessions, etc.\n\n【34】### Table 2. Physical Activity Policy Outcome Measures: Physical Activity Factors Generated From Factor Analysis and Component Variables From Individual Questions in School Health Profiles, 2002-2006\n\nPolicy Outcome Measures: FactorComponent Variables From Individual Questions in ProfilesLife fitness knowledgeTeach difference between PA, exercise, and fitnessTeach overcoming barriers to PATeach decreasing sedentary activities such as watching televisionTeach opportunities for physical activities in the communityTeach health-related fitness (cardio, endurance, strength, flexibility, composition)Teach the physical, psychological, or social benefits of PASkills knowledgeTeach how much PA is enough (frequency, intensity, time, type)Teach phases of a workout (warm-up, workout, cool-down)Teach monitoring progress to reach individual PA plan goalsTeach developing an individualized PA planSafety knowledgeTeach preventing injury during PATeach weather-related safety (heat stroke, hypothermia, sunburn)Teach dangers of using performance-enhancing drugs such as steroidsFacilitators for PAProvide community sponsored programs at school outside school hoursPromote walking/biking to and from school (activities, safe/preferable routes, storage)Provide transportation home after-school intramural activity or PA clubsNo PE exemptions for sportsNo exemption from PE because of student participation in school sportsNo exemption from PE because of student participation in community sportsNo exemption from PE because of students’ high physical fitness competence scoreNo PE exemptions for community activitiesNo exemption from PE because of student participation in community serviceNo exemption from PE because of student participation in school activities (eg, ROTC)No PE exemptions for academicsNo exemption from PE because of student participation in vocational trainingNo exemption from PE because of student participation in other courses (eg, math, science)PE teacher certificationRequirement for newly hired PE teachers to be certified, licensed, or endorsed by the statePE completion requirementRequirement for students to repeat physical education if course is failedPE requirementRequirement for physical education for students in grades 6 through 12Abbreviations: PA, physical activity; PE, physical education; ROTC, Reserve Office Training Corps.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6f8e4982-e34c-41ba-b819-b92e8d9f7cbf", "title": "West Nile Virus Infection in Plasma of Blood and Plasma Donors, United States", "text": "【0】West Nile Virus Infection in Plasma of Blood and Plasma Donors, United States\nWest Nile virus (WNV) is a flavivirus endemic to the United States; typically, hundreds of clinical cases of infection occur each year. The observed number of clinical WNV infections as collated by ArboNET  and the incidence of asymptomatic WNV infections as shown by nucleic acid testing (NAT) of the US blood supply  indicate that ≈3 million WNV infections occurred in humans during 1999– 2008.\n\n【1】Because the immune system elicits WNV neutralizing antibodies in response to WNV infection, detectable levels of WNV neutralizing antibodies in the blood of persons with previous WNV infection is expected. Consequently, lots of immune globulin-intravenous (human) (IGIV) manufactured from plasma collected in the United States contain WNV neutralizing antibodies . Those IGIV lots, each prepared from several thousand plasma donations to ensure a broad spectrum of antibodies, can be used as an epidemiologic tool that enables the surveillance of thousands of persons in a community through analysis of comparatively few samples. In this study, we demonstrated the increasing trend of WNV-neutralizing antibody titers in lots of IGIV.\n\n【2】Comparing these titers with those of persons with confirmed past WNV infection provides an independent measure of the percentage of the US population previously infected with WNV. Several WNV vaccine trials are ongoing or imminent, so information about the prevalence of past WNV infection in the United States is valuable for planning the demonstration of vaccine efficacy. Low incidence and lack of highly WNV-endemic areas in the United States preclude classic vaccine field trials because of study size requirements and cost-logistics difficulties.\n\n【3】### The Study\n\n【4】The WNV neutralization titers of several US plasma–derived IGIV products (Gammagard Liquid/KIOVIG; Gammagard S/D/ Polygam S/D; Iveegam EN \\[Baxter Healthcare Corporation, Westlake Village, CA, USA\\]) and plasma samples obtained from US blood donors after a NAT-confirmed WNV infection were determined by an infectivity assay as earlier described , adapted to a classical microneutralization format . WNV neutralization titers (i.e. the reciprocal dilution of a 1:2 series resulting in 50% neutralization \\[NT 50  ; detection limits <0.8 for undiluted IGIVs and <7.7 for 1:10 prediluted serum\\]) are reported as the mean ± SEM. An unpaired _t_ test was used to evaluate whether titer differences between 2 groups were statistically significant.\n\n【5】Using an extrapolation derived from screening the US blood supply for WNV , we calculated the average annual number of WNV infections in the United States for 1999–2008. The total number of neuroinvasive cases reported for those years to the US Centers for Disease Control and Prevention (CDC) through ArboNET was multiplied by 256 (i.e. the factor between all WNV infections and neuroinvasive cases). The cumulative infection rate for each year during 1999–2008 was then calculated by dividing the infections occurring up to a specific year by the US population for that year .\n\n【6】Although WNV was first introduced into the United States in 1999, only in 2003 did the mean WNV neutralization titers of IGIV lots released to the market start to increase markedly . According to extrapolations from the WNV screening of the US blood supply , by 2003, an estimated 0.5% of the US population had been infected with WNV, although most infections were asymptomatic.\n\n【7】A delay of ≈1 year occurs between the collection of plasma and the release of IGIV lots to the market; thus, the WNV-positive IGIV lots in 2003 reflect the larger number of WNV infections occurring in 2002. Using the same extrapolations from the US blood supply , we found that the ≈0.1% annual increments in the proportion of the US population with past WNV infection follow a straight line (r 2  \\= 0.9996), generally paralleled by the mean WNV neutralization titers of IGIV lots. During 2005–2008, when large numbers of lots of a single IGIV product (Gammagard Liquid) could be analyzed, the WNV neutralization titer increased by 3.6 per year (r 2  \\= 0.9793).\n\n【8】US plasma-derived IGIV lots released during 2008 showed variable WNV neutralization titers ranging from 2.8 to 69.8; mean ± SEM titer was 21 ± 1 (n = 256) . Compared with titers shown to be protective in an animal model of WNV infection (equivalent to >21 by the current assay) , ≈40% of the 2008 IGIV lots had higher titers.\n\n【9】Plasma obtained from persons with NAT-confirmed WNV infection had even higher titers; mean ± SEM titer was 208 ± 40 for 30 persons available for testing. When results were corrected for the immunoglobulin (Ig) G concentration in plasma (≈1%), compared with the 10% IGIV preparations, the mean neutralization titer of the plasma samples was ≈100× higher than that of the IGIV lots tested (vs. 21).\n\n【10】### Conclusions\n\n【11】The most comprehensive collation of information about the incidence of WNV infection in the United States is available from ArboNET. When that information is combined with information obtained from the nationwide screening of the blood supply for WNV RNA by NAT , the current prevalence of past WNV in the US population is estimated to be ≈1%.\n\n【12】Busch et al. has noted that large-scale, community-based serologic surveys are hardly feasible because of their expense and because WNV ELISA assays are possibly biased by cross-reactions with other flaviviruses . Nevertheless, 7 seroepidemiologic studies have been performed . Cumulatively, 5,503 persons were tested for WNV infection by ELISA, and the results have shown highly divergent seroprevalence rates ranging between 1.9%  and 14.0% .\n\n【13】The use of IGIV lots, each representing the serostatus of several thousand donors in 1 sample, makes seroepidemiology practical  because it allows a large donor population to be surveyed by analyzing comparably few samples. The use of a more complex yet functional virus neutralization assay minimizes concerns about cross-reactivity with flaviviruses of other serocomplexes (e.g. dengue virus) that occasionally circulate in the US population. Also, epidemiologic considerations render interference by St. Louis encephalitis virus, a flavivirus within the same serocomplex, highly unlikely . The specificity of the neutralization assay was confirmed by testing IGIV lots manufactured from European-derived plasma against tick-borne encephalitis virus, a flavivirus closely related to WNV and circulating in Europe. Although these lots contained high neutralization titers against tick-borne encephalitis virus, only 1 of 20 had a detectable neutralization titer of 5 against WNV (unpub. data).\n\n【14】In this study, we determined that the mean titer of samples obtained during 2003–2008 from persons with a confirmed diagnosis of WNV infection was 100× higher than the mean titers of IGIV lots produced in 2008. This determination provides an independent experimental measure of the frequency of past WNV infection in the general US population, as reflected by the plasma/blood donor community, and the results correlate well with results of previously published theoretical extrapolations , which estimated that ≈1% of the population has already been infected with WNV.\n\n【15】The increasing levels of WNV neutralizing antibodies in IGIV lots from US plasma and the particularly high titers in donors who have had a WNV infection suggest the possibility of preparing IGIV products with sufficiently high titers to be useful for WNV prophylaxis or treatment. Several ongoing or imminent WNV vaccine clinical trials stress the practical value of an independent confirmation of extrapolations that estimate the percentage of the US population with past WNV infection. Knowing the percentage of preexisting WNV seroprevalence as well as estimates of the mostly asymptomatic incidence rates  can be of vital importance in designing vaccine trials.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ce4de028-e7b9-4f32-9c2a-b58b801d5867", "title": "Studies of Reservoir Hosts for Marburg Virus", "text": "【0】Studies of Reservoir Hosts for Marburg Virus\nMarburg virus (MARV) and Ebola virus, members of the family _Filoviridae,_ cause outbreaks of severe hemorrhagic fever in Africa. Although humans have on occasion acquired infection from contact with tissues of diseased nonhuman primates and other mammals, the reservoir hosts of the viruses in nature remain unknown.\n\n【1】An outbreak of Marburg hemorrhagic fever ran a protracted course in the gold-mining village of Durba, northeastern Democratic Republic of the Congo, from October 1998 through September 2000. The outbreak involved 154 patients (confirmed and 106 suspected cases); the case-fatality ratio was 83% . Primary cases occurred in young male miners and spread as secondary cases to family members and, less frequently, to healthcare workers and others in the community. Most cases occurred in Durba, but a few secondary cases occurred elsewhere, including nosocomial infections in nearby Watsa village, where severely ill patients sought care. The occurrence of sporadic cases and short chains of human-to-human transmission suggested that infection had been repeatedly introduced into the human population; this suggestion was substantiated by the detection of at least 9 genetically distinct viruses circulating during the outbreak. Identical sequences of MARV were found in patients within but not across clusters of epidemiologically linked cases, although viruses with the same sequences reappeared at irregular intervals during the outbreak. Most (%) affected miners worked underground in Goroumbwa Mine, rather than in the 7 opencast mines in the village. Cessation of the outbreak coincided with the flooding of Goroumbwa Mine. Interviews with long-term residents and healthcare workers and review of hospital records showed that a _syndrome hémorragique de Durba_ \\[hemorrhagic syndrome of Durba\\] had been associated with the mine since at least 1987, and a survivor of a 1994 outbreak was found to have antibodies against MARV. The fauna of Goroumbwa Mine included bats, rodents, shrews, frogs, snakes, cockroaches, crickets, spiders, wasps, and moth flies . We present the results of virus reservoir host studies conducted during the outbreak.\n\n【2】### Methods\n\n【3】In parallel with human epidemiologic studies, visits were made to Durba in May and October 1999 to collect specimens for virus ecostudies. The ecostudies were approved by the International Scientific and Technical Committee for Marburg Hemorrhagic Fever Control, which was coordinated by the World Health Organization on behalf of the government of the Democratic Republic of the Congo. In view of the epidemiologic findings during the outbreak, emphasis was placed on the fauna of Goroumbwa Mine. Bats were caught with mist nets at mine entrances; rodents and shrews were caught live with Sherman traps within and close to the mine; and arthropods (cockroaches, crickets, spiders, wasps, and moth flies, plus streblid, nycteribiid, and mite parasites of bats) were collected by hand or with sweepnets. Vertebrates were euthanized and dissected on site. Blood samples were collected; and samples of liver, lung, spleen, kidney, testes, brain, salivary glands, and fetuses of pregnant females were preserved along with the arthropods in liquid nitrogen dry-shipping containers for transport to the National Institute for Communicable Diseases in South Africa. Extra liver samples were collected for phylogenetic studies on bats and rodents, and formalin-fixed tissue samples were kept for possible histopathologic and immunohistochemical examination. Carcasses were fixed in formalin for α-taxonomy purposes.\n\n【4】Vertebrate tissue and arthropod suspensions were processed and tested for filovirus nucleic acids by reverse transcription–PCR (RT-PCR) and nested PCR by using filovirus-specific large (L) protein gene primers and nested MARV-specific viral protein 35 (VP35) primers as described for samples from human patients during the outbreak . Nucleotide sequencing of amplicons and sequence data analysis were also performed as described previously , except that MEGA version 3.1 software was used . Initial RT-PCR and nested PCR were performed with pooled tissue samples of individual vertebrates; when possible, for specimens that produced positive results, all tissues were retested separately. In attempts to isolate virus as detected by indirect immunofluorescence, suspensions (≈10%) of vertebrate tissues pooled for individual animals and arthropods pooled by species were subjected to 3 serial passages in Vero 76 cell cultures. Serum samples from bats and rodents were tested for antibody to MARV by ELISA by using a modification of the technique described previously for human serum . ELISA antigen consisted of lysate of Vero cell cultures infected with the Musoke strain of MARV. Bat antibody was detected with antibat immunoglobulin–horseradish peroxidase conjugate (Bethyl, Montgomery, AL, USA) and rodent antibody with antimouse immunoglobulin conjugate (Zymed Laboratories, San Francisco, CA, USA). Net ELISA optical density values were expressed as percent positivity (PP) of a human serum sample confirmed positive for MARV and used as an internal control. Cutoff values for recording positive results were deliberately selected to be stringent at 3 × (mean + 3SD) PP values determined for stored bat (n = 188) and rodent (n = 360) serum samples that had been collected for unrelated purposes in Kruger National Park, South Africa, from 1984 through 1994, and tested at a dilution of 1:100. The Kruger bat samples were collected from 3 species of fruit bats (Megachiroptera_ ) and 12 species of insectivorous bats (Microchiroptera_ ), including samples from 56 _Chaerephon pumila_ , 32 _Rousettus aegyptiacus_ , 27 _Mops condylurus_ , 16 _Hipposideros caffer,_ plus 57 samples from 11 other species.\n\n【5】### Results and Discussion\n\n【6】The numbers of specimens collected, plus the results of RT-PCR, nested PCR, attempts to isolate virus in cell culture, and ELISA antibody determinations, are summarized in the Table . With the exception of a _Nycteris hispida_ bat, which was caught near a house in Durba, all specimens were collected within Goroumbwa Mine or its immediate surroundings. An estimated minimum of 10,000 Egyptian fruit bats (R. aegyptiacus_ ) roosted in the mine, clustered within the upper galleries. Although the numbers of insectivorous bats were difficult to estimate because these bats roosted mainly in the deeper recesses of the mine, the catch rates indicated substantial numbers of the eloquent horseshoe bat (Rhinolophus eloquens_ ) and the greater long-fingered bat (Miniopterus inflatus_ ). Few microchiropterans were caught in May, but catch rates improved in October after adjustment of trapping hours and the gauge of mist nets used. Pregnancy was recorded in 12 (%) of 50 _R. aegyptiacus_ females in May and in 2 (.2%) of 47 females in October; descended testes were found in 2 (%) of 33 males in May and 19 (%) of 76 in October. The only indication of breeding activity observed in microchiropterans was that 1/7 _Rh. eloquens_ females was pregnant in May.\n\n【7】The L primer RT-PCR, which was applied to all specimens, produced no positive result. In contrast, the nested MARV VP35 PCR, which was applied only to specimens collected in October 1999, produced positive results on specimens from 12 bats: 1 (.0%) of 33 _M. inflatus_ , 7 (.6%) of 197 _Rh. eloquens,_ and 4 (.1%) of 127 _R. aegyptiacus_ . Nested VP35 PCR on individual tissues of the positive bats produced positive results for liver, spleen, kidney, lung, salivary gland (/5 bats), and heart (/5 bats). Attempts to isolate virus in cell cultures from pooled organs were uniformly negative. Applying an ELISA cutoff value of 16.4 PP, determined as 3 × (mean + 3 SD) of values recorded for 188 bat serum samples from Kruger National Park, antibody activity to MARV was detected by ELISA in 20 (.7%) of 206 _Rh. eloquens_ and in 32 (.5%) of 156 _R. aegyptiacus_ serum specimens from Durba . Prevalence of nucleic acid or antibody did not differ significantly between male and female bats or adults and juveniles (determined on the basis of body mass) or between bats collected in May and October. The only RT-PCR–positive bat that had antibody was a _Rh. eloquens_ male collected in October. All other investigations produced negative results.\n\n【8】Phylogenetic analysis of the sequences determined for the twelve 302-nt MARV VP35 gene fragments amplified from bat specimens (GenBank accession nos. EU11794–EU118805) showed that 6 corresponded to sequences previously determined for virus isolates from humans during the epidemic , 1 corresponded to a 1975 human isolate from Zimbabwe, and the remaining 5 represented novel sequences; these last 6 variants from bats, combined with the 9 variants from humans, make a total of 15 distinct MARV sequences found to have been in circulation during the Durba epidemic . Although the differences observed between MARV sequences during the 1999 Durba outbreak were minor, the sequences were consistent in sequential isolates from individual patients and within groups of epidemiologically linked patients (e.g. intrafamilial transmission). In addition, phylogenetic analysis on L gene fragment sequences showed that the 33 virus isolates from patients resolved into exactly the same 9 groups as did the VP35 gene fragments of the same isolates . Nucleotide sequence divergences of up to 21% observed among the VP35 gene fragments detected in the Durba patients and bats are representative of the diversity of the complete MARV genome and encompass the entire genetic spectrum of isolates obtained over the past 40 years . This fact indicates that the virus evolves slowly and that any possible relationship with bats in the Goroumbwa Mine must have extended over a long period. The diversity of MARV sequences detected suggests compartmentalized circulation of virus in bat colonies, as would occur if the species involved existed as metapopulations, spatially discrete subgroups of the same species, as opposed to panmictic populations in which there are no mating restrictions . Alternatively, bats could be intermediate hosts of the virus.\n\n【9】The history of filovirus outbreaks shows several instances from which it can be inferred that bats may have served as the source of infection. Anecdotal evidence indicates that during shipment from Uganda, the monkeys associated with the first outbreak of Marburg hemorrhagic fever in Europe in 1967 were kept in a holding facility on a Lake Victoria island that had large numbers of fruit bats. In the second filovirus outbreak in 1975, Marburg hemorrhagic fever developed in 2 tourists who had slept in rooms with insectivorous bats at 2 locations in Zimbabwe . In the first recognized outbreak of Ebola hemorrhagic fever in 1976, the first 6 patients had worked in a cotton factory in Sudan in which insectivorous bats were present . In 2 separate incidents in 1980 and 1987, infection with MARV was putatively linked with entry into Kitum Cave on the slopes of Mount Elgon in Kenya, where fruit and insectivorous bats are present . In 1994, a clan of chimpanzees in a forest reserve in Côte d’Ivoire had been observed feeding in a wild fig tree with fruit bats for 2 weeks before an outbreak of fatal disease, caused by a new strain of Ebola virus, occurred . The Reston strain of Ebola virus, which is apparently nonpathogenic for humans, was imported into the United States and Europe in infected monkeys from the Philippines; on each occasion, the animals came from a holding facility where they were potentially exposed to the excretions of large numbers of fruit bats .\n\n【10】The circumstantial evidence in the Marburg hemorrhagic fever outbreak in Durba strongly implicates Goroumbwa Mine as the source of human infection. At least 9 genetic variants of MARV circulated in humans during the outbreak. And because laboratory testing was limited to a few patients, additional variants could have been undetected, as substantiated by our evidence of 6 more variants in bats. The evolution and perpetuation of multiple genetic variants of virus in a fixed location would require a suitably large reservoir host population with constant recruitment through reproduction or migration of susceptible individuals, as generally occurs in small vertebrate and invertebrate populations such as the bat population of Goroumbwa Mine. Failure to isolate live virus may be because it was present in very low concentrations, either early or late in the course of infection. This was the first detection of filovirus nucleic acid and antibody in bats, a phenomenon which was subsequently demonstrated with Ebola virus and MARV nucleic acids and antibodies in fruit bats collected in 2002 and 2005 in Gabon, where it again proved impossible to isolate live virus .\n\n【11】The nature of filovirus infection in bats may vary with age and reproductive status. A seasonal pattern in the occurrence of human disease was noted over the 2 years of the epidemic in Durba; transmission began in October–November and peaked in January–February . In the caves of Mount Elgon in Kenya, Egyptian fruit bats breed in March and September; at other sites in Kenya, the timing varies markedly; no data are available for the Durba area . The remaining species of bats found in Goroumbwa Mine breed annually, but details for this location are unknown. Thus, although the reproductive status of bats differed in May and October, evidence is insufficient to establish a clear link between breeding patterns of bats in Goroumbwa Mine and the occurrence of Marburg hemorrhagic fever. Nevertheless, many examples in human and veterinary medicine indicate that the outcome of virus infection, development of carrier status, and shedding of virus are influenced by age and reproductive status, including stage of gestation at which infection occurs and the conferral to and duration of maternal immunity in progeny . Likewise, whether insectivorous bats, fruit bats, or both, are likely to serve as the primary source of infection and whether particular species are involved with secondary transmission of infection to other species is unclear. The evolutionary distinction may exist between cave-roosting bats as hosts of MARV and forest bats as hosts of Ebola virus. Moreover, the ultimate source of infection could prove to be external, such as bat parasites or seasonally active insects in the bats’ diet. Experimental infections in colonized bats could answer some of these questions .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9e2c27ae-72da-496a-929a-40893c9ee667", "title": "Evaluation of the Healthy Lifestyles Initiative for Improving Community Capacity for Childhood Obesity Prevention", "text": "【0】Evaluation of the Healthy Lifestyles Initiative for Improving Community Capacity for Childhood Obesity Prevention\nAbstract\n--------\n\n【1】**Purpose and Objectives**\n\n【2】Policy, systems, and environmental approaches are recommended for preventing childhood obesity. The objective of our study was to evaluate the Healthy Lifestyles Initiative, which aimed to strengthen community capacity for policy, systems, and environmental approaches to healthy eating and active living among children and families.\n\n【3】**Intervention Approach**\n\n【4】The Healthy Lifestyles Initiative was developed through a collaborative process and facilitated by community organizers at a local children’s hospital. The initiative supported 218 partners from 170 community organizations through training, action planning, coalition support, one-on-one support, and the dissemination of materials and sharing of resources.\n\n【5】**Evaluation Methods**\n\n【6】Eighty initiative partners completed a brief online survey on implementation strategies engaged in, materials used, and policy, systems, and environmental activities implemented. In accordance with frameworks for implementation science, we assessed associations among the constructs by using linear regression to identify whether and which of the implementation strategies were associated with materials used and implementation of policy, systems, and environmental activities targeted by the initiative.\n\n【7】**Results**\n\n【8】Each implementation strategy was engaged in by 30% to 35% of the 80 survey respondents. The most frequently used materials were educational handouts (.3%) and posters (.3%). The most frequently implemented activities were developing or continuing partnerships (.5%) and reviewing organizational wellness policies (.3%). Completing an action plan and the number of implementation strategies engaged in were positively associated with implementation of targeted activities (action plan, effect size = 0.82; number of strategies, effect size = 0.51) and materials use (action plan, effect size = 0.59; number of strategies, effect size = 0.52). Materials use was positively associated with implementation of targeted activities (effect size = 0.35).\n\n【9】**Implications for Public Health**\n\n【10】Community-capacity–building efforts can be effective in supporting community organizations to engage in policy, systems, and environmental activities for healthy eating and active living. Multiple implementation strategies are likely needed, particularly strategies that involve a high level of engagement, such as training community organizations and working with them on structured action plans.\n\n【11】Introduction\n------------\n\n【12】Childhood obesity is a major public health concern in the United States and the world. The number of obese children and adolescents (aged 2–19 y) in the United States reached 12.7 million by 2013–2014, approximately 17% of the overall youth population . Obesity contributes to health problems, including physical problems (eg, diabetes, heart disease)  and psychological problems (eg, depression, anxiety)  and can last into adulthood . Additionally, obesity disproportionately affects marginalized communities, including racial/ethnic minority populations and people with low socioeconomic status .\n\n【13】Policy, systems, and environmental approaches are recommended as evidence-based practices for preventing childhood obesity, and these approaches are increasingly prevalent . For example, several national initiatives exist to increase community capacity for policy, systems, and environmental approaches to childhood obesity prevention . Such initiatives often include strengthening community capacity and linkages across multiple sectors to support healthy eating and active living , key priorities for obesity prevention. It is important to evaluate these efforts to determine whether they result in the desired outcomes and inform the refinement of strategies to maximize impact.\n\n【14】Implementation science has emerged in part to increase uptake of evidence-based practices into practice. Implementation science frameworks such as that posited by Proctor and colleagues  define _implementation strategies_ as approaches for supporting the adoption and implementation of evidence-based practices (eg, across the community). Successful implementation strategies are expected to lead to multiple favorable implementation outcomes, such as increased uptake, penetration, fidelity, and sustainability of the practices in participating organizations . Although much of implementation science focuses on clinical service providers , community organizations can also be considered service providers, particularly through their role in providing supportive policy, systems, and environments for childhood obesity prevention.\n\n【15】We leveraged implementation science concepts to inform the evaluation of the Healthy Lifestyles Initiative, a multisector capacity-building initiative to support policy, systems, and environmental approaches to increase healthy eating and active living and reduce rates of childhood obesity and related disparities in the Kansas City, Missouri, and Kansas City, Kansas, metropolitan area. The Healthy Lifestyles Initiative sought to increase uptake and penetration of these approaches in participating organizations through the use of multiple implementation (ie, engagement) strategies. Rather than promoting adoption of a single evidence-based intervention, the Healthy Lifestyles Initiative promoted evidence-based policy, systems, and environmental practices  more generally to support adoption and implementation across a range of community sectors, including schools, child care providers, health care providers, businesses, nonprofit community organizations, and government organizations (eg, health departments, parks and recreation departments). Our evaluation, performed through a research–practice partnership, attempted to gauge the effect of the implementation strategies on uptake and penetration of the policy, systems, and environmental approaches targeted. Knowing whether and which implementation strategies used were effective can inform program refinements and continuation and support similar programs in other communities.\n\n【16】Purpose and Objectives of the Healthy Lifestyles Initiative\n-----------------------------------------------------------\n\n【17】A team of local stakeholders from the community and health care sectors, including Children’s Mercy Kansas City, participated in the 2011–2013 national learning collaborative, _Collaborate for Healthy Weight_ , coordinated by the National Initiative for Children’s Healthcare Quality and supported by the Prevention and Public Health Fund . The team included representatives from the hospital’s community program and primary care clinics, the local health department, the YMCA, and a nonprofit community organization specializing in healthy lifestyle policy change. The collaborative supported nearly 40 teams in the United States in strengthening multisector linkages and driving policy and environmental improvements for healthy eating and active living in their local communities .\n\n【18】The Kansas City team’s participation in the collaborative resulted in the initiation of the Healthy Lifestyles Initiative in late 2013, led by members of Children’s Mercy Kansas City . Primary goals of the initiative were to promote 5 core implementation strategies to increase uptake and penetration of coordinated policy, systems, and environmental activities for childhood obesity prevention among community organizations and disseminate a consistent message on family healthy lifestyles. The Healthy Lifestyles Initiative conceptual model was mapped to Proctor and colleagues’ implementation science framework  .  \n\n【19】Conceptual model of the Healthy Lifestyles Initiative, Kansas City, Missouri, and Kansas City, Kansas, 2016. Model was mapped to Proctor and colleagues’ implementation science framework . Health outcomes were not assessed in this study. \n\n【20】Intervention Approach\n---------------------\n\n【21】The key elements of the intervention were selected by using the framework provided by _Collaborate for Healthy Weight_ , and the targeted health behaviors were informed through evidence review . The targeted outputs and activities of the initiative were for partners to implement policy, systems, and environmental activities (ie, implementation outcomes) in their organizations and in the communities in which they worked and incorporate the 12345 Fit-Tastic! message throughout their activities. The types of policy, systems, and environmental activities were broad, but activities generally were in these 8 categories: adopt new policy or change exiting policy, adopt new practices, create customized plans or goals with those served, develop or continue partnerships, initiate staff wellness activities, provide healthy lifestyles screenings or assessments, refer people served to primary care or other resources, and review organizational wellness policies.\n\n【22】A messaging campaign was developed to promote the targeted health behaviors . A public relations firm, working pro bono, presented multiple messaging options. Focus groups were conducted and stakeholders were polled to select the messages that resonated best. The message selected, 12345 Fit-Tastic!, promotes the following desired behaviors: 1 hour or more of physical activity, 2 hours maximum of screen time, 3 servings of low-fat or nonfat milk or yogurt, 4 servings of water (not sugary drinks), and 5 servings or more of fruits and vegetables per day. Extensive messaging materials and resources were developed and made available to partners (with their organization’s name and logo, if desired). All messaging materials and resources were made available on a website  to all partners.\n\n【23】The Healthy Lifestyles Initiative used 5 implementation strategies to support organizations in using the messaging materials and implementing policy, systems, and environmental activities: 1) educational training, 2) a structured action plan, 3) coalition support, 4) one-on-one support, and 5) materials dissemination and resource sharing. These 5 strategies were mapped to the implementation strategies developed by Powell et al  . Some strategies overlapped. For example, materials dissemination and resource sharing often took place during one-on-one support, and coalition support often involved educational training and one-on-one support.\n\n【24】Potential partners (ie, recipients of the intervention) in various sectors — the community (eg, nonprofit organizations, fitness centers, grocery stores), education (eg, schools, child care), health care (eg, primary care, hospitals), and government (eg, health departments, parks and recreation) — that had programs addressing childhood obesity were identified through surveys, a comprehensive database of nonprofit organizations in the region, and networking. Organizations were invited to become a partner in the initiative through electronic newsletters, word of mouth, direct contact with existing networks (eg, state health agency; Special Supplemental Nutrition Program for Women, Infants, and Children \\[WIC\\] agencies; local health departments), and presentations at local, state, and regional conferences. Additional tactics were used to engage partners (eg, local health departments, WIC agencies) in underserved areas to address health disparities. These tactics included promotion through the social networks of selected partners and involvement and support from community leaders, such as the mayor of Kansas City, Kansas. By January 2016, the Healthy Lifestyles Initiative supported 218 community partners from organizations across Kansas and Missouri, primarily in the Kansas City metropolitan area.\n\n【25】Evaluation Methods\n------------------\n\n【26】The evaluation was conducted by research partners at Children’s Mercy Kansas City in 2016. Although implementation science was not used to inform the development of the intervention, the research–practice partnership used implementation science in the evaluation. The implementation strategies being delivered and implementation activities targeted by the Healthy Lifestyles Initiative were determined by the evaluators, and the effectiveness of the implementation strategies on uptake and penetration of the implementation activities targeted were evaluated by examining data collected through a brief online survey.\n\n【27】Any person who had signed up as a partner on the Healthy Lifestyles Initiative website before January 31, 2016, was eligible to participate in the survey. Multiple partners from a single organization could sign up and most often represented a unique program, effort, or location in their organization. Of the 218 partners representing 170 organizations, 144 (%) had a moderate-to-high level of engagement with the intervention; assessment of engagement was based on whether the partner corresponded with intervention staff members after signing up on the website; 74 (%) partners had no contact after signing up. The brief online survey was emailed to the 218 partners in early 2016 through REDCap, a secure web application for building and managing online surveys and databases . Self-report was selected as the measurement mode because of its appropriateness for capturing data on policy, systems, and environmental changes, and because objective measures were not feasible given the heterogeneity in types of policy, systems, and environmental activities targeted by partner organizations. Power analysis indicated that a moderate association (r_ \\= 0.3; effect size = 0.6)  between the implementation strategies and activities and outcomes implemented would be detectible with approximately 84 respondents, and a small-to-moderate association (r_ \\= 0.2; effect size = 0.4) detectible with 193 respondents. Complete responses were obtained from 80 partners, so this study was powered to detect moderate associations. This study was approved by the Children’s Mercy Kansas City human subjects protection committee.\n\n【28】### Measures\n\n【29】The survey assessed partners’ organization type, the implementation strategies engaged in, materials used, and activities implemented.\n\n【30】**Organization type.** Participants could select 1 of 6 organization types: child care provider, faith-based organization, health care provider, health department, school, and other. Responses in the “other” classification were then categorized, according to an open-ended response, as academic institution, business, nonprofit community organization, other government organization, and parks and recreation department.\n\n【31】**Implementation strategies engaged in** . Survey respondents identified the implementation strategies in which they had engaged (yes or no) from a list of 5 options: attended at least 1 Healthy Lifestyles Initiative training session; completed a Healthy Lifestyles Initiative action plan; participated in a community coalition supported by the Healthy Lifestyles Initiative; received one-on-one support from a Healthy Lifestyles Initiative staff member; and received materials and resources from a Healthy Lifestyles Initiative staff member. We summed the number of strategies each partner engaged in (to 5) and calculated a mean for all survey respondents.\n\n【32】**Materials used** . Partners reported the number of types of materials they used from a list of 10 Healthy Lifestyles Initiative materials: 1) assessment forms, 2) behavior trackers, 3) educational handouts, 4) flags or banners, 5) logos or graphics, 6) message cards, 7) newsletters, 8) posters, 9) social media, and 10) website. Partners were asked how frequently they used the materials: never (score, 0); 1 or 2 occasions (score, 1); 3 or 4 occasions (score, 2); monthly (score, 3); weekly (score, 4); and daily (score, 5). Data on the extent of material use were captured by an open-ended question that asked participants how, where, and to whom they had distributed or shared materials and an open-ended question on the extent to which the materials had been incorporated into their organization’s regular practice. Responses to the 2 items were combined and rated for each respondent on a scale of 0 to 5, with 5 indicating the most extensive use of materials. Ratings were coded independently by one research staff member and reviewed by another research staff member for agreement, and disagreements were reconciled. A materials index was developed for the 3 variables (number of types of materials used, frequency of use, and extent of use) to indicate overall level of use. The index was computed by standardizing each variable to a range of 0 to 100 (multiplying the variables by 10, 20, and 20, respectively). A mean index was calculated for all survey respondents. Higher scores indicate greater use of materials.\n\n【33】**Policy, systems, and environmental activities implemented** . Partners were asked, “Which of the following things have you done to support children’s healthy lifestyles?” A list of 8 implementation activities targeted by the Healthy Lifestyles Initiative was provided: 1) adopted new policy or changed existing policy, 2) adopted new practices, 3) created customized plans or goals with people served, 4) developed or continued partnerships, 5) initiated staff wellness activities, 6) provided healthy lifestyles screenings or assessments, 7) referred those served to primary care or other resources, and 8) reviewed organizational wellness policies. Participants also responded to an open-ended question about the extent of the activities implemented: how often they were engaging in the activities, whether activities were being incorporated into regular practice, and how many people were reached. Responses were rated on a scale of 0 to 5, with 5 indicating the most extensive implementation. Ratings were coded independently by one research staff member and reviewed by another research staff member, and disagreements were reconciled. An implementation activities index was developed from the 2 variables (number of activities implemented and extent of implementation) to indicate overall level of implementation. The index was computed by standardizing each variable to a range of 0 to 100 (multiplying the variables by 12.5 and 20, respectively) and then calculating a mean of the 2 variables. Higher scores indicate greater levels of implementation.\n\n【34】**Additional support needed** . An open-ended question was used to assess the types of additional supports partners desired from the Healthy Lifestyles Initiative. Responses were coded by inductive thematic analyses  by one coder and confirmed by another, and no discrepancies were found.\n\n【35】### Analysis\n\n【36】Means, standard deviations, and frequencies were computed for all study variables. To investigate overlap among the strategies and activities, Spearman correlations were assessed among implementation strategies engaged in and activities implemented. To investigate relationships of implementation strategies engaged in with materials used and activities implemented, the materials index and the implementation activities index were regressed in separate single-variable linear regression models on 1) each of the 5 implementation strategies engaged in and 2) the total number of implementation strategies engaged in. To investigate whether greater use of materials was related to a greater level of implementation, the implementation activities index (dependent variable) was regressed on the materials index. All independent variables were entered in separate models because of concerns about collinearity. For the 5 binary (yes or no) implementation strategies, an effect size was calculated by dividing the regression coefficient by the standard deviation of the dependent variable . A similar approach was used to calculate effect size for continuous independent variables, except the regression coefficient was first multiplied by the standard deviation of the independent variable. _P_ < .05 was used to determine significance. All data were analyzed by using SPSS version 22.0 (IBM Corp).\n\n【37】Results\n-------\n\n【38】Of the 218 surveys sent, 12 were returned because of an invalid email address, 126 had no response, and 80 were completed (% response rate \\[80 of 206\\]). The 80 respondents represented 72 organizations and a mix of organization types; health departments, schools, health care providers, and nonprofit community organizations had the largest representation . A larger proportion of survey respondents (of 80; 81%) had a moderate-to-high level of engagement with the intervention than did partners (of 218; 66%).\n\n【39】Each Healthy Lifestyles Initiative implementation strategy was engaged in by 30.0% to 35.0% of survey respondents . The median number of implementation strategies engaged in was 2.0 (interquartile range \\[IQR\\], 0–3.0). Correlations among the implementation strategies ranged from 0.07 (completing an action plan and receiving materials and resources) and 0.32 (receiving one-on-one support and receiving materials and resources).\n\n【40】Respondents used a median 3.0 (IQR, 2.0–5.0) of the 10 types of 12345 Fit-Tastic! materials. Educational handouts and posters were the most frequently used materials, by 76.3% and 66.3% of respondents, respectively. On average, the materials were used monthly. The mean materials index was 49.3 (standard deviation \\[SD\\], 21.7). More extensive material use included providing regular presentations on the Fit-Tastic! message, creating recipes and food samples or activity plans that aligned with the message, using components of the Fit-Tastic! message to promote weight-management goals for clients, and hanging Fit-Tastic! materials in an area likely to affect behavior (eg, cafeteria, grocery store produce section).\n\n【41】Respondents implemented a median of 2.5 (IQR, 1.0–4.0) of the 8 activities targeted by the Healthy Lifestyles Initiative. Developing or continuing community partnerships (.5% of respondents) and reviewing organizational wellness policies (.3% of respondents) were the most commonly reported implementation activities. The median implementation activities index was 38.8 (IQR, 22.5–61.2). Correlations among the implementation activities ranged from −0.10 (reviewed organizational wellness policies and created customized plans or goals with people served) and 0.58 (adopted new policy or changed existing policy and adopted new practices). Implementation activities that were more extensively used than others included incorporating physical activities and healthy food options (eg, walk breaks, adding healthy menu items, adding dance classes), improving nutritional quality of children’s food options at school, educating parents on healthy meal preparation, and adding healthy lifestyles screenings to primary care visits.\n\n【42】Two implementation strategies, attending training sessions (B = 10.41; _P_ \\= .04; effect size = 0.48) and completing an action plan (B = 12.85; _P_ \\= .01; effect size = 0.59), were associated with the materials index . The total number of implementation strategies engaged in was also associated positively with materials used, with each additional implementation strategy related to a 5.65 higher score on the materials index (effect size = 0.52).\n\n【43】One implementation strategy, completing an action plan, was associated with the implementation activities index (B = 22.70; _P_ \\= .001; effect size = 0.82). Receiving coalition support was associated with the implementation activities index (B = 10.98; _P_ \\= .09; effect size = 0.40), although not significantly. The total number of implementation strategies engaged in was also associated positively with the implementation activities index, with each additional implementation strategy related to a 6.93 higher score on the implementation activities index (effect size = 0.51). The materials index was associated positively with the implementation activities index (effect size = 0.35).\n\n【44】Themes emerged on additional supports desired. Of the 45 partners who responded to this question, 19 (%) desired more support for implementation, such as greater assistance with implementation planning and affecting policy change, and guidance in how to use and adapt the materials into their program or organization. Ten (%) respondents desired different or additional materials, 8 (%) desired additional training, and 8 (%) desired increased communication with a designated contact person.\n\n【45】Implications for Public Health\n------------------------------\n\n【46】This study indicates that community-capacity–building efforts can increase uptake and penetration of policy, systems, and environmental approaches by community organizations for improving healthy eating and active living among children and families. In particular, partners who engaged in more Healthy Lifestyles Initiative implementation strategies engaged in more extensive implementation of policy, systems, and environmental activities. Completing an action plan was the most important implementation strategy. These findings suggest that community-capacity–building efforts, such as those led by the Centers for Disease Control and Prevention and local hospitals and health departments across the United States, have promise for improving policies, systems, and environments to prevent childhood obesity . Although evaluation of health and behavioral outcomes among children and families in these types of large-scale community-wide capacity efforts are often not feasible, implementation science frameworks  provide a useful model for gauging effects on implementation outcomes (ie, the intermediary factors that affect whether the efforts will ultimately improve health).\n\n【47】Partners who attended training sessions, completed an action plan, and engaged in more Healthy Lifestyles Initiative implementation strategies were more likely to use 12345 Fit-Tastic! materials. However, receiving materials or resources from the initiative team and receiving coalition support were not associated with use of these materials. The training sessions and action plan involved a higher level of engagement than other implementation strategies, suggesting that this higher level of engagement may be needed to support organizations to use healthy messaging materials. Many respondents reported the desire for more guidance on how to adapt the materials into their organization or program, and training sessions and action plans allow program staff members to provide such support. The action plan was often tailored to an organization; this type of tailoring may be needed, particularly by organizations that have less experience in supporting healthy eating and active living.\n\n【48】Completing an action plan was the single most important implementation strategy. About 30% of respondents completed an action plan, which is similar to the proportion completing each other strategy but is still a minority of participants. The action plan is a 1-page document that guides the sharing of a consistent message, implementation of a consistent assessment of weight status and healthy lifestyle behaviors and healthy lifestyles plan, and creation of policy or environmental changes to support healthy eating and active living. Organizations complete each section of the action plan with guidance from Healthy Lifestyles Initiative staff members, which may provide some accountability. Next steps in this research area should be to identify strategies for increasing the number of organizations that complete an action plan, which could be informed by investigating differences in organizational characteristics and implementation contextual factors  between completers and noncompleters. Organizations that used the healthy messaging materials were also more likely to engage in implementation activities, but the effect was smaller for using materials than for completing an action plan. Thus, disseminating messaging materials may increase organizations’ engagement in childhood obesity prevention, but messaging materials alone are not likely to be strong drivers of organization or behavior change and should be paired with policy, systems, and environmental supports such as training and action planning.\n\n【49】Each of the 5 implementation strategies was engaged in at a similar rate — by 30% to 35% of organizations. Thus, no single strategy seemed more likely to reach community partners than another strategy, but implementation rates increased for every additional implementation strategy engaged in. Two materials were most often used: the educational handouts and posters. A benefit of adopting healthy messaging materials from coordinated efforts such as the Healthy Lifestyles Initiative is that the messaging is consistent across materials and across organizations that adopt the materials. Consistency in messaging can maximize comprehension, minimize confusion, and increase awareness of selected health behaviors and policy, systems, and environmental changes needed to support the behaviors . More than half of survey respondents reported developing or continuing partnerships related to childhood obesity prevention, which appeared to be the most consistent benefit of the Healthy Lifestyles Initiative. Many organizations reported adopting new policies or practices and implementing multiple activities, which were primary targets of the initiative.\n\n【50】About one-third of partners had no correspondence with intervention staff members after signing up online. Initiatives such as the Healthy Lifestyles Initiative will have the greatest impact if they are able to reach a large portion of the targeted population of partners; maintaining partner engagement is critical. However, because available resources may limit the ability to engage all partners at ideal levels, engagement efforts may need to prioritize re-engagement of unengaged partners over supporting engaged partners or cultivating new partners. To this end, the Healthy Lifestyles Initiative used a train-the-trainer model to support highly engaged coalitions with a high reach into the target population of partners to engage and re-engage partners that may have otherwise had low levels of engagement. The initiative also prioritized resources toward continued engagement of organizations perceived to have a high reach into the target population and high potential for continued involvement and organizations representing various community sectors to maximize representation of all organizations engaged in obesity prevention. Future research should focus on improving understanding of why some organizations have low levels of engagement in such initiatives and strategies for increasing engagement. Our anecdotal evidence suggests staff turnover at partner organizations and competing priorities (eg, other grant projects) may have contributed to low levels of engagement.\n\n【51】This study is among the first to investigate how community-capacity–building efforts can increase implementation of policy, systems, and environmental approaches and use of consistent messaging among community organizations. Limitations include the lack of objective data, potential response and selection bias, and inability to determine causality. Organizations that were more involved in the initiative may have been more likely to respond to the survey, and in some cases multiple respondents represented a single organization, so findings may not generalize to nonresponders. Although it appeared that some implementation strategies, such as completing an action plan, led organizations to implement more activities, organizations that implemented more activities may have been more likely to participate in the initiative and complete the action plan (ie, reverse causality). Future studies could use objective measures, such as policy reviews, environmental audits, and body composition measures and leverage pre–post assessments, control groups, or interrupted time series designs. Assessing organizational characteristics (eg, availability of resources) and interviewing key informants could help uncover why some organizations engaged in the initiative more than others. Finally, practitioners could leverage implementation science to systematically select and tailor implementation strategies; many other effective (and potentially more effective) strategies may exist for improving community capacity for implementing policy, systems, and environmental practices .\n\n【52】Policy, systems, and environmental approaches are essential for advancing the culture of healthy eating, active living, and childhood obesity prevention . Numerous initiatives exist to build community capacity in these areas. However, more research is needed to evaluate and maximize the effectiveness of these initiatives. This study used an implementation science framework to investigate whether and which implementation strategies delivered by the Healthy Lifestyles Initiative were related to uptake and penetration of evidence-based practices. Findings suggest that community-capacity–building efforts can support community organizations in implementing policy, systems, and environmental activities but that multiple implementation strategies are likely needed, particularly activities that require a relatively high level of engagement, such as educational training and creating action plans.\n\n【53】Tables\n------\n\n【54】 Table 1. Description of Implementation Strategies Provided by the Healthy Lifestyles Initiative, Kansas City, Missouri, and Kansas City, Kansas,\n\n| Implementation Strategy | Description | Delivery | Related Constructs From Powell et al  |\n| --- | --- | --- | --- |\n| Educational training | Training sessions included data on childhood obesity, background on the Healthy Lifestyles Initiative, and information about action planning strategies. Trainees also learned how to join the Healthy Lifestyles Initiative and how to gain access to resources, such as 12345 Fit-Tastic! messaging and educational materials. | Some training sessions were delivered to coalitions or collaborative groups. Other training sessions were provided to an organization’s staff members or other individuals. Most were provided upon request of the Healthy Lifestyles Initiative partner or through a grant or other project. Training sessions were also available on a quarterly basis after an open and ongoing regional stakeholder meeting. | Conduct educational meetings |\n| Structured action plan | The action plan elements (“MAPPS for Change”) were based on elements presented to the National Institute for Children’s Health Quality’s initiative, Collaborate for a Healthy Weight: **M** —consistent use of **message** (Fit-Tastic!); **A** — consistent **assessment** of weight status and lifestyle behaviors; **P** — customized healthy lifestyle **plan** for all; **P —** **policies and practices** that enable healthy eating and active living; and **S** — **statistics** or story telling about message reach, assessment/plans completed, and policy/environmental changes made. | All partners were provided information about the MAPPS for Change action plan when they signed up to join the Healthy Lifestyles Initiative. Training sessions and one-on-one support also included information about MAPPS for Change action planning. Partners could complete the MAPPS for Change action plan document through their partner account created on the initiative’s website. Paper copies of the MAPPS for Change action plan were provided at training sessions. All partners received 1) a reminder to complete their action plan when they logged into their partner account and 2) an annual email reminder to complete or update their plans. | Develop a formal implementation blueprint and obtain formal commitments |\n| Coalition support | Supporting existing coalitions focused on healthy eating and/or active living was seen as a way to improve reach and impact of the intervention. This strategy was similar to a train-the-trainer approach, with a large amount of support being provided to the coalition leaders to infuse the Healthy Lifestyles Initiative framework throughout the coalition’s efforts. | Community coalition support included training coalition members (ie, staff from multiple organizations) and one-on-one contact with coalition leaders, primarily provided to 3 large coalitions on an ongoing basis. This support allowed for the coalition and its partners to have access to additional support and resources from the Healthy Lifestyles Initiative. | Promote network weaving and use train-the-trainer strategies. |\n| One-on-one support | One-on-one support consisted of meetings or telephone or email support with Healthy Lifestyles Initiative partners beyond initial contact and/or outside of training. Most one-on-one support focused on generating or reviewing the MAPPS for Change action plan elements and providing assistance or guidance on these plans specific to the partner organization. One-on-one support also included linking together partners with similar agendas. | One-on-one support was offered to all partners when they joined the Healthy Lifestyles Initiative and was provided on request (in response to the initial contact or at a later date). This support sometimes occurred before an organization officially joining as a partner but more often occurred with established members to assist with the MAPPS for Change action plan, ideas for implementing action plan strategies, and partnerships. | Facilitation, provide local technical assistance, and provide ongoing consultation. |\n| Materials dissemination and resource sharing | Materials dissemination and resource sharing most commonly included connecting Healthy Lifestyles Initiative partners with 12345 Fit-Tastic! educational and marketing materials. Also included were connection to other local and national resources (eg, resource guides, toolkits) and provision of data on childhood obesity or other information. | The 12345 Fit-Tastic! educational and marketing materials were available to all partners via the initiative’s website. Other information and resources were shared as requested by partners. At least quarterly, Healthy Lifestyles Initiative partners would receive email newsletters highlighting resources, partner stories, or new 12345 Fit-Tastic! materials. | Develop and distribute educational materials. |\n\n【56】 Table 2. Types of Organizations That Participated in the Healthy Lifestyles Initiative and Evaluation Study, Kansas City, Missouri, and Kansas City, Kansas, 2016 a  \n\n| Type | No. (%) of Organizations (n = 218) | Number (%) of Survey Respondents (n = 80) |\n| --- | --- | --- |\n| Academic institution | 3 (.4) | 2 (.5) |\n| Business | 14 (.4) | 3 (.8) |\n| Child care provider | 25 (.5) | 9 (.3) |\n| Nonprofit community organization | 38 (.4) | 11 (.8) |\n| Faith-based organization | 8 (.7) | 3 (.8) |\n| Health care provider | 25 (.5) | 14 (.5) |\n| Health department | 54 (.8) | 21 (.3) |\n| Other government organization | 1 (.5) | 1 (.3) |\n| Parks and recreation department | 8 (.7) | 1 (.3) |\n| School | 42 (.3) | 15 (.8) |\n\n【58】a  A brief online survey was emailed to 218 partners (defined as a person who signed up as a partner on the Healthy Lifestyles Initiative website before January 31, 2016) representing 170 organizations. More than 1 partner could represent a single organization. All data were self-reported.\n\n【59】 Table 3. Descriptive Statistics for Healthy Lifestyles Initiative Implementation Strategies Engaged In, 12345 Fit-Tastic! Materials Used, and Activities Implemented by Participating Organizations in Kansas City, Missouri, and Kansas City, Kansas, 2016 a  \n\n| Category | No. of Respondents (%) b (n = 80) |\n| --- | --- |\n| **Implementation strategies engaged in** | **Implementation strategies engaged in** |\n| Attended ≥1 training session | 28 (.0) |\n| Completed an action plan | 24 (.0) |\n| Participated in a community coalition | 28 (.0) |\n| Received one-on-one support | 26 (.5) |\n| Received materials and resources | 26 (.5) |\n| No. of implementation strategies engaged in, median (IQR) c | 2.0 (.0) |\n| **Materials used** | **Materials used** |\n| Assessment forms | 25 (.3) |\n| Behavior trackers | 5 (.3) |\n| Educational handouts | 61 (.3) |\n| Flags or banners | 20 (.0) |\n| Logos or graphics | 27 (.8) |\n| Message cards | 23 (.7) |\n| Newsletters | 13 (.3) |\n| Posters | 53 (.3) |\n| Social media | 16 (.0) |\n| Website | 46 (.5) |\n| Number of different materials used d , median (IQR) | 3.0 (.0–5.0) |\n| Frequency of materials used e , mean (SD) | 3.3 (.4) |\n| Extent of materials use f , mean (SD) | 2.5 (.4) |\n| Materials index g , mean (SD) | 49.3 (.7) |\n| **Activities implemented** | **Activities implemented** |\n| Adopted new policy or changed existing policy | 18 (.5) |\n| Adopted new practices | 23 (.7) |\n| Created customized plans or goals with people served | 21 (.3) |\n| Developed or continued partnerships | 46 (.5) |\n| Initiated staff wellness activities | 29 (.3) |\n| Provided healthy lifestyles screenings or assessments | 27 (.8) |\n| Referred those served to primary care or other resources | 26 (.5) |\n| Reviewed organizational wellness policies | 37 (.3) |\n| No. of different activities implemented h , mean (SD) | 2.8 (.2) |\n| Extent of activities implemented i , median (IQR) | 2.5 (.0–4.0) |\n| Implementation activities index j , median (IQR) | 38.8 (.5–61.2) |\n\n【61】Abbreviations: IQR, interquartile range; SD, standard deviation.  \na  A brief online survey was emailed to 218 partners (defined as a person who signed up as a partner on the Healthy Lifestyles Initiative website before January 31, 2016) representing 170 organizations. More than 1 partner could represent a single organization. All data were self-reported.  \nb  Unless otherwise indicated.  \nc  Survey respondents identified the implementation strategies in which they had engaged from a list of 5 options. The number of strategies engaged in was summed for each respondent (range, 0–5).  \nd  Survey respondents reported the number of types of materials they used from a list of 10 Healthy Lifestyles Initiative materials. The number of strategies engaged in (range, 0–10) was summed for each respondent.  \ne  Survey respondents were asked how frequently they used the materials on a scale of 0 (never) to 5 (daily).  \nf  Responses to 2 open-ended questions were rated on a scale of 0 to 5, with 5 indicating the most extensive use of materials.  \ng  A materials index, scaled from 0 to 100, with 100 indicating greatest overall use, was developed from 3 variables (number of types of materials used, frequency of use, and extent of use) to measure overall use of materials.  \nh  A list of 8 implementation activities targeted by the Healthy Lifestyles Initiative was provided. The number of strategies engaged in was summed for each respondent (range, 0–­8).  \ni  Responses to 1 open-ended question were rated on a scale of 0 to 5, with 5 indicating the most extensive implementation.  \nj  An implementation activities index, scaled from 0 to 100, with 100 indicating greatest level of implementation, was developed from 2 variables (number of activities implemented and extent of implementation) to indicate overall level of implementation.\n\n【62】 Table 4. Associations Among Healthy Lifestyles Initiative Implementation Strategies Engaged In, 12345 Fit-Tastic! Materials Used, and Activities Implemented in Kansas City, Missouri, and Kansas City, Kansas, 2016 a  \n\n| Variable | Materials Index b | Implementation Activities Index c |\n| --- | --- | --- |\n| B (Standard Error) | _P_ Value d | B (Standard Error) | _P_ Value d |\n| --- | --- | --- | --- |\n| **Implementation strategies engaged in e** | **Implementation strategies engaged in e** | **Implementation strategies engaged in e** | **Implementation strategies engaged in e** | **Implementation strategies engaged in e** |\n| Attended ≥1 training session | 10.41 (.98) | .04 | 0.16 (.12) | .98 |\n| Completed action plan | 12.85 (.11) | .01 | 22.70 (.29) | .001 |\n| Received coalition support | 1.21 (.14) | .81 | 10.98 (.32) | .09 |\n| Received one-on-one support | 1.55 (.29) | .77 | −4.71 (.51) | .47 |\n| Received materials or resources | 4.31 (.22) | .41 | 8.41 (.42) | .19 |\n| Total implementation strategies engaged in f | 5.65 (.61) | .001 | 6.93 (.06) | .001 |\n| **Materials used** | **Materials used** | **Materials used** | **Materials used** | **Materials used** |\n| Materials index b | — | — | 0.45 (.14) | .001 |\n\n【64】Abbreviation: B, unstandardized regression coefficient.  \na  A brief online survey was emailed to 218 partners (defined as a person who signed up as a partner on the Healthy Lifestyles Initiative website before January 31, 2016) representing 170 organizations. More than 1 partner could represent a single organization. All data were self-reported.  \nb  A materials index, scaled from 0 to 100, with 100 indicating greatest overall use, was developed from 3 variables (number of types of materials used, frequency of use, and extent of use) to measure overall use of materials.  \nc  An implementation activities index, scaled from 0 to 100, with 100 indicating greatest level of implementation, was developed from 2 variables (number of activities implemented and extent of implementation) to indicate overall level of implementation. A list of 8 implementation activities targeted by the Healthy Lifestyles Initiative was provided.  \nd  Estimated by using linear regression.  \ne  Survey respondents identified the implementation strategies in which they had engaged from a list of 5 options.  \nf  The number of strategies engaged in was summed for each respondent (range, 0–5).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2d459a1a-3b3a-4147-9a8c-1d05d9a9b716", "title": "Transnational Issues in Quarantine", "text": "【0】Transnational Issues in Quarantine\nHow to promote transnational collaboration in implementing quarantine was the topic of a January 19–23, 2004, conference sponsored by the Defense Threat Reduction Agency. Fifty invited participants discussed the status of quarantine planning in their 13 countries (the Americas, Israel, and several members of the European Union nations \\[EU\\]). Held in the Wilton Park Conference Centre, Sussex, United Kingdom, the conference, “Quarantine following an International Biological Weapons Attack: Building Cooperation, Achieving Consistency,” also addressed quarantine in response to emerging infectious diseases.\n\n【1】Participants first examined the legal foundation for quarantine in their countries. Federal Canadian quarantine law applies only to national ports of entry or exit; provincial laws govern quarantine in the provinces. The U.S. Centers for Disease Control and Prevention has quarantine responsibilities at national ports of entry or departure; this agency may also become involved when a disease is spreading across state borders or even within a state (when invited by the governor of the state or ordered by the U.S. Secretary of Health and Human Services or the U.S. President). In general, however, quarantine in the United States is a local or state government issue. Quarantine laws in these jurisdictions vary, and some public health authorities expressed reluctance to address their shortcomings through legislation for fear that skeptics of quarantine would further weaken the laws. Other nations would turn to the World Health Organization and its International Health Regulations of 1969 (IHR) for guidance. A revised IHR should be available by 2005, but currently it lists only three diseases—plague, cholera, and yellow fever—as subject to quarantine and offers scant help in planning quarantine. Thus, the legal framework for quarantine varies and contributes little to the construction of a consistent approach to quarantine among nations.\n\n【2】European public health officials have forged some bilateral cooperative agreements and are discussing establishing a regional disease control center for EU nations. They are not, however, developing and testing either national or transnational plans for possible large-scale quarantine. Some participants thought that consistency in developing and implementing quarantine measures was not necessarily desirable, given that each nation must deal with threats in accordance with its own culture, laws, and traditions. Others thought that inconsistencies in response to the same disease threat might encourage persons to question the need for quarantine measures and choose not to comply. The United States also has not developed comprehensive quarantine plans, trained staff, or conducted quarantine exercises in local communities, despite recently issued federal quarantine guidelines. Especially lacking are processes and procedures to clarify decision-making and coordination in communities with multiple jurisdictions.\n\n【3】The heightened concern of the United States about bioterrorism was not shared by others at the conference, although all agreed that persons would likely demand a federal response to a health crisis caused by terrorists, including any required quarantine. Other issues discussed included assurances of compensation for income lost while in quarantine (strongly recommended as a component of any quarantine plan) and psychosocial support to reduce the sense of isolation experienced by many persons while in quarantine. Officials with information management experience during health- and nonhealth-related crises commented on the need for caution in making public statements when faced with a new and evolving threat.\n\n【4】The conference permitted participants to establish working relationships with one another, but it also highlighted gaps that exist in comprehensive transnational quarantine planning.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "23b17544-fd9a-4a2a-b44a-9e1d09a07de6", "title": "Molecular Surveillance for Imported Antimicrobial Resistant Plasmodium falciparum, Ontario, Canada", "text": "【0】Molecular Surveillance for Imported Antimicrobial Resistant Plasmodium falciparum, Ontario, Canada\nMalaria remains the deadliest vectorborne infectious disease worldwide . _Plasmodium_ spp. most commonly _P. falciparum_ , are responsible for ≈229 million cases and ≈500,000 deaths from malaria annually . Although malaria incidence and death have decreased over the past decade, emerging antimalarial drug resistance, fueled by counterfeiting, overuse, and underdosing, threatens control and elimination efforts . _P. falciparum_ resistance mutations have resulted in waning efficacy in multiple antimalarial classes including artemisinins, quinolines, and antifolates . Increased international travel and climate change are exacerbating the spread of malarial vectors , making drug-resistant _P. falciparum_ malaria imported from endemic regions, in particular Africa and Southeast Asia, a growing concern . Surveillance of antimalarial drug resistance, particularly among _P. falciparum_ specimens, is crucial both to inform universal treatment guidelines and to identify global patterns of emerging resistance. We aimed to identify the prevalence of several resistance markers, including genes that confer resistance to chloroquine, mefloquine, atovaquone/proguanil, and artemisinins, and to quantify the copy number of multidrug resistance genes (pfmdr1_ ) in _P. falciparum_ isolates from malaria cases imported to Ontario, Canada, over a 10-year period.\n\n【1】### Methods\n\n【2】##### Specimens\n\n【3】From the malaria biobank at the Public Health Ontario Laboratory in Toronto, Ontario, Canada, we retrieved unique surplus whole-blood clinical specimens containing _P. falciparum_ from the years 2008–2009, 2013–2014, and 2017–2018. We confirmed the _P. falciparum_ specimens from the biobank as monoinfections using multiplex real-time quantitative PCR (qPCR)  after clinical testing, which included Giemsa-stained thick and thin blood film examination by certified medical lab technologists and rapid diagnostic test using the Abbott BinaxNOW malaria kit . To eliminate the possibility of recrudescent infections, we excluded specimens from patients who had positive blood smears in the year before our enrollment period.\n\n【4】##### Extracting DNA from Whole Blood and Detecting Parasite DNA by Real-Time qPCR\n\n【5】We extracted DNA from surplus blood (.4 mL) using the KingFisher Pure DNA Blood Kit  and stored it in 50-μL aliquots: one at −20°C for working stock and another at −80°C for storage stock. We used included BEI Resources W2 MRA-157  and ATCC 3D7  samples as _P. falciparum_ –positive controls. We tested samples using 2 species-specific duplex real-time qPCRs (P. falciparum_ and _P. vivax_ ; _P. malariae_ and _P. ovale_ ) . Samples were 25 μL total volume, including 12.5 μL Taqman Universal MasterMix (Thermo Fisher) and 5 μL template DNA. To perform all reactions, we used the Applied Biosystems 7500 Fast Real-Time PCR system (Thermo Fisher) for 2 min at 50°C and 10 min at 95°C, followed by 45 cycles of 15 s at 95°C and 1 min at 60°C.\n\n【6】##### Pyrosequencing\n\n【7】We analyzed 23 different single-nucleotide polymorphisms (SNPs) across 6 genes: _atpase6_ (PfATPase6; SNPs A623E, S769N), _cytb_ (cytochrome b; SNP Y268NSC), _dhfr_ (dihydrofolate reductase; SNPs A16V, C50R, N51I, C59R, S108N, I164L), _dhps_ (dihydropteroate synthetase; SNPs S436FA, A437G, K540E, A581G, A613TS), _mdr1_ (multidrug resistance protein; SNPs N86Y, Y184F, S1034CTR, N1042D, D1246Y), and _pfcrt_ (chloroquine resistance transporter; SNPs K76T, N75E, M74I, C72S). We performed 16 PCR reactions according to the Pyromark PCR Kit protocol  using 400 nmol/L–concentration primers published elsewhere . We performed all reactions on ABI Veriti Thermal Cyclers (Thermo Fisher) with adjusted annealing temperatures of 44°C for _dhfr_ 164 and 63.8°C for _pfcrt_ . We ran 3 μL of PCR product on a 1% agarose gel at 100 V for 30 min to ensure band presence before pyrosequencing.\n\n【8】We performed pyrosequencing using the Pyromark Q24 Vacuum Workstation and Pyromark Q24 Pyrosequencer (QIAGEN) according to manufacturer protocols. We designed 21 pyrosequencing assays using PSQ assay design software version 1.0.6  . To determine run-to-run consistency and reproducibility, we calculated the intra-assay coefficient of variation by running the _cytb_ assay on 3 samples in quintuplicate. We analyzed samples using Pyromark Q24 version 1.0.10 software (QIAGEN).\n\n【9】##### Sanger Sequencing\n\n【10】We interrogated _kelch13_ (kelch protein gene on chromosome 13) for 20 SNPs according to Institut Pasteur protocols using primers published elsewhere  . To validate the results obtained from pyrosequencing, we performed a quality control check of the gene targets using the first 10% of cases from each time period and controls. We designed sequencing primers with Primer 3  and performed PCR using primers published elsewhere  that target drug resistance genes according to the Accuprime Pfx SuperMix protocol (Thermo Fisher) ; we adjusted the annealing temperature to 60°C and cycles to 40 for _cytb_ and 45 for all _mdr1_ and _dhfr_ assays. We ran 3 μL of PCR product on a 1% agarose gel at 100 V for 30 min to ensure band presence before sequencing.\n\n【11】We performed sequencing reactions using the Big Dye Terminator v3.1 Cycle Sequencing Kit (Thermo Fisher) with a volume of 20 μL, including 1 μL of PCR product, 2 μL of Big Dye ready reaction mix, 3 μL of Thermo Fisher sequencing buffer, and 2 μL of 10 μmol/L of primer. We used a Bio-Rad C1000 Thermal Cycler  for 1 min at 96°C, 25 cycles of 10 s at 96°C, 5 s at 50°C, and 4 min at 60°C. We purified product using an in-house isopropanol protocol including incubating at room temperature using 15 μL of 100% isopropanol for 15 min, centrifuging at 3,000 × _g_ for 30 min at room temperature, centrifuging at 2,000 × _g_ for 10 min after 35 μL of 70% isopropanol had been added, then inverting the plate and centrifuging for 700 × _g_ at room temperature for 1 min. Last, we added 20 μL of Hi-Di Formamide (Thermo Fisher) to reconstitute each sample and sequenced using an Applied Biosystems 3730xl DNA Analyzer (Thermo Fisher). We standardized data using a Thermo Fisher sequencing analysis program and used a BioEdit sequence alignment editor version 7.2.5  to analyze the sequence for nucleotide changes at each identified SNP location.\n\n【12】##### _Pfmdr1_ Copy Number Analysis\n\n【13】We tested specimens for _pfmdr1_ copy number using real-time qPCR with primers and probes published elsewhere , using a total volume of 25 μL, including 12.5 μL Taqman Universal MasterMix (Thermo Fisher) and 5 μL template DNA. We performed reactions on an Applied Biosystems 7500 Fast Real-Time PCR system (Thermo Fisher) using the same conditions used to detect parasite DNA. We ran the process in triplicate for specimens and used the 2 -ΔΔCT  method to calculate relative expression .\n\n【14】##### Statistical Analyses\n\n【15】We considered a genotype mutant dominant if the mutant allelic frequency within the specimen was \\> 50% and wildtype dominant if <50%. We calculated median and range _pfmdr1_ relative copy numbers for each time period and compared all categorical variables (proportions of mutant versus wildtype genotypes) using the Fisher exact test. We calculated median and range for mutant allelic frequencies within specimens and presented them descriptively across time periods to inform surveillance for resistance mutations over time. We compared differences in median _pfmdr1_ relative copy numbers across time periods using the Kruskal-Wallis H test and set significance at p <0.05. We analyzed data using Stata version 13 software .\n\n【16】### Results\n\n【17】Over the enrollment period, we retrieved a total of 574 primary blood specimens from the malaria biobank containing \\> 1 _Plasmodium_ spp. determined using real-time qPCR; 566 (.6%) specimens contained just 1 species of _Plasmodium_ and the other 8 (.4%) contained 2 _Plasmodium_ species. Of specimens with single-species infections, 365/574 (%) were positive only for _P. falciparum_ , 142/574 (%) only for _P. vivax_ , 46/574 (%) only for _P. ovale_ , and 13/574 (.3%) only for _P. malariae_ . Of the specimens with 2 _Plasmodium_ species, 3/574 (.5%) were _P. falciparum–P. ovale_ co-infections and 5/574 (.9%) were _P. ovale–P. vivax_ co-infections. Only 243/365 (%) specimens with _P. falciparum_ monoinfection were identified as unique cases, 75 (%) occurring during July 2008–June 2009, 79 (%) during July 2013–June 2014, and 89 (%) during July 2017–June 2018. Of those 243 _P. falciparum_ –only specimens from unique cases, 169 (%) were from male patients, 66 (%) from female patients, and 8 (%) had no reported sex; there was no difference in proportions of sex across time periods (p = 0.47) . Mean age was 39.2 (range 3–88) years, which did not differ between time periods (p = 0.61) . Mean parasitemia was 0.3% (<0.01%–24.0%) across all 3 time periods, and we saw no significant differences over time (p = 0.10) . A total of 186 (%) patients had documented travel history, 81 (%) in West Africa, the most common region, and 40 (%) in Nigeria, the most common country . Only 5/243 (%) unique _P. falciparum_ specimens were from travelers to Southeast Asia, and 1 (.4%) was from a traveler to the Caribbean .\n\n【18】##### Prevalence of Mutations in Mutant Dominant Genotypes\n\n【19】All 243 unique _P. falciparum­–_ only specimens contained \\> 1 resistance mutation. We detected the highest prevalence of mutant genotypes in _dhfr_ ; 212 (%) cases demonstrated triple codon mutation N51I, C59R, and S108N . _dhfr_ SNP analysis revealed an increase in S108N mutant genotypes from 89% in 2008–2009 to 100% in 2017–2018 . _pfcrt_ SNP analysis revealed a gradual decline in K76T mutant genotype from 57% in 2008–2009 to 33% in 2017–2018 (p = 0.011), coupled with a decrease in M74I and N75E from 52% in 2008–2009 to 27% in 2017–2018 (p = 0.010). _dhps_ A613T SNP analysis revealed an overall increase in mutant genotypes, from 12% in 2008–2009 to 28% in 2017–2018 (p = 0.024). _mdr1_ N86Y and D1246Y analysis revealed a decline in mutant genotypes from 2008–2009 and 2017–2018 (p<0.003). We observed no difference in mutant genotype populations across time periods for _dhps_ K540E, A581G, A437G, S436A, S436F _mdr1_ Y184F, S1034T, or _atpase6_ A623E . Last, we observed no mutant genotype populations for _cytb_ Y268NSC across time periods .\n\n【20】##### Mutant Allelic Frequency within Wildtype Dominant Specimens\n\n【21】We presented summaries of mutant allelic frequencies within wild-type–dominant specimens (i.e. with <50% within-specimen mutation prevalence) for _dhfr_ , _pfcrt_ , _dhps_ , _mdr1_ and _mdr1_ copy number , _atpase6_ , and _cytb_ . Many minor changes in allelic frequencies of questionable importance occurred across time periods . Notable findings include that _pfcrt_ K76T and C72S mutant allele populations among wild-type infections declined significantly from 2008–2009 to 2017–2018 . Conversely, mutant allele frequency within _mdr1_ D1246Y increased from 7% in 2008–2009 to 16% in 2017–2018, despite a decline in overall proportion of mutant-dominant genotypes (i.e. with \\> 50% within-specimen mutation frequency) from 17.6% to 3.5% across time periods . Frequency of any allelic mutant among wildtype infections at _cytb_ Y268N increased from 0% in 2008–09 and 2013–2014 to 3% in 2017–2018 .\n\n【22】##### _mdr1_ Copy Number and _kelch13_ Mutations\n\n【23】Using the 3D7 comparator reference strain, which has a _pfmdr1_ copy number of 1, we found an increase in median _pfmdr1_ copy number from 1.1 (range 0.83–1.4) in 2008–2009 to 1.9 (range 0.73–5.4) in 2017–2018 (p<0.001) . _kelch13_ SNP analysis of >20 codons revealed no mutations for all analyzed specimens, despite 5 specimens with confirmed _P. falciparum_ being from Southeast Asia (data not shown). We observed 2 silent mutations at codon 553 and 561 in the 2017–2018 specimens (sequences deposited to GenBank under accession nos. OM489472 and OM489473); however, travel history was not recorded for those patients.\n\n【24】### Discussion\n\n【25】We analyzed 243 unique cases of _P. falciparum_ imported to Canada using pyrosequencing assays based on previously reported genetic markers known to confer drug resistance. Our results have provided stakeholders with a resource for tracking antimalarial resistance over time. Conventional SNP analysis involves Sanger sequencing and PCR–restriction fragment-length polymorphism, but over the past decade pyrosequencing has added utility for detecting drug-resistance markers . Now, pyrosequencing has been partially replaced by more powerful techniques, such as targeted next-generation sequencing, which remain accessible mostly just in reference or research laboratories because of bioinformatics requirements. Our analysis of _P. falciparum_ markers of genotypic resistance using pyrosequencing provides evidence of a quantitative advantage over Sanger sequencing because allelic frequencies within wild-type populations can be monitored over time to capture emerging molecular resistance. We observed significant decreases in mutant genotypes for chloroquine resistance genes and an increase over time in the proportion of mutant genotypes for _dhfr_ , the gene conferring resistance to proguanil. We also observed significant increases in markers of antifolate drug resistance correlating with increases in mutant allelic frequencies within wild-type populations.\n\n【26】The emergence of antimicrobial-resistant _P. falciparum_ began in 1957 when the first cases of resistance to chloroquine were observed along the Cambodia–Thailand border . _pfcrt_ encodes for a transmembrane protein in the digestive vacuoles of _P. falciparum_ parasites. Mutations of the _pfcrt_ gene, specifically at positions 72, 74, 75, and 76, confer resistance to chloroquine . _pfmdr1_ mutation N86Y combined with increased _pfmdr1_ copy numbers, both of which have been documented in many parts of Southeast Asia and recently in sub-Saharan Africa, is also implicated in chloroquine resistance . Our data document a decline over time in _pfcrt_ mutants from positions 72 and 76 in imported cases of _P. falciparum_ , indicating molecular reversion in chloroquine resistance at these codons. This phenomenon may reflect the reduction in chloroquine drug pressure on specimens from West Africa across the 10-year study period, during which oral artemisinin combination therapy has supplanted chloroquine use.\n\n【27】Drugs targeting folate metabolism, including pyrimethamine, sulfadoxine, and proguanil, are common components of fixed-dose combination antimalarials and inhibit purine and pyrimidine biosynthetic pathways . Point mutations in the _dhfr_ and _dhps_ genes conferring sulfadoxine and pyrimethamine resistance were identified as early as 1988 in Kenya. _dhps_ mutations S436AF, A437G, A581G, and A613TS have been identified in sulfonamide-resistant specimens primarily found in Southeast Asia . Higher levels of resistance have been identified in specimens carrying multiple mutations conferring a synergistic effect, especially double mutation S436FA and A613TS . Overall, we documented an increase in all mutant _dhps_ genotypes except K540E. Increases in mutant genotypes coincided with overuse of chloroquine alternatives, which have been heavily deployed in sub-Saharan Africa.\n\n【28】_dhfr_ S108N is the essential driving force associated with resistance to pyrimethamine . Our data highlight evidence of antifolate resistance over time, which may relate to increasing reliance on antifolate drugs in the face of chloroquine resistance. Additional mutations including _dhfr_ N51I, C59R, and I164 have been associated with more pyrimethamine-resistant specimens . We did not identify increases in either N51I or C59R mutations and detected only one I164L specimen in the 2008–2009 time frame, with no subsequent occurrences.\n\n【29】Our findings are limited by a small sample set and incomplete travel history, but this limitation may suggest a decreased prevalence of I164L mutation in sub-Saharan Africa. Of interest, the _dhfr_ A16V mutation generally occurs in conjunction with S108N, which is known to cause cycloguanil resistance . However, our data demonstrate no such correlation in the specimens we analyzed. Analyses by many groups over the years have led to the conclusion that greater numbers of point mutations in _dhfr_ confer greater resistance, and cross-resistance between pyrimethamine and cycloguanil is a possibility, whereas S108N is the necessary first mutation responsible for pyrimethamine resistance . Our results coincide with previous literature resulting in data on a wave of antifolate resistance markers in specimens of _P. falciparum_ imported to Ontario.\n\n【30】Artemisinin derivatives, the most powerful antimalarials used clinically, have been the main focus of concern surrounding recent efforts to mitigate _P. falciparum_ antimicrobial resistance. After the initial surge of chloroquine resistance, which was quickly followed by a diminution of antifolate effectiveness, the World Health Organization recommended use of artemisinin-based combination therapies as first-line drugs of choice. However, similar to the experience when generations of antimalarial drugs have been heavily used in other endemic areas, many countries in Africa have seen strains of multi­drug-resistant _P. falciparum_ appeared within 1 year of introducing artemisinin-based combination therapy formulations, occurring as early as 2003 . In Southeast Asia, artemisinin resistance has been associated with mutations in the propeller region of _P. falciparum_ _kelch13_ . At present, there is no documented large-scale spread of _kelch13_ mutations in Africa; however, _kelch13_ mutations have been documented in specimens from Bangladesh, suggesting a westward migration of mutations from their origins in the Thailand–Myanmar region .\n\n【31】Recently, there have been reports of _kelch13_ \\-independent treatment failure with artemether/lumefantrine in _P. falciparum_ cases imported to the United Kingdom by persons with travel history to Angola, Liberia, and Uganda . Despite novel mutations, including G112E, outside of the propeller-encoding domain in some patients, no reduced artemisinin susceptibility has been reported in the Greater Mekong region of Southeast Asia . Our data from specimens predominantly from West Africa correlate with these and other findings corroborating an absence of detectable mutations in 20 SNPs identified in the _kelch13_ gene between codon 440 and the 3′ end of the coding region. On the other hand, _pfATP6_ , which is also known as _pfSERCA_ or _pfATPase6_ , is a calcium ATPase gene involved in calcium ion transport. Point mutations at A623E and S769N in _pfATP6_ could be associated with artemisinin resistance. Increased prevalence of these point mutations may be responsible for the rise of resistance in areas such as Cambodia, French Guiana, and Senegal, where artemisinin use is uncontrolled . Our data suggest that artemisinin-resistant _P. falciparum_ infection is unlikely to occur in travelers returning to Canada, given that only 1 _atpase6_ A623E mutant was detected in both the 2008–2009 and 2013–2014 time frames, and none for the _atpase6_ S769N and _kelch13_ mutations.\n\n【32】The fixed-dose combination atovaquone/proguanil, an antimalarial prophylactic popular among travelers, has been recommended more broadly in response to potential resistance to other antimicrobial classes, including artemisinins . However, documented cases with _cytb_ mutations have resulted in treatment failure associated with resistance to atovaquone. We documented an exceptionally high prevalence of _P. falciparum_ specimens (%) with triple codon _dhfr_ 51, 59, 108 mutation, which confer resistance to proguanil, leaving the partner drug, atovaquone, as the sole antimalarial effective in clearing _P. falciparum_ (of every 5 cases in our setting). Any resistance to atovaquone, conferred by _cytb268_ mutations, or impaired atovaquone absorption can lead to treatment failure ; resistance as a result of _cytb_ Y268NSC has been identified in travelers returning from Kenya, Angola, and many parts of East Africa . We observed no _cytb_ \\-dominant mutants in our study, but in some specimens, the mutant allele frequency in the wild-type specimens were close to 20%, suggesting ≈1/5 of the infection could be comprised of mutant strains. Whether having a minor population of allelic mutants instead of dominant mutant allelic frequency equating to an overall mutant genotype can lead to treatment failure or recrudescence is unknown, but nevertheless, that condition may be an early indicator of emerging atovaquone resistance. Identifying subpopulations of resistant mutant strains can help determine appropriate treatment strategies and follow-up; prevalence of resistant strains as low as 10% can double in vitro 50% inhibitory concentration to chloroquine . This possibility reiterates the value of pyrosequencing and targeted next generation sequencing, which enable quantification of minor allelic populations.\n\n【33】Our conclusions may be limited by lack of complete data on travel history, history of malaria infection, and treatment strategies and outcomes among study participants. Subtle changes in mutant allelic frequency noted across time periods may be attributable to travel proclivities, period-to-period natural variation in expression, or a true change in prevalence. The technical limitations of developing a pyrosequencing assay targeting positions 74 and 75 only enabled qualitative analysis for _pfcrt_ double mutations at those positions. A technical limitation of pyrosequencing includes diminished accuracy of determining proportions <5%. Although _kelch13_ remains the only gene target analyzed via Sanger sequencing, minor alleles in mixed populations cannot be detected by Sanger sequencing; thus, pyrosequencing is a more useful and reliable sequencing technology for SNP analysis. Although pyrosequencing has largely been replaced by more effective techniques, such as targeted next-generation sequencing and possibly whole-genome sequencing, those newer techniques are limited by cost and the requirement for bioinformatics infrastructure and expertise, which generally confine their application to large reference or research laboratories. Future studies involving prospective recruitment of _P. falciparum_ –infected travelers, with complete demographic, clinical, and treatment outcome data and use of targeted next-generation sequencing to identify other SNPs, risk factors, or confounders of resistance, could fill these knowledge gaps.\n\n【34】Surveillance of SNPs can serve as a sentinel of clinical resistance. Triple codon _dhfr_ mutants, for example, were detected years before observation of increased sulfadoxine/pyrimethamine treatment failures in Kenya . Robust surveillance analyses over time of known markers conferring drug resistance in _P. falciparum_ imported by travelers are lacking. We observed a high percentage of molecular resistance to proguanil and none to atovaquone. We observed an increase in imported antifolate-resistant of _P. falciparum_ strains, including a decrease in chloroquine-resistant strains and virtually no signs of artemisinin resistance because of mutations at _kelch13._ Importation of resistant _P. falciparum_ mono-infections highlights the importance of developing better surveillance tools to monitor drug resistance patterns based on time and source region. Such surveillance would also inform clinical decision-making and serve as a resource for tracking antimalarial resistance that clinicians could monitor for possible genetic markers of reduced drug efficacy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b6308cf9-b1b5-4c37-9b3a-241beb195118", "title": "Cronobacter spp. in Common Breast Milk Substitutes, Bogotá, Colombia", "text": "【0】Cronobacter spp. in Common Breast Milk Substitutes, Bogotá, Colombia\n_Cronobacter_ spp. is a group of emerging foodborne opportunistic pathogenic microorganisms that can cause deadly disease in neonates, children, older adults, and immunocompromised persons, including meningitis, septicemia, and necrotizing enterocolitis in neonates and infants . The presence of _Cronobacter_ spp. is widespread in dry foods. Occurrence is higher in infant milk formula (IMF) because some species of _Cronobacter_ are able to tolerate different types of stress in IMF; for example, _C. sakazakii_ is able to survive up to 2 years in IMF, which increases the possibility for baby foods to become reservoirs, given that milk increases the cultivability and recovery of _C. sakazakii_ in dry environments .\n\n【1】In Colombia, according to the 2010 Nutritional Situation Survey (ENSIN, for its acronym in Spanish), breast-feeding supplementation is gradually made from birth, starting at 27% in the first months until 76% at 9 months of age . These data suggest that a large number of children can start the intake of supplementary foods, most likely in the form of bottle-feeding . IMFs are not sterile products; pathogenic microorganisms such as _Salmonella_ spp. and _Cronobacter_ spp. can be recovered from them .\n\n【2】At the local level in Colombia, the most common IMF substitutes used are corn and plantain starches. The main reason for their use is the high cost of imported foods . The microbiologic quality of these foods is important because they are given to breast-fed infants or children <1 year of age, who are vulnerable to infections. However, unlike IMFs, little attention has been paid to the contamination of IMF substitutes with _Cronobacter_ spp.\n\n【3】### The Study\n\n【4】We collected information on starch brands and consumption preferences from the stores where starch products were sold. We sampled city districts with the highest frequency of consumption (i.e. >1×/wk). The sampling was distributed per percentage weight according to city district and starch composition. We collected 36 samples of corn starch, 53 samples of plaintain starch, and 13 samples of other starches (N = 102 samples)\n\n【5】According to the evaluation conducted after enactment of the current baby food regulations in Colombia , product noncompliance was most often attributable to the presence of coliforms, which were detected in 23.5% (/102) of samples. Coliforms were detected in 50% (/24) of plantain starch samples, 12.5% (/24) of other starch samples, and 0% (/24) of corn starch samples. For the detection of _Cronobacter_ spp. we used the ISO 22964:2006 method, by which we were able to recover isolates from 35 (.3%) samples. However, recovery was already improved in the new ISO 22964:2017 version, which is desirable because _Cronobacter_ spp. have been found at levels of <1 UFC/100 g of IMFs .\n\n【6】We differentiated _Cronobacter_ spp. species by using PCR _cgcA_ enzyme  (Platinum Blue PCR Super Mix; Invitrogen, Carlsbad, CA, USA). We then improved specificity and reduced the amplification of nonspecified fragments .\n\n【7】Of the 102 samples, 34.3% (/102) were positive for _Cronobacter_ spp. (.5% \\[27/35\\] of plantain starch samples, 7.8% \\[8/35\\] of other starch samples, and 0% of corn starch samples). The _Cronobacter_ species identified were _C. sakazakii_ (% \\[26/35\\] of isolates), _C. malonaticus_ (% \\[5/35\\] of isolates), and _C. dublinensis_ (% \\[4/35\\] of isolates) .\n\n【8】The 34.3% _Cronobacter_ spp. prevalence was similar to that reported in plant derivatives (e.g. 20.1%  and 31.3% ) and in cereals (%) . Prevalence in processed and other foods has ranged from 3% to 30% . Although the greatest number of positive isolates was recovered from plantain starches, no information on _Cronobacter_ spp. in plantain matrixes is available.\n\n【9】### Conclusions\n\n【10】From the point of view of microbiologic quality, the presence of coliform bacteria and pathogens shows the risk of contamination of IMF substitutes. For _Cronobacter_ spp. the international standard is a total absence in IMF . The most recovered species in the samples we tested was _C._ _sakazakii_ , which is the species associated with the highest number of disease cases reported in neonates and whose sequence type 4 is already known to be associated with severe cases of meningitis . The PCR method we used enabled greater coverage compared with the traditional method because the identification of the species is confirmed in a single analytical run, thus speeding up the subsequent public health response and providing valuable information for epidemiologic surveillance.\n\n【11】In Bogotá, no cases of foodborne diseases related to the presence of _Cronobacter_ spp. have been reported. However, _Cronobacter_ are not subject to mandatory re-porting in Colombia because no active surveillance of this pathogen exists, and the medical community lacks information about the pathogen. It is important to char-acterize and document the presence of _Cronobacter_ spp. in different foods to assess the risk to which children <1 year of age and breast-fed infants are exposed and to identify the connection between diseases like meningitis and the consumption of contaminated baby foods produced locally. Some measures have been proposed to mitigate this risk. It is vital to indicate on the starch packaging that starch must be reconstituted in water at \\> 70°C for \\> 1 minute before consumption, and starch leftovers must be refrigerated to minimize the risk for contamination with _Cronobacter_ spp. In addition, the World Health Organization–Codex Alimentarius Code of Hygiene Practices provides guidelines for governments, industries, and consumers to support and teach caregivers about safe preparation of starches . Our findings must lead to an in-depth review of the current regulations for baby foods in Colombia and a modification of Bogotá’s System of Inspection, Surveillance, and Control so that this system can be more preventive and responsive in ensuring the health of citizens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d966e283-08cf-4562-8b5d-6bc5e7e70941", "title": "Factors Influencing Emerging Infectious Diseases in the Southeastern United States", "text": "【0】Factors Influencing Emerging Infectious Diseases in the Southeastern United States\nDuring June 23–24, 2009, the Southeastern Center for Emerging Biological Threats  at Emory University, Atlanta, Georgia, USA, convened a conference entitled Factors in Emerging Infectious Diseases in the Southeastern United States. Over the past 3 decades, more than two thirds of emerging infectious diseases have had an origin in animals . The Conference featured more than 20 presentations focusing on the impact of global migration, climate and weather changes, and ecosystem alterations on emerging vector-borne and zoonotic diseases in the Southeast. Opportunities presented by the One Health Initiative  were highlighted.\n\n【1】In his keynote address, Howard Markel, University of Michigan, presented an historical perspective. His analysis showed that the unpredictability of emerging diseases has been difficult to explain to the public during epidemics. There has been a tendency to make the victim a scapegoat and to blame individual persons or groups for importation of disease. He emphasized the importance of transparency in reporting and contrasted the experience of SARS (severe acute respiratory syndrome) with that of the ongoing 2009 (H1N1) influenza pandemic.\n\n【2】Corrie Brown, University of Georgia, discussed the globalization of disease with specific emphasis on the southeastern United States. She concluded that this region of the country was at risk for introduction and spread of microbial pathogens originating in other parts of the world because it serves as a transportation hub for North America and receives a disproportionate share of immigrants.\n\n【3】The importance of travel and migration in disease emergence  was a recurrent theme. Emory University’s Carlos Franco-Paredes addressed leprosy (Hansen disease) in the Southeast. He linked leprosy in Georgia to the construction boom in Atlanta, which has attracted immigrants, some of whom have undiagnosed disease. Patricia Walker, Center for International Health and International Travel Clinic in St. Paul, Minnesota, discussed Minnesota’s experience with refugee health problems.\n\n【4】Minnesota is home to the largest Hmong population in the United States and the largest Somali population outside Somalia. The Minnesota Immigrant Health Task Force identified 8 key steps to improve immigrant health: 1) provide equal access to healthcare for all; 2) respect patients’ language preferences; 3) provide equitable payment for necessary healthcare services; 4) develop clinical guidelines and best practices for immigrant healthcare; 5) diversify the healthcare workforce to include more immigrant and minority providers; 6) employ trained medical interpreters; 7) train healthcare providers; and 8) educate patients.\n\n【5】State refugee coordinators from Florida, Georgia, and North Carolina emphasized issues and challenges, including timely arrival notification, lack of resources, language and cultural barriers, health literacy, and stigmatization all of which must be addressed to control emerging diseases among immigrants and refugees.\n\n【6】Subsequent presentations focused on the impact of climate change on health in the region . George Luber, Associate Director for Global Climate Change, Centers for Disease Control and Prevention, emphasized that responding to extreme weather events stresses environmental health services and public health infrastructure.\n\n【7】Rebecca Eisen, CDC’s Division of Vector-Borne Infectious Diseases, explored the risk of reintroduction of vector-borne diseases. Reintroduction is made possible by transit between the southeastern United States and climatically similar endemic regions through air travel, cargo ships, exotic pet trade, migratory birds, and intentional release of species. She noted that factors contributing to reintroduction can be divided into vector-specific and vertebrate host-specific categories. She identified 7 vector-specific factors: 1) preferred breeding habitat of the vector present at the site of origin; 2) host preference; 3) vector efficiency; 4) extrinsic incubation period; 5) likelihood of surviving to the second blood meal; 6) efficiency of transovarial transmission; and 7) ability to remain infectious long-term. Vertebrate host-specific factors include duration of infectivity, duration of incubation periods, pathogen load at or above transmission threshold, and degree of virulence.\n\n【8】William Karesh, Wildlife Conservation Society, provided an overview of the interplay among human, animal, and ecosystem health. When avian influenza emerged during 2004–2005, scientists, public health, and animal health officials had little information on its prevalence and geographic distribution. Those studying the disease needed to know where wild birds were migrating and whether they carried influenza (H5N1) virus. The Global Avian Influenza Network for Surveillance in 2006 was created to answer some of these questions . The partnering of animal and human influenza specialists during the recent influenza pandemic is an example of how an integrated One Health approach can improve preparedness and response to the evolving pandemic.\n\n【9】Nina Marano, Division of Global Migration and Quarantine at the Centers for Disease Control and Prevention, addressed the public health implications of the exotic animal trade. Animals are imported into the United States for a number of reasons, e.g. exhibition at zoos, scientific education, research, and conservations programs, and as companion animals. Increasingly, animals are being legally and illegally imported for a thriving commercial pet trade, particularly through the Internet, rather than through pet stores which are subject to licensure and inspection. Zoonotic diseases associated with pocket pets include salmonellosis, lymphocyctic choriomeningitis, tularemia, and monkeypox.\n\n【10】Marguerite Pappaioanou, Executive Director, Association of American Veterinary Medical Colleges (AAVMC) summarized the challenges and opportunities in achieving an integrated early detection and response system for emerging zoonotic infectious diseases.AAVMC advocates on behalf of the academic veterinary medicine membership and works closely with its American Veterinary Medical Association and other partners promoting global collaboration by linking with veterinary medical educational institutions in developed and developing countries, establishing a dialogue between faculty and students, and fostering capacity building and training. AAVMC enthusiastically endorses the One Health Initiative, with the aim of promoting partnerships across the human, animal, agricultural, and environmental health sectors. AAVMC member institutions in the Southeast are engaged in multidisciplinary and multiinstitutional research, training, and educational programs to help put the One Health Initiative into action.\n\n【11】In summary, the conference identified challenges and opportunities to achieve an effective integrated early detection and response system for zoonotic diseases in the Southeast. Presenters and conference participants strongly supported the One Health model, which emphasizes that mitigating zoonoses requires an integrated, interdisciplinary approach at the convergence of human disease, animal disease, and environmental science at the local, regional, national, and global levels.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2236eb2c-5fa4-4336-98e2-e862931e2124", "title": "Burkholderia pseudomallei in Environment of Adolescent Siblings with Melioidosis, Kerala, India, 2019", "text": "【0】Burkholderia pseudomallei in Environment of Adolescent Siblings with Melioidosis, Kerala, India, 2019\nMelioidosis is a potentially life-threatening infection caused by the soil-dwelling bacteria _Burkholderia pseudomallei._ The clinical spectrum of disease ranges from mild localized lesions to fulminant sepsis . The disease is endemic to Southeast Asia and Australia. Recently, the southern part of India has emerged as a melioidosis hot spot; infection in children accounts for 8% of infections . The risk of children acquiring infection is high because of the likelihood of environmental exposure either through wounds acquired while playing in contaminated muddy water or by inhalation of aerosols containing the bacteria during the rainy season . We report melioidosis in 2 siblings from Kerala, India, and the subsequent isolation of _B. pseudomallei_ from the soil in their backyard.\n\n【1】### The Study\n\n【2】In July 2019, at the onset of the monsoon season, a 15-year-old boy from southern India (patient 1) was examined because of fever for 1 week along with cough, myalgia, headache, vomiting, and diarrhea, followed by rashes over the trunk and breathlessness for 1 day. At hospital admission, the child was febrile, tachycardic, and tachypneic; his blood pressure was 85/67 mm Hg, and oxygen saturation on room air was 90%. He had a few discrete pustular lesions over the trunk . Chest examination revealed bilateral crackles. Initial laboratory studies showed leukocytosis; increased levels of inflammatory markers; and abnormal liver function, renal function, and coagulation profile, suggestive of multiorgan dysfunction . The patient was admitted to the intensive care unit, and noninvasive ventilation and inotropic support were initiated along with empiric intravenous (IV) administration of piperacillin/tazobactam. Acute respiratory distress syndrome  with respiratory failure subsequently developed, and the patient was emergently intubated. Antimicrobial drugs were changed to meropenem and vancomycin. Despite these measures, the patient deteriorated rapidly, respiratory status and shock worsened, and he died within 24 hours after hospital admission.\n\n【3】Multiplex real-time PCR  of a blood sample for common tropical diseases was positive for _Burkholderia mallei_ / _pseudomallei_ . Cultures of blood and of pus aspirated from the skin lesions yielded nonfermentative gram-negative bacilli, which the Vitek 2 system  identified as _B. pseudomallei_ . Isolate identity was confirmed at the Centre for Emerging and Tropical Diseases, Manipal Academy of Higher Education (Manipal, India), by monoclonal antibody–based latex agglutination test and PCR specific for the _B. pseudomallei_ –specific _TTSS1_ gene . Both isolates were sensitive to amoxicillin/clavulanic acid, ceftazidime, cotrimoxazole, doxycycline, and meropenem.\n\n【4】One month later, the patient’s 12-year-old brother (patient 2) was examined for fever of 3 weeks’ duration along with neck swelling and sore throat of 2 weeks’ duration. He received amoxicillin/clavulanate with no response. His vital signs were stable. Throat examination revealed bilateral tonsillar exudates. His left posterior cervical lymph node was 3 × 2 cm, firm, and nontender. His initial laboratory parameters were unremarkable . Histopathology of the cervical node showed necrotizing lymphadenitis. Culture of lymph node tissue and throat swab grew _B. pseudomallei_ with sensitivity identical to that of the isolate from his sibling. IV ceftazidime was administered, but because fever persisted, IV trimethoprim/sulfamethoxazole was added on day 5. Whole-body fluorodeoxyglucose positron emission tomography magnetic resonance imaging did not show any deep-seated abscesses. Because the fever continued to persist, ceftazidime was changed to meropenem by day 10. Fever subsided by day 14, and IV meropenem and trimethoprim/sulfamethoxazole were continued for 2 more weeks. The patient was discharged with instructions to take oral trimethoprim/sulfamethoxazole for 4 months. As of 30 months later, the child remained healthy.\n\n【5】Melioidosis cases among siblings within a span of 1 month suggested exposure to a common environmental source. In November 2019, we sampled the patients’ environment and collected 20 soil samples from an area of 1,000 square feet around the patients’ house . We collected and processed soil dug from the flower bed and soil at 30 cm deep according to published consensus guideline for detection of _B_ . _pseudomallei_ in soil . We tested samples by culture and PCR for the presence of the bacteria. Of the 20 samples, 3 were positive for _B. pseudomallei_ by culture  and 18 showed positive signal in the qualitative PCR targeting the _TTSS1_ gene for _B. pseudomallei_ . Antimicrobial susceptibility was the same for all 3 environmental isolates as for the clinical strains. Multilocus sequence typing of the clinical and environmental isolates indicated that the sequence type (ST) of the strain from the 12-year-old sibling matched that of the environmental isolates (ST 1944). The sequence type of the isolate from the 15-year-old sibling was different (ST 1556). Interview of the family indicated that both siblings had spent time playing outside the house during the rain, a known risk factor for acquiring melioidosis. The other family members were made aware of the risk factors of acquiring the disease from their environment and were advised to take preventive measures. They were asked to inform their doctor regarding the possibility of melioidosis when seeking medical care for any febrile illness or respiratory infection. However, they have remained healthy throughout the follow-up period.\n\n【6】### Conclusion\n\n【7】Melioidosis is an underdiagnosed disease in India because of its diverse clinical manifestations, clinical overlap with other conditions such as tuberculosis, limited awareness by physicians and the general public, and laboratory constraints . Previous studies have reported cases from eastern, northeastern, and southern India . Both patients described in this report became ill during the monsoon season, when it rains heavily in Kerala. A recent study from southern India indicated that 90% of the children became ill during the monsoon season . Studies have shown a correlation between the disease and rainfall intensity .\n\n【8】A recent study from India reported frequent respiratory involvement and bacteremia in adults with acute cases . A study conducted in Southeast Asia reported that localized manifestations were more common in children; 46%–63% had either cutaneous lesions or lymphadenopathy . A similar study from India reported that among total cases of melioidosis, 8% were in children and the mortality rate was 9%. In that study, 54% of children had disseminated illness and 46% had localized lesions .\n\n【9】Fatal melioidosis in siblings and isolation of _B_ . _pseudomallei_ from their environment have been recently reported from Mexico . Cases of _B. pseudomallei_ infection among siblings with cystic fibrosis have been reported from Australia . _B. pseudomallei_ was also isolated from 5 of the 40 soil and water samples collected from the coastal region of Malabar, Kerala, during the 2016 rainy season . We report simultaneous isolation of _B. pseudomallei_ from humans and from their immediate surrounding environment.\n\n【10】This study highlights the perpetual risk faced by family members of affected persons and other persons residing in a region where cases of melioidosis have been reported. It also highlights the value of seeking appropriate medical care at the onset of any febrile illness, because early diagnosis and treatment can decrease illness and death associated with the disease. Public health officials should emphasize additional precautions in melioidosis-endemic regions (e.g. not going out in the field during heavy rainfall, not walking barefoot, and not drinking water from natural reservoirs in melioidosis-endemic areas). Healthcare workers and the community should be aware of the risk for melioidosis, especially during the rainy season, and diagnostic testing should be strengthened to increase prompt identification and treatment. Furthermore, the National Centre for Disease Control should add melioidosis to the notifiable disease list in India to enable increased environmental surveillance and provide data for melioidosis risk mapping.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "168c4eda-c319-40d3-b11e-47f5a08ce807", "title": "Individual-Level Fitness and Absenteeism in New York City Middle School Youths, 2006–2013", "text": "【0】Individual-Level Fitness and Absenteeism in New York City Middle School Youths, 2006–2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Youth health-related fitness positively affects academic outcomes, although limited research has focused on the relationship between fitness and school absenteeism. We examined the longitudinal association between individual children’s fitness and lagged school absenteeism over 4 years in urban middle schools.\n\n【3】**Methods**\n\n【4】Six cohorts of New York City public school students were followed from grades 5 through 8 (school years 2006–2007 through 2012–2013; n = 349,381). A 3-level longitudinal generalized linear mixed model was used to test the association of change in fitness composite percentile scores and 1-year lagged child-specific days absent.\n\n【5】**Results**\n\n【6】Adjusted 3-level negative binomial models showed that students with a more than 20% increase, 10% to 20% increase, less than 10% increase or decrease, and 10% to 20% decrease in fitness from the year prior had 11.9% (% confidence interval \\[CI\\], 7.2–16.8), 6.1% (% CI, 1.0–11.4), 2.6% (% CI, −1.1 to 6.5), and 0.4% (% CI, −4.3 to 5.4) lower absenteeism compared with students with a more than 20% fitness decrease.\n\n【7】**Conclusion**\n\n【8】Cumulative effects of fitness improvement could have a significant impact on child absenteeism over time, particularly in high-need subgroups. Future research should examine the potential for school-based fitness interventions to reduce absenteeism rates, particularly for youths who have fitness drop-offs in adolescence.\n\n【9】Introduction\n------------\n\n【10】Youth physical activity and health-related fitness (henceforth fitness) positively affects academic outcomes , potentially acting through pathways involving enhanced cognition and memory  or improvements in both physical and psychosocial wellness . Fitness and physical activity are strongly associated, and frequent vigorous physical activities are likely to improve fitness . For example, daily physical activity of at least moderate intensity is associated with reduced clustering of cardiovascular risk factors in youths, including high blood pressure, insulin level, lipids, and adiposity . However, accelerometry data show that only 42% of children aged 6 to 11 years meet international physical activity recommendations for at least 60 minutes per day of moderate to vigorous physical activity . Although these rates are similar to rates in European countries , declines in physical activity are steeper from childhood to adolescence in the United States compared with declines in other nations . This national trend is also evident in New York City (NYC), where 40% and 20% of youths aged 6 to 12 and 14 to 18, respectively, meet physical activity recommendations .\n\n【11】Another established predictor of academic performance is school absenteeism , which may mediate the observed fitness–academic achievement association. Maintaining regular attendance, defined as missing fewer than 6 excused or unexcused days per year, predicts academic success . School absenteeism, regardless of reason, predicts poor academic achievement and is associated with poor school adjustment; alcohol, tobacco, and substance use; increased rates of teen pregnancy; juvenile delinquency; and both family and home–school disengagement . Fitness improvements may both directly and indirectly reduce absenteeism, working potentially through pathways involving self-esteem, physical health, mental health, and cognitive processing .\n\n【12】Limited research has examined the fitness–absenteeism relationship , demonstrating consistent inverse associations between fitness and school absenteeism. For example, Blom et al demonstrated that students with greater fitness had lower odds of more than 8 absences per year (odds ratio \\[OR\\], 3.31; 95% confidence interval \\[CI\\], 1.51–7.28 for students with 6 compared with less than 5 healthy fitness zones achieved) . Two other articles found significant crude associations between student physical activity and absenteeism . These studies drew predominantly from cross-sectional data and did not account for a range of potential confounders, including contextual factors that contribute to absenteeism and fitness. For example, neighborhood poverty contributes to parent–school engagement and youth fitness . Similarly, school size affects programs and policy toward school attendance and physical activity . The bulk of research on fitness and absenteeism is unable to support causal hypotheses given that temporality of exposure and outcome are not known. Nuanced research in this area that draws from individual-level measures collected over multiple years and includes school-level factors is necessary to better inform policy in support of increased school-based fitness programs.\n\n【13】We analyzed the longitudinal association between change in fitness and 1-year lagged absenteeism in 6 cohorts of NYC public school students based on year of initiating middle school and followed consecutively over 4 years (fitness change from grades 5 to 6, 6 to 7, and 7 to 8 paired with days absent per year for grades 6, 7, and 8, respectively) during a 7-year study period (through 2012–2013). We hypothesized that improvements in fitness (cardiorespiratory, muscular endurance, and muscular strength fitness composite percentile scores) would predict lower subsequent absenteeism.\n\n【14】Methods\n-------\n\n【15】### Study population\n\n【16】Data were drawn from the NYC FITNESSGRAM (Fitnessgram) data set jointly managed by the NYC Department of Education (DOE) and Department of Health and Mental Hygiene (DOHMH) . It comprises annual fitness assessments collected by DOE for approximately 870,000 NYC public school students per year (grades K–12) starting in 2006–2007. This study was approved by the City University of New York and DOHMH institutional review boards.\n\n【17】The Fitnessgram is based on the Cooper Institute’s Fitnessgram, which has both strong reliability and validity . Fitnessgram performance tests provide a health assessment related to present and future health outcomes. NYC schools are mandated to have 85% or more of eligible students complete the test each year. Inclusion criteria for this study included enrollment in a NYC public school that collected Fitnessgram measurements for 2 or more consecutive years while in grades 6 through 8 during the study period (through 2012–2013) . Student cohorts were defined based on year of initiating grade 6. Students were excluded (n = 6,225) if they were enrolled for less than n − 5 days per school year (where n is the maximum number of days enrolled across all students in each given year \\[n range: 292–297 days\\]) to ensure a consistent period of observation across school years with different total instructional days per year. Next, students were excluded if they did not take the Fitnessgram test for 2 or more consecutive years (n = 56,464), attended schools with poor-quality fitness data (n = 350), or changed schools during 6th through 8th grade (to be able to account for school clustering in the analysis; n = 44,977). After the above exclusions, the final sample of 6th through 8th graders included 349,381 unique students (51% male, 83% born in the United States, 38% Hispanic, 28% non-Hispanic black, and 16% non-Hispanic white; mean \\[standard deviation (SD)\\] school population = 541 ). Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively, across 624 schools.  \n\n【18】Sample selection flowchart for the association of fitness and absenteeism in New York City (NYC) public middle school students, 2006–2007 through 2012–2013. \n\n【19】### Measures\n\n【20】The primary exposure was a categorical variable representing age- and sex-specific percentage change in fitness composite percentile scores based on the sum of percentile scores for the Progressive Aerobic Cardiovascular Endurance Run (PACER), muscle strength and endurance (curl-up and push-up) tests . Scores were converted to percentiles to account for expected improvements in performance with increasing age and by sex. The fitness variable was categorized as more than 20% decrease, 10% to 20% decrease, less than 10% change, 10% to 20% increase, and greater than 20% increase in performance from the year prior, consistent with longitudinal research on fitness and academic outcomes drawing from the Fitnessgram data set .\n\n【21】The primary outcome variable for this analysis was student-level number of days absent per year. Annual enrollment and attendance records were matched to Fitnessgram results by a unique student identifier.\n\n【22】Adjusted models included sex, age, race/ethnicity, place of birth, socioeconomic status (SES), and school size. These covariates predict both fitness and absenteeism . Age at the time of height and weight measurement was treated as a continuous variable. Race/ethnicity was based on school enrollment forms completed by parents and grouped into 5 categories: Hispanic, non-Hispanic black, non-Hispanic white, Asian/Pacific Islander, and other. Place of birth (United States vs foreign country) was included as a covariate based on literature demonstrating that immigration status is predictive of physical activity  and school attendance . SES was defined as the percentage of households in the students’ school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] poverty area) according to American Community Survey 2007–2012 data . School size classified schools, as per the literature, as small (<400 students) or nonsmall (≥400 students) .\n\n【23】Change in obesity status from the year prior (obese to not obese, consistently not obese, consistently obese, not obese to obese) was also included as a potential confounder based on the literature . Body mass index (BMI) is collected annually as a part of the Fitnessgram curriculum. Obesity was defined as having a BMI in the 95th percentile or higher for the same sex and age group using 2000 Centers for Disease Control and Prevention guidelines . Change in obesity status category was used in lieu of changes in BMI percentile to capture meaningful shifts in body composition associated with school outcomes .\n\n【24】### Statistical analysis\n\n【25】Descriptive statistics were computed to summarize sample characteristics. Next, trends in absenteeism (days absent) by fitness, grade, and demographics were examined.\n\n【26】Because observations were nested within students, nested within schools, mixed-model methods were used. Specifically, a series of crude and adjusted 3-level longitudinal generalized linear mixed models with random intercepts for student and school effects were fit to assess the fitness–absenteeism association while accounting for clustering and individual- and school-level confounders.\n\n【27】First, to determine the extent of variation in absenteeism at the school level, an unconditional model with random intercepts was fit to the data (model 1). The school-level intraclass correlation (ICC) was calculated as the ratio of the variance for the school divided by the sum of the 3 variance parameter estimates, represented as σ 2  school  **/ (σ 2  student  \\+ σ 2  school  \\+ σ 2  ε  ). Although univariate distributions for days absent demonstrated a long right-tailed Poisson distribution, the ICC was calculated based on a linear model given that the ICC definition is not well defined for Poisson models .\n\n【28】Next, the longitudinal association of change in fitness and lagged number of days absent per year was assessed by using a 3-level crude longitudinal negative binomial mixed model with random intercepts and the exposure, child-specific change in fitness from the year prior, as well as an offset term representing total instructional days per school year included in the model (model 2). Negative binomial models were used because data were overdispersed. β Coefficients represented the effects of the exposure, change in fitness on outcome, 1-year lagged number of days absent per year. Absenteeism rates were computed by calculating the incidence rate ratio, represented as exp(β).\n\n【29】Finally, potential individual- and group-level confounders were added to the model (model 3). Confounding variables included level-1 time-varying covariates for grade, year (to control for potential cohort effects), and change in obesity status from the year prior, level-2 covariates for individual sociodemographic factors (sex, race/ethnicity, place of birth), level-3 covariates for school size and SES, and interactions (grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and SES\\*race/ethnicity).\n\n【30】In these analyses, students contributed fitness-change data for 5th to 6th, 6th to 7th, and/or 7th to 8th grades (n = 349,381 unique students; 675,318 observations). A 2-sided _P_ value of less than .05 was considered significant. Analyses were performed using SAS version 9.4 software (SAS Institute, Inc).\n\n【31】Results\n-------\n\n【32】Just under 40% of students had less than 10% change in fitness from the year prior, followed by greater than 20% increase (%), greater than 20% decrease (%), 10% to 20% increase (%), and 10% to 20% decrease (%) . The mean (SD) number of days absent per year were highest among boys (.0 \\[11.7\\]) and Hispanic (.6 \\[12.9\\]) and non-Hispanic black (.3 \\[13.1\\]) racial/ethnic groups . Mean days absent were also highest among students who were born in the United States (.3 \\[12.1\\]) compared with those who were born in a foreign country (.1 \\[13.8\\]).\n\n【33】Overall, the mean number of days absent per year decreased with improvements in fitness scores from the year prior. The mean (SD) days absent per year for students with the lowest (>20% decrease) to highest (>20% increase) improvements in fitness were 11.9 (.8), 11.1 (.2), 10.7 (.9), 10.3 (.3), and 10.3 (.2). Also, fitness decreased and absenteeism increased with increasing grade . Moreover, for students in the same grade, the difference in mean days absent for those with improved versus diminished fitness became larger with increasing grade level . For example, mean (SD) days absent for students with the greatest increase (>20%) in fitness were 9.6 (.1), 9.8 (.8), and 11.9 (.7), for students in 6th, 7th, and 8th grades, respectively. In contrast, mean (SD) days absent for students with the greatest decrease (>20%) in fitness were 10.6 (.3), 11.6 (.6), and 13.9 (.3), for students in 6th, 7th, and 8th grades, respectively.  \n\n【34】Mean days absent per year by grade across fitness-change categories in New York City public middle school students (N = 349,381), 2006–2007 through 2012–2013. Change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior. Categories are based on tabulated mean estimates. \n\n【35】The ICC (model 1) demonstrated a sizable degree of variance in student absenteeism explained by schools (%). Results from model 2 showed all levels of change in fitness were significantly associated with absenteeism (P_ < .001). Compared with the reference category (>20% decrease in fitness), the absenteeism rate decreased 13.3% (% CI, 8.3–16.6), 8.3% (% CI, 3.3–12.7), 5.6% (% CI, 1.9–9.0), and 1.6% (% CI, −3.0 to 6.2) for those who had a greater than 20% increase, 10% to 20% increase, less than 10% change, and 10% to 20% decrease in fitness composite percentile scores from the year prior, respectively.\n\n【36】After adjusting for covariates (sex, race/ethnicity, change in obesity status from the year prior, place of birth, SES, and school size), and including interactions (grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and SES\\*race/ethnicity), β estimates for the association of fitness change and lagged number of days absent per year diminished but remained significant (P_ < .005). Relative to the reference category (>20% decrease in fitness), the absenteeism rate decreased 11.9% (% CI, 7.2–16.8), 6.1% (% CI, 1.0–11.4), 2.6% (% CI, −1.1 to 6.5), and 0.4% (% CI, −4.3 to 5.4) for those who had a greater than 20% increase, 10% to 20% increase, less than 10% change, and 10% to 20% decrease in fitness composite percentile scores from the year prior, respectively (model 3).\n\n【37】Sensitivity analyses were run to determine the effect of days of enrollment exclusions, BMI categorization specification, and total years of consecutive fitness change data on findings. Results showed slightly more conservative estimates for the magnitude of effects, although the inverse dose–response association remained consistent and significant (P_ < .001, _P_ \\= .004, and _P_ \\= .01 for enrollment, BMI, and fitness data sensitivity models, respectively).\n\n【38】Discussion\n----------\n\n【39】We found that all levels of 1-year change in fitness were significantly associated with absenteeism (P_ < .001) in both crude and adjusted models. Furthermore, consistent levels of fitness improvement each year at the greater than 20% level (vs >20% decrease) were found to have the potential to reduce a student’s number of days absent substantially. For example, a child with a mean 10 days absent in 6th grade would have 6.5 days absent per year in 8th grade and 1.5 days absent per year in 12th grade. This change in days absent represents a shift well within the range of regular attendance (≤5 days absent per year). Findings here are consistent with the existing cross-sectional literature on fitness and absenteeism , lending strong support for future research on the effects of youth fitness interventions on school absenteeism. NYC programs unrelated to fitness promotion have shown a 15% reduction in chronic absenteeism in 100 high-need schools over 2 years , through implementing “early warning” flags to identify at-risk students, family and student “success mentors,” progress monitoring systems, and community collaborations. However, despite gains and similar programs nationally, high absenteeism rates remain widespread, including 5 million to 7.5 million chronically absent US students each year .\n\n【40】Strengths of this study were being the first article to the authors’ knowledge to examine the association of change in fitness and lagged absenteeism, drawing from multiple years of multilevel data. Also, this analysis included a large and diverse study sample of approximately 349,000 students comprised of 6 cohorts.\n\n【41】Findings from this study may not be generalized to other cities or nationally, given a high minority and low-income population in NYC. Future work should examine potential differences in the fitness–attendance relationship by race/ethnicity and poverty status, given higher absenteeism observed in this study among both non-Hispanic black and Hispanic students and those attending schools in high poverty areas. Furthermore, although DOE protocols promote retesting students who are absent on the original testing dates, a large number of students were excluded because of missing Fitnessgram tests for 2 or more consecutive years, insufficient enrollment period, or moving schools. Not all students are required to take the Fitnessgram, including those with chronic health conditions such as severe asthma. These students, however, would be more likely to have higher absenteeism given psychosocial, family, and health factors associated with moving and long-term absences . These effects potentially would move the association farther from the null.\n\n【42】Although we offer evidence in support of a causal association between fitness change and absenteeism, a bidirectional relationship may exist between exposure and outcome. For example, it is possible that children who have higher absenteeism are more sedentary, particularly if they are ill or occupied in nonactive ways (eg, video-game playing, watching television). Domestic factors may also persist over time. In this sense, although this analysis lagged absenteeism to fitness, the temporality of exposure and outcome could be reversed. Future research should explore the directionality of fitness and absenteeism in more detail, in addition to the role of chronic conditions in this association.\n\n【43】In our study, systematic bias and differential measurement error are possible, given that the Fitnessgram data are not collected for research purposes. Data were not available on many student- and school-level factors, including self-esteem, drug and alcohol use, family structure, and individual household poverty (such as income or eligibility for free or reduced-price lunch). These factors may influence not only absenteeism but also motivation to perform well on fitness tests. Absence of this data makes it difficult to disentangle these relationships. Future work should research whether mental, social, or emotional health and peer or parent influence are antecedents to fitness on the hypothesized fitness–attendance causal pathway. This research may shed light on why some adolescents have fitness performance drop-offs and may garner particular attendance benefits from these interventions.\n\n【44】Although testing protocols are designed to promote consistency across administers, Fitnessgram testing sites may vary in their implementation of the protocol. However, in NYC the Fitnessgram is administered by physical education teachers who receive formal training on conducting the test, including manuals, video-based training, and site visits, as well as calibrated scales .\n\n【45】Fitness levels in US youths decline with increasing age at rates faster than in other nations. Diminished fitness is shown in longitudinal studies to be associated with lower academic performance, and cross-sectionally to be associated with higher absenteeism. We present evidence for a longitudinal inverse dose–response association between fitness and absenteeism in NYC middle school youths. Cumulative effects of consistent fitness improvements from 6th through 12th grades may shift a child from chronic absenteeism to regular attendance. Future research should examine the effectiveness of school-based fitness interventions to reduce absenteeism rates, particularly within subgroups that have fitness drop-offs in adolescence. Findings may inform policy mandating increases in school fitness time, including increased classroom-based physical activity and both stricter school physical education and recess policies.\n\n【46】Tables\n------\n\n【47】#####  Table 1. Demographic and Fitness-Change Characteristics of New York City Public Middle School Students (N = 349,381), 2006–2007 Through 2012–2013\n\n| Characteristic | n a,b (%) |\n| --- | --- |\n| **Sex** | **Sex** |\n| Male | 177,355  |\n| Female | 172,026  |\n| **Race/ethnicity** | **Race/ethnicity** |\n| Asian or Pacific Islander | 58,295  |\n| Hispanic | 134,453  |\n| Non-Hispanic black | 99,363  |\n| Non-Hispanic white | 55,857  |\n| **Language spoken at home** | **Language spoken at home** |\n| English | 197,727  |\n| Spanish | 86,052  |\n| Other language | 65,602  |\n| **Place of birth** | **Place of birth** |\n| United States | 289,160  |\n| Foreign country | 60,149  |\n| **Change in fitness c (all years)** | **Change in fitness c (all years)** |\n| \\>20% Decrease | 126,115  |\n| 10%–20% Decrease | 79,172  |\n| <10% Change | 253,161  |\n| 10%–20% Increase | 82,117  |\n| \\>20% Increase | 134,753  |\n| **Change in obesity status d (all years)** | **Change in obesity status d (all years)** |\n| Obese to not obese | 36,029  |\n| Consistently not obese | 504,762  |\n| Consistently obese | 119,235  |\n| Not obese to obese | 27,273  |\n| **School-area poverty e** | **School-area poverty e** |\n| Low poverty | 62,238  |\n| Medium poverty | 119,219  |\n| High poverty | 89,407  |\n| Very high poverty | 78,510  |\n| **School size** | **School size** |\n| Attending small schools (<400 students) | 59,856  |\n| Attending nonsmall schools (≥400 students) | 289,525  |\n\n【49】a  N for missing place of birth = 72; N for missing area poverty = 7; N for missing or having >1 race/ethnicity = 177.  \nb  Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively, across 624 schools.  \nc  Based on change in change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \nd  Obesity status was defined according to Centers for Disease Control and Prevention growth chart–derived norms for sex and age (in months), based on a historical reference population, and used to compute the body mass index (BMI) percentile for each child. Obesity was defined as having a BMI ≥95th percentile for youths in the same sex and age (in months) group.  \ne  Based on percentage of households in the school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] area poverty) drawing from the American Community Survey 2007–2012 .\n\n【50】#####  Table 2. Mean Days Absent per Year Across Student- and School-Level Demographic and Fitness-Change Characteristics in New York City Public Middle School Students (N = 349,381) a  , 2006–2007 Through 2012–2013\n\n| Characteristic | Student-Level b , Mean (SD) | School-Level c , Mean (SD) |\n| --- | --- | --- |\n| **Sex** | **Sex** | **Sex** |\n| Male | 11.0 (.7) | 11.2 (.5) |\n| Female | 10.1 (.0) | 10.4 (.8) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Asian or Pacific Islander | 5.5 (.7) | 6.4 (.3) |\n| Hispanic | 12.6 (.9) | 13.3 (.2) |\n| Non-Hispanic black | 12.3 (.1) | 12.8 (.3) |\n| Non-Hispanic white | 10.0 (.7) | 10.7 (.2) |\n| **Language spoken at home** | **Language spoken at home** | **Language spoken at home** |\n| English | 11.9 (.1) | 12.0 (.9) |\n| Spanish | 10.9 (.1) | 11.0 (.9) |\n| Other language | 6.0 (.4) | 6.5 (.5) |\n| **Place of birth** | **Place of birth** | **Place of birth** |\n| United States | 11.3 (.1) | 11.7 (.5) |\n| Foreign country | 11.1 (.8) | 8.1 (.8) |\n| **Change in fitness (all years) d** | **Change in fitness (all years) d** | **Change in fitness (all years) d** |\n| \\>20% Increase | 10.3 (.2) | 11.0 (.6) |\n| 10%–20% Increase | 10.3 (.3) | 10.8 (.5) |\n| <10% Change | 10.7 (.9) | 11.8 (.6) |\n| 10%–20% Decrease | 11.1 (.2) | 11.6 (.4) |\n| \\>20% Decrease | 11.9 (.8) | 12.7 (.2) |\n| **Grade e** | **Grade e** | **Grade e** |\n| Grade 6 | 10.2 (.0) | 10.8 (.1) |\n| Grade 7 | 10.9 (.5) | 11.2 (.2) |\n| Grade 8 | 13.1 (.5) | 13.1 (.6) |\n| **School-area poverty f** | **School-area poverty f** | **School-area poverty f** |\n| Low poverty | 8.5 (.2) | 8.9 (.3) |\n| Medium poverty | 9.5 (.3) | 9.8 (.2) |\n| High poverty | 11.1 (.7) | 11.4 (.6) |\n| Very high poverty | 13.1 (.3) | 13.1 (.9) |\n| **School size** | **School size** | **School size** |\n| Small schools (<400 students) | 12.0 (.3) | 11.8 (.9) |\n| Non-small schools (≥400 students) | 10.3 (.1) | 11.8 (.0) |\n\n【52】a  N for missing place of birth = 72; N for missing area poverty = 7; N for missing or having >1 race/ethnicity = 177.  \nb  Student-level columns do not account for school clustering.  \nc  School-level columns account for school clustering.  \nd  Based on change in change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \ne  Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively.  \nf  Based on percentage of households in the school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] area poverty) drawing from the American Community Survey 2007–2012 .\n\n【53】#####  Table 3. Association of Fitness Change and Attendance in New York City Public Middle School Students a  , 2006–2007 Through 2012–2013\n\n| Fitness Change b | Unadjusted (Model 2) c , IRR d (% CI) | Adjusted (Model 3) c,e , IRR d (% CI) |\n| --- | --- | --- |\n| \\>20% Increase | 1.13 (.09–1.18) | 1.12 (.07–1.17) |\n| 10%–20% Increase | 1.08 (.03–1.14) | 1.06 (.01–1.11) |\n| <10% Change | 1.06 (.02–1.09) | 1.03 (.989–1.07) |\n| 10%–20% Decrease | 1.02 (.97–1.06) | 1.00 (.96–1.05) |\n| \\>20% Decrease | 1 \\[Reference\\] | 1 \\[Reference\\] |\n\n【55】Abbreviations: CI, confidence interval; IRR, incidence rate ratio.  \na  N = 349,381 students in 6th, 7th, and 8th grades; 675,318 observations across 624 schools.  \nb  Change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \nc  Based on 3-level longitudinal negative binomial mixed models.  \nd  All estimates, _P_ < .001.  \ne  Adjusted for sex, race/ethnicity, change in obesity status from the year prior, place of birth (United States or foreign country), school size, and school-area poverty, and including interactions grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and school-area poverty\\*race/ethnicity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "83b81dce-878f-4f7b-8ea9-c674d2b0994a", "title": "Prolonged Detection of Zika Virus in Vaginal Secretions and Whole Blood", "text": "【0】Prolonged Detection of Zika Virus in Vaginal Secretions and Whole Blood\nZika virus is a rapidly emerging mosquitoborne virus . In May 2015, Brazil reported autochthonous transmission of Zika virus . Over the course of 1 year, Zika virus spread to >50 countries and territories throughout the Americas . With the now confirmed link of Zika virus infection during pregnancy leading to fetal microcephaly  and reported cases transmitted by sexual contact , it is vital to understand the natural history of infection. We report an acute case of Zika virus infection in a traveler returning from Honduras to the United States and results from serial specimens collected for >11 weeks. These new data might serve as a potential guide for public health policy.\n\n【1】### The Study\n\n【2】This study was reviewed and approved by the Baylor College of Medicine Institutional Review Board (H-30533). A previously healthy, nonpregnant, 26 year-old non-Hispanic white woman returned to the United States from Tegucigalpa, Honduras, during mid-May 2016. Five days after her return (day 0), signs and symptoms consistent with Zika virus infection developed, beginning with rash and subsequent fever, headache, and conjunctivitis . Fever and rash continued through day 5 and day 6, respectively. By day 15, desquamation was noted on the palms of both hands and soles of both feet. By day 17, all symptoms had resolved.\n\n【3】Serial specimens were longitudinally collected for >11 weeks. The first specimens were collected on day 0, two hours after onset of rash and 2 h before development of fever. All remaining specimens were collected at 3, 8, 14, 21, 28, 35, 42, 53, 64, and 81 days after onset of illness. Specimens included serum, whole blood (EDTA anticoagulated), urine, saliva, and vaginal mucosa swabs. The patient was not menstruating when vaginal swab specimens were collected.\n\n【4】RNA was extracted from serum, whole blood, and urine samples by using the QIAamp MinElute Virus Spin Kit (QIAGEN, Valencia, CA, USA) according to the manufacturer’s instructions. Oral and vaginal mucosal swab specimens were collected by using the BBL CultureSwab Collection and Transport System (Becton Dickinson, Franklin Lakes, NJ, USA). Specimens were incubated in 250 μL of AL/carrier RNA lysis buffer for 10 min at room temperature; 200 μL of phosphate-buffered saline was added before RNA extraction.\n\n【5】Eluted RNA from all samples was tested in a quantitative reverse transcription quantitative PCR (qRT-PCR) that included a TaqMan Fast Virus 1-Step Master Mix (ThermoFisher Scientific, Foster City, CA, USA) and a TaqMan ZIKV 1107 assay  with appropriate positive and negative controls. We detected Zika virus RNA in serum up to day 8 after onset of illness and in body fluids up to day 14; whole blood samples remained positive up to day 81 . Results of qRT-PCR of saliva were negative after day 8, and results for urine and vaginal swab specimens did not become negative until after day 14. We tested a day 0 serum sample for dengue virus and chikungunya virus RNA by using TaqMan assays ; all results were negative.\n\n【6】Virus isolations were performed for Vero cells in complete Dulbecco’s modiﬁed Eagle’s medium containing 10% heat-inactivated fetal bovine serum. Cells were infected with day 0 serum samples (or mock-infected with cell culture medium) and observed for cytopathic effects. Cell culture supernatants were sampled 13 days after cell culture infection, and RNA was extracted and tested for Zika virus RNA. Supernatant was collected on day 14, and viral titer was 8.5 × 10 5  PFU/mL by plaque assay. Attempts to isolate virus from the day 64 erythrocyte fraction showed no evidence of cytopathic effects, and first and second passages were negative by qRT-PCR. Because the day 81 whole blood specimen was still positive by qRT-PCR, we used ficoll to separate peripheral blood mononuclear cells and erythrocytes and found that erythrocytes were the only fraction positive for Zika virus RNA. The partial sequence of the virus we isolated was submitted to GenBank under accession no. KX928077.\n\n【7】On day 8, plasma was evaluated by using an ELISA  to assess IgM and IgG binding to Zika virus envelope protein (Zika Virus Envelope Recombinant Protein, #R01635; Meridian Life Sciences, Memphis, TN, USA); positive results were obtained. Plasma-neutralizing antibodies against Zika virus were detected (% focus reduction neutralization test titer 1:1,438), but neutralization of dengue virus serotypes 1–4 was not detected. These findings indicated a robust Zika virus–specific humoral response.\n\n【8】### Conclusions\n\n【9】Given recent concerns regarding the ongoing epidemic of Zika virus disease, there is an urgent need to document the natural history of infection and assess transmission risk through nonvector routes. We had the unique opportunity to prospectively monitor the clinical and virologic course of Zika virus infection in a patient starting on day 0.\n\n【10】We detected viral shedding in vaginal secretions up to day 14. Only 1 human study reported Zika virus RNA in cervical mucous up to day 11 after onset of signs and symptoms . Zika virus (Asian lineage strain) RNA was detected in vaginal swab specimens obtained on days 1 and 7 postinfection of nonpregnant female rhesus macaques . Zika virus replication was also detected in vaginal mucosa of mice .\n\n【11】We could not determine whether positive results by qRT-PCR indicated replicating virus. With the recent finding of possible female-to-male virus transmission , infectious virus might be present in the vaginal canal and could serve as a risk for sexual or intrapartum transmission.\n\n【12】We detected viral RNA in serum up to 8 days and in whole blood up to 81 days after onset of illness. Diagnosis of infection currently relies mostly on PCR detection of Zika virus in serum. With concerns for Zika virus infection during pregnancy, screening of whole blood might be more sensitive in identifying infected patients, particularly if an asymptomatic patient has traveled from an area where exposure is a concern, had high-risk sexual contact, or is convalescing and PCR for a serum sample would probably yield a negative result.\n\n【13】Our observation is further supported by another recent study that found whole blood samples positive for Zika virus by PCR up to 2 months postinfection . In our study, we confirmed that a positive result was attributed to the erythrocyte component of whole blood, similar to what has been found in studies of West Nile virus . One study found that West Nile virus adheres to erythrocytes and could infect Vero cells . Although we did not observe infectious virus associated with erythrocyte positivity for Zika virus at day 64, this finding is still of concern and requires further investigation. Because the last whole blood sample collected on day 81 was positive for Zika virus RNA, follow-up testing will continue to define the longevity of viremia in whole blood.\n\n【14】In conclusion, this case study advances understanding of the natural history of Zika virus. It provides new findings, including detection of Zika virus RNA in vaginal secretions up to day 14 and in erythrocytes up to day 81, the longest reported duration of detection in this sample type. A desquamating rash developed on the hands and feet of the patient, which we presume was related to her infection. To our knowledge, this finding has not been previously described.\n\n【15】Additional studies involving larger cohorts of acutely ill Zika virus–infected patients tested over a longer period would solidify our understanding of the natural history of infection, duration of viral detection, and clinical outcomes. These studies will enable further development of evidence-based policies regarding diagnosis and clinical management of Zika virus–infected patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ac593108-646e-43a6-a3d6-b1a755d7c625", "title": "Mycobacterium ulcerans Disease, Peru", "text": "【0】Mycobacterium ulcerans Disease, Peru\nBuruli ulcer (BU), a chronic ulcerative disease, has been observed in many tropical areas, but patients have usually come from Africa and Australia . Cases were also described in the Americas, mostly in French Guiana . A few cases from Surinam have also been recorded in French Guiana, and 8 cases have been reported in Mexico since 1953 .\n\n【1】In 1969, the first 2 cases from Peru were reported . A new case was reported in 1988, along with a redescription of the first 2 cases . From 1996 through 2005, 8 additional cases, which we describe here, were found in Peru .\n\n【2】### Material and Methods\n\n【3】We conducted a descriptive, retrospective survey of patients seen by members of the Instituto de Medicina Tropical Alexander von Humboldt in Lima and Iquitos. We requested referrals of new patients from other areas. The total of 8 cases occurred from 1996 through 2005.\n\n【4】##### Patients\n\n【5】We used a collective data sheet proposed by the World Health Organization (WHO)  to assess the magnitude and severity of the disease and to collect data from patients. We also obtained information from medical records. Patients who had a history of chronic ulcer and who had at least 2 different positive laboratory tests were included in this study . All the patients gave consent verbally. The publication was approved by the Human Protections Administration Office for Human Research Protection of the Universidad Peruana Cayetano Heredia (SIDISI code 52467).\n\n【6】##### Smears and Tissue Collection\n\n【7】Smears obtained by scalpel or swab were prepared with material from the necrotic base and undermined edges of the lesions and were stained with Ziehl-Neelsen. Skin biopsy samples were excised and cut into \\> 2 portions. At least 1 portion was fixed in 10% formalin and processed for histologic examination at the pathology laboratory of the Hospital Nacional Cayetano Heredia in Lima. The other portions, minced further, were inoculated after decontamination onto Löwenstein-Jensen medium in Lima as described previously ; the rest was placed in a semisolid transport medium  and sent to the Institute of Tropical Medicine, Antwerp, for culture and PCR testing .\n\n【8】### Results\n\n【9】We found 8 cases of BU from 1996 to 2006. Case characteristics are indicated in Table 1 . Five patients came from the Peruvian rainforest, the likely place of infection. Two patients reported close contact with water in the Marañón (patient 1) and Huallaga (patient 2) River basins. Three patients (patients 3, 4, and 6) lived close to Iquitos, a city on the Amazon River. One patient (patient 5) had briefly visited a swampy area in the north coast of Peru (Tumbes), and 2 other patients (patients 7 and 8) lived in the same area. All of the areas described are warm and humid. The age range at diagnosis was 18–58 years, with a male to female ratio of 3:5. No case patients had a medical or family history of tuberculosis or leprosy. The time between onset of illness and being seen by a physician was 1–8 months. Four patients noticed a nodular lesion before ulceration occurred. All patients had ulcers with typical undermined edges. The site of involvement in our patients was on the extremities, but 1 patient had gluteal lesions. The median number of lesions per patient was 2. Three patients (patients 1, 2, and 5) were lost to follow-up at an early stage, soon after diagnosis, and their disease course was unknown.\n\n【10】Patient 3 had lesions on both knees; he liked gardening and often knelt on soil and organic mulch that contained wood shavings. He first received rifampin and ethambutol for 5 weeks. Drug therapy was stopped because of hepatotoxicity. Trimethoprim-sulfamethoxazole and ciprofloxacin were then administered for 15 days. After completing oral therapy, he treated his lesions with a rifampin spray. Eight months after the start of drug therapy, the lesions were almost closed. A small ulcer remained on the right knee. The patient showed complete remission of lesions without any surgical intervention on his last control visits, 3 and 5 years after diagnosis.\n\n【11】Patient 4 had a medical history suggestive of leishmaniasis, and a smear from an ulcerated lesion was reported as positive for _Leishmania_ . She was treated with intramuscular and intralesional sodium stibogluconate, including multiple inoculation sites. One month after receiving a course of 29 intramuscular injections, she showed more lesions in the left gluteal region and was treated with herbal medicine. BU was diagnosed from purulent material removed with a syringe and needle from a closed lesion. After drainage and biopsy her lesions showed improvement, and therefore no other treatment was instituted. She continued herbal treatment until total cure. The lesions have remained inactive for 5 years.\n\n【12】Patient 6 was treated by excision surgery and later received antituberculosis treatment (regimen 1) for 6 months. Patient 7 was pregnant when first seen. After tissue specimens were taken and BU was diagnosed, she was treated conservatively with topical disinfectants until delivery. She then received the WHO-recommended rifampin and streptomycin treatment  for 31 days and had excision surgery of the largest lesions. Later, all lesions were excised. Patient 8 had lesions on the right middle finger  that ulcerated after treatment for 1 month with ciprofloxacin, clindamycin, and dexametasone . Diagnosis was made on the basis of material obtained at the first extensive debridement . He received 5 weeks of regimen 1 treatment for tuberculosis, to which streptomycin was added for the last 3 weeks before surgical debridement and autologous skin graft. Antituberculous regimen 1 was continued for 2 more weeks. Figure 2 , **panel D** shows the lesion 1 month after surgery. The patient then received 4 months of treatment with minocycline, ciprofloxacin, and trimethoprim-sulfamethoxazole and undertook rehabilitation including exercises. He recovered very good use of his right hand.\n\n【13】Patients 6, 7, and 8 were cured with antimicrobial agents and surgery. They had no recurrence after 2 years of follow-up.\n\n【14】As indicated in Table 2 , all patients’ cases were confirmed by at least 2 positive laboratory tests. Seven patients showed acid-fast bacilli (AFB) on the initial smear, and 1 was negative (patient 5). The biopsy specimens from all the patients had AFB in histopathologic sections and typical histologic lesions, i.e. necrosis of fat and an abundance of extracellular clumps of AFB. Most biopsy specimens showed little or no inflammatory infiltrate. A granulomatous infiltrate was seen in the biopsy specimen from patient 5. Cultures remained negative, and IS _2404_ PCR was positive for all 7 patients tested.\n\n【15】### Discussion\n\n【16】From 1969 until 2007, only 11 cases of BU have been reported in Peru, but no countrywide survey has been conducted to evaluate its true prevalence in Peru. BU is probably both infrequent and underreported in Peru and may often be misdiagnosed as leishmaniasis, which is more prevalent and better known. Three separate surveys suggest the rarity of BU. In the first, Saldaña-Patiño reviewed 1,620 ulcers biopsied from 1969 to 1981 and found no other BU patients apart from the 3 he reported . Second, a preliminary epidemiologic survey was conducted in the general area of the Huallaga Basin close to Tarapoto; several leishmaniasis and vascular lesions were found in 4 communities of ≈4,000 inhabitants, but no BU cases were seen (N. Donaires, MD thesis). Finally, physicians performing populationwide cysticercosis research in Tumbes (on the north coast) included a questionnaire and physical examination of all skin ulcers at the time, using as a guide a booklet in Spanish that was provided to familiarize them with BU . No skin ulcers were seen in the population surveyed.\n\n【17】The scarcity of BU cases in Peru and Mexico may be due to a lower virulence of the mycobacteria and a better immune response of patients when they become infected by _M. ulcerans_ . It is clearly not related to infrequent contact with contaminated water. As in Africa, populations living in the Amazon River Basin have frequent contacts with water for domestic activities. Similarly, the low incidence of BU in Peru does not seem to be related to the absence of _M. ulcerans_ in the environment, since the IS _2404_ PCR positivity of the environmental specimens from Peru (collected in Tarapoto, in the Huallaga River Basin) and Benin have given comparable results (% positivity in Peru vs. 10%–20% positivity in Benin) (F. Portaels et al. unpub. data).\n\n【18】The distribution of BU in Peru and elsewhere is strongly associated with wetlands, especially those with slow-flowing or stagnant water (e.g. ponds, backwaters, and swamps) . All of our patients had contacts with swampy areas in the Amazon River Basin (patients) or on the northern coast (patients). The 3 previously reported patients  had been in contact with water bodies related to tributaries of the Amazon River. In Peru, therefore, BU is present in the Peruvian jungle  and other swampy regions of the north coast.\n\n【19】In our study, the age of patients when they were first seen with BU ranged from 18 to 58 years. _M. ulcerans_ is seen mainly in children and young adults in other BU-endemic regions but may affect any age group . All patients except patient 8 had lesions on the lower limb . A similar pattern has been reported in other countries .\n\n【20】Patients sought medical assistance ≈1 month after the first lesion appeared. The longest interval to final diagnosis was 8 months, which led to a very large lesion in patient 4, who was originally being treated for leishmaniasis.\n\n【21】In Africa, the stigma associated with BU appears to be important, and its mysterious nature is often attributed to witchcraft and curses . Such concerns were not voiced by any of our patients, as they all had actively sought medical help.\n\n【22】Beside patients lost to follow-up, the clinical outcome of all patients from Peru was favorable. One patient (patient 4) was cured with herbal medicine only. Several authors report that while some topical treatments may heal BU lesions , other lesions may heal spontaneously . Patient 4 may have healed spontaneously or because of herbal medicine. Surgical treatment alone, which was until recently the mainstay of clinical management of BU in BU-endemic areas  is not practiced in Peru. Surgery is always associated with antimycobacterial drug therapy. Several centers in Africa have started to treat patients with streptomycin and rifampin according to WHO guidelines , and a recent study indicates that after 8 weeks of drug therapy ulcers may heal without surgery . The patients with reported infections in Peru up to 1988 had a favorable response to antituberculous therapy, although their lesions were large . In our study, patient 3 was also successfully treated with drug therapy without surgery. The success of antimycobacterial therapy in some areas may be correlated with a lower virulence of the _M. ulcerans_ strains and in particular with lower production of mycolactones. African strains, which produce the greatest number and quantity of mycolactones, are associated with more severe disease forms , which may explain the difficulty treating some patients with only antimycobacterial drugs.\n\n【23】All patients in our study were PCR positive, but we were unable to cultivate _M. ulcerans_ from the clinical specimens in Lima or Antwerp. This is surprising since the procedures used to cultivate _M. ulcerans_ in primary culture were identical to those used for thousands of specimens from patients from other parts of the world (which yielded 45% of positive primary cultures) . Peruvian _M. ulcerans_ strains may have different growth requirements or may be more sensitive to the antimicrobial agents in semisolid transport medium (PANTA: polymixin B, amphothericin B, nalidixic acid, trimethoprim, and azlocillin) than those from other geographic locations.\n\n【24】In conclusion, our study confirms that, although infrequently diagnosed, BU is an endemic disease in tropical swampy areas of Peru. Proper diagnosis and treatment require inclusion of simple clinical and laboratory guidelines in tuberculosis, leprosy, and leishmaniasis control programs, which reach health workers at all levels. Known BU-endemic areas should receive special emphasis. Education of populations and training of health workers are first needed to evaluate and understand the full extent of this disease in Peru.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d527d573-40ed-4bdf-986e-e7d0a4b38a87", "title": "Determining Infected Aortic Aneurysm Treatment Using Focused Detection of Helicobacter cinaedi", "text": "【0】Determining Infected Aortic Aneurysm Treatment Using Focused Detection of Helicobacter cinaedi\nInfected aortic aneurysms account for 0.7%–3% of all aortic aneurysms and are associated with a 26%–44% mortality rate . No consensus exists about appropriate antimicrobial therapy and treatment duration for infected aneurysms. Moreover, determining surgical treatment in each case requires carefully considering the causative bacterium and the patient’s medical background . Recently, several cases of infected aortic aneurysms caused by _Helicobacter cinaedi_ , a rare, difficult-to-detect causative bacterium have been reported .\n\n【1】First identified in 1984, _H. cinaedi_ , a gram-negative rod with spiral morphology and bipolar flagella, is indigenous to the intestinal tract of humans and other animals . This bacterium produces a cytolethal distending toxin that invades epithelial cells  and is associated with bacteremia in compromised hosts and infected aortic aneurysms, mediated by bacterial translocation from the intestinal mucosa . Because of the high recurrence rate for _H. cinaedi_ bacteremia, it is recommended that patients receive prolonged treatment of at least 3 months with appropriate antimicrobial drugs . We sought to determine the efficacy of treatment for infected aortic aneurysms through the focused detection of _H. cinaedi_ .\n\n【2】### The Study\n\n【3】During September 2017–January 2021, we treated 10 patients with infected aortic aneurysms from a single center in Aichi, Japan. Diagnosis, including for recurrent aneurysms, was based on either positive culture or PCR of aortic tissue resected at the time of surgery or positive blood or puncture culture of an abscess caused by a hematogenous infection in patients who did not undergo open surgery and had clinical findings localized to the aortic aneurysm.\n\n【4】We started patients on antimicrobial therapy with meropenem when _H. cinaedi_ was suspected or gram-negative rods were identified, then changed to sulbactam/ampicillin after confirming drug sensitivity. After 4 weeks of treatment or a negative inflammatory reaction, the antimicrobial treatment was switched to minocycline or amoxicillin/clavulanate for 3–6 months. If other causative bacteria were identified, antimicrobial drugs were changed based on drug sensitivity results and continued for 3–6 months. Rifampin-soaked graft replacement was the first-choice surgical treatment, irrespective of causative bacterium.\n\n【5】Among the 10 patients with infected aortic aneurysms, _H. cinaedi_ was the causative bacterium in 4, _Staphylococcus aureus_ in 3, _Salmonella enterica_ serovar Enteritidis in 1, and _Enterobacter cloacae_ in 1. Treatment included vascular replacement in 7 patients (with _H. cinaedi_ ), endovascular stent grafting in 1 (with _H. cinaedi_ ), and medical treatment in 2 patients (with _H. cinaedi_ ). One patient with aortic rupture and _Salmonella_ Enteritidis infection died postoperatively from multiorgan failure; the other 9 patients had good courses of recovery without recurrence .\n\n【6】All 4 of the _H. cinaedi_ –infected case-patients were immunocompromised (diabetes mellitus, chronic kidney disease, cancer). However, their clinical findings were mild, and C-reactive protein levels tended to be low (H. cinaedi_ /non– _H. cinaedi_ median 4.3/21.6 mg/dL) at hospital admission. All patients had saccular aneurysms and severely calcified aortas. Surgical findings showed highly adherent areas around the aneurysms with intimal defects but without abscess formation. Pathological examination revealed severe lymphocytic infiltration in the aneurysmal wall with loss of elastic fibers. In contrast, 3 of 5 patients in the non– _H. cinaedi_ group showed abscesses and hematomas around the infections .\n\n【7】We performed blood cultures using the BacT/Alert system  and grew microaerobic cultures in the presence of hydrogen using a commercial hydrogen generator . In the _H. cinaedi_ group, we used multilocus sequence typing (MLST) to identify the subtype, and we immunostained aortic tissues with antiserum against the whole-cell lysate of _H. cinaedi_ raised in rabbits  .\n\n【8】Among the _H. cinaedi_ patients, we were able to subculture isolates from 3; the range of blood culture growth times, 75.3–160.8 h (median 90.6 h), was longer than that among the non– _H. cinaedi_ patients, 12.3–28.3 h (median 12.3 h). Drug susceptibility testing demonstrated levofloxacin-resistant _H. cinaedi_ , although the sequence type on MLST was different in each case .\n\n【9】### Conclusions\n\n【10】Infected aortic aneurysms caused by _H. cinaedi_ are increasingly being recognized, especially in Japan, although the detection rate remains low . One study reported 734 cases of infected aortic aneurysms caused by various organisms, including _Salmonella_ , _Staphylococcus_ , _Streptococcus_ , and _Escherichia coli_ , but not _H. cinaedi_ . Infected aortic aneurysms are a critical disease with high mortality, and whereas identifying the causative bacteria is effective in determining treatment, 23.3%–25% of cases are caused by unidentified bacteria . In our study, the detection rate for _H. cinaedi_ in infected aortic aneurysms was high, 40%, although the absolute number of cases was small. Of note, infections caused by _H. cinaedi_ all showed good clinical courses. _H. cinaedi_ is known to cause nosocomial infections; however, nosocomial infections were ruled out as mode of infection here because the 3 isolates that underwent MLST had different sequence types .\n\n【11】The Bactec FX system , widely used for blood culturing of _H. cinaedi_ , is generally considered to be more sensitive than the bioMérieux BacT/Alert system . Nevertheless, by assuming _H. cinaedi_ was the causative bacterium of the infected aortic aneurysms for our patients and simply allowing a longer incubation period of 10 days versus the usual 5 days, the BacT/Alert system detected the bacterium in 3/4 cases, comparable to the Bactec FX system . It is sometimes difficult to grow bacterial subcultures in microaerophilic conditions; however, adding 5%–10% hydrogen effectively helps form characteristic thin-spread colonies . PCR reliably detects and identifies species, but matrix-assisted laser desorption/ionization time-of-flight mass spectrometry is also useful , although unfortunately this testing method results in a time lag between initiating treatment on and identifying bacteria.\n\n【12】_H. cinaedi_ infects atherosclerotic sites, leading to the progression of atherosclerosis through lipid accumulation . Progression is slow, often taking months, and leads to no clinical findings even if a local infection is established . Considering the slow, localized progression and difficulty of detection of atherosclerosis associated with _H. cinaedi_ infection, aortic aneurysms thought to be noninfectious might actually be infected by _H. cinaedi_ . Clarifying the relationship between this bacterium and atherosclerotic diseases might lead to additional treatments.\n\n【13】No infected aortic aneurysms caused by _H. cinaedi_ have resulted in rupture, and cases usually pass without recurrence following an appropriate period of antibiotic treatment. Because nonsurgical treatment has been shown to be effective, open surgery during the acute phase of infection might be overindicated. Endovascular stent grafting, a less invasive treatment than the standard vascular replacement procedure, followed by use of appropriate antimicrobial agents might successfully complete treatment . This treatment is beneficial among aging patients, and it is hoped that the presence of these bacteria in infected aortic aneurysms will be widely recognized, with treatment and diagnosis proceeding simultaneously.\n\n【14】In summary, although _H. cinaedi_ is a relatively rare cause of infected aortic aneurysms, it might be overlooked because of its low initial detection rate. Detection during treatment initiation can improve patient life expectancy by enabling effective antimicrobial therapy and expanding treatment options.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3e9aaa25-84f9-40fc-8c2a-dfcfdb1f4ff2", "title": "Tracking Senior Fall and Fall-Related Injury EMS Calls to Target Fall Prevention Programs, Salt Lake County, Utah", "text": "【0】Tracking Senior Fall and Fall-Related Injury EMS Calls to Target Fall Prevention Programs, Salt Lake County, Utah\nBackground\n----------\n\n【1】Every year in the United States, more than 25% of people aged 65 or older fall at least once , and these injuries are associated with high rates of illness and death . Physiological age-related changes such as reduction of sight, hearing, and muscle strength are major causes of falls among older people . Among Salt Lake County residents aged 65 or older, in 2014 the fall injury emergency department encounter rate was 458.6 per 10,000  and the fall injury hospitalization rate was 130.0 per 10,000 , and in 2016 the fall mortality rate was 5.4 per 10,000 .\n\n【2】As part of ongoing fall prevention activities, the Salt Lake County Health Department hosts the evidence-based Stepping On program for seniors. To better target this and other fall prevention programs to reduce rates of fall-related illness and death, we mapped dispatched emergency medical services (EMS) calls for falls and fall-related injuries among adults aged 65 years or older and identified areas with high prevalence.\n\n【3】Methods\n-------\n\n【4】We extracted data on EMS calls from the Utah prehospital reporting system that had an incident address in Salt Lake County; a date of incident from January 1, 2014, through April 30, 2017; and a dispatch report of 1) a fall or 2) an unconscious/fainting or unknown problem/person down. Those with a dispatch report of a fall were assumed to be correctly classified as fall-related. Narratives of those with a dispatch report of an unconscious/fainting or unknown problem/person down were searched by using SAS version 9.4 (SAS Institute, Inc) for terms suggesting evidence of a fall, such as “fall,” “fell,” or “GLF” (ground level fall). Calls were excluded if the narrative included terms indicating no fall, such as “no fall,” “negative fall,” “not sustain a fall,” “denies (any) fall,” or “not suffer a GLF.”\n\n【5】The final data set included 14,824 fall-related injuries. Of those, 93% could be geocoded (% of those geocoded with match score ≥90) and aggregated to the census tract level by using ArcGIS Pro 2.0 (Esri). Crude incidence rates were calculated by using American Community Survey 5-year population estimates for adults aged 65 years or older from 2014 through 2016, and mapped by census tract. Both counts and rates were classified by using equal intervals. Fall injury points were overlaid on census tract rates, and Google Maps (Google LLC) was used to explore neighborhoods with high counts or crude rates to identify facilities where fall prevention activities may be targeted.\n\n【6】Institutional review board (IRB) approval was obtained for the EMS data set from the Utah Department of Health IRB Committee.\n\n【7】Findings\n--------\n\n【8】Fall injury counts among adults aged 65 years or older were highest in census tracts in southeast and southwest Salt Lake County. Fall injury rates among adults aged 65 years or older were highest in census tracts in north-central and southeast Salt Lake County. Seven facilities were identified as locations of falls in these high-count or high-rate areas; all were mixed-level senior living residences (eg, independent living, assisted living, memory care). One census tract with a high rate did not have a senior living facility; most of these falls occurred in individual homes because of safety hazards. All census tracts with a high count had at least one senior living facility in which most falls occurred.\n\n【9】Action\n------\n\n【10】Results were used by community partners to secure pilot funding for the Otago Exercise Program, and they are currently being used to target Stepping On and Otago programs, collaborate with Salt Lake County Aging and Adult Services’ Meals on Wheels program to better reach the senior population vulnerable to falls, develop one-on-one prevention programs at sites with a high prevalence of falls, implement collaborative fall prevention programs with EMS community paramedicine programs, and evaluate program interventions. Interventions target individuals and include easily modifiable risk factors such as muscle strengthening and balance retraining exercises, medication review, vision and hearing checks, and improving safety around the home. However, public health would do well to partner with other sectors, such as city planning, to improve the built environment for seniors.\n\n【11】Geospatial data can be challenging to interpret, and various analyses and visualizations should be assessed together for the most accurate picture. Assessing rates without also examining counts may lead to inappropriate resource allocation because of the size of the population aged 65 or older in certain census tracts. For example, one census tract had a high rate but low count because the population aged 65 years or older in this tract was small. Similarly, census tracts with a low rate and high count indicate areas where the population aged 65 or older is large. Resources allocated to high-rate/low-count tracts may have a lesser impact in reducing the burden of falls than resources allocated to low-rate/high-count tracts. Ultimately, program managers found count data most useful for targeting resources to locations. In the future, it would be useful to compare locations by calculating rates by facility.\n\n【12】Limitations of this project include incomplete 2017 prehospital data resulting from a reporting delay, missing 2015–2016 prehospital data from one EMS agency because of data submission issues, and potential misclassification of incidents as fall-related or not fall-related.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e58f1e75-28e5-4901-8a8f-1e4c57db7559", "title": "Measles and Secondary Hemophagocytic Lymphohistiocytosis", "text": "【0】Measles and Secondary Hemophagocytic Lymphohistiocytosis\n**To the Editor:** We found interesting the article by Lupo et al. about a case of fatal measles in an immunocompetent 29-year-old woman . Perhaps, however, the possible diagnosis of secondary hemophagocytic lymphohistiocytosis (HLH) should also have been considered in that setting.\n\n【1】HLH is a potentially fatal hyperinflammatory syndrome characterized by histiocyte proliferation and hemophagocytosis. HLH may be inherited (i.e. primary, familial, generally occurring in infants) or may occur at any age secondary to infection, malignancy, or rheumatologic disease. Secondary HLH is determined according to clinical criteria from the HLH Study Group of the Histiocyte Society, which require \\> 5 of the following for a diagnosis: fever; splenomegaly; cytopenia (affecting \\> 2 cell lineages); hypertriglyceridemia or hypofibrinogenemia; hemophagocytosis in the bone marrow, spleen, or lymph nodes; low or absent natural killer cell cytotoxicity; hyperferritinemia; and elevated levels of soluble CD25.\n\n【2】We conducted a PubMed search and found 5 articles that described 6 cases of HLH in patients with measles . Pneumonia was described in all of them , and central nervous system involvement was described in 3 . Four cases occurred in children, 3 of them immunocompetent . The 2 adults were an immunocompetent 18-year-old man who had acute respiratory distress  and a 19-year-old man with acute lymphocytic leukemia who had measles pneumonia and acute hemorrhagic leukoencephalitis . The only fatal case occurred in an immunocompromised 8-year-old boy with giant-cell pneumonia .\n\n【3】The identification of hemophagocytosis in bone marrow aspirate represents only 1 of the 5–8 criteria needed for a diagnosis of HLH; conversely, a bone marrow aspirate lacking hemophagocytosis does not rule out the diagnosis of HLH. Still, we believe HLH should be considered for any patient with fever and pancytopenia, especially in the presence of respiratory distress or multiorgan dysfunction. An appropriate therapy could save the patient .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "84b66eba-1526-404b-9667-f0ef96f3a104", "title": "Feast of Sacrifice and Orf, Milan, Italy, 2015–2018", "text": "【0】Feast of Sacrifice and Orf, Milan, Italy, 2015–2018\nOrf (ecthyma contagiosum) is an infection of the skin caused by a DNA virus of the genus _Parapoxvirus_ , family _Poxviridae_ . Skin lesions (e.g. vesicles, blisters, pustules, erosions, ulcers, papules, nodules) occur at sites of inoculation of the virus 3–15 days after infection. Hands are usually affected . The differential diagnosis for orf includes milker’s nodule, anthrax, tularemia, fish tank granuloma, cutaneous leishmaniasis, pyogenic granuloma, and keratoacanthoma . The disease spontaneously heals within 6 weeks, although pain, bacterial superinfections, and regional lymphadenitis are possible . Treatment is based on topical antiseptics .\n\n【1】Orf virus usually infects sheep and goats. Humans are infected by handling infected meat from these animals; orf is considered an occupational disease in shepherds, shearers, veterinarians, butchers, and cooks . In the last few years, orf has occurred after the Muslim Feast of Sacrifice (Eid al-Adha) . In 2014, we reported a case of orf that appeared after sheep slaughtering for this feast . During 2015–2018, we observed 7 additional cases in Muslim men 18–61 years of age who were of Moroccan, Tunisian, or Egyptian origin. They had been infected 2–3 weeks after lamb slaughtering for the feast . One patient was a butcher. In all patients, 1 hand and/or fingers were involved. In 4 patients, orf presented with erythematous pustules; 3 of these patients had ulcerated nodules. In all patients, clinical diagnosis was confirmed by histopathologic examinations.\n\n【2】Orf acquired during the Feast of Sacrifice was reported in 1982 in Turkey . Other cases were subsequently reported in France , Belgium , Italy , Turkey , and the United States . Epidemics also were reported: in Belgium, 23 cases in 2000 and 44 cases in 2001 , and in Turkey, 9 cases in 2005 and 29 cases in 2009 .\n\n【3】In most reported cases, orf appeared days or weeks after the Feast of Sacrifice. This feast is celebrated 2 months and 10 days after the end of Ramadan; the exact date varies . During the feast, many Muslim families kill a lamb, which has to be bled alive . Only men may kill the lamb. Orf occurs more often in men; however, it occurs also in women, who often handle the infected meat with bare hands during preparation and cooking.\n\n【4】In conclusion, in the metropolitan area of Milan, where ≈250,000 Muslims reside, we recently observed 7 patients with orf acquired during the Feast of Sacrifice. Prevention measures are difficult. For example, most of the Muslims living in the metropolitan area of Milan are Moroccans, who travel to Italy by car, carrying the infected meat; thus, no prevention measures have been taken.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "674293e7-1ec7-4725-a2be-f14131190df9", "title": "Porcine Deltacoronavirus Infection and Transmission in Poultry, United States", "text": "【0】Porcine Deltacoronavirus Infection and Transmission in Poultry, United States\nCoronaviruses (CoVs) cause respiratory and gastrointestinal disease in humans, poultry, swine, and cattle. CoVs (family _Nidovirales_ , subfamily _Coronaviridae_ , subfamily _Coronavirinae_ ) are composed of 4 genera, _Alphacoronavirus_ , _Betacoronavirus_ , _Gammacoronavirus_ , and _Deltacoronavirus_ . Viruses from each CoV genus have been detected in diverse host species, but gammacoronaviruses and deltacoronaviruses (DCoVs) have been isolated primarily in birds . Two members of the _Alphacoronavirus_ genus, transmissible gastroenteritis coronavirus (TGEV) and porcine epidemic diarrhea virus (PEDV), cause severe diarrhea in swine. TGEV and PEDV infections have caused severe economic losses in many countries, including the United States . The _Betacoronavirus_ genus includes the notable human pathogens OC43, HKU1, severe acute respiratory syndrome (SARS) CoV, and Middle East respiratory syndrome (MERS) CoV, which mostly cause respiratory symptoms . _Gammacoronavirus_ includes avian enteric coronavirus and infectious bronchitis virus that mainly infect avian species . DCoVs previously were identified primarily in multiple songbird species and in leopard cats (Prionailurus bengalensis_ ) .\n\n【1】Porcine deltacoronavirus (PDCoV) was initially detected in 2009 in fecal samples from pigs in Asia, but its etiologic role was not identified until 2014, when it caused diarrhea in pigs in the United States . The origin of PDCoV is unknown, but because of the widespread prevalence of DCoV in songbird species and genomic similarities, researchers suspect that PDCoV may have originated from an ancestral avian DCoV .\n\n【2】In experiments, PDCoV caused diarrhea and gut lesions in infected piglets . It was detected in pigs in Hong Kong in 2012  and the United States in February 2014 . Although not considered as deadly to pigs as PEDV , PDCoV continues to circulate and cause illness in swine herds worldwide. How PDCoV emerged in swine and how it spreads remain unknown.\n\n【3】In vitro studies have shown that PDCoV utilizes a conserved region of the protein aminopeptidase N (APN) gene to infect cell lines derived from multiple species, including humans, pigs, and chickens . As a potentially emerging zoonotic pathogen, studies of PDCoV prevalence in humans and its contribution to human disease are ongoing. The ability of PDCoV to infect cells of multiple species and cause illness and death in pigs makes it a priority pathogen that should be studied further.\n\n【4】In vivo studies could validate cell culture susceptibility findings and determine whether PDCoV causes infection and disease in species other than pigs. Virus cross-species transmission among hosts plays a major role in the evolution and diversification of viruses, appearing in many instances to be preferential to coevolving within an initial host . CoVs already have demonstrated a propensity for crossing species barriers, both in animal-to-animal spread and animal-to-human spread. Initial evidence of CoVs jumping from mammalian to avian species was reported from bovine CoV infecting turkeys but not chickens . As a zoonotic CoV transmission, SARS-CoV is believed to have jumped from bats or palm civets (Paguma larvata_ ) to humans in 2002, causing 8,098 cases in 37 countries and 774 deaths . MERS-CoV emerged more recently, jumping from dromedary camels (Camelus dromedarius_ ) to humans, and has caused 1,879 cases of respiratory illness in humans and 666 deaths .\n\n【5】Understanding how cross-species transmission of CoVs occurs is critical to our ability to predict which viruses might be on the verge of SARS- or MERS-like pandemics. In addition, studies of CoV cross-species transmission can inform development of novel therapeutics and strategies to combat CoVs in susceptible animal hosts before they pose an imminent human health threat. We conducted experiments to determine the prevalence of PDCoV infection in and transmissibility among poultry.\n\n【6】### Materials and Methods\n\n【7】##### Animals\n\n【8】We obtained 25 fourteen-day-old chickens (Gallus gallus domesticus_ ) and 25 fourteen-day-old turkey poults (Meleagris gallopavo_ ) from the specific pathogen–free flock of the Ohio Agricultural Research and Development Center of The Ohio State University (Wooster, Ohio, USA). This flock has no prior exposure to swine or to PDCoV, PEDV, or TGEV. After acclimating in Biosafety Level 2 (BSL-2) facilities for 1 day, all birds appeared healthy with no evidence of diarrhea or other clinical signs. Animal protocols used in this study were approved by the Institutional Laboratory Animal Care and Use Committee of The Ohio State University.\n\n【9】##### Titration of PDCoV\n\n【10】We determined PDCoV titers from intestinal contents of pigs by 50% tissue culture infectious dose (TCID 50  ) assay . We seeded LLC porcine kidney (LLC-PK) cells at 5 × 10 4  cells/well in a 96-well plate . We washed 100% confluent monolayers once with 200 µL of maintenance media (MM): minimal essential media (MEM), 4-(hydroxyethyl)-1-piperazineethanesulfonic acid (HEPES), GlutaMAX  consisting of MEM with 1% antibiotic-antimycotic solution, 1% nonessential amino acids, and 1% HEPES. We inoculated 100 µL from 10-fold dilutions of PDCoV in 8 replicates per dilution. Each plate included 1 row of negative control MM only with 5 µg/mL of Trypsin . After absorption for 1 h, we added another 100 µL of MM with 5 µg/mL of Trypsin to each well. We monitored cytopathic effects for 3–7 days, calculated virus titers after immunofluorescent (IF) staining by using the Reed-Muench method , and expressed results as log 10  TCID 50  /mL .\n\n【11】##### Study Design\n\n【12】Birds were floor housed in a temperature-controlled BSL-2 containment room with wood litter shavings and provided ad libitum access to food and water. In consecutive experiments of poults and chicks, we randomly divided the flock of 25 birds into 2 groups, 15 uninfected and 10 infected birds. Each group was housed separately and inoculated through the choanal cleft. The uninfected group was inoculated with 200 µL of unfiltered, undiluted small intestine contents (SIC) from an uninfected gnotobiotic pig (GP-8). The infected group was inoculated with SIC from a PDCoV-infected pig (DC175) with 6.87 log 10  TCID 50  /mL. One poult in the uninfected group died of unknown causes unrelated to known pathogens before inoculation.\n\n【13】After inoculation, we observed chicks and poults for clinical signs 2 times each day. We scored fecal consistency as follows: 0, solid; 1, pasty; 2, semiliquid; and 3, liquid . We considered a fecal consistency score of \\> 2 as diarrhea. At 2 days postinoculation (dpi), we randomly assigned 5 birds from each uninfected group as sentinels and allowed them to comingle with the infected group for the duration of the experiment. We recorded body weights and collected cloacal swab, tracheal swab, and serum samples at 2, 4, 7, 9, 11, and 14 dpi. Except for sentinel birds, we euthanized 2 chicks and 2 poults from each group at 3 and 7 dpi for blood and tissue collection. We concluded the study at 14 dpi and euthanized the remaining 33 birds, including sentinels, for blood and tissue collection.\n\n【14】##### Serum Antibody Detection\n\n【15】We modified and optimized an ELISA  to detect PDCoV-specific IgY antibodies in serum from PDCoV-inoculated chicks and poults. We added 100 µL of HRP-Conjugated Streptavidin (ThermoFisher) to each well at a dilution of 1:5,000 and incubated at 37°C for 1 h. We washed wells with phosphate buffered saline solution with 0.05% Tween-20 (×5) between each step. We added 3,3′,5,5′-tetramethylbenzidine substrate , then added 100 µL of 0.3 mol/L sulfuric acid to stop the reaction. We read plates at an absorbance of 450 nm by using a SpectraMax F5  plate reader. We conducted statistical analysis by using Prism software . We used analysis of variance to compare multiple groups and a 1-tailed Student _t_ \\-test to compare groups of 2.\n\n【16】##### Histopathology and IF Staining\n\n【17】We examined gross tissues from small intestines, duodenum to ileum, and large intestines, cecum and colon, as well as other organs, including bursa, lung, liver, kidney, proventriculus, and spleen, and then fixed tissues in 10% neutral formalin for 1–2 days at room temperature for histopathology . We embedded, sectioned, and then stained samples with hematoxylin and eosin for light microscopy examination. We measured mean jejunal or ileal ratios of villus height and crypt depth (VH:CD) by using MetaMorph software , as described previously . We tested prepared tissues by IF staining to detect PDCoV antigen using a polyclonal rabbit antiserum against PDCoV (provided by E. Nelson, South Dakota State University, Brookings, SD, USA) . We also tested tissues from a PDCoV-infected pig for comparison.\n\n【18】##### Real-Time Reverse Transcription-PCR\n\n【19】We suspended cloacal swabs, tracheal swabs, SIC, and large intestine contents (LIC) in 1–4 mL MEM as a 10% suspension. We extracted RNA by using GenCatch Viral RNA Miniprep Kit . We further processed samples containing fecal matter by using OneStep PCR Inhibitor Removal Kit .\n\n【20】We determined viral RNA titers by real-time reverse transcription-PCR (rRT-PCR), as reported previously . In brief, we amplified a 541-bp fragment of the M gene that covered the quantitative RT-PCR–amplified fragment. We designed 5′-CGCGTAATCGTGTGATCTATGT-3′ and 5′-CCGGCCTTTGAAGTGGTTAT-3′ primers according to the sequence of a strain from the United States, Illinois121/2014 . We purified the PCR products by using a QIAquick PCR Purification Kit , sequenced, and then used these as the template to construct a quantitative RT-PCR standard curve. The detection limit of the rRT-PCR was 10 genomic equivalents (GEs)/reaction, which corresponded to 4.6 log 10  GE/mL of PDCoV in cloacal and tracheal samples.\n\n【21】### Results\n\n【22】##### Clinical Signs\n\n【23】By 2 dpi, 70% of infected chicks had diarrhea and fecal scores \\> 2; that percentage decreased to 17% by 9 dpi . By 14 dpi, most (/6) of the remaining infected chicks had normal feces. The 5 uninfected sentinel chicks that comingled with the infected birds at 2 dpi demonstrated mild to moderate diarrhea 2 days after comingling (dpi); diarrhea peaked 5 days after comingling (dpi). At 14 dpi, only 2 sentinel chicks had abnormal feces. Two chicks in the uninfected group had transient diarrhea during the study but tested negative for known pathogens, including PDCoV.\n\n【24】In poults, 50% exhibited diarrhea at 2 dpi. During the study, the rate of diarrhea increased, and poults did not recover by 14 dpi. Sentinel poults began exhibiting mild to moderate diarrhea 5 days after comingling (dpi), and by 9 dpi, 60% were affected. At 14 dpi, all 5 sentinel poults had moderate diarrhea.\n\n【25】Two infected chicks necropsied at 3 dpi had distended gastrointestinal tracts containing a mixture of yellow liquid and gas. Similar but less extensive findings were seen in infected chicks at 7 dpi. No gross pathology was detected in infected chicks or sentinel chicks at 14 dpi. In necropsies of infected poults, we observed distended gastrointestinal tracts containing a mixture of yellow liquid and gas at all time points.\n\n【26】##### Weights\n\n【27】Birds were weighed before inoculation and then at 2, 4, 7, 9, 11, and 14 dpi. PDCoV infection greatly affected the chicks’ weight at 2 dpi . At 4 and 7 dpi, weight gain averages in infected chicks were comparable to those in uninfected birds, but at 9 and 11 dpi, infected chicks had gained much less weight than the uninfected chicks. By 14 dpi, infected chicks rebounded and showed compensatory weight gain at higher levels than uninfected chicks.\n\n【28】Poult weight gain responses differed from those of the chicks. At 2 dpi, infected poults were gaining weight at a much higher rate than uninfected poults . However, by 4 dpi, poult weight gain was severely curtailed; several lost weight, and the average weight gain for the infected group was 0.5 g, compared with almost 10 g for uninfected poults. By 7 dpi, the infected poults recovered and gained weight at a slightly higher, but not statistically significantly different, rate than the uninfected poults. This trend continued until the end of the study.\n\n【29】##### Histopathology and IF Staining\n\n【30】We examined tissue sections by using light microscopy. We noted suspect zymogen depletion in several poults in both the infected and uninfected groups, suggesting possible inanition. We conducted VH:CD measurements of the ileum and jejunum of intestinal tissues from chicks at 14 dpi . Infected chicks had a VH:CD ratio of 4.26:1 compared with a ratio of 6.15:1 for uninfected chicks. The VH:CD ratio was lower in sentinel chicks than in uninfected chicks but the difference was not statistically significant.\n\n【31】We could not obtain enough measurements for poult tissues to provide accurate comparisons. IF tissue staining in infected poults demonstrated PDCoV antigen detectable in the epithelial cells lining the villi of the jejunum, although at reduced levels from the ileum and from infected porcine tissue . We also detected PDCoV antigen in numerous epithelial cells that had sloughed off and remained in the lumen of infected poults when compared with stained tissue sections from uninfected poults . We were unable to visualize a signal in tissues from chicks.\n\n【32】##### Serum IgY Antibody Responses\n\n【33】We analyzed serum samples collected at 2, 4, 7, 9, 11, and 14 dpi. We used indirect ELISA to test samples at 2 and 14 dpi for PDCoV-specific IgY antibodies in all birds from each group, including sentinel birds. We assigned experimental values by averaging 3 replicates. Because we did not have positive controls in chicks and poults, we established a cutoff by using the average final optical density value of uninfected birds at 2 dpi plus 2 SD. We established a separate cutoff value for sentinel birds.\n\n【34】At 14 dpi, infected chicks had increased IgY antibody levels in serum, demonstrating an antibody response to PDCoV , but sentinel chicks did not have antibody levels demonstrating exposure to PDCoV . Serum samples from poults exhibited a similar range of IgY values. The average IgY values were much higher in infected birds at 14 dpi compared with infected birds at 2 dpi and uninfected birds . The IgY greatly increased in sentinel poults at 14 dpi compared with IgY values at 2 dpi, but were still below the cutoff value .\n\n【35】##### rRT-PCR on Samples from Chicks\n\n【36】All experimentally infected chicks rapidly shed detectable viral RNA postinoculation, and viral RNA titers remained relatively constant through 11 dpi . Viral RNA from cloacal swabs reached 6.52 log 10  GE/mL by 2 dpi and remained >6.5 log 10  GE/mL until 11 dpi, when levels at 7.14 log 10  GE/mL at 9 dpi, then decreased to 5.82 log 10  GE/mL at 14 dpi. Despite an absence of noticeable respiratory signs, tracheal swab specimens also showed high levels of PDCoV RNA throughout the study . PDCoV spread rapidly from infected to naive birds, and all 5 sentinel chicks became positive for PDCoV RNA in both tracheal and cloacal swabs within 2 days of comingling with infected birds .\n\n【37】We calculated titers and viral RNA loads in SIC and LIC from infected and sentinel chicks at 3, 7, and 14 dpi . We used RNA isolated at 14 dpi from SIC of 1 infected and 1 sentinel bird to amplify an ≈1,300-bp portion of the nucleocapsid (N) gene of PDCoV, then gel extracted and sequenced the resulting product. The samples sequenced had >99% identity with the original inoculum, Ohio FD22 strain of PDCoV. We tested infectivity of intestinal contents of infected and sentinel chicks by using TCID 50  assay at 7 and 14 dpi .\n\n【38】##### rRT-PCR on Samples from Poults\n\n【39】Similar to the results from chicks, results for infected poults showed all had high levels of PDCoV RNA in cloacal and tracheal swabs through 14 dpi . Poults appeared to have higher initial viral loads, averaging 8.07 log 10  GE/mL by 2 dpi, decreasing to ≈6 log 10  GE/mL at 4 dpi, and persisting through 14 dpi . Naive birds also were susceptible to infection, and cloacal and tracheal swab specimens from all sentinel poults were positive for PDCoV RNA within 2 days after comingling with infected poults . We calculated titers and viral RNA loads in SIC and LIC from infected and sentinel poults at 3, 7, and 14 dpi . We tested infectivity of intestinal contents of infected and sentinel poults by using TCID 50  assay at 7 and 14 dpi .\n\n【40】We isolated viral RNA from the SIC of 1 infected and 1 sentinel bird at 14 dpi and used it to amplify an ≈1,300-bp portion of the N gene of PDCoV, then gel extracted and sequenced the resulting product. As we noted in chicks, the samples sequenced had >99% identity with the original inoculum, the Ohio FD22 strain of PDCoV.\n\n【41】### Discussion\n\n【42】Emerging viruses in at least 2 genera of porcine CoVs have exhibited increased propensity for interspecies transmission . Porcine APN was identified as a major cell entry receptor for PDCoV . APN is a protein that exhibits enzymatic activity, peptide processing, cholesterol uptake, and chemotaxis to cell signaling and cell adhesion . APN is widely distributed and highly conserved in amino acid sequences across species of the Animalia kingdom  and is expressed in a wide range of tissues, including epithelial cells of the kidneys , respiratory tract , and gastrointestinal tract .\n\n【43】Our data suggest that chicks and poults are susceptible to infection with PDCoV. In addition, the rapid transmission of PDCoV to the sentinel birds that comingled with infected birds demonstrates that the virus could spread easily. The length of our pilot study did not allow us to determine how long the chicks and poults would be affected by PDCoV or how long they might shed viral RNA. The chicks appeared to recover more rapidly than poults; clinical signs diminished or were completely absent by 14 dpi. However, chicks still were shedding low viral RNA titers at 14 dpi. Poults did not recover by the end of the study and still exhibited gross pathology and mild to moderate diarrhea. PDCoV RNA shedding titers were higher in poults than in the chicks. Cloacal shedding titers in chicks peaked at 9 dpi and then decreased. In poults, cloacal viral RNA shedding titers were multiphasic, peaking at 2 dpi, with additional smaller peaks at 9 and 14 dpi. The rapid onset of viral RNA shedding correlates with previous in vitro data in which the PDCoV S1 domain bound most efficiently to APN of galline origin  and cytopathic effects were observed more rapidly in leghorn male hepatoma and DF1 chicken cell lines compared with swine testicular cells (S.P. Kenney, unpub. data).\n\n【44】ELISA results showed that both chicks and poults developed PDCoV antibodies by 14 dpi. Pig infection dynamics have demonstrated a similar serum neutralizing antibody titer increase at 7–14 days . Sentinel birds had low or undetectable antibody responses compared with experimentally challenged birds, likely because of the passive infection method and because less time passed between exposure to the virus and the end of the study.\n\n【45】Recent studies demonstrated that PDCoV can infect and kill cells of other species through APN receptors . PDCoV has been reported to infect commercial chickens in vivo . The differences in susceptibility to PDCoV infection between chicks and poults we observed could be related to differences in APN expression levels between the species.\n\n【46】The true incidence rates for PDCoV infection, natural host range, reservoirs, and routes of transmission are still relatively unknown, and no plans for vaccine development have been reported . DCoV RNA has been detected in fecal samples from wild birds , Chinese ferret badgers (Melogale moschata_ ), and leopard cats . In addition to swine, calves have been shown by experimental testing to be susceptible to PDCoV infection . These data, coupled with the PDCoV binding receptor APN being conserved across many species, suggest that the host range for PDCoV is broader than initially expected . The close sequence homology between DCoV isolates from mammalian and wild bird species implies a transmission cycle in which PDCoV regularly crosses from wild birds and mammals into animal production systems, including the swine and poultry industries. More epidemiologic data are required to understand the full extent to which DCoVs are threatening food production systems and whether they pose a direct threat to human health.\n\n【47】Our results are consistent with the likelihood that avian species act as potential passthrough or intermediate hosts for PDCoV. In vivo confirmation of avian susceptibility to PDCoV suggests that in vitro data implicating human susceptibility should be evaluated further. Research regarding how PDCoV is adapting and mutating in different species and whether it infects humans is critical to determining if PDCoV poses a pandemic health risk to commercial poultry or humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "018ed5f5-a91a-4608-b2c3-5012bab08e6f", "title": "Are Community-Level Financial Data Adequate to Assess Population Health Investments?", "text": "【0】Are Community-Level Financial Data Adequate to Assess Population Health Investments?\nBackground\n----------\n\n【1】Because of the increasing attention on improving population health, policy makers in the public and private sectors are asking for better information to guide their investment decisions. The Institute of Medicine  has called on the federal government to issue an annual report on trends and disparities in social and environmental factors that affect health and to advance the use of system-based simulation models to explain the health consequences of the underlying determinants of health. These requests are consistent with calls for comparative effectiveness research to inform cost-effective allocation of resources among the multiple determinants of health .\n\n【2】Much of the variation in health outcomes among communities may result from different levels of financial and nonfinancial policy investments over time; these natural experiments should offer investment guidance for a business model of population health . Why have researchers not estimated the per capita level of investment at the community level for each of the determinants of health (medical care, public health, behaviors, and the social and physical environment and their related programs and policies) beyond which health does not improve at all or very much? Why haven’t researchers suggested benchmarks for each community according to its level of health and health-producing factors?\n\n【3】Almost nothing in the public and population health literature addresses these questions. Some empiric research has attempted to determine the balance of health determinants associated with various health outcomes; such work examines the factor itself — such as being uninsured or high school graduation rates  — but not the financial or policy resources producing them. One exception is the work of the Trust for America’s Health: it estimated that an investment of $10 per person per year in proven community-based programs to increase physical activity, improve nutrition, and prevent smoking and other tobacco use could save more than $16 billion annually within 5 years .\n\n【4】Other research explores health investment relationships in detail, estimating the effect of public services on state mortality while controlling for median income and income inequality . Total per capita expenditures in public services were significantly associated with all mortality measures, as were expenditures in education, environment, and housing. Similarly, a study of the effects of state expenditures on state-level age-adjusted mortality reported on public expenditures, tax structures, and welfare program rules and found that more generous education spending, more progressive tax systems, and more lenient welfare program rules helped to improve population health . However, the magnitudes of the effects were small, most likely because using the state as the unit of analysis masks much of the variation in outcomes and investments at local levels.\n\n【5】In _Shaping a Health Statistics Vision for the 21st Century_ , the National Committee on Vital and Health Statistics  called for a better health statistics information system to help policy makers decide how to use health resources and emphasized the importance of considering factors beyond the health of the individual, to include community and socioeconomic factors. We do not believe, however, that there yet exists a national or state data set that includes comparable measures of financial investment in the multiple determinants of population health at the substate level. The absence of such a data set is probably due to the many challenges of collecting data from multiple levels of government and making them comparable.\n\n【6】As part of its annual reports on state expenditures, the National Association of State Budget Officers noted that comparing data, even among states, is difficult because data are collected and reported differently . For example, they noted that spending on juvenile corrections may be characterized as spending on corrections in 1 state and spending on human services in another.\n\n【7】Exploring the Challenge of Comparable Data at the County Level\n--------------------------------------------------------------\n\n【8】To explore the challenge of comparable data, we examined expenditure data in 10 counties in Wisconsin in the spring of 2011. The selected urban, suburban, and rural counties represented nearly 40% of Wisconsin’s population. We searched public websites and contacted staff at state, county, and private agencies to inquire about the availability of data on total expenditures (federal, state, local, and private) and the number of people served. We evaluated the data according to whether they reflected all possible expenditures for a particular program and all possible people served. Our objective was to find comparable data at the county level in 6 categories: health care, public health, human services, income support, job development, and education.\n\n【9】### Health care\n\n【10】The Wisconsin Department of Health Services publishes regular reports on enrollment in Medicaid programs offered in Wisconsin . These reports, which do not appear to be continuously updated, identify the number of people served in each program statewide. The department provided information on the number of people in each program by county, including the Community Health Improvement Process program and Medicaid long-term care waivers, for 1 calendar year. However, people enrolled through the statewide enrollment center were not identified by county of residence. Additionally, 7 of the state’s 11 Native American tribes enroll qualifying participants into Medicaid programs but do not identify enrollees by county of residence; instead, such participants are identified by tribe.\n\n【11】### Public health\n\n【12】Local health departments in Wisconsin are organized at the county and municipal level. All local health departments are required to report to the state on their annual expenditures from all sources of revenues, including gifts, in 5 basic service areas: communicable disease surveillance, prevention, and control; generalized public health nursing program; health promotion; chronic disease prevention; and human health hazard prevention and control. Local health departments do not report per capita expenditures by service area. Instead, the per capita expenditures are calculated according to total expenditures for all service area activities. Calculations are made in this way because such expenditures promote the health of the entire county or municipality, not just individuals. Also, because local health departments are identified according to 3 levels of service, with each level offering a different set of services (level I provides the most basic and level III the most advanced), data on per capita expenditures are not comparable across all counties.\n\n【13】### Human services\n\n【14】Wisconsin has 2 reporting systems for data on expenditures in human service programs for people who have developmental disabilities, mental health issues, alcohol or other drug abuse issues, or physical or sensory disabilities. One is the Human Services Reporting System, which organizes expenditure data by standard program categories (eg, respite care, daily living skills training). This system, which permits the calculation of per capita expenditures by disability or issue group in each county, is not available electronically because it is based on COBOL (Common Business-Oriented Language) and produces only paper reports. The other system is the Human Services Revenue Report, which organizes expenditures data by disability or health issue group and revenue source. Its purpose is to allow counties to report exact costs to the state. It does not include data on the number of people served according to disability or issue group, so it does not permit calculation of per capita expenditures. Neither report includes data on all expenditures in human services, nor does either report include data on services provided by Medicaid health maintenance organizations or private providers through Medicaid fee-for-service.\n\n【15】These reports have shortcomings. The sum of expenditures identified in the 2 reports should be identical; however, Wisconsin Department of Health Services staff reported that the information is not always identical because different staff people may be responsible for completing the 2 reports. Additionally, some counties jointly administer human service programs through a single service agency and jointly report data on enrollment and expenditures, so these data are not reported by county but by agency. Finally, neither report includes data on human service expenditures made through FamilyCare, a program that delivers long-term care to the elderly and people who have disabilities. FamilyCare is delivered by consortia of counties or by nonprofit managed-care organizations (MCOs) that serve multiple counties. The publicly available data on FamilyCare enrollment identify the number of people who receive services from the consortium or MCO by county, but the data do not identify people who receive services but reside outside the service region (nonresidents), which is permitted by law. These nonresident participants are not identified by their county of residence. The data also do not identify the level of capitated payment received by the consortium or MCO that serves the nonresident. That is, does the consortium or MCO serving a nonresident receive a level of payment equal to the nonresident’s county of residence or is it same level of payment that the consortium or MCO receives for serving its county’s residents?\n\n【16】### Income supports\n\n【17】#### Earned income credit\n\n【18】Wisconsin administers an earned income tax credit that functions similarly to the federal earned income tax credit except that people who have no children are not eligible. The Wisconsin Department of Revenue generates annual reports on the number of people who receive the Wisconsin credit and the per capita credit amount for each county, but it does not publish information on the number of people who receive the federal earned income tax credit.\n\n【19】#### Special Supplemental Nutrition Program for Women, Infants, and Children\n\n【20】Wisconsin maintains enrollment data on the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) program by county. At one time, the state requested data from local WIC providers on private support used to operate the WIC program, but it no longer does so because the data were reported inconsistently. Currently, the WIC program does not publish regular public reports on the number of people served by WIC or WIC expenditures by county.\n\n【21】#### FoodShare\n\n【22】FoodShare Wisconsin, formerly known as Food Stamps, maintains data on the number of people served by county. However, participants who enroll through a statewide center are not identified by county of residence, nor do counties know how many, if any, of these recipients reside in their county. The state is interested in enrolling more participants through the statewide center because it is seen as more efficient. Participants can enroll online through the statewide center or visit their county human services agency for enrollment assistance. Because FoodShare is a federally funded program, the state is required to maintain data on total expenditures in program benefits.\n\n【23】### Job development\n\n【24】We did not find any data set that tracked local job development activity.\n\n【25】### Education\n\n【26】We identified 3 data sets on expenditures for education for kindergarten through grade 12.\n\n【27】#### National Center for Education Statistics\n\n【28】The National Center for Education Statistics annually publishes statistics on expenditures by school district and revenue source — federal, state, local, and program. It also calculates the per-student expenditure for each district. These data present 2 challenges. First, Wisconsin school districts may be organized in 1 of 3 ways: kindergarten through grade 12, kindergarten through grade 8, and Union High School districts, which serve only grades 9 through 12. Each district type has its own cost structure. Second, Wisconsin has 426 school districts. Many districts, particularly in rural parts of the state, cross county boundaries.\n\n【29】#### Child nutrition expenditures\n\n【30】School districts report the amount that they spend on student meals to the Wisconsin Department of Public Instruction. The school districts also report revenues from federal and state subsidies and program revenues associated with meal payments from students and nonstudents and meals prepared by the district for a third party, such as another school district, through contracts. The per-student meal expenditure is based on revenues from all such services. Expenditure data exist for all public school districts and for private school districts that participate in the federal and state subsidy programs. These data are not available electronically because the COBOL-based system that hosts the data produces only paper reports.\n\n【31】#### Head Start and Early Head Start\n\n【32】The US Department of Health and Human Services (HHS) maintains information on federal expenditures for Head Start and Early Head Start and the number of children served by the programs. Wisconsin provides supplemental Head Start grants that providers may use as matching funds required by the federal program. Wisconsin maintains data only on the number of children served through its own grants. It does not maintain data on how much of each supplemental grant is used as federal matching funds, nor does it or HSS request information on other private or public resources used to support these programs.\n\n【33】Conclusion\n----------\n\n【34】The National Committee on Vital and Health Statistics identified challenges related to the collection of data on health statistics. The challenges include difficulty in locating useable data, a lack of resources among public agencies for upgrading information technology systems to support greater public access to data, and a lack of enterprise-wide coordination and geographic detail in data collection efforts. To this could be added the political barriers for developing common standards among state and community jurisdictions.\n\n【35】Wisconsin data on population health expenditures are often limited by 1 or more of these challenges. If researchers, administrators, and the public are to analyze and evaluate programs and influence policy makers, much work is needed to make data accessible and comparable. Expenditure data must be collected for all programs and their funding sources, and accurate counts must be made of the people served by such programs by geographic unit. Upgrading information technology systems to provide data in a format that is easy to use and manipulate will require investments of money and staff resources. Data collection efforts need to be integrated horizontally (among state agencies, for example) and vertically (among federal, state, and local funding sources, for example).\n\n【36】We live in a resource-limited world that has disparities in health outcomes by race, sex, socioeconomics, and geography. Public and private policy makers at the national, state, and local level want to find resources to address these disparities. But as Evans and Stoddart say, “Redirecting resources means redirecting \\[someone’s\\] income\\[s\\]. \\[M\\]ost students of population health cannot confidently answer the question ‘Well, where would _you_ put the money?’” . Standardizing data is not the most exciting activity, but making the most cost-effective investments requires it . It is time for state and national government to make progress toward this objective, perhaps under the leadership of the new HSS Community Health Data Initiative . Improving our health cost effectively may depend on it.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a1002219-858e-4055-9333-e55a8c693a2a", "title": "Prevention of Mother-to-Child HIV Transmission Internationally", "text": "【0】Prevention of Mother-to-Child HIV Transmission Internationally\nNotes from presentations on “Preventing Mother-to-Child Transmission Worldwide”: the following comments were made in presentations by the above authors at the International Conference on Women and Infectious Disease.\n\n【1】Data from the Joint United Nations Programme on HIV/AIDS (UNAIDS) indicate that in 2003, 34–46 million people were living with HIV infection, and three fourths of these cases were in sub-Saharan Africa. Approximately 2.1–2.9 million children were living with HIV/AIDS. HIV transmission in sub-Saharan Africa is predominately heterosexual, and by the end of 2002, women represented 58% of HIV cases. UNAIDS estimates that in many African countries <1% of pregnant women receive needed antiretroviral prophylaxis to prevent mother-to-child HIV transmission (PMTCT). This has a substantial impact on the death rate in children, with previous gains reversed for children <5 years of age in several countries.\n\n【2】Without intervention, the risk of mother-to-child HIV transmission is 30%–35%. With antenatal HIV testing, combination antiretroviral drugs, and safer infant feeding, the risk can be reduced to 1%–2%. Interventions for PMTCT should also be provided in the broader context of prevention, including primary prevention of HIV, preventing unintended pregnancies, and care and support to HIV-infected women and their families.\n\n【3】### U.S. Government Response to Global Mother-to-Child HIV Transmission\n\n【4】In 2002, President George W. Bush introduced the International Mother and Child HIV Prevention Initiative. This initiative was coordinated across several U.S. government agencies including the Centers for Disease Control and Prevention (CDC) and U.S. Agency for International Development; the initiative focused on 14 countries in Africa and the Caribbean with high rates of HIV/AIDS. The goals of the initiative were to reduce mother-to-child transmission by up to 40%; support expanding national PMTCT programs; support linking PMTCT services with antiretroviral treatment and care for mothers, infants, and family members (PMTCT-plus); and reach up to 1 million women annually.\n\n【5】Core interventions include routinely recommending HIV counseling and testing at antenatal clinics, short-course antiretroviral prophylaxis for HIV-positive mother-infant pairs, counseling and support for safe infant feeding practices, and counseling for family planning. Additional interventions include prevention strategies for HIV-negative pregnant women and community mobilization to increase uptake and decrease stigma. By 2003, all 14 countries had started to provide services, and this initiative is now a major activity under the more comprehensive President’s Emergency Plan for AIDS Relief, which addresses the same 14 countries plus Vietnam.\n\n【6】### Implementing PMTCT Programs Internationally\n\n【7】##### Case Study in Kenya\n\n【8】Kenya has a population of 31.1 million, with 1.2 million births every year. Of the 2.2 million people living with HIV/AIDS in Kenya, 1.4 million are women. The most rapidly growing population becoming infected with HIV is women. HIV-positive women give birth to 118,000 children annually. An estimated 35,000–40,000 of those infants are HIV-positive. Ten percent of reported HIV/AIDS cases in Kenya are in children <5 years of age. PMTCT interventions include antiretroviral drug prophylaxis, optimal obstetric care, infant feeding counseling, and family planning. Replacement feeding (as opposed to breastfeeding) is only recommended in environments where it is acceptable, feasible, sustainable, and safe. Through the CDC Global AIDS Program in Kenya, 18,000 antenatal women have learned their HIV status, and 50% of those who are HIV-positive have received prophylactic antiretroviral drugs. Barriers to testing include a lack of spousal support, fear of partner violence, and fear of disclosure and the stigma that may accompany it.\n\n【9】### Case Study in Botswana\n\n【10】Botswana’s 2003 surveillance data show that 37.4% of women attending antenatal clinics are HIV-positive. Botswana has had a national PMTCT program since 2001 and an expanding antiretroviral treatment program since 2002. Both programs are free to patients. All pregnant women can receive HIV counseling and testing. Antiretroviral prophylaxis for women and infants and infant formula are provided for HIV-positive women. Although 95% of pregnant women attend antenatal clinics and deliver in health facilities, uptake of PMTCT has been low. A CDC-Botswana government survey of pregnant women was performed to explore factors influencing HIV test acceptance. Factors predicting acceptance included higher educational level, attendance at urban clinics, greater knowledge about PMTCT, planned pregnancy, discussing HIV testing with others, and knowing others who had received PMTCT or antiretroviral therapy.\n\n【11】These presentations highlight the successes of PMTCT programs as well as continuing challenges. There continues to be a need for program evaluation, operational research, and expanded PMTCT services in order to maximally prevent mother-to-child HIV transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e93853d1-b0d0-4ee5-b47f-f2e60d6b29e8", "title": "Fever Screening at Airports and Imported Dengue", "text": "【0】Fever Screening at Airports and Imported Dengue\nDengue viruses are arboviruses that cause substantial human disease in tropical and subtropical regions of the world, especially in urban and semiurban areas. Because of its high endemicity in many countries in the Western Pacific, Southeast Asia, and South American regions, dengue has become an important public health problem in most nations in these areas . Dengue is not considered endemic in Taiwan, however, and the constant importation of dengue viruses from the neighboring Southeast Asian countries through close commercial links and air travel is believed to cause local outbreaks . Until now, local outbreaks, which are most frequent in the summer and fall, have each been caused by a single imported dengue virus strain that disappears when each outbreak ends. Because waves of relatively cold temperature of ≈10°C cause low mosquito density in winter, winter outbreaks are rare, with the exceptions of large outbreaks in 1915–1916, 1942–1943, 1987–1988, and 2001–2002. Outbreaks occur mainly in southern Taiwan, where _Aedes aegypti_ and _A_ . _albopictus_ coexist, and rarely occur in central and northern Taiwan, where only _A_ . _albopictus_ exist. The dengue hemorrhagic fever cases in Taiwan are highly correlated with increasing age and secondary dengue virus infection (J-H Huang, unpub data).\n\n【1】### The Study\n\n【2】To identify imported dengue cases and reduce the local spread of newly introduced dengue viruses, the health authority, now Center for Disease Control, Taiwan, has established an integrated dengue control program that includes various surveillance systems, a network of rapid diagnostic laboratories, and mechanisms of rapid response to implement control measures . The primary objective is to prevent the introduction of new dengue viruses into Taiwan by travel and subsequent local spread. Dengue is classified as a reportable infectious disease, and suspected cases must be reported within 24 hours of clinical diagnosis in Taiwan. For effective surveillance, both passive (hospital-based reporting system) and active (such as health statement of inbound passengers, self-report, expanded screening for contacts of confirmed cases, patients with fever of unknown origin, school-based reporting, community screening) surveillance systems were established in central and local health departments.\n\n【3】For rapid diagnosis, 2 central dengue diagnostic laboratories were set up in Center for Disease Control–Taiwan, at Kun-Yan Laboratory in northern Taiwan and at a fourth branch laboratory in southern Taiwan. Serum samples from suspected dengue patients were sent to the diagnostic laboratories, and the results were reported within 24 to 48 hours. To avoid delays, the laboratory is scheduled to perform the tests on a daily basis without vacations. A rapid diagnostic system was developed to detect and differentiate various flavivirus infections on the basis of the results of 1-step real-time polymerase chain reaction (PCR) and envelope membrane (E/M)–specific capture immunoglobulin (Ig) M and IgG enzyme-linked immunosorbent assay (ELISA) . Analysis of a total of 959 acute- and convalescent-phase serum specimens from 799 confirmed dengue patients showed that 95% of acute-phase serum specimens could be identified as being from confirmed or probable case-patients based on these 2 assays .\n\n【4】In 2003, Taiwan was one of the countries heavily affected by the multinational epidemics of severe acute respiratory syndrome (SARS) . During the SARS epidemic, the body temperature of all inbound and outbound passengers at the 2 international airports was screened to prevent international spread of SARS. Since July 14, 2003, all inbound passengers have been required to complete the “SARS Survey Form” before landing and to have their body temperature taken by an infrared thermal camera. Any passenger showing body temperature >37°C is rechecked by ear temperature, and serum samples are collected and sent for SARS diagnosis if the ear temperature is >37.5°C. After July 5, 2003, the world was largely considered to be SARS free, and other causes of fever had to be considered; therefore, a panel of diagnostic tests, including tests for pathogens of dengue, malaria, enteric bacteria, and other diseases (such as yellow fever, plague), was performed for selected fever patients. Since dengue fever is among the top yearly imported reportable diseases in Taiwan, we began a trial fever screening program for dengue along with SARS screening at the airports.\n\n【5】We report our findings on early identification of dengue fever through fever screening at the 2 international airports, C.K.S. and Kaohsiung Airports, Taiwan. More than 8,000,000 inbound travelers passed through the 2 airports from July 2003 to June 2004. Among these, ≈22,000 passengers were identified as fever patients by an infrared thermal camera and rechecked by ear temperature. Diagnostic testing algorithms for screened fever patients were based on evaluation by airport clinicians. After clinical diagnosis, 3,011 serum samples were sent for laboratory diagnosis of dengue virus infection. Forty (.33%) of 3,011 serum samples were confirmed to be positive on the basis of the results of real-time PCR and E/M-specific capture IgM and IgG ELISA. During the same period, 6,005 dengue cases were reported in Taiwan (both indigenous and imported cases), which includes 935 cases from the passive surveillance system and 5,070 cases from active surveillance systems (fever patients were identified by fever screening and 2,059 cases were identified from other systems). Among these, 73 were confirmed to be imported dengue cases, including 25 cases reported from hospitals through passive surveillance and 48 cases identified by active surveillance, such as airport screening and self-report by patients. Airport fever screening alone picked up 40 (.3%) of 48 of all imported cases identified by the active surveillance system. Thus, 8 imported cases were identified by other active surveillance methods. The average length between the onsets of dengue symptoms to the time of diagnosis was 4.15 days for the 40 case-patients who were identified at the airport, as opposed to 11 days for those who were reported from the hospital. Whether the shorter length required to diagnose conditions identified by airport fever screening contributed to the low indigenous dengue in the season warrants further investigation.\n\n【6】Fever screening at the airports has also dramatically increased the proportion of imported dengue cases identified by active surveillance, 48 (.8%), of 73 which is significantly higher than the number identified during years before fever screening were implemented (p < 0.0001 by chi-square test) . The countries of origin of imported dengue fever from July 2003 to June 2004 were all located in the Western Pacific and Southeast Asia . The distribution of the countries of origin accurately reflected the frequency of air travel between Taiwan and these nations, as well as the intensity of massive dengue outbreaks during the same period in the country of origin. Analyses of dengue virus serotypes showed that various serotypes were circulating in each of these countries during this period.\n\n【7】Most of the confirmed cases (of 40) identified by airport fever screening were viremic (real-time PCR positive, IgM and IgG negative). The other 7 case-patients tested positive for dengue-specific IgM or IgG antibody, although they were febrile at the time of testing (data not shown). Estimating how many patients might have been viremic but were not picked up by the system is difficult, since persons infected with dengue virus are usually viremic from 2 to 3 days before onset of symptoms until defervescence.\n\n【8】### Conclusions\n\n【9】Our results demonstrated that fever screening at airports is an effective means of identifying imported dengue cases, whereas the health statements of inbound passengers, which have been required for years, are ineffective. Although fever screening with infrared temperature screening was implemented in an attempt to avoid SARS transmission, it proved to be effective in active surveillance of dengue. This approach seems promising for dengue and perhaps for other diseases and should be further evaluated.\n\n【10】The cost of identifying dengue virus infections with airport fever screening is similar to that of other surveillance methods. The airport fever screening method requires an infrared thermal camera, which costs approximately U.S. $43,000 for each set of instruments. In addition, 1 additional worker is needed to monitor this alarm system. The reporting procedure and clinical and laboratory diagnoses are similar to those of surveillance methods. Therefore, the method is a cost-effective means of identifying imported dengue cases.\n\n【11】Although febrile passengers suspected of having dengue virus infection were not detained at the airport, and an epidemiologic investigation was not conducted, they were provided with a mosquito net to avoid mosquito bites and instructed to report to the local health department if they felt ill. Laboratory diagnoses were performed on a daily basis, and results were reported within 24 to 48 hours. Control measures were implemented as soon as possible if probable or confirmed dengue cases were identified. Since viremic persons, going about their normal activities for a mean interval of 2 to 3 days before diagnosis, could have transmitted dengue, the laboratory detection method on its own will not be effective in preventing transmission. Therefore, developing an integrated program that includes various surveillance systems, rapid diagnostic laboratories, and emergency control measures is necessary to prevent the introduction and spread of new dengue viruses into a region. Control measures should consist of epidemiologic investigation, health education, analysis of mosquito density, source reduction, and insecticide application. As part of an integrated dengue control program, fever screening at the airport has become one of the most important active surveillance systems in Taiwan since its introduction in July 2003. We believed that this active surveillance system could also be successfully applied to screen febrile patients and reduce the introduction of many potential infectious diseases.\n\n【12】This work was in part supported by grants DOH92-DC-2005 and DOH92-DC-2006 from the Center for Disease Control, Department of Health, Taipei, Taiwan, Republic of China.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8509abbd-4355-4480-890c-5fc4a5a2b705", "title": "Longitudinal Outbreak of Multidrug-Resistant Tuberculosis in a Hospital Setting, Serbia", "text": "【0】Longitudinal Outbreak of Multidrug-Resistant Tuberculosis in a Hospital Setting, Serbia\nThe overall burden of tuberculosis (TB) in Serbia has been greatly reduced in recent years . However, a recent study revealed transmission of multidrug-resistant (MDR) _Mycobacterium tuberculosis_ complex (MTBC) strains (i.e. MTBC strains resistant to at least rifampin and isoniazid) in Belgrade . In addition, data retrieved from the national database of MDR TB patients indicate a concentrated burden of MDR TB and extensively drug-resistant (XDR) TB, defined as additional resistance to 1 fluoroquinolone and 1 of the 3 injectable second-line drugs, among psychiatric inpatients in Serbia. To gain more insights into countrywide transmission routes, strain dynamics, and bacterial evolution over time, we retrospectively investigated all (n = 110) patients who received a diagnosis of MDR TB during January 1, 2008–May 31, 2014, in Serbia.\n\n【1】### The Study\n\n【2】We subjected 1 MTBC isolate per patient to phenotypic drug susceptibility testing and whole-genome sequencing (WGS) . We retrieved patients’ demographic, epidemiologic, and clinical data from the national database of MDR TB patients, as well as from their medical and laboratory records.\n\n【3】Most patients were male (/110, 79.1%) and born in Serbia (/110, 97.3%); mean age was 49.5 years (range 15–83). We observed concurrent conditions for 55 patients; schizophrenia was the most prevalent (/55, 47.3%). Of the 110 patients, 61 (.5%) had previously experienced TB. Susceptibility testing results showed that 19/110 (.3%) MDR MTBC isolates were resistant to all first-line drugs, and 11/110 (.0%) were classified as XDR . We successfully completed WGS for 103/110 isolates, representing 93.6% of all MDR TB cases recorded over the study period.\n\n【4】We considered 6,512 single-nucleotide polymorphisms (SNPs) differentiating all isolates to analyze their phylogenetic relationships. The MDR MTBC strain population comprised 37/103 (.9%) isolates classified as lineage 4.2.2.1 (TUR genotype), 20/103 (.4%) isolates of lineage 4.1.2 (Haarlem genotype), 17/103 (.5%) isolates of lineage 2.2.1 (Beijing genotype), 15/103 (.6%) isolates of lineage 4.8 (H37Rv-like strains), 8/103 (.8%) isolates of lineage 4.4.1.1 (S-type), 2/103 (.9%) isolates of lineage 4.2.1 (URAL genotype), 1 isolate of lineage 4.1 (Ghana), and 1 nonclassified lineage 4 isolate. Among lineage 2.2.1 Beijing isolates, the previously described Europe/Russia W148 MDR outbreak isolates  were most prevalent, present in 14/17 (.4%) of the cases .\n\n【5】Seeking to identify recent chains of transmission, we defined molecular clusters as surrogate markers for epidemiologically linked cases . Overall, 63/103 (.2%) isolates could be assigned to 12 different clusters, each including 2–17 patients. The 2 largest clusters, 1 containing 14 and 1 containing 17 cases, comprised isolates of TUR genotype; the next-largest cluster was of 7 Beijing Europe/Russia W148 isolates. For all 63 suggested epidemiologic links, we were able to retrospectively identify 40 (.5%) epidemiologic links (e.g. household and social contacts) .\n\n【6】Our main finding was that 35/37 (.6%) TUR isolates shared identical mutations that confer drug resistance to isoniazid (katG_ S315T), streptomycin (rpsL_ K43R), and ethambutol (embB_ Q497R); we therefore classified them as TUR-outbreak isolates. TUR-outbreak isolates further differentiated into 2 individual transmission chains characterized by 2 distinct rifampin resistance–mediating mutations: _rpoB_ S450W in 1998 (% highest posterior density \\[HPD\\] 1993–2001) and _rpoB_ S450L in 2003 (% HPD 2000–2005) . Subsequently, both strain populations acquired individual mutations in other RNA polymerase genes (rpoA_ P25R, _rpoC_ V431M, and _rpoC_ F452L), which have been proposed to enhance the in vitro growth rate of rifampin-resistant strains . Furthermore, _rpoA_ mutations in the entire dataset were more likely to arise in clustered isolates than in unique isolates (/63 vs. 1/40; p<0.001), thus indicating their ability to restore fitness of _rpoB_ mutants, increase transmission success, or both.\n\n【7】Of the 35 TUR-outbreak isolates, 26 (.3%) were from patients hospitalized in Bela Crkva (BC) Hospital, the national center for treatment of all psychiatric patients with concomitant respiratory illnesses. Of note, 22 (.6%) of these 26 patients had been transferred from 7 different psychiatric hospitals to BC Hospital for pulmonary diagnosis and treatment; 5 were admitted at BC Hospital with either confirmed or suspected TB diagnosis . Screening for TB at time of admission had not been implemented in BC Hospital during the study period.\n\n【8】To determine the geographic origin of the 3-fold resistant TUR ancestor and to test for the putative independent introduction of 2 different rifampin-resistant cases to the BC Hospital from other hospitals, we extended our Bayesian approach with a discrete trait model introducing the likely place of infection for each patient. We used 2 assumptions: first, a fast disease progression assumed infection and diagnosis of MDR TB within the first 2 years after admission to BC Hospital; and second, a slow disease progression in which patients who received a diagnosis within 2 years after admission were identified as latent MDR TB cases, meaning they had contracted the infection in their hometown or a previous hospital.\n\n【9】The comparison of both models using path sampling clearly favored the fast progression model, suggesting the origin of the TUR outbreak in BC Hospital with a probability of 53% (i.e. node location probability; second likely origin was Belgrade, 12%) . The 2 unique rifampin-resistance mediating mutations were also more likely to have originated in BC Hospital itself (% for _rpoB_ S540W node, 95% for _rpoB_ S540L node, and <15% for other location probabilities). Individual transmission events occurred to remote cities but also within Belgrade . In comparison, applying the slow TB progression hypothesis, TUR outbreak strains would have been imported multiple times from different regions throughout the country to BC Hospital, with node location probabilities < 10% for all locations . Tracing the time of hospitalization at BC Hospital and MDR TB diagnosis of patients infected with TUR strains backward revealed that the 2 clades (defined by _rpoB_ S450L and _rpoB_ S450W) indeed coexisted over 2 decades .\n\n【10】### Conclusions\n\n【11】In a retrospective approach using WGS-based molecular epidemiology, Bayesian statistics, and detailed epidemiologic investigations, we show that MDR TB in Serbia is associated with nosocomial transmission at BC Hospital, likely accompanied by a fast progression to disease within 2 years. Drug unavailability in the 1990s , schizophrenia as a recognized cause of unsuccessful completion of TB treatment , and long-term and repeated hospitalizations under extremely adverse living conditions , together with the absence of a TB infection control program, are believed to be the main drivers of the evolutionary trajectories and success of TUR-outbreak strains in Serbia. The TUR-outbreak strain was considered intrinsically resistant to 3 first-line drugs and probably acquired an MDR genotype in 2 independent events in BC Hospital during the 1990s. Subsequently, putative compensatory mechanisms were selected, the strain acquired individual XDR genotypes, and it spread into other settings in Serbia by family contacts and other modes.\n\n【12】Detection of the extensive transmission network in BC Hospital led to the development and implementation of an appropriate TB infection control program featuring the use of rapid laboratory tests for prompt detection of new cases, completion of appropriate second-line treatment regimens, and markedly expanded contact tracing activities. Since 2015, only 1 new case of MDR TB has been recorded in BC Hospital. However, MDR TB transmission in the general population must continue to be carefully monitored.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fbc248e6-1782-49f1-a213-2a83aa24c99c", "title": "Bulleidia extructa Periprosthetic Hip Joint Infection, United States", "text": "【0】Bulleidia extructa Periprosthetic Hip Joint Infection, United States\n**To the Editor:** _Bulleidia extructa_ is an obligately anaerobic, nonmotile, non–spore-forming gram-positive bacillus first described in 2000 by Downes et al. after having isolated a bacterium from the oral cavity of persons with periodontitis and dentoalveolar abscesses that did not correspond to any known species. After phenotypic and genetic characterization, the investigators proposed a new genus, _Bulleidia_ , and the species _B. extructa_ . Since then, additional reports have associated the organism with oral infections, specifically periodontal disease . While _B. extructa_ ’s association with human periodontal disease is well documented, the bacterium has so far not been implicated in other pathogenic processes. We report here a case of a total hip arthroplasty infection caused by _B. extructa_ in an immunocompetent patient.\n\n【1】In November 2010, an 82-year-old man with a non-cemented right total hip arthroplasty that was performed 26 years previously was evaluated for right hip pain. He had been in his usual state of health without any complaints until a month earlier, when he lost his footing and hyperabducted his hip joints, involuntarily performing a split, while washing a boat cover with a power washer. Since then, he reported right hip pain that somewhat limited his mobility.\n\n【2】Physical examination revealed an antalgic gait, mild swelling of the right lower extremity, and impaired hip mobility related to pain on the right side, specifically with extension, flexion, abduction, and adduction. Results of the patient’s blood work were notable for normocytic anemia (hemoglobin 10.6 g/dL), thrombocytosis (× 10 9  /L), elevated erythrocyte sedimentation rate (mm/h), and elevated C-reactive protein (.7 mg/L). Leukocyte count was within normal limits (.6 × 10 9  cells/L). An ultrasound examination of the right hip joint showed extensive synovitis and a large, 4.3 × 5.0 × 5.1–cm vascular mass extending anteriorly from the joint space. Aspiration of the joint space yielded 1 mL of blood-stained fluid with 111,595 cells/µL (% neutrophils, 5% monocytes/ macrophages). Anaerobic bacterial culture grew a gram-positive bacillus identified as _B. extructa_ by partial 16S rRNA sequencing. DNA was prepared for PCR amplification by using PrepMan Ultra (Applied Biosystems, Foster City, CA, USA) and amplified and bidirectionally sequenced by using primers 5′-TGGAGAGTTTGATCCTGGCTCAG-3′ and 5′-TACCGCGGCTGCTGGCAC-3′. The generated 484-bp sequence differed by 2 bp from 483 bp of available sequence from _B. extructa_ GenBank accession no. AF220064. The isolate was susceptible to penicillin, clindamycin, and metronidazole by using E-test.\n\n【3】The patient underwent total hip arthroplasty resection. Intraoperatively, purulence was noted upon entering the hip joint. Histopathologic examination of removed tissue revealed acute inflammation. Five hip tissue specimens were obtained for culture; 3 specimens yielded _B. extructa_ . Six weeks of intravenous ceftriaxone treatment was prescribed, and the patient was instructed to revisit a dentist for a full dental examination. Before seeking treatment for this episode, he reported that he was seeing a dentist on a regular basis and denied any recent dental surgery or infections.\n\n【4】The patient was seen in a follow-up visit 2 months after reimplantation surgery; at that time, he reported minimal pain and had begun to bear weight on the affected side. There was no evidence for infection recurrence.\n\n【5】Periprosthetic joint infections are a major complication after joint replacement. The number of procedures for total hip and knee replacements has increased during the past 13 years . This trend is accompanied by an increase in the total number of periprosthetic joint infections, even though the overall percentage of this complication is low . The most commonly isolated organisms in periprosthetic joint infections are gram-positive cocci, specifically _Staphylococcus aureus_ and _S._ _epidermidis_ . In a retrospective review, Moran et al. examined the microbiological spectrum of 112 patients undergoing debridement and irrigation for a periprosthetic joint infection (hip , knee , elbow , ankle , shoulder ) at a tertiary care center in the United Kingdom during 1998–2003. The most frequently isolated microorganisms were coagulase-negative staphylococci (%) followed by methicillin-sensitive _S. aureus_ (%), methicillin-resistant _S. aureus_ (%), aerobic gram-negative organisms (%), and anaerobes (%). Thirty-seven percent of patient specimens grew multiple microorganisms.\n\n【6】We document the ability of _B. extructa_ to cause an infection beyond its usual habitat, the oral flora. We hypothesize that the infection in this patient might have developed from hematogenous seeding in which an undiscovered and asymptomatic oral infectious nidus might have served as the seeding focus while mild trauma to the hip could have facilitated access to the joint space.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b57f60c5-a6ad-4bab-b732-140bfec3319b", "title": "Artyfechinostomum sufrartyfex Trematode Infections in Children, Bihar, India", "text": "【0】Artyfechinostomum sufrartyfex Trematode Infections in Children, Bihar, India\nFoodborne intestinal trematodiasis, especially that caused by members of the family _Echinostomatidae_ , is an emerging yet neglected public health disease. Approximately 24 echinostome species cause human echinostomiasis and are highly endemic to Southeast Asia and the Far East; major foci are located in China, India, Indonesia, South Korea, Malaysia, the Philippines, and Thailand .\n\n【1】Previously, only 2 deaths attributed to the echinostomid fluke _Artyfechinostomum sufrartyfex_ were reported from the states of Assam and Tamil Nadu in India . During 2004–2017, several cases of echinostome infection were reported in children at Shri Shubh Lal Hospital and Research Centre in Bihar, India.\n\n【2】### The Study\n\n【3】This study was approved by the Institutional Ethics Committee of Sikkim University (SU/IEC/2017/04), Gangtok, India. A total of 170 cases of _A. sufrartyfex_ trematode infection occurred in northern Bihar, India, mostly in children < 12 years of age. The children lived in the districts of Sitamarhi and Sheohar in the state of Bihar. Signs and symptoms were diarrhea (persistent/chronic and acute) with watery or mucus-bound stool, vomiting, loss of appetite, weakness, passage of red worms in stool or vomit, swelling of the feet and the entire body, fever, cough, breathlessness, night blindness, and urticarial rashes .\n\n【4】Physical examination showed that most patients were anemic. Clinical laboratory investigations showed leukocytosis and eosinophilia. However, systemic examination showed no adverse effects of the cardiovascular, abdominal, and central nervous systems . Levels of serum alanine aminotransferase, bilirubin, blood urea, creatinine, electrolytes, sodium, potassium, and chloride were within reference limits.\n\n【5】These children were immediately hospitalized and kept under careful observation with routine monitoring of stool and vomit for worms. Once worms were observed in samples, the patients were given praziquantel (mg/kg in 3 divided doses orally for 2 days) and monitored. At administration of the drug, patients started passing more worms in stool. We recovered >50 worms but < 300 worms from each child patient. The infection subsided after the standard dose of praziquantel, and most patients recovered from the infection.\n\n【6】However, we observed 11 deaths: 2 patients each during 2004, 2007, 2008, 2012, and 2013 and 1 patient during 2009. Severe acute malnutrition with or without edema and large numbers of worms were major clinical conditions observed for these deaths. Nine children had persistent diarrhea with severe dehydration and shock, and 2 of them had acute diarrhea, severe dehydration, and shock.\n\n【7】The infected patients frequently consumed raw snails. The most prevalent snail species in the study areas was _Pila globosa_ , which the children collected from the banks of ponds/ditches and waterlogged paddy fields grossly contaminated with human and animal excreta (/170 cases, 99%). Therefore, we surveyed as many as 8 sites in 2 districts (Sitamarhi and Sheohar) for snail samples from their natural habitats .\n\n【8】We screened the snails by using a digestion technique with a 0.5% pepsin/0.1% HCl solution and found that the snails were heavily infected with metacercariae, which are the encysted infective stage of the trematode. The prevalence of metacercariae in the snails ranged from 16.12% in Hanumannagar to < 48.19% in Punaura .\n\n【9】To establish the source of infection, we attempted to identify the clinical parasite samples and the metacercariae. We morphologically identified representative parasite samples isolated from the patients . However, we could not identify metacercaria by only morphologic characteristics . Therefore, we used a molecular approach to confirm the identity of the life cycle stages.\n\n【10】We isolated total genomic DNA from individual adult trematodes . For metacercariae, we isolated genomic DNA from ≈200 cysts/isolation by using a DNA Isolation Kit  according to the manufacturer’s protocol. We then amplified and sequenced nuclear 28S rRNA and internal transcribed spacer (ITS) 2 genes and the mitochondrial cytochrome c oxidase (COI) gene for both stages by using universal trematode primers . Individual gene regions tested were 1,042 bp for the 28S rRNA gene, 433 bp for ITS2 gene, and 343 bp for the mitochondrial COI gene. We deposited sequences in GenBank (accession nos. MH236132–3, MH237730–1, and MH253673–4).\n\n【11】For specific identification of the parasites, we performed a blastn search . The 28S rRNA and mitochondrial COI gene regions showed maximum sequence identity with GenBank accession no. KF781303.1 for _A. sufrartyfex_ (%) and accession no. NC037150.1 for _A. sufrartyfex_ from Shillong, India (%). For the ITS2 gene region, we observed maximum sequence identity with GenBank accession no. JF412727 _Echinostoma malayanum_ from Khon Kaen, Thailand (%), and with accession no. EF027100.1 _A. sufrartyfex_ from Meghalaya, India (%). On the basis of these findings, we concluded that clinical specimens and metacercariae isolated from _P. globosa_ snails were the same species (A. sufrartyfex_ ).\n\n【12】Furthermore, we generated barcode sequences by using trematode-specific primers  and deposited them in the BOLD database . We obtained unique barcodes for both life cycle stages . The barcode sequence had a length of 777 bp. To check for its specificity, we performed a similarity search across the BOLD database by using the BOLD Identification System. This search showed that our sequences were highly species specific; the closest match with other species was with _Nephrostomum limai_ worms (.24% identity).\n\n【13】### Conclusions\n\n【14】Our findings conclusively establish that these children were infected with _A. sufrartyfex_ trematodes. We identified the causal agent and its infective metacercarial stage as _A. sufrartyfex_ trematodes by using morphologic and molecular approaches. For ease of accurate identification in the future, we also provide unique DNA barcodes for the species.\n\n【15】Overall, we detected 170 infected case-patients and 11 deaths from these infections. Because of lack of proper diagnostic tools available to medical practitioners in the affected parts, several other infection cases might have remained undefined. This trematode species poses a serious threat to public health in this part of India and if not contained early, might spread to other and nonendemic areas of the region.\n\n【16】We also report the prevalence of trematode metacercariae in _P. globosa_ snails from foci of infections in Bihar, thus implicating this snail species as the potential source of infection. At the same time, we found metacercaria prevalence to be quite high, which is indicative of greater transmission risk to the inhabitants who are eating raw snails.\n\n【17】We found the DNA barcodes generated for life cycle stages to be unique in the entire BOLD database. Therefore, we expect these barcodes to act as references for easy and accurate diagnosis of the disease in the future.\n\n【18】We observed that some high-risk practices, such as open defecation in the infected areas, are still rampant, which is a cause of concern because this practice helps maintain the parasite cycle in the environment. However, a cleanliness program, such as the Swachh Bharat Mission started by the Government of India , and installing toilets in every household in rural areas might immensely help to contain the parasite infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fd51cfdb-dc22-4348-ae94-93aee720d5c5", "title": "Single Nucleotide Polymorphisms in Mycobacterium tuberculosis Structural Genes", "text": "【0】Single Nucleotide Polymorphisms in Mycobacterium tuberculosis Structural Genes\n**To the Editor:** A recent article by Fraser et al. discussed the frequency of single nucleotide polymorphisms (SNPs) in two genomes of _Mycobacterium tuberculosis_ , strains H37Rv  and CDC1551 (unpublished). The article contains an inaccurate representation of our published _M. tuberculosis_ data on SNP frequency. The authors state that \"detailed comparison of strains H37Rv and CDC1551 indicates a higher frequency of polymorphism, approximately 1 in 3,000 bp, with approximately half the polymorphism \\[sic\\] occurring in the intergenic regions. In other words, 50% of the polymorphisms are in 10% of the genome. While this rate is higher than that suggested , it still represents a lower nucleotide diversity than found in limited comparisons from other pathogens.\"\n\n【1】On the basis of comparative sequence analysis of eight _M. tuberculosis_ structural gene loci (open reading frames \\[orf\\]), we initially published an estimated average number of synonymous substitutions per synonymous site (K s  value) that indicated that this pathogen had, on average, approximately 1 synonymous difference per 10,000 synonymous sites . This finding was unexpected given the relatively large population size of _M. tuberculosis_ and paleopathologic evidence suggesting its presence in humans as early as 3700 B.C. Subsequent sequence analysis of two megabases in 26 structural genes or loci in strains recovered globally confirmed the striking reduction of silent (synonymous) nucleotide substitutions compared with other human bacterial pathogens . A large study (approximately 2 Mb of comparative sequence data) of 12 genes potentially involved in ethambutol resistance  and 24 genes encoding protein targets of the host immune system  provided data consistent with the original estimate of 1 synonymous nucleotide change per 10,000 synonymous sites in structural genes in this pathogen. Our estimate did not include SNPs located in putative regulatory regions of structural genes (intergenic regions), nor did it include nonsynonymous nucleotide changes in structural genes. These classes of polymorphisms were not included in our estimates because of difficulties in ruling out the possibility that they arose as a consequence of selective pressure due to antimicrobial agent treatment or perhaps extensive in vitro passage. Synonymous nucleotide changes (neutral mutations) are commonly used to estimate many values of interest to evolutionary biologists and population geneticists.\n\n【2】The estimate provided by Fraser et al. is based on a genomewide frequency of SNPs (/3,000 nucleotide sites), 50% of which presumably are located in intergenic regions and 50% in structural genes. On the basis of a genome size of roughly 4.4 Mb, there would be roughly 1,500 total SNPs, with approximately 750 in orfs (% of genome = 3,960,000 bp) and 750 in intergenic regions (% of genome = 440,000 bp). On the basis of these estimates, the frequency of all SNPs located in structural genes would be roughly 1/5,280 bp. (An estimate of 1,300 total SNPs \\[translating to 1/6,000 bp\\] was presented by the group at a meeting held at the Banbury Center last December.) As expected, these numbers differ from our estimate (/10,000), in part because they contain both synonymous and nonsynonymous nucleotide polymorphisms.\n\n【3】We analyzed orfs (available in public databases) dispersed around the chromosome of _M. tuberculosis_ strains CDC1551 and H37Rv. Surprisingly, the number of nonsynonymous SNPs exceeded the number of synonymous SNPs. We found only approximately 323 synonymous SNPs, yielding a synonymous SNP frequency of roughly 1/12,260 bp in orfs.\n\n【4】_M. tuberculosis_ , a pathogen that infects one third of humans, clearly has an unusual if not unique molecular evolution history. Precise data on the frequency of its true SNPs genomewide are critical. At this point, data  are consistent with our original estimate of 1 synonymous nucleotide change per 10,000 synonymous sites in structural genes in natural populations of this pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "042c3f9d-fc4e-442e-812a-fb677f994ebc", "title": "Escherichia coli O104 Associated with Human Diarrhea, South Africa, 2004–2011", "text": "【0】Escherichia coli O104 Associated with Human Diarrhea, South Africa, 2004–2011\n_Escherichia coli_ is the predominant microorganism of human colonic flora . It is mostly harmless to the intestinal lumen. However, some strains (diarrheagenic _E. coli_ \\[DEC\\]) can cause disease ranging from moderate-to-severe diarrhea with complications to hemolytic uremic syndrome .\n\n【1】During May–June 2011, an outbreak of bloody diarrhea and hemolytic uremic syndrome occurred in Germany and other parts of Europe . The Shiga toxin–producing _E. coli_ (STEC) serotype O104 strain was the etiologic agent of this outbreak and accounted for >4,000 cases and 50 deaths . This outbreak strain showed an unusual combination of virulence factors of STEC and enteroaggregative _E. coli_ (EAggEC). Furthermore, the outbreak strain showed extended spectrum β-lactamase (ESBL) activity .\n\n【2】Before the 2011 outbreak, _E. coli_ O104 had been reported in parts of Europe and South Korea . In South Africa, data for _E. coli_ serotypes are scarce. However, because questions arose about the ancestral origin of the 2011 outbreak strain from Germany, we investigated the occurrence of _E. coli_ O104 associated with human diarrhea in 2 surveillance programs for enteric pathogens in South Africa during 2004–2011. We also investigated phenotypic and genotypic properties of _E. coli_ O104 strains from South Africa and compare these with properties of the outbreak strain from Germany.\n\n【3】### The Study\n\n【4】The Centre for Enteric Diseases (CED) of the National Institute for Communicable Diseases in South Africa is a reference center for human infections involving enteric pathogens including DEC, _Salmonella_ spp. _Shigella_ spp. and _Vibrio cholerae_ , and it participates in national laboratory-based surveillance for these pathogens. Isolates are voluntarily submitted to the CED from ≈200 clinical microbiology laboratories across the country. The CED also recovers enteric pathogens through its involvement in the Rotavirus Surveillance Project, which started in mid-2009 and involves 5 sentinel hospital sites in South Africa. This project is involved with identification of enteric pathogens (bacterial, viral, and parasitic) associated with diarrhea in children <5 years of age.\n\n【5】All suspected DEC isolates received at the CED were identified by using standard microbiological identification techniques. Serotyping of O antigen was performed by using tube agglutination as described . Presence of H4 antigen was determined by PCR detection of the _fliC_ H4  gene . Resistance to antimicrobial drugs (ampicillin, amoxicillin/clavulanic acid, sulfamethoxazole/trimethoprim, chloramphenicol, nalidixic acid, ciprofloxacin, tetracycline, kanamycin, imipenem, ceftriaxone, and ceftazidime) was determined by using Etests (bioMérieux, Marcy l’Etoile, France), and ESBL activity was investigated by using double-disk synergy methods, as described by Clinical and Laboratory Standards Institute (Wayne, PA, USA) 2009 guidelines .\n\n【6】PCR was used to distinguish DEC strains from nonpathogenic _E. coli_ strains. The PCR consisted of 3 multiplex reactions with published primer sequences. Reactions included 0.2 μmol/L of each primer  and 1.5 mmol/L MgCl 2  . PCR thermal cycling included 35 cycles at 94°C for 1.5 min, 60°C for 1.5 min, and 72°C for 1.5 min.\n\n【7】An isolate was identified as STEC if a PCR result was positive for Shiga toxin genes 1 or 2 (stx1_ or _stx2_ ); as enterohemorrhagic _E. coli_ if a PCR result was positive for the gene coding intimin outer membrane protein and an _stx_ gene; as enteropathogenic _E. coli_ (EPEC) if a PCR result was positive for the gene coding intimin outer membrane protein; as enterotoxigenic _E. coli_ if a PCR result was a positive for genes coding heat-stable enterotoxin or heat-labile enterotoxin; as enteroinvasive _E. coli_ if a PCR result was positive for the gene coding an invasion protein; as EAggEC if a PCR result was positive for the gene coding a transporter protein; and as diffusely adherent _E. coli_ if a PCR result was positive for the gene coding an accessory protein with a function in F1845 fimbriae production. These genes are listed in Table 1 .\n\n【8】Genotypic relatedness of strains was investigated by using a PulseNet protocol  for pulsed-field gel electrophoresis (PFGE) of _Xba_ I-digested genomic DNA in a CHEF-DR III electrophoresis system (Bio-Rad Laboratories, Hercules, CA, USA) and the following electrophoresis parameters: voltage 6 V, run temperature 14°C, run time 19 h, initial switch time 6.76 s, final switch time 35.38 s, and included angle 120°. PFGE patterns were analyzed by using BioNumerics version 6.5 software (Applied Maths, Sint-Martens-Latem, Belgium). Dendrograms of patterns were created by using unweighted pair group method with arithmetic averages. Analysis of band patterns incorporated the Dice coefficient at an optimization setting of 1.5% and a position tolerance setting of 1.5%.\n\n【9】During January 2004–May 2011, CED received >4,000 suspected DEC isolates for further laboratory characterization. Of these isolates, 7 (.2%) were serotype O104. These isolates were collected from Gauteng (n = 3), Mpumalanga (n = 2), and Eastern Cape (n = 1) and North West (n = 1) Provinces of South Africa. _E. coli_ O104 was isolated more often from children <2 years of age than from older children and adults . There were more female patients (%) affected than male patients (%). Five (%) of 7 isolates were EAggEC, and 2 (%) of 7 isolates were EPEC . PCR amplification of the _fliC H4 _ gene showed that all EAggEC isolates produced H4 antigen and that all the EPEC isolates did not produce H4 antigen. Therefore, all EAggEC were serotype O104:H4 and all EPEC were serotype O104:non-H4.\n\n【10】EPEC isolates were susceptible to all antimicrobial drugs tested . EAggEC isolates were susceptible to amoxicillin/clavulanic acid, chloramphenicol, nalidixic acid, ciprofloxacin, kanamycin, imipenem, ceftriaxone and ceftazidime; were resistant to ampicillin and sulfamethoxazole/trimethoprim; and were variably susceptible to tetracycline and streptomycin . None of the EPEC or EAggEC isolates showed ESBL activity.\n\n【11】Dendrogram analysis of PFGE patterns showed that EPEC isolates were diverse and clustered at a pattern similarity of 76%, and that EAggEC isolates were highly clonal and clustered at a pattern similarity of 90% . The PFGE pattern of the 2011 outbreak strain from Germany did not match those of strains from South Africa. Therefore, the outbreak strain from Germany was determined to be unrelated to strains from South Africa. However, the strain from Germany was most closely related to the EAggEC strain cluster from South Africa (pattern similarity 85%).\n\n【12】### Conclusions\n\n【13】Our findings show that _E. coli_ O104 is rarely associated with human diarrhea in South Africa and accounts for <1% of all DEC pathotypes identified during 2004–2011 by laboratory-based surveillance and limited sentinel surveillance. Strains of _E. coli_ O104 from South Africa were mostly associated with EAggEC pathotypes; most (/7) were identified as EAggEC. Infection with this pathotype has been associated with more persistent diarrhea (duration >14 days) . Therefore, patients infected with EAggEC are more likely to have fecal cultures tested, potentially leading to greater numbers of EAggEC isolates identified.\n\n【14】In South Africa, _E. coli_ O104 infections were more commonly identified in children than in adults. Unlike the _E. coli_ O104 strain that caused the outbreak in Germany, strains of _E. coli_ O104 from South Africa did not produce Shiga toxin and did not show ESBL activity. PFGE data supported these phenotypic data, suggesting that strains from South Africa were not related to the outbreak strain from Germany. The PFGE data also showed that strains of EAggEC O104:H4 from South Africa were highly clonal. Further work is necessary to better understand the global distribution of these isolates and the role of molecular epidemiologic techniques in characterizing this newly emerging serotype.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ddb9f314-ab2f-4d04-8d64-66024e4bd631", "title": "Hepatitis E Virus and Implications for Blood Supply Safety, Australia", "text": "【0】Hepatitis E Virus and Implications for Blood Supply Safety, Australia\n**To the Editor:** Hepatitis E virus (HEV) is an emerging public health concern for industrialized countries . Although HEV infection has been associated with travel to countries where the virus is endemic, cases of autochthonous HEV are increasing . Detection of HEV RNA in blood donations in the United Kingdom, Germany, the Netherlands, Japan, and China and accumulating reports of HEV transmitted through blood transfusion highlight the potential risk this virus poses to blood supply safety .\n\n【1】In Australia, where most HEV infections are associated with travel , an average of 25 HEV cases occurred each year during 1999–2013 . HEV infection is nationally notifiable in Australia, but the presence of subclinical infections and the lack of recent seroprevalence studies have prevented the accurate estimation of HEV incidence and population exposure. Thus, we examined HEV seroprevalence in a cohort of Australian blood donors, assessed risk factors for exposure, and used the data to examine the effectiveness of current blood safety strategies for the management of HEV in Australia.\n\n【2】Plasma samples (n = 3,237) were collected from donors during August–September 2013. Information on age, sex, state of residence, new/repeat donor status, and overseas travel disclosure was obtained. Details of any relevant blood donation deferral (malaria, diarrhea) applied on previous donation attempts were also collected. Application of a specific malaria deferral code is routine for donors disclosing travel to a malaria-endemic country, and a diarrhea deferral applies when a donor reports having had diarrhea (of viral or unknown cause) 1 week before any attempted donation.\n\n【3】All samples were tested for HEV IgG by using the Wantai HEV-IgG ELISA (Beijing Wantai Biologic Pharmacy Enterprise Co. Ltd, Beijing, China). Positive samples were tested for HEV IgM by using the Wantai HEV-IgM ELISA and for HEV RNA by using a prototype transcription-mediated amplification assay (Hologic Inc. San Diego, CA, USA).\n\n【4】Of 3,237 samples, 194 (.99%) were positive for HEV IgG (% CI 5.18–6.81). Compared with estimates from previous studies that used the Wantai ELISA , our estimate is comparable to those reported from Scotland (.7%) and New Zealand (.2%) but lower than those from the United States (.8%) and southwestern France (.5%). Considerable debate exists regarding the sensitivity and specificity of HEV detection methods ; however, on the basis of studies in France and the United Kingdom , we believe that the measured seroprevalence in our study is accurate.\n\n【5】Our findings showed an increased seroprevalence of HEV associated with previous malaria deferral, diarrhea deferral, and age (multivariate logistic regression) , the latter of which is consistent with previous findings . IgG seropositivity was also higher (.73%) in donors who had traveled to a malaria-endemic country. HEV is often endemic to malaria-endemic countries ; however, the HEV exposure status of travelers is unknown before departure, so the exact place of exposure cannot be determined. Furthermore, 3.37% of donors in our study had evidence of previous HEV exposure; these donors had not reported travel outside Australia, so they may have acquired HEV locally. Because subclinical HEV infection is possible, persons infected locally may not be identified by the current donor screening questionnaire and thus pose a potential risk to blood supply safety.\n\n【6】Detection of HEV IgM in 4 (.06%) of the 194 samples from IgG-positive donors indicates the donors had been recently exposed to HEV (% CI 0.06–4.06). All 4 donors had traveled overseas; 3 reported travel to malaria-endemic countries. HEV RNA was not detected in any of the HEV IgG–positive samples. Although it is encouraging that HEV nucleic acid was not detected, the sample size is insufficient to accurately determine the true rate of HEV RNA carriage among donors in this study; a larger study is planned.\n\n【7】Management strategies to safeguard the Australian blood supply against transfusion-transmitted HEV are based on donor selection guidelines. To identify donors with possible bacteremia/viremia, including HEV, blood donation staff members ask donors several medical, behavioral, and travel-based questions before donation. These include questions relating to general wellness, sex practices, gastric upset, diarrhea, abdominal pain, and vomiting within the previous week. In addition, for 4 months after a donor’s return from travel to a malaria-endemic country, donations are restricted to plasma for fractionation. Some protection against blood donations from HEV-infected persons may occur because HEV and malaria are co-endemic to many countries. Our findings showed a higher HEV seroprevalence among donors with prior malaria or diarrhea deferrals; thus, malaria- and diarrhea-related screening questions may reduce contributions from donors with travel-associated HEV infection.\n\n【8】Our findings showed HEV exposure in travelers and nontravelers, suggesting the possibility of imported and locally acquired HEV in Australia. Prior HEV exposure was higher in donors who were temporarily excluded from donating blood on previous donation attempts, suggesting the current management strategy in Australia is partially effective in minimizing any risk of HEV transmission through blood transfusion. However, the presence of HEV IgG in donors who reported no overseas travel and/or no prior related deferrals, coupled with the knowledge that asymptomatic infection is possible, suggests that additional safety precautions may be warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c494f5c5-59ce-40ba-a6b6-6fb20ba12484", "title": "A 16-Month Community-Based Intervention to Increase Aspirin Use for Primary Prevention of Cardiovascular Disease", "text": "【0】A 16-Month Community-Based Intervention to Increase Aspirin Use for Primary Prevention of Cardiovascular Disease\nAbstract\n--------\n\n【1】**Introduction**  \nCardiovascular diseases are the leading causes of disability and death in the United States. Primary prevention of these events may be achieved through aspirin use. The ability of a community-based intervention to increase aspirin use has not been evaluated. The objective of this study was to evaluate an educational intervention implemented to increase aspirin use for primary prevention of cardiovascular disease in a small city in Minnesota.\n\n【2】**Methods**  \nA community-based intervention was implemented during 16 months in a medium-sized community in Minnesota. Messages for aspirin use were disseminated to individuals, health care professionals, and the general population. Independent cross-sectional samples of residents (men aged 45–79, women aged 55–79) were surveyed by telephone to identify candidates for primary prevention aspirin use, examine their characteristics, and determine regular aspirin use at baseline and after the campaign at 4 months and 16 months.\n\n【3】**Results**  \nIn primary prevention candidates, regular aspirin use rates increased from 36% at baseline to 54% at 4 months (odds ratio = 2.05; 95% confidence interval, 1.09–3.88); the increase was sustained at 52% at 16 months (odds ratio = 1.89; 95% confidence interval, 1.02–3.49). The difference in aspirin use rates at 4 months and 16 months was not significant (P_ \\= .77).\n\n【4】**Conclusion**  \nAspirin use rates for primary prevention remain low. A combined public health and primary care approach can increase and sustain primary prevention aspirin use in a community setting.\n\n【5】Introduction\n------------\n\n【6】Cardiovascular diseases (CVD), specifically acute myocardial infarction (AMI) and ischemic stroke, are the leading causes of disability and death in the United States . Although significant progress in the prevention, detection, and treatment of these diseases has been made, additional reductions in CVD events would provide a major societal benefit . Primary prevention strategies offer the most effective and cost-effective means to reduce CVD morbidity and mortality. One evidence-based primary prevention approach is the use of low-dose aspirin.\n\n【7】In 1989, the Physicians’ Health Study showed the benefit of low-dose aspirin in preventing AMI among men . Similar results for ischemic stroke were confirmed in the Women’s Health Study . These and other prospective randomized, placebo-controlled trials provide a high degree of confidence in the efficacy and safety of this approach . A meta-analysis of primary prevention data showed that aspirin use among healthy adults achieved a significant 12% relative-risk reduction in nonfatal CVD events . These data led the US Preventive Services Task Force (USPSTF) in 2009 to provide an A-grade recommendation for use of low-dose aspirin for primary prevention in men aged 45 to 79 years and women aged 55 to 79 years, where the potential benefit due to a reduction in AMI for men and stroke for women outweighs the potential harm due to an increase in gastrointestinal hemorrhage . This recommendation is recognized by the Centers for Disease Control and Prevention (CDC) _Healthy People 2020_ objectives as an important goal  and is supported by the American Heart Association (AHA) Guidelines for Primary Prevention of Cardiovascular Disease and Stroke , the Million Hearts initiative of the US Department of Health and Human Services, the Centers for Medicare and Medicaid Services , and studies published since the recommendation was made .\n\n【8】Despite promotion of these guidelines, studies on primary prevention aspirin use in various populations have demonstrated low rates of use ranging between 10% and 40% . In Minnesota, rates of aspirin use are low (%–35%) among aspirin candidates in the Minneapolis–St. Paul metropolitan area . Increasing rates of aspirin use for primary prevention has the potential to reduce rates of first AMI in men and first stroke in women, but the best method for increasing population-based aspirin use among those who would likely benefit from this therapy has yet to be determined.\n\n【9】Community-based interventions offer the greatest potential for population-level reductions in CVD morbidity and mortality , but they have been inconsistent in achieving clinically relevant improvements in CVD risk factors and a net change in CVD risk. Most community risk-reduction studies were designed to address multiple modifiable CVD risk factors, such as tobacco use, diet, exercise, hypertension, and cholesterol . More recent evidence shows community-wide educational campaigns to be effective in addressing CVD risk factors such as exercise, tobacco use, alcohol use, and diabetes, and shows primary care and patient approaches to be effective in addressing high blood pressure and high cholesterol . No interventions, to our knowledge, have been developed to reduce CVD morbidity and mortality through the promotion of a single cardiovascular pharmacologic approach, such as aspirin use.\n\n【10】We developed a community-based intervention, involving public health and primary care approaches, to increase primary prevention aspirin use among the USPSTF-recommended population in a medium-sized community in Minnesota. This study examines the characteristics of candidates for primary prevention aspirin use, aspirin use among candidates, and the impact of the intervention on aspirin use.\n\n【11】Methods\n-------\n\n【12】### Community setting\n\n【13】Hibbing, Minnesota, was selected for the community-based intervention. The city  has a population of approximately 16,000, is predominantly white, has a median age of 43 years, has a median household annual income of $38,000, and 28% of its residents are college graduates . It is representative of other Minnesota municipalities but may be more racially homogenous than other small cities outside the Midwest. Three health systems offer primary care to this community.\n\n【14】### Community-based intervention\n\n【15】We conducted focus groups for the general public and for primary care physicians to inform the development of the intervention and then used a 3-tiered approach for message dissemination. First, as a one-on-one approach, we developed self-assessment tools to provide individuals with a quick means to self-identify as a primary prevention aspirin candidate who should discuss aspirin use with a health care professional. These tools were distributed via primary care clinics, pharmacies, work sites, and public and private organizations. Individuals were also identified in primary care clinics as aspirin-eligible candidates and were managed directly by medical staff and physicians. Second, a group-level intervention was promoted among primary care health professionals who were provided with continuing medical education (CME)-certified training sessions (.0 credit hour for physicians, physician assistants, and nurses) and clinic tools to identify candidates and facilitate aspirin prescriptions. Fifty-three percent (/30) of the primary care physicians and nurse practitioners and 66% (/85) of the primary care medical staff (registered nurses, licensed practical nurses, and medical assistants) across the 3 health systems completed the CME-certified training sessions. Finally, a community-wide intervention included a mass media campaign that directed individuals in the USPSTF age- and sex-recommended candidacy range to consult with their health professionals to complete an individual risk assessment. The intervention also created partnerships with community organizations to address use of aspirin at the population level for primary prevention.\n\n【16】Primary prevention aspirin messages were disseminated in 2 sequential waves from March 2012 through June 2012 and from January 2013 through April 2013 through various media and public outlets, including daily print advertisements, 30 radio spots per week, billboards, online advertisements, and a website. A formal commitment to program goals through a signed memorandum of understanding was obtained from the 3 health systems. Program staff completed all CME-certified training sessions in the primary care clinics in the health systems by April 2012, and aspirin-candidacy clinic tools were used through April 2013.\n\n【17】### Evaluation design and assessment\n\n【18】Independent cross-sectional samples of community residents were surveyed at baseline, at 4 months (after the first media campaign), and at 16 months (after the second media campaign). The survey sampling frame included residents in households with landline telephones located in the city zip code area. Study contact consisted of 1 informational letter followed by as many as 10 call attempts per household. Trained interviewers from the Minnesota Center for Survey Research  screened telephone respondents and administered a 10-minute telephone survey to men aged 45 to 79 years and women aged 55 to 79 years. For the baseline and 4-month surveys, we administered the survey to both primary and secondary prevention candidates. Because our study focused on primary prevention aspirin use, we did not survey secondary prevention candidates for the 16-month survey; we surveyed only primary prevention candidates. We oversampled women in the 4-month and 16-month surveys to achieve a balanced sex ratio. Response rate was defined as the percentage of completed interviews among telephone-to-person contacts made, excluding sex- or age-ineligible respondents. Data collection methods precluded a precise estimate of individual Framingham risk scores .\n\n【19】The telephone survey included questions designed to evaluate sociodemographic characteristics, history of CVD and atherosclerosis risk factors, aspirin use, psychosocial characteristics, and exposure to CVD health messages. Consistent with AHA and USPSTF definitions , candidates for aspirin use for primary prevention were defined as individuals who did not have a self-reported history of CVD, including heart attack, ischemic stroke, atherosclerotic lower extremity peripheral artery disease (PAD), or any heart-related, stroke-related, or PAD-related revascularization procedure. Aspirin use was examined by using the following question: “How often do you take aspirin?” Regular aspirin use was defined as use daily or every other day. Nonregular aspirin use was defined as once per week or less. No minimum duration of aspirin use was assessed in defining regular use. Individuals in the baseline sample were also invited to have their blood drawn to validate self-reported aspirin use through a serum thromboxane B 2  (STxB 2  ) measurement (Thromboxane B 2  EIA Kit, Cayman Chemical, Ann Arbor, Michigan). Self-reported aspirin use from this survey was validated in a related study to be an accurate representation of actual use via measurement of STxB 2  levels . Approval for the study was obtained from the University of Minnesota institutional review board.\n\n【20】### Analysis\n\n【21】Most analyses were limited to candidates for aspirin use. Regular aspirin use was compared across the surveys. Unadjusted odds ratios (ORs) with 95% confidence intervals (CIs) were calculated by using Stata version 12 (StataCorp LP, College Station, Texas). Logistic regression analyses found that sociodemographic variables did not confound associations, so we calculated unadjusted ORs only. We used χ 2  tests for categorical variables and _t_ tests and analysis of variance for continuous variables.\n\n【22】Results\n-------\n\n【23】Telephone survey response rates were 56% at baseline, 51% at 4 months, and 40% at 16 months. The sociodemographic distribution of the respondents was similar across the surveys except for the oversampling of women at 4 months and 16 months . The sociodemographic distribution was also representative of the community population.\n\n【24】Cardiovascular comorbidities and risk factors were similar across survey samples; approximately 11% reported having a heart attack, 4% a stroke, and 10% PAD. Approximately 19% reported having diabetes, 54% high cholesterol, and 47% high blood pressure; 13% were current smokers. Approximately two-thirds of the baseline and 4-month samples were primary prevention candidates. The distribution of age, sex, and risk factors suggest we identified an initially appropriate candidate pool for primary prevention through population-based measures, despite lack of Framingham risk scores.\n\n【25】Of the 103 people in the baseline sample, 54 (%) agreed to participate in a substudy to adjudicate the accuracy of self-reported aspirin use. Of these, 31 reported aspirin use, and 29 (%) had their aspirin use confirmed by the STxB 2  measurement.\n\n【26】The demographic and socioeconomic distribution of the aspirin use candidates was similar at baseline, 4 months, and 16 months . Among the 3 samples, the mean age ranged from 62 to 64 years, and approximately two-thirds were married. More than 90% of the primary prevention candidates were white, and more than 94% had health insurance coverage, reflecting the demographic characteristics of the community population.\n\n【27】The associations between the sociodemographic subgroups and aspirin use were also similar across the primary prevention samples . Increasing age was significantly associated with higher rates of aspirin use in all 3 samples; primary prevention candidates who used aspirin regularly were more likely to be older (aged 65–79) than younger (aged 45–64). Sex and education were not associated with aspirin use in any of the 3 samples. Income was significantly associated with aspirin use but only in the 4-month survey; regular aspirin users were more likely to have a low annual household income (<$30,000) than an income of $30,000 to $60,000 or more than $60,000. In addition, marital status was significantly associated with aspirin use but only in the 16-month survey.\n\n【28】At baseline, 36% of the primary prevention candidates reported regularly using aspirin; at 4 months, the percentage was 54%. We found a 2-fold difference in regular aspirin use between the baseline and 4-month survey (OR = 2.05; 95% CI, 1.09–3.88) and between the baseline and 16-month survey (OR = 1.89; 95% CI, 1.02–3.49). Primary prevention aspirin use rates were sustained at 16 months (% at 4 months, 52% at 16 months; _P_ \\= .77).\n\n【29】At 4 months, 46% of primary prevention candidates and at 16 months, 63% of candidates indicated they had seen or heard the program messages in their community or workplace. When asked to identify their reasons for initiating aspirin use, most primary prevention candidates who took aspirin regularly to prevent a heart attack or stroke did so in response to a recommendation by their health care provider (% at 4 months, 71% at 16 months); the next most common reason was response to media advertisements (% at 4 months, 46% at 16 months). Fewer than 30% of candidates in both surveys reported initiating aspirin use in response to a friend or family recommendation, a close personal experience with someone who suffered a heart attack or stroke, or community information.\n\n【30】Aspirin use discussions initiated by health care providers also increased among primary prevention candidates, but this increase was not significant (% at baseline; 19% at 4 months, 22% at 16 months). Regular aspirin users were significantly more likely than nonregular aspirin users to have had aspirin discussions with their health care provider at 4 months (OR = 4.62; 95% CI, 1.62–13.14) but not at 16 months (OR = 1.68; 95% CI, 0.69–4.10).\n\n【31】Regular and nonregular aspirin users differed significantly from each other on several measures of aspirin-related attitudes, perception of CVD risk, and social norms at 16 months . Regular aspirin users believed more strongly than nonregular aspirin users that aspirin use can help prevent a heart attack or stroke (P_ < .001) and that daily aspirin use is safe (P_ \\= .001) and effective (P_ \\= .04). The personal perception of CVD risk was similar among regular and nonregular aspirin users; both groups believed their chances of having a heart attack or stroke were low. The social norms among both groups differed significantly. Regular aspirin users believed more strongly that people similar to them take daily aspirin (P_ < .001) and that people close to them recommend they take aspirin (P_ < .001).\n\n【32】Discussion\n----------\n\n【33】During the past 25 years, the evidence base has expanded to demonstrate that aspirin use can prevent a first AMI in men and first ischemic stroke in women. Nevertheless, there is a paucity of research on how a community-based intervention might effectively encourage aspirin use for primary prevention to achieve the promised efficacy . Representative estimates from national  and state  population-based studies show aspirin use rates ranging from 10% to 40% among adults who are likely to benefit from this therapy . Our study showed a self-reported rate of aspirin use of 36% at baseline, which is comparable to the historical rate (%), but which increased to 54% after 4 months and was sustained at 52% at 16 months. This increase is greater than that observed during past temporal trends.\n\n【34】Studies examining the association between sex and aspirin use  have shown inconsistent findings. Our study found no differences in aspirin use between men and women and no differences between regular and nonregular aspirin users in educational level. The homogeneity of our study participants in race/ethnicity and health insurance coverage prevented a meaningful evaluation of these factors, which are predictors of regular aspirin use and modify use of other prevention-directed medications.\n\n【35】Similar to regular aspirin users in other studies , aspirin users in our study were more likely to be older (aged 65–79). Because we included only individuals who met the USPSTF age and sex recommendations for primary prevention aspirin use, our study suggests that interventions may provide the greatest benefit for those aged 45 to 64, who are at higher risk of CVD events but who may not consider themselves to be at risk. Furthermore, regular aspirin users had a more favorable attitude toward aspirin use, and as in another study , they were more likely to perceive aspirin use as a common behavior in their social network. This latter finding supports the importance of influencing social networks to achieve aspirin use.\n\n【36】Community-based interventions offer an important means to increase population-based aspirin use for primary prevention. Studies on community-based interventions have demonstrated variable success in reducing CVD risk factors, morbidity, and mortality . Reasons for limited success may relate to the complexity of simultaneously targeting multiple CVD risk factors and multiple behavioral changes. In contrast, focusing on 1 message and 1 behavioral change to increase primary prevention aspirin use, as our study did, may be more manageable. Sequential 1-message interventions may be less overwhelming to target populations than multiple-message interventions.\n\n【37】Our study suggests that CVD interventions benefit from the involvement of both health care and non-health–care public and private community partners as either intervention facilitators or conduits of CVD prevention messages, as found previously . Media campaigns complement primary care aspirin initiatives by encouraging the public to initiate aspirin-related discussions with their health care providers. Health care professionals then play a strong role in helping individuals decide to use aspirin as a primary prevention measure . In line with a recent Institute of Medicine brief , maximal synergy and impact can be achieved by the sharing of information, resources, expertise, and credit across the primary care and public health communities.\n\n【38】Our study has several limitations. The evaluation provided a pre- and post-intervention design without a nonintervention comparison group. This design cannot fully account for temporal trends in societal aspirin use; nonetheless, the magnitude of change associated with the intervention far exceeds any temporal trends. Although CVD risk and aspirin use in our study may be subject to self-report bias, this bias is likely minimal, according to the work that adjudicated self-reported aspirin use via STxB 2  measurement. This intervention was not designed, at this initial community dissemination stage, to evaluate the potential risk that could be incurred if inappropriate aspirin use were promoted. Individual 10-year CVD risk profiles based upon the Framingham Heart Study were not calculated for respondents, because we could not collect the physiologic data required to perform the calculation. Instead, candidacy for primary prevention aspirin use was determined by using the 2009 USPSTF age and sex recommendations, which are based on the Framingham risk scores. We did not examine contraindications to aspirin use, such as aspirin allergy, history of gastrointestinal bleeding, and the use of other antithrombotic or anti-inflammatory medications. Thus, it is not known whether regular aspirin users included individuals who should not be taking aspirin because of existing contraindications.\n\n【39】Rates of aspirin use for the primary prevention of CVD remain relatively low. Increasing appropriate aspirin use among a high-risk target population per USPSTF guidelines could reduce first AMI events in men and first ischemic stroke events in women and likely achieve these benefits cost-effectively. A combined public health and primary care approach, through the delivery of a community-based intervention, may offer an effective and relatively rapid means to increase and sustain aspirin use rates for primary prevention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3c0d51af-dacf-4600-b17f-36ab0d59f1f6", "title": "Scale-up of Multidrug-Resistant Tuberculosis Laboratory Services, Peru", "text": "【0】Scale-up of Multidrug-Resistant Tuberculosis Laboratory Services, Peru\n### Background\n\n【1】TB incidence in Peru is among the highest in Latin America, at 108.2/100,000 persons in 2005  . In the densely populated periphery of Lima, where half of all national cases are detected, the risk for infection with _Mycobacterium tuberculosis_ may be among the highest recently documented . Rates of MDR TB are also high, with a national prevalence of 3% among patients never treated for TB and 12.3% among previously treated patients . During 1990–2000, Peru implemented a model program based on the World Health Organization (WHO)–endorsed strategy of directly observed treatment, short course (DOTS) . Massive use of sputum smear microscopy and standardized first-line treatment resulted in effective case detection and cure, with an overall decrease in TB incidence by the end of the decade . During that period, however, the rates of MDR TB increased .\n\n【2】Because DOTS alone was insufficient to control ongoing transmission of drug-resistant strains , Partners in Health (PIH), Harvard University, Massachusetts State Laboratory Institute (MSLI), Socios en Salud, the Peruvian National Tuberculosis Control Program (NTP), and the Peruvian National Institute of Health (INS) initiated a collaborative MDR TB treatment effort in 1996 . Principles included individualized MDR TB treatment and monthly culture to monitor treatment response. Community health promoters provided direct observation of all doses given outside health clinic hours. In 1997, the NTP implemented a standardized MDR TB treatment regimen, which achieved cure rates <50% . Although protocols changed over time, treatment failures, defaulters, and relapses after first-line treatment were generally referred for standardized MDR TB therapy. Those patients whose standardized treatments failed were, in turn, referred for individualized treatment.\n\n【3】##### Expansion of Laboratory Capacity, 1996–2000\n\n【4】When we began this project, 1 level III laboratory, the National TB Reference Laboratory, performed DST on first-line drugs; 57 level II laboratories performed mycobacterial culture, and ≈1,000 level I laboratories had smear microscopy capacity . Because DST on second-line drugs was not available in Peru, isolates were initially sent to the MSLI until local capacity could be established.\n\n【5】As the MDR TB treatment program expanded in absolute numbers and geographic coverage, so too did demand for laboratory services. From 1996 through 2000, the number of mycobacterial cultures and DSTs performed yearly more than doubled . The process of program scale-up posed additional challenges in patient management, information systems, drug procurement, and regional implementation. Responding to these needs, the Bill & Melinda Gates Foundation awarded a grant for $45 million in 2000 to establish a consortium called PARTNERS, whose principal task was to achieve national coverage of MDR TB treatment in Peru and replicate this project elsewhere. Several key institutions were added to the initial group of collaborators: WHO, the Centers for Diseases Control and Prevention (CDC), and the Task Force for Child Survival and Development. Within the PARTNERS consortium, the Laboratory Improvement Project was established with specialists from MSLI, CDC, Harvard University, PIH, and INS.\n\n【6】##### Strategy to Scale-up Laboratory Services\n\n【7】NTP norms for DST indications have evolved over the past 10 years. This heterogeneous and dynamic process provided lessons on matching the choice of DST to programmatic strategies . Salient aspects guiding laboratory strategies include the choice of standardized versus individualized treatment, criteria for performing DST, rates of HIV and resistance to second-line drugs, and empiric management while awaiting results.\n\n【8】On the basis of projected numbers, DST needs would not be met unless DST on first-line drugs was decentralized to regional laboratories in areas with high rates of TB and MDR TB. In choosing methods for decentralized DST, the INS matched method features with available resources in regional laboratories . The need for a rapid DST method was clear. Given that it took an average of almost 5 months to obtain results from a conventional DST performed in Peru , physicians often had to make treatment decisions empirically. Once results did arrive, they were no longer accurate because patients had been exposed to additional drugs in the interim, to which amplified resistance could have occurred. Rapid DST implemented at the decentralized level would be the most effective way of providing timely results and decompressing the central bottleneck of DST demand.\n\n【9】The INS decided that rapid DST should serve as an initial screening test. By quickly identifying resistance to isoniazid and rifampin, isolates with drug resistance could be sent to INS for full DST while standardized MDR TB treatment was started. With input from MSLI, the INS chose the Griess method. This method is a rapid colorimetric method that uses Lowenstein-Jensen (LJ) medium prepared with antimicrobial drugs  . Previously the method was validated as an indirect method; however, INS opted to implement it as a direct method, i.e. it is performed directly with sputum. INS validation of this method yielded sensitivities and specificities of 99% and 100% to isoniazid and 94% and 100% to rifampin . Attributes of the Griess method are accuracy, fast turnaround time (days), minimal additional equipment needs, inexpensive materials and reagents, and reproducibility in laboratories proficient in mycobacterial culture.\n\n【10】On the basis of this rationale, the following plan was developed. Second-line DST (agar plate proportions method) would be implemented in the INS. Conventional first-line DST (proportions method, indirect variation by LJ medium) would be performed at regional laboratories. Direct Griess method would be performed at regional laboratories; and the indirect BACTEC-460 system (Becton Dickinson, Franklin Lakes, NJ, USA) for first-line drugs would be implemented at INS for high-risk patients, including healthcare workers, HIV-positive patients, and pediatric patients.\n\n【11】Another priority was reducing the overall turnaround time of laboratory data, defined as the time when the patient is first identified at risk for MDR TB to the time that this determination has an effect on patient care. Before DST decentralization, we conducted an assessment of turnaround times in 2 health districts and confirmed that laboratory efficiency, including decentralization of DST and implementation of rapid methods, would have limited effect if pre- and post-DST processing delays were not addressed . These delays included specimen transport, specimen processing, dissemination of results to the health center, and scheduling of clinical evaluation once results were obtained. Of 924 samples processed over 16 months, the median turnaround time was 147 days; only 81 days were caused by DST processing. On the basis of these data, we worked with leaders at national and regional levels to develop and implement strategies to reduce delays .\n\n【12】The overall strategy for laboratory scale-up comprised the following activities. First, establish clear criteria for performing DST. Second, select DST methods for use within the TB program and indications for each method. Third, decentralize first-line DST to 7 regional laboratories. Fourth, project the quantity of DST and cultures and ensure adequate supplies. Fifth, create biosafe laboratory facilities for DST. Sixth, train laboratory personnel on new methods. Seventh, train healthcare providers and level I laboratory personnel on DST indications. Eighth, validate DST methods, first in the INS and then at each implementing site. Ninth, establish and enact quality control and quality assurance protocols. Tenth, eliminate additional delays in specimen transport and result reporting. These strategies were used and modified in 3 phases of scale-up: preparation, implementation, and monitoring.\n\n【13】##### Preparation Phase\n\n【14】Key elements of the preparation phase were mobilizing political commitment (i.e. agreeing upon the strategic plan, obtaining adequate financial and human resources, and formalizing collaborations and the respective roles of different, competing and cooperating, institutions); establishing adequate laboratory infrastructure; and forming a skilled workforce. A needs assessment performed early in the project identified the need for documented biologic safety cabinet (BSC) certification and maintenance and repair of BSCs throughout the TB laboratory network. Because Peru had no trained personnel who could certify BSCs, a training program was developed and delivered with the help of MSLI and the Eagleson Institute in Sanford, Maine. The trained certifiers then certified and repaired BSCs for the TB laboratory network.\n\n【15】To proceed with decentralization efforts, INS contacted directors of regional laboratories. Only 1 of the laboratories met minimal space and biologic requirements to safely perform DST. The remaining 6 laboratories were asked to submit a proposal for laboratory renovations; only 3 were able to respond in a timely fashion. We explored why the other 3 laboratories did not respond and found that the administrative time and technical expertise required to elaborate a proposal was often not within the capacity of district and laboratory leaders.\n\n【16】We supported 2 laboratory renovations and discovered that substantial time and resources were required to complete this process. Producing detailed and thorough technical proposals required substantial input from a range of experts, including architects; building, sanitary and electrical engineers; and construction companies. We identified experts with interest and competence in designing TB health facilities and encouraged collaboration by team, with technical assistance from an engineer experienced in TB infection control at CDC. Cultivating such a team with specialized knowledge in TB infrastructure has proven to be an asset for Peru. This team has since worked on other projects to renovate TB clinics and laboratories.\n\n【17】Once elaborated, the proposals then required approval by the governmental institution responsible for approving renovations and construction of public health facilities. Construction for both projects was delayed by an average of 6 months because of these administrative requirements. District and laboratory leaders played an important role by making frequent inquiries into the status of the approval process. In the meantime, we purchased necessary equipment, materials, and supplies.\n\n【18】Another step to expand DST capacity was the training and validation process for each DST method. MSLI trained INS in DST to second-line drugs by the agar plate proportion method; validation was completed in 2005. Concomitantly, INS trained regional laboratory personnel in DST of first-line line drugs, by the LJ medium proportions method. To initiate rapid DST, the Griess method was validated first at INS; then personnel from each implementing laboratory were trained in the method. Both conventional DST and rapid DST were validated at the regional laboratories. Samples were collected under program conditions. DST was performed by trained personnel in the regional laboratories. These same strains were then sent to INS for validation.\n\n【19】INS also validated BACTEC against LJ medium proportions and sped the process by performing BACTEC culture followed by indirect BACTEC DST on first-line line drugs. Validation was done for the AccuProbe method (Gen-Probe, Inc. San Diego, CA, USA) to identify _M_ . _tuberculosis_ and _M_ . _avium_ complex. Finally, INS leaders developed standard operating procedures, including protocols for all laboratory methods, biosafety and equipment standards, and quality assurance and quality control procedures.\n\n【20】Other activities during the preparation stage were aimed at reducing turnaround time. We developed and piloted an electronic laboratory information system connecting INS, regional laboratories, and health centers to provide health personnel (physicians, nurses, and laboratory technicians) with real-time access to culture and DST results. To support the system, we worked with health district leaders to provide Internet access, computers, and Web access points at health centers . We also purchased 2 automobiles to aid in specimen transport. At the administrative level, NTP increased the frequency of MDR TB treatment–approval meetings to reduce the bottleneck of cases pending approval for initiation of MDR TB treatment.\n\n【21】##### Implementation Phase\n\n【22】After successful completion of validation procedures in regional laboratories, DST was incorporated into programmatic services. Aggregate data on DST results were reviewed by each laboratory on a monthly basis to monitor rates of contamination, culture growth, and drug resistance. INS supervisors made frequent visits to these laboratories to monitor performance and troubleshoot any challenges. For instance, when low rates of culture growth were observed among acid-fast bacilli smear-positive samples, smear microscopy slides from these samples were reviewed by a biologist and decontamination protocols were reviewed. During this period, we simultaneously trained healthcare personnel in workshops and one-on-one interactions. Laboratory and TB program directors led workshops to review programmatic norms for soliciting each DST method and to explain the performance and characteristics of each method. Health workers were also trained to use the laboratory information system. Regional administrators trained providers in patient confidentiality and established a plan for sustained Internet access and computer maintenance after the pilot phase of the information system. We secured the commitment of health center directors to guarantee that TB personnel would have access to the computers during designated hours because computers were rarely placed in the TB services areas to reduce the risk for theft and vandalism.\n\n【23】##### Monitoring Phase\n\n【24】Sustainable laboratory infrastructure depends on administrative commitment and monitoring laboratory performance quality. Throughout the entire planning and implementation stages, MSLI provided training to INS and regional laboratories in basic and method-specific quality control/quality assurance.\n\n【25】The appropriate use of DSTs and culture data by healthcare workers also required ongoing evaluation. Preliminary data demonstrate that despite the reinforcement of NTP norms, health personnel often failed to adhere to NTP norms for DST . Approximately 50% of DSTs in 2005 in Lima were requested for patients without an indication for testing by NTP norms. Of DSTs not meeting NTP norms, ≈28% of these were for patients who had MDR TB compared with 32.5% among those with NTP criteria. These findings support the need for broadened indications for DST. Monitoring laboratory and programmatic performance was not effective unless these data were fed back to healthcare personnel. An example is a series of reports generated by the information system and provided to laboratory and regional TB program directors  .\n\n【26】TB management protocols, such as DST indications and optimal DST methods, are dynamic; they must respond to changes in regional epidemiology as well as the availability of resources. For example, decentralization of DST resulted in an increased demand for DST because of increased awareness of MDR TB and availability of testing. Additionally, health professionals and patients perceived the benefit of rapid, real-time laboratory data. This increase in demand is an example of how our ongoing monitoring and evaluation could be applied to reassess the use and capacity of laboratory services. Our preliminary data of adherence to NTP indications for DST  and rates of MDR TB among risk groups  have helped inform modifications of NTP policy. The experience thus far in matching the appropriate DST methods to NTP norms should enable a rational application and operational assessment of promising new DST methods . Without adequately quantifying and responding to an increase in DST demand, laboratory operations may become bottlenecked, and excessive demand on limited personnel could result in deviations from laboratory protocols and a decrease in laboratory performance. Figures 1 and 2 reflect the level of laboratory expansion in Peru as of 2006, which demonstrates the trajectory of scale-up, not only in terms of DST, but for culture as well.\n\n【27】### Lessons Learned\n\n【28】TB programs faced with incorporating MDR TB treatment must often expand laboratory infrastructure far beyond existing capacity. Although laboratory improvement efforts in Peru have taken a decade to accomplish and are still evolving, several key lessons can be distilled from our experience.\n\n【29】##### Responding in Time and Stepwise, Overlapping Efforts to Prevent Delays\n\n【30】The introduction and decentralization of DST and culture capacity can involve a wide range of activities, ranging from obtaining permits from national authorities to purchasing automobiles to streamline specimen transport. Attention to detail, the dedication of human resources to push these activities along, and parallel planning and coordination of activities can receive inadequate priority among program planners. Although these logistics can be painfully mundane, they are often the greatest obstacles, thus indirectly causing the most serious illness due to excessive delays. The recent outbreak of XDR TB among HIV-positive populations in KwaZulu-Natal, South Africa, demonstrates the need to scale-up laboratory services in a timely but correct manner .\n\n【31】##### Coordination of National Reference Laboratory and National TB Programs\n\n【32】Political commitment must include stable leadership; a strong central, coordinating unit; and a working relationship between TB laboratories and a TB program . The importance of coordinating laboratory and programmatic efforts may seem obvious but cannot be overstated. Within the DOTS model, smear microscopy can be performed at health centers with local coordination with TB services. In contrast, MDR TB treatment requires more complex methods (culture, DST) and is usually performed and overseen at a central site. Strategies must be informed by NTP policy and vice versa. Coordination must persist because the needs of a TB program will likely change over time.\n\n【33】##### Importance of Operational Research\n\n【34】Our experience in Peru was informed by our operational research. The profile of a DST method and its characteristics, when first validated in a local laboratory, may be different from its performance, strengths, and weakness when it is operating under actual program conditions. Operational assessment of a laboratory method or strategy is the sole means of understanding its effectiveness when considered within the larger context of how the method is used, associated complexities or challenges in its implementation, the mitigation of its effect caused by other system delays, and other factors. If tools to monitor laboratory performance are incorporated into information and reporting systems at the outset, effective operational research can be conducted with minimal additional resources, coupled with ongoing feedback, to create a sustainable laboratory system.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "706169b3-ba2d-466b-bd71-3d1002b0b502", "title": "Zika Virus Infection in Patient with No Known Risk Factors, Utah, USA, 2016", "text": "【0】Zika Virus Infection in Patient with No Known Risk Factors, Utah, USA, 2016\nZika virus is an emerging mosquitoborne flavivirus transmitted primarily through the bite of infected _Aedes_ (Stegomyia_ ) mosquitoes. Other modes of transmission, including intrauterine, perinatal, sexual, blood transfusions, and laboratory exposure, have been described .\n\n【1】In June 2016, a 73-year-old man (index patient) died in a hospital in Salt Lake City, Utah, USA  . He had returned from Mexico 11 days previously and began feeling ill 3 days after his arrival in the United States. He sought care 2 days after illness onset and was hospitalized 3 days later. After admission, his health rapidly declined, and he died 3 days later of suspected dengue hemorrhagic shock syndrome. Postmortem testing identified Zika virus RNA in a blood sample obtained during hospitalization; the level of viremia in his serum sample was uncharacteristically high .\n\n【2】Six days after the death of the index patient, subjective fever, rash, and conjunctivitis developed in a 38-year-old man who was a family contact (patient A) . Patient A had not traveled to an area with ongoing Zika virus transmission, had not had sexual contact with a person who recently traveled to such an area, and had not received a blood transfusion or organ transplant. However, patient A had contact with the index patient during his period of viremia. Patient A also visited the 2 residences of the index patient after his death, suggesting possible vectorborne transmission from _Ae. aegypti_ mosquitoes, which have been previously identified in Utah . Urine obtained from patient A 7 days after illness onset was positive for Zika virus RNA, and a day 11 serum sample was positive for Zika virus IgM and Zika virus and dengue virus neutralizing antibodies . Given the lack of travel or other risk factors for acquiring Zika virus for patient A, a public health investigation was launched to better define his exposures and determine a probable source of infection.\n\n【3】### Methods\n\n【4】For this investigation, we defined a contact as a person who resided in the same household with the index patient or who had direct contact with the index patient or his blood or other body fluids, such as conjunctival discharge, respiratory secretions, vomit, stool, or urine, when he was potentially viremic (defined as the date the index patient returned to the United States until his death). Evidence of recent infection was defined as a person with Zika virus RNA in serum or urine or Zika virus IgM and neutralizing antibodies in serum samples that were negative for neutralizing antibodies against dengue virus . We obtained consent for all persons who provided a sample or participated.\n\n【5】##### Assessment of Person-to-Person Transmission\n\n【6】##### Evaluation of Family Contacts and Mortuary Workers\n\n【7】State and local health department staff interviewed all family members and friends identified as potential contacts of the index patient from the date when he was found to be positive for Zika virus through when patient A showed a positive test result. Interviewers asked about exposures to the index patient and any recent travel or vaccination that might affect Zika virus test results. All contacts were asked to provide a blood or urine sample to test for recent Zika virus infection. All community funeral and mortuary workers who had contact with the body of the index patient met the definition of a contact and were asked about their exposures and to provide a blood sample.\n\n【8】##### Assessment of Healthcare Personnel\n\n【9】Hospital medical records of the index patient were reviewed to characterize and quantify clinical conditions and procedures that generated blood or body fluid. A list of all healthcare personnel who potentially interacted with the index patient was generated on the basis of employee assignments. Healthcare personnel with possible contact were called and an interview was scheduled. If after 2 attempts they could not be reached, they were categorized as not reachable. Healthcare personnel were interviewed to determine the level of interaction (e.g. type of contact, type of care provided, exposure to blood or body fluids, use of personal protective equipment \\[PPE\\]); recent travel; and vaccinations. Healthcare personnel were defined as having other concerning factors if they reported being pregnant, were attempting to become pregnant, or had \\> 2 signs/symptoms consistent with Zika virus infection (i.e. fever, rash, arthralgia, or conjunctivitis) in the previous 30 days. All employees with direct contact or other concerning factors were asked to provide blood or urine samples for Zika virus testing .\n\n【10】##### Assessment of Potential Vectorborne Transmission\n\n【11】##### Vector Assessment\n\n【12】Local mosquito abatement districts worked in collaboration with the Centers for Disease Control (CDC, Fort Collins, CO, USA) to conduct larval and adult mosquito surveillance in the 3 areas where the index patient (residences) and patient A resided. Door-to-door household surveys were conducted, and light traps, CO 2  traps, gravid traps, and BioGents traps (BioGents, Regensberg, Germany) were deployed to collect different species at sites around residences of patient A and the index patient. During mid-July, mosquito abatement district adult mosquito collection was performed for 2 days. Five days after the first trapping, CDC and local mosquito abatement districts collected mosquitoes at 12 sites in each of the 3 areas described. Specifically, 18 BioGents-2, 6 gravid, and 6 CDC light traps with CO 2  were used for 30 traps/day. Ovicups were set at 9 sites to detect container-inhabiting _Aedes_ mosquitoes. Adult mosquitoes were shipped on dry ice to CDC in Fort Collins for processing and testing.\n\n【13】Potential larval and pupal habitats were inspected at the 3 residences of interest and nearby homes. Aquatic stages were collected and transported to mosquito control district facilities for rearing and identification.\n\n【14】##### Community Assessment\n\n【15】All households within a 200-m radius of the 2 properties where the index patient stayed while potentially viremic were surveyed. We determined this radius on the basis of a compromise between the likely movement of _Aedes_ mosquitos (estimated as ≈30–450 m/d) and the number of households that could be surveyed . A household was eligible for inclusion if \\> 1 resident of the house had resided in the household for the month before illness onset of patient A. A person who slept in the house \\> 2 days/week was considered a household member.\n\n【16】Teams visited households in late July for 4 days, provided information about the investigation, and obtained verbal consent. If the person did not wish to participate, they were considered refusing. If no residents or heads of household were available at the initial visit, they were revisited 2 more times at a different time and day. If after 3 visits no one answered, the household was considered not available. For participating households, verbal consent or assent to participate was obtained from all household members \\> 12 years of age. For children <18 years of age, permission was obtained from parents or legal guardians.\n\n【17】We surveyed all consenting household members by using a questionnaire that captured information on demographics, signs/symptoms of possible Zika virus infection, recent travel or sexual contact with a traveler, receipt of flavivirus vaccines, pregnancy status, exposures to mosquitoes, and personal and household protective measures. We defined signs/symptoms of possible Zika virus infection as fever, rash, conjunctivitis, or arthralgia with onset after the date of return of the index patient to the United States. Persons whose signs/symptoms began before the return of the index patient or were explained by an alternate etiology (e.g. culture-proven bacterial infection) were not included among those with reported signs/symptoms. After completion of the survey, we asked each household member \\> 6 months of age to provide a blood sample; for participants symptomatic within the previous 2 weeks, a urine sample was also collected. We did not collect Samples from infants <6 months of age because interpretation of results would be complicated by maternally derived antibodies. However, parents did respond to the questionnaire for those infants.\n\n【18】##### Laboratory Testing\n\n【19】Serum samples were tested by using a Zika virus IgM capture ELISA at CDC (Fort Collins) or the Utah Public Health Laboratory (Salt Lake City, UT, USA) per standard protocol . Samples positive for Zika virus IgM were confirmed by using a 90% plaque reduction neutralization test at CDC (Fort Collins) . Urine samples were tested by using a reverse transcription PCR (RT-PCR) (Trioplex assay) for Zika virus at the Utah Public Health Laboratory . If a person reported signs/symptoms in the past week, then their serum sample was also tested by RT-PCR for Zika virus RNA per testing guidelines . Mosquito pools were tested for Zika virus and West Nile virus RNA by using described methods .\n\n【20】##### Data Collection and Analysis\n\n【21】We entered survey data with unique identification numbers for participants into either REDCap  or Epi Info (CDC) databases for analysis. We summarized continuous variables as medians and ranges and dichotomous variables as frequencies and proportions. For the community assessment, we first estimated the number of persons residing in the areas around residences of the index patient on the basis of average household size values at the ZIP code level obtained from the 2010 US Census . We then used the hypergeometric distribution to calculate the probability that a nonparticipant residing in the areas near the residences of the index patient could have been infected with Zika virus.\n\n【22】Procedures and data collection tools for healthcare personnel and community assessments were reviewed by human subject advisors at the Utah Department of Health and CDC. These procedures and tools were determined to be part of a nonresearch public health response.\n\n【23】### Results\n\n【24】##### Evaluation of Family Contacts and Mortuary Worker\n\n【25】A total of 22 family members or friends potentially interacted with the index patient from his return to the United States until after his death; 19 (%) met the definition of a contact. Of the 19 family contacts, 15 resided with or visited the index patient at his residences, and 13 visited him at the hospital. The most common interactions with the index patient included kissing, primarily on the cheek (n = 6), and assisting in care (e.g. cleaning up vomit, stool, or urine, or wiping tears) (n = 6). These activities were performed without PPE. Twelve community mortuary workers also interacted with the index patient in the hospital or mortuary, and all met the definition of a contact. Other than patient A, no other contact reported a Zika-like illness after their interaction with the index patient.\n\n【26】Of the 19 family or friend contacts, 18 were negative for Zika virus IgM in serum (n = 14) or Zika virus RNA by PCR in urine (n = 17). Only patient A had recent evidence of Zika virus infection. All 12 mortuary workers were negative for Zika virus IgM in serum.\n\n【27】The most recent travel of patient A was to Mexico >1 year earlier. He had not had sexual contact with someone who had recently traveled to an area where Zika virus was known to be circulating and had not received any blood transfusions or organ transplants. He did not have any serious underlying conditions and was not immunosuppressed. Similar to other family members, patient A visited the residences where the index patient was staying before the index patient was hospitalized and after his death. Before hospitalization of the index patient, patient A had only casual contact (e.g. hugging and kissing) with the index patient.\n\n【28】During hospitalization of the index patient, patient A reported staying for 2 days and nights (>48 hours) in his room in the intensive care unit (ICU) and reported hugging, kissing, and touching him frequently. He assisted hospital staff in moving the index patient after a bowel movement, but did not come into contact with fecal matter or any other body fluid. Patient A had no breaks in his skin, including no chronic skin conditions, oral lesions, recent dental work, or needle exposures. Interactions of patient A with the index patient were similar to those reported by other family members. The wife of the index patient reported more frequent and direct contact (assisted with bodily functions, patient cleaning, and in-home care) with the index patient than patient A.\n\n【29】##### Assessment of Healthcare Workers\n\n【30】The index patient was evaluated in the emergency department twice before being admitted and transferred to an ICU for the duration of his hospitalization. He required intensive clinical care, including mechanical ventilation, hemodialysis (continuous renal replacement therapy), and multiple procedures, including central and arterial line placement and endotracheal intubation . These procedures provided opportunities for contact of healthcare personnel with blood or other body fluids.\n\n【31】A total of 132 healthcare personnel were identified as having potential contact with the index patient, or his immediate environment, waste, or medical equipment. Twenty denied any interaction with the index patient and 14 were not reachable, resulting in 98 (%) available for a complete interview. Of these personnel, 86 (%) reported contact with the index patient or his immediate environment or had other concerning factors.\n\n【32】Of the 86 workers, 54 (%) had contact with the index patient in the ICU and 26 (%) had contact with the index patient in the emergency department. Most (%) workers provided direct patient care, and 57 (%) reported contact with blood or other body fluids . These 57 healthcare workers reported 128 separate exposures to blood or body fluids, including 39 (%) exposures to blood, 35 (%) to sweat, 18 (%) to respiratory secretions, 15 (%) to urine, 10 (%) to stool, 8 (%) to tears, and 3 (%) to vomit. The most common PPE ensemble worn during these encounters was gloves only (workers, 63%) followed by gloves and gown (%); 10 (%) encounters occurred without any PPE being used, including 8 encounters with sweat and 2 with tears . No healthcare workers reported blood or body fluid contact with nonintact skin or mucous membranes, and there were no percutaneous exposures. Two healthcare workers reported having blood-soaked scrubs after postmortem cleaning of the body.\n\n【33】Eighty (%) of 86 healthcare workers provided blood samples for testing, and 1 person with recent signs (rash and conjunctivitis) provided a urine sample for testing. All 80 (%) serum samples were negative for Zika virus IgM, and the 1 urine sample was negative for Zika virus by RT-PCR.\n\n【34】##### Assessment of Vectorborne Transmission\n\n【35】##### Vector Surveillance\n\n【36】Larvae and pupae collected at the 3 residences and nearby areas included only _Culex_ and _Culiseta_ species (Cx. pipiens_ , _Cx. tarsalis_ , _Cs. incidens_ , and _Cs. inornata_ ). Inspection of egg papers from ovicups failed to detect viable mosquito eggs. Mosquito collections from all traps failed to detect the invasive species _Aedes aegypti_ or _Ae. albopictus_ . A total of 5,875 adult mosquitoes representing 7 species (Cx. pipiens_ , 658 _Cx. tarsalis_ , 4 _Culex_ spp. 299 _Cs. incidens_ , 138 _Cs. inornata_ , 7 _Ae. dorsalis_ , 1 _Ae. vexans_ , 2 _Aedes_ spp _._ and 1 _Anopheles freeborni_ ) were collected and tested for virus in 501 pools. All mosquito pools were negative for Zika virus by RT-PCR. However, 2 pools containing female _Cx. pipiens_ mosquitoes were positive for West Nile virus RNA by RT-PCR with screening and confirmatory primers.\n\n【37】##### Community Survey\n\n【38】There were 226 occupied households within a 200-m radius of the residences where the index patient stayed. Of these households, 89 (%) had \\> 1 resident who completed a survey; 72 (%) were not available, 62 (%) refused participation, and 3 (%) were excluded because residents were unable to be interviewed in their native language.\n\n【39】From the 89 participating households, 218 persons completed a questionnaire (for themselves or family members < 12 years of age). Of the 218 participants, 119 (%) were female; median age was 35 years (range 4 months–80 years) . Most (%) participants spent an average of \\> 1 hours/day outdoors during the preceding month. However, most (%) reported never wearing insect repellent. Less than half (%) reported being bitten by mosquitoes in the preceding month. Only 22 (%) participants reported traveling in the preceding year, but most of them (%) reported traveling to a country where Zika virus was known to be circulating. Twenty-eight (%) participants reported having \\> 1 of the 4 Zika virus–associated signs/symptoms in the month before their interview; 15 (%) reported having \\> 2 signs/symptoms.\n\n【40】Of the 218 participants, 132 (%) also provided _\\>_ 1 sample, including 124 who provided only a blood sample, 6 who provided blood and urine samples, and 2 who provided only a urine sample. Of the 130 blood samples, 2 were positive for Zika virus IgM. However, these results for the 2 samples were not confirmed by plaque reduction neutralization test (these samples were negative for Zika virus and dengue virus neutralizing antibodies). Because these 2 samples were obtained from asymptomatic persons, urine samples were not obtained. All other serum samples were negative for Zika virus IgM. All 8 urine samples were negative for Zika virus RNA.\n\n【41】Given the number of persons estimated to live in households in the survey area and who provided a specimen, and that no positive samples were observed, we found that the exact upper 95% confidence limit for the proportion of Zika virus–positive persons in the (untested) population of nearby residents was 2.0% when we used sampling without replacement and the finite population. Thus, it is highly likely that <2.0% of unsampled persons would have been infected with Zika virus.\n\n【42】### Discussion\n\n【43】Our investigation of patient A did not identify the probable source of his infection and did not identify any additional persons recently infected with Zika virus among family contacts, healthcare workers, or community members. The index patient was unique when compared with other persons with Zika virus disease because his illness was fatal and his relative viral load was estimated to be ≈100,000 times higher than the average level reported . These characteristics, combined with a lack of additional exposure for patient A, make it likely that the index patient was the source of infection for patient A, although inability to sequence virus obtained from patient A prevented a definitive confirmation . None of the other family members became infected, despite similar or more frequent and direct contact with the index patient during his viremic period. No healthcare personnel became infected despite the index patient having substantial invasive procedures and moderate production of body fluids. Overall, our findings suggest the infection of patient A represents a rare transmission event through unknown, but likely, person-to-person mechanisms.\n\n【44】On the basis of reported contact of patient A with the index patient, patient A might have been exposed to saliva or tears of the index patient, although patient A had no skin lesions or noted mucous membrane exposures that would have increased his likelihood of becoming infected, particularly when compared with other family members. To date, Zika virus has been detected by viral culture in several body fluids, including blood, urine, amniotic fluid, a conjunctival swab specimen, breast milk, semen, and saliva . Zika virus RNA also has been detected in cerebrospinal fluid, aqueous humor, cervical mucous, and nasopharyngeal, vaginal, and endocervical swab specimens . However, our knowledge about the timing and amount of Zika virus in blood or body fluids of the index patient was limited. Thus, we are unable to definitively state how patient A was infected.\n\n【45】Although the index patient had a high level of viremia, no healthcare personnel showed evidence of recent Zika virus infection. There were >100 reported encounters with blood and other body fluids with a variety of PPE reflecting standard precautions, which is probably representative of care given to patients in an ICU . This finding suggests that healthcare workers caring for severely ill patients with Zika virus disease should continue to use standard precautions with correct PPE when handling body fluids to prevent infection .\n\n【46】Although _Ae. aegypti_ mosquitoes were previously identified in southwestern Utah in 2013 , our vector investigations did not identify _Ae. aegypti_ or _Ae. albopictus_ mosquitoes. None of the other mosquitoes collected were positive for Zika virus, but 2 pools were positive for West Nile virus, a finding that supports the efficacy of entomologic surveillance. In addition, no persons tested in the 200-m radius around the households in which the index patient stayed had evidence of a recent Zika virus infection. Although low-level transmission could not be definitively ruled out, results of the vector and community investigations do not support the suggestion that patient A was infected by a mosquito that had fed on the index patient before his hospitalization.\n\n【47】Our investigation had several limitations. First, although family contacts and healthcare workers were interviewed several times by professionals trained in interview techniques, recall bias regarding specific exposures they might have had with the index patient was likely. Second, we probably did not identify all healthcare workers who had contact with index patient because information was obtained retrospectively from the chart for the patient and staffing schedule. Third, documentation in the medical records regarding type and amounts of body fluids might have been incomplete, leading to underestimation of healthcare personnel exposure. Fourth, because of incomplete participation in the community survey, we might have missed persons who were infected by Zika virus in the community, an event we estimated to be low. These limitations might have prohibited identification of an alternate source of infection for patient A. We did not explore potential differences in susceptibility between patient A and other contacts of the index patient but focused on exposure. Thus, other factors, such as history of flavivirus infection in contacts of the index patient, might have contributed to a difference in susceptibility to infection between patient A and other persons.\n\n【48】Currently, Zika virus is known to be transmitted by the bite of an infected mosquito, congenitally from an infected mother to her fetus, sexually, through blood transfusion, and by laboratory exposure . Healthcare providers and public health officials should be aware that person-to-person transmission beyond sexual transmission might occur, albeit rarely, and should be investigated to determine the potential source of infection by obtaining various body fluids from persons suspected of transmitting the virus to another person through an undetermined route. Additional investigation is needed to determine the infectious risk various body fluids represent for person-to-person transmission and to determine host factors that might increase susceptibility for infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6be3bcd-a93f-4136-8c4c-31ef53301eec", "title": "Mosquito Control Activities during Local Transmission of Zika Virus, Miami-Dade County, Florida, USA, 2016", "text": "【0】Mosquito Control Activities during Local Transmission of Zika Virus, Miami-Dade County, Florida, USA, 2016\nZika virus (ZIKV), a flavivirus that can cause birth defects and is associated with Guillain-Barré syndrome, has rapidly spread throughout the Western Hemisphere . The virus is spread primarily by the bite of infected _Aedes aegypti_ mosquitoes; sexual transmission and bloodborne transmission also have been documented . The southern United States is habitable for _Ae. aegypti_ mosquitoes, which are predominantly an urban species. Miami-Dade County, Florida, has well-established _Ae. aegypti_ mosquito populations and is a major travel destination . In addition, the county has a large population of residents who routinely visit countries that had Zika outbreaks in 2016.\n\n【1】In January 2016, the county documented its first travel-associated case of ZIKV infection . The first cluster of local vector-transmitted cases was identified through epidemiologic investigation in the Wynwood neighborhood of Miami on July 21, 2016, and was confirmed and announced on July 29 . Following Centers for Disease Control and Prevention (CDC) guidelines  a “red zone” or travel warning was declared for pregnant women to avoid unnecessary travel to areas within ≈1 square mile around the cluster of cases . Subsequent clusters were identified south of 28th Street in Miami Beach on August 19, initiating a second red zone of 1.5 square miles. On September 16, a cluster was identified north of 28th Street in Miami Beach, expanding the second red zone by another 1.5 square miles to the north. On October 13, a fourth cluster was identified, and a red zone was declared in the Little River area of Miami, although all but 1 case occurred before October 13.\n\n【2】As each cluster was identified, Miami-Dade County Health Department began intensified epidemiologic surveillance to detect additional cases. Concurrently, the Miami-Dade County Mosquito Control Division (MCD) initiated intensive mosquito control activities within each new cluster of local transmission. We describe the mosquito control activities used to address the clusters of locally acquired ZIKV and their effect on subsequent mosquito numbers and Zika transmission.\n\n【3】### Mosquito Control Methods\n\n【4】The Miami-Dade County Health Department notified the county MCD of all suspected or confirmed ZIKV infections. Relevant addresses (i.e. home, work) associated with each notification were inspected for the presence of _Ae. aegypti_ mosquitoes. On the basis of the inspection, source reduction and application of larvicide, adulticide, or both were performed as needed. In addition, MCD attempted to inspect all properties in a 150-meter radius of the case-patient’s house. MCD made multiple visits to reach all homes. At a minimum, front yards of all properties were evaluated, and when house occupants granted permission, backyards as well. MCD left educational materials at all properties.\n\n【5】In the red zones, control activities expanded to include all properties within the zone. Inspection of individual properties helped the MCD define the most common containers that served as larval habitats. The MCD recorded only presence or absence of larvae and did not attempt to quantify or identify the species; thus, mosquito species other than _Ae. aegypti_ might have been present. Regardless of the mosquito species present, the MCD treated them either by removing the water (dumping) or applying a larvicide. During July 23–December 29, MCD conducted 352,209 property inspections countywide. The Wynwood red zone had 1,721 parcels on which 5,974 inspections occurred. During August 19–December 29, MCD conducted 8,755 inspections in the southern Miami Beach (parcels) and 6,872 inspections in the northern Miami Beach (parcels) red zones. In Little River, MCD conducted 3,239 inspections on the 2,075 parcels within the red zone during October 14–December 29.\n\n【6】The 24,795 inspections in the 4 red zones identified a total of 2,720 containers with larval mosquitoes. Most (%) containers with larval mosquitoes were of 3 types: drains, predominately storm drains (%); plants, predominately bromeliads (%); and small containers that were easily dumped (%). Saucers beneath potted plants were included in the small containers–dumpable category. The next most common larval mosquito habitat was tires, constituting 4% of larvae-positive containers. The remaining container types represented < 1% of the total: small containers–permanent, plastic construction barriers, fountains, pools, boats, ponds, ditches, and hot tubs.\n\n【7】The distribution of the most common container types was not uniform across the county. In Wynwood, plants were the most abundant container with larvae (%) . In northern Miami Beach, plants accounted for 61% of the containers with larvae . In southern Miami Beach, drains contributed almost half (%) of the larval sites . In Little River, small containers–dumpable accounted for 39% of containers with mosquito larvae . In addition, red zones received ultralow-volume (ULV) spraying of adulticide and low-volume spraying of larvicide delivered by airplane or truck-mounted equipment .\n\n【8】### Mosquito Surveillance\n\n【9】A routine surveillance system for _Ae. aegypti_ mosquitoes was not in place before August 2016. In each red zone, surveillance for adult _Ae. aegypti_ mosquitoes was initiated as soon as a new zone was identified. BG Sentinel traps enhanced with BG-lures  and dry ice were deployed. Trap density was 17–19 traps/zone/night. Adult mosquitoes from each trap were counted and identified daily until the red zone designation was removed. The predominant species collected in the BG Sentinel traps was _Ae. aegypti_ (%), followed by _Culex quinquefasciatus_ (L.) (%). All other mosquito species comprised <1%. To compare different treatment strategies in Wynwood, we set additional traps in an area around the red zone that received aerial adulticide applications only and inside the red zone where both aerial adulticide and larvicide were applied. Because traps were not readily available in August and early September, traps were moved after 2 weeks from the Wynwood adulticide only area for use in subsequent red zones. As a result, continued surveillance in the area that received aerial adulticide only was not available for longer-term (weeks) comparison to aerial adulticide plus larvicide treatments.\n\n【10】### Insecticide Resistance\n\n【11】##### Laboratory Assays\n\n【12】The insecticide resistance status of _Ae. aegypti_ mosquitoes in Miami-Dade County was not known at the beginning of the outbreak. At the same time that the intensified mosquito control activities began in Wynwood and Miami Beach, _Ae. aegypti_ eggs and adults were collected to evaluate their susceptibility to the active ingredients found in various commercial adulticide products, including those routinely used by the MCD. Eggs were reared in an insectary at 27°C and 80%–90% humidity, with 14 h daylight, and the resulting adults were used in the laboratory bioassays. CDC bottle bioassay  was performed using technical-grade permethrin, 43 μg/bottle; deltamethrin, 0.75 μg/bottle; etofenprox, 12.5 μg/bottle; sumithrin, 20 μg/bottle; naled, 2.25 μg/bottle; and malathion, 400 μg/bottle. Bottle concentrations and threshold times were based on prior calibration of the assay as described previously . All technical-grade insecticides came from ChemService Inc. _Ae. aegypti_ Orlando strain mosquitoes were used as a susceptible comparison colony. This colony was started in 1952 at what is now the US Department of Agriculture’s Agricultural Research Service, Center for Medical, Agricultural and Veterinary Entomology (Gainesville, FL, USA). The CDC bottle bioassay revealed high levels of resistance to all synthetic pyrethroids at the diagnostic time; sumithrin (%–14% death), etofenprox (%–7% death), permethrin (%–12% death), and deltamethrin (%–65% death). We found no resistance to malathion or naled .\n\n【13】##### Field Assays\n\n【14】Because resistance in laboratory assays does not directly translate to product failure in the field, we field-tested commercial products to find the most efficacious pyrethroid product for use in truck-mounted ULV spraying. The MCD used the midlabel rate  for product application before and early in the outbreak. Mosquitoes collected in BG sentinel traps were held in a BugDorm2 Insect Tent  and supplied with 10% sucrose until use in field testing. Field testing consisted of placing adult mosquitoes in cages, then exposing them to the commercial product applied at the mid-label rate with a truck-mounted Grizzly ULV Sprayer . Cages were 7.6 m and 15.2 m from the road. Fifteen minutes after insecticide exposure, mosquitoes were transferred to clean holding containers (.5-mL cardboard ice cream cups covered with netting) and given access to a 10% sucrose solution. Deaths were recorded 24 h after treatment . Additional field testing was conducted using the highest label rate  of DeltaGard  and Zenivex . We chose Zenivex for additional testing because the wind appeared to shift direction during the initial application, causing the treatment to not fully reach all cages. We chose DeltaGard because it performed the best in both bottle bioassay and the previous field testing using the midlabel rate. DeltaGard was selected as the best performing product when applied at the highest label rate (.00134 lb/acre) and was used for truck-mounted spraying in the northern Miami Beach and Little River red zones where aerial spraying did not occur.\n\n【15】### Insecticide Treatments\n\n【16】In addition to treating vegetation with a synthetic pyrethroid during property inspections, MCD applied adulticides using trucks throughout the county using pyrethroid products . Biomist 30+30 (Clarke) was the most commonly used product before insecticide resistance testing. Because mosquito numbers remained high after initial hand- and truck-based applications, the MCD opted to include aerial applications of Dibrom , a product containing naled. The red zones of Wynwood and southern Miami Beach received 4 aerial applications of Dibrom. The first 2 applications occurred within 4 days of each other, followed by 2 more applications at 1-week intervals. A 10 square mile area centered on the red zone in Wynwood and a 1.5-mile area encompassing the southern part of Miami Beach were treated. Because of reduced daylight during fall and the start of the school year, aerial applications in the subsequent red zones were not feasible. In addition, insecticide field testing showed that the highest label rate of DeltaGard could be effective (% death). Thus, in the red zones in northern Miami Beach and Little River, ≈1.5 square mile each were treated with DeltaGard using a truck-mounted Grizzly ULV Sprayer. Treatments occurred on a similar schedule as was used for aerial spraying, with the initial 2 treatments within 4 days of each other followed by 2 more treatments at weekly intervals.\n\n【17】Vectobac WGD , a larvicide product containing _Bacillus thuringiensis israelensis_ , was applied every 7 days for 4 weeks in all 4 red zones. This larvicide was applied by aircraft in Wynwood and by a CSM3 Turbine Vector Sprayer/Duster  mounted on a truck in southern Miami Beach, northern Miami Beach, and Little River. In Wynwood, a 2 m 2  area was treated, but in the other red zones, larvicide treatments covered the same areas as the adulticide treatments.\n\n【18】### Effect on Mosquito Abundance and Zika Infections\n\n【19】Because the response to the Zika outbreak in southern Florida was an emergency public health intervention, there was no time to set up proper controls. Therefore, we cannot evaluate properly using common comparison techniques the effect of the interventions. Instead, we used a changepoint analysis. A changepoint occurs if a time at which the statistical properties of the ordered sequence of observed case counts change. Case counts evaluated here are adult _Ae. aegypti_ counts from BG Sentinel traps. A sequence can have >1 changepoint. In this analysis, the characteristic we assessed is the mean _Ae. aegypti_ count change during the time observed. We consider 2 hypotheses: 1) _Ae. aegypti_ counts during the entire period derive from a Poisson distribution with a constant mean, and 2) \\> 2 time intervals exist, in each of which the _Ae. aegypti_ counts derive from Poisson distributions with different means. We used a likelihood approach using binary segmentation, as described previously , and implemented in the R package changepoint  to identify whether the data were consistent with hypothesis 1 or hypothesis 2. With each binary segmentation of the sequence, the Akaike information criterion with a correction for small sample size (AICcs) of the models with and without a changepoint, were computed. A model with the changepoint was considered a better fit if its AICc was smaller by \\> 6 than the AICc of the model without the changepoint, which corresponds roughly with a type I error of 0.05.\n\n【20】We obtained dates of human cases from the Florida Deptartment of Health website . Because the Little River red zone was declared when mosquito numbers were naturally dropping due to seasonality and mosquitoborne transmission had ceased by the time that red zone was identified, we did not evaluate changepoint analysis and transmission after spraying.\n\n【21】Although we cannot make a statistical association with the location of the changepoints in the other red zones, it is interesting that the first changepoints occurred after adulticide treatments began. In the Wynwood red zone that received both aerial adulticide and larvicide , this changepoint represents a large drop in mean _Ae. aegypti_ counts. With further adulticiding and larviciding, the counts remained low and, in fact, dropped further on August 24. On approximately this date, insecticide applications stopped, and this date is followed by a third changepoint at which mean _Ae. aegypti_ counts increased again.\n\n【22】The only changepoint in the 10-mile area around Wynwood that received adulticide only occurred after the adulticiding began. We do not know what the mean _Ae. aegypti_ counts were before August 9 . However, superimposing the counts for 10-mile region around the Wynwood neighborhood over those for the Wynwood neighborhood (as shown in Figure 3 , panel C) showed that the mean _Ae. aegypti_ counts for August 9 were comparable. Counts before August 9 might also have been comparable, but we have no way to verify that possibility. _Ae. aegypti_ counts then increased in the region around Wynwood, whereas mean counts within Wynwood remained low. One possible explanation for this increase is that larviciding was not done in the 10-mile region around Wynwood. This observation reinforces the concept that both adulticiding and larviciding are needed to quickly reduce mosquito populations and maintain suppression. As reported previously , detection of new Zika virus infections in Wynwood stopped after adulticide treatments began .\n\n【23】In the southern Miami Beach red zone, we again saw that the first changepoint occurred after the first adulticide treatments . There was a slight increase in mosquito count after the initial decrease (although this is not statistically a changepoint). Once the larvicide treatments began on September 6, mean _Ae. aegypti_ counts decreased again, and 2 changepoints in mean counts followed the start of the larviciding. New cases of Zika virus ceased immediately after the first aerial adulticide treatments. However, for 4 weeks, single cases occurred roughly weekly after the last aerial adulticide treatment.\n\n【24】In the northern Miami Beach red zone, the first changepoint again occurred after the first adulticide treatments . After larviciding, the mean counts remained low and were followed by 2 more changepoints in mean _Ae. aegypti_ counts. Again, for a third time no new Zika virus infections occurred after the first adulticide treatments.\n\n【25】We do not know what the _Ae. aegypti_ counts were ahead of treatments or what would have occurred if treatments had not been initiated. However, graphing the _Ae. aegypti_ counts from Wynwood and Miami Beach together  suggests that, before treatments began, the mosquito counts remained consistently high throughout the season (≈30–50 mosquitoes per trap). Only after the first adulticiding in each area did the mean mosquito counts drop statistically and vector-transmitted Zika virus infections cease.\n\n【26】### Lessons Learned\n\n【27】The purpose of using adulticides in an outbreak is to immediately reduce the number of adult mosquitoes (particularly older ones) that might be capable of transmitting disease. We observed interruption of vectorborne Zika virus transmission in Wynwood and both red zones in Miami Beach after beginning intensive adulticiding. In the United States, adulticide treatments using space-spraying techniques against _Ae. aegypti_ mosquitoes have been shown to quickly knock down adult populations . However, these adult mosquito reductions are transient because not all mosquitoes will be active (and thus exposed) during application; in addition, adulticides do not control larvae and pupae, new adult mosquitoes will quickly repopulate an area. Therefore, repeated adulticide treatments are needed to eliminate newly emerging mosquitoes.\n\n【28】The use of larvicides alone does not immediately control adult mosquito populations, and it is not unusual to see the effect of larvicides until several weeks after their application . Our observation in Wynwood, where mosquito numbers remained suppressed when both adulticide and larvicide applications occurred, compared with the area that received only adulticide treatments, reinforces the necessity of a combination approach to achieve and sustain impact. Observations that aerial adulticiding and combinations of adult and larval mosquito control can successfully interrupt vectorborne disease transmission have been previously reported. Aerial adulticiding in California stopped West Nile virus transmission in an area that received the treatment, whereas cases continued to occur in untreated surrounding areas . Although larvicides are typically not recommended as part of a malaria control program, an example of the effect of both adulticide and larvicide contributing to reduction of disease was documented in Kenya, where transmission of malaria decreased substantially after a combination of larvicide and insecticide-treated nets were used . The combination approach can prolong the recovery of a treated mosquito population because adult mosquitoes are killed, thereby immediately interrupting virus transmission and deposition of new eggs, and emergence of new adults is interrupted by the larvicide, keeping the population from quickly rebounding and thus preventing ongoing virus transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "30eb0e58-a47c-4e78-87dd-8a596117c2a5", "title": "Highly Pathogenic Avian Influenza Clade 2.3.4.4 Subtype H5N6 Viruses Isolated from Wild Whooper Swans, Mongolia, 2020", "text": "【0】Highly Pathogenic Avian Influenza Clade 2.3.4.4 Subtype H5N6 Viruses Isolated from Wild Whooper Swans, Mongolia, 2020\nHighly pathogenic avian influenza (HPAI) H5Nx viruses have been continuous threat to poultry and public health since the detection of A/Goose/Guangdong/1/1996(H5N1) (Gs/GD) in 1996 in Guangdong Province, China. The Gs/GD-lineage has evolved into 10 genetically distinct clades  and subclades . A novel clade 2.3.4.4 of H5Nx viruses bearing multiple neuraminidase subtypes, including N2, N5, N6, and N8, has been identified in China since 2008 , and the H5 genes have been phylogenetically differentiated into 4 subgroups (A–D) . Clade 2.3.4.4 H5N6 viruses have been causing worldwide epizootics in poultry and wild birds, and human cases have also been reported since 2014 . In this study, we report the identification and genetic analysis of 2 HPAI clade 2.3.4.4 H5N6 viruses isolated from whooper swan carcasses in central Mongolia during April 2020.\n\n【1】Wild whooper swan (Cygnus cygnus_ ) carcasses were found in April 2020 on the banks of 2 small ponds (°25′39.8′′N, 102°36′09.6′′E, and 47°56′22.0′′N, 102°32′54.0′′E) around the Orkhon River located nearby Khunt Lake (Bulgan Province, 48°25′59.8′′N, 102°34′51.1′′E) and Doitiin Tsgaaan Lake (Arkhangal Province, 47°34′20.1′′N, 102°31′45.4′′E) . These lakes are a major stopover site of migratory wild birds in central Mongolia between their breeding sites in the north and the wintering sites in the south and are also major breeding and molting habitats of wild bird species, including whooper swan. The lakes were the outbreak sites of HPAI H5N1 in wild birds in 2005, 2006, and 2009 . Two HPAI viruses, A/Whooper swan/Mongolia/24/2020(H5N6) and A/Whooper swan/Mongolia/25/2020(H5N6), referred to as MN-H5N6/2020 viruses in this article, were isolated from 2 brain tissue samples of whooper swan carcasses. We conducted whole-genome sequencing  and phylogenetic analysis on the isolates . The nucleotide sequences have been deposited in GenBank (accession nos. MT872354–69).\n\n【2】The 2 MN-H5N6/2020 viruses shared high nucleotide similarity (.65%–100%) across all 8 gene segments. Polybasic amino acid motif on the cleavage site of hemagglutinin (HA) genes (PLRERRRKR/G) suggested that the MN-H5N6/2020 viruses are HPAI. BLAST  and GISAID  searches showed that the MN-H5N6/2020 viruses share high nucleotide identity (.4%–99.9%) across all 8 gene segments with the HPAI H5N6 viruses, referred to as Xinjiang-H5N6/2020 viruses in this article, isolated from wild swans (mute swans \\[ _Cygnus olor_ \\] and whooper swans) during January 2020 in Xinjiang Province, China  , which is located ≈4,800 km southwest of the isolation sites of the MN-H5N6/2020 viruses. These results suggested that the MN-H5N6/2020 viruses might have been introduced through the Central Asian flyway to central Mongolia most likely during early spring migration in 2020.\n\n【3】In the maximum-likelihood phylogenetic trees, all 8 gene segments of the MN-H5N6/2020 were closely clustered with the sequences of the Xinjiang-H5N6/2020 viruses and the H5N6 viruses of clade 2.3.4.4 group C isolated during 2016–2019 in China, Vietnam, and Russia, including human isolates . The phylogenetic relationship and high nucleotide identity indicated that the MN-H5N6/2020 viruses possess the identical genome constellation with the Xinjiang-H5N6/2020 viruses . The time of most recent common ancestor for each gene of the MN-H5N6/2020 viruses and the Xinjiang-H5N6/2020 viruses ranged from May to December 2019, suggesting that the MN-H5N6/2020 viruses and the Xinjiang-H5N6/2020 viruses had diverged from a common ancestor most likely during the second half of the previous year . The time of most recent common ancestor for each gene of the MN-H5N6/2020 viruses ranged from January through March 2020. These data and understanding of waterfowl migration patterns suggest that H5N6 viruses were maintained among wild birds during fall and winter 2019 and reached Mongolia by late winter, most likely carried by long-distance flights of infected migrating wild birds during spring migration. A previous satellite-tracking study of whooper swans between northern China and Mongolia showed that the most stable period for the wintering population of whooper swans in the Sanmenxia Reservoir area was from late December to early January . For spring migration, departure dates of the wintering population of whooper swans ranged from February 17 to March 27, and arrival dates at the breeding sites in Mongolia ranged from February 27 to May 23. This spring bird migration pattern coincided with the timing and direction of H5N6 virus transmission between Xinjiang and Mongolia.\n\n【4】The MN-H5N6/2020 and the Xinjiang-H5N6/2020 viruses had mutations associated with increased HA receptor binding affinity to human-like receptor (α-2,6 sialic acid), including D94N, S133A, S154N, and T156A (H5 numbering)  , although they maintained the amino acids related to the binding tropism to avian-like (α-2,3 sialic acid) receptors (Q and 224G). Unlike 7 H5N6 human isolates of clade 2.3.4.4 group C, the MN-H5N6/2020 and the Xinjiang-H5N6/2020 viruses had amino acid substitution at HA position 188 (H5 numbering), from threonin to isoleucine, which is known to enhance receptor binding affinity to human-like receptor . In the maximum-likelihood phylogenetic tree and maximum clade credibility tree of polymerase basic 2 gene, the closest isolates of the MN-H5N6/2020 and Xinjiang-H5N6/2020 viruses were human H5N6 isolates from China .\n\n【5】Wild migratory birds have played an important role in disseminating and maintaining of Gs/GD lineage HPAI H5Nx viruses, as observed in the epizootics of H5N1 clade 2.2 during 2005–2006 , H5N1 clade 2.3.2 in 2009 , clade 2.3.2.1c in 2015 , and clade 2.3.4.4 since 2014 . During widespread dissemination of the HPAIV clade 2.2 during 2005–2006 and clade 2.3.2 in 2009, these viruses were also detected from wild birds at Doitiin Tsgaaan Lake and Khunt Lake, highlighting that these areas are useful locations for monitoring of HPAI in wild birds as a pathway for the spread of HPAI during migration of waterfowl. Identifying 2 H5N6 HPAI viruses in wild waterfowl in this area signifies the potential for wide spread of this clade 2.3.4.4 H5N6 viruses during the 2020 fall migration.\n\n【6】Since the first report of a human infection with HPAI clade 2.3.4.4 H5N6 virus in Sichuan Province, China, in April 2014 , a total of 24 cases had been reported from China as of August 2020 . The mammalian host-specific markers found in the MN-H5N6/2020 viruses suggest that these viruses are potentially infectious for mammals. Considering the possibility of future dispersal of the H5N6 HPAI viruses through wild birds and the presence of mammalian host-specific genetic markers, enhanced active surveillance in wild birds, poultry, and mammals is needed to monitor spread and understand the potential for zoonotic infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "72e17701-1331-4f2f-954e-e0f59048f1ec", "title": "The Role of Public Health in Addressing Racial and Ethnic Disparities in Mental Health and Mental Illness", "text": "【0】The Role of Public Health in Addressing Racial and Ethnic Disparities in Mental Health and Mental Illness\nAbstract\n--------\n\n【1】Racial/ethnic minority populations are underserved in the American mental health care system. Disparity in treatment between whites and African Americans has increased substantially since the 1990s. Racial/ethnic minorities may be disproportionately affected by limited English proficiency, remote geographic settings, stigma, fragmented services, cost, comorbidity of mental illness and chronic diseases, cultural understanding of health care services, and incarceration. We present a model that illustrates how social determinants of health, interventions, and outcomes interact to affect mental health and mental illness. Public health approaches to these concerns include preventive strategies and federal agency collaborations that optimize the resilience of racial/ethnic minorities. We recommend strategies such as enhanced surveillance, research, evidence-based practice, and public policies that set standards for tracking and reducing disparities.  \n\n【2】Introduction\n------------\n\n【3】Racial and ethnic diversity is increasing in the United States; by 2042, racial/ethnic minorities are expected to surpass non-Hispanic whites as the majority population in the United States . Nevertheless, racial/ethnic minority populations remain underserved in the mental health care system . The Surgeon General advocates a public health approach to eliminate disparities in mental health among racial/ethnic minorities . The New Freedom Commission on Mental Health recommends providing access to high-quality and culturally effective care that includes underserved, remote, and rural areas of the country and recommends that states monitor mental health service disparities through their comprehensive state mental health plans . We describe mental health disparities among racial/ethnic minority populations, such as differences in prevalence rates, diagnoses, access to care, and sources of care. We propose a model that illustrates the relationships between the social determinants of mental health, the necessary public health interventions, and the expected positive outcomes resulting from these interventions in racial/ethnic minority populations. Finally, we offer additional public health recommendations for consideration.  \n\n【4】Existing Mental Health Disparities in Racial/Ethnic Minority Populations\n------------------------------------------------------------------------\n\n【5】Health disparities are defined as “differences in the overall rate of disease incidence, prevalence, morbidity, mortality, or survival” . The Surgeon General’s report on mental health noted that mental health and mental illness are not polar opposites . Mental health is the successful performance of mental function, resulting in productive activities, fulfilling relationships, and the ability to adapt to change and adversity. Mental illness refers to mental disorders characterized by alterations in thinking, mood, or behavior associated with distress or impaired functioning. The evolution and nuances of definitions of mental health and mental illness are discussed in depth elsewhere in this issue of _Preventing Chronic Disease_ . The prevalence of mental illness differs between whites and racial/ethnic minority populations. For example, among American Indian tribes, the lifetime prevalence of any psychiatric disorder is higher (%-54% for men, 41%-46% for women) than among the overall US population (% for men, 38% for women) . The prevalence of any psychiatric disorder in the past 12 months is 15% for African Americans, 9% for Asian Americans, 16% for Hispanics, and 21% for non-Hispanic whites . Although African Americans and Hispanics have a lower risk of lifetime prevalence of mental disorders than do whites , they have a higher risk of persistence (longer course of illness)  and disability from mental illness . Furthermore, despite similarities in the measured prevalence of mental illnesses, racial/ethnic minority populations are disproportionately represented in vulnerable populations, such as the homeless and incarcerated, whose responses are not typically recorded for community surveys . Racial/ethnic minorities who meet criteria for a mental disorder diagnosis may be undiagnosed or misdiagnosed. African Americans with an affective disorder are more likely to be diagnosed with schizophrenia than are white patients . Similarly, Hispanics are more likely to be diagnosed with affective disorders than are whites, although the difference is attenuated by adjusting for sociodemographic, care setting, and payment characteristics . Another issue of concern is the quality of care received by racial/ethnic minorities . African Americans and Hispanics are less likely than whites to receive guideline-based care for depression and anxiety , and African Americans are less likely than whites to be treated with atypical antipsychotic drugs and other newer agents . Racial/ethnic minorities face barriers in accessing mental health care, particularly cost of care and fragmented services . Only 20% of American Indians report that they access care through the Indian Health Service . Furthermore, limited English proficiency and health literacy pose barriers for immigrant populations. In a 2007 study, only 8% of Hispanics who did not speak English, who reported a need for mental health services, received services. Likewise, of the non-English speaking Asian/Pacific Islander population who reported a need for mental health services, only 11% received services . Geographic accessibility is another barrier to care . People who live in rural areas have less access to mental health services than do their more urban counterparts . Even in rural areas where mental health services are available, some populations may not receive culturally appropriate services because of language barriers. Medical insurance coverage is also a barrier to mental health care access among some racial/ethnic minorities. Sixteen percent of the overall US population is uninsured, compared with 25% of African Americans and 40% of Hispanics . Immigrants are 2.7 times more likely to be without health insurance than are US-born residents . White and racial/ethnic minority populations have different sources of care. African Americans are more likely than whites to use emergency services, primary care physicians, and alternative treatments rather than specialists for diagnosis or treatment of mental disorders. African Americans are also overrepresented in inpatient treatment and underrepresented in outpatient treatment . A study of Southwest and Northern Plains tribal members living on or near reservations found that they are less likely than the overall population to seek help for mental illness (from specialists, other medical providers, or traditional or spiritual healers) . Asian Americans are less likely than whites to visit community mental health centers, emergency departments, self-help groups, or a psychiatric clinic in a general hospital .  \n\n【6】A Proposed Model to Overcome Disparities in Mental Health for Racial/Ethnic Minority Populations\n------------------------------------------------------------------------------------------------\n\n【7】We propose a model in which social determinants, interventions, and outcomes influence disparities in mental health and mental illness and are in turn subject to influence. The model illustrates the interactions of these factors to promote mental health and prevent mental illness .  \\[View enlarged image and descriptive text .\\] **Figure.** A proposed model to illustrate the interactions of social determinants, interventions, and outcomes to promote mental health. \n\n【8】### Social determinants\n\n【9】The model’s social determinants are poverty, housing status, education, access to resources, institutionalization, social determinants of equity, stress, and physical health status. Living in poverty is associated with increased risk for mental, physical, and substance use disorders . Racial/ethnic minorities experience low socioeconomic status disproportionately in terms of income, occupation, and education, and this status has been significantly associated with mental illness . Housing status refers to the effects of residential segregation in shaping socioeconomic status. Segregation in housing is associated with social problems such as high unemployment, reduced economic development, concentrated poverty, suboptimal education, and diminished access to health and mental health care . Segregated housing units often are substandard and expose occupants to high levels of environmental toxins. The high density of fast-food restaurants and scarcity of markets with fresh fruits and vegetables in these neighborhoods contribute to poor dietary habits . Furthermore, substandard housing and lack of infrastructure in poor urban neighborhoods where many racial/ethnic minority groups live have led to communities that are disenfranchised, with diminished social networks . Racial/ethnic minorities are overrepresented among people who live in poverty, are homeless, or are institutionalized. Racial/ethnic minorities make up only 26% of the US population , but they make up 57% of the incarcerated population . Estimates from interviews with jail inmates in 2002 and with state and federal prisoners in 2004 revealed that more than half of all inmates have a recent history of mental illness ; yet only 34% of state prisoners, 24% of federal prisoners, and 17% of local jail inmates have been treated for mental illness . Social determinants of equity also influence mental health. For example, an intact family provides a strong, protective social network. Protective factors are defined as characteristics or conditions that diminish the likelihood that people will develop mental illness . Religious belief and social support are examples of protective factors. Although not conclusive, some research findings suggest a link between spirituality and well-being, which may operate through family ties . Negative social factors such as racism, racial bias, and discrimination contribute to poor physical and mental health among racial/ethnic minority populations . These populations also experience a higher rate of other negative life events such as victimization, abuse, and trauma, defined in the figure as stress . The public health system must define disparities in diverse populations, set measurable goals for improvement, focus on community-based research, and acknowledge community needs pertaining to the social determinants of health. \n\n【10】### Interventions\n\n【11】Public health interventions are necessary to help prevent mental illness and promote mental health in racial/ethnic minority populations. The interventions depicted in our model are social support, surveillance/research, cultural competency programs, public health information, evidence-based and community-defined approaches, and policy. Social support from the community, neighborhood, and family decreases emotional distress and the need for formal treatment . These informal networks can be seen as coping mechanisms that help make mental health programs successful . Current surveillance and research approaches are not adequate to address mental health disparities in racial/ethnic minority populations, particularly among Asian Americans and American Indians/Alaska Natives. Surveillance and research on these populations should be expanded to provide adequate data to monitor the prevalence, incidence, and severity of mental illness and access to care. These data may lead to the development of culturally based treatments and public health approaches to evaluate their efficacy . Language barriers and ignorance of cultural differences increase the likelihood of misdiagnosis. Including cultural competency programs in clinician training may help, as may encouraging racial/ethnic minorities to enter health care professions. Furthermore, dissemination of public health information through such techniques as social marketing could help eliminate the stigma of mental illness among some racial/ethnic minority groups and in so doing could encourage seeking help for mental illness. Public health approaches that are defined by the community, that are evidence-based , and that engage racial/ethnic minority populations are necessary to achieve desired outcomes. Finally, national policies must attempt to improve mental health among racial/ethnic minority populations through better access to both mental and physical health care systems . \n\n【12】### Outcomes\n\n【13】The outcomes in the model are morbidity, mortality, stigma, functioning, equity of services, well-being, and resilience. Higher morbidity and mortality are associated with chronic diseases among mentally ill people in some racial/ethnic minority populations . For example, among American Indians, a history of trauma, stress, or depression increases the risk for diabetes . The stigma of mental illness among some racial/ethnic minority groups can cause people to avoid seeking specialty mental health care and refuse to receive medication or counseling . Such avoidance increases the likelihood of seeking care only at the stage of mental health crisis, when the risk of needing emergency services or involuntary hospitalization is higher. Other factors, such as lack of insurance, limited health literacy, and cultural beliefs, also interfere with help seeking, screening, and health assessment. Positive outcomes include successful mental functioning, equity of services, and well-being, achieved through a combination of protective factors and tailored public health interventions. Resilience, or the capacity to recover from adversity, is often achieved through the positive effect of protective factors mediating negative social determinants. Resilience can preserve mental health even in the presence of poor socioeconomic circumstances. Religious participation, volunteerism, and neighborhood collaboration have also been identified as protective factors, strengthening resilience among racial/ethnic minority populations . The intersection of social determinants of health, interventions, and outcomes illustrates how protective factors combined with interventions counterbalance potentially negative social determinants of mental health.  \n\n【14】Summary and Next Steps\n----------------------\n\n【15】For nearly a decade, federal reports have called for a public health approach that addresses the social determinants of mental illness to eliminate disparities in diagnosis and treatment. A successful approach must consider racial/ethnic minorities in their social contexts because these social determinants influence health outcomes. As the US population grows increasingly diverse, providers must become more culturally sensitive in the treatment of mental illness. Provider training in the unique barriers to mental health among racial/ethnic minorities may be achieved by mandating such training in medical school and continuing education. Increases in the number of racial/ethnic minority health care providers could also increase the availability of culturally sensitive mental health care. However, the stigma of mental illness diagnosis and treatment among some groups may delay or prevent mental health care; social marketing campaigns may help alleviate such stigma. To establish mental health in the public health arena, prevention should be emphasized. An example of universal prevention is the early identification of women with postpartum depression to reduce long-term risks for themselves and their infants . The need to recognize symptoms and provide early screening and treatment for common mental illnesses should be emphasized to racial/ethnic minority populations, the general public, and health care practitioners. Collaborative efforts to eliminate health disparities are under way among several federal agencies. One example is the Federal Executive Steering Committee on Mental Health, an interagency group representing 9 federal agencies, led by the Department of Health and Human Services. This group was formed as a result of the New Freedom Commission on Mental Health , which called for a fundamental transformation of the US mental health care delivery system to one driven by consumer and family needs that focuses on building resilience, facilitating recovery, and eliminating disparities in mental health services. Among the steering committee’s initial priorities are chronic disease and mental illness comorbidities. We suggest the following strategies to address the disparities in mental health services and prevalence of mental illness in racial/ethnic minority populations: Surveillance systems should more comprehensively monitor the prevalence, severity, treatment access, and outcomes for mental illness. Surveillance programs are discussed elsewhere in this issue of _Preventing Chronic Disease_ .Research priorities should include models that elucidate causal pathways for mental illness in vulnerable populations.Research should evaluate the effect of race, culture, language, mental illness, and chronic disease on illness and death and use the information to develop community engagement models.Programs and policies on mental health and illness should be integrated into the public health system.Federal agencies should collaborate to provide mental health care to incarcerated and recently released populations.Professional employee training, recruitment, and retention activities should lead to effective mechanisms for a multicultural mental health workforce.A public health approach that promotes mental health among racial/ethnic minority populations and eliminates treatment disparities requires public policies that track and reduce disparities and seek solutions for these diverse communities. These goals will not be fulfilled quickly. They start with improving service programs, overcoming barriers to care, and building public health capacity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dbe9d357-b526-43bd-8ff1-53a0ae0be848", "title": "Parechovirus Genotype 3 Outbreak among Infants, New South Wales, Australia, 2013–2014", "text": "【0】Parechovirus Genotype 3 Outbreak among Infants, New South Wales, Australia, 2013–2014\nThe clinical manifestations of infection with human parechoviruses (HPeVs), members of the family _Picornaviridiae_ , are often indistinguishable from those caused by human enterovirus infections. Over the past decade, outbreaks of human parechovirus genotype 3 (HPeV3) have been reported from the Northern Hemisphere and are particularly well documented in Japan (where the virus was discovered), Canada, the United Kingdom, Denmark, and the Netherlands . Of the 16 HPeV genotypes, HPeV3 is the most aggressive and causes a sepsis-like syndrome in neonates . HPeV infection seems to follow a seasonal pattern; incidence is higher in summer and autumn . It can be spread by the fecal–oral and respiratory routes .\n\n【1】On November 22, 2013, Health Protection New South Wales (NSW), Australia, was notified of a possible cluster of HPeV cases at The Children’s Hospital at Westmead in Sydney. At that time, 7 neonates had experienced rapid onset of acute sepsis-like illness with fever >38°C and a combination of irritability/pain, diarrhea, confluent erythematous rash, tachycardia, tachypnea, encephalitis, myoclonic jerks, and hepatitis. Inquiries revealed that neonates described as “red, hot, angry” had also been admitted to other tertiary children’s hospitals in NSW . An expert advisory group comprising staff from the NSW Ministry of Health, Health Protection NSW, public health units, and the Sydney Children’s Hospital Network was convened to coordinate the investigation.\n\n【2】On November 25, 2013, PCR detection of HPeV RNA confirmed HPeV infection in 2 of the children. The NSW public health network and clinicians agreed that a surveillance program should be initiated to gather information on the epidemiologic and clinical characteristics and outcomes of children with HPeV infection.\n\n【3】In addition to the public health response, Health Protection NSW issued a media release to alert members of the public to the outbreak. On November 29, 2013, HPeV3 information including a case definition, instructions for accessing diagnostic testing, and recommended clinical management was distributed to all emergency departments, pediatricians, and early childhood health services in NSW. During the outbreak, the expert advisory group met regularly via teleconference to discuss and address any emerging issues. HPeV3 active surveillance activities were concluded on January 31, 2014, while other forms of surveillance continued into February 2014. We describe the epidemiology of the outbreak as observed through several surveillance mechanisms.\n\n【4】### Methods\n\n【5】HPeV infection is not a notifiable disease under the Public Health Act 2010 (NSW). This HPeV3 outbreak was detected and reported by clinicians alert to unusual clusters and patterns of disease. Other forms of surveillance were developed as a result of this alert. Surveillance consisted of 3 components: 1) active surveillance (case finding at the sentinel sites); 2) passive surveillance (laboratory reporting of all positive HPeV specimens from sentinel sites and elsewhere in NSW to Health Protection NSW); and 3) syndromic surveillance (reporting of infants seen in emergency departments by the NSW syndromic surveillance system that uses near real-time emergency department and ambulance data ) . The sentinel sites were 3 tertiary children’s hospitals in NSW: The Children’s Hospital at Westmead, The Sydney Children’s Hospital Randwick, and John Hunter Children’s Hospital Newcastle. Passive and syndromic surveillance continued into February 2014. In addition to surveillance, public health communication in the form of an HPeV information sheet for clinicians was distributed on November 29, 2013, alerting emergency department staff, pediatricians, and early childhood health service staff of current HPeV activity in NSW, providing a description of HPeV infection, and recommending management options (i.e. early laboratory testing and provision of supportive care after receipt of confirmation of HPeV infection).\n\n【6】##### Active Surveillance\n\n【7】Active surveillance activities commenced at the 3 hospitals (sentinel sites) on November 25, 2013, and continued through January 19, 2014; however, some retrospective case finding was included for cases with onset dating back to October 1, 2013, when the outbreak was thought to have started. A patient with a suspected case of HPeV was defined as a neonate or young infant <3 months of age with sepsis-like illness and fever >38.0°C and \\> 2 of the following: irritability/pain, rash, diarrhea, tachycardia, tachypnea, encephalitis, myoclonic jerks, or hepatitis. A patient with a confirmed case of HPeV was a suspected case-patient for whom PCR was positive for HPeV. Clinicians at the sentinel sites collected case information by using an HPeV case investigation form and PCR testing of patient stool, cerebrospinal fluid (CSF), nasopharyngeal aspirate, throat swab, rectal swab, or whole blood samples for HPeV; stool and CSF samples were preferred. These data were entered into the NSW Notifiable Conditions Information Management System. Weekly reports informed the NSW public health network of outbreak progression.\n\n【8】##### Passive Surveillance\n\n【9】All positive HPeV test results from NSW residents referred to the Victorian Infectious Diseases Reference Laboratory (VIDRL) in Melbourne, Victoria (the only laboratory in the region testing for HPeV), from October 1, 2013, through February 2, 2014, were reported to NSW Health. Specimen date; estimated illness onset date; sample type; and patient date of birth, sex, and postcode were recorded in the NSW Notifiable Conditions Information Management System.\n\n【10】##### Syndromic Surveillance\n\n【11】The NSW emergency department syndromic surveillance system monitored the number of infants <1 year of age for whom a provisional diagnosis of fever/unspecified infection was made in the emergency department and the number of patients who required hospital admission, including admission to critical care wards. A diagnosis of fever/unspecified infection can include fever symptoms, unspecified viral infection, unspecified viremia, unspecified bacteremia, unspecified bacterial infection, or unspecified infection. Weekly reports compared recent data with historical data from the previous 5 years.\n\n【12】##### Laboratory Methods\n\n【13】From all clinical samples, nucleic acid was extracted by using QIAGEN DX reagents (QIAGEN, Hilden, Germany) on a QIAxtractor NA extraction robot (QIAGEN). cDNA was synthesized by using a method previously described  and was tested in an HPeV real-time PCR selective for the 5′ untranslated region, which was developed at VIDRL . (The primer and probe sequence details for this assay can be supplied upon request to G.C.)\n\n【14】Molecular analysis to obtain the HPeV genotype was performed on selected samples that had been positive by real-time PCR. Specimens from 41 patients were selected for genotyping on the basis of ensuring representation of infants’ geographic locations, ages, sex, illness onset dates, specimen types, and sex. Identification of specific HPeV genotypes was obtained through amplification of the viral protein 1 gene by use of a gel-based seminested PCR . The generated PCR products were sequenced and compared with reference sequences by using the primers and methods described elsewhere .\n\n【15】##### Statistical Analyses\n\n【16】Descriptive analysis of epidemiologic variables and patient demographic characteristics were performed. Characteristics of infants <3 months of age, such as length of stay (LOS), were compared by using _t_ \\-tests to determine the effects of public health messaging. Analyses was performed by using SAS version 9.2 (SAS Institute, Inc. Cary, NC, USA) and Microsoft Excel (Microsoft, Redmond, WA, USA).\n\n【17】### Results\n\n【18】##### Laboratory Findings\n\n【19】From November 1, 2013, through February 28, 2014, a total of 420 specimens were submitted for HPeV PCR testing; for some patients, >1 specimen was submitted. PCR results were HPeV positive for 289 (%) specimens from 198 patients . In addition to confirming HPeV RNA in samples from 198 patients in NSW, HPeV type 3 was identified from all 41 (%) positive samples for which molecular analysis was subsequently performed. The phylogenetic tree demonstrating all HPeV3 isolates genotyped at VIDRL during the outbreak is reported elsewhere .\n\n【20】Because of the algorithms used in the testing, enterovirus results were also available for all samples submitted . A total of 194 patients had HPeV infection only, 4 had dual infections (HPeV and enterovirus), and 14 had enterovirus infection only . Results for the rest of the patients were negative. Focusing on CSF and fecal samples, 123 (%) of 168 CSF samples were HPeV positive by PCR (mean cycle threshold \\[C t  \\] detection value 31.6), and 114 (%) of 156 fecal samples were positive (mean C t  27.2) . PCR was run for 45 cycles; therefore, C t  values >45 were considered negative.\n\n【21】##### Active and Passive Surveillance Findings\n\n【22】Active surveillance identified 94 infants whose illness met the definition of a confirmed case (patient <3 months of age and HPeV-positive laboratory results, originating from sentinel sites). Passive surveillance spanning specimen collection dates from October 1, 2013, through February 2, 2014, identified another 89 laboratory-confirmed cases in NSW in patients 0–17 months of age. The outbreak peaked during the first 2 weeks of December 2013 .\n\n【23】More cases ([57%\\]) were in male than female patients; median patient age was 1.51 months (or median 46 days, range 0–537 days) . Intrafamily HPeV3 transmission was identified in twins, 2 parent–child pairs, and a set of cousins. A descriptive case series of the infants infected with HPeV3 during this outbreak, containing further clinical details on select cases, is reported elsewhere .\n\n【24】Analysis of case investigation forms from the sentinel sites reporting HPeV signs and symptoms showed that the most commonly reported signs for infants <3 months of age were fever (%), irritability/pain (%), tachycardia (%), and rash (%) . Similar signs were displayed by those \\> 3 months of age; however, 20% fewer in this age group had fever and tachycardia . As described previously, all infants at the sentinel sites were well at the time of hospital discharge, and further longitudinal follow-up studies are examining the long-term outcomes of these infections having occurred in early life .\n\n【25】Of the 183 confirmed cases, 108 (%) were captured by the 3 sentinel surveillance sites, and another 75 (%) were diagnosed at other hospitals. Most (%) patients resided in the Sydney metropolitan area, and the remaining 43% were from regional or rural areas of NSW. This finding compares with 64% and 36% of the NSW population residing in metropolitan Sydney and regional/rural areas, respectively .\n\n【26】Analysis of case investigation forms for the 108 patients at the sentinel sites also showed that 103 (%) patients were admitted to hospital and had an average LOS of 4.4  days . Mean LOS was greater for infants <3 months of age (.5, range 1–13 days) than for those \\> 3 months of age (.7, range 1–11 days). The rate of admission to an intensive care unit was lower for older infants (%) than for those <3 months of age (%) .\n\n【27】The trend statistic on the distribution of LOS in infants <3 months of age showed that LOS was significantly reduced among infants <3 months of age who became ill after the HPeV alert was sent on November 29, 2013, to emergency departments and pediatricians. Mean LOS at the sentinel sites was 5.7 days before and 4.0 days after sending of the alert (Satterthwaite _t_ \\-test, p<0.05) .\n\n【28】##### Syndromic Surveillance Findings\n\n【29】The number of infants <1 year of age with a provisional emergency department diagnosis of fever/unspecified requiring hospital admission began to rise sharply in mid-November 2013 and peaked during the first week of December . At the peak, the number of admissions was 83, compared with an average of 52 for the same week in previous years. Admissions remained elevated until mid-January 2014, when they returned to background levels. Admissions to critical care wards spiked during the second week of December, when 9 patients were admitted, well above the average of 1 admission per week for the same week in previous years . During the surveillance period, most admissions were to the 2 children’s hospitals in metropolitan Sydney.\n\n【30】### Discussion\n\n【31】This outbreak is probably one of the first large parechovirus outbreaks to be reported in Australia. We observed a large number of cases over a relatively short period (≈4 months), peaking around late spring/early summer, which is earlier than documented seasonality for parechovirus in the Northern Hemisphere . Although sequencing in this series was incomplete (% of patients), this parechovirus outbreak was determined to have been caused by HPeV3 for the following reasons: all sequenced HPeV-positive samples were genotype 3; the epidemiology and spectrum of illness seen by clinicians at the sentinel sites was relatively homogenous and in keeping with other reports of HPeV3 infections in infants ; and PCR was enterovirus positive for only 18 patients but HPeV positive for 198 patients.\n\n【32】Infected infants in this outbreak were older than those reported elsewhere. The median age of 46 days was higher than that reported in Denmark (days) and the Netherlands (days) . The occurrence of a substantial proportion (%) of HPeV3 infections in infants \\> 3 months of age was consistent with results from a US study reporting 18% of cases in infants >60 days of age but contrary to other data indicating that HPeV3 infection occurs almost exclusively in infants <3 months of age .\n\n【33】The age cutoff in the case definition for active surveillance at the sentinel sites may have introduced a bias toward HPeV infection being more frequently suspected in infants <3 months of age; thus, these infants might have undergone more HPeV testing than older infants, thereby underestimating the true mean age of infants affected in this outbreak. Had the age cutoff in the case definition been set at 12 months (all infants), bias toward younger infants would have been avoided, and the median age might have been older than that reported here.\n\n【34】After the November 29, 2013, release of the HPeV alert to emergency department staff and pediatricians, mean LOS among infants <3 months of age decreased by 30%, from ≈5.7 days at the start of the outbreak to 4.0 days (p = 0.0250). This statistically significant finding reflects a degree of effectiveness of public health messaging in raising clinician awareness of HPeV. During the latter part of the outbreak, after Health Protection NSW issued the alert, informed clinicians may have felt more comfortable discontinuing treatment and discharging infants sooner if their illness met the criteria of the HPeV case definition. The alert seemed to result in improved clinical management of cases, avoidance of unnecessary prolonged exposure to empirically prescribed antimicrobial drugs, provision of appropriate supportive treatment, and earlier discharge from hospital. The mean LOS during the NSW outbreak (.4 days) was lower than that reported in the Netherlands (days), according to an analysis of retrospective diagnoses when no public health intervention would have taken place . Another factor that could contribute to reduced LOS in the latter part of an outbreak is the increasing age of young infants becoming ill with HPeV infection in the second half of the outbreak, although this factor was not statistically significant in this dataset .\n\n【35】The male:female split in this outbreak was less pronounced than that reported for other studies, although more cases consistently occurred in boys. Others have reported a higher preponderance of infection in boys, ranging from 70% to 90%, compared with our finding of 57% .\n\n【36】Clinical signs reported in the literature for HPeV3-infected infants were consistent with our findings of fever, irritability, and encephalitis . However, rash occurred with much higher frequency (% vs. 17%) in the NSW outbreak than in other outbreaks .\n\n【37】Syndromic surveillance that used emergency department data proved to be a useful and timely way to monitor emergency department hospital admissions temporally associated with the HPeV3 outbreak. Increased presentation of infants <1 year of age with a provisional emergency department diagnosis of fever/unspecified infection requiring hospital admission, in particular admission to critical care, were associated with increased detection of HPeV3 at the clinical level. Emergency department syndromic surveillance reports also helped confirm an overall decline in admissions from emergency departments in early 2014, supporting the eventual withdrawal of active surveillance. Emergency department syndromic surveillance in NSW does not routinely monitor age-specific or admission-specific (hospital ward or critical care unit) aberrations; that is, all children <5 years of age are monitored as a group. In the future, data generated through the emergency department syndromic surveillance system may continue to be useful for monitoring the evolution of an HPeV or similar outbreak. The cost of maintaining emergency department syndromic surveillance like that used during this outbreak (fever/unspecified infection among children <1 year of age and admission to hospital) would need to be considered. Costs of doing so include personnel time for checking reports and investigating signals, infrastructure costs, and opportunity costs; choosing to monitor HPeV3-related signals indirectly means that signals for diseases that are not prioritized are not monitored. In addition, the sensitivity of this grouping will need to be tested in future outbreaks before it can be considered a reliable proxy indicator of a seasonal outbreak\n\n【38】We initiated sentinel surveillance on the assumption that nearly all neonates with severe HPeV3 disease would be referred to 1 of the 3 tertiary children’s hospitals in NSW. Through passive surveillance we identified an additional 41% of HPeV patients who had been seen at other health facilities. Enhanced data were not collected for these presumably milder cases, which has limited our capacity to conduct more extensive significance testing across the observed differences in clinical features. Recording more information on potential exposures (e.g. infants’ daycare attendance, existence of older siblings, and occurrence of family illness in weeks preceding infants’ admission) would have further aided our understanding of HPeV3 transmission in the community.\n\n【39】### Conclusion\n\n【40】The objectives of HPeV surveillance were achieved: document the outbreak, describe the clinical features of cases, help inform clinicians and the public, monitor the evolution of the outbreak, and add to the knowledge base. The HPeV3 infection outbreak in NSW, Australia, differed slightly from that documented in the Northern Hemisphere; the NSW outbreak apparently affected slightly older infants (as well as neonates and young infants), cases were more evenly split between boys and girls, and rash occurred at a considerably higher frequency. The value of awareness-raising communication strategies was demonstrated by the statistically significant 30% reduction in LOS during the outbreak immediately after release of the alert to emergency department staff and pediatricians. This alert helped to minimize unneeded use of antimicrobial drugs and reduce unnecessary hospitalization. Although active surveillance is resource intensive, it has helped to define HPeV3 infection in NSW and link it with a syndromic surveillance indicator in the emergency department syndromic surveillance system. Syndromic surveillance is a potentially useful proxy indicator that should be considered for future detection and surveillance of seasonal outbreaks of viral infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9c24e5fc-09b4-44b6-b4b1-ab0fef7bf0f2", "title": "Emerging Chagas Disease: Trophic Network and Cycle of Transmission of Trypanosoma cruzi from Palm Trees in the Amazon", "text": "【0】Emerging Chagas Disease: Trophic Network and Cycle of Transmission of Trypanosoma cruzi from Palm Trees in the Amazon\nThe tropical moist broadleaf forests of Latin America are an important region for conservation of biodiversity . In the Amazon Basin (area 8,214,284 km 2  ), blocks of original habitat are still intact, while some ecoregions 1 are almost completely converted or degraded, allowing major components of biodiversity to steadily erode . Populations of several endangered wildlife species have declined, and human habitat and land use are considered a threat to most native species and communities.\n\n【1】Palm trees propagate in ecoregions of the Amazon Basin ecosystem amid other vegetation or in enormous palm forests. Of approximately 2,800 palm species worldwide, 387 (.8%) are native to the basin . Palm trees have been used to study the evolution of biological diversity and are excellent markers of ecologic fitness in the Amazon Basin . In addition, these trees may play an important role in the forest ecosystem. Palm trees produce 15 tons of dry organic material per hectare per year (threefold more than other species of trees) and recuperate more rapidly after fire than other forest species. A single native palm tree may serve as shelter and food for diverse fauna (wild mammals, snakes, scorpions, spiders, amphibians, and many species of insects). Palm trees are also an important economic resource for residents of the Amazon region, who collect and sell palm roots, stipe, leaves, fruits, seeds, heart of palm, and inflorescences . The babassu palm _Attalaea phalerata_ reaches an average density of 200 trees per hectare in the state of Maranhão, but lower densities were reported in the states of Piaui, Goiás-Tocantins, and Mato Grosso , where babassu trees number an estimated 11 x 10 8  .\n\n【2】Not much attention has been given to human health conditions in this ecosystem , probably because pathologic conditions in tropical broadleaf forests are difficult to quantify in these isolated, often impoverished communities. Nineteen sylvatic species of triatomines were identified in the Amazon Basin , six in association with palm tree microhabitats . Eleven of these species were infected with _T. cruzi_ or _T. cruzi_ \\-like flagellates . None of these triatomine species, with the possible exception of _T. rubrofasciata_ , have adapted to human habitats in the Amazon Basin . Since 1924, when _T. cruzi_ infection in wild squirrel monkeys (Crisotrix sciureus_ ) was described , sporadic human _T. cruzi_ infections have been reported in the basin . However, this enzootic protozoan infection received attention only after 1969, when acute cases of human Chagas disease were described in Belém, State of Pará, Brazil . Further evidence shows that _T. cruzi_ infections are endemic in the Amazon Basin  .\n\n【3】Spellerberg and Hardes  describe the major threats to rain forest conservation as shifting agriculture, cattle ranching, logging, and industrialization (mining, hydroelectric dams), to which we add land colonization. We hypothesized that certain effects of human economic activity (population growth, land use, deforestation, lack of environmental education programs, and human predation on mammals and birds) in a defined ecoregion of this major ecosystem may therefore pose a risk for outbreaks of acute Chagas disease. We evaluated these risk factors in the Tocantins River moist forest ecoregion , which is considered vulnerable to human intervention. We conducted a pilot study to assess seropositivity for _T. cruzi_ infections in the human population of Paço do Lumiar County, state of Maranhão, Brazil, and to search for triatomines that harbor these infections in palm trees. We also gathered and analyzed demographic, environmental, and socioeconomic data.\n\n【4】### Methods\n\n【5】The field study was carried out in Paço do Lumiar county (population 55,000), 20 km from São Luiz, capital of the state of Maranhão. The county is part of the Tocantins moist forest ecoregion . We worked in 15 villages, separated by partly deforested argilaceous pathways with scattered houses, where mud-walled, thatch-roofed houses are usually located beneath or beside large palm trees. The surroundings consist of shady, partly deforested areas, where dogs, cats, chickens, pigs, cows, and horses live; no clear delineation separates peridomestic areas from the dense rain forest habitat of wild animals. The county's economy depends on subsistence agriculture and fishing. Raising domestic animals, producing manioc root flour and grains, and harvesting greens and fruits necessitate clearing areas of forest.\n\n【6】We conducted a serologic survey to assess the prevalence of _T. cruzi_ infection in 25,451 county residents >1 and <75 years of age (<72% were 30 years of age) in these 15 villages with <1,200 houses. Fingerprick blood samples from study participants were collected onto Whatman (Clifton, NJ) 1-mm filter paper for seropositivity assessment. After air-drying at room temperature, each set of 10 blood samples was sealed in clean plastic wrap and kept dry in an ice box during the day of collection. The blood samples were then stored frozen until analysis. At the laboratory, filter paper blood samples were punched out and eluted in 100 µL of phosphate-buffered saline, pH 7.4, as described . The test was standardized for obtaining 5 µL of blood in 1 cm 2  of the filter paper, and serum proteins were eluted in 100 µL of PBS, pH 7.4, yielded a 1:20 final dilution for screening seropositivity. For quality control, 10% of the samples were analyzed by a second examiner.\n\n【7】##### Trapping Triatomines\n\n【8】The strategy for trapping sylvatic triatomine bugs derived from published work , as well as observations by local residents that triatomines attracted by light fly from palm trees to houses at night. A night visit to one house resulted in capturing two triatomines on the wall near a light bulb; both these specimens had protozoan flagellates in the intestinal contents.\n\n【9】Researchers and field workers spoke to residents at clubs or social organizations. Dried triatomines were displayed, and community leaders requested that triatomines in houses be captured and stored (in a 5 x 3 cm translucent plastic container with holes in the cap). This strategy proved efficient for collecting triatomines in the rainy season, when the bugs invaded the houses.\n\n【10】We systematically dissected 23 palm trees (A. phalerata_ ) in backyards in five villages of the county. Each tree was cut into segments: stipe, crown shaft, fronds, petiole, and leaves. Each segment was carefully searched for insects, mammals, bird nests, and animal vestiges.\n\n【11】During microhabitat dissection, we captured 67 nymphs and 95 adults of three species of triatomines. The precipitin test was used to type blood in the intestinal contents of 44 adult male and female triatomines. The test consisted of two-dimensional immunodiffusion of blood in the insects' intestinal fluid against taxon-specific antisera .\n\n【12】##### Trapping and Identifying Wild Animals\n\n【13】The rich bird fauna in the research area included _Aratinga jandaia, Buzeo magnirostris, Colombina passerine, Coragyps atratus, Crotophaga ani, Guira guira, Otus choliba, Pitangus sulphuratus, Turdus fumigatus,_ and _Tyranus muscivora_ . We did not capture birds because they are refractory to _T. cruzi_ infections. However, we captured sylvatic mammals (Didelphis marsupialis_ ) near houses in the study area. These ancient marsupials eat palm-tree fruits and rest and nest in the clefts between the stipe and their fronds. They leave these hiding places at night to search for fruits, chicken eggs, baby chicks, and food scraps. Using nylon net or wooden box-traps baited with mango and banana, we captured 12 adult _D. marsupialis_ but were unable to trap _Caluromys_ sp. seen in the palm trees around the study area.\n\n【14】##### Biologic Characterization of Kinetoplastid Flagellates\n\n【15】Parasitic protozoa in the feces of insect vectors of Chagas disease infections or in blood agar broth were demonstrated directly by light microscopy. Flagellate protozoan infections in _D. marsupialis_ were detected by xenodiagnosis or hemoculture . For xenodiagnosis, 20 first-instar uninfected nymphs of _Dipetalogaster maximus_ took a blood meal from each adult _D. marsupialis_ captured in the field. Thirty days later, the feces of the triatomines were examined by microscopy for flagellates. Any parasitic flagellates in the feces of triatomines or in hemocultures were subjected to passage in weanling mice. This procedure consisted of intraperitoneal injection of a saline dilution of the metacyclic flagellates into mice. Two weeks later, trypomastigote forms of the parasite were identified in the blood of the mice, then 100 µL of infected blood was seeded in blood-agar slants, the supernatant of which yielded parasitic forms, which were used for mass production in nutrient-rich liver infusion tryptose medium. One isolate from _R. pictipes_ (Rp_ 1) and three isolates from _D. marsupialis (Dm_ 1, _Dm_ 2, and _Dm_ 3) were characterized.\n\n【16】##### Phenotypic and Genotypic Characterization of _T. cruzi_ \\-like Isolates\n\n【17】Specific antibodies in sera from Chagas disease patients with parasitologically confirmed _T. cruzi_ infection were used as phenotypic markers for the counterpart herein called _T. cruzi_ \\-like parasitic infection. Binding of antibodies to epimastigote forms grown in liver infusion tryptose (LIT) medium and to amastigote forms in sections of murine tissues was detected by indirect immunofluorescence assay . Epimastigote and amastigote forms of the archetype Berenice stock of _T. cruzi_ were used as positive controls. For negative controls, parasitic forms from both sources were treated with sera of _T. cruzi_ antibody-negative persons.\n\n【18】We extracted DNA of parasitic forms from each of three flagellate protozoa derived from _D. marsupialis_ and one isolate of _R. pictipes._ The epimastigote forms grown in LIT were used for extraction of nuclear and kinetoplast DNA, essentially as described . DNA samples were analyzed by polymerase chain reaction (PCR) with specific primers for the constant regions of minicircles of kDNA and for highly repetitive sequences of nuclear DNA of _T. cruzi_ , as described . In addition, we used the _r_ DNA nested set of primers D71/72A, which can amplify sequences of 125 and 110 bp, respectively, from type II or I parasites . The reactions were run in parallel with 100 pg of protozoan flagellates _Rp1_ , _Dm1, Dm2,_ amd _Dm3_ . As positive controls, we used 100 pg of DNA from _T. cruzi_ Berenice (Type 1) and _Dm28_ (Type 2). Negative controls were 100 pg of DNA from _Leishmania braziliensis_  and _T. rangeli_ .\n\n【19】Formalin-killed epimastigote forms of _T. cruzi_ \\-like flagellates from _Rp_ 1, _Dm_ 1, _Dm_ 2, _Dm_ 3, and _Dm_ 4 and the Berenice stock of _T. cruzi_ were used. The probes consisted of a nuclear DNA sequence PCR amplified with primer sets TcZ1/2 . The probe was labeled with biotin according to the manufacturer's protocol. Cells fixed in glass slides were hybridized with selected DNA probes and stained with fluorescein-labeled streptavidin (Sigma, St. Louis).\n\n【20】Each of 10 _D. marsupialis_ trapped in the wild and BALB/c mice receiving _T. cruzi_ \\-like parasites isolated from triatomines were subjected to histopathologic study. The animals were euthanized, and tissue samples from organs and tissues were fixed in 10% formalin. Three representative sections of skeletal muscles, heart, esophagus, small and large intestine, liver, kidney, spleen, and lung were stained with hematoxylin and eosin for examination by microscopy. An average of six sections from the scent glands of the marsupials was taken for histopathologic study.\n\n【21】### Results\n\n【22】By indirect immunofluorescence test for anti- _T. cruzi_ antibodies in human blood collected on filter paper , 212 (.83%) of persons tested had specific antibodies for _T. cruzi_ infections . Positive serologic results in young populations indicate recent transmission and acute infection. Forty-six children <10 years of age (.18% of the total study population) were antibody-positive for _T. cruzi_ and were considered acutely infected.\n\n【23】Our results, which show seroprevalence of _T. cruzi_ infections in the absence of hematophagous bugs or their vestiges (excreta and molted skins) in houses, prompted us to search for triatomines in the ecosystem where the population was infected or continues to be at risk. The strategy for capture of triatomines consisted in surveillance of houses by residents (household members captured bugs in the house and placed them in plastic containers) or in dissection of palm trees in backyards of houses . This householder-assisted surveillance and capture method yielded 52 triatomine bugs (R. pictipes_ and 16 _R. neglectus_ ). Triatomine excreta and molted skins in these houses were neither reported by inhabitants nor detected by field workers. Adult triatomines were captured in houses only during the rainy season. We also captured 133 triatomines in 23 palm trees cut down in backyards in five villages. Careful dissection of these trees allowed detection of different developmental stages of triatomines in clefts of palm frond-sheets .\n\n【24】Remains of animal species (e.g. nests, hair, feathers) on which triatomine bugs prey were identified in palm trees in backyards in five villages. Twice when a tree was cut down, adult opossums (Didelphis_ ) ran out of the fronds into the forest. Nests of marsupials and birds were easily detected on dissection of palm fronds and crowns. In addition to opossums and birds, we identified molds and captured and identified different species of various taxa of invertebrate and vertebrate animals  in the 23 palm trees . Molds were found in stipes, fronds, and crowns, and insects in roots, stipes, inflorescence, fruits, fronds, crowns, and leaves. The clefts formed by frond sheets were particularly rich in Amphibia, Arachnida, and Hemiptera. Triatomines were detected at the bottom of clefts where marsupials built their nests. Bird nests were found in the fronds and crowns where abundant species of insects were available for predation.\n\n【25】Scarcity of blood flagellates in marsupials precluded detection by direct microscopy. However, the metacyclic flagellates recovered by xenodiagnosis were subinoculated in weanling mice. Two weeks after injection, trypomastigote forms of the parasitic protozoan morphologically indistinguishable from _T. cruzi_ were detected in blood of the mice. To define and further characterize these isolates, we used phenotypic and genotypic molecular characterizations. In the first group, antibodies in sera of chronic Chagas disease patients reacted indistinctly with antigenic determinants in the surface of _T. cruzi_ Berenice and with isolates _Dm_ 1, _Dm_ 2, and _Dm_ 3 from _D. marsupialis_ and with _Rp1_ from _R. pictipes_ .\n\n【26】Genotypic kDNA and nuclear DNA (nDNA) markers were used to genetically characterize these wild flagellate protozoan isolates. PCR amplification of template DNA from each of these _T. cruzi_ isolates showed that kDNA primers S35/36  amplified _cruzi_ , a laboratory standard for virulent _T. cruzi_ . In addition, when we used PCR with mini-exon intergenic spacer primers TC/TC1/TC2  and _r_ DNA primers D71/72 , amplification resulted in the same bands, using template DNA from wild _T. cruzi Dm 28c_ , _Dm_ 1, _Dm_ 2, _Dm_ 3, and _Rp_ 1 and from Berenice _T. cruzi_ isolated from a patient with acute Chagas disease . These molecular features allowed classification of wild isolates of _T. cruzi_ as phylogenetic type I, while Berenice was _T. cruzi_ II . These results were further confirmed by in situ hybridization of wild _T. cruzi_ with a biotinilated 198-bp sequence derived from Berenice template DNA , which was amplified with specific nDNA primers Tcz1/2 .\n\n【27】### Conclusions\n\n【28】Several authors  described _T. cruzi_ \\-like flagellate protozoans (instead of _T. cruzi_ ) in the blood of various classes of mammals and in the feces of triatomine species from the Amazon Basin. We detected blood flagellates in 9 of 10 Didelphidae captured in backyards of houses in Paço do Lumiar county. In addition, we isolated flagellates from feces of _R. pictipes_ , which showed the same morphologic features described for _T. cruzi._ In biological characterizations, these flagellates induced low-level parasitemia in laboratory mice. However, histopathologic lesions in marsupials and laboratory mice were similar to those described in Chagas disease patients. Furthermore, nuclear DNA markers displayed all features of the _T. cruzi_ standards Berenice and _Dm_ 28c _T. cruzi_ , which differed from those shown by _T. rangeli_ and _L. braziliensis_ . These data confirm the parasitic flagellates present in triatomines and mammals in this ecoregion as _T. cruzi_ .\n\n【29】Birds are refractory to _T. cruzi_ infections , but several authors  describe marsupials as important wildlife reservoirs of this parasitic protozoon. We used traps with fruit baits to capture 12 marsupials, which underwent karyotyping and parasitologic and pathologic examinations. The karyotyping confirmed these mammals as _Didelphis marsupialis_ . Nine of 10 trapped marsupials had protozoan blood flagellates isolated by xenodiagnosis or hemoculture. Histopathologic study of heart sections from these infected animals showed typical myocarditis, characterized by mononuclear cell infiltrates and target cell lysis. Inflammatory infiltrates were seen in skeletal muscles, esophagus, and small and large intestines. Histopathologic study of representative tissue sections taken from the only opossum that did not have the parasite detected showed no histopathologic alterations (data not shown).\n\n【30】American trypanosomiasis has been considered an ancient zoonosis in which insect vectors and mammal hosts sympatrically occupy vast areas of South America . Wild _T. cruzi_ infections and the bug vectors are syntopically adapted to mammalian host habitats under natural equilibrium. _T. cruzi_ infections and human Chagas disease occur over a large geographic area, limited by parallels 42° N to 42°S . The Tocantins moist forest, much of which is subject to severe disturbance of the environmental equilibrium, provided conditions for disease outbreaks . The growing human populations encroaching on that natural ecoregion are fed upon by the triatomine vector _R. pictipes,_ and cases of acute _T. cruzi_ infections in humans have exponentially increased in the last three decades.\n\n【31】In this study, we describe a trophic network of five levels comprising different species dwelling in palm tree microhabitats. A single class of top predator mammal (Didelphidae_ ) was found in the study area. The absence of other taxa of top wild predators upon which bugs feed may contribute to peridomiciliar and domiciliar invasion during the wet season. This observation contrasts with earlier descriptions of seven families of mammals, belonging to Primates, Edentates, Marsupials, Carnivores, Rodents, and Chiroptera classes , which were hosts for triatomines in relatively undisturbed ecoregions. Elimination of a single class of invertebrate or vertebrate animals in a trophic network may be a major risk factor leading to more triatomine species entering houses and initiating a new cycle of transmission of _T. cruzi_ infection.\n\n【32】In our study, a child <10 years of age with a positive immunofluorescence test  was considered a host of acute _T. cruzi_ infection. Considering the age-specific prevalence of _T. cruzi_ infections in adults  and the fact that for each acute case that is clinically identified an estimated 20 to 100 others are unrecognized , autochthonous human Chagas disease in the Amazon Basin may reach 7,860 to 39,300 cases. The latter figure is consistent with serologic evidence of _T. cruzi_ infection in the Brazilian Amazon region presented in the national report on Chagas disease . The characteristics of transmission of infections described here do not indicate a need for insecticide spraying in the Amazon region, for the cycle of transmission of _T. cruzi_ is deeply embedded in a natural trophic network comprising wild animals belonging to several classes and trophic levels.\n\n【33】Risk factors associated with the possibility of emergence of endemic Chagas disease in the Amazon Basin have been described . First, the broadleaf moist rain forest ecosystem may be invaded by triatomine species (T. infestans_ and _T. rubrofasciata_ ), which are considered completely adapted to human domiciles, or by other triatomines (P. megistus_ and _T. brasiliensis_ , _T. pseudomaculata_ , and _T. sordida_ ), which can be found in different ecosystems but frequently enter and colonize houses . Second, several Amazon Basin triatomine species (R. pictipes, R. prolixus, R. neglectus, R. nausutus, T. vitticeps, T. rubrovaria_ and others) can adapt to human dwellings, where they could become important vectors of the _T. cruzi_ infections . We found no vestiges of triatomine colonization in houses or their surroundings in our study area. We hypothesize that starving adult _R. pictipes_ and _R. neglectus_ may leave their natural shelters at night to feed on human hosts, probably attracted by light in the houses.\n\n【34】Factors associated with triatomines flying from palm tree to houses need to be clarified. The scarcity of birds and mammals during the wet season may be an important factor associated with anthropic predation and the presence of _T. cruzi-_ infected insects in houses in the rainy season. Domiciliation of triatomines may not be required for an increasing endemicity of Chagas disease in the Amazon Basin . For example, sylvatic _Rhodnius brethesi_ recently bit people harvesting palm fibers in Barcelos in the northwestern part of the State of Amazonas, 490 km upriver from Manaus, leading to an acute case of Chagas disease . Blood samples from residents of this locality tested by immunofluorescence assay had 12.5% positivity for anti- _T. cruzi_ antibodies . These observations suggest that to prevent transmission of _T. cruzi_ infections to humans in the Amazon Basin, new strategies are needed, which will not necessarily be similar to those used in controlling endemic Chagas disease in other ecosystems, such as the Cerrado and Caatinga ecosystems in Brazil .\n\n【35】Alternatively, the enormous task of controlling emerging Chagas disease in the Amazon Basin should rely initially on an information, education, and communication program, which encourages control measures by the householder (e.g. use of screens, bed nets, insecticide-treated fabrics, and vegetation management) . Such a program for prevention of contact with triatomines should be conducted directly in communities, elementary schools, and churches and social clubs, reinforced by social marketing and mass media communications. Further studies are also needed for identifying new and integrated (chemical and nonchemical) strategies required for controlling _T. cruzi_ vectors in the Amazon Basin, which may not necessarily be similar to those already shown to be partially effective in controlling the domestic vectors of endemic Chagas disease in other ecosystems in the Americas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a3edc9a4-0ca5-4753-96f1-896aa274ef8e", "title": "Boiling and Bacillus Spores", "text": "【0】Boiling and Bacillus Spores\n**To the Editor:** Public health authorities rely upon “boil water” advisories to alert consumers if a potable water supply is deemed unsuitable for consumption. Holding water at a rolling boil for 1 minute will inactivate waterborne pathogens, including encysted protozoa . Spores of _Bacillus anthracis_ , the agent that causes anthrax, are one of the microorganisms most refractory to inactivation by the boiling water method. This study was conducted to determine the resistance of spores of _B_ . _anthracis_ Sterne and three other strains of _Bacillus_ spp. in boiling water.\n\n【1】_B_ . _anthracis_ Sterne (Colorado Serum Co. Denver, CO) was grown on soil extract peptone beef extract medium . Spores were harvested from the agar plates and washed four times by centrifugation with sterile distilled water, treated with 50% (vol/vol) ethanol while being shaken at 100 rpm for 2 h, then washed an additional four times by centrifugation with sterile distilled water. Spores of one of the _B_ . _cereus_ strains were obtained from a commercial source (Raven Biological Laboratories, Omaha, NE). Spores were produced in broth cultures for the other _Bacillus_ spp. The second _B_ . _cereus_ (ATCC 9592) was grown in a generic sporulation medium , and _B_ . _thuringiensis_ var. _israelensis_ (ATCC 35646) was grown in Schaefer’s medium . Spores were purified by gradient separation using RenoCal-76 (Bracco Diagnostics, Princeton, NJ) . Spore preparations were stored in 40% (vol/vol) ethanol at 5°C until used.\n\n【2】Duplicate experiments for each species were conducted in 1-L glass beakers containing 500 mL of municipal drinking water (±2°C, pH 8.2±0.5, free available chlorine 0.5±0.3 mg/L). The beakers were left uncovered or covered with a watch glass. Steam was allowed to escape from the covered beakers through the mouth of the pouring spout. Water samples were injected with the spore preparations, heated to boiling on a hot plate, and held at boiling temperature for various times. Measuring the boiling times began when the sample reached a rolling boil. A thermocouple thermometer (Cole-Parmer, Vernon Hills, IL) directly above the liquid-air interface determined the air temperature above the boiling water after 5 min of exposure. At the conclusion of the various boiling times, the samples were removed from the heat source and allowed to cool at room temperature before analysis. These samples contained <0.2 mg/L of free available chlorine. Decimal dilutions of the water samples were analyzed in triplicate by the membrane filter procedure with nutrient agar .\n\n【3】Spores of all strains of the _Bacillus_ spp. analyzed in this study were inactivated after boiling for 3–5 min in a covered vessel . Spores still survived after 5 min of boiling in an open vessel for all of the _Bacillus_ spp. Temperatures immediately above the surface of the boiling water in the covered vessels averaged 98.9°C, while the temperature immediately above the water level in the uncovered vessels averaged 77.3°C.\n\n【4】In a comprehensive literature review citing published reports dating back to 1882, Murray  noted that boiling times reported to destroy _B_ . _anthracis_ spores varied over a range of 1 to 12 min. In his own study of 17 strains of _B_ . _anthracis_ , Murray  found that boiling times of 5 to 10 min were required to achieve inactivation. Stein and Rogers  reported that vigorous boiling for 3 to 5 min destroyed spores from 43 strains of _B. anthracis_ .\n\n【5】In our study, boiling water in a covered vessel for 3 to 5 min destroyed spores of the _Bacillus_ spp. by greater than four orders of magnitude. Boiling for 5 min in an uncovered vessel was not as effective as boiling in a covered vessel and allowed all _Bacillus_ spp. spores to survive. On the basis of the initial levels of spores used in this study, holding water at a rolling boil for 1–3 min in an open container would not inactivate the spores. Boiling time refers to the total time the water is held at a rolling boil and should not be confused with the first sign of bubbles from dissolved gases in the water. Since water boils at lower temperatures at higher altitudes (approximately 90°C at 3 km), boiling times must also compensate for decreased atmospheric pressure conditions .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "414aaff5-d088-4d6e-b7a9-cde624da2cc9", "title": "Designing Competencies for Chronic Disease Practice", "text": "【0】Designing Competencies for Chronic Disease Practice\nAbstract\n--------\n\n【1】**Introduction**Competencies are the cornerstone of effective public health practice, and practice specialties require competencies specific to their work. Although more than 30 specialty competency sets have been developed, a particular need remained to define competencies required of professionals who practice chronic disease prevention and control. To that end, the National Association of Chronic Disease Directors (NACDD) engaged a group of stakeholders in developing competencies for chronic disease practice. **Methods**Concept mapping was blended with document analysis of existing competencies in public health to develop a unique framework. Public health experts reviewed the results, providing extensive and richer understanding of the issues. **Results**The final product presents an integrated picture that highlights interrelationships among the specific skills and knowledge required for leading and managing state chronic disease programs. Those competencies fall into 7 clusters: 1) lead strategically, 2) manage people, 3) manage programs and resources, 4) design and evaluate programs, 5) use public health science, 6) influence policies and systems change, and 7) build support. **Conclusion**The project yielded a framework with a categorization scheme and language that reflects how chronic disease practitioners view their work, including integrating communications and cultural competency skills into relevant job functions. Influencing policies and systems change has distinct relevance to chronic disease practice. We suggest uses of the competencies in the field.  \n\n【2】Introduction\n------------\n\n【3】We describe a project of the National Association of Chronic Disease Directors (NACDD) to develop a comprehensive set of competencies for public health practitioners working in chronic disease prevention and control. The project focused on the specific knowledge and skills required in the field — what managers and leaders of chronic disease programs in states need to know and be able to do. The identification of competencies needed by public health practitioners, creation of competency-based curricula, and assurance that professionals have mastery of competencies have been emphasized as priorities by the US Public Health Service , the Centers for Disease Control and Prevention , the Institute of Medicine , and the Public Health Agency/Faculty Forum . Competencies specific to chronic disease practice will allow professional development to be tailored to the unique aspects of chronic disease. The project reported here focused specifically on the way knowledge and skills are used in the _practice_ of chronic disease prevention and control in public health. Managers will gain a richer understanding of the competencies required on the job in this field and can use the framework as the basis for planning their own or their staff’s professional development. These competencies can be used to re-examine job descriptions, job requirements, and training needs. We synthesized literature on competencies in public health and input from brainstorming sessions with chronic disease practitioners to identify a set of chronic disease competencies. We then used concept mapping  to organize these competencies into a framework. Concept mapping is a well-documented social research method  that has been used in public health projects .  \n\n【4】Methods\n-------\n\n【5】All new public health competencies are expected to conform to 2 documents: _Core Competencies for Public Health Professionals_  and _Essential Public Health Services_ . The Council on Linkages Between Academia and Public Health Practice, funded by the Health Resources and Services Administration, used a lengthy review process involving more than 1,000 professionals to develop the _Core Competencies_ . The 17 Council on Linkages member organizations from across the spectrum of public health endorsed _Core Competencies_  as a framework and incentive for competencies development. _Essential Services_ undergirds the National Public Health Performance Standards Program to improve the quality of public health and the performance of public health systems . Many public health professionals have applied _Essential Services_ to their activities. However, neither _Core Competencies_ nor _Essential Services_ provides direction to specific public health practice specialties, nor were they intended to do so. Although other public health specialties have created their own competencies, no consensus has been reached on the competencies required for chronic disease practice. The first step was to identify and develop a possible set of competencies, seeking saturation on the range of knowledge and skills required of chronic disease practitioners. We analyzed literature and documents, received input from practitioners in the field, and conducted a group review of a draft set of ideas. To ensure that the competencies for chronic disease practice were well grounded in earlier initiatives, the team studied competencies from major health professional organizations, federal agencies, and prior NACDD efforts. The search, covering 1989 through March 2006, yielded 42 sources. Selected competencies were excluded if they were not relevant to chronic disease programs or focused on direct clinical care. The third author (KQ) and another expert in concept mapping trained the fourth author (SS) on this process. The fourth author contributed content and context expertise to the analysis. Starting with the _Core Competencies_ and the _Essential Services_ , the fourth author analyzed each relevant competency verbatim from each source to identify statements that completed the following phrase: _“A specific thing that leaders and managers of chronic disease programs in states need to know or be able to do is ._ .” The fourth author read each document, extracted each relevant competency verbatim, compared it with already extracted competencies and added it to a database if it contributed a new idea, and referenced the source. Through this document analysis, we derived 145 potentially applicable competencies. We solicited input from members of a specially invited Competencies Planning Group, the NACDD Board of Directors, and the NACDD Professional Development Committee (N = 37) to brainstorm ideas that completed the same focus prompt: “ _A specific thing that leaders and managers of chronic disease programs in states need to know or be able to do is_ .” Participants brainstormed anonymously on a dedicated Web site from March 27 to April 11, 2006. They could see the contributions of other participants, but they could not see the results of the document analysis. The brainstorming generated approximately 75 ideas. We merged the derived set of competencies with the brainstormed ideas by eliminating duplicates and combining overlapping statements to create a set of 220 draft statements. At a half-day meeting, the Competencies Planning Group discussed each of the 220 proposed statements, deciding by consensus whether to include each statement in the final set or to edit or add ideas. The planning group selected single-idea statements on the basis of their uniqueness, clarity, brevity, and relevance to state chronic disease program managers and leaders. The facilitator (the third author, KQ) ensured that group decisions were based on 1 or more of those selection criteria and that all participants’ views were heard. Statements that included the same keywords (eg, all draft statements that referenced _budgets_ ) were examined together to help identify nuances of meaning and redundant statements. To eliminate overlap and repetition of the same idea from different sources, the group’s discussion focused on slight differences in wording, exploring the similarities and differences between the items and striving to select the statement that represented the core idea in the clearest and most relevant terms to chronic diseases prevention and control. Other concept mapping projects  have used this _idea synthesis_ or _statement reduction_ process, which is described in detail by Kane and Trochim . The goal is to draw on the planning group’s expertise to arrive at a comprehensive yet manageably sized set of competency statements. The review process yielded 100 discrete ideas. Of this set, 28 competencies came from the brainstormed statements, 26 from the _Core Competencies_ , and the remainder from other sources. All adhered to public health workforce classification standards and formatting . Next we asked the 37 participants to sort the 100 final statements into categories on the basis of similarity (of 37 \\[46%\\] responded), following these instructions: Sort all the statements into categories (anywhere from 5 to 20 categories usually works well).Put each statement into only 1 category.Do not create a “miscellaneous” or “other” category with unrelated statements.Group the statements according to how similar in meaning they are to one another. Do not sort according to importance or priority.We used Concept Systems software version 4.0 (Concept Systems, Ithaca, New York) to aggregate and analyze the sorting data. This system applies multidimensional scaling (MDS) and hierarchical cluster analysis to integrate the sorting information and to develop a series of maps and reports. The Competencies Planning Group reviewed the analysis results at a June 2006 meeting, choosing labels for each cluster and grouping clusters into regions. The fourth author (SS) interviewed 8 public health leaders selected by the planning group to gather feedback on the draft competencies framework. The interviewees were people who work with state chronic disease practitioners, including key strategic partners, people who have crossed over from state chronic disease programs to another sector, and stakeholders whose perspective may not have been captured through the concept mapping process. The 8 interviewees commented on the uniqueness of the domains to chronic disease, identified potential gaps, and recommended ways to use the competencies. These interviews provided a final, external review of the framework to ensure the comprehensiveness, face validity, and usefulness of the competency set. The planning group reviewed the interviewees’ suggested additions and added 4 competencies, yielding a final set of 104 competencies . Having these experts comment on the overall framework rather than participating in the earlier brainstorming offered a fresh perspective for critique. The NACDD Professional Development Committee discussed and reviewed all results in August 2006. No additions or revisions were proposed, suggesting both face validity and saturation of the conceptual territory.  \n\n【6】Results\n-------\n\n【7】A point map captures the underlying structure for the conceptual maps. The MDS analysis translates qualitative judgments about similarity into quantitative distances on a 2-dimensional map where each statement is represented by a numbered point. Statements closer together on the map are conceptually similar, according to participants (ie, more participants paired those ideas together into the same category in individual sorting). Statements farther apart are more dissimilar . Using hierarchical cluster analysis of the MDS output, the third author (KQ) chose a 7-cluster solution that best fit the statement set, creating the point cluster map . The planning group reviewed that solution and labeled the 7 clusters: 1) use public health science, 2) design and evaluate programs, 3) manage resources, 4) manage people, 5) lead strategically, 6) build support, and 7) influence policies and systems change.  **Figure.** Point Cluster Map of Competencies for Chronic Disease PracticeEach numbered point represents 1 competency as identified through the National Association of Chronic Disease Competencies for Chronic Disease Practice project in 2006. Competencies were identified through analysis of 42 documents and brainstorming by 37 practitioners. The results were sorted by 17 practitioners to yield the point cluster configuration shown here. Appendix A shows the full text corresponding to each numbered point. Each labeled polygon represents a domain of conceptually similar competencies. Just as the distance relationships between the points signify degree of similarity, so do the distance relationships between clusters. For instance, a region of clusters on the left side of the map (manage people_ , _lead strategically_ , _build support_ , and parts of _influence policies and systems change_ ) are people-oriented. Those on the right side of the map (manage resources_ , _design and evaluate programs_ , _use public health science_ , and parts of _influence policies and systems change_ ) primarily involve data, systems, information, and technical aspects of the job. Planning group members also overlaid a time-management perspective onto the map. Clusters in the upper right (manage people_ and _manage resources_ ) occupy much of the day-to-day, immediate activities of chronic disease leaders and managers. Clusters in the lower right (design and evaluate programs_ and _use public health science_ ) are intermediate-term activities. Clusters on the left represent important long-term strategic objectives. Following conventions used in the _Core Competencies_ , the clusters became the competency domains for NACDD’s Competencies for Chronic Disease Practice. Each domain represents a sphere of activity or function and contains a detailed set of specific, related competency statements . NACDD Professional Development Committee members identified the following domains as being practiced most uniquely in chronic disease prevention and control (in this order): Influence policies and systems change.Build support.Lead strategically.Design and evaluate programs.The Professional Development Committee’s ranking was consistent with input from the public health leaders who were interviewed. Three of the 4 competencies added by the interviewees were in the clusters _influence policy and system changes_ and _build support_ .  \n\n【8】Discussion\n----------\n\n【9】A content analysis of the literature, coupled with concept mapping  and expert interviews, yielded a map of the skills and knowledge specific to leading and managing chronic disease programs that improve public health. The stakeholders who contributed to the organizing structure produced a collaboratively authored framework of competencies. The map aggregates each participant’s conceptual sorting of the 100 ideas. By converting similarity (qualitative sorting information) into a physical location on the map, the conceptual map shows how participants viewed relationships between competencies and reflects a practice-oriented perspective. The domains in this framework integrate knowledge and skills as they are applied to job-related activities. For instance, the Competencies for Chronic Disease Practice address communication skills where the skill applies to a job function. Many communication-related competencies are included in _build support_ . That cluster focuses on communication toward the goal of establishing working relationships with stakeholders to build support for chronic disease prevention and control. Other communication skills are in the cluster _influence policies and systems change_ . The focus here is on implementing strategies to change the health-related policies and systems of private or public organizations. The clusters _build support_ and _influence policies and systems change_ are adjacent on the map, indicating they are closely related, but distinct. Thus, the communication skills are represented on the map in relation to how they contribute to major job-related functions or tasks. Cultural competency skills also are integrated where the skill would be most readily applied, rather than emerging as a separate cluster. For instance, the statement _apply principles of cultural appropriateness to program design_ is grouped with other statements about program design in _design and evaluate programs_ . The statement _recruit and retain a diverse chronic disease workforce_ implies cultural competencies related to the task _manage people_ . Other statements regarding cultural congruency appear in the cluster _use public health science_ . Thus, skills are integrated in relation to specific job functions and tasks, illustrating an applied, practice-oriented perspective. The project also suggests areas that may be practiced in unique ways in chronic disease. NACDD members and expert interviewees described the cluster _influence policies and systems change_ as unique to chronic disease prevention and control, compared with other public health specialties. _Build support_ and _lead strategically_ also may be practiced in a unique way in chronic disease prevention and control. These areas are near each other on the concept map and are also the domains interpreted as part of the long-term strategic work of program directors and managers. Program design and policy and systems change — both of which are forms of intervention in public health — are divided into separate clusters in the NACDD framework, which further suggests that policy and systems change may involve a unique application of skills in the chronic disease field. Practitioners seem to perceive that the science of public health makes an important contribution to both types of interventions, based on the fact that the cluster _Use public health science_ lies between these 2 types of interventions. These findings invite further exploration of how those particular domains are practiced in chronic disease prevention to ensure that professional development and educational opportunities for chronic disease practitioners address the actual requirements of this subspecialty in public health. The Competencies for Chronic Disease Practice can be used to shape individual professional development plans. They might also form the basis for job descriptions in this public health specialty. Human Resources personnel and chronic disease leaders may use these competencies to tailor a standard template to a specific position. These competencies may prompt a re-examination of common assumptions about basic job requirements. For instance, on the basis of the framework, experience in program management may be equally or even more important to many jobs in contemporary chronic disease prevention and control than clinical certifications such as registered nurse or registered dietitian. The Competencies for Chronic Disease Practice can also form the foundation for professional development curricula for chronic disease practitioners. Professional development providers may develop courses that address specific clusters. Because the competencies describe job-related functions, using this framework as a curricular foundation may help ensure that professional development activities are job-related and have the greatest appeal to practitioners. Cross-cutting issues like cultural competency and communication should be integrated into many courses, focusing on how skills apply to specific job activities. Finally, courses could include case studies and examples drawn from chronic disease practice, helping to ensure relevance and applicability. Only 17 participants completed the sorting process underlying the maps organizing framework, which constitutes a limitation of this project. Map results tend to stabilize at about 25 to 30 participants . Thus, input from additional participants may have led to a slightly different final map configuration. Nonetheless, the map framework seems to have face validity with practitioners, as broader audiences (eg, the external interviewees and subsequent NACDD general members’ presentations) have responded well to it. Given the burden of chronic disease in this country, it is important to describe the skills and knowledge specific to leading and managing chronic disease programs. This effort describes the elements of chronic disease practice and forms the foundation for recruitment and succession planning, professional development planning, assessment and performance reviews, and curriculum development that will assure a well-educated and competent chronic disease workforce in public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "153bd01c-5291-41a7-bcbd-a0b710b5cdf9", "title": "Erratum, Volume 9, Dec. 15 Release", "text": "【0】Erratum, Volume 9, Dec. 15 Release\n### ERRATUM\n\n【1】Erratum, Vol. 9, Dec. 15 Release\n================================\n\n【2】_Suggested citation for this article_ : Erratum. Prev Chronic Dis 2012;9:110080e.\n\n【3】In the article “Promoting Smoke-free Environments and Tobacco Cessation in Residential Treatment Facilities for Mental Health and Substance Addictions, Oregon, 2010,” the suggested citation incorrectly identified the year of publication year as 2011. The correct year of publication is 2012. We regret any confusion or inconvenience this error may have caused.\n\n【4】* * *\n\n【5】The opinions expressed by authors contributing to this journal do not necessarily reflect the opinions of the U.S. Department of Health and Human Services, the Public Health Service, the Centers for Disease Control and Prevention, or the authors' affiliated institutions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2ef265d1-8260-484f-88e5-5efd0e6ef600", "title": "Multispacer Typing of Bartonella henselae Isolates from Humans and Cats, Japan", "text": "【0】Multispacer Typing of Bartonella henselae Isolates from Humans and Cats, Japan\nThe causative agent of cat-scratch disease (CSD), _Bartonella henselae_ , is a gram-negative bacterium associated with cats. Human infection usually occurs through scratches or bites by infected cats and typically is seen with localized lymphadenopathy. Occasionally, the infection may have an atypical manifestation, such as endocarditis, encephalopathy, neuroretinitis, or systemic CSD with hepatic and splenic granuloma .\n\n【1】_B. henselae_ strains are classified into two 16S rRNA genotypes, 16S type I/Houston-1 and 16S type II/Marseille. Although both genotypes are present worldwide, 16S type II appears to be dominant in the cat population of Europe, whereas 16S type I is more common in Asia, including Japan .\n\n【2】Multispacer typing (MST) is a nucleotide sequencing-based genotyping method that uses highly variable intergenic spacers as typing markers. It is the most suitable genotyping procedure for evaluating the population structure of closely related strains of _B. henselae_ . Previously, 50 MST genotypes from 201 _B. henselae_ strains were phylogenetically organized into 4 lineages, and human strains mostly grouped within 2 of these lineages, Houston-1 and Marseille . Because genotypic data on _B. henselae_ from Asian countries are limited, we applied MST to 56 human and cat specimens to determine the genotypic distribution and relationship of human and cat strains of _B. henselae_ in Japan.\n\n【3】### The Study\n\n【4】During 1997 through 2008, we collected 56 _B. henselae_ specimens from western Japan, mainly from Yamaguchi prefecture; the specimens included 1 _B. henselae_ isolate from a patient with endocarditis , 24 clinical specimens from CSD patients who had test results positive for _B. henselae_ DNA, and 31 _B. henselae_ isolates from domestic cats . The 24 clinical specimens included 5 lymph node specimens and 16 pus specimens from patients with typical CSD, 1 blood specimen from a patient with bacteremia, 1 liver specimen from a patient with hepatic granuloma, and 1 spleen specimen from a patient with splenic granuloma. Total genomic DNA was extracted from the specimens by using the QIAamp DNA Mini Kit (QIAGEN, Hilden, Germany). _B. henselae_ DNA was detected by using PCR with specific primers for the 16S–23S rRNA intergenic spacer  and the _htrA_ gene , and the 16S rRNA genotype was confirmed by partial sequencing of the 16S rRNA gene .\n\n【5】In addition, previously described MST primers were used to amplify and sequence the 9 intergenic spacers in _B. henselae_ DNA . Locus-specific PCR was performed for spacer S1; direct sequencing was unsuccessful because of an unusual number of variable number tandem repeats (VNTR) . Spacer sequences were assigned according to published data . For each of the 9 spacers (S1–S9), we identified 8, 2, 3, 4, 3, 2, 3, 2, and 3 genotypes, respectively . Three novel spacer sequences were deposited in the DNA Data Bank of Japan with these accession numbers: AB558532, S2 genotype 9; AB558533, S4 genotype 7; and AB558534, S4 genotype 8. We identified 13 different MST genotypes among the 56 specimens . Of these 13 MST genotypes, 7 were novel (types 51–57). Six MST genotypes belonged to human and cat strains, including 2 predominant MST genotypes (and 35). All MST data were deposited in the MST-Rick database .\n\n【6】Subsequently, we analyzed the phylogenetic relationships of the 7 novel MST genotypes identified in this study with the 50 previously identified genotypes. Multiple sequence alignment of the concatenated spacer sequences was performed by using ClustalW . Finally, a phylogenetic tree was constructed by using the unweighted pair-group method with arithmetic mean (UPGMA) in MEGA4 . This phylogenetic tree is grouped into 4 clusters . Of the 13 MST genotypes identified in this study, 12 genotypes belonged to cluster 1, but one genotype (MST genotype 52) belonged to cluster 4.\n\n【7】### Conclusions\n\n【8】This study showed that MST genotypes in Japan were mainly grouped into 1 lineage (cluster 1), which was composed of Asian and American strains of _B. henselae_ , and that the genotypic distribution of human strains coincided with that of cat strains. Although only 1 human strain from the West Indies belonged to cluster 1 before this study , we discovered that all 25 of our human strains from Japan were grouped into cluster 1. These results demonstrate that human infections can be caused by _B. henselae_ strains in cluster 1, which differed from clusters corresponding to the Houston-1 and Marseille type strains.\n\n【9】In a previous study, MST genotype 35 was the most common genotype in cluster 1, and 4/6 (%) of Japanese cat strains belonged to this genotype . In this study, MST genotypes 14 (%; 20/56) and 35 (%; 13/56) were predominant genotypes. Additionally, most human strains (%; 15/17) belonging to these genotypes were isolated from patients with typical CSD; 2 strains with MST genotype 35 were isolated from patients with endocarditis and bacteremia .\n\n【10】The genotypic distribution of the human strains in this study differed from that reported by Li et al. because their strains isolated in France were grouped under 2 lineages (Houston-1 and Marseille). However, we found that the lineages of human strains matched those of cat strains in each country. These results are consistent with the role of cats as the major reservoir of _B. henselae_ .\n\n【11】In this study, we identified 2 cat strains that were classified into cluster 4. These strains belonged to 16S type II, which is rare in Japan . In previous MST studies, strains in cluster 4 were isolated from cats and belonged to 16S type II . Intriguingly, similar lineages consisting of 16S type II isolates from cats were observed in other genotyping studies involving the use of multilocus sequence typing (MLST)  and multiple locus variable number tandem repeat analysis (MLVA) . Thus, these lineages may be less pathogenic for humans. However, further studies are needed to investigate this hypothesis.\n\n【12】When we characterized the strains in this study by MLST we found that almost all of them shared the same sequence type as Houston-1 . In contrast, we identified 13 MST genotypes that belonged to different clusters than Houston-1. The lower resolving power of MLST is mostly likely due to sequence conservation in the 8 housekeeping genes selected for the method. MST has a higher resolving power because the spacers used in this method are more variable than MLST markers. As a result, MST is better suited for evaluating the population structure of closely related _B. henselae_ strains.\n\n【13】We conclude that the MST genotypes in Japan are mainly grouped into cluster 1 and that the genotypic distribution of human strains coincides with that of cat strains. In Japan, human infections can be caused by _B. henselae_ strains in cluster 1, distinct from clusters containing the Houston-1 and Marseille type strains. These results improve our understanding of the population structure of and geographic relationship between human and cat strains of _B. henselae_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6415fd05-2599-45e0-a066-89e7963693ca", "title": "Molecular Characterization of Rotavirus Gastroenteritis Strains, Iraqi Kurdistan", "text": "【0】Molecular Characterization of Rotavirus Gastroenteritis Strains, Iraqi Kurdistan\nRotavirus is the single most important cause of severe gastroenteritis in young children throughout the world. Globally, an estimated 702,000 children die each year due to rotavirus diarrhea . This large impact of rotavirus disease has speeded the development of rotavirus vaccines, and 2 live, attenuated rotavirus vaccines are expected to be available for global use within the next few years . Therefore, determining the prevalence and types of rotaviruses within regions is essential to prepare for introducing a vaccine.\n\n【1】Rotavirus, a member of the family _Reoviridae_ , has a triple-layered capsid that contains 11 segments of double-stranded genomic RNA. While protective immunity against rotavirus infection is not completely understood, serotype-specific immunity is believed to play a major role . Rotavirus serotypes are defined by genome segment 4 for the P (protease-sensitive protein) type and by genome segment 9 (or 7 or 8, depending on the strain) for the G (glycoprotein) type. Fourteen G types exist, of which G1–G4 are commonly found in children with diarrhea, but a recent increase in the detection of serotype G8 and G9 strains has captured considerable attention . While >24 P types have been reported in the literature, only P, P, and P are commonly found among human rotaviruses .\n\n【2】In Iraq, the death rate in children <5 years of age was reported to be 130/1,000 for boys and 120/1,000 for girls in 2003 . Diarrhea is a major cause of illness and death in Iraqi children; however, little information exists about the origin of childhood diarrhea. Only a single study showed that rotavirus accounted for 24% of acute diarrhea in hospitalized children in Basrah .\n\n【3】### The Study\n\n【4】This descriptive, cross-sectional study of 6 weeks' duration was undertaken at Erbil Paediatric Hospital in Iraqi Kurdistan between March and May 2005. The study recruited 260 children from 1 month to 5 years of age who were admitted with acute diarrhea (defined as the passage of watery or loose stools >3 times per day for <2 weeks' duration). Basic demographic, epidemiologic, and clinical information were collected prospectively, according to a pro forma. Ethical approval for the research was obtained from the review boards of the Liverpool School of Tropical Medicine and Erbil Paediatric Hospital. The hospital serves a population of ≈1.5 million, and ≈3,116 births per month occur in this population.\n\n【5】A commercial enzyme-linked immunosorbent assay (ELISA) was used to detect rotavirus antigen (Rotaclone, Meridian Diagnostics, Cincinnati, OH, USA). Stool samples were then stored frozen in the laboratory of the study hospital until they were transported to Liverpool for rotavirus genotyping and electropherotyping. All samples  with an absorbance equal to or greater than the positive control for the ELISA were subjected to genotyping. Rotavirus genomic RNA was extracted with guanidine isothiocyanate, followed by adsorption to and elution from silica particles according to the method described by Gentsch et al. The purified RNA was then used to determine the P type and G type of rotavirus present in the stool specimens by reverse transcription-polymerase chain reaction as described by Gentsch et al. and by Gouvea et al. Rotavirus electropherotypes were determined by polyacrylamide gel electrophoresis according to the method described by Koshimura et al. with some modifications.\n\n【6】Of 260 stool specimens tested by ELISA, 96 (%) were positive for rotavirus. Rotavirus-positive patients had a mean age (SD) of 9.3 (.5) months compared to 11.1 (.1) months in the rotavirus-negative patients. These results suggest that rotavirus positive cases were slightly younger, although the difference was not statistically significant (p = 0.14). Rotavirus-positive patients were similar to rotavirus-negative patients in most of the epidemiologic and clinical characteristics (data not shown). However, rotavirus-positive patients were more likely to exhibit vomiting and have a shorter duration of diarrhea (p<0.01 for both analyses).\n\n【7】Of the 66 rotavirus strains that underwent molecular characterization, 25 (%) were G1, 11 (%) were G2, 13 (%) were G4, and 7 (%) were G9. Four (%) were mixed infections (G1/G2, 1 G2/G4), and 6 (%) were G nontypeable. A total of 7 (%) were P, 10 (%) were P, and 45 (%) were P. One showed mixed P and P genotypes (mixed with G1/G2), and 3 (%) were P nontypeable. None of the rotaviruses was both G and P nontypeable.\n\n【8】A total of 8 different P and G genotype combinations were detected . The most common combinations were PG1 (%), PG4 (%), PG2 (%), PG1 (%), and PG9 (%). The unusual combination of PG9 was detected in 1 of the patients.\n\n【9】An electropherotype was obtained for 50 of the 66 genotyped strains. Of these, 11 (%) had a short electropherotype, and 39 (%) had a long electropherotype . Most of the short electropherotypes were the expected G2 strains; however, 1 strain (PG9) also had a short electropherotype.\n\n【10】### Conclusions\n\n【11】The only other study of viral gastroenteritis from Iraq (Basrah in the south) demonstrated that 24% of children with acute gastroenteritis were infected with rotavirus . This figure is somewhat lower than the 37% detection rate in our study. Moreover, the prevalence we found is similar to those reported from neighboring countries such as Iran (%) , Jordan (%) , Kuwait (%) , and Turkey (%) . However, our study was undertaken over a 6-week period from the end of March to the beginning of May 2005. No information is available on the seasonal prevalence of rotavirus infection in Iraq, and a longer study is warranted to determine the true prevalence of rotavirus infection and its seasonality in northern Iraq. However, the peaks of rotavirus infection in Iran, Kuwait, and Turkey were February–March, March–May, and December, respectively . More than 75% of our cases of rotavirus diarrhea occurred in children <1 year of age, with an overall mean age of slightly more than 9 months. This pattern is similar to that in many developing countries. In Jordan the mean age of children with rotavirus diarrhea was 7.2 months . However, in other countries in the region the distribution was different; 30% of the infants with rotavirus in Iran were <1 year of age , 50% in Kuwait were <1 year of age, and 63% in Turkey were <2 years of age .\n\n【12】Although this study period was brief, we detected a variety of rotavirus strains. Four of the major global human rotavirus genotypes (G1, G2, G4, G9) were detected, as were each of the major P genotypes (P, P, P). In Iran, in a study undertaken in 2001 and 2002, only G1 and G2 rotaviruses were detected, and the only P types were P and P , and in Turkey over a 2-year period , G types G1–G4 and G9, as well as each of the 3 major human P types were found . In Iraq, the combinations PG1 and PG4 accounted for >50% of the strains of rotavirus. In Iran, PG1 accounted for 95% of the strains, but PG4 was not detected . In Turkey, PG4 (%) and PG1 (%) accounted for more than two thirds of the strains . G3 rotaviruses were not detected in Iraq or Iran, and in Turkey only 1 of the 65 strains was of genotype G3. Genotype G9 was detected in 13% of the Iraqi strains, a similar finding to results in Turkey . We also detected mixed rotavirus infections in 6% of our patients, again similar to the findings in Turkey . The presence of mixed rotavirus infections indicates that new rotavirus strains may evolve by reassortment .\n\n【13】Finally, among the G9 strains, one PG9 had a long electropherotype, and one PG9 had a short electropherotype. The PG9 and PG9 strains were both cultured and subgrouped by ELISA with monoclonal antibodies and found to be of subgroup II. Partial sequences (bp) were obtained for their VP7 genes . They showed 99.4% similarity to each other and >99% similarity to strains from Australia (AY307087), Belgium (AY487858, AY487856), and India (RG9491165). A strain similar to our PG9, called variant 3, was first detected in India, and strains similar to our PG9, called variant 2, have been described in Bangladesh and in the United States .\n\n【14】Although the major global genotypes (except for G3 strains) were detected, clearly, rotavirus strains are continuing to diversify in Iraq and other parts of the region. This circumstance may pose challenges to the efficacy of rotavirus vaccines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "92b847c9-9130-40c8-9484-785b7043dff0", "title": "Invasive Salmonella enterica Serotype Typhimurium Infections, Democratic Republic of the Congo, 2007–2011", "text": "【0】Invasive Salmonella enterica Serotype Typhimurium Infections, Democratic Republic of the Congo, 2007–2011\n_Salmonella enterica_ serotype Typhimurium multilocus sequence type (ST) 313 has been reported as an emerging cause of invasive salmonellosis in sub-Saharan Africa . ST313 is almost exclusively from sub-Saharan Africa, is characterized by a degraded genome capacity similar to that of _S. enterica_ Typhi, has high rates of antimicrobial drug resistance, and is associated with bloodstream infections and mortality rates >25% . Whole-genome sequence analysis of 129 ST313 strains, isolated during 1988–2010 from 7 countries of sub-Saharan Africa, identified 2 dominant genetic lineages, I and II. These lineages emerged ≈52 and ≈32 years ago, respectively, possibly coevolving with the spread of HIV . Although lineage I has not been observed since the mid-2000s, lineage II has been observed with increasing frequency. However, data from Central Africa, particularly the Democratic Republic of the Congo (DRC) are scarce, and information is limited to 10 genomes from strains isolated >20 years ago . To determine whether ST313 is the dominant ST among invasive _S. enterica_ Typhimurium in the DRC, we studied 180 isolates collected during 2007–2011.\n\n【1】### The Study\n\n【2】We earlier described a series of invasive non-Typhi _S. enterica_ isolates from blood cultures collected in 7 of the 11 provinces in the DRC during 2007–2011  . In that study, a health care facility–based survey was administered to persons who met the eligibility criteria of suspected bacteremia at time of presentation and patient age >2 months. Blood culture vials were shipped to Kinshasa and processed according to standard identification procedures . A total of 233 non-Typhi _S. enterica_ isolates were recovered, 184 (%) of which belonged to serotype Typhimurium . The serotypes for all _S. enterica_ Typhimurium isolates were determined locally and later confirmed at the Institute of Tropical Medicine, Antwerp, Belgium. Most (/184, 98.7%) _S. enterica_ Typhimurium isolates were subsequently shipped to the Pasteur Institute, Paris, France, for further analysis.\n\n【3】The population structure of these 180 _S. enterica_ Typhimurium isolates was assessed by CRISPOL typing. CRISPOL is a recently developed high-throughput assay based on clustered regularly interspaced short palindromic repeat (CRISPR) polymorphisms . This bead-based hybridization assay is designed to detect the presence or absence of 72 short variable DNA sequences (spacers) from both CRISPR loci of _S. enterica_ Typhimurium. Initially, 245 different CRISPOL types (CTs) were identified in a 2012 study that included 2,200 isolates ; just before we conducted the study reported here, the CRISPOL _Salmonella_ Typhimurium database of the Pasteur Institute contained >7,000 strains comprising >750 different CTs.\n\n【4】A total of 174 (.7%) _S. enterica_ Typhimurium isolates from the DRC belonged to the CT28 group, of which 163 (.5%) were CT28. A total of 11 (.1%) isolates belonged to 7 other CTs that were single-spacer variants (loss of a single spacer), single-event variants (loss of \\> 2 contiguous spacers), or double-event variants of CT28 . Six (.3%) isolates belonged to 2 CTs not related to CT28. CT28 had been associated with ST313 in a multidrug-resistant DT56 _S. enterica_ Typhimurium isolate from Senegal and in the D23580 ST313 lineage II genome . In contrast, the analysis of raw pyrosequence data for genome A130 , representative of ST313 lineage I, corresponded to CT698, distinct from the CT28 group.\n\n【5】To confirm the association of ST313 to the CT28 group, we performed multilocus sequence typing (MLST)  on 12 isolates. A total of 3 isolates belonged to CT28, and 1 isolate of each single-spacer, single-event, and double-event variant was tested, resulting in 10 isolates from the CT28 group. We also performed MLST on 1 isolate of each of the 2 non–CT28 group isolates. All 10 CT28 group isolates tested were ST313; both non–CT28 group isolates tested were ST19.\n\n【6】Antimicrobial drug susceptibility has been studied with a limited panel of 7 drugs . We performed additional susceptibility testing by disk diffusion with a panel of 32 antimicrobial agents (Bio-Rad, Marnes-La-Coquette, France) . Extended-spectrum β-lactamase (ESBL) phenotype was assessed by using the double-disk synergy method . For all ESBL-producing isolates, MICs of ceftriaxone, ceftazidime, azithromycin, and imipenem were determined by the Etest macromethod (bioMérieux, Marcy L'Etoile, France). Results were interpreted according to break points defined by the Antibiogram Committee of the French Society for Microbiology . Susceptible strains were defined as having a ceftriaxone MIC < 1 mg/L, ceftazidime MIC < 4 mg/L, azithromycin MIC < 16 mg/L, and imipenem MIC < 2 mg/L. Resistance was defined as having a ceftriaxone MIC >2 mg/L, ceftazidime MIC >4 mg/L, azithromycin MIC >16 mg/L, and imipenem MIC >8 mg/L. The presence of macrolide resistance genes was assessed by PCR and sequencing as described elsewhere . Of the 174 CT28 group isolates, 167 (%) were resistant to ampicillin, chloramphenicol, and trimethoprim/sulfamethoxazole in combination with other drugs ; the remaining isolates were resistant to 1 or 2 of these drugs. Two isolates were resistant to extended-spectrum cephalosporins (ceftriaxone MIC 6–32 mg/L, ceftazidime MIC 4–32 mg/L); both contain the ESBL _bla_ SHV-2a  gene . We report that both isolates contain the _mph_ (A) gene encoding a macrolide 2′-phosphotransferase that inactivates macrolides (azithromycin MIC 96–128 mg/L). All 6 non-CT28 group isolates were susceptible to all drugs tested .\n\n【7】### Conclusion\n\n【8】Our data are based on the analysis of _S. enterica_ Typhimurium isolates recovered from >9,600 blood cultures collected during a 4-year period from distinct parts of the DRC. We found that >96% of the _S. enterica_ Typhimurium isolates belonged to the CT28 group. Because of the strong association between CT28 group and ST313, our findings suggest high rates of ST313 among invasive salmonellosis in the DRC.\n\n【9】Of the 10 genomes from the DRC isolated during 1988–1992 , genetic lineages I and II were identified at approximately equal rates. Of the more recent isolates  described here, all ST313 isolates belonged to the CT28 group, associated with lineage II. A notable feature of lineage II is chloramphenicol resistance resulting from a _cat_ gene within a specific Tn21-like element, carried by the virulence-associated plasmid pSLT . In the set described herein, we observed chloramphenicol resistance in >97% of all isolates belonging to the CT28 group. The almost complete replacement of lineage I isolates by lineage II isolates from Kenya and Malawi has also been reported .\n\n【10】Our data are based on invasive _S. enterica_ Typhimurium isolates collected in a nonsystematic health care facility–based approach and do not include noninvasive strains of _S. enterica_ Typhimurium. Wain et al. recently cited unpublished data showing that ST313 _S. enterica_ Typhimurium might be a common cause of gastroenteritis among immune-competent patients. A human reservoir for multidrug-resistant _S. enterica_ Typhimurium and Enteritidis in Kenya has been suggested because of the presence of similar strains in asymptomatic siblings and parents of index case-patients (carriage prevalence 6.9%) . Whole-genome sequencing of ST313 strains has shown genome degradation, including pseudogene formation and chromosomal deletions as have been observed for human-restricted _S. enterica_ Typhi , suggesting that ST313 might be undergoing an evolution toward niche specialization or, more likely, human adaptation .\n\n【11】Our results indicate very high rates of multidrug-resistant _S. enterica_ Typhimurium ST313 among invasive non-Typhi _Salmonella_ infections in the DRC. Future field studies involving patients with uncomplicated _Salmonella_ spp. infections will help determine whether ST313 _S. enterica_ Typhimurium in Central Africa is an opportunist or a primary pathogen. Systematic analyses of potential nonhuman and human reservoirs of _S. enterica_ Typhimurium might provide a better understanding of the transmission dynamics of this emerging pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a066eb2f-2334-4039-9a3f-7a165c22cfcc", "title": "Multidrug-Resistant Escherichia coli Bacteremia", "text": "【0】Multidrug-Resistant Escherichia coli Bacteremia\n**To the Editor:** Extraintestinal pathogenic _Escherichai coli_ (ExPEC) bacteria have the ability to cause diverse and serious diseases, such as urinary tract infections (UTIs) and bacteremia ; incidence of bacteremia is increasing globally . The emergence of multidrug resistance in _E. coli_ is also becoming a global concern, with particular emphasis on _E. coli_ sequence type (ST) 131, which is being increasingly reported in UTIs. Drug resistance is mediated by extended-spectrum β-lactamases (ESBLs), mainly of the CTX-M family, particularly CTX-M-15 and 14, and less frequently of the SHV and OXA families . Few studies are available regarding the characterization of _E. coli_ strains causing bacteremia.\n\n【1】We characterized 140 _E. coli_ isolates from bacteremia patients treated at Nottingham University Hospital (Nottingham, UK) over a 5-month period, with the aim of developing an epidemiologic profile of the population of ExPEC that causes bacteremia. For context, we compared the isolates with 125 _E. coli_ isolates from urine samples collected during the same period. Cases were selected to include isolates from a diverse patient group: patient ages ranged from 1 month to 90 years; patient sex was evenly divided between male and female; infections were community- and hospital-associated; and suspected sources of infection varied. Antimicrobial drug susceptibility tests, PCR detection of ESBL genes ,  multilocus sequence typing using the Achtman scheme , and virulence-associated gene (VAG) carriage screening by PCR were performed on isolates as described .\n\n【2】Significantly more bacteremia _E. coli_ isolates than urine _E. coli_ isolates were resistant to ciprofloxacin (.7% vs. 8.8%; p < 0.001) and cefradine (.0% vs. 11.2%; p < 0.05). These results were reflected in the number of isolates in the 2 populations displaying a multidrug- resistance phenotype (resistance to antimicrobial drugs belonging to \\> 2 classes); a significantly higher number of multidrug-resistant bacteremia _E. coli_ isolates than multidrug-resistant urine isolates were found (.7% vs. 32%; p = 0.01). PCR screening for ESBL carriage showed significantly higher ESBL carriage in bacteremia _E. coli_ isolates than urine isolates for _bla_ SHV  (.7% vs. 5.6%; p = 0.008), _bla_ CTX-M  (.3% vs. 17.6%; p = 0.025), and _bla_ OXA  (.3% vs. 6.4%; p = 0.037). Total ESBL carriage for bacteremia isolates was also significantly higher than for urine isolates (.3% vs. 29.6%; p < 0.001).\n\n【3】Multilocus sequence types were determined for all _E. coli_ isolates. A total of 63 STs were found among the urine isolates ; the highest prevalence was ST73 (n = 16, 12.8%), followed by ST131 (n = 9, 7.2%), ST69 (n = 9, 7.2%), ST95 (n = 6, 4.8%), ST404 (n = 6, 4.8%), ST127 (n = 4, 3.2%), ST141 (n = 4, 3.2%), and ST10 (n = 3, 2.4%). Prevalence patterns of STs among bacteremia _E. coli_ isolates were noticeably different . Three main STs were obtained. ST131 dominated (n = 30, 21.43%) and was significantly higher in prevalence than for the urine isolates (p < 0.001). ST73 (n = 24, 17.14%) and ST95 (n = 13, 9.29%) were the other 2 primary STs found. The 8 most prevalent STs in the bacteremia isolates represented 59.29% of the total population, whereas the 8 most prevalent STs in the urine isolates represented 45.6% of the total population. This finding is suggestive of selection of a smaller number of dominant STs in bacteremia.\n\n【4】ESBL carriage was mapped onto minimum-spanning trees for the 2 isolate groups. ESBL carriage among urine isolates was focused on a small number of STs; 19 (.16%) of the 63 STs contained ESBL-positive isolates . The predominant ST73 group contained 18.75% ESBL-positive isolates; the other predominant STs exhibited ESBL-positive isolates at the following levels: ST131 (.44%), ST69 (.33%), ST95 (%), and ST10 (%). In contrast, 30 (.72%) of the 58 STs among bacteremia isolates contained ESBL-positive isolates, significantly higher than for the urine isolates (p = 0.016). At the ST level, predominant STs had higher ESBL carriage in the bacteremia isolates than in the urine isolates: ST131 (%), ST73 (%), ST12 (%), ST10 (%), ST14 (%), ST2278 (.33%). ST95 (.15%) and ST69 (%) showed comparable levels. These results suggest that ESBL drug resistance is selecting for dominant ExPEC bacteremia strains.\n\n【5】To investigate whether the differences in ST observations between bacteremia and urine isolates could be attributable to differences in virulence genes, VAGs of all isolates were screened by multiplex PCR. VAGs were found equally distributed across the 2 populations, with no statistically significant difference (p = 0.675). Comparison of serum resistance levels between urine and blood isolates also showed no phenotypic differences.\n\n【6】In conclusion, we found high levels of ESBL carriage and multidrug resistance in ExPEC isolates that cause bacteremia. A comparison with urine isolates provided evidence that ESBL-mediated drug resistance appears to be the selective pressure in the emergence of dominant STs in bacteremia. Future research should focus on identifying if prolonged antimicrobial drug treatment in bacteremia patients is leading to this selection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "95fb9bff-abe4-4d48-9c47-76c22476ff4a", "title": "Neisseria gonorrhoeae Resistant to Ceftriaxone and Cefixime, Argentina", "text": "【0】Neisseria gonorrhoeae Resistant to Ceftriaxone and Cefixime, Argentina\n**To the Editor:** Antimicrobial resistance in _Neisseria gonorrhoeae_ is increasing globally. In recent years, gonococcal strains with resistance to the extended-spectrum cephalosporin (ESC) ceftriaxone have been reported from many countries . In South America, 7 ceftriaxone-resistant strains (MICs >0.25 μg/mL) were reported from Brazil in 2007; however, these isolates have not been characterized . Emergence of cephalosporin-resistant gonorrhea would substantially limit treatment options and represent a major public health concern. We report an _N. gonorrhoeae_ isolate in Argentina that was resistant to ceftriaxone and cefixime.\n\n【1】In September 2014, a 19-year-old heterosexual man with no underlying disease was admitted to a hospital emergency department in Rio Negro, Argentina. Physical examination showed a purulent urethral discharge. He reported having had unprotected insertive vaginal sex with multiple partners in the past few months. The patient denied recent travel outside Argentina. He had a history of gonococcal urethritis (March 2014), which was treated with a single dose (mg) of ceftriaxone; a gonococcal isolate obtained at that time was not sent to a reference laboratory as part of the Argentinian Gonococcal Antimicrobial Susceptibility Surveillance Program for determination of the antimicrobial susceptibility profile.\n\n【2】At the time of admission, we obtained and cultured a urethral swab specimen and identified an isolate as _N. gonorrhoeae_ by using conventional methods . The isolate was sent to a reference laboratory, and its identity was confirmed by using the Phadebact GC Monoclonal Test (MKL Diagnostic AB, Sollentuna, Sweden) and matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (Bruker Daltonik GmbH, Bremen, Germany). The patient was given a single dose (mg) of ceftriaxone. Five days later, the patient returned for clinical evaluation, and symptoms had resolved. Culture for test of cure was not performed.\n\n【3】We determined MICs of the isolate for penicillin, cefixime, ceftriaxone, tetracycline, ciprofloxacin, and azithromycin by using the agar dilution method according to the standard of the Clinical and Laboratory Standards Institute . Results were interpreted in accordance with breakpoints of this standard, except for azithromycin, for which we applied breakpoints of the European Committee on Antimicrobial Susceptibility Testing .\n\n【4】This isolate was resistant to ceftriaxone and cefixime (MIC 0.5 μg/mL), penicillin (μg/mL), tetracycline (μg/mL), and azithromycin (μg/mL). However, the isolate was susceptible to ciprofloxacin (.03 μg/mL). We obtained a negative result for β-lactamase by using the chromogenic nitrocefin disk assay (Becton Dickinson, Franklin Lakes, NJ, USA).\n\n【5】Resistance determinants involved in ceftriaxone resistance were amplified by PCR and sequenced as reported . We than analyzed full-length _penA_ and _pilQ_ gene sequences and other genetic determinants, including _mtrR_ , _penB_ , and _ponA_ genes. We purified and sequenced PCR products by using an ABI 3500 Genetic Analyzer (Applied Biosystems, Foster City, CA, USA) and performed molecular epidemiologic characterization by using _N. gonorrhoeae_ multiantigen sequence typing .\n\n【6】Sequence analysis of the _penA_ gene, which encodes penicillin-binding protein 2 (PBP2), identified a nonmosaic PBP2 IX allele. Analysis of the _mtrR_ gene and its promoter identified a single nucleotide (A) deletion in the inverted repeat of the promoter region, and a single amino acid substitution at position H105 (H→Y). Amino acid substitutions in the _porB1b_ gene were found at positions G120 (G→K) and A121 (A→D). The _ponA_ gene, which encodes PBP1, had an amino acid substitution at position L421 (L→P), and full-length PilQ amino acid sequence had sequence type VI. The isolate was assigned to serogroup PorB1b (WII/III), and _N. gonorrhoeae_ multiantigen sequence typing showed that it had novel sequence type ST13064 (porB_ \\-7592 and _tbpB_ \\-33).\n\n【7】Mosaic PBP2 alleles have been strongly associated with decreased susceptibility or resistance to ESCs . However, the isolate we obtained had a nonmosaic PBP IX allele, which contains the P551L substitution that has been associated with increased MICs for ESCs . Association of the nonmosaic PBP IX allele, most likely with _mtrR_ , _penB_ , and _ponA_ gene mutations, might be involved in resistance to ESC . Although Whiley et al. did not report an association between sequence type VI and resistance to ESCs, the contribution of this sequence type to ESC resistance requires additional studies.\n\n【8】Isolates with decreased susceptibility and resistance to ESC have now emerged in Argentina , increasing from 1.1% in 2011 to 5.6% in 2014 (Argentinian Gonococcal Antimicrobial Susceptibility Surveillance Program, unpub. data). Treatment failures and isolates with reduced susceptibilities and resistance to ESC have been reported . However, in Argentina, syndromic management of gonorrhea has resulted in suboptimal diagnosis and lack of specimens to culture to distinguish between treatment failure and reinfection. Syndromic management represents a major problem that not only compromises surveillance program but also increases selective pressure and facilitates development of drug resistance. Accordingly, surveillance should be strengthened to support detection and verification of asymptomatic infections and treatment failures, identify communities at high risk, and trace sexual contacts. These efforts should be used in public health responses to mitigate emergence and spread of ESC-resistant gonococci.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "03ea4b2e-8709-4598-827a-e3490fe1cc74", "title": "Hepatitis B Infection, Eastern India", "text": "【0】Hepatitis B Infection, Eastern India\n**To the Editor** : The National Institute of Cholera and Enteric Diseases (ICMR), Kolkata, India, conducted a serologic study in July 2003 to determine the rate of hepatitis B virus (HBV) infection of brothel-based commercial sex workers. These study participants worked in the South-24 Parganas district of West Bengal, one of the eastern states of India. Routine immunization to prevent HBV infection is not a practice in India, and chronic HBV infection is endemic . The nature of their work makes commercial sex workers more vulnerable to HBV infection, which could accelerate the infection’s spread into the general community, particularly in areas with low literacy rates and socioeconomic status.\n\n【1】The study participants were 167 commercial sex workers from three prominent brothels, which were located in small towns and along the national highway of the district. Blood samples from the participants were tested by using the HBsAg unlinked anonymous method. The results showed that 23.3% (unpub. data, National Institute of Cholera and Enteric Diseases) of the commercial sex workers were infected with HBV. The Figure shows the distribution of commercial sex workers and prevalence of HBV infection by age group.\n\n【2】To ascertain a possible route of transmission other than sexual activity, all study participants were questioned about their medical history, including jaundice, injury requiring blood transfusion, surgical events, and tattooing. The evidence indicated that HBV infection in the study participants was not acquired by any route other than sexual activity. Since the carrier rate of chronic HBV in the general population is approximately 5% in eastern India , this increase in chronic HBV infection can be attributed to sexual transmission.\n\n【3】The Figure indicates that HBV infection is not correlated to age or duration of commercial sex work; the rate of infection is distributed almost equally in all age groups except the 26- to 30-year-old group in which it was slightly lower. Most of the HBV-infected commercial sex workers likely were infected early in their profession and remained infected throughout their lives, which led to higher rate of chronic infection as compared with the general population.\n\n【4】This higher rate of chronic HBV infection among commercial sex workers (.3%) is of concern, particularly in a country with an estimated 4.58 million persons infected with HIV. In India, >80% of HIV infection is transmitted sexually . HIV infection can affect the natural course of HBV; sexual activity between HIV- and HBV-infected persons could prolong the infected status of those infected with HBV, as was shown in a previous study . This sexual activity could facilitate HBV transmission, particularly in areas that have few resources or where the rate of condom use is low or questionable.\n\n【5】After this study concluded, all study participants were notified of their infection status and advised to use condoms when engaging in sexual activity. The commercial sex workers and their family members were advised that they should be tested for HBV infection and receive HBV vaccination if test results were negative. Local health authorities were advised that commercial sex workers and their clients should be vaccinated to prevent HBV infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5ef1af2f-8537-4722-b039-14bab0363d54", "title": "Autochthonous and Dormant Cryptococcus gattii Infections in Europe", "text": "【0】Autochthonous and Dormant Cryptococcus gattii Infections in Europe\nDuring the past decade, the basidiomycetous yeast _Cryptococcus gattii_ has stepped out of the shadows of its sibling _C. neoformans_ . The latter species mainly infects immunocompromised persons, and _C. gattii_ mainly infects apparently immunocompetent persons. _C. neoformans_ is found globally, and _C. gattii_ has been mostly limited to tropical and subtropical areas in Central Africa, northern Australia, and Central and South America . However, this distribution pattern changed after an unprecedented outbreak of _C. gattii_ emerged in the temperate climate of British Columbia, Canada, and expanded to the Pacific Northwest region of Canada and the United States . Epidemiologic studies have shown that _C. gattii_ occurs in areas other than tropical or subtropical zones, such as in Mediterranean Europe, northern Europe, and western Australia .\n\n【1】For the purpose of studying the epidemiology of _C. gattii_ , a broad variety of molecular biological techniques have been developed, including PCR fingerprinting, restriction fragment length polymorphism analysis of the _PLB1_ and _URA5_ loci, amplified fragment length polymorphism (AFLP) fingerprint analysis, and several multilocus sequence typing (MLST) approaches . These laboratory investigations have shown that _C. gattii_ can be divided into 5 distinct genotypes: AFLP4/VGI, AFLP5/VGIII, AFLP6/VGII, AFLP7/VGIV, and AFLP10/VGIV . Serotype B strains occur in genotypes AFLP4/VGI, AFLP6/VGII, and AFLP10/VGIV; serotype C strains are restricted to genotypes AFLP5/VGIII and AFLP7/VGIV .\n\n【2】Recently, a consensus MLST scheme was proposed for epidemiologic investigations of _C. gattii_ and _C. neoformans_ , specifically, the nuclear loci _CAP59_ , _GPD1_ , IGS1, _LAC1_ , _PLB1_ , _SOD1_ , and _URA5_ . So far, this consensus MLST scheme has been used to study the population structure of _C. neoformans_ strains from Thailand and _C. gattii_ strains from Australia .\n\n【3】We investigated the occurrence of _C. gattii_ in Europe, focusing on whether this pathogen is emerging and, if so, how to explain this emergence pattern. Furthermore, we explored whether the infections originated from Europe or were introduced from other continents. To achieve these goals, members of the European Confederation of Medical Mycology were asked to send recently obtained human patient isolates of the species for detailed AFLP and MLST analyses. Thus, the genetic diversity of the yeast was used to trace its geographic origin to identify where the infections were acquired. AFLP genotyping results were combined with published _C. gattii_ MLST results from Byrnes et al. and Fraser et al. which were extended to match the _Cryptococcus_ consensus MLST scheme . Our study produced the following 5 conclusions: all hitherto known genotypes of _C. gattii_ are emerging in Europe; genotype AFLP4/VGI isolates predominate; a _C. gattii_ cluster, which is endemic to Mediterranean Europe and genetically distinct from the other populations, exists; several human infections are caused by travel-related acquisition of _C. gattii_ outside Europe; and autochthonous cases occur in Europe.\n\n【4】### Materials and Methods\n\n【5】##### Strains and Media\n\n【6】We compared a collection of 107 isolates collected from Europe with 194 isolates collected globally . All isolates were checked for purity and cultivated on malt extract agar medium (Oxoid, Basingstoke, UK). Cultures were incubated for 2 days at 30°C. A working collection was made by growing _C. gattii_ strains on malt extract agar slants for 2 days at 30°C, after which the strains were stored at 4°C. Strains were stored long term at −80°C by using the Microbank system (Pro-Lab Diagnostics, Richmond Hill, Ontario, Canada).\n\n【7】##### Amplification and Sequencing of MLST Loci\n\n【8】Genomic DNA extraction, AFLP genotyping, and mating-type determination by amplification of either the _STE12_ **a** or _STE12_ α locus were performed as described . The 7 nuclear consensus MLST loci (CAP59_ , _GPD1_ , IGS1, _LAC1_ , _PLB1_ , _SOD1_ , and _URA5_ ) were amplified by using the preferred primer combinations . To compare the current set of _C. gattii_ strains with those from a published _C. gattii_ population biology study, we included the 3 nuclear loci that are not part of the consensus MLST scheme (CAP10_ , _MPD1_ , and _TEF1_ α) . We also included isolates from a global study by Byrnes et al. and Fraser et al. and subjected them to amplification and sequencing of the _CAP59_ , _SOD1_ , and _URA5_ loci.\n\n【9】Amplifications were conducted in a 25-µL PCR mixture containing 37.5 mmol/L MgCl 2  (Bioline, London, UK), 1× PCR buffer (Bioline), 1.9 mmol/L dNTPs (Bioline), 0.5 U Taq DNA polymerase (Bioline), 5 pmol of both primers (Biolegio, Nijmegen, the Netherlands) , and ≈100 ng of genomic DNA. PCRs were conducted with an initial denaturation step at 94°C for 5 min, followed by 35 cycles of denaturation at 94°C for 30s, annealing for 30s , extension at 72°C for 1 min, followed by 72°C for 5 min and a final dwell at 21°C.\n\n【10】Sequencing reactions were conducted with the BigDye version 3.1 chemistry kit (Applied Biosystems, Foster City, CA, USA) as described . For all amplification products except _CAP59_ , the initial amplification primers were used for sequencing reactions. For _CAP59_ , the newly designed forward primer CAP59L-Fwd and the original reverse primer JOHE15438  were used.\n\n【11】##### Sequence Alignment and Phylogenetic and Recombination Analyses\n\n【12】Consensus sequences were assembled and checked for ambiguities by using SeqMan version 8.0.2 (DNASTAR, Madison, WI, USA). Sequence alignments were generated with MEGA version 5  by using the standard settings and manual correction. The genome sequence databases of reference strains H99  and JEC21  were used to extract the corresponding sequences for all 10 investigated nuclear loci to serve as an outgroup. The best fitting nucleotide substitution model was determined by using MrModeltest version 2  and was conducted for the complete _C. gattii_ 10-loci MLST, for the accepted consensus MLST scheme (CAP59_ , _GPD1_ , IGS1, _LAC1_ , _PLB1_ , _SOD1_ , and _URA5_ ) , and a previously launched _C. gattii_ MLST scheme (CAP10_ , _GPD1_ , IGS1, _LAC1_ , _MPD1_ , _PLB1_ , and _TEF1_ ) . As a result, the HKY G+I model (Hasegawa-Kishino-Yano plus gamma distributed with invariant sites) was the best model to use for analyzing the phylogeny of the _C. gattii_ isolates for all 3 datasets. The evolutionary history was inferred by using the maximum-likelihood method in MEGA version 5 . A bootstrap consensus tree was inferred from 1,000 replicates to show the relevant lineages obtained in this analysis.\n\n【13】We calculated the haplotype diversity (H R _ ), equal to the Simpson diversity index (D_ ), by using the Microsoft Excel (Microsoft, Redmond, WA, USA) add-in called Haplotype Analysis . For this purpose, sequences were collapsed into sequence type numbers .\n\n【14】### Results\n\n【15】##### AFLP Genotypes and Geographic Distribution\n\n【16】The 301 _C. gattii_ isolates collected from Europe and other areas around the world could be divided into the following genotypes: 146 AFLP4/VGI (.2%; 72 from Europe), 22 AFLP5/VGIII (.6%; 1 from Europe), 108 AFLP6/VGII (.1%; 23 from Europe), 13 AFLP7/VGIV (.5%; 2 from Europe), and 2 AFLP10/VGIV (.7%; both from Europe). From 10 isolates (AFLP8 \\[5 from Europe\\] and 3 AFLP9 \\[2 from Europe\\]), genotypes represented interspecies _C. neoformans_ × _C. gattii_ hybrids. These 10 isolates were excluded from further analysis because amplified fragments of hybrid isolates will result in mixtures of different alleles. The 57 human patient isolates from Europe were obtained from 40 patients . The genotypic diversity of the remaining 291 _C. gattii_ isolates (from Europe, 191 from other areas) with a haploid genotype is shown in the Table .\n\n【17】##### Phylogeographic Origin\n\n【18】Sequence type diversity was calculated for each of the MLST loci, all 10 loci, and the combined datasets according to Fraser et al. and Meyer et al. The _CAP10_ locus showed the lowest overall diversity (n ST  \\= 19; _D_ ST  \\= 0.765) and the IGS1 locus showed the highest diversity (n ST  \\= 52; _D_ ST  \\= 0.930). The 10-loci MLST dataset showed the highest diversity (n ST  \\= 150; _D_ ST  \\= 0.975), followed by the MLST scheme of Meyer et al. (D_ ST  \\= 0.971)  and Fraser et al. (D_ ST  of 0.959) . The latter MLST scheme differentiated more sequence types than the consensus MLST scheme (n ST  \\= 136 vs. 127).\n\n【19】Maximum-likelihood analysis of the MLST data showed that the _C. gattii_ isolates clustered in 5 monophyletic clusters, which were highly supported and agreed with the AFLP genotypes . When each genotypic cluster was separately analyzed, support values of the branches were low, <75 . For genotype AFLP4/VGI isolates, bootstrap support values for nearly all branches were low; for the second largest group formed by AFLP6/VGII isolates, branches were better supported.\n\n【20】Phylogenetic analysis demonstrated that several human patient genotype AFLP4/VGI isolates from Europe had an autochthonous origin because they formed a separate cluster with environmental and animal isolates from the same area. A set of isolates from human patients on the Iberian Peninsula (CCA232, CCA242L, CCA242T, CCA311, CCA312, CL6148) were found to be genetically indistinguishable from isolates obtained from animals or the environment in the same area (indicated in Technical Appendix Figure as the European Mediterranean cluster). The human patient isolates from Europe with genotype AFLP4/VGI, which were probably acquired within Europe because patients did not have histories of travel outside Europe, are IP2005/215 and IP2006/958 (France), RKI85/888 (Germany), 5UM and 75UM (Italy), RKI97/482 (Portugal), and CBS2502 (the Netherlands). Some of these human patient isolates were closely related (e.g. IP2005/215 and RKI97/482) or even genetically indistinguishable (e.g. CBS2502 and RKI85/888).\n\n【21】A large proportion of the human patient and environmental isolates of _C. gattii_ AFLP4/VGI from Italy and Spain formed a novel autochthonous Mediterranean MLST cluster that was genetically homogeneous, irrespective of their origin or mating type . _C. gattii_ AFLP4/VGI was involved with numerous small outbreaks among goats in Spain, and the isolates were genetically indistinguishable from recently obtained human patient, animal, and environmental isolates from different provinces in Spain as well as from AFLP4/VGI mating-type **a** isolates from Italy .\n\n【22】Several _C. gattii_ genotype AFLP6/VGII infections were found to have originated in Europe (e.g. the IP1998/1037–1 and −2, IP2003/125, and CCA242 isolates), and all fell within a cluster that could not be linked to an environmental source. The same holds true for the human patient isolates from Greece (AV54S, –W, and IUM01–4731), which came from the same patient who had no history of travel outside Greece.\n\n【23】In the phylogenetic analysis, isolates from human patients in Europe were also observed next to those originating from other geographic areas. The most striking example was a set of 4 human patient isolates from citizens of Denmark, the Netherlands, Germany, and Switzerland, in whom cryptococcosis developed after they had visited Vancouver Island, British Columbia, Canada. The isolates obtained from these tourists (CBS10485, RKI06/496, RKI01/774, and CBS10866, respectively) had MLST profiles identical to that of _C. gattii_ AFLP6A/VGIIa, the genotype that caused the Vancouver Island outbreak  . The outbreak-related sets of AFLP6A/VGIIa and AFLP6C/VGIIc isolates from the Vancouver Island and Pacific Northwest outbreaks, respectively, are within outbreak-specific clusters . A similar finding was observed for a set of 5 human patient _C. gattii_ AFLP6B/VGIIb isolates (IP1996/1120–1 and −2, IP1999/901–1 and −2, and IP2000/87) in France, which had been obtained from patients who had emigrated from Africa to France and which were genetically indistinguishable from an isolate (IP2001/935–1) from a resident of Senegal . This set of 5 isolates from France and 1 from Senegal were closely related to isolates from the Vancouver Island and Pacific Northwest outbreaks; however, it seems unlikely that the patient from Senegal had traveled to these outbreak areas, and none of the patients in France reported having traveled to Canada or the United States.\n\n【24】A set of _C. gattii_ AFLP4/VGI mating-type α isolates from Portugal (IP1997/18) and Belgium (IHEM19725B and –S) was found to be indistinguishable from human patient isolates from the Democratic Republic of the Congo and Rwanda (B3939 and CBS6289, which were both mating-type **a** , and IHEM10602S, IHEM10769S, and IHEM10769W, which were mating-type α) . Both phenotypically different isolates of IHEM19725 were obtained from an HIV-infected patient from Rwanda who had emigrated to Belgium. The CBS1622 isolate from Europe, obtained from a patient with the oldest documented case of a _C. gattii_ infection , was found to be genetically indistinguishable from a set of isolates from North America.\n\n【25】The single _C. gattii_ AFLP5/VGIII isolate from a 24-year-old immunocompetent patient from Germany (isolate RKI97/310)  clustered with human patient and environmental isolates from Mexico . The 2 _C. gattii_ AFLP7/VGIV isolates CBS7952D and CBS7952S, obtained from an HIV-infected patient in Sweden, were genetically indistinguishable from each other and had a unique MLST profile that clustered with human patient isolates from Africa. According to the patient’s history, years before the onset of cryptococcal infection, she had emigrated from Zambia to Sweden . Another example of a reactivated dormant _C. gattii_ infection is that of the human patient isolate from Italy, IUM92–6682 (AFLP4/VGI), obtained from an immunocompetent immigrant from Brazil, which had an identical MLST profile to 6 human patient mating-type α isolates (IHEM14934, IHEM14956, IHEM14965, IHEM14968, IHEM14976, and IHEM14984) from Brazil .\n\n【26】Infection was acquired outside Europe for 24 of these patients (isolates) and within Europe for 16 patients (isolates) . Among these 57 human patient isolates, most ([82.5%\\]) were obtained since 1995, the remaining 10 (.5%) were isolated during 1895–1994. One of these isolates originated from 1985, another 2 isolates (CBS1622 and CBS2502), from 1895 and 1957, were retrospectively found to represent _C. gattii_ isolates from Europe .\n\n【27】### Discussion\n\n【28】In Europe, _C. gattii_ has been reported as a rare cause of apparently autochthonous cryptococcal infections . The earliest documented case that turned out to be caused by _C. gattii_ in Europe was made by Curtis in 1896 . Until the 1980s, cryptococcosis was rarely observed in Europe and _C. gattii_ infections were especially rare; only 2 cases have retrospectively been found . In the 1980s, infections were reported for 2 immunocompetent citizens of Germany, who had never traveled abroad . Subsequent case reports described _C. gattii_ infections that were imported or acquired in Europe . Since 1995, the number of reports of _C. gattii_ infections increased and describe _C. gattii_ infections in humans and animals from Greece , Italy , and Spain . In the current study, 8 (%) cases were observed until 1995, and 32 (%) cases in humans have been observed since 1995. We excluded cases for which an isolate was not available for confirmation. This exclusion is especially relevant in that we re-identified a reported _C. gattii_ infection as actually being caused by _C. neoformans_ . These data suggest that during the past 2 decades, _C. gattii_ has been emerging in Europe.\n\n【29】In the current study, 16 (%) of the human patient isolates from Europe were found to have an autochthonous origin in Europe that either could be linked to environmental isolates from the Mediterranean area or that came from patients who had never traveled outside their resident country. A total of 24 (%) isolates could be linked to _C. gattii_ –endemic regions in Brazil, the United States, Africa, and the Vancouver Island outbreak region. These observations demonstrate that _C. gattii_ infections can be imported subclinically and can cause infections after being dormant for many years.\n\n【30】Nearly all isolates from Mediterranean Europe belonged to genotype AFLP4/VGI, and MLST analysis showed that these isolates form a separate cluster within this genotype  . _C. gattii_ genotype AFLP4/VGI has also recently been reported from the environment in the Netherlands, but these isolates were not similar to any of the AFLP4/VGI isolates from Mediterranean Europe or to isolate CBS2502, which was isolated in 1957 from a pregnant citizen of the Netherlands, who had never traveled abroad but who died of cryptococcosis . Isolate CBS2502 is genotypically identical to isolate RKI85/888, which was isolated from a previously healthy citizen of Germany, who also had never traveled outside Germany. These 2 cases strongly suggest that different genotypes of _C. gattii_ AFLP4/VGI occur in the environment of northwestern Europe because isolates from patients were genetically different from the recently reported isolates from the environment of the Netherlands (this study; _4_ ).\n\n【31】In conclusion, _C. gattii_ is emerging in Europe, and the isolates from Europe can be divided into 5 genotypic clusters. Most _C. gattii_ infections in Europe are probably autochthonous, and several infections are proven to have been acquired outside the European continent, e.g. during visits to _C. gattii_ –endemic regions, such as the Vancouver Island outbreak area or before migration to Europe from _C. gattii_ –endemic regions in Africa and South America. Reactivation of dormant _C. gattii_ infections and of infections acquired outside Europe after immune suppression occurs more often than previously assumed. This finding might suggest that _C. gattii_ infections caused by certain genotypes are associated with altered immune status of the human host. Thus, _C. gattii_ is probably a more opportunistic pathogen, as has recently been hypothesized, than a strictly primary pathogen . _C. gattii_ isolates with genotypes AFLP5/VGIII and AFLP7/VGIV were rarely found in Europe and were all acquired outside the European continent. However, these genotypes have frequently been isolated from HIV-infected persons and other immunocompromised patients in Africa and the American continents . A connection seems to exist between these _C. gattii_ genotypes and the host’s immune status. Further epidemiologic and immunologic research is needed to unravel this apparent correlation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "47689831-2f20-4ae0-8e9b-c3cb4d50c66a", "title": "Bluetongue in Captive Yaks", "text": "【0】Bluetongue in Captive Yaks\n**To the Editor:** In August 2006, several northern European countries including Belgium reported cases of bluetongue (BT) . This noncontagious, arthropod-borne animal disease is caused by _Bluetongue virus_ (BTV), genus _Orbivirus,_ family _Reoviridae_ . The genome of BTV consists of 10 segments of double-stranded RNA; 24 serotypes have been reported . Serotype 8 (BTV-8) was implicated in the emergence in Belgium . All ruminant species are thought to be susceptible to BT . We report laboratory-confirmed clinical cases of BT in yaks (Bos grunniens grunniens_ ).\n\n【1】Yaks living in captivity in a Belgian animal park showed clinical signs of BT. A clinical examination performed on 1 yak showed loss of weight associated with a progressive weakness linked to anorexia, ulcerative and necrotic lesions on the muzzle with some crusts and mucopurulent nasal discharge, and udder erythema with papules and crusts. The tongue was severely swollen and cyanotic and protruded from the mouth . The animal was reluctant to move and was recumbent (possibly as a consequence of podal lesions linked to BT); it died 7 days after examination. Necropsies were performed on carcasses of this and another yak. The main lesions found were severe diffuse congestion of the lungs with edema and emphysema, acute hemorrhagic enteritis restricted to the ileum and jejunum, and petechial hemorrhages on the abomasums. No lesions characteristic of coronitis were noted.\n\n【2】Samples of spleen and bone marrow were taken and prepared according to the method of Parsonson and McColl . A real-time reverse transcription quantitative–PCR (RT-qPCR) targeting BTV segment 5 (RT-qPCR\\_S5) was used to detect BTV RNA in tissues samples. Each test was performed in parallel with a RT-qPCR that amplifies β-actin mRNA as an internal control (RT-qPCR\\_ACT). Both assays were conducted according to Toussaint and others , with slight modifications. Briefly, total RNA was purified from 25 mg of tissue by Trizol extraction (Invitrogen, Carlsbad, CA, USA) and denatured by heating for 3 min at 95°C with 10% dimethylsulfoxide (Sigma-Aldrich, St. Louis, MO, USA). Reverse transcription reactions were conducted by using the Taqman reverse transcription reagents according to the manufacturer instructions (Applied Biosystems, Foster City, CA, USA). RT-qPCR reactions consisted of 1× concentrated Taqman fast universal PCR master mix (Applied Biosystems), 375 nM (β actin) or 500 nM (BTV) of each primer, 250 nM Taqman probe, and 5 μL cDNA. Cycling conditions were as follows: 1 cycle at 95°C for 20 s, followed by 45 cycles of 1 s at 95°C, and 20 s at 60°C. The specificity of the RT-qPCR used had been previously tested against prototype strains of genetically related viruses (strains of epizootic hemorrhagic disease virus and 9 strains of African horse sickness virus) . The RT-qPCR tests confirmed BTV viremia.\n\n【3】The yak species in its natural biotope is usually rarely exposed to competent _Culicoides_ vectors. Antibodies against BTV have been found in many wild ruminants , and our results extend the number of ruminant species susceptible to BTV. In the northern European BT outbreak, lesions in cattle and sheep were mainly localized to the regions of the muzzle, mouth, and eye; clinical signs were not always obvious . As in cattle and sheep, clinical signs in yaks were observed on the muzzle, in the periocular region, and around and inside the mouth. These signs clearly reflected viral-induced endothelial damage triggering disseminated intravascular coagulation and a hemorrhagic diathesis commonly described in sheep and cattle . In our case, lesions depicted pronounced microvascular damage. According to the severity of the lesions and rates of illness and death observed, the yak, like sheep, appears to be highly susceptible to BTV.\n\n【4】In the epidemiology of BT in African countries, cattle and wild ruminant species such as antelopes play a role as asymptomatic reservoir hosts of the virus . Some wild ruminant species in captivity could also play this role in European countries affected by the recent BT outbreak. These cases could be of particular concern for all parks and zoos that gather numerous wild ruminants. Illness, reproductive failure, and deaths usually reported with BT  could generate substantial losses on these premises. Moreover, the source of BTV-8 in the northern European outbreak remains unclear, and the role of wild ruminant species has to be taken into account. In the future, European authorities should consider vaccination to prevent the spread of the disease in European member states . All premises with wild ruminants need to be involved in BT control and prophylaxis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e8632b58-4727-4830-991f-76b95504a13e", "title": "Responding to Communicable Diseases in Internationally Mobile Populations at Points of Entry and along Porous Borders, Nigeria, Benin, and Togo", "text": "【0】Responding to Communicable Diseases in Internationally Mobile Populations at Points of Entry and along Porous Borders, Nigeria, Benin, and Togo\nThe consequences of insufficient national and regional public health capacities at points of entry (POEs), such as established airports, seaports, or ground crossings, in border regions and among internationally mobile populations became apparent during the 2014–2016 West Africa Ebola epidemic. Within weeks of the first Ebola case in a remote area of Guinea, the epidemic had inconspicuously spread across land borders to Liberia and Sierra Leone . A limited number of cases spread over land to Senegal and Mali and through air travel to Nigeria, Spain, and the United States . Throughout the almost 2-year epidemic, common local and long-distance international human movement between highly connected communities increased the geographic impact of disease.\n\n【1】National public health systems are designed to detect communicable diseases among established communities and healthcare infrastructure and to respond to minimize their domestic spread. Economic, linguistic, familial, health-seeking, and other factors influence the complexity of cross-border networks. The associated formal and informal international movement challenges national systems’ capacities to detect public health events among these mobile populations . Border health strategies minimizing the risk of importation and exportation of disease through POEs, as well as across porous land borders, are not a common feature of national surveillance systems.\n\n【2】In 2005, all World Health Organization (WHO) signatory member states renewed their commitment to addressing the elevated health risks of our increasingly interconnected world by adopting the revised International Health Regulations 2005 (IHR 2005) . These regulations define legally binding requirements to mitigate the international spread of disease, including required public health capacities at POEs and detection and response collaboration between neighboring and regional countries. Under the IHR 2005, member states are responsible for designating the airports, seaports, and, where justified for public health reasons, ground crossings that must meet POE core capacity requirements defined in the IHR Annex 1 . Many countries have not yet met IHR 2005 obligations for designated POEs, leaving them particularly vulnerable to possible importation or exportation of communicable diseases .\n\n【3】The US Centers for Disease Control and Prevention (CDC) Division of Global Migration and Quarantine (DGMQ), part of the National Center for Emerging and Zoonotic Infectious Diseases, oversees the achievement and maintenance of IHR 2005 core capacities at US POEs. Given this domestic experience, DGMQ began responding to requests for technical assistance from Guinea, Liberia, Sierra Leone, and other regional countries in August 2014 to initiate and strengthen border health measures, primarily exit screening at international airports . These measures helped Ebola-affected countries meet WHO recommendations, thereby enabling at least some commercial air carriers to continue servicing these countries and providing a vital channel for provision of supplies and response personnel .\n\n【4】As the number of Ebola cases declined, DGMQ evolved its strategy in the region from outbreak response to longer-term border health capacity building under the premise that effective border health strategies before and during a public health event can help reduce the risk of exporting or importing a communicable disease. Border health strategies could potentially obviate the need for unaffected countries to implement costly entry-screening measures for persons returning from affected countries, as many Western countries did during the Ebola epidemic . In this article, we describe a set of border health system strengthening strategies, along with successes and lessons learned from integrating those strategies through partnerships with Nigeria, Benin, and Togo. These countries are highlighted because of their contributions to enhanced global health security through their substantial progress with implementing a comprehensive border health approach.\n\n【5】### Strategies\n\n【6】DGMQ created the International Border Team (IBT), which, with funding from the Global Health Security Agenda, established formal partnerships with 10 countries (Benin, Côte d’Ivoire, Ghana, Guinea, Guinea-Bissau, Liberia, Nigeria, Senegal, Sierra Leone, and Togo) to advance a comprehensive border health strategy . In this article, we describe in detail the development of each component in this strategy: 1) operational IHR 2005–compliant public health emergency response plans (PHERPs) and supporting standard operating procedures (SOPs) at nationally prioritized POEs; 2) plans for allocating resources to strengthen detection, notification, and referral procedures for prioritized geographic areas and POEs at highest risk for importation or exportation of a high-consequence communicable disease owing to population connectivity and international travel patterns; and 3) timely cross-border and regional public health data sharing, coordination, and collaboration to detect and respond to communicable disease.\n\n【7】##### Border Health Strategy 1—Developing POE-Specific PHERPs and SOPs\n\n【8】The IHR 2005 require designated POE to demonstrate capacity for “appropriate public health emergency response by establishing and maintaining a public health emergency contingency plan” . At many POEs, individual agencies often know appropriate procedures to take during a public health event, yet the procedures are not documented or shared. In the absence of an agreed-upon plan, stakeholders risk gaps or redundancies in communication, surveillance, and response efforts, consequently increasing the risk of an uncoordinated and delayed response. Public health response plans and SOPs are beneficial at IHR-designated POE, as well as at smaller, less-resourced POEs.\n\n【9】A POE PHERP with accompanying SOPs is a multiagency coordination plan that describes procedures to prevent the introduction and transmission of suspected communicable diseases through that POE during routine and response operations. Having the SOPs in writing—available, trained on, and exercised—ensures a timely and coordinated response with all involved sectors. In the airport context, public health, civil aviation, airport authorities, safety and security agencies, airlines, medical and ambulance services, police, and other agencies that have a role in implementing the PHERP are all critical participants in developing, finalizing, exercising, and operationalizing the plan.\n\n【10】The International Civil Aviation Organization (ICAO), the UN specialized agency that ensures that member states’ civil aviation operations and regulations conform to global norms, has developed aviation sector guidelines in accordance with IHR 2005, including those for the development of public health emergency contingency plans at airports . When developing an airport plan, partners must reconcile ICAO guidance with multiple other global guidance documents as well as other key airport and country planning documents, such as the Aerodrome Emergency Plan, the National Civil Aviation Plan, and the National Public Health Plan. To facilitate the PHERP development process, IBT created a template document, consolidating the WHO Guide for Public Health Emergency Contingency Planning at Designated Points of Entry  and the ICAO Template for a National Aviation Public Health Emergency Preparedness Plan . IBT also documented the methodology to create a PHERP through a core planning team, and incorporated lessons learned from DGMQ’s experience in developing communicable disease response plans in the United States. Partner countries have also applied the PHERP development process to seaports and ground crossings.\n\n【11】##### Border Health Strategy 2—Establishing Priorities for Capacity Building at Identified POE and Border Regions\n\n【12】Nations often have insufficient financial and personnel resources to build robust border health capacity at all POEs and along entire international borders. To address these challenges, nations can strengthen border health by allocating resources to select POEs and border areas, prioritized by public health risk of importation or exportation of communicable disease, among other considerations. IBT has developed a low-resource field method to gather information from national, subnational, and local stakeholders and community members to characterize population mobility patterns and strength of proximal and distant intercommunity connectivity. This method consists of key informant and focus group discussion guides that a facilitator uses in conjunction with maps of the relevant geographic areas to guide participants through describing the characteristics of those who move into, through, between, and out of identified areas with population movement and connectivity patterns that may increase the impact of a public health event. Nations can use the information, summarized in reports and on maps, to inform their understanding of areas, including POEs, at disproportionately higher public health risk of importation or exportation of communicable disease based on human movement .\n\n【13】The WHO IHR 2005 core capacities self-assessment tool enables nations to quantitatively measure current IHR capacities at POEs . However, the IHR self-assessment tool was developed to evaluate capacities at designated POEs, often international airports, with established infrastructure and resources, and is not as applicable to lower-resource POEs, such as many ground crossings, especially those that are far from urban centers. Further, although the IHR self-assessment tool reserves space to record comments for each question, tool implementation and results analysis focus on the quantitative results. In 2015, IBT developed the Border Health Capacity Discussion Guide (BHCDG) and piloted it in 5 West Africa countries . The BHCDG complements the IHR self-assessment tool by gathering qualitative information from national, subnational, and border area stakeholders on border health capacities, where infrastructure may not be robust. Nations can use the BHCDG alone or with the IHR self-assessment tool to better understand current capacities and develop an action plan to strengthen gaps in detection, notification, and referral procedures. Specifically, the guide facilitates the collection of qualitative information on the following:\n\n【14】*   Communication capacity: communication systems, including identified points of contact for ground crossings, to report and receive notifications of public health events and communication efforts to inform travelers and neighboring communities on public health events or interventions\n\n【15】*   Information and data systems: border characteristics, including additional, proximal, unofficial ground crossings, traveler volume, purpose of travel; surveillance systems that incorporate health assessments and responses to public health events at ground crossings; and plans and procedures for public health data sharing with cross-border and regional counterparts about events, such as outbreaks and case investigations\n\n【16】*   Response and referral systems: public health and medical services available at and/or near ground crossings and coordination with referral health facilities and response plans and training describing how to prepare for, and respond to, public health events at ground crossings.\n\n【17】##### Border Health Strategy 3—Timely Cross-Border and Regional Public Health Collaboration\n\n【18】Effective and timely national health surveillance, coupled with communication and coordination with neighboring and regional countries, supports achieving the IHR principle to protect “all people of the world from the international spread of disease” (IHR 2005 Article 3.3 ). Through border health strategies 1 and 2, nations build public health capacities at designated and prioritized POEs and border areas to better detect and notify public health events among most international travelers. However, persons travel across porous borders outside a POE or may pass through a POE undetected by health screening measures for several reasons, including being asymptomatic while traveling. Border health strategy 3 addresses the development of cross-border relationships that support prompt communication and coordination between neighboring and regional countries to report and respond to communicable disease events with elevated risk of cross-border transmission.\n\n【19】Nations should incorporate all POE, regardless of infrastructure, into their national health surveillance systems as additional peripheral reporting units expected to follow standard, site-appropriate detecting and reporting practices . For example, after detecting an ill traveler, a POE official could record event information on a standardized surveillance report form and submit that form to the POE’s referral facility or surveillance unit. Where POEs are not staffed, and along borders without identified POEs, nations can provide communities with additional education to empower them to report potential priority communicable diseases following standard procedures.\n\n【20】To support binational and multinational public health collaboration and coordination, nations can develop and disseminate clear national- and local-level plans that, among other objectives, define when and what public health event information to share across a border, and how to maintain coordination with cross-border counterparts. Some of these collaborations exist informally, but without formalized documentation they may not be clearly defined, may be challenging to supervise, and may not reflect the most current policies and priorities. In addition to documenting domestic plans at the national and local levels, nations can work with neighbors to create integrated cross-border communication and response plans. Real-time data sharing and coordination across borders benefit from maintenance of multinational plans and procedures, along with routine communication to ensure that the plans reflect current priorities.\n\n【21】### Successes and Lessons Learned\n\n【22】##### Border Health Strategy\n\n【23】Port Health Services of the Federal Ministry of Health in Nigeria, with implementation support from Pro-Health International (PHI) and technical guidance from IBT, began developing, operationalizing, and training staff on PHERPs at 2 international airports: Murtala Muhammed International Airport in Lagos, the 5th busiest airport in Africa, and Nnamdi Azikiwe International Airport in Abuja, the 13th busiest  . To develop these plans, Port Health Services, Federal Ministry of Health, airport authority, civil aviation, airlines, immigration, customs, and security partners actively participated in a series of PHERP and SOP development workshops facilitated by IBT and PHI.\n\n【24】During the introductory PHERP workshops, participants established multiagency core planning teams composed of 8 to 10 persons nominated based on their experience, knowledge, and ability to represent their agencies during the planning process. PHI and Port Health Services, with technical guidance from IBT, facilitated a series of core planning team meetings for Murtala Muhammed International Airport, resulting in a complete PHERP after 10 months. The approved plan now serves as one of the first IHR 2005–compliant PHERPs in West Africa.\n\n【25】The Nnamdi Azikiwe International Airport core planning team, established in March 2016, also finished its PHERP after 11 months of planning. In addition, the core planning team, IBT, and PHI are initiating a new training curriculum and exercise schedule. This training and exercise series is informed by best practices from US CDC quarantine stations and designed to enable responders to execute the PHERP. These tools and workshops can be adapted for use at other types of POE, such as seaports or ground crossings.\n\n【26】##### Border Health Strategy\n\n【27】In 2016, Togo and Benin, with implementation support from the Abidjan Lagos Corridor Organization (ALCO) and technical guidance from IBT, used IBT field methods to better understand population movement patterns and connectivity related to economic opportunities, healthcare seeking, and cultural festivals, among other factors, with a geographic focus along the international coastal highway—critical because of the high international traveler volume. These countries are working collaboratively, along with IBT and ALCO, to interpret and map the information about crucial points of interest and linguistic, tribal, and other factors associated with the populations that congregate or travel to or through these points. In addition, the countries are using the information to improve national and cross-border surveillance plans including, for example, strengthening preparedness for movement associated with annual celebrations attracting regional visitors. Further, Togo, Benin, and Nigeria are analyzing population mobility and retrospective cholera surveillance data to inform coordinated preparedness and response plans. The countries used this approach to strengthen cross-border coordination during a multinational Lassa fever outbreak in early 2017.\n\n【28】The Benin and Togo ministries of health used the BHCDG following its adoption, in consultation with WHO, ALCO, and IBT, at nationally prioritized ground crossings along the corridor (Kodjoviakopé and Sanvee Condji in Togo and Hillacondji and Kraké in Benin) and a binationally prioritized ground crossing on their shared border (Tohoun and Aplahoue) . The BHCDG findings gathered from local officials at the POE revealed details about a consistent lack of plans and procedures for responding to public health events, few or no formal mechanisms for collaboration or communication with the neighboring country during a health crisis, and lack of transport and referral mechanisms in place for ill travelers identified at the border. The ministries of health, with technical support from IBT, are implementing an action plan to address the identified areas for improvement using the BHCDG results.\n\n【29】In Nigeria, PHI facilitated BHCDG discussions with personnel at the Semé and Idiroko ground crossings with Benin, the busiest ground crossing in Nigeria. PHI, in collaboration with WHO and the Federal Ministry of Health, adapted the BHCDG to focus on border health human resources, the surveillance system, and binational and regional data sharing—areas not covered in depth by the IHR 2005 self-assessment tool. These discussions occurred 2 weeks after a baseline IHR self-assessment conducted by WHO and national authorities. PHI presented results from the BHCDG discussions to the IHR competent authority and the WHO, who are developing a POE-specific action plan to address gaps identified through the IHR self-assessment and BHCDG activity.\n\n【30】##### Border Health Strategy\n\n【31】Nigeria’s surveillance system has identified border communities as key components and provides them with tailored training on how to detect and report public health event information. This training, implemented by PHI and the Nigeria Centre for Disease Control, led to improved relationships and communication between border area personnel, health facilities, Local Government Area (Nigeria’s district-level administrative unit) surveillance and Port Health officials, and the national level.\n\n【32】The International Border Team and ALCO, with cosponsoring from the US Agency for International Development (USAID) Benin, facilitated 2 multinational meetings among Nigeria, Benin, Togo, Ghana, and Cote d’Ivoire (which participated in the second meeting only), to formalize cross-border and regional public health data sharing and coordination strategies. Participants included IHR 2005 national focal points, ministry of health legal representatives, national and local public health surveillance leads, national immigration representatives, local port health and quarantine representatives, national and local agricultural and animal health representatives, and the Field Epidemiology Training Program Benin resident advisor. Products from these successful meetings include a draft memorandum of understanding and 7 supporting SOPs and annexes covering the following topics: priority diseases for real-time cross-border reporting; minimum reporting requirements for a cross-border report of a communicable disease; national activities to support cross-border coordination across public health response activation phases; determination of whether a public health event meets criteria for a cross-border report of a communicable disease; determination of whether a public health event meets criteria for responding to a cross-border report of a communicable disease; communication structure for reporting a cross-border event; and communication structure for responding to a cross-border report of a public health event.\n\n【33】In addition to signing the final documents, follow-up steps include consolidating and disseminating cross-border contact information for public health officials working in border districts. Participants noted that they will use the final compendium of jointly produced documents as a training manual for officials working along the borders.\n\n【34】### Discussion\n\n【35】Human mobility is inherently associated with the spread of infectious diseases . As transportation networks expand, the speed of travel increases, the volume of passengers and the goods they transport grows, and the potential for the spread of pathogens and their vectors from remote locations to distant countries increases. The Global Health Security Agenda was launched in 2014 to accelerate IHR 2005 implementation to advance global capacity to rapidly detect, respond to, and control public health emergences at their source . To be maximally effective, a comprehensive global health security agenda must incorporate POEs, border regions, and internationally mobile populations. We have described a set of border health strategies that, when implemented together, are designed to advance national, binational, and regional border health systems. These advanced systems can contribute to improved early detection, effective communication, and timely and adequate response, thereby reducing the risk of international spread of communicable diseases without hindering the free movement of persons and goods.\n\n【36】Border health approaches, for the most part, can leverage tools and strategies in the existing public health and medical systems and infrastructure. The International Border Team’s experience working with partners in Nigeria, Benin, and Togo demonstrates several successes with implementing low-resource methods to strengthen border health capacities. Perhaps the most noteworthy success across all border health strategies was the bringing together of partners to improve multinational and multisectoral collaborations and communication.\n\n【37】Having a written emergency response plan is a key IHR 2005 requirement. Almost complete PHERPs and priority SOPs have been developed for 2 of the highest-volume international airports in West Africa, with others at varying stages of development. Completion, operationalization, and exercising of the PHERPs will help countries meet several of their IHR requirements.\n\n【38】National leaders in each country, along with ALCO and PHI, expressed that the additional information provided by the BHCDG helped them develop a more complete understanding of existing border health capacities and added context to the quantitative results of the IHR 2005 self-assessment. BHCDG information also catalyzed expanding POE-focused capacity-building plans to encompass strengthening communication networks with neighboring countries. The countries plan on using the BHCDG at other priority ground crossings identified, in part, by using information on cross-border population mobility and connectivity.\n\n【39】The challenges experienced to date may be typical of any multisectoral, multinational partnership and were often overcome because of the value placed on the partnerships and in maintaining open dialogue. West Africa has many critical public health challenges; occasional delays in implementation of the comprehensive border health strategy are the result of partners having to respond to competing, higher-priority problems. Achieving consensus on plans and approvals to implement new strategies is time-consuming because of the number of stakeholders who must validate them. Finally, incompatible technology and processes, as well as different languages in neighboring countries, add complexity to information sharing.\n\n【40】Despite the challenges, for resource-limited countries with porous land borders and high cross-border mobility resulting from shared familial, cultural, linguistic, and economic ties, border health security, and therefore health security as a whole, is best achieved by implementing a comprehensive border health strategy involving relevant local, national, and regional sectors. The examples from Nigeria, Benin, and Togo demonstrate that development of a border health system can be successful by including PHERPs for POEs, prioritizing border areas through risk-based assessments using the BHCDG and population mobility mapping, and enhancing timely cross-border surveillance and coordination. Implementing these strategies will help to achieve global health security by supporting countries to prevent the spread of potential health threats across international borders.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "16950819-6a59-40ed-8b1d-209db4799e7a", "title": "Carriage Rate and Effects of Vaccination after Outbreaks of Serogroup C Meningococcal Disease, Brazil, 2010", "text": "【0】Carriage Rate and Effects of Vaccination after Outbreaks of Serogroup C Meningococcal Disease, Brazil, 2010\nIn Brazil, meningococcal disease is endemic; 1.5–2.0 cases per 100,000 inhabitants were reported during 2000–2009. Since 2002, a substantial increase has been observed in the proportion of cases attributed to meningococcus serogroup C (MenC) that is associated with the sequence type (ST) 103 complex, and MenC is currently responsible for most cases of meningococcal disease in Brazil .\n\n【1】Several outbreaks of MenC disease have been reported in Brazil in recent years . To control these outbreaks, chemoprophylaxis is administered to contacts of infected persons, and vaccination is often recommended for persons in age groups at higher risk for infection. In these reactive vaccination campaigns, meningococcal C conjugate (MCC) vaccine use is restricted to children <2 years of age because of cost and supply issues; meningococcal A/C polysaccharide vaccine is recommended for persons \\> 2 years of age .\n\n【2】Published data describing meningococci carriage in Brazil are limited. Few studies have been conducted that assess 1) the role of carriage prevalence in the dynamics of carriage and disease or 2) the potential effect of control programs, such as vaccination, on the transmission of meningococci. Thus, we conducted a cross-sectional study with the primary objective of assessing the prevalence of meningococcal carriage among workers at 2 oil refineries in São Paulo State, Brazil, where outbreaks of MenC disease occurred in 2010. We also investigated the effect of meningococcal A/C polysaccharide vaccination and risk factors on pharyngeal carriage of meningococci.\n\n【3】### Methods\n\n【4】During March 29–June 30, 2010, an outbreak of MenC disease, associated with the ST103 complex, occurred in an oil refinery (Refinery A) with 17,590 workers in São Paulo State, Brazil. A total of 18 cases and 3 deaths (case-fatality rate 16.7%) were associated with the outbreak. Six of the cases and 2 deaths involved Refinery A workers, and 12 of the cases and 1 death involved contacts (family members) of the refinery workers. The case-patients were residents of Cosmópolis, a municipality with 59,000 inhabitants located near Refinery A.\n\n【5】On March 29, health authorities were notified of the first 3 case-patients (adult workers at Refinery A and an 8-month-old child whose father worked at Refinery A). An investigation was initiated, and chemoprophylaxis with rifampin was recommended for all close contacts of the 3 index case-patients. During the following 2 weeks, 5 new cases of MenC disease were identified (in Refinery A workers and 2 in children who were relatives of Refinery A workers). With these new cases, the incidence of meningococcal disease reached 34.1 cases/100,000 persons at Refinery A. Meningococcal A/C polysaccharide vaccination was recommended for all 17,590 workers at Refinery A. Vaccination began on April 16, and 1 week later, 91% coverage of workers at Refinery A was achieved. However, despite the vaccination program, 10 new cases of MenC disease occurred: 9 cases were in family contacts and 1 case was in a Refinery A worker who had received vaccine 1 day before symptom onset.\n\n【6】The incidence of MenC disease in Cosmópolis subsequently reached 20.2 cases/100,000 persons. Cases occurred in relatives (months to 16 years of age) of Refinery A workers, prompting a mass vaccination of 18,571 inhabitants of Cosmópolis who were 2 months to 19 years of age. Vaccination began on June 30, and 90.5% coverage was achieved 1 week later. Infants and toddlers received MCC vaccine, and persons 2–19 years of age received meningococcal A/C polysaccharide vaccine. In the months following the vaccination campaign, no more MenC cases were reported, and the outbreak was considered controlled.\n\n【7】The second outbreak of MenC disease occurred in a refinery with 16,000 workers (Refinery B) in São José dos Campos, a city with 610,095 inhabitants in São Paulo State. On July 10, 2010, a worker at Refinery B was reported to have MenC disease, and on July 18, a second worker was reported to be infected. An investigation identified 10 other reported cases in São José dos Campos during April–July, 2010; the 10 cases were in children <4 years of age who were household contacts of Refinery B workers. Of the 12 identified case-patients, 6 died. As in Cosmópolis, these initial cases were considered the index cases. In Refinery B, the incidence of meningococcal disease reached 12.5 cases/100,000 persons, and a decision was made to provide chemoprophylaxis, but not vaccine, to all close contacts of index case-patients. On August 8, 1 new case of meningococcal disease was reported in a family contact of a Refinery B worker; no further cases were reported in 2010.\n\n【8】Beginning in December 2010, we conducted a cross-sectional study of 483 workers (years of age) from Refinery A, where mass vaccination had been recommended, and Refinery B, where mass vaccination had not been advised. All study participants gave informed consent. A questionnaire was used to obtain information regarding age, sex, recent respiratory tract infections, active and passive smoking, alcohol consumption, recent antimicrobial drug use, length of employment at the refinery, number of household members living in the same room, level of education, and meningococcal A/C polysaccharide vaccination status.\n\n【9】##### Specimen Collection\n\n【10】During the first 2 weeks of December, 2010, we obtained oropharyngeal swab samples from 483 refinery workers (vaccinated workers from Refinery A and 245 nonvaccinated workers from Refinery B). The samples were immediately put into transport medium  and, within 4–5 h, sent to the Adolfo Lutz Institute (São Paulo, Brazil), the National Reference Laboratory for Bacterial Meningitis, where they were stored until use. The stored oropharyngeal swabs were plated onto selective medium, and after 24–48 h of incubation at 37°C (±2°) in 5% CO 2  , the samples were inspected. Samples with meningococcus-like colonies were subcultured on blood agar medium for species identification. Isolates identified as _Neisseria meningitidis_ were serogrouped by using an agglutination test. Antisera were obtained for serogroups A, B, C, E, W, X, Y, and Z .\n\n【11】##### DNA Extraction and Real-Time PCR\n\n【12】DNA from each sample was extracted and purified by using the QIAamp DNA MiniKit (QIAGEN, Alameda, CA, USA) or a similar testing kit according to the manufacturer’s instructions. Primers and fluorescent probes were used for the detection of _N. meningitidis_ _ctrA_  and _sodC_ genes by real-time PCR . Samples positive for _N. meningitidis_ were genotyped by using primers and fluorescent probes for _N. meningitidis_ serogroups A, B, C, W, and X.\n\n【13】##### Serotyping and Multilocus Sequence Typing\n\n【14】Serotyping for all _N. meningitidis_ isolates was performed by dot blot analysis, using whole-cell suspensions as described . Multilocus sequence typing was performed according to the methods of Maiden et al. Primers, determination of sequence alleles, and designation of sequence types are described on the Neisseria Multi Locus Sequence Typing website .\n\n【15】##### Statistical Analyses\n\n【16】Using an estimate that the prevalence of meningococci carriage among adults would be ≈18% (±5%), we calculated that ≈225 study participants from each refinery would be needed to analyze all variables. Demographic data for all participants and typing results of _N. meningitidis_ isolates were entered into an EpiInfo database  and compared by using the 2-sided Fisher exact test. Assessment of risk factors was performed using Fisher exact test.\n\n【17】### Results\n\n【18】Of the 483 oropharyngeal samples tested, 104 (.5%; 95% CI 18.0%–25.5%) were positive for meningococci. Carriage rates were similar among workers from both refineries (.4% vs. 21.6%). Of the 104 positive samples, 95 were detected by culture and real-time PCR, 1 was detected by culture only, and 8 were detected by real-time PCR only.\n\n【19】The serogroup and genogoup could be determined for 56 of the 104 meningococci-positive samples: 27 (.2%) were serogroup C, 9 (.1%) serogroup B, 8 (.3%) serogroup E, 7 (.5%) serogroup Y, and 5 (.9%) serogroup W. The serogroup could not be determined for 48 (.1%) isolates. The difference in MenC carriage rates among workers at the 2 refineries was not significant: 6.3% at Refinery A and 4.9% at Refinery B (p = 0.48) .\n\n【20】##### Serotyping and Multilocus Sequence Typing\n\n【21】A total of 38 different serotype–serosubtype antigen combinations were identified among the 96 _N. meningitidis_ isolates. Among MenC isolates, phenotype C:23:P.14–6 was the most prevalent (/13 \\[77%\\]). Eleven different STs were found among 27 isolates characterized by multilocus sequence typing. The 11 STs were grouped into 6 different clonal complexes: ST103 complex (n = 7), ST11 complex (n = 5), ST213 complex (n = 1), ST32 complex (n = 3), ST41/44/Lineage 3 (n = 1), ST461 (n = 1). The most prevalent clonal complex, ST103 complex, was represented by ST3780 (n = 6) .\n\n【22】We did not find an increased risk of meningococci carriage associated with any of the potential risk factors studied, except low level of education. A low education level (i.e. not completing secondary education) was significantly associated with a higher risk for carriage of meningococci, regardless of serogroup identification .\n\n【23】### Conclusions\n\n【24】Most published studies report a consistently low rate (usually <1%) of MenC carriage during outbreaks of MenC disease . However, after outbreaks at 2 oil refineries in São Paulo State, Brazil, we found high rates (.3% and 4.9%, respectively) of MenC carriage among refinery workers.\n\n【25】Mass vaccination with a meningococcal A/C polysaccharide vaccine was conducted at Refinery A, and high coverage (%) was achieved among workers. This intervention controlled the MenC outbreak in the refinery; only 1 new case occurred after the vaccination campaign, but that case cannot be considered the result of a vaccine failure because it occurred <14 days after the refinery worker was vaccinated. These findings likely indicate that the workers received direct protection against MenC from vaccine. However, after the vaccination campaign, 9 new cases of MenC infection occurred in children who were household contacts of vaccinated workers, without any known contact among them.\n\n【26】The prevalence of MenC carriage was high among workers at both refineries, even though 91% of Refinery A workers had received meningococcal A/C polysaccharide vaccine 6 months before our study began. More striking, carriage rates among vaccinated and nonvaccinated workers were similar. These findings suggest that meningococcal A/C polysaccharide vaccine had no effect on MenC carriage. Most of the studies conducted among nonmilitary populations demonstrated that these vaccines cannot significantly reduce meningococcal carriage . The short-term persistence of circulating antibodies and the quality of the immune response induced after vaccination with a polysaccharide vaccine may partly explain why these vaccines have no effect on carriage .\n\n【27】In contrast to polysaccharide vaccines, conjugate vaccines lead to the production of very high antibody concentrations, even in infants, and induce immunologic memory with higher antibody avidity and increased serum bactericidal activity, thus providing more robust long-term protection. In addition, conjugate vaccines also prevent the acquisition of carriage among vaccinees and, by interrupting transmission, provide indirect protection to unvaccinated, susceptible persons; this herd immunity proved key to the success of MCC vaccination programs in various countries .\n\n【28】The characterization of the _N. meningitidis_ strains isolated from the patients (workers and family contacts) during the outbreak in Refinery A has been described . The characterization showed that all MenC isolates were genetically related and displayed the same phenotype, C:23:P1.14–6, associated with ST3780 of the ST103 complex. The characterization of the 13 MenC carriage strains recovered from workers at both refineries in our study showed that most (/13) displayed the C:23:P1.14–6 phenotype. These strains displayed 2 STs: ST3780, which belongs to ST103 complex, and ST8730, assigned without clonal complex. In Brazil, the increase in MenC disease during the last decade has been associated with the emergence of this virulent clone belonging to the ST103 complex . The ability of MCC vaccines to effect carriage of strains from the ST103 complex has yet to be shown. The recent introduction of MCC vaccine in the routine immunization program in Brazil will provide this opportunity, highlighting the importance of carefully designed studies to measure the effect of the vaccine on carriage and transmission.\n\n【29】Meningococcal carriage was not associated with any of the risk factors evaluated in our study, except the level of education, which was inversely related to the prevalence of carriage. The higher percentage of MenC carriers among study participants with a lower level of education presumably reflects associated socioeconomic conditions and social behaviors. Less-educated workers in oil refinery settings are also more likely to perform activities that require the use of ear devices as protection from the loud environment. The wearing of such devices forces workers to stay very close to each other to facilitate conversation among them, and such close working situations also facilitate transmission of meningococci.\n\n【30】Although the relationship between meningococci carriage prevalence and disease incidence is not fully understood, the evidence gathered during this study showed a dominance of the C:23:P1.14–6 phenotype strain among workers from both refineries, reinforcing the concept that the dominance of a particular strain is an important marker of epidemic conditions . Also, in accordance with previous findings from other studies, we observed that polysaccharide vaccination had no effect on carriage and did not interrupt transmission to susceptible contacts . These results represent a challenge to the current policy of using the meningococcal polysaccharide A/C vaccine to control outbreaks of MenC disease, and they have key implications for future vaccination strategies. Our findings emphasize the need to review such policies and to consider using MCC vaccines rather than meningococcal polysaccharide A/C vaccines to control MenC disease outbreaks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "06610a0b-685f-4b00-8b45-e4e6813979a4", "title": "Novel Type of Chronic Wasting Disease Detected in Moose (Alces alces), Norway", "text": "【0】Novel Type of Chronic Wasting Disease Detected in Moose (Alces alces), Norway\nTransmissible spongiform encephalopathies (TSEs), or prion diseases, are fatal and transmissible neurodegenerative diseases that include scrapie in sheep and goats, bovine spongiform encephalopathy (BSE) in cattle, chronic wasting disease (CWD) in cervids, and Creutzfeldt-Jakob disease in humans. TSEs are characterized by the misfolding of the normal host-encoded cellular prion protein (PrP C  ) into an abnormal disease-associated isoform (PrP Sc  ). PrP Sc  is considered to be the main or exclusive component of prions, the transmissible agents for TSEs . TSEs might have a genetic, infectious, or sporadic origin. Classical scrapie and CWD can be highly contagious, spreading directly among animals or through environmental contamination.\n\n【1】Since its first description in Colorado in 1967, CWD has been detected in new geographic areas and with increasing prevalence in captive and free-ranging cervids. Currently, the disease has been diagnosed in 25 states in the United States and in 2 Canada provinces , along with cases in South Korea associated with importation of infected cervids from Canada . High disease prevalence in some areas represents a challenge for preservation of wild cervids and mitigation of human exposure to CWD-related prions (PrP Sc  ). The high prevalence might be a plausible explanation for local deer population decline .\n\n【2】Species naturally affected by CWD include white-tailed deer (Odocoileus virginianus_ ), mule deer (O. hemionus_ ), moose (Alces alces_ ), elk or wapiti (Cervus canadensis_ ), and red deer (C. elaphus_ ). In 2016, CWD was reported for the first time in Europe in wild reindeer (Rangifer tarandus_ ) , a species never previously found to be naturally infected. The biochemical analysis and immunohistochemical (IHC) distribution of PrP Sc  from Norway reindeer revealed a pattern indistinguishable from North America isolates .\n\n【3】We report 3 cases of CWD detected in moose in Norway, characterized by biochemical and IHC features clearly different from CWD cases previously described in North America and Norway. Our findings suggest the involvement of a different type of CWD prion.\n\n【4】### Materials and Methods\n\n【5】##### Animals and Tissues\n\n【6】The 3 moose were found in Trøndelag County in central Norway. The first case, moose no. 1 (ID P138), was emaciated and demonstrated abnormal behavior, showing reduced fear of humans. The second case, moose no. 2 (ID P153), was found dead in a river. Necropsy revealed normal body condition and pregnancy with twins; trauma was the cause of death. The third case, moose no. 3 (ID CD11399), was observed showing abnormal behaviors, including reduced fear of humans. Necropsy revealed a poor body condition and a severe dislocation of the left hip joint, which might have influenced the animal’s behavior. All 3 were older female moose (and 13 years old, based on counts of cementum annuli in the root of the first incisor ).\n\n【7】Samples included in this study are described in the Table . We performed the primary diagnostic test (TeSeE ELISA; Bio-Rad Laboratories, Inc. Hercules, CA, USA) for detection of protease-resistant core of PrP Sc  (PrP res  ) on the 3 moose and 1 reindeer from Norway  in the medulla oblongata at the level of the obex. After the initial positive test results, the remaining brain tissues were divided; one half was fixed in 10% neutral buffered formalin, and the other half was frozen. In addition, lymph nodes (Ln) from moose no. 1 (retropharyngeal, submandibular, and jejunal Ln), moose no. 3 (retropharyngeal, parotid, prescapular, and submandibular Ln, and tonsils), and the reindeer (tracheobronchial Ln) were equally divided and formalin fixed or frozen.\n\n【8】##### Genotyping of Moose _PRNP_\n\n【9】DNA was extracted from 100 mg of brain tissue by using a DNeasy Blood and Tissue Kit (QIAGEN, Hilden, Germany), according to the manufacturer’s instructions. The _PRNP_ coding sequence was amplified in a 50 µL final volume by using 5 µL of extracted DNA, eluate 1X AmpliTaq Gold 360 PCR Buffer (Life Technologies, Carlsbad, CA, USA ), 2.5 mmol/L MgCl 2  , 1X 360 GC Enhancer (Life Technologies), 200 µmol/L dNTPs, 0.25 µmol/L of forward (′-GCTGACACCCTCTTTATTTTGCAG-3′) and reverse (′-GATTAAGAAGATAATGAAAACAGGAAG −3′) primers , and 0.5 µL AmpliTaq Gold 360 (Life Technologies), according to the following amplification protocol: 5 min at 96°C; 30 s at 96°C; 15 s at 57°C; 90 s at 72°C for 40 cycles; and 4 min at 72°C. Amplicons were purified with the Illustra ExoProStar 1-Step clean-up kit (GE Healthcare Life Sciences, Little Chalfont, UK), sequenced using the Big Dye Terminator Cycle Sequencing Kit v1.1 (Life Technologies), and purified with the Big Dye XTerminator Purification Kit (Life Technologies), and detected by using an ABI PRISM 3130 apparatus (Life Technologies).\n\n【10】##### Anti–Prion Protein Monoclonal Antibodies\n\n【11】Several antibodies with different epitopes (sheep prion protein \\[PrP\\] numbering) were used for discriminatory Western blot (WB) and IHC. SAF84 (aa 167–173) was obtained from Bertin Pharma (Montigny-le-Bretonneux, France), L42 (aa 148–153) from R-Biopharm (Darmstadt, Germany), 9A2 (aa 102–104) and 12B2 (aa 93–97) from Wageningen Bioveterinary Research (Lelystad, Netherlands), and F99/97.6 (aa 220–225) from VMRD, Inc. (Pullman, WA, USA).\n\n【12】##### Immunohistochemistry\n\n【13】Brain, Ln, and tonsil tissues were formalin fixed for \\> 48 h and processed by standard histopathologic techniques. We used IHC to visualize the distribution of PrP Sc  as previously described . We applied a commercially available kit (EnVisionTM + System HRP \\[AEC\\]; DAKO, Glostrup, Denmark) by using the monoclonal antibodies (mAbs) 12B2, 9A2, L42, SAF 84, or F99/97.6 for 30 min at 37°C and counterstained with hematoxylin. In each run, tissue from CWD-negative moose and reindeer were added as negative controls.\n\n【14】##### PrP res  Detection\n\n【15】We initially tested the obex samples by using the TeSeE SAP ELISA and confirmed positive ELISA results by using the TeSeE WB. Both tests were performed as recommended by the manufacturer (Bio-Rad).\n\n【16】##### Typing of PrP res\n\n【17】We performed further characterization of PrP res  by discriminatory immunoblotting, according to the ISS discriminatory WB method . Brain homogenates at 10% (wt/vol) in 100 mmol/L Tris-HCl (pH 7.4) 2% sarkosyl were incubated for 1 h at 37°C with Proteinase K (Sigma-Aldrich, St. Louis, Missouri , USA) to a final concentration of 200 µg/mL. Protease treatment was stopped with 3 mmol/L PMSF (Sigma-Aldrich). Aliquots of samples were added with an equal volume of isopropanol/butanol (:1 vol/vol) and centrifuged at 20,000 × _g_ for 10 min. The pellets were resuspended in denaturing sample buffer (NuPAGE LDS Sample Buffer; Life Technologies) and heated for 10 min at 90°C.\n\n【18】We loaded each sample onto 12% bis-Tris polyacrylamide gels (Invitrogen) for electrophoresis with subsequent WB on polyvinylidene fluoride membranes using the Trans-Blot Turbo Transfer System (Bio-Rad) according to the manufacturer’s instructions. The blots were processed with anti-PrP mAbs by using the SNAP i.d. 2.0 system (Millipore, Burlington, MA, USA) according to the manufacturer’s instructions. After incubation with horseradish peroxidase–conjugated anti–mouse immunoglobulin (Pierce Biotechnology, Waltham, MA, USA ) at 1:20,000, the PrP bands were detected by using enhanced chemiluminescent substrate (SuperSignal Femto; Pierce Biotechnology) and ChemiDoc imaging system (Bio-Rad). The chemiluminescence signal was quantified by using Image Lab 5.2.1 (Bio-Rad).\n\n【19】We performed deglycosylation by adding 18 μL of 0.2 mmol/L sodium phosphate buffer (pH 7.4) containing 0.8% Nonidet P40 (Roche) and 2 μL (U/ml) di N-Glycosidase F (Roche) to 5 μL of proteinase K–digested and denaturated samples. We then incubated the mixtures for 3 h at 37°C with gentle shaking.\n\n【20】### Results\n\n【21】CWD was diagnosed in 2 moose in May 2016 in Norway’s Selbu municipality and in 1 moose in October 2017 in Lierne municipality. Selbu and Lierne are respectively located ≈300 and ≈450 km northeast of Nordfjella, where CWD in reindeer was detected in 2016. Norway is populated by several species of wild cervids with varying degrees of overlapping range. Seasonal migrations are common and distances might exceed 150 km . However, studies tracking global positioning satellite–collared moose have not documented regular seasonal migrations between Selbu and Lierne municipalities, suggesting that these can be considered different moose subpopulations.\n\n【22】We initially detected PrP Sc  in brain samples by using a rapid test and then confirmed by WB (data not shown) and IHC. Sequencing analysis of the entire PrP coding sequence revealed that the 3 moose had the wild type PrP genotype, homozygous for lysine at codon 109 and for methionine at codon 209 (KK 109  MM 209  ) .\n\n【23】##### Discriminatory PrP Sc  Immunohistochemistry Show Differences between Reindeer and Moose\n\n【24】The distribution of PrP Sc  staining was examined by IHC and compared in the tissues of the 3 moose and the reindeer by using 5 different antibodies . No staining was observed in CWD-negative reindeer and moose independently of the antibody used. The distribution of PrP Sc  in the reindeer was identical for each of the 5 antibodies and did not differ from the description of PrP Sc  distribution in North America cervids . The labeling was most consistent within the gray matter of the medulla oblongata, particularly in the dorsal motor of the vagus nerve . The thalamic and brain stem regions of the brain were most affected, with a minimal amount of PrP Sc  identified dorsal to the corpus callosum.\n\n【25】PrP Sc  labeling in the moose brains  was clearly different from that of the reindeer . In the moose, after staining with F99/97.6 and L42, PrP Sc  was almost exclusively observed as intraneuronal aggregates, although intraastrocytic type (multiple small granules scattered in the cytoplasm of astrocyte-resembling cells) and intramicroglial type (single or a few large granules in close proximity to microglia-like nuclei) were also observed in the cerebral cortices and olfactory bulb . The degree of PrP Sc  staining was more intensive and appeared more widespread in the neuropil using SAF84.\n\n【26】At the level of the obex, we found stained neurons in all nuclei, whereas the dorsal motor of the vagus nerve was not remarkably stained, as observed in reindeer. The intensity of labeling varied among the 3 moose; no. 2 displayed sparse labeling, no. 3 widespread and abundant labeling, and no. 1 intermediate labeling intensity. We observed PrP Sc  in all parts of the brain investigated except the cerebella of moose nos. 1 and 2. A diffuse or discrete punctate staining was observed in the granular layer of the cerebellum of moose no. 3, with stronger staining in some Golgi neurons . In all 3 moose, the cortical regions showed laminar staining of neurons in all the cell layers, especially in fusiform-shaped neurons. The neurons of the olfactory tubercle from all 3 also stained strongly, and some glia-associated staining could be observed.\n\n【27】In contrast to the reindeer, the downstream flexible tail mAbs 12B2 and 9A2 did not stain in the moose , suggesting that the moose PrP Sc  was truncated by endogenous proteases further upstream in the N terminus than was reindeer PrP Sc  . Contrary to previous findings in reindeer, PrP Sc  was not detected in the Ln from moose no. 1 or in the Ln and tonsils from moose no. 3  by either IHC or ELISA.\n\n【28】##### PrP Sc  from Norway Moose Compared with Other CWD Isolates from Canada and Norway\n\n【29】We compared the PrP Sc  features in moose from Norway with those of other CWD isolates from Norway and Canada by discriminatory WB, which enabled comparison of PrP res  by epitope mapping with different antibodies. Norway moose PrP res  had a lower apparent molecular weight (MW) than PrP res  from Norway reindeer  or from Canada isolates . This lower MW was explained by the occurrence of more C-terminal cleavage of PrP Sc  by protease K, as confirmed by the partial loss of the 12B2 epitope .\n\n【30】Given the unusual pattern observed in moose isolates from Norway, we further investigated their biochemical characteristics with additional mAbs and by enzymatic deglycosylation . Moose samples showed a main C-terminal fragment of ≈17 kDa, detected with SAF84, L42, and 9A2, and an additional glycosylated C-terminal fragment of ≈13 kDa (CTF13) detected only with SAF84. The N terminal 12B2 epitope was mainly lost, although a small amount of PrP res  was still detectable in moose no. 1  and no. 3  with this antibody.\n\n【31】In moose nos. 2 and 3, an additional glycosylated C-terminal fragment of ≈16 kDa (CTF16) was detected by SAF84 and L42 mAbs . We cannot exclude that a small amount of CTF16 was also present in moose no. 1, given that a weak PrP res  fragment of ≈16 kDa was detectable upon deglycosylation and long exposure of blots . Moose nos. 1 and 3 also had a nonglycosylated internal fragment of ≈10 kDa, cleaved at both N and C termini of PrP Sc  , which was recognized by using mAbs 9A2 . Moreover, the analysis of PrP res  from different neuroanatomic regions showed that the slight differences observed among the 3 moose were not dependent on the area analyzed .\n\n【32】##### Comparison of the PrP Sc  Features of the Norway Moose with Sheep and Cattle Prion Strains from Europe\n\n【33】Comparison with ovine and bovine prions was performed to determine the N terminal cleavage of the main PrP res  fragment by analyzing the different PrP res  fragments in each sample, the MW of these fragments, and the L42/12B2 antibody ratio . Among ovine prions, classical scrapie and atypical/Nor98 were easily discriminated from moose isolates . Classical scrapie PrP res  had a higher MW than moose PrP res  , as confirmed by the preservation of 12B2 epitope. As previously observed , Nor98 PrP res  was cleaved at both the N and C termini, and the characteristic 11–12 kDa band was detected by L42, 9A2, and 12B2 mAbs . In contrast, CH1641 samples showed molecular features partially overlapping with the moose . CH1641 samples showed a PrP res  of ≈17 kDa and were accompanied by an additional C-terminal fragment of 13–14 kDa detected by using SAF84 mAbs . However, CTF16 and the internal PrP res  fragment of 10 kDa could not be detected in CH1641 samples.\n\n【34】Moose PrP Sc  did not overlap with any type of bovine PrP Sc  . The lack of the 12B2 epitope in moose PrP res  was similar to C-type and atypical L-type BSE, but the 2 bovine prions had neither CTF13, CTF16, nor the internal fragment . H-type atypical BSE showed the CTF13 and the internal fragment similar to moose PrP res  , but the main PrP res  fragment showed a higher MW and preserved the 12B2 epitope .\n\n【35】The ratio of reactivity obtained with L42 and 12B2 antibodies reflected the N terminal cleavage of the main fragment of PrP Sc  , enabling confirmation that the differences observed in MW of PrP res  actually depend on different N terminal proteinase K cleavages, irrespective of the host species . Values >2 are indicative of BSE-like cleavage, whereas values <1 indicate a better preservation of 12B2 epitope compared with scrapie. In this respect, the behavior of moose PrP res  was BSE-like (ratio >2). However, moose no. 1 had a ratio lower than moose nos. 2 and 3. The CH1641-like field sample was similar to moose no. 1 in this respect, whereas CH1641 was similar to moose nos. 2 and 3. Finally, the value <1 observed for PrP res  in H-type atypical BSE, CWD in reindeer, and CWD isolates from Canada reflected their higher MW compared with classical scrapie .\n\n【36】### Discussion\n\n【37】Although CWD has been detected in several captive and free-ranging cervid species from a large geographic area in North America, <10 cases in moose have been reported . We report 3 naturally occurring cases of prion disease in moose in Norway that showed molecular and IHC phenotypes differing from those previously described for classical CWD in North America, as well as in reindeer in Norway. The phenomenon of strain variation is well known in prion diseases and is often associated with phenotype variation in natural hosts, as observed in bovines with classical, H-type, or L-type BSE, and in sheep with classical or atypical/Nor98 scrapie. Identification of a new CWD phenotype in 3 moose in Norway can be suggestive of a new CWD strain. Although the existence of CWD strain variation in North America has been inferred from transmission studies , this phenomenon has not been directly associated with phenotypic variations in natural hosts so far.\n\n【38】The phenotype variant found in moose from Norway could be hypothetically attributed to host species factors. To address this issue, we directly compared PrP Sc  characteristics in the Norway moose with those in a Canada moose with CWD. In agreement with the available evidence, we found that the Canada moose PrP Sc  had features different from Norway moose PrP Sc  and were indistinguishable from other cervids with classical CWD. This finding suggests that the variant PrP Sc  type observed in Norway moose could not simply reflect a host species factor. Notably, in both natural and experimental conditions, CWD-affected moose in North America have been reported to display disease features indistinguishable from CWD in other cervids and had detectable PrP Sc  in lymphoid tissues .\n\n【39】Species-specific amino acid polymorphisms in the cervid PrP are associated with CWD susceptibility, incubation time, and pathology . In transmission experiments, atypical features were reported in elk or wapiti and mule deer with genotypes associated with a relative resistance to disease, extension of the incubation period, or both . Moose PrP is polymorphic at codon 109 (K/Q) and 209 (M/I), combined in 3 alleles: K 109  M 209  (observed in Europe and North America), Q 109  M 209  (observed in Europe), and K 109  I 209  (observed in North America) . The 3 moose with CWD from Norway had the KK 109  MM 209  genotype, whereas the moose case from Canada used for comparison had the KK 109  II 209  genotype. Thus, we cannot exclude that the differences observed between Norway and Canada moose in our study are dependent on differences in PrP genotype. However, a classical CWD phenotype has been reported in naturally  and experimentally infected  moose with the KK 109  MM 209  genotype, suggesting that a difference at PrP codon 209 is probably not the cause of the variant phenotype observed in moose in Norway. All of these findings suggest that neither the species nor the individual PrP genotypes are likely to have caused the variant phenotypes observed and imply that this variant phenotype could represent a novel CWD strain.\n\n【40】CWD is known to be a highly contagious disease in North America; however, data relating to the disease in moose are sparse and insufficient to understand the epidemiology and the implications of CWD in this species. The apparent low CWD prevalence reported for moose in North America compared with other cervid species might be attributable to the individual social behavior of moose and the minimal habitat overlap between moose and other cervids in areas with CWD. Additionally, surveillance program design, disease variability, and host genetics might influence the prevalence of the disease. Based on the epizootic dynamics in North America, CWD plausibly could have become established in reindeer in Norway more than a decade ago . In this scenario, the disease in moose could possibly be linked to the disease observed in reindeer, with strain mutation or phenotype shift putatively caused by interspecific transmission. However, a main cause of strain mutation after interspecies transmission (i.e. PrP amino acid differences between the donor and host species) is not relevant in this case because reindeer and moose share the same PrP primary sequence. An alternative hypothesis could be that moose have a prion disease that is independent of the reindeer epidemic, being either specific to the Norwegian moose or acquired by species other than the reindeer.\n\n【41】The 3 moose were 13, 14, and 13 years of age. Although moose can reach ages beyond 20 years, we consider these moose as old because female moose >10–12 years of age start to show signs of senescence and declining survival and reproduction rates . The old age of the moose, the absence of lymphoid tissue involvement, and the low disease prevalence observed so far (of 10,531 moose tested) could suggest that CWD in moose is less contagious than classical CWD or could represent a spontaneous TSE. The finding that the affected moose were from the same geographic area does not seem to support a spontaneous origin of the disease; however, the actual evidence for geographic clustering could have been biased by oversampling in Trøndelag County, where the first positive moose was detected. Lack of detailed data on the ages of the moose tested so far in different geographic areas prevents any definitive conclusion. Still, the recent detection of a positive moose in Finland, several hundred kilometers from Trøndelag County, might indicate that the disease is not restricted to Norway . The ongoing intensive surveillance in Norway and several European Union countries with large moose populations will help to better clarify the actual geographic distribution and prevalence and will be critical for understanding the contagious or spontaneous nature of the disease.\n\n【42】The 3 moose analyzed shared a distinctive IHC pattern, mainly characterized by intraneuronal accumulation of PrP Sc  , and common PrP Sc  features, such as the proteinase K N-terminal cleavage and the presence of an additional CTF13 fragment. However, we also observed unexpected differences among the 3 moose. By WB, the CTF16 fragment was observed in moose nos. 2 and 3 but not in moose no. 1, whereas the nonglycosylated internal fragment of 10 kDa was evident in moose nos. 1 and 3 but could not be detected in moose no. 2. Furthermore, we also showed that these differences did not depend on the brain area investigated. We cannot rule out that these slight differences might depend on technical issues rather than represent actual PrP Sc  variations. The outcome of the ongoing bioassay experiments will help to clarify the meaning of the observed variations.\n\n【43】By comparing the moose PrP Sc  features with other animal TSEs circulating in Europe, we found no evidence of similarities with bovine and ovine prions. Minimal similarities were observed with CH1641 samples; however, CH1641 cases have not yet been detected in Norway. Bioassay in a large spectrum of rodent models will assist in determining whether these molecular similarities imply biologic association between the atypical CWD in moose and small ruminant CH1641. Transmission studies in several rodent models are under way and will help to clarify whether the different phenotype observed (designated Nor16CWD) could reflect the presence of a new cervid prion strain in moose from Norway.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "59054b6a-cbce-44a1-b97b-7c9765b1798e", "title": "Antibody Responses after Classroom Exposure to Teacher with Coronavirus Disease, March 2020", "text": "【0】Antibody Responses after Classroom Exposure to Teacher with Coronavirus Disease, March 2020\nIn late February 2020, a teacher experienced headache, sore throat, myalgia, and fatigue while traveling in Europe, where community transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was ongoing . After arriving back in the United States, the teacher returned to school February 24–27 while experiencing the same symptoms plus limited cough. An oropharyngeal swab sample collected on March 1 was positive for SARS-CoV-2 by reverse transcription PCR (cycle threshold values N1 = 35.05, N2 = 35.2; RNase P = 23.58). All students who attended classes with the infected teacher were instructed to quarantine themselves at home through March 12. After the quarantine period, we conducted a serologic survey to assess potential SARS-CoV-2 transmission in a classroom setting.\n\n【1】During February 24–27, the teacher taught 16 classes, all in the same room, each with < 30 students. Of the 16 classes, 10 were discussion-based, in which the teacher reported walking around the room and speaking directly with students (interactive classes). For the other 6 classes, the teacher sat mostly in 1 location and close interactions with students were limited (noninteractive classes). On March 10, we contacted 120 students ([40%\\] enrolled in interactive classes, 72 \\[60%\\] enrolled in noninteractive classes) whose only known exposure was through classroom contact with the teacher and invited them to participate in our serologic survey; 21 (%) students volunteered.\n\n【2】Median participant age was 17 years (range 5–18 years). Five (%) participants had interactive classroom contact; mean in-class time was 108 minutes. Sixteen (%) participants had noninteractive classroom contact only; mean in-class time was 50 minutes.\n\n【3】Participating students completed a questionnaire about symptoms experienced during the quarantine period and provided a blood specimen. On March 13, whole blood (mL) was collected and serum was separated before samples were frozen at −80°C for shipping. The Centers for Disease Control and Prevention tested the samples for antibodies by ELISA, as described previously . We considered reciprocal titers of \\> 400 to be positive and reciprocal titers of \\> 100 but <400 to be indeterminate.\n\n【4】Of the 5 students with interactive classroom contact, results for 2 (students A and B) were suggestive of previous SARS-CoV-2 infection; results for student A were positive and for student B indeterminate . Students A and B were not in the classroom during the same period and sat in different locations in the classroom. Student A had a reciprocal titer of 400 and spent 135 minutes in interactive classes. Beginning February 26, this student experienced intermittent myalgia, rhinorrhea, and cough for 9 days. Student B had a reciprocal titer of 100, spent 90 minutes in the interactive classroom, and reported no symptoms. The remaining 3 students (students C–E) had reciprocal titers of <100. Student C spent 135 minutes in interactive classes and reported no symptoms. Students D and E each spent 90 minutes in interactive classes and reported limited symptoms. Student D reported subjective fever and headache lasting 1 day, and student E reported rhinorrhea lasting 1 day. Although no serologic evidence of previous infection was found for participants with noninteractive classroom contact only, 7 (%) reported symptoms. The most common symptoms among participants with noninteractive classroom contact were sore throat (n = 3), headache (n = 3), rhinorrhea (n = 2), and myalgia (n = 2).\n\n【5】Although SARS-CoV-2 transmission from symptomatic persons to close contacts has been well established, risks associated with classroom contact are not well known. The positive results for student A suggest past infection with SARS-CoV-2. The meaning of the indeterminate result for student B is less clear in this context, but this result may be suggestive of past infection. We do not know whether results from student A or B are indicative of immunity to SARS-CoV-2 infection. The symptoms reported by both students are consistent with those reported by children and adolescents with mildly symptomatic and asymptomatic coronavirus disease .\n\n【6】This survey is subject to limitations. First, we based the definition of interactive classroom contact on reported usual behavior by the teacher; however, variability of contact for each participant was not defined. Second, reported symptoms might have been affected by students’ expectation of the survey’s intent, leading to social desirability bias. Third, because of low participation, the results may not be generalizable to all students who had contact with the teacher. Fourth, among students who chose to participate, participation might have been influenced by their perceived risk or symptoms experienced during the quarantine period, leading to selection bias. Fifth, potential infections may not have been detected because blood collection ≈14 days after exposure may have been too soon for development of SARS-CoV-2 antibodies. Last, the only known exposure for participating students was the infected teacher; however, students could have been exposed by unrecognized community transmission. The risk for transmission from mildly symptomatic or asymptomatic persons is not well known.\n\n【7】Widespread school closures have mostly eliminated the risk for classroom transmission of SARS-CoV-2. However, these results suggest that classroom interaction between an infected teacher and students might result in virus transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9086fb79-168c-4308-abb6-8d21e34c42d0", "title": "Therapeutic Drug Monitoring for Slow Response to Tuberculosis Treatment in a State Control Program, Virginia, USA", "text": "【0】Therapeutic Drug Monitoring for Slow Response to Tuberculosis Treatment in a State Control Program, Virginia, USA\nWorldwide, tuberculosis (TB) remains the leading cause of death from a curable infectious disease; ≈1.4 million deaths occurred in 2008 alone . Death is a consequence of delayed diagnosis and ineffective or incomplete treatment because cure rates exceed 95% with appropriate therapy . Slow response to therapy can lead to prolonged infectiousness, extended treatment duration, acquired drug resistance, or recurrence of TB after treatment. The reasons for slow response are diverse, but measurement of serum anti-TB drug levels, or therapeutic drug monitoring (TDM), is a potentially useful tool for uncovering the causes of slow response . Low serum levels can be a consequence of malabsorption, inaccurate dosing, altered metabolism, or drug–drug interactions , but in most instances low serum levels can be readily corrected with dose adjustment.\n\n【1】TDM is currently recommended in TB treatment guidelines as optional , and few large TB control programs have access to routine TDM. Although published reports describe patients for whom slow response was attributable to low drug levels, questions remain about how best to implement TDM on a programmatic scale . Definitions of slow response vary, and recommendations for which medications to prioritize for TDM are lacking. Furthermore, for general populations receiving TB therapy, TDM is unlikely to be of benefit, given the infrequency of treatment failure or TB recurrence . Although it is well known that certain patients, such as those infected with HIV and thus prone to malabsorption, are at higher risk for low drug levels , studies of TDM that included patients responding well to anti-TB medications found lower than expected drug levels of isoniazid and rifampin in many patients with adequate clinical response . Therefore, identification of patients at risk for slow response is critical within a TB control program. In addition, TDM performed earlier in the time course of slow response may also affect other major programmatic outcomes, such as treatment duration.\n\n【2】In the state of Virginia it is mandatory for providers to report all cases of TB to the Virginia Department of Health. Each case is assigned to a nurse case manager, who oversees and monitors the progress of each patient until treatment is completed. Directly observed therapy is administered by the nurse case manager or a trained outreach worker. After 4 weeks of therapy, patients are screened by the nurse case manager. Medical consultation for patients with ongoing symptoms is provided by the state TB clinicians in an effort to identify slow response earlier and to prevent acquired drug resistance. Clinicians define slow response in a patient as after \\> 30 days from the start of treatment the patient has \\> 2 of the following findings: sputum smear positive for acid-fast bacilli; no improvement in TB-specific symptoms, including fever, cough, weight loss, and/or night sweats; and no improvement in chest radiograph lesions previously identified as consistent with TB. Routine TDM among patients who met criteria for slow response was instituted by March 2007.\n\n【3】We performed a retrospective cohort study among patients slow to respond to pulmonary TB treatment in the state of Virginia to determine the prevalence of lower than expected levels of isoniazid, rifampin, ethambutol, and pyrazinamide measured at the time of estimated peak serum concentration (C max  ). Secondary aims included investigation of risk factors for levels below the expected range, evaluation of the mean change and likelihood of achieving a level within the expected range after dose adjustment, and comparison of outcomes between persons with slow responses with those with low and expected levels. The study was approved by the institutional review boards for human subjects research at the University of Virginia and the Virginia Department of Health.\n\n【4】### Methods\n\n【5】##### Patients\n\n【6】Patients were identified for inclusion in the study by using routine TB surveillance data recorded in the Virginia TB Registry. All patients who were \\> 18 years of age, had confirmed _Mycobacterium tuberculosis_ cultures, and started TB therapy in the state of Virginia during March 1, 2007–May 1, 2009, were eligible. We included patients who had been treated for pulmonary TB or pulmonary TB and extrapulmonary TB and who began a regimen of isoniazid, rifampin, ethambutol, and pyrazinamide. All _M. tuberculosis_ specimens were sent to the state TB laboratory, where drug-susceptibility testing was performed after secondary culture of the isolate by using the automated Bactec MGIT 960 system (Becton Dickinson, Sparks, MD, USA). Patients were excluded if their original isolate was later found to be resistant to \\> 1 first-line medication. Patients were also excluded if they had TDM performed for reasons other than slow response.\n\n【7】Surveillance data were retrieved from the state TB registry and included demographics (age, sex, race/ethnicity, country of origin, and homelessness), TB history (prior episodes of TB, sputum smear and culture status of current TB episode, and chest radiograph abnormalities), coexisting conditions (diabetes, HIV infection, intravenous drug use, and excessive alcohol use), and treatment outcomes (completion of TB treatment, duration of completed TB treatment, relapse of TB following treatment completion, acquisition of drug resistance in a previously susceptible TB strain, and death from any cause during TB treatment). Information about medication-related adverse events following anti-TB drug dose increase was obtained from personal communication with the state TB medical consultants.\n\n【8】##### Therapeutic Drug Monitoring\n\n【9】The standard procedure for TDM was for patients to be given their daily dose of TB medications in the morning while fasting and then observed for 2 hours, during which they were restricted from eating or drinking. At 2 hours after medication administration, venous blood was collected and serum was separated before transport on dry ice to the regional referral laboratory. The drug levels from blood collected 2 hours after medication administration (C 2hr  ) were used as the estimated peak maximum serum concentration (C max  ) as per standard practice and were determined by using high-performance liquid chromatography (for isoniazid and rifampin) or gas chromatography with mass spectrometry (for ethambutol and pyrazinamide). Expected C 2hr  ranges were provided and were consistent with published norms . C 2hr  levels were also recorded for patients with initial low levels in whom follow-up TDM was performed after dose adjustment.\n\n【10】##### Data Analysis\n\n【11】Demographic and clinical characteristics were compared with the χ 2  statistic or, for nonparametric data, the Mann-Whitney U test. For the determination of risk factors for C 2hr  levels below the expected range, values were dichotomized into normal if the value was within or above the expected range, or low if the value fell below the expected range. Bivariate and multivariate logistic regression analyses were used to determine risk factors for either a low isoniazid or a low rifampin level. The multivariate model included any variable with p<0.1 in bivariate analysis and relevant demographic characteristics. Paired Student _t_ tests were used to report the mean change in C 2hr  levels following dose adjustment. Medications dosed \\> 5× per week were considered daily dosed. Biweekly dosing was used for some patients for isoniazid and rifampin, with the isoniazid biweekly dose at 3× the usual daily dose. The rifampin dose was unchanged regardless of dosing frequency. The log-rank test was used to compare treatment duration and for patients who had not completed therapy at the time of analysis; data were right censored for survival analysis. All tests of significance were 2 sided. Data were analyzed with SPSS version 17.0 software (SPSS Inc. Chicago, IL, USA).\n\n【12】### Results\n\n【13】During the study, 350 patients were treated with an initial regimen of isoniazid, rifampin, ethambutol, and pyrazinamide for pulmonary TB; of these patients, 45 (%) met criteria for slow response. Thirty-seven patients were excluded from the study (with normal response and 3 with slow response) after drug-susceptibility testing showed resistance to \\> 1 medication of the treatment regimen. An additional 2 patients were excluded because TDM was performed for reasons other than slow response. Thus, 311 patients were included in the study, of whom 42 (%) met criteria for slow response . At the time of TDM among patients meeting criteria for slow response, all had persistent TB-related symptoms. Of the 23 patients with initial smear-positive sputum specimens, 17 (%) had specimens that remained smear positive.\n\n【14】The mean (SD) age for patients in the study was 46 years (years), and 204 (%) were men . The most common ethnicity was Asian ([33%\\]) and 228 (%) were foreign born. Among all patients, 291 (%) had no history of TB. Most patients tested had a positive tuberculin skin test (TST) result, although 85 (%) did not have a TST reading recorded. There were 287 patients with a sputum smear recorded at the time of diagnosis; of these smears, 193 (%) were positive for acid-fast bacilli. Ninety-five percent  of patients had a chest radiograph with findings suggestive of TB, of which 122 (%) were cavitary. There was no significant difference in the proportion of patients with a positive TST result, a positive sputum smear, or a chest radiograph showing cavitation among the 42 with a slow response and the remaining patients with adequate response. Among patients meeting criteria for slow response, none were HIV infected and none reported using illicit drugs (either intravenous or nonintravenous). The only significant predictor of slow response was diabetes (unadjusted odds ratio \\[OR\\] 6.5, 95% confidence interval \\[CI\\] 3.2–13.5, p<0.001; adjusted OR \\[aOR\\] 6.3, 95% CI 2.8–14.0, p<0.001).\n\n【15】##### Initial C 2hr  Levels\n\n【16】All 42 patients who were slow to respond were monitored for rifampin, and 22 (%) had a C 2hr  level below the expected range; 1 (%) had a high level . For daily or biweekly dosed rifampin, the median C 2hr  level was 7.4 µg/mL (interquartile range \\[IQR\\] 2.5–11.4 µg/mL, expected range 8–24 µg/mL) . Thirty-nine patients were monitored for isoniazid; 23 (%) had levels below the expected range. For daily dosed isoniazid, the median C 2hr  was 1.90 µg/mL (IQR 1.1–3.5 µg/mL, expected range 3–6 µg/mL), and for biweekly dosing, 9.8 µg/mL (IQR 2.8–11.2 µg/mL, expected range 9–18 µg/mL). Among the 39 patients who were tested for isoniazid and rifampin levels, 13 (%) had levels below the expected range for both medications. Twenty-six patients were monitored for ethambutol; 8 (%) had levels below the expected range. The median C 2hr  level for ethambutol was 2.5 µg/mL (IQR 1.7–3.2 µg/mL, expected range 2–6 µg/mL). Twenty patients were monitored for pyrazinamide, all had levels within the expected range; median C 2hr  level was 28.1 µg/mL (IQR 26.5–33.2 µg/mL, expected range 20–50 µg/mL).\n\n【17】##### Risk Factors for Low Isoniazid or Rifampin Levels\n\n【18】Analyses of risk factors for low levels of isoniazid or low levels of rifampin were performed, but small sample size precluded meaningful analysis of risk factors for low ethambutol levels. Patients with diabetes were at significantly increased risk of having a low rifampin level (OR 5.8, 95% CI 1.4–23.1, p = 0.01; aOR 5.7, 95% CI 1.2–25.7, p = 0.03) . Patients who received isoniazid biweekly were less likely to have low isoniazid levels than those who received isoniazid daily, but this association was not statistically significant in multivariate analysis (OR 0.21, 95% CI 0.05–0.91, p = 0.04; aOR 0.47, 95% CI 0.09–2.5, p = 0.37) .\n\n【19】##### Follow-up C 2hr  Levels after Dose Adjustment\n\n【20】Eighteen patients with rifampin levels below the expected range had follow-up TDM after dose adjustment. Levels for all patients increased from the initial to the follow-up level with a mean (SD) change of 11.0 µg/mL (.7 µg/mL; p<0.001); 16 (%) had levels in the expected range after the first dose adjustment . Fourteen patients had follow-up TDM for daily-dosed isoniazid levels below the expected range; monitoring detected increased levels in 12 patients, with a mean (SD) change of 3.4 µg/mL (.9 µg/mL; p = 0.001); 4 (%) patients had levels in the expected range. Four patients had follow-up TDM for biweekly-dosed isoniazid levels below the expected range, and all had increased levels with a mean (SD) change of 11.8 µg/mL (.1 µg/mL; p = 0.03); 3 (%) patients had levels in the expected range.\n\n【21】Rifampin levels below the expected range were significantly more likely to be corrected to within the expected range following the first dose adjustment than were daily-dosed isoniazid levels below the expected range (p = 0.01). There was no significant difference in the likelihood of correction to the expected range between daily and biweekly dosed isoniazid. No follow-up levels of ethambutol or pyrazinamide were reported. There were no reported medication-related adverse events following dose increase.\n\n【22】##### Treatment Outcomes\n\n【23】Complete outcomes were available for 32 (%) patients; 10 patients continued receiving treatment. Twenty-seven patients successfully completed treatment, 3 patients died, and 2 patients moved out of the state where follow-up was incomplete. Median time to completion of therapy among all 27 patients was 45 weeks (IQR 40–51 weeks). Among the 14 patients with initial rifampin levels below the expected range who completed treatment, the median duration was 40 weeks (IQR 38–48 weeks) compared with a median duration of 47 weeks (IQR 44–55 weeks) for the 13 patients with initial rifampin levels within the expected range (log-rank p = 0.17).\n\n【24】There were no reports of relapse of infection over a median of 14.5 months (IQR 7–25 months) from the conclusion of treatment. No patient had documented acquisition of medication resistance in follow-up TB cultures while on treatment. All 3 deaths occurred shortly after TDM was performed: 1 patient had isoniazid, rifampin, ethambutol, and pyrazinamide levels within expected ranges, 1 patient had isoniazid, rifampin, and ethambutol levels within expected ranges, and 1 patient had isoniazid, rifampin, and ethambutol levels below the expected ranges.\n\n【25】### Discussion\n\n【26】The major finding of this study is that among patients being treated for pulmonary TB in Virginia, most patients that met criteria for slow response to therapy were found to have C 2hr  levels of rifampin and isoniazid below the expected range; many patients also had low levels of ethambutol. Given the high frequency of patients who were slow to respond to both key first-line medications, isoniazid and rifampin, and the well-tolerated subsequent increase in levels documented after dose adjustment, TDM appears to be a useful strategy for identifying a remediable cause of slow response at a programmatic level. Furthermore, the median duration of therapy for patients with rifampin levels below the expected range was nearly 2 months shorter than that for patients with normal rifampin levels. Although it is not specifically known if identification and correction of lower than expected levels brought about a rapid improvement in TB clinical signs and symptoms, the comparatively shorter course represents a substantial cost savings when considering personnel involved in monitoring and medication administration, as well as diagnostic tests averted in the work-up of otherwise unexplained slow response.\n\n【27】We found that ≈90% of patients with lower than expected rifampin levels who were subsequently tested after the first dose adjustment achieved target levels. Dose-titration studies of rifampin confirm a continuously increasing response of early bactericidal activity by measurement of sputum colony counts with corresponding increase in rifampin dose . Rifampin has been tolerated at doses as high as 1,200 mg in small studies, and larger trials are ongoing to study high-dose rifampin in an effort to shorten therapeutic duration . Given increasing evidence that rifampin may be underdosed for many patients regardless of TB outcome , the findings of this study suggest that rifampin is a prime medication to prioritize for early TDM for patients for whom TB therapy is failing.\n\n【28】Diabetes was significantly associated with slow response in our study population, and, among persons with a slow response with diabetes, C 2hr  levels of rifampin were significantly more likely to be below the expected range. Patients with diabetes are at greater risk for incident TB  and are more likely to have poor TB treatment outcomes , which may partially be explained by inadequate pharmacotherapy. A growing body of evidence has demonstrated reduced rifampin exposure in patients with TB and diabetes, which may be in part related to impaired absorption . Although rifampin absorption may not be blunted by delayed gastric emptying , hyperglycemia can decrease gastric hydrochloric acid secretion, which results in a higher gastric pH and reduced rifampin absorption . Markers of glycemic control were not available for analysis in this study, but it may be of further use to risk stratify patients with diabetes based on disease severity. It is suspected, though not tested, that drug–drug interactions were also playing a role in the observed lower levels of rifampin in patients with diabetes from our study population. Given preexisting knowledge of the association between diabetes and poor treatment outcome, there may have been a bias on behalf of the TB control staff to characterize a patient with diabetes as a person with a slow response based on symptom persistence. Nevertheless, our findings raise the possibility of TB programs studying the benefit of routine TDM for rifampin among all patients with diabetes at the start of TB therapy.\n\n【29】Routine TDM among persons with slow responses at a relatively early point in the treatment course reflects policy change within Virginia’s TB control program. Given the high prevalence of low levels of key medications and the observed ease of correction after dose adjustment, we recommend that similar TB programs investigate the applicability of TDM within their own settings. Further generalization must be cautiously considered, however, because relevant factors that may adversely affect pharmacokinetics, such as the patient’s weight at the time of TDM, concurrent medication use, chronic kidney disease, or cirrhosis, were not available in these surveillance data for comparison . Other limitations to the study must be taken into account. Blood was collected at 2 hours after medication administration to estimate C max  . A second blood collection at 6 hours can additionally distinguish patients whose absorption may be delayed secondary to poor gastric emptying ; however, the frequency of delayed absorption has been rare in other cohorts for which 2 and 6 hour measurements were performed .\n\n【30】Additionally, given that the prevalence of lower than expected drug levels was not known for patients with adequate response to anti-TB therapy, the overall contribution of pharmacotherapy to the cause of slow response in this cohort cannot be fully assessed. Surveillance data did not permit comparison of culture positivity in patients who met criteria for slow response at the time of TDM matched to patients with adequate response for whom TDM was not performed. Use of TDM as early as 4 weeks, as was performed in this study, may have selected for patients that might otherwise have improved after 8 weeks of therapy regardless of other interventions. Lastly, further study may find, given that the cost of TDM (≈$80 US per individual drug) may be substantial for some TB control programs, that it is more economical to start therapy with increased drug dosages for patients at higher risk for slow response.\n\n【31】In summary, routine TDM among patients meeting criteria for slow response to TB therapy in Virginia identified most of those tested to have C 2hr  levels of rifampin and isoniazid below the expected range; many patients also had low levels of ethambutol. Most patients with repeat TDM following dose adjustment to rifampin and isoniazid had levels within the expected range, suggesting a clinically actionable result. Given the comparative ease of correcting low rifampin levels and the shorter duration of therapy in those with a correctable rifampin level, this medication is particularly appealing to target for programmatic intervention. Further prospective studies should evaluate the benefit of routine TDM for rifampin early in the treatment course among patients with diabetes or of higher initial doses of rifampin among all groups at risk for slow response.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "56725e60-9822-408a-8ae0-6295bc49fbaa", "title": "Rates of Influenza-like Illness and Winter School Breaks, Chile, 2004–2010", "text": "【0】Rates of Influenza-like Illness and Winter School Breaks, Chile, 2004–2010\nInfluenza pandemic preparedness plans to mitigate effects of a severe pandemic recommend layered medical and social distancing interventions, including school closings, cancellation of large public gatherings, and face mask use . Because schoolchildren are considered to be high transmitters of influenza virus (higher contact rates, enhanced susceptibility to infection, and increased virus shedding relative to that among persons in other age groups), prompt school closure is expected to reduce transmission during a pandemic .\n\n【1】Although several empirical studies have linked school activities with influenza virus transmission , few studies have considered data from multiple epidemic periods, and little information is available from the Southern Hemisphere. School breaks and school teacher strikes provide natural experiments in which the effect of school terms on influenza transmission dynamics can be explored. On the basis of 21 years of surveillance data, Cauchemez et al. found a 16%–18% reduction in incidence of influenza-like illness (ILI) associated with the 2-week school winter break periods in France. A study of variation in contact rate patterns in Europe suggested a 13%–40% reduction in the basic reproduction number associated with school breaks in Belgium, Great Britain, and the Netherlands . A 12-day teacher strike in Israel in the winter of 1999 was also associated with a reduction (%) in weekly rates of respiratory disease . A single study is available from the Southern Hemisphere and indicates a 14% reduction in ILI incidence during winter break in Argentina during 2005–2008; the largest decrease was observed among children 5–14 years of age . In our study, we quantified the effect of school break cycles on the age distribution of ILI patients in Chile during 2004–2010.\n\n【2】### The Study\n\n【3】We obtained weekly age-specific ILI incidence rates during 2004–2010 from a systematic national surveillance system in Chile . ILI surveillance relies on 42 sentinel outpatient sites located throughout the country; these sites are representative of the general population and systematically report weekly age-specific physician visits for ILI  . We characterized the effect of the 2-week winter break period on influenza transmission during 2004–2010 by comparing trends in weekly ILI incidence rates among schoolchildren (and 15–19 years of age) and adults (and \\> 65 years of age). To estimate changes in the age distribution of ILI patients, on the basis of methods used in previous work , we compared the weekly ratios of ILI incidence rates for schoolchildren and adults during the 2-week period before, during, and after the winter break by using a 1-sided Z test. We also considered a 6-week window  before and after the winter break as a sensitivity analysis. A decline in the schoolchildren-to-adult incidence rate ratio indicates a shift in the age distribution of patients toward adults, suggestive of decreased influenza transmission among schoolchildren .\n\n【4】In Chile, wintertime influenza activity peaks during May–September, which is typical of temperate regions in the Southern Hemisphere . The 2-week winter school break typically coincides with the influenza season and is synchronous throughout the country; ≈95% of educational institutions follow the break periods set by the Ministry of Education.\n\n【5】The reduction in the schoolchildren-to-adults incidence rate ratios was maintained for an average of 2 weeks after the end of the winter break. The decline in ratios was primarily caused by a decrease in ILI rates among schoolchildren; the average (+ standard error of the estimate) reduction in ILI incidence among schoolchildren (years of age) in the 2 weeks during the winter break compared with the 2 weeks before was 67.2% \\+ 2.1% (p<0.001). This reduction occurred systematically in each winter of the study period. In contrast, the average reduction in adult ILI incidence (> 20 years of age) was more modest but remained significant at 37.4% \\+ 0.9% (p<0.001).\n\n【6】Furthermore, the incidence rate ratios for school-age children to middle-age adults significantly increased after the winter break, signaling a return toward a higher proportion of ILI cases among children, although the ratio did not return to prebreak levels . In contrast, the ratio comparing rates for children with rates for adults did not change. Our results did not change when we used a 6-week period before and after the winter break period instead of a 2-week period  or when we excluded the 2009 pandemic year from our analysis .\n\n【7】### Conclusions\n\n【8】We have shown that a two-thirds decline in ILI incidence among schoolchildren coincided with the onset of the school winter break in Chile; this pattern was consistent across the 7 years of the study. In line with a prior study in Argentina , the average reduction in schoolchildren-to-adults incidence rate ratio was sustained for up to 2 weeks after school sessions resumed. This time scale is consistent with the natural history of influenza virus infection, which has a serial interval (interval between cases) of 2–3 days, so that it takes a few successive chains of transmission to reach full-scale transmission.\n\n【9】Similar to findings from prior studies , our findings are based on analysis of ILI incidence, which is a broad indicator of respiratory disease activity in a community and is not entirely specific for influenza. Our results could be affected by changes in health-seeking behavior during the winter break. However, our ILI data are well correlated with influenza virus activity data  , and large increases in incidence among schoolchildren during winter 2009 coincide with the influenza A(H1N1)pdm09 virus pandemic period, suggesting that fluctuations in ILI incidence in Chile are primarily attributable to influenza. Our data also support the conclusion that school closure during pandemic situations is effective. Although the winter break took place near the peak of the 2009 influenza A(H1N1) pandemic in Chile, it was correlated with changes in the age distribution of patients hospitalized for influenza A(H1N1)pdm09 virus infection .\n\n【10】Overall, our study findings add to the body of information provided by empirical studies, supporting the implementation of school closure to achieve temporary reductions in ILI incidence rates, especially among school-age children, including in the Southern Hemisphere temperate setting . Our finding that ILI incidence was more modestly reduced among adults during winter breaks is consistent with past work on the age-specific transmission dynamics of influenza . School closure may be particularly useful in pandemic situations to gain time until pharmaceutical measures (vaccines, antiviral medications) become available and to mitigate the burden on health care institutions by reducing the surge of influenza patients. There is still, however, little information available from tropical and Southern Hemisphere settings, which are characterized by complex influenza seasonality patterns and/or low connectivity with the rest of the world and particular demographic and health conditions. Systematic multicountry and multiyear comparison of the effects of school closures could shed light on the effectiveness of school-based intervention policies under different epidemiologic, behavioral, and demographic situations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cbbc3891-b48e-4d8e-b17a-af50a66947ac", "title": "Neighborhood Resources to Support Healthy Diets and Physical Activity Among US Military Veterans", "text": "【0】Neighborhood Resources to Support Healthy Diets and Physical Activity Among US Military Veterans\nPEER REVIEWED\n\n【1】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\nTables\n------\n\n【2】#####  Table 1. Sociodemographic Characteristics of Census-Tract Populations and Geographic Characteristics of Census Tracts, by Concentration of Veterans, Continental US Census Tracts, 2009–2013 a  \n\n| Variable | All Tracts (N = 71,899) | Concentration of Veterans by Quartile a |\n| --- | --- | --- |\n| Low (N = 17,972) | Mid-Low (N = 17,980) | Mid-High (N = 17,974) | High (N = 17,973) |\n| --- | --- | --- | --- |\n| **Population, mean (SD), n** | 3,268  | 3,280  | 3,340  | 3,295  | 3,156  |\n| **Concentration of veterans, a mean (SD), % \\[IQR\\]** | 9.1 (.7) \\[6.0–11.5\\] | 3.8 (.6) \\[2.6–5.1\\] | 7.5 (.8) \\[6.8–8.2\\] | 10.1 (.8) \\[9.5–10.8\\] | 14.9 (.5) \\[12.4–15.8\\] |\n| **Adults aged >65 y, mean (SD), %** | 18.1 (.6) | 12.8 (.4) | 16.8 (.0) | 19.4 (.2) | 23.3 (.9) |\n| **Race/ethnicity, mean (SD), %** | **Race/ethnicity, mean (SD), %** | **Race/ethnicity, mean (SD), %** | **Race/ethnicity, mean (SD), %** | **Race/ethnicity, mean (SD), %** | **Race/ethnicity, mean (SD), %** |\n| Non-Hispanic white | 63.7 (.2) | 39.9 (.5) | 63.2 (.1) | 74.1 (.4) | 77.6 (.3) |\n| Non-Hispanic black | 13.5 (.1) | 17.9 (.3) | 15.4 (.9) | 11.4 (.3) | 9.3 (.0) |\n| Non-Hispanic Asian | 4.3 (.3) | 8.7 (.9) | 4.3 (.0) | 2.3 (.2) | 1.9 (.4) |\n| Non-Hispanic other | 3.0 (.1) | 3.0 (.1) | 3.0 (.3) | 2.9 (.8) | 3.1 (.1) |\n| Hispanic | 15.5 (.2) | 30.5 (.2) | 14.1 (.8) | 9.3 (.2) | 8.1 (.2) |\n| **Education, mean (SD), %** | **Education, mean (SD), %** | **Education, mean (SD), %** | **Education, mean (SD), %** | **Education, mean (SD), %** | **Education, mean (SD), %** |\n| <High school diploma or GED | 14.7 (.6) | 21.2 (.1) | 14.1 (.4) | 12.6 (.0) | 11.0 (.7) |\n| High school diploma, no bachelor’s degree | 57.5 (.0) | 48.6 (.5) | 56.2 (.6) | 61.5 (.7) | 63.8 (.8) |\n| Bachelor’s degree or more | 27.7 (.6) | 30.2 (.0) | 29.7 (.5) | 25.9 (.9) | 25.2 (.1) |\n| **Median household income, mean (SD), in thousands, $** | 56.4 (.3) | 54.1 (.8) | 60.2 (.6) | 56.4 (.8) | 54.9 (.0) |\n| **Population per km 2 , mean (SD), in thousands, n** | 5.1 (.5) | 13.1 (.8) | 3.7 (.9) | 2.2 (.3) | 1.6 (.1) |\n| **Distance to military installation, b mean (SD), km** | 69.3 (.0) | 57.3 (.2) | 71.6 (.6) | 79.2 (.7) | 69.1 (.7) |\n| **Urbanicity, c n (%)** | **Urbanicity, c n (%)** | **Urbanicity, c n (%)** | **Urbanicity, c n (%)** | **Urbanicity, c n (%)** | **Urbanicity, c n (%)** |\n| Large central metropolitan | 22,499 (.3) | 10,525 (.6) | 5,648 (.4) | 3,650 (.3) | 2,676 (.9) |\n| Large fringe metropolitan | 16,470 (.9) | 3,476 (.3) | 4,933 (.4) | 4,373 (.3) | 3,688 (.5) |\n| Medium metropolitan | 14,503 (.2) | 2,337 (.0) | 3,556 (.8) | 3,941 (.9) | 4,669 (.0) |\n| Small metropolitan | 6,527 (.1) | 733 (.1) | 1,371 (.6) | 1,990 (.1) | 2,433 (.5) |\n| Micropolitan | 6,511 (.1) | 565 (.1) | 1,414 (.9) | 2,206 (.3) | 2,326 (.9) |\n| Noncore | 5,389 (.5) | 336 (.9) | 1,058 (.9) | 1,814 (.1) | 2,181 (.1) |\n| **Census division, n (%)** | **Census division, n (%)** | **Census division, n (%)** | **Census division, n (%)** | **Census division, n (%)** | **Census division, n (%)** |\n| New England | 3,357 (.7) | 884 (.9) | 915 (.1) | 851 (.7) | 707 (.9) |\n| Middle Atlantic | 10,047 (.0) | 4,063 (.6) | 2,401 (.4) | 2,080 (.6) | 1,503 (.4) |\n| East North Central | 11,702 (.3) | 2,393 (.3) | 3,310 (.4) | 3,593 (.0) | 2,406 (.4) |\n| West North Central | 5,264 (.3) | 560 (.1) | 1,264 (.0) | 1,897 (.6) | 1,543 (.6) |\n| South Atlantic | 13,528 (.8) | 2,366 (.2) | 3,110 (.3) | 3,215 (.9) | 4,837 (.9) |\n| East South Central | 4,425 (.2) | 592 (.3) | 1,283 (.1) | 1,435 (.0) | 1,115 (.2) |\n| West South Central | 8,085 (.2) | 2,004 (.2) | 2,115 (.8) | 2,003 (.1) | 1,963 (.9) |\n| Mountain | 5,211 (.2) | 914 (.1) | 1,176 (.5) | 1,255 (.0) | 1,866 (.4) |\n| Pacific | 10,280 (.3) | 4,196 (.3) | 2,406 (.4) | 1,645 (.2) | 2,033 (.3) |\n\n【4】Abbreviations: GED, general equivalency diploma; IQR, interquartile range; SD, standard deviation.  \na  Data source: 5-year estimates from the American Community Survey 2009–2013, including data on self-reported veteran status . Concentration of veterans calculated as percentage of veterans among the adult population in each census tract. Concentrations categorized into quartiles: low (%–6.0%), mid-low (.0%–8.8%), mid-high (.8%–11.5%), and high (.5%–100%). Estimated veteran population: all tracts, 21,080,150; low-concentration census tracts, 2,237,420; mid-low–concentration census tracts, 4,507,889; mid-high–concentration census tracts, 5,991,587; high-concentration census tracts 8,343,254.  \nb  Location data from the Defense Installations Spatial Data Infrastructure Program . Distance to nearest military installation calculated as the straight-line distance from the census-tract centroid to the closest boundary of the installation.  \nc  Each census tract classified according to its county location per National Center for Health Statistics’ 2013 Urban–Rural Classification Scheme for Counties .\n\n【5】#####  Table 2. Availability of Food and Physical Activity Venues in Continental US Census Tracts, by Concentration of Veterans, 2009–2013 a  \n\n| Venues | All Tracts (N = 71,899) | Census Tract Veteran Concentration Quartile a |\n| --- | --- | --- |\n| Low (N = 17,972) | Mid–Low (N = 17,980) | Mid–High (N = 17,974) | High (N = 17,973) |\n| --- | --- | --- | --- |\n| **Venue Counts, Mean (SD) \\[IQR\\]** | **Venue Counts, Mean (SD) \\[IQR\\]** | **Venue Counts, Mean (SD) \\[IQR\\]** | **Venue Counts, Mean (SD) \\[IQR\\]** | **Venue Counts, Mean (SD) \\[IQR\\]** | **Venue Counts, Mean (SD) \\[IQR\\]** |\n| **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** |\n| All | 1.4 (.7) \\[0–1.6\\] | 3.2 (.4) \\[0.6–3.7\\] | 1.1 (.7) \\[0.1–1.6\\] | 0.7 (.1) \\[0–1.0\\] | 0.5 (.9) \\[0–0.8\\] |\n| Chain | 0.8 (.2) \\[0–1.0\\] | 1.5 (.8) \\[0.2–2.0\\] | 0.7 (.0) \\[0–1.0\\] | 0.5 (.7) \\[0–0.7\\] | 0.4 (.6) \\[0–0.5\\] |\n| Nonchain | 0.6 (.7) \\[0–0.5\\] | 1.7 (.0) \\[0–1.9\\] | 0.4 (.0) \\[0–0.5\\] | 0.2 (.6) \\[0–0.2\\] | 0.1 (.4) \\[0–0.1\\] |\n| **Grocery stores** | 4.2 (.9) \\[0–2.3\\] | 13.0 (.5) \\[0.8–11.6\\] | 2.4 (.1) \\[0–2.1\\] | 1.0 (.7) \\[0–0.9\\] | 0.6 (.0) \\[0–0.5\\] |\n| **Convenience stores** | 4.2 (.8) \\[0.3–5.7\\] | 8.7 (.3) \\[2.8–11.8\\] | 3.7 (.4) \\[0.5–5.6\\] | 2.4 (.2) \\[0.1–3.7\\] | 1.8 (.6) \\[0.1–2.6\\] |\n| **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** |\n| All | 11.5 (.2) \\[0.6–13.1\\] | 26.8 (.7) \\[6.3–27.9\\] | 9.1 (.6) \\[1.0–12.6\\] | 5.7 (.3) \\[0.2–8.6\\] | 4.3 (.5) \\[0.1–6.3\\] |\n| Chain | 4.6 (.9) \\[0.2–6.7\\] | 8.9 (.3) \\[2.8–11.6\\] | 4.1 (.1) \\[0.4–6.3\\] | 2.9 (.0) \\[0.1–4.5\\] | 2.4 (.6) \\[0–3.4\\] |\n| Nonchain | 6.9 (.3) \\[0.2–6.1\\] | 17.9 (.8) \\[2.5–16.9\\] | 5.0 (.5) \\[0.4–5.9\\] | 2.8 (.1) \\[0.1–3.7\\] | 1.9 (.5) \\[0–2.6\\] |\n| **Commercial fitness facilities** | 4.6 (.3) \\[0.2–5.1\\] | 9.8 (.5) \\[1.8–10.6\\] | 4.0 (.8) \\[0.4–5.3\\] | 2.5 (.8) \\[0.1–3.5\\] | 1.9 (.2) \\[0.1–2.6\\] |\n| **Parks b** | 2.3 (.7) \\[0.2–3.5\\] | 3.7 (.2) \\[1.4–5.2\\] | 2.5 (.7) \\[0.4–3.9\\] | 1.7 (.3) \\[0.1–2.6\\] | 1.3 (.9) \\[0.1–1.7\\] |\n| **≥1 Venue, n (%)** | **≥1 Venue, n (%)** | **≥1 Venue, n (%)** | **≥1 Venue, n (%)** | **≥1 Venue, n (%)** | **≥1 Venue, n (%)** |\n| **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** |\n| All | 26,466 (.8) | 11,964 (.6) | 6,718 (.4) | 4,418 (.6) | 3,366 (.7) |\n| Chain | 18,842 (.2) | 8,869 (.4) | 4,584 (.5) | 2,988 (.6) | 2,401 (.4) |\n| Nonchain | 11,068 (.4) | 7,290 (.6) | 2,185 (.2) | 1,024 (.7) | 569 (.2) |\n| **Grocery stores** | 26,780 (.3) | 13,032 (.5) | 6,983 (.8) | 4,062 (.6) | 2,703 (.0) |\n| **Convenience stores** | 44,847 (.4) | 15,622 (.9) | 11,935 (.4) | 9,398 (.3) | 7,892 (.9) |\n| **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** |\n| All | 50,761 (.6) | 16,493 (.8) | 13,535 (.3) | 11,038 (.4) | 9,695 (.9) |\n| Chain | 45,015 (.6) | 15,603 (.8) | 11,881 (.1) | 9,462 (.6) | 8,069 (.9) |\n| Nonchain | 43,635 (.7) | 15,486 (.2) | 11,742 (.3) | 8,951 (.8) | 7,456 (.5) |\n| **Commercial fitness facilities** | 43,633 (.7) | 14,993 (.4) | 11,740 (.3) | 9,117 (.7) | 7,783 (.3) |\n| **Parks b** | 40,438 (.2) | 14,659 (.6) | 10,944 (.9) | 8,125 (.2) | 6,710 (.3) |\n\n【7】Abbreviations: IQR, interquartile range; SD, standard deviation.  \na  Data source: 5-year estimates from the American Community Survey 2009–2013, including data on self-reported veteran status . Neighborhood venue availability was defined as a count of food outlets, commercial fitness facilities, or parks; it represents the average number within a 1-mile radius of the geographic center of all 30 m × 30 m cells (areas) covered by the census tracts with similar veteran concentrations. Concentration of veterans calculated as percentage of veterans among the adult population in each census tract. Concentrations categorized into quartiles: low (%–6.0%), mid-low (.0%–8.8%), mid-high (.8%–11.5%), and high (.5%–100%). Associations between concentration of veterans and residential environmental feature were tested by using 1-way ANOVA; all found to be statistically significant at _P_ < .001.  \nb  Includes local, state, and national parks.\n\n【8】#####  Table 3. Adjusted Associations Between Concentration of Veterans a  and Supermarket Availability in Continental US Census Tracts, 2009–2013 b  \n\n| Variable | β (SE) \\[ _P_ Value\\] |\n| --- | --- |\n| All Tracts (N = 71,899) | Nonmetropolitan Tracts c (N = 11,900) |\n| --- | --- |\n| Model 1 | Model 2 | Model 1 | Model 2 |\n| --- | --- | --- | --- |\n| **Concentration of veterans, by quartile a** | **Concentration of veterans, by quartile a** | **Concentration of veterans, by quartile a** | **Concentration of veterans, by quartile a** | **Concentration of veterans, by quartile a** |\n| Low | Reference | Reference | Reference | Reference |\n| Mid-low | NA | −0.204 (.019) \\[<.001\\] | NA | 0.047 (.012) \\[<.001\\] |\n| Mid-high | NA | −0.205 (.021) \\[<.001\\] | NA | 0.048 (.013) \\[<.001\\] |\n| High | NA | −0.226 (.023) \\[<.001\\] | NA | 0.028 (.013) \\[.04\\] |\n| **Percentage of adults aged >65** | 0.003 (<0.001) \\[<.001\\] | 0.006 (<0.001) \\[<.001\\] | 0.004 (<0.001) \\[<.001\\] | 0.005 (<0.001) \\[<.001\\] |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Percentage non-Hispanic white | Reference | Reference | Reference | Reference |\n| Percentage non-Hispanic black | 0.001 (<0.001) \\[<.001\\] | 0.001 (<0.001) \\[.13\\] | 0.002 (<0.001) \\[<.001\\] | 0.002 (<0.001) \\[<.001\\] |\n| Percentage non-Hispanic Asian | 0.008 (<0.001) \\[<.001\\] | 0.006 (<0.001) \\[<.001\\] | 0.001 (.002) \\[.58\\] | 0.002 (.002) \\[.41\\] |\n| Percentage non-Hispanic other | 0.005 (.001) \\[<.001\\] | 0.005 (.001) \\[<.001\\] | 0 (<0.001) \\[.33\\] | 0 (<0.001) \\[.42\\] |\n| Percentage Hispanic | 0.001 (<0.001) \\[.02\\] | 0 (<0.001) \\[.90\\] | 0.001 (<0.001) \\[.003\\] | 0.001 (<0.001) \\[.002\\] |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| <High school diploma or GED | Reference | Reference | Reference | Reference |\n| High school diploma, no bachelor’s degree | −0.017 (<0.001) \\[<.001\\] | −0.015 (<0.001) \\[<.001\\] | −0.001 (<0.001) \\[.03\\] | −0.001 (<0.001) \\[<.001\\] |\n| Bachelor’s degree or more | 0.007 (<0.001) \\[<.001\\] | 0.008 (<0.001) \\[<.001\\] | 0.002 (<0.001) \\[.002\\] | 0.002 (<0.001) \\[.002\\] |\n| **Median household income, in thousands, $** | −0.007 (<0.001) \\[<.001\\] | −0.007 (<0.001) \\[<.001\\] | −0.003 (<0.001) \\[<.001\\] | −0.003 (<0.001) \\[<.001\\] |\n| **Population density, per km 2 , in thousands** | 0 (<0.001) \\[<.001\\] | 0 (<0.001) \\[<.001\\] | 0 (<0.001) \\[<.001\\] | 0 (<0.001) \\[<.001\\] |\n| **Urbanicity c** | **Urbanicity c** | **Urbanicity c** | **Urbanicity c** | **Urbanicity c** |\n| Large central metropolitan | Reference | Reference | NA | NA |\n| Large fringe metropolitan | −0.506 (.018) \\[<.001\\] | −0.502 (.018) \\[<.001\\] | NA | NA |\n| Medium metropolitan | −0.434 (.018) \\[<.001\\] | −0.417 (.018) \\[<.001\\] | NA | NA |\n| Small metropolitan | −0.473 (.024) \\[<.001\\] | −0.453 (.024) \\[<.001\\] | NA | NA |\n| Micropolitan | −0.512 (.025) \\[<.001\\] | −0.487 (.025) \\[<.001\\] | Reference | Reference |\n| Noncore | −0.552 (.028) \\[<.001\\] | −0.531 (.028) \\[<.001\\] | −0.072 (.006) \\[<.001\\] | −0.072 (.006) \\[<.001\\] |\n| **Census division** | **Census division** | **Census division** | **Census division** | **Census division** |\n| Middle Atlantic | Reference | Reference | Reference | Reference |\n| New England | −0.668 (.032) \\[<.001\\] | −0.663 (.032) \\[<.001\\] | 0.060 (.018) \\[.001\\] | 0.061 (.018) \\[.001\\] |\n| East North Central | −0.518 (.022) \\[<.001\\] | −0.483 (.022) \\[<.001\\] | 0.068 (.013) \\[<.001\\] | 0.068 (.013) \\[<.001\\] |\n| West North Central | −0.518 (.028) \\[<.001\\] | −0.426 (.029) \\[<.001\\] | 0.041 (.013) \\[.002\\] | 0.041 (.013) \\[.002\\] |\n| South Atlantic | −0.581 (.022) \\[<.001\\] | −0.542 (.022) \\[<.001\\] | −0.001 (.014) \\[.96\\] | −0.002 (.014) \\[.86\\] |\n| East South Central | −0.633 (.03) \\[<.001\\] | −0.603 (.03) \\[<.001\\] | 0 (.014) \\[.10\\] | −0.001 (.014) \\[.95\\] |\n| West South Central | −0.714 (.025) \\[<.001\\] | −0.647 (.026) \\[<.001\\] | 0.007 (.014) \\[.60\\] | 0.007 (.014) \\[.62\\] |\n| Mountain | −0.603 (.029) \\[<.001\\] | −0.561 (.029) \\[<.001\\] | 0.037 (.016) \\[.02\\] | 0.039 (.016) \\[.01\\] |\n| Pacific | −0.374 (.024) \\[<.001\\] | −0.348 (.024) \\[<.001\\] | 0.053 (.017) \\[.002\\] | 0.057 (.017) \\[.001\\] |\n| **Distance to military installation, km d** | NA | −0.001 (<0.001) \\[<.001\\] | NA | 0 (<0.001) \\[.92\\] |\n\n【10】Abbreviations: GED, general equivalency diploma; NA, not applicable; SE, standard error.  \na  Data source: 5-year estimates from the American Community Survey 2009–2013, including data on self-reported veteran status . Concentration of veterans calculated as percentage of veterans among the adult population in each census tract. Concentrations categorized into quartiles: low (%–6.0%), mid-low (.0%–8.8%), mid-high (.8%–11.5%), and high (.5%–100%).  \nb  Neighborhood venue availability defined as a count of food outlets, commercial fitness facilities, or parks; it represents the average number within a 1-mile radius of the geographic center of all 30 m × 30 m cells (areas) covered by the census tracts with similar veteran concentrations. Model 1 includes sociodemographic, economic, and geographic characteristics of census tracts. Model 2 adds the concentration of veterans per census tract and a measure of distance to the nearest military installation. _P_ values were obtained from ordinary least squares regression models.  \nc  Each census tract classified according to its county location per National Center for Health Statistics’ 2013 Urban–Rural Classification Scheme for Counties .  \nd  Location data from the Defense Installations Spatial Data Infrastructure Program . Distance to nearest military installation calculated as the straight-line distance from the census-tract centroid to the closest boundary of the installation.\n\n【11】Appendices: Supplementary Table and Figure\n------------------------------------------\n\n| Attribute | All Tracts (N = 71,899) | Metropolitan Tracts (N = 59,999) | Nonmetropolitan Tracts (N = 11,900) |\n| --- | --- | --- | --- |\n| Low-Mid | High-Mid | High | Low-Mid | High-Mid | High | Low-Mid | High-Mid | High |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** | **Supermarkets** |\n| All | –0.204 | –0.205 | –0.226 | –0.217 | –0.220 | –0.242 | 0.047 | 0.048 | 0.028 |\n| Chain | –0.085 | –0.123 | –0.177 | –0.093 | –0.136 | –0.199 | 0.036 | 0.034 | 0.017 |\n| Nonchain | –0.119 | –0.082 | –0.049 | –0.124 | –0.084 | –0.043 | 0.011 | 0.014 | 0.010 |\n| **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** | **Grocery stores and convenience stores** |\n| Grocery | –0.957 | –0.501 | 0.080 | –0.940 | –0.466 | 0.209 b | 0.045 | 0.040 | 0.041 |\n| Convenience | –0.847 | –1.065 | –1.102 | –0.860 | –1.138 | –1.206 | 0.283 | 0.287 | 0.224 |\n| **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** | **Fast food restaurants** |\n| All | –1.994 | –2.061 | –2.079 | –2.099 | –2.161 | –2.144 | 0.308 | 0.233 | 0.111 c |\n| Chain | –0.453 | –0.521 | –0.667 | –0.491 | –0.569 | –0.750 | 0.177 | 0.119 | 0.044 |\n| Nonchain | –1.541 | –1.540 | –1.413 | –1.608 | –1.592 | –1.394 | 0.132 | 0.114 | 0.067 |\n| **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** | **Recreational venues** |\n| Commercial fitness facilities | –0.785 | –1.019 | –1.281 | –0.885 | –1.124 | –1.390 | 0.261 | 0.215 | 0.174 |\n| Parks | –0.052 d | –0.317 | –0.491 | –0.056 e | –0.348 | –0.575 | 0.123 | 0.105 f | 0.120 |\n\n【13】a  Percentage of veterans among the adult population in each census tract, determined by using 5-year estimates from the American Community Survey 2009–2013, including data on self-reported veteran status . Concentrations categorized into quartiles: low (%–6.0%), mid-low (.0%–8.8%), mid-high (.8%–11.5%), and high (.5%–100%). Neighborhood venue availability defined as a count of food outlets, commercial fitness facilities, or parks; it represents the average number within a 1-mile radius of the geographic center of all the 30 m × 30 m cells (areas) covered by the census tracts with similar veteran concentrations. _P_ values determined by ordinary least-squares regression. Unless otherwise indicated, _P_ value is <.001. Reference group for all values is the low quartile.  \nb  _P_ \\= .14.  \nc  _P_ \\= .15.  \nd  _P_ \\= .05.  \ne  _P_ \\= .06.  \nf  _P_ \\= .001.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7a63ac22-d339-42ad-a65e-679e33f203e2", "title": "Postexposure Treatment and Animal Rabies, Ontario, 1958-2000", "text": "【0】Postexposure Treatment and Animal Rabies, Ontario, 1958-2000\nJurisdictions across North America have identified animal rabies as a serious public health concern  because the epidemiology of human rabies closely follows the epizoology of animal rabies . Recent studies have examined the relationship between animal rabies and postexposure treatment (PET) for reasons of surveillance, economic impact, epidemiology, and appropriate treatment . We investigated the nature of this relationship in Ontario from 1958 to 2000, focusing on the impact of two important advances in rabies prevention: a) the introduction of human diploid cell rabies vaccine (HDCV) and b) the initiation of an oral rabies vaccination program (ORVP).\n\n【1】Animal rabies has had a long and varied history in Ontario. Before the 1950s, sporadic outbreaks of rabies occurred, usually associated with dogs. In the early 1950s, a rabies epizootic swept southward from the Arctic, entered northern Ontario in 1954, and by 1958 became enzootic in southern Ontario in red foxes (Vulpes vulpes_ ) and striped skunks (Mephitis mephitis_ )  . Sylvatic rabies has, in turn, infected companion animals and livestock, the two groups responsible for most subsequent human exposures (Honig JM, unpub. data, 1985). In 1999, the strain of raccoon rabies that has moved north along the eastern seaboard of the United States entered eastern Ontario from northern New York; by the end of 2000, there were 48 reported cases in raccoons (during 1999; 40 during 2000)  . Since 1958, Ontario has averaged 1,200 to 1,300 animal cases per year, for a total of >56,000 cases by 2000. The burden on the public health system has been substantial; for example, >63,000 PETs were reported in the same period. In addition, public health officials have had to investigate all contacts between humans and animals in which rabies may have been transmitted. In the 1980s, for example, at the height of the rabies enzootic, 15,000 to 25,000 such investigations were carried out annually  .\n\n【2】In Canada, all animal rabies collection and laboratory diagnoses are handled by the Canadian Food Inspection Agency (CFIA), the federal ministry responsible for establishing the collection protocols and the laboratory diagnosis of submitted specimens that were suspected of carrying _Rabies virus_ (RABV). District veterinary officers throughout the country are responsible for specimen collection and the decision to send specimens to federally operated laboratories for testing.\n\n【3】During the study period, Ontario’s Ministry of Health and Long-Term Care (MOHLTC) distributed vaccine to physicians, free of charge, for the prevention of human rabies. In the fiscal year 1980-81, the ministry began distributing the newly licensed HDCV to replace earlier Semple and duck embryo vaccines. By the fiscal year 1983-84, all distributed vaccines were HDCV  . HDCV was an important advance in rabies prevention because “it is a better immunogen with fewer side effects and requires far fewer doses than the previously recommended duck embryo vaccines”  .\n\n【4】A second important advance in rabies prevention was ORVP. In 1989, the Ontario Ministry of Natural Resources initiated an ORVP in eastern Ontario, targeting the principal wildlife vectors  . By 1994, the ministry had extended ORVP to cover the epizootic area in southern Ontario, and over 1 million vaccine baits were dropped annually. The program resulted in a dramatic drop in rabies incidence in southern Ontario  .\n\n【5】### Methods\n\n【6】We gathered the PET and animal rabies data  from two government agencies. The MOHLTC annual reports from 1958 to 1978 list the number of courses of rabies vaccine distributed in each calendar year; for this paper, we considered each such course as a PET. For 1979 to 1988 and 1998 to 2000, we obtained similar records directly from internal reports in the MOHLTC. For 1989 to 1997, we obtained vaccine distribution data from the Public Health and Epidemiology Reports for Ontario  . We obtained the annual number of laboratory-confirmed cases of animal rabies in Ontario directly from CFIA. Data were compiled on all terrestrial animals and bats that tested positive for RABV. Data on the number of negative test results were not available.\n\n【7】Our PET and rabies data were maintained for the entire study period by two central government agencies with a consistent mandate for collecting and reporting. Unfortunately, because these two agencies operate independently, we could not match the individual human treatments to the specific specimens that tested positive for rabies.\n\n【8】Human population data were obtained from Statistics Canada Quarterly Estimates of Population for Canada, Provinces and Territories, 1951-2000.\n\n【9】We used regression analysis to examine the relationship between PET and the number of laboratory-confirmed cases of rabies in terrestrial animals and bats in Ontario. Analyses were done for the periods 1958 to 1980 and 1981 to 2000. As previously noted, HDCV was used during the second period. We used SPSS (release 10.0.5, SPSS Inc. Chicago, IL **)** to perform the regressions.\n\n【10】### Results\n\n【11】From 1958 to 1980, the ratio of human treatments to animal cases was <1 in most years . After HDCV was introduced in 1980, the yearly ratios of human treatments to animal cases were >1. Furthermore, from 1980 to 1981, the rate of PET per 100,000 persons almost doubled. The annual number of PETs increased from an average of approximately 1,000 in the 1970s to an average of more than 2,000 per year during the 1980s. During the 1980s and early 1990s, the annual number of PETs closely paralleled the annual number of animal cases.\n\n【12】The regression for the 1958 to 1980 period showed a weak but significant relationship between PET and animal rabies (R 2  \\=0.42, p<0.001, n = 23, intercept = 557 \\[standard error, SE, 135.4\\], slope = 0.358 \\[SE 0.092\\]). After 1980 the relationship was much stronger (R 2  \\=0.91, p<0.001, n = 20, intercept = 861 \\[SE 100.5\\], slope = 0.877 \\[SE 0.067\\]). The slopes of these regressions indicate that before 1980, there were approximately three reports of rabid animals for every PET, whereas after 1980, the ratio was approximately 1:1. Finally, the regression demonstrates that the base level of treatments after 1980 was 861, approximately 55% higher than the base level  before 1980.\n\n【13】Following the initiation of ORVP, the regular cycle of animal rabies was broken in the early 1990s  and the number of laboratory-confirmed rabid animals declined. The number of human treatments also declined by 50%, from more than 2,000 per year throughout most of the 1980s to approximately 1,000 per year in the late 1990s.\n\n【14】During the period 1968 to 1980, there was an apparent change in the relationship between PET and animal rabies compared with the initial 1958 to 1967 period of the enzootic . The ratios of PET per rabid animal and PET rate per 100,000 persons for the 1958 to 1967 period were 0.9 and 16.0, respectively, and declined to mean values of 0.7 and 13.2, respectively, for the 1968 to 1980 period.\n\n【15】### Discussion\n\n【16】There was a dramatic change in the relationship between PET and animal rabies coincident with the introduction of HDCV in 1980. Furthermore, after 1980, the positive correlation between human treatments and animal rabies strengthened. We could not find any directives from the MOHLTC or published studies indicating a change in treatment policy when HDCV was introduced. Since HDCV had fewer side effects and the vaccination regime was simpler and less traumatic than previous treatments, there was, perhaps, less reluctance to administer PET after 1980 and, therefore, the use of PET increased and paralleled the incidence of animal rabies more closely.\n\n【17】When Ontario began its ORVP, one of the arguments for it was that animal rabies would be reduced and human treatments would follow suit. Our findings support this argument and are consistent with reports from other jurisdictions . The decline in human treatments in Ontario, however, was not as rapid as the decline in rabies cases. In fact, in recent years, while animal rabies incidence dropped to approximately 100 to 200 reported cases per year, PET leveled off at approximately 1,000 per year, about the same level as immediately before HDCV was introduced. We suspect the reasons for the continued high ratio of PET to animal rabies are varied and complex. For instance, with the continued presence of rabies, if a suspect animal is not available for testing, PET is administered as a precautionary measure. Furthermore, rabies in bats has been unaffected by the ORVP. Bats have been implicated in more than half (of 28 cases) of the human rabies cases diagnosed in the United States since 1980  and in a recent death in Quebec  . The recent spread of raccoon rabies from New York State into southeastern Ontario has increased the media coverage of rabies and will contribute to uncertainty about the presence of the RABV in the province. Under these circumstances, with the relatively safe HDCV, a continuing high number of treatments should be expected. Even in the absence of further rabies cases, the regression results suggest, at the 95% confidence interval, annual treatments would range from 650 to 1,072 annually.\n\n【18】The decline in PET per rabid animal and PET rate per 100,000 persons in the 1968 to 1980 period hints at other factors affecting the relationship between animal rabies and the administration of PET. We were unable to find any evidence in published studies detailing a change of government policy about the administration of PET or some traumatic event that could initiate a de facto policy change. Indeed, studies during that period recommended a treatment approach similar to today’s guidelines  . Furthermore, compulsory vaccination of companion animals was not an explanation. Under the Health Protection and Promotion Act, a regulation governing rabies immunization was not introduced until 1984 and it has taken until 2000 for all district health units in southern Ontario to be included in it  . All we know is that, early in this period, rabies incidence across southern Ontario had stabilized and developed regular cycles in various regions  . We can only speculate that, as animal rabies incidence became more predictable, health professionals and the public learned to manage the risk and there was less pressure to give PET, especially with older vaccines and their lengthy regime of injections. Experience in the United States indicated that consultation with state health departments during management of potential rabies exposure reduced PETs  .\n\n【19】### Conclusions\n\n【20】Our data suggest that human interventions have played a major role in the relationship between PET and animal rabies. The introduction of a new, safer vaccine was associated with a sudden increase in the number of PETs per rabid animal. Furthermore, while the introduction of an ORVP reduced animal rabies, PET did not drop at a similar rate and has appeared to stabilize at approximately 1,000 persons per year. This stabilization, despite the diminishing number of rabies cases, is important in estimating the economic impact of rabies control and public education. However, as our data for the 1968 to 1980 period show, there are other, as-yet-unknown factors that affect the animal rabies/PET relationship.\n\n【21】We believe that two general approaches are needed for the future study of this complex relationship. First, we need details of the circumstances of rabies incidents involving human exposures, such as those assembled by Moran et al. Honig (unpub. data), and the Public Health Branch, Ministry of Health  . For Ontario, assembling these data will require follow-up interviews on a case-by-case basis. Second, if we can obtain data on the distribution of PETs by the 32 health units in southern Ontario (we have distribution data for animal rabies), we may gain further insight by (a) examining the distribution and interaction of human and animal populations; (b) investigating the influence of the geographic scale at which the relationships are examined; and (c) making regional comparisons of the administration of PET and the relative surveillance efforts in an area over time, given the history of rabies incidence and public awareness campaigns in the area.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cd99960d-32dd-42c5-a6c9-7ce28749e4f8", "title": "Multidrug-Resistant Shiga Toxin–Producing Escherichia coli O118:H16 in Latin America", "text": "【0】Multidrug-Resistant Shiga Toxin–Producing Escherichia coli O118:H16 in Latin America\n**To the Editor:** We report the first isolation of a multiple antimicrobial drug–resistant strain of Shiga toxin–producing _Escherichia coli_ (STEC) O118:H16 from cattle in Latin America. The strain was isolated during a study of fecal STEC in 205 healthy and 139 diarrheic cattle on 12 beef farms in the state of São Paulo, Brazil, in February 2000; one case of STEC was found in a 1-month-old calf with diarrhea. This bovine STEC O118:H16 strain showed resistance to eight antimicrobial substances; the following resistance (R)-genes were detected: ampicillin (bla_ TEM1-like  ), kanamycin and neomycin (aphA1_ ), streptomycin (strA/B_ ), sulphametoxazol (sul2_ ), tetracyclin (tet_ \\[A\\]), trimethoprim (no _dfrA1,_ _A5_ , _A7_ , _A12_ , _A14_ , or _A17_ ), and trimethoprim/sulphamethoxazol. The STEC O118:H16 strain from Brazil was found to be similar for virulence genes (Shiga toxin 1 \\[ _stx_ 1\\], intimin beta 1 \\[ _eae_ β1\\], and EHEC-hemolysin \\[E- _hly_ A\\]) and for antimicrobial drug resistance to STEC O118:H16 strains, which were isolated in different countries of Europe . Beginning in 1986, STEC O118:H16 was identified as an emerging pathogen for calves and humans in Belgium and Germany . Cattle and human STEC O118:H16 isolates were similar in virulence attributes and antimicrobial drug resistance and belonged to a distinct genetic clone . Transmission of these pathogens from cattle to humans on farms was observed .\n\n【1】Beginning in 1996, STEC O118:H16 has become important as an emerging pathogen in humans and has been associated with bloody diarrhea and hemolytic uremic syndrome . Analysis of the antimicrobial resistance profiles showed that >96% of the European STEC O118:H16 strains showed resistance to one or more antimicrobial drugs in contrast to the 10% to 15% drug-resistant strains that were detected among STEC belonging to other serotypes . STEC O118:H16 showing multiresistance in up to eight different antimicrobial drugs predominated among younger isolates, indicating that drug resistance genes have accumulated over time in STEC O118:H16 strains. The frequency of antimicrobial drug resistance in STEC and Stx-negative _E. coli_ in humans and animals was compared in a study by Schroeder et al. Among human clinical _E. coli_ isolates, antimicrobial resistance was less frequently observed in STEC than in Stx-negative strains, whereas in cattle, antibiotic-resistant strains were found at similar frequencies in both groups of _E. coli_ . The relatively higher frequency of antimicrobial-resistant STEC in cattle was explained by the use of antimicrobial drugs in cattle production, whereas human infections with STEC are generally not treated with antibiotics . Cattle could thus be an important source of new emerging antibiotic-resistant STEC strains such as O118:H16.\n\n【2】The genetic basis of antimicrobial resistance in STEC O118:H16 is broad, including R-plasmids, integrons, transposons, and chromosomally inherited drug-resistance genes. Fluoroquinolone resistance has also been acquired by some STEC O118:H16 strains . The heterogenicity of antimicrobial drug–resistance patterns, the increase of multidrug-resistant strains over time of isolation, and the evidence for multiple aquisition and genetic location of R-determinants indicate that strains belonging to the STEC O118:H16 clone have a propensity to acquire and accumulate R-genes. The finding that multidrug-resistant STEC O118:H16 is isolated from cattle in South America indicates the global spread of this new emerging EHEC type.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a39b5b9a-1892-4176-bf5f-ec640d3ce77b", "title": "Tuberculosis Elimination in the Netherlands", "text": "【0】Tuberculosis Elimination in the Netherlands\nReported rates of tuberculosis (TB) in the Netherlands in 2003 were 3.5 per 100,000 among Dutch residents and 125 per 100,000 among the non-Dutch. The non-Dutch are formally defined as those without a Dutch passport, but in practice they include mostly foreign-born persons. During the past 10 years, TB reports in the Netherlands declined among the Dutch (from 693 in 1993 to 531 in 2002) and remained approximately stable among the non-Dutch (at an average of 892 per year). To what extent these patterns were attributable to changes in TB transmission and to what extent to changes in the introduction of new strains from abroad or from reactivation of latent infection are unclear.\n\n【1】According to mathematical models, eliminating TB as a public health problem, defined as a prevalence of _Mycobacterium tuberculosis_ infection of <1%, has been predicted to occur among the Dutch by 2030 if no further changes in transmission occur . However, if elimination is defined as an incidence of smear-positive disease of <1 case per million population per year , TB is not expected to be eliminated by that date because of transmission from foreign-source cases . The proportion of all Dutch TB cases in the period 1993–1998 attributable to recent transmission from a non-Dutch source case has been estimated at 12% to 20% with DNA-fingerprinting data . With time, an increasing proportion of TB cases among the Dutch were expected to be attributable to recent transmission from non-Dutch source cases . However, direct observations with fingerprinting results have not yet been used to evaluate these model predictions.\n\n【2】Immigration patterns in the Netherlands have varied in the past decade: large numbers of persons from countries with high TB endemicity have sought asylum in the early 1990s. In recent years, these numbers became smaller after stricter immigration laws were passed. Shifts in countries of origin of immigrants have also occurred, and some of these countries had much higher TB rates than others. Therefore, the introduction of new strains from abroad may be expected to have varied over time. Control measures, in contrast, have shown little change over the study period. TB screening is obligatory at entry and is offered every 6 months for 2 years on a voluntary basis. No routine screening for and treatment of latent infection exist for immigrants.\n\n【3】This study attempted to determine, by DNA-fingerprinting of _M. tuberculosis_ isolates, to what extent TB trends from 1995 to 2002 were determined by changes in the introduction of new strains and by changes in ongoing transmission. We also describe the trend of TB transmission from non-Dutch source patients to the Dutch population. The combined evidence is used to assess the prospects for eliminating TB in the Netherlands.\n\n【4】### Methods\n\n【5】Patient and treatment data since 1993 were available in the Netherlands Tuberculosis Register, an anonymous case register maintained by the KNCV Tuberculosis Foundation. Reporting to the register is voluntary, but cross-matching with mandatory reports to the ministry of health on all patients who have started TB treatment suggests >99% completeness. The register includes data on demographic characteristics, clinical features, risk groups, treatment given, and treatment outcome. From January 1993 to December 2002, a total of 15,331 TB patients were registered, including those without bacteriologic confirmation.\n\n【6】Over the same period, 10,356 first _M. tuberculosis_ isolates of TB patients were subjected to standard IS _6110_ restriction fragment length polymorphism (RFLP) analysis . Subtyping with the polymorphic GC-rich sequence probe was carried out for strains with <5 IS _6110_ copies. IS _6110_ RFLP patterns were analyzed by using the Bionumerics software, version 3.5 for Windows (Applied Maths, Sint-Martens-Latem, Belgium).\n\n【7】Information from the 2 databases was combined; sex, date of birth, postal area code, and year of diagnosis were used as identifiers. A perfect match was obtained for 7,529 (%) isolates and a near-perfect match for 981 (%). Both groups were included, yielding a total study size of 8,510 (%) culture-positive patients. Mismatches may be due to administrative errors in a database, unreliable date of birth (e.g. for some immigrant groups), or postal area code (e.g. homeless), and the exclusion of persons with identical identifiers.\n\n【8】TB can occur soon after primary infection or reinfection (recent transmission) or as the result of endogenous reactivation of latent infection . The cut-off point for separating recent from remote transmission is arbitrary: some researchers used 5 years , others 2 years , and others 1 year . We estimated the percentage of cases with identical RFLP patterns occurring within a given period after each culture-positive case with Kaplan-Meier survival analysis, as suggested by Jasmer et al. The Kaplan-Meier estimate of the probability that a patient was followed by another with an identical fingerprint was 46.2% for the total study period and 33.7% for a 2-year period. Thus, of all cases followed by a patient with an identical fingerprint within 10 years, 73% were followed within 2 years. Using this information, we defined strains as new if the RFLP pattern had not been observed in another patient during the previous 2 years. Other strains were attributed to ongoing transmission. During the first 2 study years , judging whether strains were new was not possible; therefore, data from these 2 years were used to define new strains from 1995 onwards but were otherwise excluded from the analysis.\n\n【9】The observation period in which secondary cases can be observed is longer for strains introduced earlier in the study period than for those introduced later. To obtain an unbiased estimate of the trend of the number of secondary cases generated by source cases, secondary cases arising >2 years after a new strain was introduced were excluded. Thus, all patients were assigned to 1 of the following 3 mutually exclusive categories: case with a new strain, secondary case within 2 years of the introduction of a new strain, and secondary case >2 years after the introduction of a new strain. To assess the trend of transmission between Dutch and non-Dutch persons, secondary cases were attributed to a source case-patient, defined as the patient from whom the new strain was first isolated .\n\n【10】Population data by year, age group, sex, and (Dutch/non-Dutch) nationality were obtained from Statistics Netherlands  and used as denominators for incidence rates. Relative risks of TB by year of diagnosis, age, sex, and Dutch or non-Dutch nationality were determined separately for new strains and secondary cases with Poisson regression. Risk factors for the average number of secondary cases per new strain were also identified with Poisson regression .\n\n【11】### Results\n\n【12】Of the 8,510 TB patients with known RFLP results in the period 1993–2002, 1,580 were found in 1993 to 1994, and 6,930 in 1995 to 2002. Of the latter, 4,594 (%) had new strains, 1,198 (%) had secondary cases within 2 years of the introduction of a new strain, and 1,138 (%) had secondary cases >2 years after a new strain was introduced.\n\n【13】The incidence of TB with new strains was on average 52 per 100,000 among the non-Dutch and 1.4 per 100,000 among the Dutch. The incidence declined over the study period among the Dutch (rate ratio per year 0.96, 95% confidence interval \\[CI\\] 0.94–0.98) and was stable among the non-Dutch (rate ratio per year 1.02, 95% CI 1.00–1.03, p = 0.06) . The incidence of all cases attributed to recent transmission, regardless of the duration of the cluster, was 23 per 100,000 among the non-Dutch and 0.9 per 100,000 among the Dutch. The incidence declined among the Dutch (rate ratio per year 0.97, 95% CI 0.95–1.00, p = 0.03) but not among the non-Dutch (rate ratio per year 0.99, 95% CI 0.97–1.02) .\n\n【14】Reduction of TB incidence with new strains among the Dutch was restricted to those \\> 65 years of age . Incidence was stable at 0.85/100,000 in the age group <65 years (rate ratio per year 1.0, 95% CI 0.97–1.03), declined from 3.5 to 2.2 in those 65–74 years of age (rate ratio per year 0.91, 95% CI 0.86–0.95), and declined from 9.4 to 4.8 in those \\> 75 years of age (rate ratio per year 0.92, 95% CI 0.87–0.95). The incidence rate in the \\> 75-year age group declined more rapidly than the number of cases in that age group, since the population in that age group increased from 847,000 in 1995 to 979,000 in 2002.\n\n【15】Of the 4,594 patients with new strains in the period 1995–2002, a total of 3,459 were found in the period 1995–2000 and could be followed up for 2 years. Of the 1,318 Dutch patients with new strains, 182 (%) generated secondary cases at an average of 1.7 cases per cluster (.2 Dutch and 0.5 non-Dutch) . The average number of secondary cases generated was 0.23 per new strain and declined steeply with the age of the source case-patient (rate ratio per age group 0.74, 95% CI 0.70–0.78) . The average number of secondary cases generated did not depend on the sex of the source patient (p > 0.5). The average number of secondary case-patients per new strain did not differ significantly over time (rate ratio per year 0.96, 95% CI 0.89–1.02).\n\n【16】Of the 2,141 non-Dutch patients with new strains, 283 (%) generated secondary cases at an average of 1.9 cases per cluster (.5 Dutch and 1.4 non-Dutch) . The average number of secondary cases generated was 0.25 overall, declined with age of the source patient (rate ratio per age group 0.86, 95% CI 0.80–0.92), and was lower for female than male source patients (rate ratio 0.68, 95% CI 0.57–0.81) . The average number of secondary cases per new strain over time did not change (rate ratio per year 0.97, 95% CI 0.93–1.02).\n\n【17】In clusters starting in the period 1995–2000, an increasing proportion of Dutch secondary TB cases was attributable to a non-Dutch source case as time progressed (χ 2  trend  4.49, p = 0.03) . This trend was observed not only among those cases arising within 2 years of the start of the cluster  but also among all Dutch secondary cases, regardless of cluster duration (χ 2  trend  42, p < 0.001, data not shown). The proportion of Dutch secondary cases attributable to a non-Dutch source case declined steeply with age, both among all Dutch secondary case-patients (χ 2  trend  41, p < 0.001) and among those arising within 2 years of the start of the cluster (χ 2  trend  27, p < 0.001) . The proportion of cases attributed to a non-Dutch source patient was not associated with sex of the Dutch secondary case-patient .\n\n【18】### Discussion\n\n【19】This study suggests that the declining TB incidence among the Dutch in the Netherlands during the past decade has been achieved under stable control conditions. Among the Dutch, the incidence of TB attributable to new strains declined, particularly among the elderly. The incidence of TB cases due to recent transmission declined as well, a result of fewer new strains being introduced. The average number of secondary cases per new strain did not change significantly. An overall reduction in incidence of clustered cases among the U.S.-born population was also observed in San Francisco  and New York  and was attributed to improved control. In the Netherlands, we do not attribute the decline to improved control but to a cohort effect. Research may determine to what extent the reported declines in San Francisco and New York could be explained by a reduction in number of secondary cases per newly introduced strain and whether a cohort effect played a role in those settings.\n\n【20】In industrialized countries the annual risk for _M. tuberculosis_ infection has declined steeply over the past century ; as a result, compared to younger persons, older persons were exposed to much higher risks for infection in their youth . Thus, the prevalence of infection increases sharply with age. The risk for TB due to reactivation of latent infection therefore increases with age as well. Within the older age groups, this risk is now declining with each calendar year as earlier birth cohorts leave and more recent birth cohorts with lower infection prevalence enter the age group.\n\n【21】The incidence of culture-positive TB with new strains among Dutch persons <65 years of age was stable in the past decade, at 0.85 per 100,000 population in our matched dataset, and thus ≈1/100,000 or 10 per million if failure to match is taken into account. Of these new case-patients with a known sputum smear result, 63% had smear-positive TB. If elimination of TB as a public health problem is defined as achieving an incidence of new smear-positive TB cases of <1 per million , elimination is unlikely to be achieved under current epidemiologic conditions and control efforts.\n\n【22】Our study confirms previous predictions from a mathematical model about the increasing importance of transmission from immigrants to the Dutch population . The number of cases observed among the Dutch is best explained by immigrant scenarios 1 and 2 in the modeling study , which assume that a Dutch TB patient is 8 times more likely than a non-Dutch TB patient to infect a Dutch person.\n\n【23】Over time, the decline of TB incidence among elderly Dutch will become less important as the birth cohorts with a high prevalence of infection are replaced with cohorts with much lower infection rates. Contact with highly TB-endemic countries through immigrants and international travel, on the other hand, is becoming increasingly important as a determinant of TB trends in the Netherlands. This finding was shown in this study by the increasing proportion over time of Dutch patients with secondary cases attributed to a non-Dutch source patient. This finding suggests the need for further reorientation of the focus of TB control within the Netherlands towards immigrants and their contacts and reemphasizes the importance of global TB control for achieving TB elimination in countries with low incidence of this disease .\n\n【24】The separation of TB patients into those with new strains, attributed to reactivation or acquisition abroad, and secondary cases attributed to recent transmission is likely to be imperfect for the following reasons. Some strains identified as new may have represented ongoing transmission in the presence of strain evolution. Some strains attributed to ongoing transmission may represent remote transmission, particularly among the elderly . In this national database, epidemiologic confirmation of linkage between patients was far from complete . However, in a recent, more detailed study in Amsterdam, most clustered patients were found to have epidemiologic links . Missing data as a result of incomplete matching may have contributed to a slight underestimate of the observed clustering percentage and of the number of secondary cases per source case . However, since the matching percentage was not associated with calender year (data not shown), this underestimate should not affect the trend estimates. Some source cases may have been misclassified, in particular in large clusters. These sources of misclassification are expected to reduce the observed difference between cases with new strains and those attributed to recent transmission but do not invalidate the main conclusion that TB incidence among the Dutch was reduced mainly because of fewer reactivation cases among persons \\> 65 years of age.\n\n【25】We conclude that the decline of TB in the Netherlands during the past decade was mainly the result of a cohort effect: older birth cohorts with high infection prevalence were replaced by those with lower infection prevalence. Contact through immigrants and international travel with countries with high TB incidence increasingly determines TB trends in the Netherlands and will prevent achieving TB elimination under current conditions. Global TB control is required to achieve TB elimination in countries with a low incidence of this disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "efe225b6-2010-4bb5-9697-af3cd19538db", "title": "Pediatric SARS-CoV-2 Seroprevalence, Oregon, USA, November 1, 2020–June 30, 2022", "text": "【0】Pediatric SARS-CoV-2 Seroprevalence, Oregon, USA, November 1, 2020–June 30, 2022\nThrough June 30, 2022, a total of 140,820 pediatric cases of COVID-19 had been reported in Oregon, USA, representing ≈17.3% of all reported COVID-19 cases in the state. However, understanding the true burden of pediatric COVID-19 infection poses a challenge. Children are more likely to have asymptomatic or mild disease, and pediatric infections are, therefore, less likely to be reported to public health authorities . Clarifying pediatric SARS-CoV-2 prevalence is important because it is well established that children can transmit SARS-CoV-2 to other children and adults . In addition, children are at risk for severe complications, including postinfectious multisystem inflammatory syndrome in children (MIS-C) . Seroprevalence provides additional insight into the true cumulative incidence of COVID-19 in children.\n\n【1】### The Study\n\n【2】To estimate the seroprevalence of COVID-19 infection in children in Oregon, blood was collected in 6 phases during November 1, 2020–June 30, 2022, from a cross-sectional convenience sample and tested for SARS-CoV-2 nucleocapsid IgG, in alignment with the World Health Organization seroepidemiologic investigation protocol . We recruited Oregon healthcare facilities with \\> 6 inpatient pediatric hospital beds to participate in this study and asked them to provide < 100 specimens per phase; 5 facilities agreed to participate. We asked facilities to submit random samples of deidentified residual serum samples from patients < 17 years of age visiting any ambulatory, emergency, or inpatient healthcare setting and to include specimen collection date and patient’s date of birth. The initial round of sampling was November 1–December 31, 2020. We extended the project timeline and collected additional samples bimonthly during October 1, 2021–June 30, 2022.\n\n【3】Specimens were stored according to instructions provided by the test manufacturer and transported to the Oregon State Public Health Laboratory (Hillsboro, Oregon, USA). We tested the specimens with a SARS-CoV-2 IgG assay , which detects antibodies to the nucleocapsid protein of SARS-CoV-2. Nucleocapsid IgG immunoassays detect antibodies produced after infection and do not detect antibodies produced after vaccination with vaccines licensed for use in the United States. The manufacturer reports test sensitivity (Sn) of 100% (% CI 95.9%–100%) at \\> 14 days past symptom onset and specificity (Sp) of 99.6% (% CI 99.1%–99.9%). We calculated unadjusted seroprevalence estimates for each collection period as the percentage of all specimens that tested positive. We adjusted seroprevalence estimates for test performance as observed prevalence + (specificity – 1) divided by sensitivity + (specificity – 1) .\n\n【4】We obtained adjusted 95% CI with parametric bootstrapping . Because SARS-CoV-2 antibody detection is dependent on test timing and assay, we also performed sensitivity analysis with seroprevalence estimates adjusted for declining assay sensitivity over 130 days of convalescence   We used R version 4.1.2  for all analyses.\n\n【5】We collected 1,869 specimens from 5 facilities during 6 phases. The mean number of specimens collected during each phase was 312 (range 215–438). Overall, we observed a strong linear trend (p = 0.001) for adjusted seroprevalence estimates during November 1, 2020—December 31, 2021; seroprevalence increased by ≈0.7% per 4-week period during that period . After the Omicron surge, adjusted seroprevalence increased sharply from 13.4% (% CI 9.8%–18.4%) in December 2021 to 38.8% (% CI 32.8%–46.5%) in February 2022. Adjusted seroprevalence estimates then decreased but did not return to pre–February 2022 levels . Adjusting for declining assay sensitivity over time led to estimates that were larger and less stable over convalescence .\n\n【6】### Conclusions\n\n【7】Our repeated cross-sectional study estimated pediatric seroprevalence in Oregon at 6 points across 20 months of pandemic response. During that period, K–12 public schools reopened statewide, vaccines were rolled out in 2 phases to children 12–15 and 5–11 years of age, and universal indoor masking mandates remained in place during the school year until March 12, 2022 . After widespread school reopening in early 2021, with a masking mandate in place, pediatric seroprevalence in October 2021 was 11.8%, and no increase in trend was observed. The only sudden increase in pediatric seroprevalence followed the Omicron surge in early 2022. Seroprevalence began to decline after its February 2022 peak but did not return to pre-Omicron levels.\n\n【8】Seroprevalence can provide more accurate estimates of the true cumulative incidence of SARS-CoV-2 infection than case reporting to public health entities does, because seroprevalence data capture evidence of previous infection in persons who are not tested through the traditional healthcare system because of asymptomatic or mild disease, lack of testing access, refusal to test, or self-testing at home . We estimated 1.7–2.8 times the number of infections in children from seroprevalence than the reported cumulative incidence in Oregon . This total is a lower degree of underascertainment than had been reported in seroprevalence studies of children during May–July 2021 in the United States .\n\n【9】Because seroprevalence studies measure circulating antibodies at the time of testing and SARS-CoV-2 antibodies wane over time, seroprevalence is limited in its ability to estimate cumulative incidence as the pandemic progresses . In addition, time to seroreversion is dependent on the target antigen and the assay used . One study found a mean time of seroreversion of 19 weeks with use of the Abbott IgG immunoassay, compared with 91 weeks using the Roche pan-Ig immunoassay . In our study, waning immunity was apparent as estimated seroprevalence decreased following its Omicron-related peak in February 2022. A sensitivity analysis, adjusting for declining sensitivity of the Abbott immunoassay, partially accounted for this decline; more complex models have been published to correct seroprevalence estimates for assay performance . However, now that essentially all persons in the United States have been infected with SARS-CoV-2, the use of an assay with waning sensitivity to remote infections may permit continued examination of more granular temporal changes in seroprevalence in the context of changing policy and variant predominance.\n\n【10】The Centers for Disease Control and Prevention began collecting pediatric seroprevalence data for the Multistate Assessment of SARS-CoV-2 Seroprevalence in Commercial Labs (MASS-C) in July 2020 . MASS-C estimates of pediatric seroprevalence in Oregon are consistently higher than our estimates . Although MASS-C similarly derives its estimates from a convenience sample of serum specimens, testing is performed with the Roche antinucleocapsid total antibody assay . In addition, MASS-C estimates are weighted for age and sex, and the demographics of that pediatric population may demographically differ from our study population.\n\n【11】We obtained our convenience sample from children who received care from large pediatric healthcare facilities throughout the state; findings are not necessarily generalizable to the entire state pediatric population. Limitations of seroprevalence testing include lack of antibody development by some infected persons (including immunocompromised persons) and, in others, waning of antibodies to undetectable levels, such that seroprevalence becomes a less reliable proxy for cumulative incidence as the duration of the pandemic increases .\n\n【12】Traditional public health case-based reporting substantially underestimates the burden of COVID-19. In this study, seroprevalence estimates made using an assay with waning sensitivity to remote infections provided evidence that the widespread reopening of schools with a masking mandate in place did not increase the rate of pediatric SARS-CoV-2 infections. Case-based cumulative incidence estimates failed to capture the magnitude of the Omicron variant’s effect on Oregon’s pediatric population. Serosurveillance of SARS-CoV-2 antibodies in Oregon’s pediatric population complements case-based surveillance and can inform future public health interventions and policy decisions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7a83e3da-a272-4ddd-adf0-2c718d182cbb", "title": "Youth Risk Behavior Surveillance System (YRBSS)", "text": "【0】### 2021 YRBS Results Now Available\n\n【1】The Youth Risk Behavior Surveillance System (YRBSS) is a set of surveys that track behaviors that can lead to poor health in students grades 9 through 12. Learn more about YRBSS .\n\n【2】2021 YRBSS Data Highlights\n--------------------------\n\n【3】YRBSS Results\n\n【4】View 2021 YRBS national, state, and local school district results.\n\n【5】YRBSS Data & Documentation\n\n【6】Access 2021 YRBS national, state and local school district data.\n\n【7】YRBS Data Summary & Trends\n\n【8】View data on health behaviors and experiences among U.S. high school students.\n\n【9】*   2021 MMWR Reports\n*   2021 Supplementary Tables\n*   Results Toolkit\n*   CDC Feature\n\n【10】### More on YRBSS\n\n【11】*   Overview\n\n【12】*   Frequently Asked Questions\n\n【13】*   Methods\n\n【14】*   Participation Maps & History\n\n【15】*   Questionnaires\n\n【16】*   Why YRBS?\n\n【17】### Data Analysis Tools\n\n【18】Youth Online Data Analysis Tool\n\n【19】Analyze national, state, and local YRBSS data from high school and middle school surveys\n\n【20】YRBS Explorer\n\n【21】Explore national state, and local school district data via tables and graphs\n\n【22】YRBS Analysis Tool\n\n【23】Provide users with easy access to data on health-risk behaviors", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f3cded20-d2f7-44b0-93d1-349907611b2d", "title": "Distribution of Japanese Encephalitis Virus, Japan and Southeast Asia, 2016–2018", "text": "【0】Distribution of Japanese Encephalitis Virus, Japan and Southeast Asia, 2016–2018\nThe locations of epidemics of arthropodborne viruses (arboviruses) are strongly associated with the distribution of their vectors. In general, the distribution of arboviruses can expand through the dispersal, transfer, and migration of their vector arthropods and reservoir animals. Mosquitoes transmit a variety of viral pathogens (e.g. dengue, Zika, and chikungunya viruses) and have caused a number of arboviral epidemics throughout the world . Japanese encephalitis virus (JEV; family _Flaviviridae_ , genus _Flavivirus_ ) is a mosquitoborne arbovirus that causes a severe form of encephalitis in humans. JEV is distributed across most of Asia, the western Pacific, and northern Australia . The World Health Organization has estimated that the annual number of Japanese encephalitis cases worldwide exceeds 60,000 . JEV is transmitted primarily by mosquitoes of the _Culex vishnui_ subgroup, principally _Cx_ . _tritaeniorhynchus_ Giles; pigs and wading ardeid birds, such as egrets and herons, are known to be the major amplifying hosts .\n\n【1】On the basis of their genome sequences, JEVs are classified into 5 genotypes . JEV genotype I (GI), which has been further classified into subgenotypes GIa and GIb, and JEV GIII are the dominant lineages and have been detected widely throughout Asia. JEV GII is the thirdmost common lineage and has been found in Indonesia, Singapore, South Korea, Malaysia, and Australia. JEV GIV and GV are rare lineages; only a few viruses of these genotypes have been isolated from Indonesia, Malaysia, and China as of October 2019. Over the past 30 years, JEV GIa has displaced GIII as the dominant lineage in many countries of Asia . Although the origin and spreading pattern of JEV genotypes across the world have been investigated in some reports , the exact mechanisms of JEV genotype shift remain unclear.\n\n【2】### The Study\n\n【3】To study the epidemiology of arbovirus infection, we, an international team of researchers in Japan, Thailand, the Philippines, and Indonesia, conducted arbovirus surveillance in our respective countries during 2016–2018 with the support of our governments. In each country, we collected mosquitoes in and around cattle or pig housing using sweeping nets and aspirators. We collected mosquitoes that had digested blood in their midguts. We identified their species and sorted them into pools, which we used for virus isolation. We also collected serum samples from pigs and wild boars from each country to use for virus isolation.\n\n【4】We passed homogenized mosquito or serum samples through 0.45-μm filters  and inoculated filtrates onto monolayers of 3 culture cell lines (mammalian cell lines Vero9013 and BHK-21 and mosquito cell line C6/36). We assessed cytopathic effect (CPE) daily and collected supernatants from cells that exhibited CPE. If we observed no CPE, we passaged the cells 5 times for 7 days each, after which, if virus was present, CPE should have become apparent. We extracted RNA from culture supernatants using the QIAamp Viral RNA Mini Kit  and subjected the resulting RNA to reverse transcription PCR (RT-PCR) using the QIAGEN One-Step RT-PCR Kit and 2 universal flavivirus-specific primer sets (MAMD and cFD2 or FU2 and cFD3)  to screen for flaviviruses. To determine genome sequences, we used the QIAGEN One-Step RT-PCR Kit, TaKaRa LA RT-PCR Kit version 2.1 , and Invitrogen 5′ RACE System for Rapid Amplification of cDNA Ends version 2.0  as needed in combination with several JEV-specific primers .\n\n【5】Of 945 pig serum samples, we selected 56 candidate samples for virus isolation on the basis of their RT-PCR results with the MAMD and cFD2 primers, and from these samples, we obtained the full or partial genome sequences of 5 JEV isolates . Out of a total of 22,277 mosquitoes comprising \\> 16 species, we obtained the full or partial genome sequences of only 2 JEV isolates . Overall, we obtained the full-genome sequence of 4 of the 7 JEV isolates and the partial genome sequence (envelope gene) of the remaining 3 isolates (DDBJ accession nos. LC461956–62).\n\n【6】A preliminary study we performed showed that many pigs in these countries possessed antibodies against JEV (K. Maeda, unpub. data). We found that JEV was more often isolated from serum samples from JEV antibody–negative pigs in farms where JEV seroprevalence was low. Because JEV isolation seems to be difficult in endemic regions, we suggest selecting younger pigs, which are less likely to be JEV antibody positive, for virus isolation studies to increase the chances of success.\n\n【7】The 2 JEV isolates we recovered in Japan, JEV/MQ/Yamaguchi/803/2016 and JEV/MQ/Yamaguchi/804/2016, were GIa and not GIII. In a phylogenetic analysis, these viruses clustered with JEV isolates recovered from Japan (in 2013 and 2009), China (in 2007), and South Korea (in 2010) . The 2 viruses collected in Japan in 2016 were most closely related to viruses recovered in 2013 from the same site that had been sampled in our previous study , suggesting that this strain has been maintained in the Yamaguchi area of Japan since at least 2013.\n\n【8】The JEV isolate we obtained from Thailand, JEV/sw/Thailand/185/2017, was GIb and clustered with other JEV isolates detected in Thailand during 1985–2005 , as well as isolates from Myanmar in 2010, Cambodia in 2014 and 2015, and Singapore in 2014 . Thus, these JEVs have been maintained in Thailand and other parts of the southern peninsula of continental Asia for >30 years.\n\n【9】JEV is distributed extensively throughout the Philippines . However, only 3 JEV GIII isolates from the Philippines (which were obtained from pigs during 1984–1986) were available for genetic analysis. Our 2 Philippines-derived JEV isolates, JEV/sw/Mindanao/K3/2018 and JEV/sw/Mindanao/K4/2018, clustered with these isolates . Our 2 Indonesia JEV isolates, JEV/sw/Bali/93/2017 and JEV/sw/Bali/94/2017, were obtained from the island of Bali, where a JEV vaccination program began in March 2018 . These JEV isolates clustered with other Indonesia isolates obtained in the 1980s  .\n\n【10】### Conclusions\n\n【11】In summary, our phylogenetic analysis revealed that the JEV isolates we obtained from Japan were GIa, the isolate from Thailand was GIb, the isolates from the Philippines were GIII, and the isolates from Indonesia were GIV . These results indicated that JEV GIII and GIV are still active and being maintained in parts of Asia.\n\n【12】Our data demonstrate that a number of the JEV isolates we obtained in select countries of Southeast Asia during 2016–2018 were phylogenetically related to isolates reported in the same country in the 1980s, suggesting that some JEV strains have been maintained in their corresponding regions. Contrary to our expectation, the JEV transmission cycle seems to have been maintained indigenously. JEV strains are presumed to be transferred between JEV-endemic regions by movement of arthropod vectors and bird reservoirs. Nonetheless, we infer that fixation of an invading JEV strain into a new region is difficult unless the new strain possesses properties advantageous for virus growth and expansion . However, the genotype shift from GIII to GIa has occurred in East Asia since the 1990s, indicating that GIa must have had some sort of growth advantage over GIII that permitted its spreading to and expansion in these countries. Our findings that JEV strain invasion in Asia is infrequent could assist in public health decision-making regarding vaccine formulation and campaign strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "94a09f14-2bc9-4697-96d1-7eade709bdb5", "title": "Recurring Methicillin-resistant Staphylococcus aureus Infections in a Football Team", "text": "【0】Recurring Methicillin-resistant Staphylococcus aureus Infections in a Football Team\nFootball-related skin infections have gained national notoriety and public interest . Media coverage of high-profile athletes and teams with skin and soft tissue infection (SSTI) has provided more impetus for research of these infections. Annually, 60,000 college football players compete among 600 teams . The community-associated methicillin resistant _Staphylococcus aureus_ (CA-MRSA) strains have been a cause of SSTI outbreaks among athletes participating in football, wrestling, rugby, soccer, fencing and canoeing . SSTIs (pustules, “insect bites,” boils, and abscesses) are the hallmarks of CA-MRSA infections . CA-MRSA causes disease in young, otherwise healthy persons without the usual risk factors for MRSA infections . In addition, CA-MRSA has unique molecular markers (SCC _mec_ IV and Panton-Valentine leukocidin) and fewer resistance genes to non β-lactam antimicrobial drugs than healthcare-associated MRSA strains .\n\n【1】In August 2002, the Los Angeles County Department of Health Services (LACDHS) received reports of 2 college football players (players X and Y) on team A hospitalized for SSTIs due to MRSA, which was later identified as a community-associated strain (USA300) . No other MRSA SSTI was reported on team A until 1 year later. On August 25, 2003, an infectious disease physician notified LACDHS of the hospitalizations of 4 different players on team A with MRSA SSTIs. Despite the lack of background SSTI data on this team, the recurrence of infections prompted an investigation with objectives of identifying players with MRSA SSTIs and nasal carriage, conducting epidemiologic studies, implementing outbreak-control measures, and determining the genotype of the outbreak strain.\n\n【2】Team A was a college football program with 107 players on the roster at the time of the outbreak. The team practiced and played 11 of their 13 games on grass fields. Players began their football season with training camp from August 5 to 18, 2003. In camp, players were sequestered and lived together, in suites of 4 per dormitory, to foster camaraderie among teammates. Rigorous practices were held twice daily in the hot, summer weather.\n\n【3】### Methods\n\n【4】##### Case Finding\n\n【5】Case-players were defined as team A members with MRSA culture-confirmed SSTIs or SSTIs presumably caused by the USA300 strain in the outbreak period August 5 to September 5, 2003. Because we suspected that disease exposure occurred during camp, we chose the study period from the start of training camp to ≈2 weeks after the end. Our experience with other SSTI outbreaks found that in most persons lesions develop within 2 weeks postexposure to CA-MRSA. To find case-players, we reviewed the trainer’s treatment log to identify players with skin lesions who required medical or surgical interventions. We asked the staff to conduct skin inspections of all players. Players were encouraged to report any skin lesion. In addition, we queried the student health center to determine if these infections were prevalent on campus.\n\n【6】##### Nasal Carriage Study\n\n【7】As soon as the current outbreak was recognized, a returning player (player X) was suspected to be the source of infection. Player X had 1 of the 2 cases of CA-MRSA SSTIs discovered in 2002. His locker was directly across from the index case-player, and he was a roommate, during camp, of another case-player. Trainers obtained a nasal culture from player X on August 25. On September 3, trainers obtained cultures from the anterior nares of 99 available team members for a nasal carriage study.\n\n【8】##### Laboratory Study\n\n【9】MRSA isolates from case-players and nasal carriers were characterized by using pulsed-field gel electrophoresis (PFGE) with the _Sma_ I and _Eag_ I restriction enzymes . PFGE patterns of the isolates were compared with the USA300 strain responsible for other SSTI outbreaks in Los Angeles County . This strain was previously determined to contain SCC _mec_ IV by the Centers for Disease Control and Prevention . We also characterized a sample of methicillin-susceptible _Staphylococcus aureus_ (MSSA) isolates from players’ nasal cultures.\n\n【10】##### Case-Control and Carrier-Control Studies\n\n【11】On the basis of anecdotal reports of players sleeping in the locker room on used towels and delaying treatment of cuts and abrasions, we hypothesized that poor hygiene habits and compromised skin integrity might predispose players to infection. We designed a standard questionnaire to collect data on player demographics, living situation, football activities, exposure to persons with skin infections, hygiene practices, histories of skin lesions, and clinical symptoms. Trained health department employees administered the questionnaires in person.\n\n【12】We conducted unmatched case-control and carrier-control studies. Controls were selected by jersey numbers, by using a random-number generator, from asymptomatic teammates without nasal carriage of MRSA. Teammates with positive nasal cultures for MRSA were considered carriers. Carrier-players were defined as carriers with matching PFGE pattern to the USA300 strain. We excluded non-USA300 strains carriers to remove players who might represent the background prevalence of MRSA in the community. Players who were not available for interviews were not included. Bivariate analysis was completed by using Fisher exact test in Epi Info version 3.3 (CDC, Atlanta, GA, USA). Statistical significance was defined as p values <0.05. Because of the small sample size and zero-valued cells, similar risk factors from the bivariate analysis were grouped into categories. Multivariate analysis was completed by using the conditional exact test in SAS version 8 (SAS Institute Inc. Cary, NC, USA).\n\n【13】##### Outbreak Control Interventions\n\n【14】Upon recognition of the outbreak on August 25, team A instituted daily hexachlorophene showers for all players, increased the frequency of cleaning the facilities and athletic gear, disinfected the whirlpool tubs, provided more towels, and posted hand-hygiene signs in the locker room. Once nasal culture results were available, team physicians attempted to decolonize carriers with intranasal mupirocin . We recommended improving the timeliness of wound care, barring case-players from playing unless wounds were covered, discouraging the sharing of personal items and tubs, prohibiting sleeping in the locker room, and checking laundry procedures. We also disseminated CA-MRSA educational materials to staff and team members .\n\n【15】### Results\n\n【16】##### Characteristics of Case-players\n\n【17】We identified 11 case-players out of 107 team members for an attack rate of 10%. Cases were diagnosed during or within 2 weeks of the end of training camp . The first case was diagnosed on August 15, the last on September 1. With 1 exception, infections occurred before the first scheduled game on August 30. The most common sign was a boil . The elbow was the most common body site infected. No infection was at a current site of skin trauma or occurred at >1 body location simultaneously. Before hospitalization, the index and second case-players were given cephalexin and levofloxacin, respectively, for their infections without any clinical improvement. In total, 4 case-players were hospitalized and treated with parenteral vancomycin. Subsequent nonhospitalized case-players were treated with doxycycline and rifampin. Lesions of 9 players required surgical incision and drainage. All case-players ultimately responded to treatment with resolution of their infections. The median age of case-players was 20 years, with a median tenure of 2 years on team A. Linemen had the highest attack rate (%) among all field positions . No quarterbacks, wide receivers, or special team players (kickers, punters) were affected. All were healthy men without underlying illnesses. Eight (%) case-players interviewed reported having never worn elbow pads, and 6 (%) usually did not have cuts or abrasions covered until >1 hour postinjury.\n\n【18】##### Characteristics of Carriers\n\n【19】Nasal cultures were obtained from 99 (%) of 107 team members. Twenty-six (%) cultures were positive for _Staphylococcus aureus_ , among which 8 (%) were positive for MRSA, including player X. Player Y’s nasal culture was negative. The median age of carriers was 20 years (range 18–21 years), and median tenure on the team was 2.5 years (range 1–5 years). MRSA carriage was highest in linemen (%). We identified 1 case-player with nasal carriage of MRSA. However, trainers obtained nasal cultures after all case-players had begun antimicrobial treatment. Locker room assignments showed clustering of case-players and carrier-players, notably the proximity of the potential source player (player X) to the index case-player . Among MSSA carriers (n = 18), no clustering of locker locations was seen. MSSA carriage was highest among linemen (%) and cornerbacks/safeties (%).\n\n【20】##### Laboratory Results\n\n【21】Four (%) of seven MRSA isolates from culture-confirmed case-players were available for PFGE analysis. All were indistinguishable from each other, the USA300 strain found in Los Angeles County, and the isolates from 2 cases (players X and Y) in 2002. We denoted this genotype as strain A. Of 6 (%) available MRSA isolates from 8 carriers, 4 (%) were indistinguishable from strain A. Two carriers had unique MRSA genotypes (strains B and C) with \\> 7 bands difference between them and between strain A. Strains A, B, and C, player X and Y’s isolates, demonstrate community-associated antimicrobial susceptibility phenotypes . Among 5 MSSA isolates characterized, all had \\> 7 bands difference among themselves as well as from the USA300 strain.\n\n【22】##### Case-control and Carrier-control Study Results\n\n【23】Ten of 11 case-players were enrolled in the study; 1 was unavailable for interview. During camp, case-players were 15 times more likely than controls to have shared bars of soap with teammates and more likely to have had preexisting cuts or abrasions .\n\n【24】Five of 6 carrier-players were available for interviews. Carrier-players were 60 times more likely than controls to have had a locker adjacent to or across from a teammate with an SSTI and 47 times more likely to have shared towels with teammates . Carrier-players were more likely than controls to lived on campus in a dormitory or fraternity house. Among carrier-players and controls, players who lived on campus had a higher mean number of roommates than those who lived in off-campus apartments (.3 vs. 1.5, p = 0.046).\n\n【25】Potential risk factors were grouped into 3 categories: “sharing” (sharing soap/towels with teammates), “skin injury” (cuts, abrasions), and “close contact” (locker adjacent to case-players, living on-campus). Multivariate analysis including these categories indicated that sharing was a significant risk factor for CA-MRSA infection (OR 12.1, 95% CI 1.83–108, p = 0.006) and carriage (OR 17.4, 95% CI 1.03–undefined, p = 0.047).\n\n【26】##### Postintervention Surveillance\n\n【27】Daily hexachlorophene showers were in use from August 25 to September 19. No new infections were reported during the 4 weeks after the discontinuation of the hexachlorophene showers. From October 20 to November 9, MRSA SSTI developed in 4 players: a lineman with a chin abscess, a linebacker (player Y from 2002) with an elbow boil, a quarterback (player Z) with folliculitis on a leg, and a tight end with a gluteal boil. Three MRSA isolates (except from the tight end) were available for PFGE; all matched strain A. The lineman in this cluster shared bars of soap with his roommate, a case-player.\n\n【28】Because of ongoing disease transmission and to identify potential reservoirs of MRSA, all 28 staff and student trainers and managers were nasally cultured on November 3; 11 (%) were positive for MSSA. None was positive for MRSA. On November 22, we observed an official game. Previously unidentified lapses in hygiene practices occurred on the sidelines. We observed that student trainers reused hand towels between players, and players shared towels among themselves. Subsequently, the team switched to single-use towels on the sidelines. No new infections were reported for the remainder of the 2003 season. In the following season (August–December 2004), no MRSA SSTI outbreak occurred on team A. However, player Z had a recurrence of MRSA pustules on the forearm and leg in October 2004. He responded to outpatient treatment with doxycycline, rifampin, and incision and drainage of the lesions. His MRSA isolate was not available for PFGE. Throughout the last 3 football seasons, we received no reports of SSTI outbreaks among opposing athletes after playing this team.\n\n【29】### Discussion\n\n【30】This report is the first of recurring CA-MRSA SSTIs in a football team during consecutive seasons. From 2 cases in 2002 to an outbreak involving 11 players in 2003 and then 1 case in 2004, we have shown that eradicating these infections is difficult once they become established in a football team. Infections were likely propagated year to year from previously infected players, and they appear to be susceptible to recurring colonization and infection themselves.\n\n【31】Consistent with other reports, our findings implicate sharing personal items and improper wound care as risk factors for CA-MRSA infections . While the concept is counterintuitive, soap sharing was also associated with MRSA infections in a prison outbreak . Therefore, teams should consider switching to liquid soaps in an outbreak situation and always provide prompt wound care.\n\n【32】Linemen were identified as a high-risk subgroup. They engage in frequent and aggressive skin-to-skin contact during games, similar to hand-to-hand combat maneuvers as reported in a military MRSA outbreak . In addition, linemen tend to be physically larger than their teammates. Increased body mass index and lineman position were risk factors for CA-MRSA infection in another football team outbreak .\n\n【33】Two recent reported CA-MRSA outbreaks in football teams detected no nasal carriage in their combined cohort of 182 football players . In contrast, we document a high MRSA nasal carriage rate (%) among team A players even while hexachlorophene showers were provided. The actual carriage rate might be higher, since we obtained nasal cultures after all case-players had begun antimicrobial treatment. Additional case-players may have been carriers as well, but they may have been decolonized before culture. Further research is needed to study the association between nasal carriage of CA-MRSA and SSTI to develop decolonization guidelines. The data facilitated a carrier-control study. Similar to risk factors for infection, nasal acquisition of CA-MRSA is associated with sharing personal items, particularly in the locker room.\n\n【34】Crowded living conditions during training camp appear to facilitate the acquisition of CA-MRSA, which then propagates in on-campus housing. Investigators of an outbreak among military recruits found an association between having a roommate with an SSTI and MRSA infection . Consequently, players’ living arrangements should be as dispersed as possible.\n\n【35】Unique to our investigation are 1 confirmed and 2 presumed community-associated strains of MRSA. We presented laboratory results indicating that the outbreak strain was likely the USA300 genotype. Since we do not have PFGE results from 6 case-players, different strains could have caused those infections. However, a multiclonal outbreak is unlikely, since other MRSA SSTI outbreaks in Los Angeles County among soccer players, men who have sex with men, jail inmates, and newborns have been exclusively due to the USA300 strain (Los Angeles County Department of Health Services, unpub. data). In contrast, our limited data do not suggest a clonal spread of MSSA on this team. Multilocus sequence typing was not available locally, which prevented further characterization of the isolates.\n\n【36】Selection bias of case-players and controls is a limitation of this study. Enrollment of players with uncultured infections and those without PFGE results introduces the possibility of misdiagnosis and misclassification. Most football teams assign jersey numbers on the basis of field position. Therefore, our control selection method might not have captured a representative sample of the team. However, the distribution of field positions among controls and the entire team appears similar . The small sample size produces less precise (wide confidence intervals) results and prohibits more in-depth multivariate analyses. Reporting bias is possible, since players and the team fear negative publicity, and we do not have data on risk factors during the off-season. In order to maintain confidentiality, we were unable to interview several players because of high media scrutiny.\n\n【37】As CA-MRSA strains become more prevalent in the community , SSTIs will likely continue to afflict football players. Despite comprehensive infection control interventions, sporadic cases of MRSA SSTIs continue to occur on this team. However, a recurrent outbreak was averted in the latest season likely because of increased vigilance to proper hygiene practices and awareness of this disease among the staff and players.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c0e46800-8d7a-4a5f-83cc-a546812d169e", "title": "Global Public Health Surveillance under New International Health Regulations", "text": "【0】Global Public Health Surveillance under New International Health Regulations\nOn May 23, 2005, the World Health Assembly adopted the new International Health Regulations (IHR 2005)  as an international treaty. This step concluded the decade-long effort led by the World Health Organization (WHO) to revise the old regulations (IHR 1969) to make them more effective against global disease threats. Originally adopted in 1951  and last substantially changed in 1969 , IHR 1969 had lost its effectiveness and relevance by the mid-1990s, if not earlier .\n\n【1】The resurgence of infectious diseases noted in the first half of the 1990s showed IHR 1969's limitations. For example, after smallpox was eradicated in the late 1970s, IHR 1969 only applied to the traditionally \"quarantinable\" diseases of cholera, plague, and yellow fever. In addition, IHR 1969 restricted surveillance to information provided only by governments, lacked mechanisms for swiftly assessing and investigating public health risks, contained no strategies for developing surveillance capacities and infrastructure, and failed to generate compliance by WHO member states. WHO began revising IHR 1969 in 1995 , and IHR 2005's adoption completed the modernization of this important body of international law on public health.\n\n【2】IHR 2005 departs radically from IHR 1969 and represents a historic development in international law on public health . IHR 2005 expands the scope of the regulations' application, strengthens WHO's authority in surveillance and response, contains more demanding surveillance and response obligations, and applies human rights principles to public health interventions. The most dramatic of these changes involves a new surveillance system that far surpasses what the IHR 1969 contained. After reviewing key surveillance concepts and frameworks, this article describes IHR 2005's surveillance regime and assesses its likely performance. It concludes by discussing obstacles that could prevent IHR 2005 from becoming an effective global public health surveillance system and addressing how these obstacles might be overcome.\n\n【3】### Key Surveillance Concepts and Evaluation Framework\n\n【4】Public health surveillance has been defined as \"the ongoing systematic collection, analysis, and interpretation of outcome-specific data for use in the planning, implementation, and evaluation of public health practice\" . A surveillance system requires structures and processes to support these ongoing functions .\n\n【5】The Centers for Disease Control and Prevention (CDC) developed guidelines that identify the essential elements and attributes for an effective public health surveillance system . According to these guidelines, evaluating surveillance systems involves 2 main steps: 1) describing the purpose, operation, and elements of the system and 2) assessing its performance according to key attributes. This article uses this 2-step approach to evaluate the global public health surveillance system prescribed by IHR 2005.\n\n【6】### Surveillance System Specified in IHR\n\n【7】In the CDC framework, describing a surveillance system includes 4 main elements: 1) health-related events under surveillance and their public health importance, 2) purpose and objectives of the system, 3) components and processes of the system, and 4) resources needed to operate it .\n\n【8】##### Health-related Events under Surveillance\n\n【9】IHR 2005 identifies health-related events that each country that agrees to be bound by the regulations (a \"state party\") must report to WHO. In terms of health-related events that occur in its territory, a state party must notify WHO of \"all events which may constitute a public health emergency of international concern\" (article 6.1). These events include any unexpected or unusual public health event regardless of its origin or source (article 7). IHR 2005 also requires state parties, as far as is practicable, to inform WHO of public health risks identified outside their territories that may cause international disease spread, as manifested by exported or imported human cases, vectors that may carry infection or contamination, or contaminated goods (article 9.2).\n\n【10】IHR 2005 provides guidance to assist state parties' compliance with these obligations in 4 ways. First, IHR 2005 defines a \"public health emergency of international concern\" (PHEIC) as \"an extraordinary event which is determined \\[by the WHO Director-General\\]… (i) to constitute a public health risk to other States through the international spread of disease and (ii) to potentially require a coordinated international response\" (article 1.1). Unlike IHR 1969's limited scope of application to just 3 communicable diseases , IHR 2005 defines disease as an illness or medical condition that does or could threaten human health regardless of its source or origin (article 1.1). This scope therefore encompasses communicable and noncommunicable disease events, whether naturally occurring, accidentally caused, or intentionally created.\n\n【11】Second, IHR 2005 contains a \"decision instrument\" (annex 2) that helps state parties identify whether a health-related event may constitute a PHEIC and therefore requires formal notification to WHO . The decision instrument focuses on risk assessment criteria of public health importance, including the seriousness of the public health impact and the likelihood of international spread.\n\n【12】Third, IHR 2005 includes a list of diseases for which a single case may constitute a PHEIC and must be reported to WHO immediately. This list consists of smallpox, poliomyelitis, human influenza caused by new subtypes, and severe acute respiratory syndrome (SARS). A second list of diseases exists  for which a single case requires the decision instrument to be used to assess the event, but notification is determined by the assessment and is not automatic. Finally, IHR 2005 also encourages state parties to consult with WHO over events that do not meet the criteria for formal notification but may still be of public health relevance (article 8).\n\n【13】IHR 2005's expansion of the range of public health events under surveillance and the use of risk assessment criteria in deciding what is reportable is possibly the single most important surveillance advance in IHR 2005. This change greatly enhances effective surveillance of emerging infectious diseases, which are \"infections that have newly appeared in a population or have existed but are rapidly increasing in incidence or geographic range\" . IHR 2005's surveillance strategy, especially the decision instrument, has been specifically designed to make IHR 2005 directly applicable to emerging infectious disease events, which are usually unexpected and often threaten to spread internationally.\n\n【14】In addition to events that may constitute a PHEIC, IHR 2005 also requires state parties to report the health measures (e.g. border screening, quarantine) that they implement in response to such events (article 6). State parties are also specifically required to inform WHO within 48 hours of implementing additional health measures that interfere with international trade and travel, unless the WHO Director-General has recommended such measures (article 43).\n\n【15】##### Purpose and Objectives of Surveillance under IHR\n\n【16】IHR 2005's purpose is to prevent, protect against, control, and facilitate public health responses to the international spread of disease (article 2), and IHR 2005 makes surveillance central to guiding effective public health action against cross-border disease threats. The regulations define surveillance as \"the systematic ongoing collection, collation and analysis of data for public health purposes and the timely dissemination of public health information for assessment and public health response as necessary\" (article 1.1). Surveillance is central to IHR 2005's public health objectives, which explains why IHR 2005 requires all state parties to develop, strengthen, and maintain core surveillance capacities (article 5.1). This obligation goes beyond anything concerning surveillance in IHR 1969, which did not address surveillance infrastructure and capabilities beyond a general requirement for a state party to notify WHO of any outbreak of a disease subject to the regulations.\n\n【17】##### Components and Processes of IHR 2005 Surveillance\n\n【18】IHR 2005 describes key aspects of the surveillance process from the local to the global level. As part of IHR 2005's core surveillance and response capacity requirements, each state party has to develop and maintain capabilities to detect, assess, and report disease events at the local, intermediate, and national levels (article 5.1, annex 1). Officials at the national level must be able to report through the national IHR focal point to WHO when required under IHR 2005 (articles 4.2 and 6). The regulations also mandate that WHO establish IHR contact points that are always accessible to state parties (article 4.3). Connecting these levels produces the surveillance architecture illustrated in Figure 2 .\n\n【19】Requiring that a national IHR focal point be established is another surveillance initiative in IHR 2005. The focal point is designed to facilitate rapid sharing of surveillance information because it is responsible for communicating with the WHO IHR contact points and disseminating information within the state party (article 4.2). By linking national IHR focal points through WHO, IHR 2005 establishes a global network that improves the real-time flow of surveillance information from the local to the global level and also between state parties (article 4.4).\n\n【20】##### Resources Needed to Operate IHR 2005's Surveillance System\n\n【21】Building and maintaining the surveillance system envisioned in IHR 2005 will require substantial financial and technical resources. State parties will be primarily responsible for providing resources needed to develop their core surveillance capacities. Each state party has to assess its ability to meet the core surveillance requirements by June 2009. In addition, each state party has to develop and implement a plan for ensuring compliance with core surveillance obligations (articles 5.1 and 5.2, annex 1).\n\n【22】WHO is obliged to assist state parties in meeting their surveillance system obligations (article 5.3), but this provision does not allocate any WHO funds for this purpose. State parties are required to collaborate with each other in providing technical cooperation and logistical support for surveillance capabilities and in mobilizing financial resources to facilitate implementation of IHR 2005 (article 44.1).\n\n【23】### Evaluating the IHR 2005 Surveillance System's Attributes and Potential Performance\n\n【24】Key attributes of effective surveillance systems identified by CDC are usefulness, sensitivity, timeliness, stability, simplicity, flexibility, acceptability, data quality, positive predictive value, and representativeness. Of these attributes, usefulness, sensitivity, timeliness, and stability will be most critical to the success of the IHR 2005 surveillance system. Simplicity, acceptability, and flexibility will affect the establishment and sustainability of the surveillance system. Data quality, positive predictive value, and representativeness are central to accurately characterizing health-related events under surveillance. Table 1 summarizes these attributes, provides commentary on their relevance to effective surveillance under IHR 2005, and assesses the likely performance of the IHR 2005 surveillance system for each attribute. The following paragraphs concentrate on assessing IHR 2005 with respect to the key attributes of usefulness, sensitivity, timeliness, and stability.\n\n【25】##### Usefulness of the Surveillance System\n\n【26】The central premise of IHR 2005 is that rapidly detecting PHEIC will support improved disease prevention and control both within and between state parties. Ample evidence shows that delayed recognition and response to emerging diseases may result in adverse consequences in terms of illness and death, spread to other countries, and disruption of trade and travel . The usefulness of surveillance under IHR 2005 represents the sum of all the critical system attributes and can only be assessed after the system is in operation, so this attribute is not discussed here. However, for the future sustainability and development of IHR 2005, we must evaluate its overall usefulness and document its contribution to prevention and control of adverse health events. IHR includes mechanisms to review and, if necessary, amend its provisions and in particular requires periodic evaluation of the functioning of the decision instrument (article 54).\n\n【27】##### Sensitivity of the Surveillance System\n\n【28】The IHR 2005 surveillance provisions imply 100% sensitivity as a standard, namely the reporting of all events that meet notification requirements. The use of risk assessment criteria  also allows for higher sensitivity for PHEIC than would be possible with a list of predetermined disease threats (as in IHR 1969). To test the potential sensitivity of the decision instrument proposed in drafts of the revised IHR in 2004, investigators in the United Kingdom applied the then-proposed decision instrument to all events (N = 30) that were important enough to have been published in the national surveillance bulletin for England and Wales during 2003 . According to this method, 12 of the 30 events would have been reportable under the decision instrument. These events included all those that were considered potential PHEIC. Investigators concluded that the decision instrument was highly sensitive for selecting outbreaks and incidents that require reporting under the proposed IHR revision.\n\n【29】The sensitivity of the IHR 2005 surveillance system will probably be affected by 2 factors. First, in all likelihood, inadequate capacities at the local and intermediate levels within state parties will limit the system's sensitivity more than capacities at the national level. Second, state parties may not always be willing to comply with their reporting obligations in the face of possible adverse political and economic consequences that may result from alerting the world to a disease event in their territories. Fear of such adverse consequences undermined reporting obligations in IHR 1969.\n\n【30】IHR 2005 incorporates strategies to address these potential limitations. First, as noted above, IHR 2005 requires state parties to build and maintain core local, intermediate, and national surveillance capabilities (article 5.1, annex 1). Fulfillment of this obligation will improve surveillance capacity vertically, from local to national levels, which should support higher sensitivity.\n\n【31】Second, IHR 2005 permits WHO to improve sensitivity by collecting and using information from multiple sources. IHR 1969 only allowed WHO to use information provided by state parties , and failure of state parties to abide by their reporting obligations adversely affected WHO surveillance activities . Under IHR 2005, WHO can collect, analyze, and use information gathered from governments, other intergovernmental organizations, and nongovernmental organizations and actors (article 9.1). By permitting WHO to cast its surveillance network beyond information it receives from governments, IHR 2005 creates opportunities for WHO to improve the sensitivity of the surveillance system and avoid being blocked by governmental failure to comply with reporting requirements.\n\n【32】##### Timeliness of the Surveillance System\n\n【33】Public health practitioners understand how timely notification of public health risks is necessary for effective intervention strategies , lessons reiterated in the SARS pandemic . Timely surveillance is also stressed in connection with strategies to deal with pandemic influenza . Timeliness may be the most important attribute that IHR 2005 will have to demonstrate to be effective.\n\n【34】IHR 2005 contains several provisions that relate to timeliness. National-level assessments with the decision instrument must be completed within 48 hours (annex 1, part A, 6\\[a\\]). State parties must then notify WHO within 24 hours of assessing any event that may constitute a PHEIC or that is unexpected or unusual (articles 6.1 and 7). The same 24-hour requirement applies to reporting public health risk outside a state party's territory that may constitute a PHEIC (article 9). State parties must also respond within 24 hours to all requests that WHO makes for verification of health-related events in their territories (article 10.2).\n\n【35】Timeliness of reporting is likely to be affected more by actions taken at local and intermediate levels than national-level provision of information to WHO. In this regard, IHR 2005 includes the core surveillance capacity that local and intermediate public health entities must be able to carry out their reporting responsibilities immediately (annex 1).\n\n【36】WHO's ability to draw on a wide array of sources of information, including the Internet and nongovernmental organizations and actors, may enhance the timeliness of the IHR 2005 surveillance system . In countries that have less well-developed local, intermediate, and national surveillance systems, nongovernmental sources of information can often provide information faster than governments. Accessing this type of information early and often helps WHO contact countries sooner, which increases the chances of more effective interventions.\n\n【37】##### Stability of the Surveillance System\n\n【38】The obligations each state party has to build and maintain core capacities in surveillance at the local, intermediary, and national levels, combined with the responsibilities for surveillance WHO has globally, should construct a global surveillance system that will be stable and reliable over time. Recognizing that core capacities at the national level and below will not develop overnight, IHR 2005 gives state parties until June 2012 to develop these capacities (article 5.1). State parties can obtain a 2-year extension on this deadline by submitting a justified need and an implementation plan and can request an additional 2-year extension, which the WHO Director-General has the discretion to approve or deny (article 5.2).\n\n【39】The 5-year grace period, and the possibility of 2-year extensions, was a necessary compromise and reflects the difficulties many developing states will have in improving their surveillance systems. The stability and reliability of the IHR 2005 surveillance system are designed to increase steadily as the grace period and any extensions come to an end.\n\n【40】### Potential Obstacles to Achieving IHR 2005 Surveillance System Objectives\n\n【41】Continued lamentations about the weaknesses of public health surveillance nationally and globally  illustrate that achieving useful, sensitive, timely, and stable surveillance through IHR 2005 will be a challenge for states and the international community. Several potential obstacles, including technical, resource, governance, legal, and political concerns, will complicate and frustrate efforts to improve national and global surveillance capabilities. Table 2 summarizes these potential barriers and possible responses.\n\n【42】##### Technical Issues\n\n【43】Emerging infectious diseases often create technical challenges for surveillance, even for the most technologically advanced and well-resourced countries. The sensitivity of surveillance systems for new pathogens has historically been limited, particularly if such pathogens presented themselves in unusual or unexpected ways. Recent modeling has shown that the ability to control the spread of a new pathogen is influenced by the proportion of transmission that occurs before the onset of overt symptoms or through asymptomatic infection . This property explains why diseases such as influenza and HIV may be more difficult to control than smallpox or SARS.\n\n【44】Consequently, surveillance needs to be sufficiently sensitive to detect infectious agents that have not yet resulted in large numbers of diagnosed cases. One approach to this challenge is syndromic surveillance , but such surveillance has not been effective in detecting emerging infectious diseases early . In fact, WHO abandoned syndromic surveillance as a strategy for the revised IHR after pilot studies demonstrated that it was not effective . Improved diagnostic technologies may also help public health authorities identify new pathogenic threats . Strategies for enhancing reporting processes have been well described .\n\n【45】##### Resource Issues\n\n【46】The demands of IHR 2005 surveillance obligations will confront many countries, particularly developing countries, with resource challenges. IHR 2005 does not include financing mechanisms, which leaves each state party to bear the financial costs of improving its own local, intermediate, and national level surveillance capabilities. The obligation on state parties and WHO to collaborate in mobilizing financial resources (article 44) is a weak obligation at best. The lack of economic resources will, if not more vigorously addressed as recommended by the UN Secretary-General , retard progress on all aspects of the upgraded surveillance system. WHO, in conjunction with the United Nations and the World Bank, could consider developing a global strategy to support the development and maintenance of core surveillance capacities.\n\n【47】##### Governance Issues\n\n【48】Governance obstacles include managerial and administrative weaknesses in countries from the local to the national level. Few countries have conducted a systematic review of their surveillance systems, and thus most lack detailed knowledge of gaps and limitations in their surveillance infrastructures and how to address these problems . Only a few states have assessed their ability to detect and respond to emerging disease threats, such as those posed by bioterrorism agents . The IHR 2005 requirement that each state party assess the condition of its public health surveillance within 2 years of the regulations' entry into force should help countries improve their national governance for surveillance purposes. Again, many states will need external assistance with such work.\n\n【49】##### Legal Issues\n\n【50】State parties may face legal complications in implementing IHR 2005 within their national legal and constitutional systems. For example, the United States has indicated that requirements of US federalism may affect its compliance with IHR 2005 . The US position suggests that other countries may also wish to formulate reservations to IHR 2005 to account for the demands of their national constitutional structures and systems of law . Whether such reservations will undermine the IHR 2005 surveillance system cannot be assessed, but this concern has to be monitored closely as countries determine whether reservations are required under their national constitutional systems. IHR 2005 also specifies that domestic legislation and administrative arrangements be adjusted fully with IHR 2005 by June 2007, or by June 2008 after a suitable declaration to the WHO Director-General (article 59.3). Helping state parties update their public health law may be technical assistance that industrialized countries can provide.\n\n【51】##### Political Issues\n\n【52】Questions remain about the level of political commitment countries will demonstrate in implementing IHR 2005. IHR 1969 suffered because state parties frequently failed to report notifiable diseases and routinely applied excessive trade and travel restrictions . The relevance of such trade and travel concerns was most recently illustrated during the SARS pandemic through China's initial fears that disclosing the pandemic would harm its economy and foreign trade . WHO's access to nongovernmental sources of surveillance information reduces the incentives that state parties once had to hide disease events, as was demonstrated during the SARS pandemic . In addition, IHR 2005 includes provisions that require WHO to recommend, and state parties to use, control measures that are no more restrictive than necessary to achieve the desired level of health protection (articles 17, 43). Uncertainty lingers, however, as to whether these obligations will fare better in terms of state party compliance than similar ones in IHR 1969.\n\n【53】### Conclusion\n\n【54】Establishing effective global public health surveillance is at the heart of IHR 2005. Evaluating the surveillance system specified by IHR 2005 is necessary to understand the potential for this new set of international legal rules to contribute to global health governance. IHR 2005 prescribes essential elements of a surveillance system and seeks to achieve the critical attributes of usefulness, sensitivity, timeliness, and stability. These features resonate with other aspects of IHR 2005 that make it a seminal development for global health governance. In May 2006, the World Health Assembly adopted a resolution urging WHO member states to comply immediately, on a voluntary basis, with IHR 2005 in light of the threat posed by avian influenza .\n\n【55】The task of turning the IHR 2005 vision of an effective global public health surveillance system into reality is daunting. Of the obstacles complicating this challenge, lack of financial resources to upgrade surveillance systems, especially in developing countries, will be the most difficult to overcome. In IHR 2005, public health has been given a governance regime unlike anything in the history of international law on public health. Turning the blueprint detailed in IHR 2005 into functional architecture that benefits all is one of the great public health challenges of the first decades of the 21st century.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "233e591e-2f33-4672-ae13-c29dde010eef", "title": "Astrovirus in White-Tailed Deer, United States, 2018", "text": "【0】Astrovirus in White-Tailed Deer, United States, 2018\nAstrovirus is a positive-sense, single-stranded RNA virus first identified in feces of children with gastroenteritis in 1975. Since then, astrovirus has been found in a wide variety of mammals and birds . The family _Astroviridae_ comprises 2 genera, _Mamastrovirus_ and _Avastrovirus_ , and classification is based on host origin. Astroviruses cause diarrhea and neurologic diseases in mammals and a spectrum of diseases, including diarrhea, hepatitis, and nephritis, in birds . Astrovirus is associated with respiratory disease in humans, cattle, and pigs  and has also been found in fecal samples from roe deer with gastrointestinal illness in Denmark . Whether astrovirus circulates in other species of deer remains unclear.\n\n【1】In September 2018, the Veterinary Diagnostic Laboratory at Iowa State University (Ames, Iowa, USA) received 5 sets of tissue samples collected from deer of the same farm for identification of the infectious cause of death of 5 male white-tailed deer 8–14 weeks of age. The pen-raised deer experienced pneumonia and sudden death. Postmortem examinations showed pleural fluid in the lungs, pneumonia, and purple-mottled lungs. Histopathologic observations revealed that 3 deer had necrotizing bronchopneumonia, and 2 had interstitial pneumonia.\n\n【2】Although different combinations of the bacterial pathogens _Bibersteinia trehalosi_ , _Tureperella pyogenes_ , _Fusobacterium necrophorum_ , and _Pasteurella multocida_ were found in all cases, an underlying viral cause could not be excluded. Therefore, we used next-generation sequencing, first with pooled lung samples and then with individual lung samples, using Nextera XT DNA Library Preparation Kit with the MiSeq platform and MiSeq Reagent Kit v2 . A bioinformatic analysis indicated the presence of an astrovirus along with the bacteria. The complete genome sequence (nt) of this astrovirus  was found in the pooled lung tissue sample and 1 lung tissue sample, and partial genomes were found in the other 4 lung samples. A complete-genome comparison revealed that BoAstV/JPN/Ishikawa24-6/2013 (bovine isolate from Japan) had the highest identity (.9%) to WI65268. Further nucleotide sequence analysis revealed that WI65268 had a similar genome organization as other astroviruses .\n\n【3】Sequence comparisons of the amino acid sequences of the 3 open reading frames (ORFs) showed that WI65268 was closely related to 4 bovine astroviruses from Asia: B18 (ORF1a 71.9% sequence identity), Kagoshima1-7 and B76-2 (ORF1b 87.8% sequence identity), and Hokkaido11-55 (ORF2 46.8% sequence identity, distance value 0.479) . In contrast, WI65268 showed low amino acid sequence identities to US bovine strain BSRI-1 for all 3 ORFs (ORF1a 37.0%, ORF1b 68.3%, ORF2 38.8%) . The 2 available astrovirus sequences from roe deer (GenBank accession nos. HM447045 and HM447046) from Europe comprised only partial genomic sequences. WI65268 had low identities (.0% HM447045 and 34.4% HM447046) and pairwise distances (.787 HM447045 and 0.813 HM447046) to these isolates. On the basis of the International Committee on Taxonomy of Viruses p-distance criteria (new genotypes are assigned at a value of \\> 0.378) , WI65268 represents a novel astrovirus genotype.\n\n【4】Phylogenetic analysis of the complete genome showed that WI65268 is distantly related to other bovine, dromedary, takin, and yak strains . In phylogenetic analyses of ORF1a and ORF1b protein sequences, WI65268 clustered with bovine, yak, and takin astrovirus isolates from Asia . However, in an analysis of ORF2 (capsid) protein, WI65268 clustered with 2 bovine isolates from Japan and was distantly related to the cluster formed by the bovine, yak, and takin isolates from Asia , strongly indicating that WI65268 is a recombinant. We used Recombination Detection Program 5  to confirm that WI65268 was a recombinant and characterize the recombination event (Kagoshima1-7 at 1–5,031 and 5,651–7,967 and Kagoshima2-3-2 at 5,032–5,650). Reverse transcription PCR and sequencing results confirmed that sequences at the 2 junctional sites were the same as those found by next-generation sequencing.\n\n【5】Pathogens causing respiratory disease in domesticated animals, such as cattle and pigs, are relatively well studied. However, pathogens causing these diseases in wildlife animals, such as deer, are not well characterized. In this study, the new astrovirus we found or the bacterial pathogens could have contributed to the respiratory disease observed. Whether astrovirus plays a major or just synergistic role in respiratory disease in deer should be explored further.\n\n【6】Astrovirus was previously identified in roe deer with gastrointestinal illness in Europe and found to be closely related to bovine astrovirus isolates from Hong Kong, China, of the same genus (Mamastrovirus 33_ ) . WI65268 was also closely related to bovine isolates from Japan but distantly related to roe deer and Hong Kong bovine astrovirus isolates. An additional analysis of genetic distances of related isolates on the basis of ORF2 tentatively classified WI65268 as a novel species .\n\n【7】Determining the evolution of WI65268 any further is difficult without further epidemiologic data. Bovine or bovida astroviruses might be able to cross species barriers and replicate in deer, as suggested in a previous study , in which a bovine astrovirus isolate clustered with a porcine astrovirus type 5 instead of other bovine astroviruses. Further surveillance of white-tailed deer for astrovirus is needed for field monitoring.\n\n【8】About the Author", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "64bb87b8-2e29-413f-a52c-0efe932f4c36", "title": "Norovirus GII.21 in Children with Diarrhea, Bhutan", "text": "【0】Norovirus GII.21 in Children with Diarrhea, Bhutan\n**To the Editor:** Noroviruses are nonenveloped viruses of the family _Caliciviridae_ with a single-stranded RNA genome. In developing countries, noroviruses cause >200,000 deaths annually among children <5 years of age . Noroviruses are divided into 6 genogroups, GI–GVI, which are further divided into genotypes. Twenty-nine of the genotypes cause human infections . Worldwide, most GII infections are caused by GII.4, followed by GII.3 or GII.6 and then other genotypes, such as GII.2, GII.12, GII.13, GII.17, and GII.7, in varying proportions. We report that GII.21 is the major genotype causing diarrhea in children in Bhutan.\n\n【1】During February 2010–December 2012, fecal samples were collected from children <5 years of age with watery diarrhea who were seen at the outpatient and inpatient clinics of the Department of Pediatrics, Jigme Dorji Wangchuk National Referral Hospital, Thimphu, Bhutan. We extracted RNA from rotavirus-negative fecal samples by using the QIAamp Viral RNA Kit (QIAGEN, Hilden, Germany); norovirus was detected by reverse transcription PCR by amplifying capsid gene at C region . PCR results were confirmed by nucleotide sequencing of the amplicons . We determined genogroups and genotypes by submitting nucleotide sequences to the Norovirus Genotyping Tool .\n\n【2】We collected 15 water samples , including some from streams and water tanks that are sources of the Thimphu city water supply. Some samples were arbitrarily collected from taps from different locations in Thimphu. All samples were assayed for total coliforms, thermotolerant coliforms, and norovirus .\n\n【3】We performed a multiple sequence alignment using MUSCLE and conducted the phylogenetic analyses with the neighbor-joining method using MEGAS software . The branching patterns were evaluated statistically on the basis of bootstrap analyses of 1,000 replicates.\n\n【4】We tested 270 samples for norovirus. The mean age of children tested was 13.9 months. Sixty-four (.7%) children were positive for norovirus. Results were positive for genotype GI in 4/210 samples (/56 in 2010, 3/123 in 2011, and 1/31 in 2012); all positive samples were from boys in the outpatient clinic. Genotype distribution of norovirus GI was 1 GI.3 in 2012 and 1 GI.1 and 2 GI.9 in 2011.\n\n【5】Sixty of the 270 samples were positive for genotype GII norovirus (/73 in 2010, 24/147 in 2011, and 19/50 in 2012). Of these, 32 (.3%) were from boys. Forty-six children were outpatients. Most (%) of the GII norovirus cases occurred in children <23 months of age; 63.3% of infection occurred in children 6–23 months of age . By patient age group, the pattern for norovirus GII.21 was similar; 76% of infections occurred in children <23 months, and 62% infections occurred in children 6–23 months.\n\n【6】We determined seasonal distribution of norovirus GII for 52 cases; 21 cases occurred during winter, and only 2 cases occurred during autumn. Nine cases each occurred during spring and summer, and 11 cases occurred during the rainy season (late June through late September). In 2010, GII.3 predominated, followed by GII.4 and GII.7; in 2011, GII.21 was dominant, followed by GII.6, GII.2, and GII.8, then others; and in 2012, GII.21 remained the dominant strain, followed by GII.2, GII.4, and GII.6 . A Sydney variant of norovirus GII.4 was identified in 2012. Only 1 tap water sample (sample W8) from a house was positive for norovirus and coliform bacteria; and nucleotide sequencing of the norovirus amplicon confirmed it as GII.21. Phylogenetic analysis showed that non-GII.21 genotypes from Bhutan were closely associated with strains from Thailand and Korea .\n\n【7】In Bhutan, similar to other countries, genogroup GII is mainly responsible for norovirus infections, and most infections occur in younger children . In 2010, norovirus GII.3 was the dominant genotype in Thimphu, but in 2011, GII.21 became dominant and continued throughout 2012. GII.21 has been identified mostly in wastewater or rivers, and infections in human have been infrequently attributed to it . One GII.21 outbreak in a long-term care facility for elderly persons has been reported from the United States .\n\n【8】Why children in Thimphu were infected by GII.21 is not clear. The detection of norovirus GII.21 in 1 water sample suggests that the source of the outbreak might be tap water; however, GII.21 was not detected in the water related to the tap water supply system. Further examination using repeated samples from different sources is needed. Continuous dominance by GII.21 over 2 years indicates that norovirus of this genotype might have been established in the children of Thimphu, and human-to-human transmission might be ongoing. Determining the environmental source of norovirus GII.21 in Bhutan and developing prevention strategies to control the spread are urgently needed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bbe605bb-67ff-41d0-98c1-bbfad652c0d6", "title": "Arcanobacterium pyogenes Sepsis in Farmer, Brazil", "text": "【0】Arcanobacterium pyogenes Sepsis in Farmer, Brazil\n**To the Editor:** _Arcanobacterium pyogenes_ is a normal inhabitant of the mucous membranes of domestic animals, such as cattle, sheep, swine, and goats . Diseases caused by this agent have been reported for persons who live in rural areas and have underlying illnesses such as cancer and diabetes . A recent literature review , elicited by a case of _A. pyogenes_ endocarditis, found 13 unequivocal cases of human infection with this agent; many patients had a history of close contact with domestic animals. However, septicemia was not reported.\n\n【1】In June 2006, a 27-year-old immunocompetent man was hospitalized in Campinas (São Paulo, Brazil) for fever, cough with purulent bloody sputum, and discharge from and pain in both ears. The patient was a farmer who lived in the rural Amazon area and had extensive contact with cattle and swine. For the past 3 days he had been taking amoxicillin, 1.5 g/day, for chronic otitis media. At the time of hospital admission, his temperature was 38.9°C, respiratory rate 24 breaths/min, and blood pressure 100/70 mm Hg. He had palpable hepatosplenomegaly, but no murmur was detected in the precordium. Computed tomography (CT) scan of his chest showed multiple pulmonary nodules and alveolar infiltrates with small cavities suggestive of septic infarctions. Abdominal CT scan confirmed hepatosplenomegaly with focal lesions in the spleen. CT scan of the middle ear showed bilateral cholesteatoma and mastoiditis. No abnormalities were found during 2 transthoracic echocardiography procedures. Laboratory values were PaO 2  63.4 mm Hg, hemoglobin 11.9 g/dL, thrombocytes 109 × 10 3  /mm 3  , leukocytes 11.05 × 10 3  /mm 3  with a left shift, serum albumin 2.6 g/dL, alanine aminotransferase 108 U/L, and total bilirubin 2.57 mg/dL. Blood and the ear secretion were submitted for culture. Cefepime was prescribed, 2 g/twice a day.\n\n【2】Three blood cultures grew gram-positive bacilli, initially identified as _Corynebacterium_ spp. sensitive to penicillin, ampicillin, ceftriaxone, gentamicin, clindamycin, vancomycin, and resistant to erythromycin, as determined by disk-diffusion test. Ear discharge culture grew _Proteus mirabilis,_ sensitive to β-lactams, cephalosporins, and aminoglycosides. Subcultures of the gram-positive bacilli on sheep blood agar grew pinpointed, grayish, β-hemolytic colonies, identified as _A. pyogenes_ by use of API Coryne 2.0 kit (bioMérieux, Durham, NC, USA; code 4732761). Although susceptibility standards are not available for this organism, it was considered susceptible to penicillin and ampicillin by combining the disk-diffusion test with the MICs, as determined by the Etest (.06 mg/L for penicillin, 0.023 mg/L for ampicillin).\n\n【3】Partial 16S rDNA was amplified by using primers p27f and BAC1401r and sequenced by using primers 1100r, 765fs, and 10f. Sequences were compared with those available in GenBank by using gapped BLASTN 2.0.5 software . Identification to the species level was defined as a 16S rDNA sequence identity \\> 97%. Phylogenetic analysis was performed by using MEGA version 4.0  after multiple alignments of data by ClustalX ; gaps were treated as missing data. Clustering was performed by the neighbor-joining method . Bootstrap analysis was used to evaluate tree topology of the neighbor-joining data by performing 1,000 resamplings . BLASTN analysis of the 16S sequence of the isolate showed 99% identity with the 16S sequence of _A. pyogenes_ . Phylogenetic analyses with MEGA grouped this isolate with _A. pyogenes_ NCTC 5224 in a branch separated from other species; this grouping supported the phenotypic identification.\n\n【4】During the 7 days after admission, the patient’s condition worsened, cefepime was withdrawn, and ampicillin 6 g/day plus gentamicin 240 mg/day were prescribed. The patient became afebrile, gradually recovered, and was discharged after 28 days of therapy.\n\n【5】Our patient had otitis media that progressed to sepsis, which was diagnosed by clinical, laboratory, and imaging findings. The causative agent may have been undetectable in ear discharge if it was overshadowed by a strain of _P. mirabilis,_ a fastidious organism that also colonizes or co-infects this site. Endocarditis could not be ruled out because transesophageal echocardiography was not available.\n\n【6】_A. pyogenes_ is usually susceptible to benzyl penicillin, ampicillin, gentamicin, and macrolides and resistant to trimethoprim/sulfamethoxazole, streptomycin, and tetracyclines . The isolate from this patient was sensitive to β-lactams, ceftriaxone, and gentamicin. However, susceptibility standards are not available because _A. pyogenes_ rarely causes disease in humans. The patient had taken oral amoxicillin before admission, but his condition had not improved; subsequent addition of cefepime was also unsuccessful. The organism was probably sensitive to ampicillin, considering the low MIC and the expected serum concentration of the drug, but diffusion into the middle ear may have been poor or the local conditions caused by the cholesteatoma may have influenced the poor outcome of initial therapy. Treatment with intravenous ampicillin plus gentamicin produced full recovery.\n\n【7】Clinical laboratories do not routinely attempt to identify this organism. However, even in the absence of substantial concurrent illness, _A. pyogenes_ must be considered as an etiologic agent of several human infections, especially septicemia, for patients with a history of close contact with domestic animals, mainly cattle and swine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "047b35ff-8652-41d6-95ea-913bfca2fe1e", "title": "Equine Piroplasmosis Associated with Amblyomma cajennense Ticks, Texas, USA", "text": "【0】Equine Piroplasmosis Associated with Amblyomma cajennense Ticks, Texas, USA\n_Theileria equi_ (incertae sedis_ ; _Piroplasma equi_ Laveran, 1901) is one of the etiologic agents of equine piroplasmosis. This parasite infects equids worldwide, but a few countries (Australia, Great Britain, Japan, United States, and Canada) are classified as free of this disease. These and several other countries restrict entry or internal movement of horses on the basis of their serologic response to _T. equi_ antigen.\n\n【1】Particular tick species are obligate intermediate hosts and vectors for _T. equi_ , which undergoes a complex developmental cycle in the vector similar to that of other apicomplexan hemoparasites . Asymptomatic persistent parasitemia detectable by serologic analysis or PCR develops in equids that survive acute infection. International movement of asymptomatic carriers poses a risk for introduction of equine piroplasmosis into regions free of this disease, but endemic transmission occurs only in regions that have competent vectors.\n\n【2】The World Organisation for Animal Health has listed the United States as free of equine piroplasmosis since 1978, although recent cases have occurred. Some of these cases may have resulted because the complement fixation test, formerly used for import screening, was not sufficiently sensitive to make a correct diagnosis. When transmission has occurred, it has been iatrogenic rather than vector-borne. Only 2 experimentally competent vectors of _T. equi_ are known in the United States: _Dermacentor variabilis_ (American dog tick) and _Rhipicephalus_ (Boophilus_ ) _microplus_ (southern cattle tick) . However, of the 90 tick species in the United States, few have been tested for equine piroplasmosis vector competence .\n\n【3】### The Study\n\n【4】On October 2, 2009, a mare in Kleberg County, Texas, USA, showed clinical signs of equine piroplasmosis. Serologic testing at the Animal Plant Health Inspection Service, National Veterinary Service Laboratories (NVSL), US Department of Agriculture (USDA) (Ames, IA, USA) with a commercially available competititve ELISA (VMRD Inc. Pullman, WA, USA) detected _T. equi_ antibodies. The remaining 359 horses on the index ranch were tested in the same way, and 292 (.1%) of 360 were seropositive for _T. equi_ on initial screening .\n\n【5】Ticks collected from horses on the index ranch were shipped alive to NVSL. Identifications were made by using morphologic characteristics, geographic distribution, biologic characteristics, and host associations . NVSL received ticks from 228 horses; >1 species was present on 41 animals. The dominant tick, _Amblyomma cajennense_ , was collected from 180 (.9%) horses .\n\n【6】All ticks were identified and sent to the Agricultural Research Service, Animal Disease Research Unit, USDA (Pullman, WA, USA) for transmission studies. Live males and partially fed females were pooled by species and held at 25°C and a relative humidity of 98% until they were allowed to reattach and feed on uninfected horses.\n\n【7】A total of 104 _A. cajennense_ ticks (male and 79 female) were placed on a horse on October 30, 31, and November 2, 2009. These ticks had been removed from 73 horses on the index ranch, of which 68 (.2%) were seropositive for _T. equi_ . Females were allowed to reattach and feed until repletion; males were removed when all females were replete. All ticks were removed by November 18, 2009. Twenty-four fully engorged females and 3 live males were recovered. The horse had a fever (>39°C) 14 days after the ticks were first applied. Parasitized erythrocytes on a stained blood smear peaked at 0.3% on day 17. No other clinical signs of infection were evident. Serologic analysis and PCR  confirmed _T. equi_ infection.\n\n【8】Twenty-nine _D. variabilis_ ticks (male and 17 female) were placed on a second uninfected horse on October 30 and 31 and November 2 and 12, 2009. These ticks had been removed from 17 horses, of which 11 were seropositive and 1 was seronegative for _T. equi_ ; 5 had an unknown infection status. All ticks were removed by November 24, 2009. Six fully engorged females and 7 live males were recovered. This horse had a slight fever (°C) 15 days after tick attachment but otherwise showed no clinical signs. No organisms were found in blood smears, but this horse was positive for _T. equi_ by PCR 42 days after the first ticks were attached and by competitive ELISA 87 days after tick attachment.\n\n【9】### Conclusions\n\n【10】Ranch staff reported that they used no practices that would result in movement of blood-contaminated materials between horses (e.g. no reuse of needles), which suggests that iatrogenic transmission was not responsible for this outbreak. Consequently, the high prevalence of _T. equi_ infection implies a focus of vector-borne transmission.\n\n【11】_A. cajennense_ ticks were the most abundant species on horses during the period (October–November) of this investigation . Our results demonstrate that _A. cajennense_ ticks naturally acquired infection while feeding on infected horses and transmitted _T. equi_ intrastadially when they reattach and feed on uninfected hosts. _A. cajennense_ ticks have not been shown experimentally to be a competent vector for _T._ _equi_ . This species is a 3-host tick, and all life stages are known to feed aggressively on a wide variety of hosts, including horses. The natural distribution of _A. cajennense_ ticks includes southeast Texas; they are not known to be present in other parts of the United States where cases of equine piroplasmosis have occurred . Although this study demonstrates that _A. cajennense_ ticks are an experimental intrastadial vector, additional studies are needed to fully characterize the vector capacity of this species, particularly with regard to interstadial transmission.\n\n【12】Immature stages of _D. variabilis_ ticks occur almost exclusively on rodents; only adults were found on horses at the ranch. Although these ticks were able to transmit _T._ _equi_ intrastadially to an uninfected horse, the small proportion of infested horses on the ranch (.2%) and low transmission efficiency of this species (G.A. Scoles, unpub. data) make it unlikely that _D. variabilis_ ticks were responsible for the high infection prevalence.\n\n【13】_A. maculatum_ , the Gulf Coast tick, was the second most abundant species on horses at the index ranch during the study (.7%). However, this species survived poorly during handling and transport, probably because it is less tolerant of desiccation than are _A. cajennense_ ticks , and we did not have enough viable ticks to attempt transmission feeding. Whether this species can act as a vector for _T. equi_ is unknown. _D_ . (Anocentor_ ) _nitens_ ticks were collected from 7 (%) horses sampled. This species is a proven vector of _Babesia caballi_ but has not been shown to be a vector of _T. equi_ . _R. microplus_ ticks are limited to a quarantine zone along the Texas–Mexico border. The index ranch is north of this zone, and no _R. microplus_ ticks were found on horses at this ranch.\n\n【14】Although _T. equi_ can be transmitted iatrogenically, e.g. by common needle use , this route of transmission is improbable with good management practices. Vector-borne transmission is more likely than iatrogenic transmission to establish and maintain a large focus of infection, such as in this outbreak. Additional tick studies are needed to determine whether other indigenous tick species are involved in transmission at this site. However, if _A. cajennense_ ticks are the primary vector, the outbreak will likely be confined to this region because southeastern Texas is the northern extent of the range of this tick in the United States . Given knowledge of tick species that are competent vectors, spread of this parasite can be controlled by testing requirements and limits on regional movement of equines on the basis of presence or absence of such competent vectors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3b38d709-3042-4845-a28d-0685fa957037", "title": "Freedom of Information Act Importance to CDC Mission", "text": "【0】Freedom of Information Act Importance to CDC Mission\n### Welcome to the Centers for Disease Control and Prevention (CDC) Freedom of Information Act (FOIA) site.\n\n【1】CDC is the nation’s leading public health agency, dedicated to saving lives and protecting the health of Americans. CDC ensures its science and research activities and its employees comply with federal laws, regulations, and policies in order to exercise the highest level of scientific integrity. At the core of this mission is information sharing—not just health information and disease study results, but information CDC gathers as part of a continuous process of putting information into action. As a science-based agency funded by U.S. taxpayers, CDC is committed to openness and accountability.\n\n【2】Like all federal agencies, CDC is required to disclose records requested in writing by any person unless the records (or a part of the records) are protected from disclosure by any of the nine exemptions contained in the law. This site provides information about the CDC FOIA Office program and how to access CDC records.\n\n【3】FOIA Public Access Link (PAL)\n\n【4】Frequently Asked Questions\n\n【5】PAL Reading Room\n\n【6】CDC FOIA Statistics\n\n【7】FOIA Requester Service Centers\n\n【8】*   AoA - Administration on Aging\n*   ACF - Administration for Children and Families\n*   AHRQ - Agency for Healthcare Research and Quality\n*   CDC - Centers for Disease Control and Prevention\n*   CMS - Centers for Medicare & Medicaid Services\n*   FDA - Food and Drug Administration\n*   HRSA - Health Resources and Services Administration\n*   IHS - Indian Health Service\n*   NIH - National Institutes of Health\n*   OIG - Office of Inspector General\n*   PSC - Program Support Center\n*   PHS - Public Health Service\n*   SAMHSA - Substance Abuse and Mental Health Services Administration", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "57bcad2c-3668-4299-a3a5-203305172ff7", "title": "Norovirus GII.P16/GII.2–Associated Gastroenteritis, China, 2016", "text": "【0】Norovirus GII.P16/GII.2–Associated Gastroenteritis, China, 2016\nNoroviruses, the main cause of nonbacterial acute gastroenteritis outbreaks , are positive-sense, single-stranded RNA viruses within the family _Caliciviridae_ . The genome contains 3 open reading frames (ORFs). ORF1 encodes nonstructural proteins, including an RNA-dependent RNA polymerase (RdRp), ORF2 encodes a capsid protein (viral protein 1 \\[VP1\\]), and ORF3 encodes a minor capsid protein (VP2). On the basis of RdRp and VP1 gene sequences, noroviruses are classified into 7 genogroups (GI–GVII) . GI, GII, and GIV noroviruses can infect humans; GI and GII viruses, the most common, include at least 31 distinct genotypes .\n\n【1】Since 2002, GII.4 viruses have been the most common norovirus genotype circulating worldwide . However, during the winter of 2014–15 in parts of Asia, a new GII.17 strain emerged as the major cause of acute gastroenteritis outbreaks , suggesting that non–GII.4 norovirus might become the predominant genotype. We report that, in late 2016 in China, the number of norovirus outbreaks increased significantly over the same period during the previous 4 years (in 2016 vs. 6 in 2013, 11 in 2013, 36 in 2014, and 14 in 2015). A GII.P16-GII.2 virus caused 79% of the 56 outbreaks in 2016.\n\n【2】### The Study\n\n【3】Acute gastroenteritis outbreaks, defined as \\> 20 patients with vomiting and diarrhea associated with a common source of infection within 1 week, are reported to the National Emergent Public Health Event Information Management System established by the Chinese Center for Disease Control and Prevention (China CDC). Fecal specimens from each outbreak were tested for noroviruses by the local Center for Disease Control and Prevention using commercial real-time reverse transcription PCR (RT-PCR) kits (BioPerfectus Technology Co. Jiangsu, China; Aodong Inspection & Testing Technology Co. Shenzhen, China). At least 3 norovirus-positive samples from each outbreak were transported to the China CDC for study. The China CDC Institutional Review Board for human subject protection approved this study.\n\n【4】During 2012–2016, a total of 313 norovirus outbreaks in 24 provinces and 96 cities in China were reported to the National Emergent Public Health Event Information Management System; 109 (%) were reported in 2016, mostly in winter. In November and December 2016, norovirus outbreaks increased sharply, accounting for 56 (%) of the 109 norovirus outbreaks . These outbreaks occurred mainly in kindergartens (%) and schools (%) .\n\n【5】Partial capsid genes were amplified by conventional RT-PCR for genotyping, as described previously . Of the outbreaks,25 (%) of 32 were caused by GII.17 noroviruses during the winter of 2014–15 and 9 (%) of 15 by GII.3 noroviruses during the winter of 2015–16. In 2016, samples from 94 (%) of the 109 outbreaks were genotyped. The most common norovirus genotype was GII.2 ([52%\\]), followed by GII.3 and multiple genotypes ([9%\\] each); GII.4Sydney 2012 ([7%\\]); GII.17 and GII.6 ([5%\\] each); and others ([13%\\]). Of the 8 outbreaks involving multiple genotypes, 3 included GII.P16-GII.2 . Of the 49 GII.2 outbreaks in 2016, 44 (%) were reported during November and December 2016. Thirty-eight (%) GII.2 norovirus outbreaks occurred in kindergartens. Samples from all GII.2 norovirus outbreaks were selected for dual genotyping using conventional RT-PCR spanning the ORF1/ORF2 region, as developed by the US Centers for Disease Control and Prevention (Atlanta, GA, USA) . Genotyping results showed 48 outbreaks were caused by GII.P16/GII.2 norovirus, and 1 outbreak was caused by the GII.P2/GII.2 norovirus. The first GII.P16/GII.2 norovirus was detected in August 2016.\n\n【6】We obtained complete capsid sequences and nearly complete RdRp sequences from samples from 30 GII.2 outbreaks using nested PCR . Furthermore, 2 complete genomes (JS1208 and BJSMQ) were determined, showing the highest nucleotide identities (%) to those of the HS255/2011/USA  and Miyagi1/2012/JP  strains. Overall, 35 GII.2 VP1 sequences were 99% nt identical to each other, and the 28 RdRp sequences were closely related to the GII.P16 norovirus recently detected in Germany . All nucleotide sequences obtained were deposited in GenBank (accession nos. KY421121–KY421185).\n\n【7】The predicted 35 GII.2 VP1 amino acid sequences we determined were aligned with those for GII.2 strains from the 1970s to 2016 that are available in GenBank, and <15 aa mutations were found in the VP1 region, suggesting that the GII.2 norovirus is still circulating stably worldwide. This finding is consistent with a recent report showing that the non–GII.4 norovirus remains static . The x-ray crystal structure of the capsid-protruding domain of GII.2 strain Snow Mountain virus (SMV)  was recently determined . Compared with the histo-blood group antigen (HBGA) binding surface of strain SMV, the GII.2 strains in this study had 15 aa mutations in VP1, including 8 aa mutations at residues 335–349 around the HBGA binding site I in the P2 domain, although the conserved set of residues (Asn352, Arg353, Asp382, and Gly445) required for binding the fucose moiety of HBGAs in SMV was unchanged. Sequence analysis of the RdRp region revealed that 28 GII.2 strains in this study differed from GII.P16/GII.4 strain CA3477 by ≈1–5 aa mutations. A sequence analysis of 35 predicted GII.2 VP2 sequences showed mutations (divergent residues) at the C-terminus of VP2 compared with those of HS255 and Miyagi1. ORF1 of JS1208 and BJSMQ also differed from that of GII.P16/GII.4 strain CA3477 by 11 aa.\n\n【8】Phylogenetic analysis using VP1 showed that 35 strains in this study formed a subcluster independent of the GII.2 subcluster; the strains were most closely related to the norovirus GII.2 subcluster that included HS255 and Miyagi1. However, the RdRp of 28 GII.2 strains formed a single cluster and showed maximum relatedness to those of GII.P16/GII.4 strain CA3477 . The complete genomes of GII.P2/GII.2 strain Malaysia  and GII.P16/GII.4 strain CA3477  were used as query sequences to predict possible recombination breakpoints of the JS1208 genome by SimPlot software version 3.5.1 . The recombination breakpoint was predicted to be at nucleotide position 5088 at the boundary between ORF1 and ORF2.\n\n【9】### Conclusions\n\n【10】In China, the number of norovirus outbreaks increased substantially in late 2016, greatly surpassing norovirus activity reported during the same months of the previous 4 years. Most of these outbreaks were associated with a GII.P16-GII.2 strain, similar to a recently reported pattern in Germany . Seventy-eight percent of the GII.2 outbreaks occurred in kindergartens. The exact reason is unknown but is in line with the model presented by Parra et al. in which non-GII.4 genotypes seem to infect infants more frequently because adults have built immunity to different genotypes over time. This finding should be confirmed by serum inhibition/neutralization tests for specific genotypes or strains, such as GII.P16-GII.2, in the population. Continuous surveillance is needed to explore the epidemiologic, clinical, and evolutionary characteristics of this GII.P16-GII.2 strain. Additional studies should explore the mechanisms behind the emergence of this active strain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "417b6af2-2b87-4d59-a0ba-780149421f6c", "title": "Ancylostoma ceylanicum Hookworm in Myanmar Refugees, Thailand, 2012–2015", "text": "【0】Ancylostoma ceylanicum Hookworm in Myanmar Refugees, Thailand, 2012–2015\nHookworm infection affects >470 million persons worldwide . Childhood infection has been associated with growth stunting, severe anemia, and iron deficiency . _Ancylostoma duodenale_ and _Necator americanus_ hookworms are believed to be the most prevalent species that infect humans. Infections with these species are acquired by transdermal penetration of the hookworm larvae or by the fecal-oral route (A. duodenale_ hookworm only), and infection is limited to humans . Dogs and cats infected with _Ancylostoma ceylanicum_ hookworm have been found in close association with human populations . With use of molecular techniques, an increased number of human _A. ceylanicum_ hookworm infections have been documented in parts of Asia and the Solomon Islands .\n\n【1】Hookworms and other soil-transmitted helminths are neglected tropical pathogens targeted for worldwide control by the World Health Organization by 2030 . This goal is being pursued through the mass administration of benzimidazole compounds (i.e. albendazole, mebendazole). However, the ubiquitous use of a single class of drug in both human and veterinary medicine has raised concern for the emergence of drug resistance . We report hookworm infection and cure rates in a large cohort of US-bound refugees from Myanmar residing in camps in Thailand along the Myanmar–Thailand border and assess the presence of β-tubulin mutations that could confer drug resistance among persons with persistent infection.\n\n【2】### Materials and Methods\n\n【3】##### Recruitment of Participants and Sample Collection\n\n【4】This investigation was part of a larger program involving refugees living in camps along the Myanmar–Thailand border that was conducted by the Centers for Disease Control and Prevention during 2012–2015 . In our analysis, we included 1,839 (%) of the 2,004 refugees \\> 6 months of age from this cohort who provided fecal samples. Fecal samples were collected at time point 1 (T1, during the required medical examination for US resettlement), T2 (before departing the refugee camp), and T3 (after US resettlement). All refugees were offered albendazole after fecal specimens were collected to treat presumptive infection with helminths.\n\n【5】##### Molecular Detection of Hookworm Species\n\n【6】We performed stool extraction  as previously described . For the initial 233 samples, we performed quantitative PCR (qPCR) with primer-probe sets Ad1 (for amplifying _A. duodenale_ ) and Na (for amplifying _N. americanus_ )  . Then, we switched to a more sensitive primer-probe set targeting a repetitive element in _A. duodenale_ (Ad2) . When qPCR with the Ad2 primer-probe set failed to amplify the samples positive by qPCR with Ad1, we subjected 7 discordant samples to PCR restriction fragment length polymorphism, as previously described . We also sequenced these PCR products using standard (Sanger) sequencing technology.\n\n【7】##### Definitions\n\n【8】Participants whose fecal samples were positive for hookworm DNA by qPCR and became negative at the immediate next time point were considered cured. Those whose fecal samples were negative for hookworm DNA by qPCR but then positive the immediate next time point were considered to have a newly acquired infection. We defined persistent infection as having detectable hookworm DNA at 2 successive time points.\n\n【9】##### Single-Nucleotide Polymorphism Detection in _N. americanus_ –Positive Samples\n\n【10】For the participants who were positive for _N. americanus_ hookworm at all 3 time points, we tested fecal samples from T1 and T3 for β-tubulin single-nucleotide polymorphisms (SNPs) at codon 200. We also tested refugee T3 fecal samples for SNP167 by using an allele-specific real-time PCR approach with common primers and SNP-specific probes  and defined heterozygosity and homozygosity of SNPs on the basis of change in the cycle threshold, similar to previously described methods  .\n\n【11】##### Statistical Analyses\n\n【12】Unless stated otherwise, we used the geometric mean to measure central tendency. We determined the odds ratios (ORs) of risk factors for infection with _A. ceylanicum_ and _N. americanus_ hookworms by using a generalized linear model that used overdispersion with binomial distribution and logit link. We performed a maximum likelihood analysis using JMP 12.0.1 . We used these models to test the following parameters: sex, camp, age (infants and toddlers <2 years of age, children 2–18 years of age, and adults >18 years of age), and co-infections . The cutoff of <2 years of age for infants and toddlers was chosen because, after this age, the rates of mouthing, a prominent cause of fecal-oral contamination, decrease . We performed cure rate comparisons using a 2-tailed Fisher exact test. We compared the geometric mean eosinophil and hemoglobin concentrations of those monoinfected with _A. ceylanicum_ or _N. americanus_ hookworm (excluding co-infections involving both hookworms and _Strongyloides stercoralis_ , _Ascaris lumbricoides_ , and _Trichuris trichiura_ roundworms) by the Mann-Whitney test using Prism GraphPad 6.0e .\n\n【13】### Results\n\n【14】A total of 4,330 fecal samples from 1,839 refugees underwent DNA extraction and multiparallel qPCR. After excluding 4 participants positive for hookworm at T1 who did not receive albendazole, the number of participants with T1-T2 paired samples totaled 1,548; we had samples from all 3 time points for 517 participants. The geometric mean time between T1 and T2 sample collections was 188 (range 48–1,013) days and between T2 and T3 was 46.2 (range 14–413) days. Baseline hookworm infection (any type) in this population was high, ranging from 25.9%–32.8% depending on the camp of residence .\n\n【15】##### Detection of _A. ceylanicum_\n\n【16】When a highly sensitive primer-probe set specific to the _A. duodenale_ genome (Ad2) was used on samples positive for _Ancylostoma_ DNA by qPCR with primer-probe set Ad1, none was positive. Because of concern that Ad1 might enable the cross-amplification of other _Ancylostoma_ spp. we performed seminested PCR and _Mva_ I and _Psp_ 1406I digestion with 7 samples positive by Ad1 but negative by Ad2. Results from restriction fragment length polymorphism PCR indicated the presence of _A. ceylanicum_ hookworm ; these results were further confirmed by sequencing . The internal transcribed spacer 2 regions of the _A. duodenale_ and _A. ceylanicum_ genomes, which the Ad1 primer-probe set aligned with, are identical . Cross-species identification with this primer-probe set has been previously predicted , although not previously demonstrated in the literature.\n\n【17】We then determined the prevalence of _A. ceylanicum_ hookworm among the refugee population using a primer-probe set specific to a repetitive DNA element in the _A. ceylanicum_ genome (Ac). When using Ac, the total number of _A. ceylanicum_ –positive samples increased from 106 (using the Ad1 set) to 124 (using the Ac set). We tested or retested these samples (n = 124) by qPCR using the Ad2 primer-probe set, and 0% were positive. All samples positive by qPCR with Ad1 were positive by qPCR with Ac.\n\n【18】##### Response to Treatment\n\n【19】Baseline prevalence of _N. americanus_ (.3%) hookworm was higher than that of _A. ceylanicum_ (.3%) hookworm among all participants (n = 1,839); likewise, prevalence of _N. americanus_ (.4%) species was higher than that of _A. ceylanicum_ (.4%) species among all participants who gave paired T1-T2 fecal samples (n = 1,548) . Refugees in their sixth (years) and seventh (years) decades of life had the highest _N. americanus_ hookworm prevalence (>40% positive), and refugees in their third decade of life had the highest _A. ceylanicum_ hookworm prevalence (%) .\n\n【20】_A. ceylanicum_ infection had a higher cure rate than did _N. americanus_ infection ; 92.8% of _A. ceylanicum_ hookworm–infected refugees were cured by T2, despite the relatively long time that elapsed between T1 and T2. Of the samples paired for T2 and T3, all 7 _A. ceylanicum_ hookworm–infected refugees at T2 were cured by T3. At T2, the _N. americanus_ infection cure rate was 69%, and after the second administration of albendazole at T3, 42% (/50) were cured, resulting in 29 participants with persistent _N. americanus_ infection at resettlement in the United States. Combining cure rates across all time points, the overall _A. ceylanicum_ hookworm cure rate was 93.3% (/90), higher than that for _N. americanus_ hookworm (.9%, 292/443; p<0.001).\n\n【21】In total, 151 refugees had persistent _N. americanus_ infections, and 6 refugees had persistent _A. ceylanicum_ infections. At T2, the hookworm genomic DNA relative quantity in fecal samples of those with persistent _N. americanus_ infections (n = 122) decreased significantly (p<0.001) . Of the 6 persons with persistent _A. ceylanicum_ infections at T2, 4 (%) had a decrease and 2 (%) an increase in hookworm genomic DNA in their fecal samples. Those infected with _N. americanus_ hookworm at T2 and T3 (n = 29) had no significant decrease in parasite genomic DNA relative quantity at T3, and 26 refugees were positive for _N. americanus_ hookworm at all 3 time points.\n\n【22】We detected a small number of newly acquired hookworm infections. Of the 1,548 participants with T1-T2 paired samples, we detected 15 (≈1%) new _A. ceylanicum_ infections and 36 (.3%) new _N. americanus_ infections . Of the 517 participants with T2-T3 paired samples, we detected 3 (.58%) new _A. ceylanicum_ infections and 13 (.5%) new _N. americanus_ infections.\n\n【23】##### Risk Factors Associated with _N. americanus_ and _A. ceylanicum_ Infection\n\n【24】A generalized linear model was used to assess whether coincident helminth or protozoa infection, age, sex, or camp affected the risk for infection with _N. americanus_ or _A. ceylanicum_ hookworm at T1 . The _y_ \\-intercept for the model of _N. americanus_ hookworm was 2.4 (% CI 2.14–2.69) and for _A. ceylanicum_ hookworm 3.24 (% CI 2.77–3.74). The strongest predictors (ORs >1.5 and p values <0.0001) of _N. americanus_ infection were adult age (OR 3.83), _T. trichiura_ infection (OR 1.90), and _A. lumbricoides_ infection (OR 1.71). Female participants had a reduced odds of _N. americanus_ infection (OR 0.68; p<0.0001). _N. americanus_ infection was the only 1 of the 5 infections assessed that was associated with _A. ceylanicum_ co-infection (OR 2.08; p = 0.0018); female sex (OR 0.57; p<0.0001) and residence in camp 1 (Mae La, OR 0.69; p = 0.03) were associated with reduced odds of _A. ceylanicum_ infection.\n\n【25】We used a similar model that took age, sex, and camp into account to compare participants who cleared their infection with _N. americanus_ hookworm after 1 treatment (n = 290) with those who did not clear infection after 2 treatments (n = 26). Female sex, but not age or camp, was associated with clearance after a single treatment (OR 2.14, 95% CI 1.23–4.44; p = 0.0045).\n\n【26】##### _N. americanus_ Benzimidazole Resistance and β-Tubulin SNP Changes\n\n【27】Because 26 participants were positive for _N. americanus_ hookworm at all 3 time points despite 2 courses of albendazole treatment, benzimidazole drug resistance was a concern. In total, 19 of 26 persistently infected participants had T1 fecal samples available for DNA reextraction and SNP200 testing; 3 samples showed no amplification, and 16 were wild type. We then performed DNA reextraction and SNP200 and SNP167 testing with the T3 fecal samples available (n = 24). Only 11 of 24 samples had sufficient quantities of _N. americanus_ DNA (>150 pg/µL) to be amplified. All samples were positive for wild-type SNP200 and SNP167. None were homozygous or heterozygous for mutant alleles. Thus, no alterations in the β-tubulin gene could be detected at codons 167 or 200 to account for drug resistance.\n\n【28】An additional 213 samples (from multiple time points) had sufficient _N. americanus_ DNA to test SNP200 variation further; 173 of 213 samples were evaluable by allelic discrimination qPCR. All were homozygous wild type for SNP200.\n\n【29】##### Blood Cell Concentration Differences Between _N. americanus_ and _A. ceylanicum_ Infections\n\n【30】We further evaluated refugees with either _N. americanus_ (n = 143) or _A. ceylanicum_ (n = 24) hookworm monoinfections; participants co-infected with both hookworms or other soil-transmitted helminths (i.e. _S. stercoralis_ , _A. lumbricoides_ , and _T. trichiura_ roundworms) were excluded. Peripheral blood eosinophil concentrations were significantly higher (p<0.001) in those with _A. ceylanicum_ monoinfections (geometric mean 8.49 × 10 8  cells/L, 95% CI 5.98–12.04 × 10 8  cells/L) than those with _N. americanus_ monoinfections (geometric mean 3.44 × 10 8  cells/L, 95% CI 2.92–4.05 × 10 8  cells/L) . The hemoglobin levels did not differ between those with only _A. ceylanicum_ and those with only _N. americanus_ infections .\n\n【31】### Discussion\n\n【32】_A. ceylanicum_ hookworm is increasingly being recognized as a pathogen in humans, particularly in Southeast Asia . _A. ceylanicum_ is the only hookworm species known to achieve patency in both humans and other animals (e.g. dogs and cats) . In this group of US-bound refugees from Myanmar, _N. americanus_ infection was the most prevalent hookworm infection at all 3 time points tested (.4% at baseline); _A. ceylanicum_ was the only other hookworm species found, with a baseline prevalence of 5.4%. Because _A. ceylanicum_ infection has been described in both Thailand  and Myanmar , whether these persons were infected in their home country or in the camps in Thailand is unknown. Camp allocation seemed to have an effect on infection status for both hookworms; camps 2 and 3 imparted a higher risk for _A. ceylanicum_ hookworm acquisition, and camp 1 had a higher rate of _N. americanus_ infection. Evidence of newly acquired infections for both hookworms at T2 and T3  indicates that active transmission of both species was ongoing at these 3 camps. However, whether the majority of hookworm infections at baseline were acquired prior to entry or while residing in the camps is unknown. Differential infection rates at the 3 camps might reflect environmental and hygiene conditions in the camps, the historical exposure of the persons at these camps, or other factors. Of note, a risk factor for infection with either _A. ceylanicum_ or _N. americanus_ hookworm was infection with the other hookworm . This finding contrasts with previous surveys showing that _A. ceylanicum_ and _N. americanus_ co-infections are rare . Although _A. ceylanicum_ infection by the fecal-oral route is thought to be possible , walking barefoot has been shown to be a major risk factor for _A. ceylanicum_ and _N. americanus_ hookworm infections . Thus, the high propensity for co-infection in the population we evaluated suggests a similar mode of transmission, namely transdermal penetration, for both hookworms.\n\n【33】Despite possible co-transmission, those infected with _A. ceylanicum_ hookworm had slightly different risk factors than those infected with _N. americanus_ hookworm. _A. ceylanicum_ hookworm prevalence peaked in the third decade of life, compared with _N. americanus_ prevalence, which peaked in the sixth and seventh decades. However, _N. americanus_ prevalence remained high for many decades of life (years of age).\n\n【34】_T. trichiura_ roundworm, _Entamoeba histolytica_ ameba, and _A. lumbricoides_ roundworm infection as risk factors for _N. americanus_ hookworm co-infection reflects the high prevalence of parasitic infections in this population , in a setting with inadequate sanitary infrastructure despite improvement efforts . Why infections with these pathogens but not _S. stercoralis_ roundworm (acquired similarly to hookworm) or _Giardia duodenalis_ protozoa (acquired similarly to _E. histolytica_ ameba) put refugees at risk for _N. americanus_ infection deserves further study.\n\n【35】Female participants were less likely to acquire both _N. americanus_ and _A. ceylanicum_ hookworms, a finding that might reflect differential exposures, differences in immunity , differences in albendazole metabolism , or a combination of these factors. Although eosinophilia has been previously reported in experimental human infections with _A. ceylanicum_ hookworm  and in _A. ceylanicum_ case reports , a more striking eosinophilia was seen among those with _A. ceylanicum_ infections than those with _N. americanus_ infections. This observation supports the idea that, unlike the hookworms that only infect humans, the zoonotic _A. ceylanicum_ parasite might be less able to downregulate the host’s IgE-mediated response to infection . No difference in hemoglobin levels was found between those with either hookworm species. Anemia is typically seen in _A. duodenale_ infection and is less commonly associated with _N. americanus_ infection ; for _A. ceylanicum_ infection, data on anemia are scant.\n\n【36】The treatment for _N. americanus_ infection was only modestly effective (cure rate 42%–69%). It has been suggested that deworming might have enabled the zoonotic _A. ceylanicum_ hookworm to fill a niche left by a decrease in anthropophilic hookworms . That _A. ceylanicum_ infection was largely cured after single courses of treatment with albendazole (cure rate 92.8%–100%) is reassuring, although follow-up time periods in this evaluation were more varied than in most controlled studies specifically assessing response to treatment. Still, reinfection and newly acquired infection rates (.58%–0.97% for _A. ceylanicum_ and 2.3%–2.5% for _N. americanus_ ) were modest after deworming, compared with previous projections suggesting reinfection rates as high as 30%  after 3 months. With the average time between T1 and T2 exceeding 6 months (and the wide time range of 1–33 months), this population experienced a lower reinfection rate than has been suggested for endemic areas.\n\n【37】Persons who were infected with _N. americanus_ hookworm at all 3 time points represent a group that might have never cleared the infection or might have cleared infection but were subsequently reinfected. However, the time between sample collections was not statistically different between those who were persistently positive and those who cleared infection after a single treatment (E.M. O’Connell, unpub. data). Reports of benzimidazole resistance are increasing in the literature on veterinary medicine; resistance in canine _Ancylostoma caninum_ hookworm  and phylogenetically similar bovine intestinal nematodes  was associated with SNPs in the β-tubulin gene, particularly in codon 200 but also in codon 167 and, rarely, in codon 198. Likewise, other studies have shown low cure rates after benzimidazole administration in the setting of _N. americanus_ infection, and research suggests that resistance is emerging . One group found that albendazole administration exerts selective pressure on _T. trichiura_ codon 200 in Kenya and Haiti . The same group found that, in pooled _N. americanus_ eggs from Haiti, the allele containing the resistant codon 200 had a mean allelic frequency of 36% . A frequency of 0% was found for the homozygous resistant genotype in hookworm eggs (species not identified) from Haiti and Panama , and although the authors reported a 2.3% frequency of the homozygous resistant genotype in hookworm eggs from Kenya, the frequency of the resistant genotype after treatment did not increase , raising questions about the significance of this allele. Our examination of codons 167 and 200 in samples persistently positive for _N. americanus_ hookworm across all 3 time points, and of codon 200 in those with high levels of _N. americanus_ DNA in fecal samples at any time point, revealed only homozygous wild-type β-tubulin genes.\n\n【38】Several possibilities might explain why β-tubulin mutations were not found to account for persistent _N. americanus_ infections. First, the lesser-known codon 198 or another codon within the β-tubulin gene might be responsible for resistance in the hookworms infecting these refugees. Second, whereas amplification rates for our qPCR assay were as high as or higher than those in most other reports, up to 54% of samples positive for _N. americanus_ DNA at T3 did not amplify adequately to determine genotype. Therefore, mutations in SNPs at either codon 167 or 200 could have been missed. Last, β-tubulin might not be the only gene involved in drug resistance and responsible for low cure rates in _N. americanus_ . In this population, 88.5% (of 26) of those persistently positive for _N. americanus_ at all 3 time points were male. The peak concentration in serum and area under the serum concentration time curve for albendazole sulfoxide and albendazole sulfone (the main active metabolites of albendazole) have been found to be higher in female than in male volunteers . Also, male refugees were possibly more likely than female refugees to quickly reacquire infection due to differences in environmental exposures.\n\n【39】One lesson from this evaluation was that unbiased surveys are necessary before pursuing highly specific molecular techniques when profiling organisms that infect a population, particularly when considering emerging infectious diseases. Although the use of the highly repetitive genomic sequence for _A. duodenale_ hookworm is superior in sensitivity and specificity than the internal transcribed spacer 2 region , targeting the more conserved genomic region ultimately enabled detection of an unexpected organism, _A. ceylanicum_ hookworm, which otherwise would have been missed in this population.\n\n【40】This project had several limitations. Control and nontreatment groups were absent. Participation was completely voluntary. The number of fecal sample collections for the third time point dropped off substantially because of logistical issues relating to collecting and shipping samples from the various US states after resettlement. Unlike other studies focused on _A. ceylanicum_ hookworm, this evaluation did not include surveying the local cat and dog populations to establish potential reservoirs, which might be a worthwhile future research direction.\n\n【41】In summary, this cohort of US-bound refugees living in 3 camps in Thailand on the Myanmar–Thailand border was found to have a high prevalence of _N. americanus_ hookworm with suboptimal cure rates after albendazole administration that do not seem to be attributable to mutations in the β-tubulin gene at codons 200 or 167. In addition, _A. ceylanicum_ hookworm was the only other hookworm species identified in this population. _A. ceylanicum_ infection had a much higher cure rate after a single course of albendazole, and those with _A. ceylanicum_ monoinfection had a similar hemoglobin level as those with _N. americanus_ monoinfection. Future mapping efforts of soil-transmitted helminths should take into account the emergence of _A. ceylanicum_ hookworm infection in humans to further understand its distribution across the world. The recognition of the increased importance of zoonotic _A. ceylanicum_ hookworm over that of _A. duodenale_ hookworm in some populations raises epidemiologic questions about transmission dynamics and the differential effect on local health of these 2 species.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b93ba378-0048-4919-8c99-f733ae03d790", "title": "Formative Evaluation to Increase Availability of Healthy Snacks and Beverages in Stores Near Schools in Two Rural Oregon Counties, 2013", "text": "【0】Formative Evaluation to Increase Availability of Healthy Snacks and Beverages in Stores Near Schools in Two Rural Oregon Counties, 2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Children living in rural areas are at greater risk for obesity than their urban counterparts. Differences in healthy food access may contribute to this disparity. Most healthy food access initiatives target stores in urban areas. We conducted a formative evaluation to increase availability of healthy snacks and beverages in food stores near schools in rural Oregon.\n\n【3】**Methods**\n\n【4】We assessed availability of healthy snacks and beverages in food stores (n = 15) using the SNACZ (Students Now Advocating to Create Healthy Snacking Zones) checklist and conducted in-depth interviews with food store owners (n = 6). Frequency distributions were computed for SNACZ checklist items, and interview data were analyzed by using applied thematic analysis.\n\n【5】**Results**\n\n【6】Overall, availability of healthy snacks and beverages in study communities was low. Four interrelated themes regarding store owner perspectives on stocking healthy snacks and beverages emerged from the interviews: customer demand, space constraints, vendor influence, and perishability.\n\n【7】**Conclusion**\n\n【8】In addition to working with food store owners, efforts to increase availability of healthy snacks and beverages in rural areas should engage young people, food buyers (eg, schools), and vendors as stakeholders for identifying strategies to increase demand for and availability of these items. Further research will be needed to determine which strategies or combinations of strategies are feasible to implement in the study communities.\n\n【9】Introduction\n------------\n\n【10】Children living in rural areas are at greater risk for obesity than their urban counterparts . Differences in healthy food access may contribute to this disparity . Supermarkets, which can be scarce in areas of low population density, nearly always have a wider selection of healthy food at low prices than do convenience stores or small grocery stores, which are more common in rural areas . Likewise, food stores near rural schools carry fewer healthy snacks and beverages than stores near urban schools . Because children often visit food stores near their schools , increasing availability of healthier options in such stores may help to reduce childhood obesity. Most healthy corner store initiatives have targeted stores in urban areas .\n\n【11】We describe findings that were collected as part of a larger community-based participatory research project to engage young people in promoting healthy snacking in and around their schools in eastern Oregon. The project, SNACZ (Students Now Advocating to Create Healthy Snacking Zones), reaches young people through 4-H clubs; one SNACZ club is established for each study community. The goal of the evaluation was twofold: obtain baseline data on availability of healthy snacks and beverages in stores near rural schools and understand store owner perspectives on stocking these items. The findings will guide food store intervention strategies in the study communities.\n\n【12】Methods\n-------\n\n【13】This study was conducted in 8 communities of fewer than 2,000 residents in 2 eastern Oregon counties. Five communities were in Union County, the intervention site for SNACZ; 3 were in Wallowa County, the control site. The communities are geographically isolated by mountains or agricultural land. Each community has its own school district, with centrally located elementary/middle and high schools and at least 1 small store, but no supermarkets. In both counties, there are seasonal farm stands, farmers markets, and “u-pick” farms. Approximately 47.9% and 47.3% of students in Union and Wallowa counties, respectively, were eligible for free or reduced-price lunch .\n\n【14】This mixed methods convergent study  used food store checklists and in-depth interviews. In August 2012, we identified food stores in the 8 study communities by using the telephone book and a database from the Oregon Employment Department . We categorized each store as one of the following: a supermarket that sold food and other products (eg, clothing), grocery store that sold canned and dried foods and perishable items, convenience store that sold primarily convenience items (eg, processed snacks, ready-to-eat/heat foods), or gas station food mart (ie, convenience store attached to a gas station). Next, using geographic information system (ArcGIS 10.1; ESRI), we identified stores (n = 15) that fell within a 0.5-mile radial buffer from the geometric center of elementary/middle schools and used a ground-truthing procedure  to confirm the existence of the stores. We then sent letters to store owners inviting them to participate in SNACZ. One owner of a store in an intervention site community declined the invitation. We therefore identified a second store to work with the SNACZ club in this community. This store fell within a 2-mile radial buffer from the geometric center of the elementary/middle school but was still within the school district boundary. The owner of this store agreed to participate in SNACZ in May 2013. The store owners received a $20 participation incentive. Across the 15 stores, 9 were in the intervention site (grocery stores, 5 convenience stores, and 2 gas station food marts); 6 were in the control site (grocery stores, 1 convenience store, and 2 gas station food marts). Of the 15 store owners, all agreed to participate in the food store checklists, for a participation rate of 100%. Owners of grocery stores and convenience stores in the intervention site were invited to participate in interviews; 1 convenience store owner declined to participate. The interview participation rate was 86%.\n\n【15】### Assessment of healthy snack and beverage availability\n\n【16】We completed the SNACZ food store checklist  during May and June 2013. The checklist included 48 items: 6 beverages, 18 processed snacks, and 24 ready-to-eat and single-portion fresh fruits and vegetables. A “healthy” snack or beverage appropriate for consumption by children was defined as a single-serving product that met the Institute of Medicine Tier 1 nutrition standards for competitive foods and beverages sold in schools for total calories, total calories from fat and saturated fat, _trans_ fat, total sugars, and sodium . We also assessed availability of the same items in multiportion sizes. For each of the 4 categories of items, multiportion was defined as follows: beverages in portion sizes greater than 8 ounces; processed snacks with more than 1 serving per package; fresh produce sold in bags, bundles, party trays, or large containers (eg, strawberries in quart-size container). The checklist was completed by a trained research assistant. The checklist took approximately 8 minutes to complete. All SNACZ study procedures were approved by the Oregon Health and Science University Institutional Review Board, Portland, Oregon.\n\n【17】We pooled checklist data from stores in the intervention and control sites for this analysis to increase the sample size. We computed frequency distributions for individual food and beverage items by using Stata software (version 11; StataCorp LP).\n\n【18】### Food store owner interviews\n\n【19】We conducted the interviews during May and June 2013. The semistructured interview guide included questions about ordering snacks and beverages; experience selling healthy snacks and beverages, including fresh produce; and barriers to stocking healthy snacks and beverages (Box). The interviews lasted approximately 45 minutes and took place in the owners’ stores. The interviews were audio-recorded and transcribed verbatim.\n\n【20】Box. Questions for interviews with owners of food stores in a rural Oregon county\n\n【21】*   *   When you are ordering snack foods, what do you consider? Do you try to carry a selection of healthier snacks, such as baked chips or low-fat baked goods like bagels?\n    *   If you currently carry, or have tried to carry, healthier snacks in your store, what has been your experience with selling these?\n    *   When you are ordering beverages, what do you consider? Do you try to carry a selection of healthier beverages, such as 100% fruit juice or skim milk?\n    *   If you currently carry, or have tried to carry, healthier beverages in your store, what has been your experience with selling these?\n    *   What do you consider when ordering fruits and vegetables for your store?\n    *   Do you carry, or have you tried to carry, any ready-to-eat fruits or vegetables that children might choose as snacks? If yes, what has been your experience with selling these?\n    *   What do you see as being the major barriers to carrying healthier snacks and beverages?\n    *   What, if anything, could potentially be done to overcome these barriers?\n\n【22】We analyzed the interview data by using applied thematic analysis  targeted toward discovering themes with practical applications. We created a codebook with 13 operationally defined codes based on the interview questions and preliminary review of interview transcripts. Authors B.T.I. and N.E.F independently coded all 6 interview transcripts and compared them for inter-coder reliability. We resolved coding discrepancies through discussion and clarifying code definitions until we reached 100% agreement. When major code changes were made, we recoded the data with a revised dictionary. After all interviews were coded, we developed a series of displays  to determine themes within and across interviews and to draw and verify conclusions about the data.\n\n【23】Results\n-------\n\n【24】### Healthy snack and beverage availability\n\n【25】Overall, few stores carried healthy snacks or beverages in multiportion sizes; even fewer carried the checklist items in single-portion sizes . All stores carried plain bottled water in both sizes; other healthy beverages (ie, low-fat and nonfat plain milk, flavored milk, 100% fruit juice, soy milk) were available only in a multiportion size. All stores carried single-portion nuts and seeds and most carried multiportion crackers, nuts and seeds, graham or animal crackers, and dried fruit that met the checklist criteria. Six of 18 processed snacks were not available in any store in any size. Only one-third of the stores carried yogurt in a single-serving container that met the checklist criteria. Across the stores, availability of fresh produce in both single-portion and multiportion sizes was low. Most stores sold loose apples and oranges but fewer than half carried loose bananas; other ready-to-eat fruits were uncommon. Availability of fresh vegetables in single-portion sizes was rare; across the stores, cherry tomatoes were the only single-portion product available (n = 4). The number of stores that carried fresh produce in multiportion sizes also was low. Five stores carried strawberries in quart-size containers and about one-quarter carried bananas and grapes in large bunches. Only 6 stores carried baby carrots in multiportion bags. Bags of apples were not available at any store, and only 2 carried bags of oranges.\n\n【26】### Food store owner interviews\n\n【27】Four interrelated themes regarding store owner perspectives on stocking healthy snacks and beverages emerged from the interviews: customer demand, space constraints, vendor influence, and perishability .\n\n【28】#### Customer demand\n\n【29】Across the 6 store owners, customer demand was the primary criterion by which they made their stocking decisions. One participant said, “If I think it is something that will sell in my location I may initially order some of it to try and see if it sells and that determines whether I will keep it in stock or not” (Store 15: convenience store).\n\n【30】The participants indicated that, although they did stock some healthy snacks and beverages, the items were largely selected based on customer requests. For example, 3 store owners indicated that they kept low-sugar and gluten-free items in stock to meet the needs of their customers with diabetes or who are gluten intolerant. Five store owners indicated that they have tried to carry healthy snacks and beverages in their stores and that their experiences were generally unsuccessful: sales of packaged snacks (eg, baked chips) “come and go” but were not fast movers, and fruits and vegetables, except for apples and oranges, often spoiled before they sold.\n\n【31】#### Space constraints\n\n【32】Five of 6 store owners indicated that space constraints limited the number and variety of products they could carry in their stores. Store owners were reluctant to devote scarce shelf space to new products and instead preferred to stock products that had proven records of selling. In addition, because many products were available only through their vendors in large case-pack quantities and because maintaining items in stock is costly, store owners were hesitant to try new products, especially perishable products that may not have a high turnover (ie, the number of times inventory is sold and replaced during a certain time period). For example, 1 participant said, “With fruit, we’re not able to carry a variety. For example, we can’t carry 2 or 3 different kinds of apples because we need to buy them in 40 lb. boxes so we can only buy one variety” (Store 10: convenience store).\n\n【33】#### Vendor influence\n\n【34】All store owners reported that their vendors carried few healthy snacks and beverages. When healthier options were available, they were generally available only in large quantities (eg, 24 bags of baked potato chips, 40 lb box of apples), too large to sell to store owners’ limited customer base. Those who wanted to carry these products purchased small quantities “in town,” and re-sold them at their stores. The driving distance for these “buying trips,” which typically took place at stores in the larger surrounding communities, was approximately 50 to 80 miles round-trip. One store owner said that he was able to work with his vendor to purchase small quantities of products on a case-by-case basis. Four store owners also indicated that vendor “buy-back” agreements, in which vendors repurchased products that didn’t sell, also influenced what they carried. Although these agreements reduced the financial risk of trying new products for store owners, they increased the financial risk for vendors, which made vendors wary of carrying products they perceived to be slow movers. For example, 1 participant said\n\n【35】> \\[This practice\\] is one reason why they are so leery about bringing in an item. If they have to buy it back outdated, they lose on their commissions. \\[I\\]f they brought in 100 loaves of bread for me and they have to buy back 20, they have to take that off their commission. They are very leery about bringing in new items that they don’t think will sell. (Store 11: grocery store).\n\n【36】Vendors also promoted specific products to store owners, which influenced what the stores carried. One participant said\n\n【37】> Most of our beverages come from Pepsi and Coke. \\[Our vendors say\\] “This juice is really hittin’ it right now, so can we pull this product and put this in instead?” And I like trying this . so, if we have a suggestion from them, it seems that we always go with it. (Store 13: convenience store).\n\n【38】For some products, vendors provided the food and beverage displays (eg, coolers) and controlled the display inventory.\n\n【39】#### Perishability\n\n【40】When asked to identify barriers to stocking healthy snacks and beverages, 4 of 6 participants indicated that perishability of products such as fresh fruits and vegetables made it difficult to carry these items. One participant said\n\n【41】> I think the main thing is the healthier something usually is, the shorter the lifespan is on the shelf. That’s the big thing. You’ve got less preservatives, less holding power. The short lifespan is probably the biggest thing, the hardest thing for me. If I had unquestionable amount of time, it wouldn’t bother me to let it sit there. But once it goes outdated, I have to mark it down or get rid of it and that makes it cost prohibitive. (Store 11: grocery store).\n\n【42】Store owners did state that because demand for fruits and vegetables was higher during the summer they were more likely to stock a wider variety of these items, but only on a seasonal basis. Other perishable products carried by some of the store owners included those they were required to stock as a store authorized to accept payment through the Special Supplemental Nutrition Program for Women, Infants, and Children. Two participants had an in-store deli or an adjacent restaurant, in which they repurposed perishables (eg, day old bread, bananas with brown spots) as ingredients in their deli or restaurant menu items (eg, croutons, banana bread) before the products spoiled. This practice for reducing shrink (ie, the difference between what can potentially sell at full retail minus what is actually sold) by repurposing food is common in the food service industry.\n\n【43】Discussion\n----------\n\n【44】Consistent with previous studies of small stores in both urban and rural areas , most items on the SNACZ food store checklist were not available or found only in a small percentage of the stores surveyed. Our findings, and those of others , suggest that low availability of healthy options in small stores is due, in part, to a lack of perceived customer demand for these products. Because the stores in our study are near schools, engaging young people may be an effective strategy for creating and increasing demand for specific snacks and beverages that can be sold in the participating food stores. In Philadelphia, the Food Trust has worked with young people to increase the supply of healthy options in food stores near urban schools and the demand for these products by young people .\n\n【45】Our work also suggests that in rural areas, low population density and lack of product delivery options may also contribute to low availability of healthy options. Consistent with findings reported by others , store owners in our study indicated that when healthy snacks and beverages were available through their vendors, they were available only in quantities too large for their limited customer base. Although 1 store owner indicated that his vendor was willing to work with him to meet his needs, others indicated that their vendors were unwilling to split cases. Because of their location, the store owners in our study were able to select from only a limited number of vendors, which further hampered their ability to stock healthy options, including produce.\n\n【46】Given the challenges described above, potential strategies for increasing availability of heathy options in food stores participating in SNACZ include establishing partnerships between food stores, schools, and SNACZ clubs to develop consistent marketing messages to increase demand for healthy snacks and beverages that can be purchased in the food stores; providing store owners with financial incentives to offset costs associated with increasing availability of healthy snacks and beverages (eg, cooler to display single-portion fruit); facilitating partnerships between food stores and other food buyers (eg, schools) to coordinate their orders to increase their respective purchasing power; and engaging vendors as stakeholders in strategies to increase availability of healthy snacks and beverages, especially in single-portion sizes, in food stores.\n\n【47】Strengths of this study include our mixed-methods approach, which allowed us to illustrate checklist results with interview findings to develop a more complete understanding of healthy snack and beverage access in the study communities. Limitations include the small sample size, the potential for interview participant response bias, and the narrow geographic location, which limits generalizability of our findings to other rural areas.\n\n【48】Convenience stores and small grocery stores may be important sources of healthy snacks and beverages for children in rural communities, where availability of affordable and healthy food can be scarce. Efforts to increase availability of healthy snacks and beverages in these communities should engage young people, food buyers (eg, stores, schools), vendors, and store owners. Further research is needed to determine which strategies are feasible to implement in the study communities.\n\n【49】Tables\n------\n\n【50】#####  Table 1. Availability of Healthy Snacks and Beverages in Single-Portion and Multiportion Sizes in 15 Stores Near Rural Elementary/Middle Schools, Oregon,\n\n| Category/Item | Single-portion, % (n) | Multiportion, % (n) |\n| --- | --- | --- |\n| **Beverages** | **Beverages** | **Beverages** |\n| Water without flavoring, additives, carbonation, or caffeine | 100  | 100  |\n| Low-fat (%) milk, 8-oz portion | 0 | 67  |\n| Nonfat milk, 8-oz portion | 0 | 60  |\n| 1% or nonfat flavored milk, 8-oz portion | 0 | 7  |\n| 100% fruit juice | 0 | 100  |\n| Soy milk | 0 | 33  |\n| **Snacks** | **Snacks** | **Snacks** |\n| Chips | 0 | 0 |\n| Chex Mix | 0 | 60  |\n| Crackers | 0 | 80  |\n| Pretzels | 0 | 7  |\n| Rice cakes | 0 | 27  |\n| Popcorn | 0 | 0 |\n| Nuts and seeds | 100  | 93  |\n| Trail mix | 0 | 0 |\n| Cookies | 0 | 47  |\n| Graham or animal crackers | 0 | 93  |\n| Granola bars | 33  | 53  |\n| Bagels | 0 | 0 |\n| Muffins | 0 | 0 |\n| Popsicles or other frozen desserts | 0 | 0 |\n| Yogurt | 33  | 13  |\n| Applesauce, unsweetened | 0 | 33  |\n| Other canned or bottled fruit | 0 | 60  |\n| Dried fruit with no added sugar | 0 | 73  |\n| **Fresh fruits** | **Fresh fruits** | **Fresh fruits** |\n| Apples | 60  | 0 |\n| Apricots | 7  | 0 |\n| Bananas | 47  | 27  |\n| Blueberries | 13  | 0 |\n| Cherries | 0 | 20  |\n| Grapefruit | 20  | 0 |\n| Grapes | 7  | 27  |\n| Melon (cut up) | 0 | 13  |\n| Nectarines | 7  | 0 |\n| Oranges | 60  | 13  |\n| Peaches | 13  | 0 |\n| Pears | 33  | 0 |\n| Pineapple (cut up) | 0 | 7  |\n| Plums | 0 | 0 |\n| Strawberries | 0 | 33  |\n| Mixed fresh fruit (ie, fruit salad) | 0 | 13  |\n| Other ready-to-eat and single-portion fresh fruit (eg, kiwis) | 33  | 0 |\n| **Fresh vegetables** | **Fresh vegetables** | **Fresh vegetables** |\n| Broccoli florets | 0 | 7  |\n| Carrots (baby) | 0 | 40  |\n| Cauliflower florets | 0 | 7  |\n| Celery sticks | 0 | 0 |\n| Cherry tomatoes | 27  | 7  |\n| Mixed fresh vegetables | 0 | 7  |\n| Other ready-to-eat and single-portion fresh vegetables (eg, snap peas) | 0 | 20  |\n\n【52】#####  Table 2. Themes and Supporting Quotes From Qualitative Interviews With Store Owners (n = 6) in a Rural Oregon County,\n\n| Theme | No. of Store Owners Referencing Theme | Quotes to Support Theme |\n| --- | --- | --- |\n| **Customer demand** | 6 | “We try to stay customer-oriented. That’s the main thing. I want my customers coming through the door…that’s the main drive of anything. You want to bring in something that’s going to sell, that’s not going to sit there and be dead inventory.” (Store 11: grocery store) |\n| **Customer demand** | 6 | “I would love to carry celery sticks and carrots and ranch dip and stuff, but I tried it and they do not sell. I can’t keep buying that stuff and throwing it away.” (Store 14: convenience store) |\n| **Customer demand** | 6 | “We sell a lot of yogurt. That has surprised me. I’ve noticed a lot of kids getting a yogurt with their meal. I think they learned that at school with some programs. Maybe even starting in preschool. They get started in school and then they like it.” (Store 1: grocery store) |\n| **Space constraints** | 5 | “We just don’t have very much room for \\[healthier products\\].” (Store 1: grocery store) |\n| **Space constraints** | 5 | I have a small area, so I have to take \\[that\\] into consideration. If \\[healthy products are\\] requested and I have a place to market them, I would do that. (Store 15: convenience store) |\n| **Vendor influence** | 6 | “The contract we have \\[with vendors\\] indicates that vendors will take back the expired products, so vendors don’t want us to stock what they believe won’t sell.” (Store 10: convenience store) |\n| **Vendor influence** | 6 | “\\[M\\]y soda coolers are provided by the distributors, so I have to carry in it what they distribute. Most of \\[the vendors\\] merchandise the accounts, so they put in and kind of control the inventory.” (Store 15: convenience store) |\n| **Vendor influence** | 6 | “With fresh fruits and vegetables, sometimes they are really hard to get in. So, I will run to \\[larger community\\] so we have it in here. Because it’s too expensive for us to get it trucked here and be able to carry it at a reasonable price for children or young adults who may want to purchase them.” (Store 13: convenience store) |\n| **Perishability** | 4 | “I am fortunate because I have the restaurant so I can rotate stuff over before it goes bad. The items I can use, \\[like\\] the produce, I can take it over and make soups out of it. I have a benefit that other stores don’t have.” (Store 14: convenience store) |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4c803ec5-2df7-4e57-bfdf-e21cdb1389b7", "title": "Imported Measles and Implications for Its Elimination in Taiwan", "text": "【0】Imported Measles and Implications for Its Elimination in Taiwan\nThe availability of a measles vaccine in the 1960s led to substantial reductions in the incidence of measles globally. Along with improvements in living conditions, nutrition, and the availability of antimicrobial drugs for secondary bacterial infections, the mortality rate was greatly reduced . A live-attenuated measles vaccine was introduced to Taiwan in 1968, and a routine vaccination policy was established in 1978 to provide measles vaccine to infants at 9 and 15 months of age. During 1992–1994 and 2001–2004, two catch-up campaigns were implemented, targeting birth cohorts September 1976 to September 1990 and September 1990 to September 1994, respectively. Starting in 2006, the measles, mumps, and rubella (MMR) vaccine targeted infants 12–15 months old and 6-year-old children. The coverage rates for the first dose of measles vaccine and the second dose of MMR were 91% and 95%, respectively, since 1996. In recent decades in Taiwan, 4 major measles outbreaks have occurred, with 2,219 (in 1985), 1,386 , 1,060 , and 303  reported cases. Beginning in 1993, the annual number of reported measles cases was <100; by the end of 2007, the annual number of confirmed measles cases was <10 . The ability to actively control measles has increased in Taiwan since 1991 because of a combined plan to eliminate polio, measles, congenital rubella syndrome and neonatal tetanus.\n\n【1】The key to controlling measles is achieving and sustaining high levels of vaccination coverage. Finding a way to prevent the transmission of measles is an important topic for any country in the elimination phase. Here, we report several measles outbreaks that occurred in Taiwan during 2008 and 2009.\n\n【2】### The Study\n\n【3】In total, 140 suspected measles cases were reported to the Taiwan Centers for Disease Control from November 2008 to May 2009, and 53 were confirmed . The criteria for confirming cases included laboratory diagnosis and establishing an epidemiologic link to the laboratory-diagnosed case. The criteria for the laboratory diagnosis included the presence of measles immunoglobulin (Ig) M, isolation of measles virus, or identification of measles virus by reverse transcription PCR .\n\n【4】A timetable for occurrence of the 53 measles cases was constructed  on the basis of the week of onset of rash for each patient. The locations of the hospitals that reported these cases are shown in Figure 1 , panel B.\n\n【5】Cluster 1 involved nosocomial infections in hospital A (case-patients 1–5) and hospital B (case-patients 6–8); the transmission between case-patient 5 and case-patient 6 was through household contact. The index case-patient (case 7) was discovered later. Cluster 2 (case-patients 9–21) caused nosocomial infections in hospitals C (case-patients 9–10) and D (case-patients 10–17 and 19). Case-patient 18 was reported by hospital F, case-patient 20 from clinic b and case-patient 21 from hospital O. With the exception of case-patient 20, who attended kindergarten with case-patient 19, there was no exposure linkage between case-patients 18 and 21; the phylogenic data  did, however, indicate the patients were infected with related strains. Cluster 3 involved case-patients 22–24; the index case-patient was the source of transmission and caused the nosocomial infection in hospital E. Cluster 4 (case-patients 25–33) caused nosocomial infections in hospital F (case-patients 25 and 27–33), and case-patients 25 and 26 attended the same school. Cluster 5 (case-patients 34–35) was a nosocomial infection in hospital G. Cluster 6 involved 10 cases: the index case-patient, case 37, was reported by hospital I. Case-patients 36–43 were from the same military base, and case-patients 37, 44, and 45 were relatives. Cluster 7 caused nosocomial infections in hospital H (case-patients 46 and 47) as well as household transmissions (case-patients 47 and 48). Cluster 8 (case-patients 49 and 50) was transmitted by contact at a dormitory. Case-patient 51 had a sporadic case from an unidentified source; case-patients 52 and 53 had sporadic cases linked to importation from Vietnam and India, respectively.\n\n【6】Of these 53 patients with confirmed measles, 36 (.9%) were male. There were 15 children (.3%) <12 months of age and 19 children (.8%) from 12 months to 6 years of age. Adults accounted for 19 cases (.8%). Only 1 patient (.88%) had received 2 doses of a measles-containing vaccine; 4 patients (.5%) had received 1 dose; 7 patients (.2%) recalled having received vaccines; 9 adults (.0%) did not know their vaccination status; and the remaining 31 patients (.4%) had not received any vaccines. Among the 6 nosocomial measles outbreaks, 2 infection sources (clusters 2 and 5) were children <12 months old and therefore not eligible for the first dose of the MMR vaccine, 4 clusters (clusters 1, 3, 4, and 7) were caused by children between 13–17 months old who were not vaccinated, and 1 cluster (cluster 8) was caused by an adult from Vietnam with an uncertain vaccination history. The outbreak on the military base was caused by an adult who was assumed to have been vaccinated.\n\n【7】Phylogenic analysis determined that the measles virus genotypes from these 8 outbreaks fell into 6 clades . Although the source was untraceable, the measles virus sequences from cluster 6 belonged to genotype G3; the sequence was 100% identical to a G3 strain (MVi/H Kajang.MYS/11.09) isolated from Malaysia . The other sequences all belonged to genotype H1 and were grouped into 5 lineages. Clusters 1, 2, and 3 were linked to importation from the People’s Republic of China and were grouped into different lineages. Clusters 4, 5, 7, and 8, which originated in Vietnam, were divided into 2 lineages, with clusters 4, 7, and 8 in the same lineage and cluster 5 in another lineage.\n\n【8】### Conclusions\n\n【9】Among these 53 confirmed cases, 26 (%) patients contracted measles from hospital exposure, and 3 patients (.7%) were hospital staff. Vaccination policies for workers in health care institutes should be strengthened because these workers have the highest potential risk of contracting the disease . Many other febrile exanthematic diseases caused by pathogens other than measles virus, such as enterovirus, rubella virus, parvovirus B19, human herpesvirus-6, or Kawasaki syndrome, could have symptoms similar to measles and be misdiagnosed . It is therefore vital to ensure that clinicians are fully aware of the disease and can recognize it in a timely manner.\n\n【10】After import-associated measles outbreaks were recognized, the MMR vaccination was recommended for persons planning to travel to measles-endemic countries. Successful immunization policies that include routinely vaccinating children with 2 doses of a measles-containing vaccine greatly reduce disease contraction among preschool and primary school children . Because measles outbreaks may occur in areas with high vaccine coverage , the higher proportion of measles cases in adults, especially in young adults, should be addressed further and evaluated with regard to the possible need for additional immunizations.\n\n【11】The major threats to measles control in Taiwan are similar to those faced by other countries in the elimination phase; the key questions are 1) how to detect the index case in real time, and 2) how to prevent import-associated transmission. Among these continuous outbreaks, 6 chains originated from children <18 months of age without adequate immunization who had recently traveled to measles-endemic countries. Clinicians being alert for patients being brought for treatment with a fever and rash at pediatric clinics will greatly improve the sensitivity of measles surveillance systems and prevent further nosocomial transmission. Special emphasis should be paid to pediatric patients without vaccination records and with a history of travel abroad. Of course, the ultimate goal of eliminating measles requires an agreement from all countries to contain the disease in their own territories.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c7c0ec7f-301c-4618-af5c-0f7e9e33a5ab", "title": "Traveling with Children", "text": "【0】### Traveling with Children\n\n【1】Children may face the same health risks as their parents during travel, but the health consequences can be more serious. For example, some illnesses can be difficult to recognize in children especially if they can’t talk yet to express what they are feeling, or children may be more likely to encounter health risks such as animals because of their size and curiosity. If you are planning to travel with children familiarize yourself with the information on this page to help everyone stay safe and healthy.\n\n【2】**Make an appointment** with your child’s primary health care professional or a travel medicine specialist at least one month before you leave. They can help you get destination-specific vaccines, medicines, and information. Discussing your itinerary and planned activities with your provider allows them to give more specific advice and recommendations.\n\n【3】**Make sure your child is up to date on all routine vaccines** . Routine vaccinations protect your child from infectious diseases such as measles that can spread quickly in groups of unvaccinated people. Many diseases prevented by routine vaccination are not common in the United States but are still common in other countries.\n\n【4】Some routine vaccines for young children have different recommendations if you plan to travel internationally. For example, although the first dose of measles-mumps-rubella (MMR) vaccine is not usually given until after 12 months of age, infants 6 to 11 months old should get 1 dose of before international travel. Some travel vaccines can be given on an accelerated schedule, meaning doses are given in a shorter period of time. Not all travel vaccines can be given to very young children, so it's important to check with a travel medicine doctor or your child's pediatrician, as early as possible before traveling.\n\n【5】**Plan for the unexpected.** It is important to plan for unexpected events as much as possible. Doing so can help you get quality health care or avoid being stranded at a destination. A few steps you can take to plan for unexpected events are to get travel insurance **,** learn where to get health care during travel **,** pack a travel health kit **,** and enroll in the Department of State’s STEP .\n\n【6】### Diarrhea\n\n【7】Diarrhea is among the most common illness experienced by children who travel abroad. Diarrhea can lead to dehydration. The best treatment for children with diarrhea is plenty of fluids; there is usually no need to give medicine. If your child appears to be dehydrated, has a fever, or bloody stools, get medical attention immediately. Keep in mind:\n\n【8】*   Oral rehydration salts (available online or in stores in most developing countries) may be used to prevent dehydration.\n*   Over-the-counter drugs that contain bismuth (Pepto-Bismol or Kaopectate) should NOT be used in children.\n*   Antibiotics are usually reserved for serious cases.\n*   Other common treatments for diarrhea, such as loperamide, are not recommended for children younger than 6 years old.\n\n【9】For infants, breastfeeding is the best way to prevent diarrhea and keep infants who have diarrhea hydrated. If you use formula, you may need to bring your own. Follow the manufacturer's instructions on how to prepare it. If there is poor water quality where you are traveling, you should use sterile water to prepare formula and to sterilize bottles, nipples, caps, and rings before using them. You can sterilize items in a dishwasher, boil in water for five minutes, use a microwave steam sterilizer bag or use bleach if none of the other options are available.\n\n【10】Everyone should choose safer food and drinks to prevent diarrhea. This includes eating foods that are served hot or are dry or packaged. Drink bottled, canned, or hot drinks and only drink milk that has been pasteurized. For short trips, you may want to bring a supply of snacks from home for times that available food may not be safe.\n\n【11】Wash hands with soap and water. If soap and water are not available, you can use an alcohol-based hand sanitizer that contains at least 60% alcohol. Use soap and clean water and to wash bottles, pacifiers, and toys that fall on the floor.\n\n【12】### Diseases Spread by Bugs\n\n【13】Mosquitoes can spread diseases, such as Zika , chikungunya , malaria , dengue , and yellow fever . Ticks can spread diseases such as Lyme disease and Tick-borne encephalitis.\n\n【14】Children can be protected against most of these diseases by using Environmental Protection Agency (EPA)-registered insect repellents with one of the following active ingredients:\n\n【15】*   DEET\n*   Picaridin (known as KBR 3023 and icaridin outside the US)\n*   IR3535\n*   Oil of lemon eucalyptus (OLE)\n*   Para-menthane-diol (PMD)\n*   2-undecanone\n\n【16】When using insect repellent on your child, always follow label instructions. Spray insect repellent onto your hands and then apply to a child’s face. **Do not use** products containing oil of lemon eucalyptus or para-menthane-diol on children under 3 years old. **Do not** apply insect repellent to a child’s hands, eyes, mouth, cuts, or irritated skin. If also using sunscreen, apply sunscreen first.\n\n【17】More steps to protect your child include dressing them in clothing that covers the arms and legs. Cover strollers and baby carriers with mosquito netting.\n\n【18】##### **Malaria**\n\n【19】Malaria is a serious infection that children can get while traveling internationally. Children visiting friends and relatives in areas with malaria may be at higher risk because they usually stay for longer periods of time.\n\n【20】Children traveling to an area with malaria should take malaria prevention medicine. Your health care professional can help you know which medicine your child should take. Many malaria prevention medicines have a bitter taste, but a pharmacist can crush the capsules and put the powder in a flavorless gelatin capsule. Because of the risk of overdose, malaria drugs should be stored in childproof containers and kept out of the reach of children.\n\n【21】Even if you are taking malaria medicine you should still use bug spray and take other steps to avoid bug bites .\n\n【22】### Rabies\n\n【23】Rabies is spread primarily through contact (bites, scratches, or licks) with animals that can have rabies (mammal) . Rabies is almost always fatal if not treated promptly. Children are at greater risk for rabies because they are smaller than adults, may play with animals and may not report bites.\n\n【24】Supervise children closely around animals, especially around dogs and puppies, cats and kittens, and wildlife. Any animal bite should be washed thoroughly with soap and water and you should seek medical attention immediately.\n\n【25】### Road Safety\n\n【26】Children should always wear a seat belt or sit in appropriate car and booster seats. Research car seat guidelines for the country you are going to; a car seat from the United States may not be approved in another country for use. In general, children are safest traveling in the back seat. No one should ever travel in the back of a pickup truck. See Traffic and Road Safety for more tips to avoid getting in an accident.\n\n【27】### Water Activities\n\n【28】When doing water activities, supervise children closely and have them wear a life jacket.\n\n【29】**You might also want to see the following topics to prepare for traveling with children:**\n\n【30】*   Travel to High Elevations\n*   Motion Sickness\n*   Protect Your Skin from the Sun\n*   Swimming and Diving Safety\n*   Travel and Breastfeeding\n\n【31】### After Travel\n\n【32】**If your child recently traveled and feels sick,** particularly if they have a fever, talk to their healthcare provider, and tell them about the travel.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0eda537a-be96-453d-887d-0ccea3d5a83f", "title": "Instituting a Smoke-Free Policy for City Recreation Centers and Playgrounds, Philadelphia, Pennsylvania, 2010", "text": "【0】Instituting a Smoke-Free Policy for City Recreation Centers and Playgrounds, Philadelphia, Pennsylvania, 2010\nBackground\n----------\n\n【1】Health consequences of exposure to secondhand smoke are well documented . As these consequences have become better understood, government’s role has increased in affording protections from secondhand smoke, particularly through implementation of clean indoor air laws . Recently, outdoor smoke-free policies have also gained traction. As of December 2012, more than 600 municipalities have smoke-free parks and more than 100 have smoke-free beaches .\n\n【2】Nevertheless, adoption of outdoor smoke-free policies remains limited, particularly in large, urban areas outside of California. Thirteen of the 50 largest US cities (excluding Philadelphia) have smoke-free parks, but 7 are in California alone, and 6 are spread among the remaining 49 states and Puerto Rico . Widespread adoption is potentially being hindered by lengthy legislative processes, perceived enforcement challenges , and a lack of nearby communities with similar policies . Prior research highlights important strategies for smoke-free policy implementation, such as involving youth, using local data to garner support, and emphasizing community education; however, regulatory (rule-making) approaches may be underutilized .\n\n【3】In this article we describe our process for developing and implementing a smoke-free policy for municipal recreation centers and playgrounds in Philadelphia. We provide a legal framework emphasizing regulatory rule making and an implementation process that other municipalities can consider to advance similar policies. Finally, we summarize early policy impacts.\n\n【4】Community Context\n-----------------\n\n【5】Of the 10 largest US cities, Philadelphia has the highest prevalence of adult smoking (Public Health Management Corporation \\[PHMC\\], Southeastern Pennsylvania Household Health Survey Data, 2008–2012) and among the highest rates of teen smoking (Centers for Disease Control and Prevention \\[CDC\\], Youth Risk Behavior Survey, 2011). Smoking-attributable diseases lead to approximately 2,100 deaths and $675 million in productivity losses annually in Philadelphia . Factors that account for these high rates include high levels of poverty, relatively low cigarette prices, ready availability of cigarettes, and smoking norms perpetuated by high rates of smoking .\n\n【6】In response to public health concerns surrounding tobacco, Philadelphia passed the Clean Indoor Air Worker Protection Law (CIAWPL) in 2006, which banned smoking in nearly all workplaces, restaurants, and bars as of January 2007. From 2008 through 2010, smoking among adults decreased from 27.3% to 25.2%, a statistically significant decline and the first large decrease in a decade (PHMC data, 2008–2012). This decrease may be partly attributable to CIAWPL and the federal cigarette tax increase in 2009. In 2009, the Philadelphia Department of Public Health (PDPH) developed a 5-year strategic plan to further decrease smoking and exposure to secondhand smoke.\n\n【7】In mid-2010, Philadelphia Mayor Michael Nutter requested that PDPH assess the public health evidence for outdoor smoke-free spaces and develop a smoke-free policy plan. Key areas of evidence in support of our policy included reducing risk of exposure to secondhand smoke, particularly for children, supporting a normative message that smoking is harmful, motivating smokers to quit, and mitigating sanitation costs associated with tobacco use. PDPH worked closely with Philadelphia Parks and Recreation (PPR) on this policy plan.\n\n【8】Methods\n-------\n\n【9】We divided the policy change process into 3 stages: 1) making the policy case, 2) vetting policy options and engaging stakeholders, and 3) implementing policy, including communications and enforcement strategies .\n\n【10】### Making the policy case\n\n【11】Establishing the public health evidence for policy change is critical to engaging diverse stakeholders with varied tobacco-related interests and for policy implementation and compliance . We outlined 4 main areas of evidence related to an outdoor smoke-free policy affecting municipal spaces. First, secondhand smoke can be harmful not only indoors but also in outdoor settings . Children, with their developing lungs, are particularly vulnerable to secondhand smoke , and in 2010, 23% of children in Philadelphia had been diagnosed with asthma (PHMC data, 2010).\n\n【12】Second, smoke-free policies support a normative message that smoking is unsafe and that nonsmokers have the right to be protected. This is especially significant for children because children are influenced by their perceptions of normal behavior . Clean indoor air laws are associated with lower odds of youth progressing from experimental to regular smoking . Accordingly, places that serve youth should be protected spaces for children.\n\n【13】Third, smoke-free policies are a strong motivator for smokers to try to quit. Smoke-free workplace policies are associated with increased cessation attempts . Because the City of Philadelphia employs 25,000 staff members and provides primary care to 90,000 patients through 8 community health centers, it stands much to gain from increased attempts to quit smoking and decreased smoking.\n\n【14】Fourth, outdoor smoke-free policies can alleviate sanitation burdens related to discarded cigarette butts and packages. Municipal cost savings are particularly important when localities are facing budget constraints. One analysis suggests that every pack of cigarettes leads to $0.22 in litter removal costs . This results in millions to tens of millions of dollars in costs annually for a large city.\n\n【15】### Vetting policy options and engaging stakeholders\n\n【16】Key internal and external stakeholders included 1) civic leaders, 2) PPR leadership and staff, and 3) community and public health advocates, including youth. Stakeholder engagement focused on identifying motivators for and concerns about outdoor smoke-free policies and deciding which spaces should be included in such policies.\n\n【17】Having championed passage of CIAWPL, the mayor knew of the tobacco control challenges in Philadelphia and the importance of smoke-free policies. A focus on child-serving institutions within government aligned with 1 of the mayor’s core objectives — improving children’s health and education. Thus, in mid-2010, he directed PDPH to explore outdoor smoke-free policy options with PPR leadership.\n\n【18】PDPH initiated a conversation with PPR in fall 2010. Discussion centered on the public health benefits of focusing on child-serving spaces and the potential challenges with compliance. PDPH had learned from other jurisdictions that signage, earned media, and staff and patron education could be sufficient for achieving compliance . PDPH and PPR discussed how these tasks could be shared across agencies and, as appropriate, with external partners.\n\n【19】PPR oversees municipal recreation centers (n = 55), playgrounds (n = 97), pools (n = 70), and parks (n = 64) serving millions of visitors annually. The first 3 spaces are clustered in low-income neighborhoods and used primarily by children during after-school hours and by families on weekends . As in many large cities, recreation centers in Philadelphia are large complexes comprising athletic fields, playgrounds, buildings, and, in some cases, pools.\n\n【20】PDPH and PPR decided to focus on these child-serving spaces because they allowed for the strongest case to be made about public health benefits, mitigating harm from secondhand smoke, and creating nonsmoking norms. Additionally, parks presented numerous logistical barriers. Philadelphia’s parks system consists of 63 small neighborhood parks and Fairmount Park, an 8,000-acre park with vast green spaces, performance venues, museums, and historical buildings. PPR leadership noted that Fairmount Park’s size would make implementation in that setting particularly difficult. Moreover, neighborhood parks are represented by a decentralized network of local associations with varying degrees of influence, creating potential barriers to consensus development.\n\n【21】PPR leadership contacted 2 groups in late 2010 and early 2011 regarding a smoke-free policy for recreation spaces: on-the-ground agency staff and resident-led advisory councils. Key staff included 8 district managers, who oversee 10 to 15 recreation centers each. They expressed concerns similar to those of PPR leadership regarding enforcement, particularly in light of more pressing challenges such as violence and drug use on recreation center grounds. These issues were addressed in the educational sessions provided to PPR staff before roll-out. Advisory council leaders, who also have long-standing community ties to recreation centers, were generally supportive of the policy change. They identified a smoke-free policy as a way to protect children and perceived such policy as a natural extension of recreation center standards. Although accommodating adult smokers (both staff and patrons) was discussed, designated smoking areas were rejected because they would compromise the core goal of protecting children from the normative effects of seeing trusted adults smoking.\n\n【22】PDPH also engaged community and public health leaders outside of government through a variety of existing forums. The SmokeFree Philly Coalition brings together cessation providers, clinicians, employers, insurers, and public health organizations to promote cessation, decrease smoking initiation, and reduce secondhand smoke exposure among Philadelphians. The coalition recommended involving clean air and environmental organizations as allies, enhancing cessation support for municipal employees directly affected by the policy, and incorporating youth voices in the policy launch. To this point, the Philadelphia Youth Commission — comprising 15 youth commissioners appointed by the mayor and city council to represent the city’s youth — served as a critical partner. Commission members emphasized that teens are frequent users of recreation centers and pools even though most programming is oriented toward younger children. They also expressed interest in helping communicate the benefits of smoke-free spaces to peers, adults, and the media.\n\n【23】PDPH also presented evidence on the public health effects of smoke-free policies to the Get Healthy Philly Leadership Team, which is chaired by the mayor and includes leaders from private corporations, universities, health insurers, foundations, the School District of Philadelphia, and city council. Members viewed a smoke-free recreation policy as a natural extension of the city’s prior clean indoor air efforts and cited the new policy as motivation for other large employers to make their outdoor campuses smoke-free.\n\n【24】### Implementing policy\n\n【25】At the direction of the mayor, PDPH considered various vehicles for enacting the smoke-free policy . Potential vehicles included legislation via city council, a regulation from a city agency, and an executive order of the mayor. PDPH leadership consulted with its legal counsel early in the process to vet these options because local policy vehicle options depend on local and state laws.\n\n【26】Each policy vehicle has potential benefits and risks . An executive order is a mayoral directive that affects management and operations of city government but lacks the full force of law. An executive order alone would not have been enforceable among the general public but can still be a powerful tool in commanding attention for an important issue. Legislation and regulation (rule making), on the other hand, carry the full force of law but each has pros and cons. A regulation is potentially quicker and involves the agency responsible for implementation; however, administrative regulators are perceived as less accountable because they are not elected, and a successor administration may quickly reverse a regulation. Legislation is potentially slower and may not sufficiently engage the agency responsible for implementation; nevertheless, legislators are directly accountable to the people, and efforts to reverse a policy will also require legislative action.\n\n【27】We targeted recreation centers and not parks more broadly, confirmed widespread stakeholder support, and focused on the public health benefits of protecting children from secondhand smoke. The mayor decided that these factors allowed the city to proceed with an executive order (directing the Parks and Recreation Commissioner to adopt a smoke-free policy) and PPR regulation (adopting a smoke-free policy). The executive order helped garner media attention and demonstrated the mayor’s strong commitment to this policy, and the regulation gave the policy the full force of law.\n\n【28】Once agreement was reached on the policy and the policy enactment vehicle, PDPH and SmokeFree Philly Coalition members began educating recreation center directors in early 2011. Seven sessions were conducted at meetings with individual district managers and the directors of the 10 to 15 recreation centers overseen by them. Education focused on the policy rationale; its applicability to all recreation centers, playgrounds, and pools; and how PPR staff should respond to noncompliance. Practical suggestions included pointing to a nearby smoke-free sign, providing a wallet card with information on cessation resources, and politely asking patrons to stop smoking. We believed that enforcement could be accomplished by educating patrons and encouraging self-enforcement, not by punishing or fining .\n\n【29】Recreation center leaders were provided with more than 4,000 wallet cards and 250 informational posters  for distribution and posting. PDPH developed a hotline through which staff and patrons could report violations of the policy. In response, PDPH educators would reach out to recreation center leaders to reinforce the policy and, as needed, provide in-person education for staff and patrons. PDPH distributed a memorandum explaining the policy to more than 400 sports leagues that hold permits for using PPR facilities. The memorandum was tailored to league organizers and coaches who could communicate the policy change to players and families.\n\n【30】Another key aspect of enforcement was facilitating compliance through cessation support because smoke-free policies can induce smokers to make quit attempts . Wallet cards, informational posters, and signage were meant to encourage people to call the Pennsylvania Free Quitline (QUIT-NOW) and visit a website . PDPH and PPR staff collaborated to offer free community-based cessation classes at recreation centers. Even before adopting an outdoor smoke-free policy, the City of Philadelphia expanded cessation benefits for 7,000 city employees and their dependents. PDPH partnered with local Medicaid plans to put Food and Drug Administration (FDA)-approved cessation therapies on their formularies at low to no cost for patients. PDPH implemented a large mass media campaign to promote quitting and sponsored a quitline-based nicotine patch giveaway in 2010 and 2011 for 10,000 Philadelphians.\n\n【31】In May 2011, Mayor Michael Nutter signed Executive Order 5–11 at Kingsessing Recreation Center. Attendees included Health and Parks and Recreation commissioners, a youth commissioner, and children from the neighborhood. We selected the location based on high rates of smoking in the area, and we facilitated youth involvement by selecting locations with child-oriented facilities (eg, swing sets), and proximity to a nearby school. This setting highlighted the policy’s public health potential to reduce secondhand smoke exposure of vulnerable populations and promote nonsmoking norms. Press, partners, and community members attended the event in response to a media advisory, a press release, and communications through the SmokeFree Philly Coalition.\n\n【32】In subsequent weeks, PDPH and SmokeFree Philly Coalition members continued educational sessions with recreation center leaders, sports league organizers and coaches, and other stakeholders. In July 2011, the Parks and Recreation commissioner promulgated a regulation making city recreation centers and playgrounds smoke-free. The regulation became effective in August 2011. During the next 6 months, more than 1,000 permanent smoke-free signs were installed at all recreation centers, playgrounds, and pools.\n\n【33】Outcome\n-------\n\n【34】We used available data sources to estimate near-term effects of this policy, including PPR administrative data, geospatial analysis, community focus groups, and a telephone-based survey of Philadelphia smokers that had been fielded to evaluate a contemporaneous mass media campaign.\n\n【35】A total of 222 recreation centers, playgrounds, and pools have become smoke-free. This change added nearly 850 smoke-free acres to Philadelphia . An estimated 3.6 million annual visits — most of which are made by children — will now be smoke-free . In addition, the _Philadelphia Inquirer,_ the city’s main daily paper, published an editorial praising the policy .\n\n【36】On the basis of coverage of the executive order signing ceremony in May 2011, we estimate that more than 770,000 impressions were generated via television and print media (and 541,518 impressions, respectively) (Terry Johnson, personal communication, June 2012). This figure does not include impressions for 5 major online media outlets for which data were unavailable. Television and online stories featured parents expressing gratitude that their children were being protected. According to a monthly cross-sectional monitoring survey of adult smokers in Philadelphia from June 2011 through September 2011, approximately 50% were aware of the smoke-free recreation center policy and, of those, more than 75% were supportive of it (Laura Gibson, Sarah Parvanta, Bob Hornik, Michelle Jeong, Emily Brennan; Annenberg School for Communication, campaign recall monitoring survey \\[unpublished raw data\\], 2011) . PDPH, in partnership with WHYY (local public media) and the Penn Project for Civic Engagement (Penn Project), hosted 4 civic dialogue sessions in September and October 2011 entitled “Smoke Signals: Forums on Tobacco Use in Philadelphia.” The Penn Project sought volunteer participants from community groups and high schools, and more than 75 people participated in 4 sessions. Groups were asked by a moderator whether they supported 9 different policy proposals. Six of 7 groups at these sessions expressed support for protecting people from secondhand smoke in outdoor spaces.\n\n【37】Interpretation\n--------------\n\n【38】Smoke-free policies affecting outdoor municipal spaces hold great potential to reduce exposure to secondhand smoke, promote nonsmoking norms, and provide additional motivation to quit smoking. Our experience suggests that localities can successfully review and synthesize the evidence for such policies, engage stakeholders in policy assessment, and implement and enforce these policies to enhance the public’s health.\n\n【39】In Philadelphia, PDPH led efforts to document the benefits of an outdoor smoke-free policy in light of high smoking rates among adults and youth, normative smoking behaviors in low-income communities, and the large number of children who use municipal recreation spaces and are especially vulnerable to secondhand smoke. PDPH worked closely with PPR to vet policy options and focus initial efforts on facilities that primarily serve children. Critical input was solicited from city residents, community leaders, agency staff, and public health organizations.\n\n【40】We enacted our policy through administrative actions of the executive branch, which have been overlooked as a critical tobacco control tool. Depending on local context, other communities should consider this approach. By combining a mayoral executive order and an agency regulation, we were able to garner attention for our smoke-free outdoor policy, advance it quickly, and instill it with the full force of law. Lastly, we implemented a strong communications strategy for staff, patrons, and the public to ensure awareness of the policy change and engage community members in efforts toward compliance. These communications were also leveraged to promote evidence-based smoking cessation resources.\n\n【41】Our observations have several limitations. One, we did not evaluate compliance through objective measures. We focused on documenting the process of policy adoption as a model for other localities. Two, we were able to adopt our policy with backing by elected and appointed leaders. Although our legal approach (rule making instead of legislation) was appropriate for Philadelphia, each municipality should consult with legal counsel because local authorities may differ. Moreover, engaging legal counsel is important to ensure that localities are adhering to restrictions placed on the use of government funds. Three, we relied on policy, legal, analytic, and communications staff with tobacco control expertise throughout the policy development process. Not all localities may have these resources, though municipalities can rely on existing public health staff with similar capabilities. Despite these limitations, we believe our experience can guide other municipalities interested in an outdoor smoke-free policy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1eeef21e-dbf7-49e1-b26c-8eaa44746439", "title": "H3N2 Influenza Virus Transmission from Swine to Turkeys, United States", "text": "【0】H3N2 Influenza Virus Transmission from Swine to Turkeys, United States\nInfluenza A virus is a highly infectious pathogen of a limited number of birds and mammals. Individual viruses are generally host-specific and are not readily transmitted between species. The species barrier reflects, at least in part, the different receptor preferences of mammalian and avian viruses. Researchers have suggested that human tracheal epithelial cells lack receptors for the attachment of avian influenza viruses and that avian tracheal epithelial cells lack the appropriate receptors for human viruses . Pigs, however, possess receptors for both avian and mammalian viruses and are postulated to be the host in which influenza viruses of different origins can genetically reassort .\n\n【1】Currently, three subtypes (H1N1, H1N2, and H3N2) of influenza virus are commonly found in pigs worldwide. Depending on the location, these viruses are derived from mammalian viruses, avian viruses, or reassortants of the two . Until 1998, the classic H1N1 lineage was the only influenza virus circulating widely in the swine population in the United States . In 1998, H3N2 triple reassortants with genes derived from human (HA, NA, and PB1), swine (M, NS, and NP), and avian viruses (PA and PB2) were first isolated in the United States; they have since become endemic in swine populations . These viruses underwent further reassortment to create additional H3N2 viruses isolated from pigs , as well as H1N2 viruses isolated from pigs , turkey , and wild duck ; this finding demonstrates that viruses containing this gene combination can cross the species barrier. We describe the isolation and characterization of H3N2 influenza viruses from domestic turkeys in the United States. Genetic and serologic characterization showed that these viruses have high homology to each other and to swine influenza viruses recently circulating among pigs in North America.\n\n【2】### Materials and Methods\n\n【3】##### Virus Isolation\n\n【4】In 2003, influenza viruses were isolated from two geographically separate turkey farms: one in the Midwest (in February) and one in the eastern United States (in March). We isolated influenza viruses from each farm by using Madin-Darby canine kidney (MDCK) cells supplemented with 1 μg/mL L-(tosylamido-2-phenyl) ethyl chloromethyl ketone (TPCK)-treated trypsin. Briefly, the samples were add to monolayers of MDCK cells and incubated for 1 h at 37°C to allow viral adsorption to the cells. The inoculum was decanted, Eagle’s minimum essential medium supplemented with 0.2% bovine serum albumin was added, and monolayers were incubated for 3–5 days at 37°C. After cytopathic effects appeared, influenza virus was confirmed by using hemagglutination of chicken erythrocytes and reverse transcription–polymerase chain reaction (RT-PCR) against the HA gene as previously described .\n\n【5】##### Antigenic Analysis\n\n【6】To examine the antigenic relations between the newly isolated viruses and other avian influenza viruses (i.e. their serologic cross-reactivity), we conducted HI assays with a panel of reference antisera against all 15 HA subtypes and swine antisera to recent swine H3N2 viruses; A/Sw/NC/39615/01, A/Sw/MN/23062/02, A/Sw/TX/46710-35/02, A/Sw/TX/46710-37/03, A/Sw/NC/5854/02, and A/Sw/MO/22583/02 **.** To determine the extent of turkey-to-turkey transmission on infected farms, we collected serum from 12 apparently healthy birds in one infected flock and used the HI assay  to test for antibodies to influenza virus surface glycoproteins.\n\n【7】##### DNA Sequencing\n\n【8】Viral RNA was extracted from supernatants of culture of infected cells by using the RNeasy Mini Kit (Qiagen, Chatsworth, CA) according to the manufacturer’s instructions. Reverse transcription and PCR amplification were carried out under standard conditions by using influenza-specific primers . PCR products were purified by using a QIAquick PCR purification kit (Qiagen). Sequencing reactions were performed at the Hartwell Center for Bioinformatics and Biotechnology at St. Jude Children’s Research Hospital.\n\n【9】##### Sequence Analysis\n\n【10】DNA sequences were compiled and edited by using the Lasergene sequence analysis software package (DNASTAR, Madison, WI). Alignment of each influenza virus sequence was created by using the program Clustal\\_X . Rooted phylograms were prepared by using the neighbor-joining algorithm and then plotted by using NJplot . In this study, we used the following regions for phylogenetic analyses: PB2, 10-1262; PB1, 66-1368; PA, 8-1290; HA, 1-1002; NP, 55-962; NA, 41-1123; M, 1-889; and NS, 1-842.\n\n【11】##### Animal Experiments\n\n【12】Each virus was passaged in 10-day-old embryonated chickens eggs before infection of animals. Virus replication in chickens (specific pathogen−free white leghorn broilers, Charles River SPAFAS), quail (Coturnix coturnix_ , B&D Game Farm), turkey (commercial breeding operation), mice (Balb/C, Jackson Laboratories), and outbred pigs (Midwest Research Swine) was measured after intranasal inoculation with virus-infected allantoic fluid containing 10 6  EID 50  (egg infectious dose 50) of virus, as previously described . All animals were 4–5 weeks old, and the numbers of each species used are listed in Table 2 . Tracheal swabs were collected from chickens, quails, and turkeys on postinfection days 3–7, and virus was titrated in 10-day-old embryonated chicken eggs. Injected mice were weighed daily on days 0–7 postinfection, and animals were killed on day 3, 5, and 7 postinfection, and virus in lung tissue was titrated. Four-week-old pigs free of influenza antibody were intranasally infected with each virus (two pigs for each virus), and nasal swabs were collected on days 2–7 postinfection for virus titration in embryonated eggs. On days 5 and 7 postinfection, one of the two pigs injected with each virus was killed and subjected to pathologic examination and titration of virus in organs.\n\n【13】### Results\n\n【14】##### Virus Isolation\n\n【15】Although not always present, clinical signs among turkeys at each farm included depression, coughing, sneezing, loss of appetite, and decreased egg production. Because of the susceptibility of turkeys to H1N1 swine influenza viruses , the flocks had been vaccinated with an autogenous vaccine against the H1N1 subtype. Four influenza viruses were isolated (in MDCK cells) from tracheal swabs of the affected birds. Only one isolate from each farm was selected for characterization, because preliminary antigenic and sequence analyses of the HA genes of isolates showed that all viruses in each farm were identical. These isolates were designated A/turkey/North Carolina/16108/03 (A/Tk/NC/16108/03) and A/turkey/Minnesota/764/03 (A/Tk/MN/764/03).\n\n【16】##### Antigenic and Genetic Characterization\n\n【17】The isolated viruses did not react with any of the reference antisera, even with early  swine antisera. We identified them as H3N2 viruses by using RT-PCR under conditions optimized for swine influenza viruses .\n\n【18】Sequence analysis of the PCR products demonstrated >98% nucleotide identity in each gene segment of the two isolates. According to the Influenza Sequence Database , the greatest sequence similarity was to genes of recent, triple-reassortant swine H3N2 (viruses containing gene segments derived from swine, avian, and human viruses) and H1N2 viruses from the United States . A degree of similarity of >97% between the turkey and swine viruses indicated that interspecies transfer had occurred. This report is, to our knowledge, the first of transmission of swine H3N2 viruses to turkeys.\n\n【19】Phylogenetic analysis of the HA and NA genes of the turkey viruses with those of a number of swine viruses collected in surveillance studies (unpub. data) demonstrated that A/Tk/NC/16108/03 and A/Tk/MN/764/03 are closely related to a swine H3N2 virus isolated in North Carolina in 2002 (A/swine/North Carolina/29974/02) . The GenBank numbers assigned to the sequences determined in this study are AY779253–AY779270.\n\n【20】##### Animal Infections\n\n【21】Serum taken from each of the 12 healthy birds in one of the infected flocks was highly reactive with the turkey H3N2 isolates (HI titer 1,280), demonstrating that transmission had occurred not only between pigs and turkey but also between turkeys. Therefore, these viruses may have had an opportunity to adapt to the avian host. To investigate this possibility, we compared replication of the two turkey isolates with that of A/swine/NC/29974/02 (the most genetically similar swine virus) and that of A/swine/TX/4199-2/98 (the index triple-reassortant H3N2 virus) in chickens, quail, turkey, mice, and pigs. Animals were infected intranasally with 10 6  EID 50  of the respective influenza viruses, and virus was titered in lung homogenates through 7 days after infection . All viruses were recovered from pig and mouse lung homogenates (log 10  EID 50  /gram of tissue); this titer demonstrated that viruses of the swine H3N2 triple-reassortant lineage could replicate in pigs and mice without adaptation. None of the viruses were recovered from infected chickens. Only the turkey isolates were reproducibly recovered from the lung homogenates of turkey and quail. A/swine/NC/29974/02, the most genetically similar turkey virus, was isolated only from one of four infected turkeys on day 3 postinfection. Therefore, the turkey isolates appear to be better adapted to avian hosts than do their closest swine counterparts, but certain swine H3N2 viruses also have the potential to replicate in turkeys.\n\n【22】### Discussion\n\n【23】We present the isolation of swinelike H3N2 influenza viruses from two geographically distinct turkey farms in the United States. These viruses are closely related to swine H3N2 viruses that emerged in pigs in the United States in 1998 and have since become endemic. Although infection of turkeys with H1N1 and H1N2 swine influenza viruses has been documented , to our knowledge, this report is the first of swinelike H3N2 infection in this host. The clinical signs of infection in these turkeys were not severe, but our findings have implications for influenza ecology and the possibility of further evolution of these viruses.\n\n【24】Two properties of the swine-like H3N2 infection in turkeys raise concerns about the potential for further viral evolution. The first is that the H3N2 viruses have been successfully established in pigs and have demonstrated an ability to reassort with human , swine , and avian viruses . The second is that turkeys appear to be highly susceptible to infection by influenza viruses from aquatic birds , thus increasing opportunities for further reassortment. Although the fitness of such a swine-avian reassortant cannot be predicted, and the infected flocks showed no evidence of coinfection with avian viruses, continuing to monitor turkey populations for the presence of swine virus-like gene segments is prudent.\n\n【25】We found strong evidence that turkey-to-turkey transmission had occurred in at least one of the flocks, and the viruses were similar despite their geographically distinct origins. Although detailed information is not available, there are no obvious links between the infected flocks, and direct movement of turkeys between the two farms was unlikely. Further epidemiologic investigation of the flocks and surrounding swine herds is warranted. In a small scale retrospective screen of 125 turkey serum samples collected from 2000 to 2004, we were unable to detect any evidence of H3N2 infection using standard HI assays (data not shown), which suggests that such infections are not widespread. Although the most obvious explanation for the dual outbreak is a common source of infection, both flocks may have been infected by a swine virus circulating in pigs in both areas.\n\n【26】Although we recovered one swine isolate genetically similar to the turkey isolates from one of 4 infected turkeys, the ability of only the turkey isolates to replicate in quails suggests that the viruses may have already begun to adapt to the avian host. This adaptation to an avian host did not occur at the expense of the ability to replicate in pigs. Both turkey isolates were shed by experimentally infected swine for at least 5 days; this shedding pattern is consistent with that of genetically similar swine isolates. To speculate that the turkey isolates can replicate in quail and, possibly, other avian hosts as a result of their passage in turkeys is tempting. However, the traits that allowed the viruses to be transmitted to turkeys may also allow them to infect quail. The adaptation did not appear to be a result of egg passage, as we found no differences in HA sequence between MDCK and egg-grown virus. To investigate the possible mechanisms of avian adaptation, we compared the amino acid sequences of the HA1 proteins of the turkey isolates and the swine H3N2 viruses. The turkey isolates and A/swine/NC/29974/02 differed at only seven amino acids, two of which (residues 137 and 226) were in the receptor binding site. Although we found substitutions at these two sites in other swine isolates represented in sequence databases (data not shown), only the turkey isolates had both the Y137S and V226I substitutions. Ongoing reverse genetics experiments will help to identify the relative contributions of these amino acids to avian adaptation.\n\n【27】The introduction of H3N2 viruses to the U.S. swine population has significantly affected patterns of animal influenza in this country. Not only have the H3N2 viruses successfully established and reassorted in pigs, but H1N1 , H1N2 , and now H3N2 isolates carrying the internal genes of H3N2 viruses have been isolated from birds. Viruses of this genetic composition will likely continue to evolve and cause problems for animal and, potentially, human health. The swine population is likely a reservoir of yet another lineage of influenza viruses that have demonstrated the ability to reassort and be transmitted between species.\n\n【28】Repeated introductions of swine influenza viruses to turkeys, which may be coinfected with avian influenza viruses, provide opportunities for the emergence of novel reassortants with genes adapted for replication in pigs or even humans. Our studies emphasize the continuing need to monitor pigs and domestic birds to better understand interspecies transmission and the emergence of novel influenza viruses that have the potential to infect humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1f5cdc54-0426-4130-9c94-78eb59243b14", "title": "Rat-to-Elephant-to-Human Transmission of Cowpox Virus", "text": "【0】Rat-to-Elephant-to-Human Transmission of Cowpox Virus\n**To the Editor:** Despite the eradication of smallpox in the past century, other orthopoxviruses, such as monkeypox virus, vaccinia virus in Brazil, and cowpox virus (CPXV) in Europe , still infect humans. CPXV has been restricted to the Old World with wild rodents as its natural reservoir . Human CPXV infections are commonly described in relation to contact with diseased domestic cats, rarely directly from rats . Human infections usually remain localized and self-limiting but can become fatal in immunosuppressed patients . CPXV infections in captive exotic animals have been reported to be transmitted by rodents .\n\n【1】In February 2007, a circus elephant (Elephas maximus_ ) in northern Germany exhibited disseminated ulcerative lesions of the skin and mucosal membranes  caused by CPXV infection; the elephant was euthanized after treatment attempts failed. Electron micrographs of negative-stained biopsy specimens of tongue lesions showed orthopoxvirus particles. The presence of orthopoxvirus after routine virus isolation in Hep2 cells was confirmed in direct immunofluorescence assay with orthopox-specific antibodies. The morphologic feature of hemorrhagic pocks on the chorioallantoic membrane (CAM) of infected embryonated hen’s eggs indicated CPXV. This finding was confirmed by sequence analysis of the complete hemagglutinin (HA) open reading frame (ORF), which showed 99% homology of 921 bp to CPXV isolated in 1984 from an elephant in Hamburg, Germany . A serum sample was drawn from the elephant 2 weeks before euthanasia. An indirect fluorescent antibody test (IFAT) detected immunoglobulin (Ig) G antibodies against the new corresponding elephant virus isolate (termed CPXV GuWi) with a titer of 1,260. According to the owner, the >40-year-old female elephant had never been vaccinated with vaccinia virus.\n\n【2】Eight days after the elephant’s death, a circumscribed lesion developed on the back of the right hand of a 19-year-old immunocompetent, healthy, unvaccinated animal keeper. CPXV was isolated from lesion fluid and was found to be homologous by using the HA ORF to CPXV GuWi. A convalescent-phase serum sample from the keeper taken 3 weeks later showed a significant increase in IgM (from 40 to 2,560), IgG (from 20 to 10,240), and neutralizing antibody (from <5 to 80) titers.\n\n【3】Further simultaneous investigations were undertaken to determine the source of infection. Because no felids were kept on the circus premises, the focus centered on wild rodents that had propagated and infested the area because of the mild winter. Six days after the elephant’s death, 4 rats (Rattus norvegicus_ ) were caught and tested for orthopoxvirus antibodies. Although none of the rats had epidermal lesions or other pathologic changes indicative of a poxvirus infection, all were tested by IFAT and found to be serologically positive (IgG titers 40, 320, 2,560, and >10,240; IgM titers <5, <5, 160, and 2,560), indicating a recent infection in at least 2 animals. CPXV-typical pock morphologic features on the CAM could be visualized after homogenized liver and spleen of the animal with the highest titer was passaged 3 times. Infected CAM and original organ tissues (liver and spleen) showed CPXV by PCR and subsequent sequencing. The corresponding HA ORF displayed perfect homology to the viruses isolated from the elephant and the animal keeper.\n\n【4】We report CPXV infection in humans transmitted from an elephant, with rats as a probable source of the elephant’s infection . Although the animal keeper was infected by direct contact with the elephant, the exact transmission route from rat to elephant remains unclear. Nevertheless, rats have proven to be a natural reservoir for CPXV , and infections persisting for >3 weeks were shown for other rodents . No data about CPXV prevalence in rats are available, and no data for CPXV isolates from rats have been published in Germany. Therefore, further studies on rats as CPXV reservoir are needed to estimate the potential risk for infection among humans and exotic animals. Zoo and circus animals, especially elephants, seem to be highly susceptible to generalized CPXV infections. Although modified vaccinia virus Ankara was authorized in Germany to be used in vaccinating exotic animals , this case highlights the need for increased efforts toward general vaccination of potentially susceptible exotic animals in Europe.\n\n【5】The sequence identity of the HA ORFs also demonstrates a low mutation rate of CPXV after it crosses species barriers. As the Figure , **panel B** , infers, there is a phylogenetic difference between CPCV GuWi and CPXV from a human patient living in the same geographic area (CPXV #2), which indicates the cocirculation of >1 CPXV variant . Considering the extremely high virus load in infected animals, the broad host range of CPXV, and the abandoned vaccination against smallpox, this case emphasizes the risk among humans of acquiring CPXV infection . It also highlights the need for increased awareness regarding clinical features of orthopoxvirus infections and the importance of developing new antiviral drugs against orthopoxviruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d0634ddc-6381-49e5-ac89-d118b837a933", "title": "Fulminant Supraglottitis from Neisseria meningitidis", "text": "【0】Fulminant Supraglottitis from Neisseria meningitidis\n**To the Editor:** A 68-year-old Caucasian woman with non–insulin-dependent diabetes mellitus, hypertension, and peripheral vascular disease sought treatment at an emergency department after experiencing 2 days of pharyngitis and 1 day of fatigue and dysphagia for solid food. The morning of admission she noted dysphagia for solid food and liquids, dysphonia, severe anterior neck pain, swelling, and erythema, dyspnea, and a temperature of 102.3°F (°C). A computed tomographic (CT) scan demonstrated substantial neck soft tissue edema and narrowing of the oropharynx and hypopharynx. She received single doses of intravenous ampicillin/sulbactam, clindamycin, dexamethasone (mg), and methylprednisolone (mg) before being evacuated by air to our intensive care unit (ICU) at Walter Reed Army Medical Center. Intravenous ampicillin/sulbactam, 3 g every 6 hours, and clindamycin, 900 mg every 8 hours, were continued after the transfer. Two doses of intravenous vancomycin, 1 g every 12 hours, were given before vancomycin was discontinued. Results of laboratory studies were the following: leukocyte count 13.3/mm 3  (% polymorphonuclear leukocytes, 18% bands) and normal hematocrit, platelet count, blood urea nitrogen and creatinine concentrations, and liver-associated enzymes.\n\n【1】A marker pen was used to track the rapid advance of erythema overnight from her anterior, inferior chin to the top of her breasts . The infectious disease service was consulted the next morning. When she was examined, her condition had improved; she had normal vital signs, a slightly hoarse voice, and the ability to swallow some saliva. She had no headache or meningismus. The chest erythema was receding. Oral examination demonstrated erythema and an abrasion in the posterior pharynx. Her tongue was not elevated and her uvula was midline. Anterior firm edema without crepitus extended from her chin to the mid-neck. Results of her examination were otherwise unremarkable. The infectious disease consultant recommended restarting a course of vancomycin and discontinuing clindamycin.\n\n【2】A follow-up CT scan with contrast demonstrated anterior cervical soft tissue edema and patent airway with surrounding abnormal thickness and soft tissue density. No abscess or clot was seen. Endoscopic examination in the ICU showed diffuse erythema and generalized supraglottic edema affecting mostly the epiglottis and arytenoids. Dental examination demonstrated no acute pathologic features. Blood cultures at our hospital yielded no growth, and throat culture was negative for group A streptococci.\n\n【3】The patient recovered without requiring intubation . On the day of discharge, a blood culture from the referring hospital’s emergency department was reported to be positive for _Neisseria meningitidis_ , serogroup Y. Immediate family members and the otolaryngologists who conducted the endoscopic examination were given postexposure prophylaxis. The patient also received terminal prophylaxis. Results of a screening CH50 for terminal complement deficiency were normal.\n\n【4】This patient’s condition is consistent with fulminant meningococcal supraglottitis. Supraglottitis implies involvement of the epiglottis and surrounding structures and is more commonly used to describe adult infection than is epiglottitis . Epiglottitis has become more common in adults than in children since the introduction of the _Haemophilus influenzae_ type b vaccine. Other organisms responsible for epiglottitis in adults include _H. influenzae_ , _H. parainfluenzae_ , pneumococci, _Staphylococcus aureus_ , and group A streptococci .\n\n【5】Despite its propensity to colonize the upper respiratory tract, _N. meningitidis_ has rarely been identified as a cause of supraglottitis or other deep neck infections. Only 6 cases have been reported, the first in 1995 . Previously reported cases were equally apportioned by sex, and patients were 44 to 95 years of age . Including our patient, 3 of 7 were diabetic . None showed evidence of meningitis or fulminant meningococcemia, but all had fever, pharyngitis, and airway compromise. Five required airway intervention: 3 intubations and 2 urgent tracheostomies. Two received steroids , a 54-year-old man required urgent tracheostomy before receiving steroids, and a 60-year-old man’s condition “deteriorated rapidly,” but the report does not indicate the interval between receipt of steroids and intubation. Although steroids have been used, their benefit is unproven, and no controlled clinical trials have been conducted .\n\n【6】Blood cultures have been positive from all reported case-patients. Two isolates were typed as serogroup B, 4 as serogroup Y, and the serotype of 1 was unreported. Meningococcal strains causing supraglottitis appear to be more locally aggressive but cause less disseminated disease, possibly due to decreased tropism for endothelial cells .\n\n【7】To our knowledge, ours is the second case of meningococcal supraglottitis reported with severe neck edema and cellulitis; a 44-year-old woman in a prior review had features similar to our patient . An 81-year-old woman with diabetes was noted to have “reddish swelling” on the right side of the neck , but little was described beyond that. All 3 had serogroup Y infection. We wondered whether serogroup Y might have a propensity to cause cellulitis; however, a review of 10 cases of meningococcal cellulitis included patients with multiple serogroups: C (cases), B (cases), Y (cases), and unknown (cases) .\n\n【8】_N. meningitidis_ may cause supraglottitis more frequently than is recognized . Timely drawing of blood cultures in relation to administration of antimicrobial drugs is most likely to identify this pathogen in this setting. Because of its public health implications and potential for rapid progression to airway compromise, _N. meningitidis_ should be considered among the differential diagnoses of supraglottitis/epiglottitis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ab5014be-82a6-4115-9eff-8d6ac313ba9a", "title": "Who Would Pay for State Alcohol Tax Increases in the United States?", "text": "【0】Who Would Pay for State Alcohol Tax Increases in the United States?\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Despite strong evidence that increasing alcohol taxes reduces alcohol-related harm, state alcohol taxes have declined in real terms during the past 3 decades. Opponents of tax increases argue that they are unfair to “responsible” drinkers and those who are financially disadvantaged. The objectives of this study were to assess the impact of hypothetical state alcohol tax increases on the cost of alcohol for adults in the United States on the basis of alcohol consumption and sociodemographic characteristics.\n\n【3】**Methods**\n\n【4】The increased net cost of alcohol (ie, product plus tax) from a series of hypothetical state alcohol tax increases was modeled for all 50 states using data from the 2011 Behavioral Risk Factor Surveillance System, IMPACT Databank, and the Alcohol Policy Information System. Costs were assessed by drinking pattern (excessive vs nonexcessive) and by sociodemographic characteristics.\n\n【5】**Results**\n\n【6】Among states, excessive drinkers would pay 4.8 to 6.8 times as much as nonexcessive drinkers on a per capita basis and would pay at least 72% of aggregate costs. For nonexcessive drinkers, the annual cost from even the largest hypothetical tax increase ($0.25 per drink) would average less than $10.00. Drinkers with higher household incomes and non-Hispanic white drinkers would pay higher per capita costs than people with lower incomes and racial/ethnic minorities.\n\n【7】**Conclusion**\n\n【8】State-specific tax increases would cost more for excessive drinkers, those with higher incomes, and non-Hispanic whites. Costs to nonexcessive drinkers would be modest. Findings are relevant to developing evidence-based public health practice for a leading preventable cause of death.\n\n【9】Introduction\n------------\n\n【10】Excessive alcohol consumption is the fourth leading preventable cause of death in the United States, where it is responsible for approximately 88,000 deaths annually, nearly 70% of which involve adults aged 20 to 64 years . Excessive alcohol consumption is also a risk factor for unintentional injuries, violence, liver disease, stroke, dementia, hypertension, several types of cancer, sexual assault, fetal alcohol spectrum disorders, and alcohol use disorders . In addition, excessive alcohol use cost the United States $249 billion in 2010, or approximately $2.05 per drink, and about $2.00 of every $5.00 were paid by government .\n\n【11】Although alcohol taxes have been used to raise revenue, part of their historical justification has also been their public health benefit . Among alcohol policy interventions, evidence of effectiveness in reducing excessive drinking and related harms is the strongest, most complete, and consistent for raising alcohol taxes . Meta-analyses find that higher alcohol taxes reduce excessive drinking as well as alcohol-related injuries, diseases, and death . Increasing alcohol taxes is recommended by the Community Preventive Services Task Force to reduce excessive alcohol use and related harms .\n\n【12】Although some states apply value-based alcohol taxes (ie, taxes based on a percentage of price) , all 50 states levy volume-based excise taxes (ie, taxes based on a fixed dollar amount per unit volume) as their principal form of taxation. Because excise taxes are not based on a percentage of the price of alcohol, inflation erodes their value unless they are increased. As of 2009, the average state beer tax had declined by approximately 70% in “real dollar” (ie, inflation-adjusted) terms since 1970, and the inflation-adjusted federal beer tax had declined by approximately 40% since its last increase in 1991 . In addition, current alcohol taxes do not cover alcohol-related costs. In 2006, the combined federal and state taxes on alcoholic beverages were approximately $0.12 per drink, while the median cost to state governments (in the form of health care expenditures, criminal justice system costs, etc.) was $0.78 per drink .\n\n【13】Despite their effectiveness and potential to generate government revenue to offset costs, increased taxes encounter robust political opposition. Principal concerns raised by opponents of alcohol tax increases are that 1) alcohol taxes are unfair to ”responsible” (ie, nonexcessive) drinkers; and 2) the cost of alcohol tax increases are disproportionately paid by those who are economically disadvantaged . Although it is true that alcohol taxes are not applied differentially based on a consumer’s drinking pattern or income, over 90% of adult excessive drinkers binge drink (consume ≥4 drinks per occasion for women, and ≥5 drinks per occasion for men), over half of the alcohol consumed by US adults is in the form of drinking binges, and the prevalence of alcohol consumption and binge drinking increases with household income . These facts suggest that the counterarguments may be invalid.\n\n【14】Because these counterarguments have not been empirically addressed at the state level, and because they are an important obstacle to maintaining or increasing state alcohol taxes, the objective of this study was to assess the impact of a series of hypothetical state alcohol tax increases on the total cost of alcohol (product plus tax) to adults in all 50 US states on the basis of alcohol consumption and sociodemographic characteristics. Information from this study is important for state policy makers and citizens and to inform evidence-based public health practice for treating a leading preventable cause of death in the United States.\n\n【15】Methods\n-------\n\n【16】State-specific data on alcohol consumption were obtained from the 2011 Behavioral Risk Factor Surveillance System (BRFSS).cdc.gov/brfss/technical\\_infodata/quality.htm. In brief, the BRFSS is a state-based random–digit-dial telephone survey of people aged 18 years or older that is conducted monthly in all states, the District of Columbia (DC), and 3 territories and coordinated by the Centers for Disease Control and Prevention (CDC). The BRFSS core alcohol questions (ie, questions asked of all drinkers in all states) were used to assess drinking frequency, average number of drinks consumed during drinking days, binge drinking frequency, and binge drinking intensity (ie, average largest number of drinks consumed on any occasion) within the past 30 days among US adults aged 18 or older. Weighted data from the 50 US states and Washington, DC, were used in this study.\n\n【17】Excessive drinking was defined as binge drinking, heavy drinking, or any alcohol consumption by respondents aged 18 to 20 years of age (ie, adults under the minimum legal drinking age). Binge drinking was defined as consuming 5 or more drinks for men or 4 or more drinks for women on 1 occasion or more . Heavy drinking was defined as consuming an average of more than 2 drinks per day for men or more than 1 drink per day for women. Average daily alcohol consumption was calculated by multiplying the number of drinking days in the past 30 days by the usual number of drinks consumed during drinking days and then dividing by 30 . Nonexcessive drinkers were those who consumed alcohol in the past 30 days but were not classified as excessive drinkers.\n\n【18】The net cost of alcohol tax increases was calculated on the basis of change in the retail price of alcohol that would be expected to occur following a hypothetical tax increase. Because state-specific data about alcohol prices do not exist, we used national data from IMPACT Databank , which reports the average on- and off-premises price for beer, wine, and spirits, respectively. The average price of a standard drink in all 50 states and Washington, DC, was calculated by using beverage-specific price data from IMPACT Databank, weighted by state-specific data on beverage-specific per capita alcohol consumption using data from the Alcohol Epidemiologic Data System . Our model assumed that 75% of the alcohol sold in the United States is purchased in off-premises establishments (eg, liquor stores, grocery stores, and other retail locations) . This assumption enabled us to calculate weighted cost per drink in each state.\n\n【19】The state-specific tax-per-drink was calculated by combining all state taxes (ie, excise, ad valorem, and sales taxes) for beer, wine, and spirits to determine a weighted average of state taxes paid per drink . State-specific tax rates for 2011 were obtained from the Alcohol Policy Information System, administered by the National Institute on Alcohol Abuse and Alcoholism . Among states with monopolies on wholesaling or retailing (or both) of spirits or wine (“control states”), there are no data about effective tax rates on monopolized beverages because states apply a series of markups on the retail price of the controlled beverage(s), and in some cases, the markup procedures are not transparent. However, in states with exclusively private retailing of spirits or wine (“noncontrol states”), there is a strong correlation between the taxes paid for a drink of beer and the average tax paid for a drink of any type . Using these correlation data and beer tax data from control states, we built a linear regression model to interpolate the average tax for monopolized beverages in control states.\n\n【20】The total cost per drink was calculated by summing the average price and average tax per drink in each state. Once the cost per drink was established, a revised cost per drink (product plus tax) was calculated for each of the series of hypothetical tax increases. The expected reduction in alcohol consumption following tax-related price increases was calculated by using beverage-specific estimates of price elasticity from a recent meta-analysis . Assuming a 100% pass-through of the tax to the price paid by consumers, we calculated the expected decreases in consumption for tax increases of $0.05, $0.10, and $0.25 per drink, and following a 5% increase in the retail price of a drink. After factoring in the impact of the price increase on average daily consumption, annual alcohol consumption was calculated by multiplying the average number of drinks per day by 365. The net cost of the hypothetical alcohol tax increase for individuals was then calculated by subtracting current annual expenditures from the total annual expenditures anticipated after each of the hypothetical tax increases. Per capita and aggregate net cost increases were then assessed among subgroups on the basis of alcohol consumption patterns (excessive, nonexcessive, nondrinkers), annual household income (<$25,000, $25,000–<$50,000, $50,000–<$75,000, ≥$75,000), employment status (employed for wages, nonemployed \\[ie, unemployed, self-employed, homemakers, students, people who are retired, and people who are unable to work\\]), and race/ethnicity (non-Hispanic white, other).\n\n【21】Results\n-------\n\n【22】In 2011, 55.1% of US adults reported drinking in the past 30 days. Overall, 33.6% of US adults (.9% of current drinkers) were classified as nonexcessive drinkers, and 21.5% of US adults (.1% of current drinkers) were classified as excessive drinkers . Table 1 also illustrates the characteristics of nonexcessive and excessive drinkers in terms of sociodemographic factors and the baseline number of drinks consumed annually among those groups. Among nonexcessive drinkers, for example, those with the highest incomes and non-Hispanic whites constituted the largest proportion of the population and also consumed more drinks annually on average.\n\n【23】Among all adults in US states, excessive drinkers would pay about 5 times as much in additional net per capita costs (product plus tax) as nonexcessive drinkers for hypothetical alcohol tax increases, and nonexcessive drinkers would pay less than $10 annually for the largest ($0.25 per drink) tax increase . For example, a $0.05 per drink tax increase would increase the state average annual cost of alcohol by $12.56 for excessive drinkers compared with $2.32 for nonexcessive drinkers, and a $0.25 per drink tax increase (the largest increase) would increase the average annual cost of alcohol by $49.49 for excessive drinkers compared with $9.15 for nonexcessive drinkers.\n\n【24】By sociodemographic characteristics, average per capita costs among US adults (including nondrinkers) from tax increases would be higher among people with higher incomes, employed people, and non-Hispanic whites . For example, after a tax increase of $0.25 per drink, people earning less than $25,000 would pay an average additional cost of $11.64 per year, while those earning $75,000 or more would pay an additional $16.98 per year. Similarly, following this same hypothetical tax increase, those who are employed would pay more on average than nonemployed people ($16.15 vs $10.35 annually), and non-Hispanic whites would pay more on average than people of other racial/ethnic groups ($14.75 vs $10.96).\n\n【25】Among current drinkers, stratified by drinking group and sociodemographic characteristics , nonexcessive drinkers would pay little in increased per capita costs; for example, the highest average per capita annual cost in any stratum of income, employment, and racial/ethnicity was $10.31 for the largest tax increase ($0.25 per drink). In addition, excessive drinkers would pay 4.8 to 6.8 times more in per capita average annual costs than would nonexcessive drinkers. In terms of aggregate costs, all strata of nonexcessive drinkers would pay less than 30% of costs compared with their corresponding groups of excessive drinkers, although there are more nonexcessive drinkers than excessive drinkers.\n\n【26】By income, per capita costs were lower for those with less income, and the percentage of aggregate costs paid by nonexcessive drinkers as a group was lower among those earning less income (eg, those earning <$25,000 would pay 13.8% of the aggregate costs, while those earning $75,000 or more annually would pay 28.5% of costs) . Among nonexcessive drinkers, the average annual per capita cost of tax increases was similar by employment status, but in aggregate, the nonemployed paid only 25.0% of costs. People from racial/ethnic minorities would also pay less in annual per capita costs and a lower proportion of aggregate costs (.8%, state range 16.7%–22.1%) than would non-Hispanic whites (.8%, range 20.1%–26.3%).\n\n【27】By state, the average annual increase in per capita costs to nonexcessive drinkers from even the smallest tax increase ($0.05 per drink) did not exceed $3.00 . Similarly, the increase in per capita costs from the largest tax increase ($0.25 per drink) did not exceed $12.00 in any state. In contrast, excessive drinkers would pay from 4.1 (District of Columbia and Tennessee) to 7.6 (Arkansas) times more in average annual per capita costs than nonexcessive drinkers. Nonexcessive drinkers in Indiana had the lowest average annual increase in costs, while those in Maine had the highest average annual increase in costs. The proportion of total costs paid by nonexcessive drinkers as a group ranged from 16.5% (Hawaii, Utah) to 31.3% (Tennessee).\n\n【28】Discussion\n----------\n\n【29】On a per capita basis, nonexcessive drinkers would typically pay less than $10.00 annually in average net costs for the largest hypothetical state alcohol tax increase ($0.25 per drink) and less than $2.50 annually for the smallest tax increase ($0.05 per drink). Furthermore, excessive drinkers would pay more than 5 times as much as nonexcessive drinkers in per capita increased costs. As a group, excessive drinkers would also pay approximately three-quarters of the aggregate costs, even though nonexcessive drinkers outnumber excessive drinkers.\n\n【30】Because excessive drinkers account for most alcohol-related harms in the United States , their increased cost relative to nonexcessive drinkers appears justifiable. That excessive drinkers would pay most of the increased cost for state alcohol tax increases is not surprising and reflects the skewed distribution of alcohol consumption in the United States. More than half of the alcohol consumed by US adults is in the form of binge drinks, and binge drinkers were responsible for about three-quarters of the $249 billion in economic costs due to excessive drinking in the United States in 2010 .\n\n【31】Members of low-income households and racial/ethnic minorities, including those who are nonexcessive drinkers, would also pay less in per capita and aggregate costs following tax increases compared with non-Hispanic whites or those with higher incomes. These findings contradict the belief that those who are economically disadvantaged would bear most of the costs from alcohol tax increases.\n\n【32】The finding that the cost of state alcohol tax increases would be similar for excessive drinkers in lower and higher income groups is consistent with the results of other studies . However, increasing the price of alcohol might be a particularly effective way to reduce the risk of alcohol-related harms in low-income populations. Binge drinking intensity (the number of drinks consumed during a binge drinking episode) is higher among low-income groups compared with high-income groups , and proportionate reductions in consumption among those consuming on the “steep” portion of the risk curve would yield larger reductions in risk relative to those who drink less.\n\n【33】This research has limitations. First, because there are no adequate state-specific estimates of alcohol prices in the United States, national estimates of prices were added to state-specific taxes to estimate state prices for alcohol. Therefore, we may have overestimated the increase in net costs to drinkers in states with alcohol prices that are below the national estimates, because these states would have both a lower pre-tax price on alcohol and experienced a greater reduction in consumption due to higher percentage increases in post-tax retail prices. Second, we used beverage-specific price elasticities from a meta-analysis . Price elasticities may vary on the basis of many factors (age, income, drinking pattern); however, across the literature, estimates are inconsistent and there has been no systematic review or meta-analysis of tax elasticities on the basis of these factors. Were the price elasticity for alcohol more negative for nonexcessive drinkers compared with excessive drinkers, excessive drinkers would have paid more in per capita and aggregate costs from alcohol tax increases because their drinking would have been relatively less affected by price increases. Similarly, were the price elasticity more negative for low-income persons, high-income persons would have paid relatively more in cost increases. Third, it is possible that those who drink more may purchase cheaper alcohol, so that a given excise tax increase may represent a greater percentage increase in cost and a larger decrease in consumption compared with those who drink less when price elasticity is applied .\n\n【34】Surveys based on self-report are subject to nonresponse bias, and alcohol consumption may be underreported among respondents . Therefore, our cost estimates are likely to be conservative, particularly for excessive drinkers who are particularly prone to underreporting their consumption and who may also be less likely to participate in surveys or other alcohol studies . Because this was a study of the change in the total cost of alcohol (product plus tax) due to tax increases, it is important to note that total cost increases are less than the amount paid in additional taxes because total costs are offset by the fact that consumers would purchase less alcohol in response to increased prices. However, reporting net cost is preferable because it represents the “bottom line” in terms of how much more consumers would pay for alcohol from tax increases.\n\n【35】Although many evidence-based strategies exist for reducing excessive alcohol use, a recent study found that a few policies that affect the price and availability of alcoholic beverages, including higher alcohol taxes and controls on alcohol outlet density, have the greatest impact on binge drinking among US adults . Raising alcohol taxes could also reduce the gap between alcohol-related costs incurred by state governments versus the revenues derived from state alcohol taxes.\n\n【36】State-specific tax increases would cost more for excessive drinkers, those with higher incomes, and non-Hispanic whites. Costs to nonexcessive drinkers would be modest. This study may be helpful in educating policy makers and the public about the distribution of costs from state alcohol tax increases and can inform debate about the promulgation of evidence-based public health practices to reduce a leading preventable cause of death and social problems in the United States.\n\n【37】Tables\n------\n\n【38】#####  Table 1. **Population Distributions and Mean Number of Drinks Consumed at Baseline, by Drinking Status and Sociodemographic Characteristics, US Adults Aged =18 Years, Behavioral Risk Factor Surveillance System, 2011**  \n\n| Sociodemographic Characteristics | Nonexcessive Drinkers a (n = 73,480,718) | Excessive Drinkers b (n = 47,200,442) | Nondrinkers c (n = 98,262,571) |\n| --- | --- | --- | --- |\n| Population Proportion, % | Mean No. of Drinks Per Year | Population Proportion, % | Mean No. of Drinks Per Year | Population Proportion, % |\n| --- | --- | --- | --- | --- |\n| **All respondents** | 100 | 128.0 | 100 | 689.1 | 100 |\n| **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** |\n| <25,000 | 19.5 | 102.1 | 23.8 | 689.5 | 41.0 |\n| 25,000–<50,000 | 25.3 | 124.6 | 25.1 | 722.1 | 27.3 |\n| 50,000–<75,000 | 18.4 | 128.7 | 16.9 | 678.6 | 13.3 |\n| \\=75,000 | 37.0 | 144.5 | 34.1 | 689.4 | 18.4 |\n| **Employment status d** | **Employment status d** | **Employment status d** | **Employment status d** | **Employment status d** | **Employment status d** |\n| Employed | 57.3 | 125.9 | 66.5 | 696.3 | 44.6 |\n| Nonemployed | 42.7 | 131.6 | 33.5 | 674.0 | 55.4 |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 87.8 | 133.4 | 85.3 | 703.8 | 73.4 |\n| Other e | 12.2 | 105.8 | 14.7 | 632.8 | 26.6 |\n\n【40】a  Nonexcessive drinkers (.6% of population, 60.9% of drinkers) were defined as respondents who consumed alcohol during the past 30 days but did not binge drink (=5 drinks for men or =4 drinks for women on at least one occasion), was not categorized as a heavy drinker (daily average of >2 drinks for men or >1 drink for women \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), and aged 21 years or older.  \nb  Excessive drinkers (.5% of population, 39.1% of drinkers) were defined as respondents who during the past 30 days either engaged in binge drinking (=5 drinks for men or =4 drinks for women on at least one occasion), was categorized as a heavy drinker (daily average of >2 drinks for men or >1 drink for women \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), or aged 18–20 years and reported drinking any alcohol.  \nc  Nondrinkers were 44.9% of the population.  \nd  Employment for wages was self-reported. Those who were nonemployed were unemployed, self-employed, homemakers, students, retired, or unable to work.  \ne  “Other” racial/ethnic categories were non-Hispanic black, Hispanic white, Asian, or other race/ethnicity.\n\n【41】#####  Table 2. **Average Annual Per Capita Cost a  of 4 Alcohol Tax Increases, by Drinking Status, Income, Employment Status, and Race/Ethnicity, US Adults Aged =18 Years, b  Behavioral Risk Factor Surveillance System, 2011**  \n\n| Characteristics | Mean Annual Per Capita Net Cost in US $ (state minimum, state maximum) |\n| --- | --- |\n| $0.05 per drink increase | $0.10 per drink increase | $0.25 per drink increase | 5% increase in drink price |\n| --- | --- | --- | --- |\n| **Alcohol consumption** | **Alcohol consumption** | **Alcohol consumption** | **Alcohol consumption** | **Alcohol consumption** |\n| Excessive c | 12.56 (.42–6.39) | 23.79 (.69–31.13) | 49.49 (.67–65.42) | 19.38 (.37–25.58) |\n| Nonexcessive d | 2.32 (.98–2.96) | 4.40 (.75–5.64) | 9.15 (.7–12.00) | 3.59 (.97–4.91) |\n| Nondrinker | NA | NA | NA | NA |\n| **Household income, $** | **Household income, $** | **Household income, $** | **Household income, $** | **Household income, $** |\n| <25,000 | 2.95 (.97–4.64) | 5.59 (.86–8.81) | 11.64 (.95–18.39) | 4.55 (.58–7.21) |\n| 25,000–<50,000 | 3.48 (.19–4.69) | 6.60 (.17–8.89) | 13.74 (.78–18.48) | 5.37 (.11–6.80) |\n| 50,000–<75,000 | 3.60 (.52–5.31) | 6.83 (.69–9.99) | 14.19 (.09–20.20) | 5.55 (.37–8.29) |\n| \\=75,000 | 4.31 (.44–5.28) | 8.16 (.50–9.98) | 16.98 (.39–20.61) | 6.68 (.16–9.31) |\n| **Employment status** | **Employment status** | **Employment status** | **Employment status** | **Employment status** |\n| Employed for wages e | 4.10 (.70–5.01) | 7.77 (.12–9.43) | 16.15 (.58–19.12) | 6.33 (.10–7.34) |\n| Unemployed f | 2.62 (.15–4.17) | 4.97 (.20–7.91) | 10.35 (.66–16.51) | 4.05 (.86–6.48) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 3.75 (.50–4.37) | 7.10 (.73–8.55) | 14.75 (.83–17.77) | 5.77 (.8–6.86) |\n| Other g | 2.77 (.97–3.33) | 5.26 (.75–6.33) | 10.96 (.82–13.20) | 4.31 (.07–5.19) |\n\n【43】Abbreviation: NA, not applicable.  \na  The net cost is the change in the amount a person would spend on alcohol (product plus tax) as a result of a hypothetical tax increase. This table includes all US adults, so average per capita costs are calculated by dividing costs among drinkers and nondrinkers in each group.  \nb  All US adults, including nondrinkers.  \nc  An excessive drinker was defined as a respondent who during the past 30 days either engaged in binge drinking (=5 drinks for men or =4 drinks for women on at least one occasion), was categorized as a heavy drinker (daily average of >2 drinks for men or >1 drink for women, \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), or who was aged 18–20 years and reported drinking any alcohol.  \nd  A nonexcessive drinker was defined as a respondent who consumed alcohol during the past 30 days but did not binge drink (=5 drinks for men or =4 drinks for women on at least one occasion), was not categorized as a heavy drinker (daily average of >2 drinks for men or >1 drinks for women, \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), and who was aged 21 years or older.  \ne  Self-reported.  \nf  Self-reported; includes self-employed, unemployed, homemaker, student, retired, or unable to work.  \ng  Other race/ethnicity includes non-Hispanic black, Hispanic white, Asian, and other racial/ethnic group.\n\n【44】#####  Table 3. **Average Annual Per Capita Net Cost a  for Nonexcessive Drinkers, b  by Sociodemographic Factors, Excessive c  Drinker: Nonexcessive Drinker Per Capita Cost Ratio, and Proportion of Total Costs Paid by Nonexcessive Drinkers, US Adults Aged =18 Years, Behavioral Risk Factor Surveillance System, 2011**  \n\n| Sociodemographic characteristics | Mean Annual Per Capita Net Cost From Alcohol Tax Increases for Nonexcessive Drinkers (State Minimum, State Maximum) | Excessive Drinker: Nonexcessive Drinker Per Capita Cost Ratio (State Range) | Percentage of Aggregate Costs Paid by Nonexcessive Drinkers (State Range) |\n| --- | --- | --- | --- |\n| $0.05 Per Drink Increase | $0.10 Per Drink Increase | $0.25 Per Drink Increase | 5% Increase in Drink Price |\n| --- | --- | --- | --- |\n| **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** | **Annual household income, $** |\n| <25,000 | 1.86 (.37–2.51) | 3.52 (.57–4.76) | 7.33 (.14–10.02) | 2.87 (.01–3.94) | 6.8 (.1–8.2) | 13.8 (.2–16.3) |\n| 25,000–<50,000 | 2.26 (.70–3.36) | 4.28 (.23–6.40) | 8.90 (.74–13.55) | 3.49 (.60–5.43) | 5.8 (.4–7.0) | 21.1 (.2–25.2) |\n| 50,000–<75,000 | 2.33 (.83–2.98) | 4.43 (.47–5.68) | 9.20 (.23–12.08) | 3.61 (.79–4.94) | 5.3 (.1–6.7) | 26.2 (.2–30.1) |\n| \\=75,000 | 2.61 (.13–3.46) | 4.96 (.01–6.60) | 10.31 (.22–14.03) | 4.06 (.09–5.74) | 4.8 (.9–6.4) | 28.5 (.4–34.2) |\n| **Employment status** | **Employment status** | **Employment status** | **Employment status** | **Employment status** | **Employment status** | **Employment status** |\n| Employed d | 2.28 (.96–2.83) | 4.33 (.70–5.39) | 9.00 (.63–11.46) | 3.53 (.92–4.69) | 5.6 (.5–7.1) | 21.2 (.2–24.5) |\n| Nonemployed e | 2.39 (.00–3.28) | 4.53 (.78–6.25) | 9.42 (.78–13.28) | 3.70 (.88–5.43) | 5.1 (.1–6.4) | 25.0 (.2–29.4) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| White, non-Hispanic | 2.48 (.01–3.32) | 4.70 (.85–6.34) | 9.76 (.94–13.35) | 3.83 (.97–5.57) | 5.4 (.2–6.7) | 23.8 (.1–26.3) |\n| Other f | 1.88 (.38–2.53) | 3.57 (.63–4.88) | 7.44 (.22–10.09) | 2.93 (.09–4.02) | 5.9 (.5–7.0) | 18.8 (.7–22.1) |\n\n【46】a  The net cost is the change in the amount of money a person would spend on alcohol (product plus tax) as a result of hypothetical tax increases.  \nb  A nonexcessive drinker is defined as a respondent who consumed alcohol during the past 30 days but within those 30 days did not binge drink (=5 drinks for men or =4 drinks for women on at least one occasion), was not categorized as a heavy drinker (daily average of >2 drinks for men or >1 drink for women \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), and who was aged 21 or older.  \nc  An excessive drinker is defined as respondent who during the past 30 days either engaged in binge drinking (=5 drinks for men or =4 drinks for women during at least one occasion) or was categorized as a heavy drinker (daily average of >2 drinks for men or >1 drink for women \\[ie, >60 drinks per month for men or >30 drinks per month for women\\]), or who was aged 18–20 and reported drinking any alcohol.  \nd  Self-reported.  \ne  Nonemployed includes self-employed, unemployed, homemaker, student, retired, or unable to work.  \nf  Other race/ethnicity includes non-Hispanic black, Hispanic white, Asian, and other racial/ethnic group.\n\n【47】#####  Table 4. **State-Specific Average Annual Per Capita Net Cost a  for Nonexcessive Drinkers b  , Excessive Drinker c  : Nonexcessive Drinker Per Capita Cost Ratio, and Proportion of Total Costs Paid by Nonexcessive Drinkers, US Adults Aged =18 Years, Behavioral Risk Factor Surveillance System, 2011**  \n\n| State | Mean Annual Per Capita Net Cost ($) From Tax Increases for Nonexcessive Drinkers | Excessive Drinker: Nonexcessive Drinker Per Capita Cost Ratio | Percentage of Aggregate Costs Paid by Nonexcessive Drinkers |\n| --- | --- | --- | --- |\n| $0.05 Per Drink | $0.10 Per Drink | $0.25 Per Drink | 5% of Drink Price |\n| --- | --- | --- | --- |\n| Alabama | 2.39 | 4.54 | 9.56 | 3.65 | 5.8 | 19.8 |\n| Alaska | 2.44 | 4.61 | 9.39 | 3.77 | 5.5 | 21.7 |\n| Arizona | 2.36 | 4.47 | 9.27 | 3.61 | 4.7 | 24.6 |\n| Arkansas | 2.15 | 4.08 | 8.58 | 3.36 | 7.6 | 17.0 |\n| California | 2.22 | 4.22 | 8.81 | 3.59 | 5.2 | 22.8 |\n| Colorado | 2.38 | 4.47 | 9.06 | 3.52 | 4.6 | 27.9 |\n| Connecticut | 2.34 | 4.41 | 9.06 | 3.79 | 4.8 | 29.1 |\n| Delaware | 2.20 | 4.13 | 8.25 | 3.23 | 5.6 | 20.3 |\n| District of Columbia | 2.53 | 4.78 | 9.86 | 4.45 | 4.1 | 24.3 |\n| Florida | 2.31 | 4.36 | 9.03 | 3.65 | 5.6 | 23.9 |\n| Georgia | 2.32 | 4.39 | 9.10 | 3.48 | 6.0 | 20.7 |\n| Hawaii | 2.44 | 4.64 | 9.70 | 3.83 | 5.8 | 16.5 |\n| Idaho | 2.35 | 4.46 | 9.37 | 3.90 | 5.2 | 23.4 |\n| Illinois | 2.22 | 4.21 | 8.79 | 3.45 | 6.2 | 17.7 |\n| Indiana | 1.98 | 3.75 | 7.71 | 2.97 | 5.8 | 20.9 |\n| Iowa | 2.22 | 4.21 | 8.75 | 3.22 | 6.3 | 18.1 |\n| Kansas | 2.25 | 4.26 | 8.89 | 3.40 | 5.6 | 22.7 |\n| Kentucky | 2.12 | 4.03 | 8.40 | 3.25 | 6.4 | 16.7 |\n| Louisiana | 2.16 | 4.09 | 8.42 | 3.12 | 6.4 | 18.3 |\n| Maine | 2.68 | 5.08 | 10.53 | 4.15 | 4.9 | 27.8 |\n| Maryland | 2.18 | 4.11 | 8.34 | 3.34 | 5.2 | 25.2 |\n| Massachusetts | 2.55 | 4.82 | 9.89 | 3.99 | 4.8 | 26.6 |\n| Michigan | 2.27 | 4.28 | 8.79 | 3.43 | 5.6 | 21.1 |\n| Minnesota | 2.28 | 4.30 | 8.83 | 3.59 | 5.2 | 22.1 |\n| Mississippi | 2.15 | 4.08 | 8.51 | 3.15 | 6.2 | 19.3 |\n| Missouri | 2.16 | 4.08 | 8.36 | 3.14 | 6.0 | 18.8 |\n| Montana | 2.63 | 4.97 | 10.21 | 3.73 | 5.0 | 22.5 |\n| Nebraska | 2.26 | 4.29 | 8.95 | 3.27 | 5.6 | 19.9 |\n| Nevada | 2.33 | 4.41 | 9.11 | 3.62 | 5.7 | 22.0 |\n| New Hampshire | 2.57 | 4.83 | 9.76 | 4.00 | 5.4 | 26.8 |\n| New Jersey | 2.17 | 4.10 | 8.43 | 3.53 | 5.2 | 26.8 |\n| New Mexico | 2.49 | 4.73 | 9.85 | 3.74 | 5.2 | 24.9 |\n| New York | 2.27 | 4.29 | 8.83 | 3.53 | 4.9 | 25.1 |\n| North Carolina | 2.55 | 4.84 | 10.16 | 3.97 | 5.1 | 23.7 |\n| North Dakota | 2.05 | 3.86 | 7.89 | 3.01 | 5.4 | 19.9 |\n| Ohio | 2.38 | 4.53 | 9.57 | 3.50 | 5.7 | 20.3 |\n| Oklahoma | 2.02 | 3.84 | 8.11 | 3.14 | 6.4 | 17.9 |\n| Oregon | 2.48 | 4.67 | 9.54 | 3.59 | 5.1 | 27.7 |\n| Pennsylvania | 2.39 | 4.53 | 9.47 | 3.47 | 5.4 | 23.6 |\n| Rhode Island | 2.33 | 4.41 | 9.07 | 3.70 | 4.6 | 26.2 |\n| South Carolina | 2.42 | 4.59 | 9.58 | 3.67 | 5.9 | 20.8 |\n| South Dakota | 2.19 | 4.14 | 8.52 | 3.15 | 5.1 | 21.5 |\n| Tennessee | 2.70 | 5.15 | 10.91 | 4.37 | 4.1 | 31.3 |\n| Texas | 2.38 | 4.53 | 9.67 | 3.75 | 5.9 | 20.0 |\n| Utah | 2.33 | 4.40 | 9.10 | 3.53 | 5.7 | 16.5 |\n| Vermont | 2.96 | 5.64 | 12.00 | 4.91 | 4.5 | 30.1 |\n| Virginia | 2.64 | 5.00 | 10.47 | 4.06 | 5.0 | 25.7 |\n| Washington | 2.58 | 4.89 | 10.25 | 4.20 | 4.3 | 29.5 |\n| West Virginia | 2.38 | 4.52 | 9.51 | 3.37 | 6.8 | 19.7 |\n| Wisconsin | 2.35 | 4.42 | 8.97 | 3.44 | 5.1 | 20.9 |\n| Wyoming | 2.37 | 4.45 | 8.96 | 3.37 | 5.7 | 20.8 |\n\n【49】a  The net cost is defined as the change in the amount of money a person would spend on alcohol (product plus tax) as a result of hypothetical tax increases.  \nb  A nonexcessive drinker was defined as a respondent who consumed alcohol during the past 30 days but within those 30 days did not binge drink (=5 drinks for men or =4 drinks for women on at least one occasion), was not categorized as a heavy drinker (daily average of =2 drinks for men or =1 drink for women \\[ie, =60 drinks per month for men or =30 drinks per month for women\\]), and who was 21 years of age or older.  \nc  An excessive drinker is defined as a respondent who during the past 30 days either engaged in binge drinking (=5 drinks for men or =4 drinks for women during at least one occasion) or was categorized as a heavy drinker (daily average of =2 drinks for men or =1 drinks for women \\[ie, =60 drinks per month for men or =30 drinks per month for women\\]), or who was 18 to 20 years of age and reported any alcohol consumption.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aaad157f-3e7b-4fa6-88cb-a614b2ab1892", "title": "Evaluating Diabetes Mobile Applications for Health Literate Designs and Functionality, 2014", "text": "【0】Evaluating Diabetes Mobile Applications for Health Literate Designs and Functionality, 2014\nAbstract\n--------\n\n【1】**Introduction**  \nThe expansion of mobile health technologies, particularly for diabetes-related applications (apps), grew exponentially in the past decade. This study sought to examine the extent to which current mobile apps for diabetes have health literate features recommended by participants in an Institute of Medicine Roundtable and compare the health literate features by app cost (free or not).\n\n【2】**Methods**  \nWe used diabetes-related keywords to identify diabetes-related apps for iOS devices. A random sample of 110 apps (% of total number of apps identified) was selected for coding. The coding scheme was adapted from the discussion paper produced by participants in the Institute of Medicine Roundtable.\n\n【3】**Results**  \nMost diabetes apps in this sample addressed diabetes management and therapeutics, and paid apps were more likely than free apps to use plain language strategies, to label links clearly, and to have at least 1 feature (a “back” button) that helps with the organization.\n\n【4】**Conclusion**  \nPaid apps were more likely than free apps to use strategies that should be more useful and engaging for people with low health literacy. Future work can investigate ways to make free diabetes mobile apps more user-friendly and accessible.\n\n【5】Introduction\n------------\n\n【6】The expansion of health care technologies, particularly mobile health technologies, grew exponentially in the past 10 years . Mobile health (or mHealth) is a subset of eHealth, defined as “the use of mobile computing and communication technologies in health care and public health” . The expansion of mHealth applications (apps) is also documented. A systematic review of mHealth research showed a surge in the number of scientific articles on this topic from 2005 through 2011 . More than half of the 352 studies involved testing of a mobile app, and most (%) studies applied a quantitative methodology .\n\n【7】The rising availability of these mobile technologies corresponds with an increase in ownership of mobile telephones and tablets and the use of apps. National surveys in 2013 found that 34% of US adults owned a tablet and 91% owned a cellular telephone . Of cellular telephone owners, 55% had a smartphone  and 50% downloaded apps on their phone, an increase from 22% in 2009 . Although smartphone and tablet ownership has increased in nearly every major demographic group, ownership varies by income and age group . For example, younger adults, regardless of income, are likely to own a smartphone, whereas older adults who own a smartphone are more likely to be in upper income levels .\n\n【8】The review of mHealth apps  found that, in general, mHealth research focused predominantly on chronic conditions, and of these conditions, diabetes was the most frequent focus . A recent review of diabetes-specific apps available in 2013 for the iOS and Android operating systems  evaluated range of functions, target audiences, languages, cost, ratings, interfaces, and usability and found that apps had moderate to good usability among older (aged 50 or older) adults who had diabetes when the apps had a small range of functions. The review emphasized that simplicity of use and understandable terminology is especially important for maximum usability among older populations.\n\n【9】The most common types of diabetes apps identified were for health tracking or self-monitoring tasks such as recording blood glucose levels, insulin levels, and medication use . Other types of apps were insulin-dose calculators , physician-directed apps, food reference databases, social forums or blogs, and exercise apps . These diabetes apps did not demonstrate clinical effectiveness or integrate with health care delivery systems; other limitations included potential threats to safety and privacy, usability issues, and lack of personal feedback .\n\n【10】Although the surge in the development of diabetes apps and smartphone ownership continues, it is questionable how relevant and appropriate these types of health information technologies are for people with low health literacy — people who have a limited ability to obtain, process, and understand basic health information for making health decisions . In general, those with low health literacy are less likely to understand health information and know how to access and obtain prevention services , have worse glycemic control , and have higher rates of diabetes complications .\n\n【11】People with low health literacy are less likely to access and use health information technology  or be computer literate . Having eHealth literacy promotes self-efficacy for, and use of, health apps . Reviews of mobile apps  recommend certain design features to increase effort, efficiency, and satisfaction among low-literacy users, including the following: text-based interfaces ; graphical cues, including bigger widgets ; language support in text and audio ; “back” and “home” buttons ; linear navigation (navigation bar and scrollbars) and minimal hierarchical structures ; and avoidance of nonnumeric text input and scrolling menus .\n\n【12】A 2013 Institute of Medicine (IOM) Roundtable on Health Literacy’s Collaborative on New Technologies yielded a discussion paper in which the authors summarize and suggest strategies for improving health literacy and usability in the development of health literate apps . These strategies are based on those developed by the US Department of Health and Human Services  and adapted for the development of mHealth apps. These strategies, along with actions , include the following:\n\n【13】*   learning about users — identifying users and what they are trying to do and engaging them in the design process;\n*   writing actionable content — putting the most important information first, staying positive and realistic, providing action steps, and writing in plain language;\n*   displaying content clearly — using short paragraphs, large font size, white space, and clear labels;\n*   organizing and simplifying — using labels and providing easy access to home pages, linear information paths, and search and browse functionality;\n*   engaging users — including printer-friendly tools, simplified controls and buttons, and interactive content; and\n*   evaluating and revising the site — using experienced moderators to test the site with users of low literacy and low health literacy.\n\n【14】These recommendations were developed for the creation of apps, but they have not yet been used to evaluate existing diabetes-related mHealth apps. The objective of this study was to evaluate diabetes-related mHealth apps according to the recommendations of the IOM Roundtable discussion paper . Additionally, because the cost of an app may lead a user to select a free app instead of an app that has even a minimal cost, we compared the health literate features of diabetes apps by app cost (free or not).\n\n【15】Methods\n-------\n\n【16】### Sample\n\n【17】The keywords “diabetes,” “diabetic,” “type 1 diabetes,” and “type 2 diabetes” were entered into the search field of the Apple App Store in April 2014 to identify English-language diabetes-related apps for iOS devices (iPad and iPhone). Our search yielded 460 apps that contained keywords in the app name, description, or reviews. We randomly selected 110 (%) apps .\n\n【18】### Coding\n\n【19】Coders first downloaded each app to an iPad and familiarized themselves with the app’s features. Next, coders entered information for each app into an electronic database. Apps were coded for general characteristics listed in the App Store. Then, coders recorded diabetes-related content and recommendations for designing health literate mobile apps as published in the IOM discussion paper . Trained research assistants first conducted the coding in teams while training and then coded individually. Intercoder reliability was calculated on the basis of a 10% sample and indicated substantial agreement : κ = 0.77 (% confidence interval, 0.71–0.83). Each of 4 coders analyzed approximately 30 apps.\n\n【20】#### General characteristics\n\n【21】The Apple App Store provided the following data for each app: title, price, age rating, category, the total number of ratings provided by app users, and the number of stars (star rating), which ranged from 1 star to 5 stars (with 5 being the highest rating). The age rating was classified as 4 years or older (no objectionable material); 12 years or older (mild language, frequent/intense/realistic violence, and mild or infrequent mature or suggestive content not suitable for those <12 y); or 17 years or older (must be 17 years old to purchase, may contain frequent and intense offensive language, violence, or mature themes not suitable for those <17 y). The app category included health and fitness, medical, food and drink, education, lifestyle, social networking, business, reference, and utilities.\n\n【22】#### Public health variables\n\n【23】The research team developed categories for 3 public health variables and 4 health literate design strategies. For each variable, coders could select multiple categories (ie, categories were not mutually exclusive). The public health variables coded were type of diabetes, diabetes continuum, and app focus. The type of diabetes was mentioned in the app description or on the app itself and was categorized as type 1 diabetes, type 2 diabetes, prediabetes, gestational diabetes, “not specified,” or “other.” “Diabetes continuum” refers to the stage of diabetes-related behavior targeted by the app. We developed the following categories for state of behavior: prevention (eg, healthy eating, exercising); screening, diagnosis, or symptoms of diabetes (eg, getting a blood glucose check for a diagnosis of diabetes or symptoms of diabetes); management or therapeutics (eg, checking blood glucose regularly, eating healthy and exercising, preventing complications); or none (ie, no diabetes behaviors mentioned). “App focus” refers to the diabetes-related focus of the app, including primary prevention (eg, health promotion activities such as eating fruits and vegetables and regular exercise); screening (eg, blood glucose check to screen for diabetes); symptoms or diagnosis (eg, confirming diagnosis of diabetes through a blood glucose test, identifying diabetes by signs or symptoms); management or therapeutics (eg, insulin therapy for the regulation of blood glucose, lifestyle changes); complications (eg, the biological consequences of untreated diabetes, risk factors for complications); and research, science, and technology (eg, technology developed to manage or prevent diabetes).\n\n【24】The health literate design strategies  assessed by coders were 1) writing in plain language (eg, using common, everyday words; using personal pronouns such as “you”; avoiding undefined technical or medical terms; using active voice, action words, and present tense; keeping sentences short); 2) displaying content clearly (eg, limiting paragraph size by using bullets and short lists, labeling links for images and descriptions, using images that facilitate learning, using bold colors with contrast, avoiding dark backgrounds); 3) organizing and simplifying the app (eg, easy access to a homepage or menu page; a “back” button; the ability to search and browse; integration with email, calendar, and maps); and 4) engaging users _(eg, printer-friendly tools and resources; interactive content; audio and visual features; connections to new media such as Twitter or text messaging).\n\n【25】We used SPSS v.20 (IBM Corporation) to calculate descriptive statistics and _t_ tests to identify associations between app characteristics and price (free vs not free). Significance was determined at a level of α = .05 for 2-tailed tests.\n\n【26】Results\n-------\n\n【27】Of the 110 apps, 76 (%) were free . General characteristics were not significantly different according to whether the app was free or not. The 34 apps that were not free ranged in price from $0.99 to $29.99, with a mean price of $4.57 (interquartile range, $1.99–$5.99). User ratings were provided for 65 apps; the average number of ratings per app was 148, and the average star rating was 3.4. Most apps (.2%) were rated for ages 4 years or older, and the categories in which the apps were placed most often were health and fitness (.6%) and medical (.6%).\n\n【28】None of the public health variables were significantly different according to whether the app was free or not. Most (.3%) apps did not specify diabetes type. Only 5 apps specified type 1 diabetes, and only 5 apps specified type 2 diabetes. Across the diabetes continuum, most (.7%) apps addressed behaviors related to diabetes management or therapeutics, and a third (.3%) addressed prevention. Other apps addressed diabetes screening, diagnosis, or symptoms (.5%), and 12.3% did not address any stage on the continuum. Consistent with our findings on continuum, 66.4% of apps focused on management or therapeutics, and 30.9% focused on primary prevention.\n\n【29】**Using plain language.** Across all plain language strategies combined, 84% of apps used at least 1 strategy, and this finding did not differ by whether the app was free or not. However, we found significant differences between the 2 types of apps according to several strategies. Paid diabetes apps were significantly more likely to use common, everyday words (.2%, paid vs 75.0%, free; _P_ \\= .05); avoid undefined technical or medical terms (.3% vs 65.8%; _P_ \\= .04); and use active voice (.2% vs 68.4%; _P_ \\= .03), action words (.2% vs 69.7%; _P_ \\= .04), and present tense (.1% vs 75.0%; _P_ \\= .02).\n\n【30】**Displaying content clearly.** Paid apps were more likely than free apps to label links clearly (.0% vs 89.5%, _P_ \\= .05). Of both kinds of apps, most (.9%) used bold colors with contrast but only 30.9% used images that facilitated learning; we found no differences between free and paid apps for these strategies.\n\n【31】**Organizing and simplifying.** Most (.6%) apps provided easy access to a homepage or menu page; 85.3% of paid apps had a homepage, whereas 68.0% of free apps had one (P_ \\= .06); 70.6% of paid apps had easy access to a menu page, whereas 52.0% of free apps had one (P_ \\= .07). Paid apps were significantly more likely to include a “back” button (.1% vs 75.0%, _P_ \\= .006). No other differences between free and paid apps were found for other organizing strategies. Less than half (%) of all apps were integrated with other applications (email, calendar, or maps).\n\n【32】**Engaging users.** Most (.0%) apps had interactive content that users could tailor, but otherwise, less than 17% of apps had other engaging features such as printer-friendly tools, audio and visual features, or integration with new media such as Twitter or texting.\n\n【33】Discussion\n----------\n\n【34】In this study, a sample of diabetes-related apps was coded for public health characteristics and health literate design strategies for mHealth apps. These apps were rated highly by users, and most were classified as appropriate for children and adults. Consistent with other studies of health apps and diabetes apps, most of the diabetes apps in this sample addressed diabetes management and therapeutics . Paid apps were more likely than free apps to use health literate design strategies such as using plain language, labeling links clearly, and having a “back” button to help with organization.\n\n【35】One explanation for these differences is that with paid apps, perhaps more effort was undertaken to conduct formative research and usability testing before product launch. Those activities may have identified functions in the app for which the user experience could be improved to increase understanding and ease of use.\n\n【36】Because low health literacy is more likely among people of low socioeconomic status , the cost of apps may be prohibitive for people with low health literacy. If these people are more likely to use free diabetes apps, then they are more likely to have apps that lack features that enhance usability and understanding. Further research can identify diabetes app characteristics, including functionality, cost, and ratings, that may influence potential users to pay for an app instead of downloading one for free. Because user ratings of free apps and paid apps did not differ significantly, it would also be helpful to conduct usability tests to directly compare levels of satisfaction for free and paid apps. Also, by understanding which types of diabetes apps people with low literacy would choose to use regardless of cost, we could identify where and how resources for improving the health literacy of mobile diabetes apps would be best used.\n\n【37】Our study has several limitations. Although we used _Health Literacy Online_  as a tool to rate existing apps, its original purpose was to help guide the design of health websites, including strategies for testing usability. Not all _Health Literacy Online_ strategies and actions, such as those requiring knowledge of the app developers’ target users and usability processes, were included in the codebook for our study because of a lack of information. Because the information coded in our study could be ascertained only by viewing the app description and ratings information and using the downloaded app, we did not have enough background information to know the history of each app’s development or what, if any, usability testing was done before its launch. _Health Literacy Online_ is one of several tools that can help guide the creation of health literate mHealth applications or to assess the health literacy of health information materials, including those that are digitally based (eg, mobile apps, websites, computer applications) . Using this tool for existing apps may not be as appropriate or useful for usability outcomes, but it is a starting point to help evaluate the health literacy of diabetes apps.\n\n【38】Another limitation of this study is the generalizability of the sample of diabetes apps selected. The use of a simple random sampling strategy yielded 4 of the top 10 most popular diabetes apps in the App Store (as of February 2015). In addition, the search terms included only diabetes-related terms, because we were interested only in apps that self-identified as diabetes-related through the app name or description. Other search terms such as “glucose” or “blood sugar” were not used. However, an additional search using the terms “glucose” or “blood sugar” yielded 294 apps, 176 of which also appeared on the list of diabetes-related apps.\n\n【39】Finally, the sample of apps examined included only iOS apps and did not include any Android-compatible apps. Because African American cellular telephone owners are more likely than whites or Latinos to own an Android telephone instead of an iPhone (%, African Americans; 26%, whites; 27%, Latinos) , this may limit the study’s applicability to more diverse audiences.\n\n【40】In general, the findings of this study indicate that additional work should be done to improve mHealth apps. In particular, encouraging a development process for free diabetes-related apps to make them more user-friendly and accessible to diverse audiences could potentially increase their use and understandability among audiences, especially people with low health literacy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fe23bc28-3fe7-4ad1-b046-91c23747bb90", "title": "Polypharmacy and Health-Related Quality of Life/Psychological Distress Among Patients With Chronic Disease", "text": "【0】Polypharmacy and Health-Related Quality of Life/Psychological Distress Among Patients With Chronic Disease\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】To date, no study has investigated the impact of polypharmacy (use of ≥5 medications concurrently) on health-related quality of life (HRQOL) and psychological distress in a combined sample of chronic disease patients and patients with multimorbidity, using diverse HRQOL measures. This study aimed to explore the association between polypharmacy and HRQOL/psychological distress by using data from a cross-sectional study in Flanders (Belgium).\n\n【3】**Methods**\n\n【4】We analyzed cross-sectional survey data on 544 chronically ill patients recruited from June 2019 through June 2021. HRQOL was measured with the EuroQol-5 Dimension-5 Level questionnaire (EQ-5D-5L) and the 12-Item Short Form Health Survey (SF-12); psychological distress was measured with the Hospital Anxiety and Depression Scale (HADS). Multiple linear regression models were built to assess the association between polypharmacy and HRQOL/psychological distress.\n\n【5】**Results**\n\n【6】Overall, compared with patients without polypharmacy, patients with polypharmacy reported worse EQ-5D-5L index values, EuroQol visual analogue scale (EQ-VAS) scores, SF-12 physical component scores (PCS), SF-12 mental component scores (MCS), and HADS anxiety and depression subscales. In the final regression model adjusting for age, sex, educational attainment, and multimorbidity, polypharmacy remained significantly associated with lower HRQOL in terms of the EQ-5D-5L index (β = −0.12; _P_ \\= .008), EQ-VAS (β = −0.11; _P_ \\= .01), and SF-12 PCS (β = −0.15; _P_ \\= .002) but not with psychological distress (HADS) and SF-12 MCS.\n\n【7】**Conclusion**\n\n【8】This study found that polypharmacy was negatively associated with the physical domain of HRQOL, but not with the mental domain, among patients with chronic diseases. These results may be especially important for patients with multimorbidity, given their greater risk of polypharmacy.\n\n【9】Introduction\n------------\n\n【10】During the past few decades, chronic diseases have predominated over infectious diseases, and their prevalence is still rising . In addition, people with chronic conditions often have multimorbidity — multiple chronic conditions at the same time . Having 1 or more chronic conditions has negative effects on a person’s well-being and health-related quality of life (HRQOL) . In addition to optimal longevity, an ultimate goal of modern health care is to achieve the best possible life for patients in terms of an optimal HRQOL, especially since the shift from problem-oriented care to goal-oriented care . HRQOL has become an important outcome because it captures a person’s self-perceived physical, mental, and social functioning .\n\n【11】Because chronically ill patients tend to have a higher risk of multimorbidity, multiple drug use is common . The resulting polypharmacy, defined as the use of 5 or more medications , however, increases the risk of adverse drug–drug or drug–disease interactions and the risk of a “prescription cascade” (ie, the process whereby side effects of drugs are misdiagnosed as symptoms of another medical event and lead to an additional prescription ), which may negatively affect a patient’s HRQOL . The association between polypharmacy and low HRQOL has been documented for single health conditions such as end-stage kidney disease , arthritis , and cardiometabolic risk factors . However, to date, no study has investigated the impact of polypharmacy on HRQOL in a combined sample of chronic disease patients and patients with multimorbidity by using diverse HRQOL measures. Hence, this study aimed to explore the association between polypharmacy and HRQOL among people with chronic conditions and multimorbidity by using data from a cross-sectional study in Flanders (Belgium).\n\n【12】Methods\n-------\n\n【13】### Study design\n\n【14】We used data from a cross-sectional study (the QAPICHE study, an acronym for “quality of life in patients with chronic disease”) conducted in Flanders, Belgium. Detailed information on the study aims and methodology can be found elsewhere . In brief, the QAPICHE study provides insight into the HRQOL of chronically ill patients by comparing patient groups and investigating determinants.\n\n【15】The QAPICHE study was approved by the Ethical Committee of the Ghent University Hospital, Belgium  and is registered on ClinicalTrials.gov . Written informed consent was obtained from all participants in the study.\n\n【16】### Study population\n\n【17】A total of 544 chronically ill people, recruited from June 2019 through June 2021, participated in the study. Of these participants, 287 (.8%) were recruited via 56 general practitioners and the remaining 257 (.2%) persons were recruited via 18 officially recognized Flemish patient organizations. Inclusion criteria were being an adult (aged ≥18 y) and being diagnosed with at least 1 of the following chronic diseases: any cardiometabolic disorder, any mental disorder, or any musculoskeletal disorder. These 3 disease groups were selected because they are associated with lower HRQOL than other disease groups and because musculoskeletal and cardiometabolic disorders are highly prevalent in Belgium . Patients with insufficient understanding of the Dutch language to complete the questionnaire were excluded.\n\n【18】### Measures\n\n【19】#### Health-related quality of life\n\n【20】The EuroQol-5 Dimension-5 Level questionnaire (EQ-5D-5L) is composed of a descriptive system covering 5 dimensions (ie, mobility, self-care, usual activities, pain/discomfort, and anxiety/depression) defined by 5 severity levels (ie, no problems, slight problems, moderate problems, severe problems, and extreme problems/unable to) from which a single index value or utility value can be calculated, ranging from 0 (death) to 1 (perfect health) and with negative values indicating health states perceived to be worse than death . Recently, an EQ-5D-5L value set was developed according to the health state preferences of the general population of Belgium . Possible index values of this value set range from −0.532 to 1. The EuroQol visual analogue scale (EQ-VAS) measures respondents’ self-rated health on a 0 (worst imaginable health) to 100 (best imaginable health) scale. However, focusing only on utility values would result in a loss of potentially relevant information. As such, analyzing the individual dimensions as categorical outcome (no problems vs any problems) is recommended .\n\n【21】The 12-Item Short Form Health Survey (SF-12) is a shortened version of the 36-Item Short Form Health Survey (SF-36) . The instrument contains 12 items that evaluate 8 domains pertaining to HRQOL: physical functioning, role-physical, bodily pain, general health, vitality, social functioning, role-emotional, and mental health. Both a physical component summary (PCS) score and a mental health component summary (MCS) score can be calculated according to a US general population scoring algorithm . The scores range from 0 to 100, with 0 representing the lowest level of health and 100 the highest level of health.\n\n【22】#### Psychological distress\n\n【23】The 14-item Hospital Anxiety and Depression Scale (HADS)  is a brief instrument to determine the presence of anxiety and depressive states. Each item has a 4-point response scale (range, 0–3); 7 items are related to anxiety and 7 items to depression. Item scores can be added to obtain a score for the anxiety subscale (HADS-anxiety) and the depression subscale (HADS-depression) separately. A maximum score of 21 can be achieved per subscale. The higher the overall score, the higher the levels of anxiety and depression.\n\n【24】#### Polypharmacy\n\n【25】Medication use was assessed with the question, “Which medication are you currently taking?” Medicines from the same pharmacologic subgroup (eg, fast-acting and long-acting insulin) were considered separate medicines. Although no consensus exists on the definition of polypharmacy, the QAPICHE study defined polypharmacy as the use of 5 or more medications concurrently . This value is linked to adverse outcomes such as disability, frailty, and mortality . For informational purposes, we added 2 more categories (to <10 medications and ≥10 medications).\n\n【26】#### Sociodemographic characteristics\n\n【27】Patient demographic variables included age (≥65 y), sex (male, female), marital status (married, widowed/divorced, single), and educational attainment. Educational attainment was classified into low (lower secondary education or less), intermediate (higher secondary education), and high (higher education), according to the International Standard Classification of Education .\n\n【28】#### Multimorbidity\n\n【29】Participants were asked to respond yes or no to having a chronic condition from a list of 24 chronic conditions. A response of yes indicated a condition that had been confirmed by a physician or for which the participant was taking prescribed drugs. The following chronic conditions and diseases were measured: cardiovascular disease, stroke, heart failure (including valve problems or replacement), hypertension, high cholesterol, chronic joint disease (eg, rheumatoid arthritis), chronic musculoskeletal conditions causing pain or limitation, osteoporosis, chronic disease of the back/neck, any cancer in the previous 5 years, chronic lung disease (eg, asthma, chronic obstructive pulmonary disease), type 1 and 2 diabetes, thyroid disorder, chronic kidney disease, chronic urinary problems (eg, incontinence), chronic stomach problems, colon problems (eg, irritable bowel, Crohn disease), chronic hepatitis, headache disorder (eg, migraine), depression/chronic anxiety, mental disorder (eg, schizophrenia), neurologic disorder (eg, epilepsy), Alzheimer disease/dementia, and chronic skin disease. Defining multimorbidity as the presence of 2 or more chronic diseases is not universally accepted, especially when highly prevalent conditions (eg, hypertension, osteoporosis) are included, because these result in higher prevalence rates of multimorbidity . Hence, we defined multimorbidity as having 3 or more concurrent diseases and used the simple count method .\n\n【30】### Statistical analysis\n\n【31】First, we performed descriptive analyses. For continuous variables, we calculated means and SDs; for categorical variables, we calculated percentages. After checking the assumptions, multiple linear regression models assessed the association between polypharmacy and HRQOL. Likewise, we used logistic regression to investigate the association between polypharmacy and each EQ-5D-5L dimension. Initially, in both analyses, we adjusted the model for age, sex, and educational attainment to account for potential confounding; we then applied a further adjustment for multimorbidity. We used SPSS statistical software version 27.0 (IBM Corp) to analyze data. Significance levels were set at _P_ < .05.\n\n【32】Results\n-------\n\n【33】The mean (SD) age of patients was 58.6 (.8) years, and 67.1% were women . Low and high education was reported by 28.4% and 39.1% of respondents, respectively. Multimorbidity occurred in 63.4% of the participants. The mean number of medications consumed by patients was 5.7 (SD, 3.8; range 0–20), and most reported polypharmacy (.9%). Of 528 patients, 45.1% (n = 238) used 0 to <5 medications per day, 42.6% (n = 225) used 5 to <10 medications per day, and 12.3% (n = 65) used 10 or more medications per day. The prevalence of polypharmacy was higher among patients with multimorbidity than among patients without multimorbidity (.3% vs 33.7%, _P_ < .001).\n\n【34】### HRQOL/psychological distress outcomes\n\n【35】Patients with polypharmacy had a higher frequency of problems on all EQ-5D-5L dimensions compared with patients without polypharmacy . We found significant differences for the dimensions of mobility (P_ \\= .01) and usual activities (P_ \\= .02). In general, compared with patients without polypharmacy, patients with polypharmacy reported worse EQ-5D-5L index values (.69 vs 0.60; _P_ < .001), EQ-VAS scores (.6 vs 58.0; _P_ \\= .005), SF-12 PCS scores (.2 vs 35.0; _P_ < .001), SF-12 MCS scores (.3 vs 42.8; _P_ \\= .68), HADS-anxiety scores (.5 vs 7.6; _P_ \\= .88), and HADS-depression scores (.5 vs 7.3; _P_ \\= .06) .\n\n【36】**  \nPercentage of patients reporting problems on the 5 EQ-5D-5L dimensions, by whether patient reported polypharmacy, defined in study as use of ≥5 medications. _P_ values were determined by logistic regression and were adjusted for age, sex, educational attainment, and multimorbidity; significance level set at <.05. Abbreviation: EQ-5D-5L, EuroQol-5 Dimension-5 Level questionnaire. \n\n【37】In the analysis of HRQOL/psychological distress according to medication use, divided into 3 groups (to <5 medications, 5 to <10 medications, ≥10 medications), the higher the medication use, the worse the results on the EQ-5D-5L, the SF-12, and both HADS subscales  .\n\n【38】**  \nEQ-5D-5L index, EQ-VAS, SF-12 physical component score, SF-12 mental health component score, HADS anxiety subscale, and HADS depression subscale, by medication use. The horizontal bar inside the boxes indicates the median, the x indicates the mean, and the lower and upper ends of the boxes are the first and third quartiles. The whiskers indicate minimum and maximum values. _P_ values were determined by _F_ test (way analysis of variance) and adjusted for age, sex, educational attainment, and multimorbidity; significance level set at <.05. Abbreviations: EQ-5D-5L, EuroQol 5 Dimension-5 Level questionnaire; EQ-VAS, EuroQol visual analogue scale; HADS, Hospital Anxiety and Depression Scale; SF-12, 12-Item Short Form Health Survey. \n\n【39】### Associations between polypharmacy and HRQOL/psychological distress\n\n【40】In the fully adjusted regression model (age, sex, educational attainment, multimorbidity), polypharmacy was significantly associated with lower HRQOL only in terms of EQ-5D-5L index (β = −0.12; _P_ \\= .008), EQ-VAS (β = −0.11; _P_ \\= .01), and SF-12 PCS (β = −0.15; _P_ \\= .002) . We observed no significant associations between polypharmacy and psychological distress (HADS) nor with the SF-12 MCS in the fully adjusted model.\n\n【41】Discussion\n----------\n\n【42】Literature suggests that patients with polypharmacy have impaired HRQOL outcomes . This study examined the relationship between polypharmacy and HRQOL among adults with diverse chronic diseases, accounting for comorbidity. Most studies that investigated the association between polypharmacy and HRQOL focused on single diseases and used only 1 instrument. Our study, however, focused on several types of chronic diseases and used several instruments: HRQOL was measured via the EQ-5D-5L and the SF-12, and psychological distress was measured via the HADS. Information on the different aspects of HRQOL is important for aligning medical treatment to patient needs.\n\n【43】We found that most chronically ill patients reported polypharmacy, and as expected, the prevalence of polypharmacy was higher among patients with multimorbidity, in line with previous findings . The management of multiple chronic conditions often requires multiple prescriptions, increasing the risk of polypharmacy in patients with multimorbidity. Polypharmacy might affect a patient’s well-being. We found that patients with high levels of medication use had lower HRQOL and higher levels of psychological distress compared with patients with low levels of medication use. However, differences were clinically relevant only for the EQ-5D-5L index  and the SF-12 PCS . After adjusting for covariates (age, sex, educational attainment, and multimorbidity), results from the regression analysis showed that polypharmacy was negatively associated with the EQ-5D-5L index, the EQ-VAS, and the SF-12 PCS. No significant associations were found for the mental component of HRQOL nor psychological distress, suggesting an unfavorable effect of polypharmacy only on the physical domain of patients’ HRQOL. Indeed, overprescription of medication can cause increased risk of adverse drug–drug or drug–disease interactions, ultimately affecting physical functioning . However, inconsistency remains; some studies showed no significant association of polypharmacy with SF-12 MCS scores , while others found polypharmacy to be linked to an increased risk of depression, because some medications tend to cause or worsen depressive symptoms .\n\n【44】Pharmacologic treatment can be modified; hence, targeted interventions are recommended. Further research should investigate which medications affect the physical aspects of HRQOL. Medications that may cause harmful drug–drug or drug–disease interactions can be reduced or eliminated, while the remaining medications can focus on the patient’s most important functional priorities . Moreover, our results support the need for health care professionals to be aware of the negative effects of polypharmacy on HRQOL and to recognize drug-related adverse events, especially when treating patients with multimorbidity. In addition, health care professionals should assess the potential benefits and harms of prescribing multiple drugs to achieve ideal pharmacotherapy and to prescribe the medicines that can improve the HRQOL of this group . Evidence-based guidelines for patients with multimorbidity can help to inform deprescribing decisions, defined as “the process of withdrawal of an inappropriate medication, supervised by a health care professional with the goal of managing polypharmacy and improving outcomes” .\n\n【45】### Limitations and strengths\n\n【46】This study has several limitations. First, the study had a cross-sectional design; therefore, no statements about causality can be made. Second, bias toward more favorable results may have occurred because of selective nonresponse effects: patients with a more severe disease profile may not have completed the questionnaire . Third, the outbreak of the COVID-19 pandemic occurred during the data collection process. As such, pandemic-related stress may have worsened the HRQOL of patients who were included in the study during the pandemic. Fourth, data were self-reported, which might have produced recall bias. Fifth, because of ethical and privacy restrictions, we did not have access to patients’ medical records to evaluate medication use.\n\n【47】This study also had important strengths. First is the use of 3 international, validated instruments to assess HRQOL and psychological distress (ie, the EQ-5D-5L, the SF-12, and HADS). Second, we selected several variables to control for confounding in the regression analysis, including multimorbidity. However, medication use and multimorbidity are strongly correlated; hence, polypharmacy is commonly used as a proxy for multimorbidity. As a result, multimorbidity — not polypharmacy — may be the underlying cause of lower HRQOL . Nonetheless, polypharmacy remained significantly associated with HRQOL, even after adjustment for multimorbidity, indicating that polypharmacy independently affects HRQOL . Additionally, information on disease duration and disease severity was not available and not controlled for in our analyses, although they can affect both polypharmacy and HRQOL.\n\n【48】### Conclusion\n\n【49】In line with previous evidence, our study found that polypharmacy was negatively associated with the physical domain of HRQOL, but not the mental domain, among patients with chronic diseases. These results may be especially important for patients with multimorbidity, given their greater risk of polypharmacy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1ffdce83-6daf-4454-a2ee-a4bd60aa42db", "title": "Phylogenetic Analysis of West Nile Virus Genome, Iran", "text": "【0】Phylogenetic Analysis of West Nile Virus Genome, Iran\n**To the Editor:** West Nile virus (WNV) is a single-stranded, positive-sense RNA virus (≈11 kb) that is taxonomically classified within the family _Flaviviridae_ , genus _Flavivirus_ . WNV is found in Africa, Eurasia, Australia, and North America .\n\n【1】Comprehensive studies on phylogenetic relatedness of WNV strains have showed that WNV can be grouped into 5 lineages. Lineage 1 contains WNV strains from different regions, including northern, western, and central Africa; southern and eastern Europe; India; and the Middle East. Lineage 1 is subdivided into 3 clades. Clade 1A contains strains from Europe, northern Africa, the United States, and Israel, clade 1B contains Kunjin virus from Australia. Lineage 2 contains isolates from west, central, and eastern Africa and Madagascar. There is evidence that lineage 2 circulates in some regions of Europe (e.g. Italy, Austria, and Greece) . Lineage 3 contains Rabensburg virus 97–103, which was isolated in 1997 from _Culex pipiens_ mosquitoes in South Moravia in the Czech Republic. Lineage 4 contains a new variant of WNV (strain LEIVKrnd88–190), which was isolated in 1998 from _Dermacentor marginatus_ ticks in a valley in the northwestern Caucasus Mountains of Russia. Lineage 5 contains an WNV isolate from India (strain 804994) . In this study, we compared the phylogenetic relationships of WNV circulating in Iran to other WNV strains by using a partial WNV sequence isolated from an Iranian patient.\n\n【2】WNV was obtained from a blood sample from an Iranian patient who had encephalitis and was hospitalized in 2009 in Isfahan in the central highlands of Iran. The patient reported no history of animal contact, insect bites, blood transfusions, transplantations, and travel. He exhibited fever, headache, hypertension, and vomiting. On initial examination, he had a body temperature of 40°C. Laboratory investigations on the day of admission showed a leukocyte count of 240 cells/μL, a protein level of 52 mg/dL, and a glucose level of 50 mg/dL in a cerebrospinal fluid sample.\n\n【3】Further examinations were undertaken, and samples were sent to the Arboviruses and Viral Hemorrhagic Fevers Laboratory at Pasteur Institute of Iran in Teheran. For an IgG ELISA, wells in test plates were coated overnight with mouse hyperimmune ascitic fluid. Native antigen was added, and wells were incubated and washed. Test samples and peroxidase-labeled anti-human or anti-animal immunoglobulin were added. After incubation for 10 min, optical densities were read .\n\n【4】Viral RNA was extracted by using the QIAmp Viral RNA Mini Kit (QIAGEN, Hilden, Germany) from serum of the patient. A reverse transcription PCR was conducted by using a One-Step RT-PCR Kit (QIAGEN). Samples were subjected to 1 cycle at 50°C for 30 min to synthesize cDNA; 95°C for 15 min; and 95°C for 30 s, 54°C for 30 s, and 72°C for 60 s; and a final extension at 72°C for 5 min . The serum sample was positive for IgG against WNV. Molecular tests showed positive results for WNV.\n\n【5】The PCR product was sequenced by using the Big Dye Terminator V3.1 Cycle Sequencing Kit (Applied Biosystems, Foster City, CA, USA), the modified Sanger sequencing method, and an ABI Genetic Analyzer 3130 (Applied Biosystems) . Multiple alignments of nucleotide sequences were made by using ClustalW . A phylogenetic tree was constructed by using 27 representative sequences of WNV and Japanese encephalitis virus (available in GenBank) and a 358-nt sequence of WNV , from the patient, which corresponds to nt 259–616 in the late region of the capsid gene and the early region of the membrane gene. Phylogenetic status of the sequence for the WNV strain from the patient was assessed by using the neighbor-joining algorithm in MEGA5 . Reliability of phylogenetic groupings was evaluated by using the bootstrap test (replications). Japanese encephalitis virus SA14 sequence was used as an outgroup in phylogenetic analysis of partial genome sequences .\n\n【6】The phylogenetic tree identified clustering of isolates in 5 lineages. Lineage 1 had 2 sublineage (clade 1A and clade 1B). All sequences in lineage 1 were geographically distinguishable. Clade 1A contained strains from Europe, Africa, the Middle East, and the United States; clade 1B included the Australian strain NSW 2011 (JN887352). The WNV sequence from Iran (KJ486150) was grouped into lineage 2 and had 99% identity with the 358-bp region of WNV strain ArB3573/82 from the Central African Republic. Although it was believed that lineage 2 strains circulate only in Africa, reports of their emergence primarily in Balkan countries  and in our study support the presence of a lineage WNV 2 strain in the Middle East, particularly in western Asia .\n\n【7】Although the patient did not report any mosquito bites, this infection route cannot be excluded because he was a farmer and spent most of his time outdoors. Previous studies have demonstrated that nearly all human WNV infections were a consequence of mosquito bites . Our study should increase awareness of WNV infections as a public health threat. In future studies, priority should be given to investigations of the ecology, occurrence, and epidemiology of the different WNV strains circulating in Iran.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eb1013ef-f7d1-49ac-ac4d-679e8aa16c15", "title": "SARS Epidemic in the Press", "text": "【0】SARS Epidemic in the Press\n**To the Editor:** On March 12th, the World Health Organization _(WHO) issued a global alert regarding severe acute respiratory syndrome (SARS) in Vietnam, Hong Kong, and China’s Guangdong Province. Three days later, for the first time in its history, WHO recommended postponing nonessential travel to the affected areas and screening airline passengers . These initiatives, together with the awareness of the modes transmission of the coronavirus associated with SARS (SARS-CoV), led to extensive press coverage.\n\n【1】To describe the extent of this coverage in Italy and to identify the events that prompted peak coverage, we reviewed the five Italian daily newspapers with the highest circulation  from March 12 to March 30. The articles were identified by hand search (reading headlines, subheads, and titles) and were classified according to the publication date and page number. We assigned one point to full articles and to front-page articles or headlines and half a point to short articles. We also reviewed all national newspapers for articles published before the travel advisory (March 12–15).\n\n【2】Before the travel advisory, no articles were published in the five newspapers, whereas on March 14, one article was published in a smaller newspaper (“Osservatore Romano”, the Vatican newspaper). On March 16 (the day after the advisory), six articles appeared in the five newspapers; through May 31, a total of 750 articles were published. The proportion of articles that appeared on the front-page was 9.6%, although this percentage was higher early in the study (%) than at the time of absolute peak coverage (%).\n\n【3】After the first wave of articles in mid-March, several peaks occurred until mid-April. The events prompting these peaks were identified by determining the most frequently covered topics, specifically: the death of Carlo Urbani, the Italian WHO officer who identified the disease in Hanoi; the first two probable cases in Italy; the death of a suspected case in Naples; and the press conference announcing the first meeting of the Italian National Task Force. The highest peak occurred on April 23, after the announcement that the number of cases had reached 4,000 and that a vaccine would not be available anytime soon. In the days after the peak, coverage remained quite high, in association with the definition of SARS as a “global threat” by WHO and the twofold increase in the number of probable cases in Italy. The high press coverage was followed by an overall decrease, although small peaks occurred in association with the conflicts among European Ministries on airport measures, increased quarantine measures in China, and the identification of the civet cat as a probable source of SARS-CoV. Coverage tended to be greater on weekends, probably because political stories constitute less competition for space on these days.\n\n【4】Evidently, the daily newspaper coverage of SARS has been quite extensive in Italy, especially in the aftermath of WHO alerts and statements by the Ministry of Health regarding new cases and more stringent control measures. During outbreaks of infections, both the media and the public are often criticized for overreacting, yet public concern over serious health hazards is essential in guiding prevention activities  and in deciding whether to adopt measures that could place restrictions on civil rights, such as quarantine . Although we did not evaluate the quality of risk-communication of the journalists or of the experts quoted in the articles, wide press coverage of the WHO global alert may have contributed to public-health bodies’ taking action towards containing the epidemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "60c2800c-172d-4e86-b0a4-147743235426", "title": "Vaccinia Virus Infections in Martial Arts Gym, Maryland, USA, 2008", "text": "【0】Vaccinia Virus Infections in Martial Arts Gym, Maryland, USA, 2008\nVaccinia virus (VACV) is the virus used in the live vaccine against smallpox. Smallpox was declared eradicated by the World Health Organization in 1980 , and routine childhood smallpox vaccination ceased after 1972 in the United States. Since 2002, smallpox vaccinations have again been administered to some military personnel and health care workers, and they continue to be recommended for laboratory workers who work with nonattenuated orthopoxviruses . VACV infections are transmissible and can cause severe complications in those with weakened immune systems . We report a cluster of community-acquired VACV infections at a martial arts gym in Maryland, USA.\n\n【1】### Case-Patients\n\n【2】In July 2008, the Michigan Department of Community Health (MDCH) Bureau of Laboratories reported a suspected orthopoxvirus infection to the Centers for Disease Control and Prevention (CDC). In the affected person, a 26-year-old male resident of Maryland , multiple pustules had developed on the arm, chin, and back of the knee on June 16 . He sought treatment after a fever and headache developed on June 19 and was advised to go to an emergency room if his fever worsened . His condition did not improve, and he was hospitalized in Maryland on June 23. His lesions were umbilicated pustules ≈0.5 cm in diameter with similar morphologic features . The cause of his illness remained undetermined, and he was discharged on June 26 after defervescence. Contact precautions were employed during the hospitalization; however, the patient was not isolated in a negative-pressure isolation room.\n\n【3】Lesion samples were collected on June 24 and forwarded by the Maryland hospital to a virology reference laboratory in Michigan for testing. The samples were negative by PCR for varicella, adenovirus, and herpes simplex virus. Cytopathic effect suggestive of an orthopoxvirus was noted in MRC-5, A549, and primary rhesus monkey kidney cells, and the samples were forwarded to the MDCH laboratory for further testing. On July 4, the MDCH laboratory confirmed the presence of orthopoxvirus DNA in the lesion sample using an orthopoxvirus (nonvariola) and an orthopoxvirus generic real-time PCR . The specimens were sent to CDC for confirmation and virus species identification. Real-time PCRs designed to differentiate orthopoxvirus species were performed, and samples were positive for VACV DNA but not for monkeypox virus DNA  .\n\n【4】The Maryland Department of Health and Mental Hygiene was contacted on July 3 and, in collaboration with the Montgomery County Department of Health and Human Services, began an investigation. The patient was asked whether he recently received smallpox vaccination or had history suggestive of exposure to orthopoxviruses such as monkeypox virus (i.e. contact with animals, recent international travel). He reported having neither; however, his wife and child had returned from a trip to Brazil 2 weeks before his illness. Human VACV infections caused by contact with infected dairy cattle occur in regions of Brazil . His wife only visited an urban area in Brazil, reported no contact with farm animals, and reported no fever or rash.\n\n【5】Sequence analysis of a 160-bp fragment of the hemagglutinin gene from the virus isolate was performed at CDC to determine whether the VACV strain originated from smallpox vaccine or from a strain that occurs naturally in Brazil. The isolate matched the strain used in the ACAM2000 smallpox vaccine and was distinctive from known Brazilian VACV . ACAM2000 is the second-generation smallpox vaccine, which replaced Dryvax in January/February 2008 .\n\n【6】The patient reported belonging to a martial arts gym; he reported having several military personnel as recent sparring partners before the onset of his illness. He also reported that a recent sparring partner had exhibited a rash around the same time. This person, a 28-year-old man (case-patient 2), was contacted and described having a 4-day rash on his right forearm in mid to late June with no systemic symptoms . Samples of his serum, collected 2–3 weeks after lesion onset, were sent to CDC for testing and showed modestly elevated levels of anti-orthopoxvirus immunoglobulin (Ig) M antibodies . Scab material tested at CDC was weakly positive for orthopoxvirus DNA by using the orthopoxvirus nonvariola and orthopoxvirus (generic) PCRs.\n\n【7】In the absence of an explanation for these 2 VACV (ACAM2000) infections, Maryland public health officials launched an investigation at the gym to identify additional cases and pinpoint the source of infection. Approximately 400 surveys were distributed to gym members through email and by hand at the gym. Members were asked whether they had any recent skin lesions similar to those shown in an attached photo. They were asked whether they had recently received a smallpox vaccination or had contact with someone recently vaccinated.\n\n【8】Ninety-five gym members responded to the survey. Several reported having received a smallpox vaccination previously, but none reported vaccination within the prior 2 months. Thirteen gym members reported skin lesions or rash but no recent smallpox vaccination. Two of these persons (case-patients 3 and 4) were clinically diagnosed with methicillin-resistant _Staphylococcus aureus_ (MRSA) by health care providers in late June and early July. Both attended the gym on the same day as case-patient 1 . Serum samples were collected from these men 3–4 weeks after lesion onset and sent to CDC for testing. Both had elevated levels of anti-orthopoxvirus IgG and IgM antibodies, indicative of a recent exposure .\n\n【9】Maryland public health officials reviewed cleaning protocols at the gym. They determined that equipment and pads were cleaned at least twice daily (stemming from a concern about MRSA transmission) and that appropriate cleaning products were being used.\n\n【10】CDC identified 5 civilian clinics that had received ACAM2000 vaccine since late February in the Maryland area. These clinics reported having vaccinated 65 persons; none were members of the martial arts gym. The Military Vaccine Agency (Milvax) cross-checked its list of recent military vaccinees against the gym member list since late February. Although several of those identified as being vaccinated had an association with the gym, they were either not currently gym members or were not at the gym during this period. The source of virus introduction into the martial arts gym remains unknown. No further infections have been identified among gym members or health care workers exposed to case-patients.\n\n【11】### Conclusions\n\n【12】This cluster of community-acquired VACV infection was possibly the result of sequential person-to-person spread of virus through direct physical contact, although transmission through fomites cannot be ruled out. The ultimate source-person responsible for introducing the virus into the gym was not identified, but given the limited time that ACAM2000 had been available to providers in the region (late February 2008), the most likely source was a recent vaccinee. None of the current gym members were known to have been vaccinated within the 4 weeks before illness onset of the first case-patient. Unrecognized transmission of VACV among gym members may have been ongoing over several months.\n\n【13】Multiple cases of VACV infection caused by secondary transmission have been noted recently . Materials such as towels and bedding used by the vaccinee should be treated as potential fomites and should not be shared with others . To our knowledge, this is the first reported cluster of community acquired VACV in which an obvious source-person was not identified. This cluster highlights the need to reinforce transmission precautions to recent vaccinees and indicates that physicians should include VACV infections on the differential of vesiculopustular rash lesions and take appropriate infection control precautions, even in the absence of a known exposure to smallpox vaccine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db29fbad-da0b-4312-add3-e9b7e1fb91ad", "title": "Retrospective Cohort Study of Lassa Fever in Pregnancy, Southern Nigeria", "text": "【0】Retrospective Cohort Study of Lassa Fever in Pregnancy, Southern Nigeria\nLassa fever (LF), a viral hemorrhagic fever endemic to West Africa , was first reported in 1969 from northern Nigeria . Since that time, LF has been documented in several countries in West Africa, including Sierra Leone, Liberia, Guinea, Mali, and, more recently, Benin and Togo . Historical reports of LF in pregnancy have described poor maternal and fetal outcomes; an early direct comparison in Sierra Leone reported a 50% mortality rate in pregnant women (n = 30), compared with 16% among nonpregnant women (n = 234) . A subsequent study conducted in Sierra Leone in 1988 observed a smaller disparity of a 21% mortality rate in pregnant women (n = 68) versus 13% among nonpregnant women (n = 79) but found a worse mortality rate (%, n = 40) for mothers in the third trimester and an overall fetal and neonatal mortality rate of 87% . Although the Sierra Leone study demonstrated the considerable contribution of LF to overall maternal mortality rates at a single hospital, since then, large studies of pregnant patients with LF have been lacking, leading to difficulty in estimating the actual regional burden on maternal health. This problem is exacerbated by the large variability in Lassa virus across regions  and the often nonspecific early signs and symptoms of the disease, including overlap with other common infectious diseases in the region, such as malaria, influenza, and bacterial sepsis .\n\n【1】Pathophysiologically, the poor outcome of LF in pregnancy has been attributed to the higher viral loads often observed in pregnant compared with nonpregnant patients, possibly because of the poorly understood immunologic changes in pregnancy or the affinity of the virus for the highly vascularized placenta . Also, the overlap of symptoms such as nausea, headache, and abdominal pain with complicated or even uncomplicated pregnancy might further delay identification or diagnosis and result in worse outcomes when the infection is severe .\n\n【2】The management of LF in pregnancy requires making difficult decisions with sparse data for guidance. The antiviral drug ribavirin represents the only established pharmacologic therapy for LF and is believed to substantially reduce overall mortality rates , although the mechanism of action is not clear and data on safety of the drug in pregnancy are limited . The 1988 Sierra Leone study, which has been the largest LF case series in the literature to date, found that delivery, spontaneous abortion (miscarriage), and evacuation of the uterus all improve maternal outcome ; the study recommended active obstetric management, particularly because the authors observed high fetal mortality rate irrespective of the modality of management . However, patients in the study did not receive ribavirin before delivery because of published evidence of teratogenicity in animal studies . To improve maternal and fetal outcomes and explore their relationship to clinical signs and symptoms, we retrospectively analyzed >9 years of records at a hospital in Nigeria that treated a substantial number of LF cases and applied both a more conservative approach to obstetric management and more liberal antepartum use of ribavirin.\n\n【3】### Methods\n\n【4】Irrua Specialist Teaching Hospital (ISTH) is a tertiary-care federal hospital that served as the LF national referral center for the duration of our study. Providers at ISTH and outside facilities in >30 states refer specimens for Lassa virus (LASV) testing by reverse-transcription PCR (RT-PCR) for patients who fit our previously described case definition ; outside patients who test positive by RT-PCR are often referred to ISTH for management. Two authors (S.O. and J.O.) reviewed and abstracted from hospital medical records clinical data for every laboratory-confirmed case of LF in a pregnant woman eventually admitted to ISTH during January 2009–March 2018. During this period, 44 pregnant women were managed for LF. On initial review of these patient records, we found 30 patients for whom documentation of signs and symptoms at admission and during hospitalization were sufficiently complete for inclusion in this study. Comparison data for facility maternal mortality rates over the same period was based on summaries maintained by the ISTH Department of Obstetrics and Gynaecology.\n\n【5】During the study period, pregnant women with confirmed LF received the same supportive care as nonpregnant LF patients, including intravenous fluids (either normal saline or lactated Ringer solution) to improve organ perfusion. All patients had antimicrobial prophylaxis with broad-spectrum antibiotics (ceftriaxone in most cases) and antimalarial drugs (typically oral artemether/lumefantrine; when required, parenteral treatment was quinine before 2014 and artemether after 2014). Fifteen of the 30 pregnant women in our series received blood transfusions for anemia (defined as packed red cell volume < 25%), and 2 women required renal dialysis (they were referred for nephrologic evaluation when urine output was <0.5 mL/kg/h, although final decision to dialyze was clinical).\n\n【6】All patients were treated with intravenous ribavirin, except 2 patients who died shortly after arrival. Intravenous ribavirin treatment was started with a 100 mg/kg loading dose (two thirds dose on admission, one third 8 hours later), then 16 mg/kg every 6 hours for 4 days, followed by 8 mg/kg every 8 hours for 6 days. All pregnant women had an ultrasound scan on arrival to confirm fetal cardiac activity. After admission, fetal well-being was monitored at least 2 times each week by using either Doppler velocimetry, a nonstress test, or a biophysical profile, as appropriate for gestational age. We used a conservative obstetric management strategy throughout, only inducing labor in cases of fetal compromise and only performing uterine evacuation in women with fetal death. When uterine evacuation was required, we initiated vaginal misoprostol, dosed as appropriate for gestational age and consistent with International Federation of Gynecology and Obstetrics guidelines , followed by manual vacuum aspiration and curettage when required.\n\n【7】Before 2014, women who cleared viremia on the basis of RT-PCR results were discharged home and had outpatient weekly follow-up, including a nonstress test or biophysical profile. After 2014, in response to a case of late intrauterine fetal death after discharge, women were kept as inpatients for close monitoring until delivery. In all cases, patients had postpartum follow-up at 2 and 6 weeks after delivery. Newborns were not discharged until \\> 1 week after delivery, in accordance with hospital protocol for LASV-exposed infants.\n\n【8】For our study, we defined the first trimester as <14 weeks from the first day of the last menstrual period (LMP), the second trimester as 14 weeks to <28 weeks from LMP, and the third trimester as \\> 28 weeks from LMP. We defined stillbirth as delivery after 28 weeks’ gestation of a baby without signs of life (i.e. absent cardiac and respiratory activity). Perinatal mortality rate attributable to LF was defined as the proportion of all pregnancies with laboratory-confirmed maternal diagnosis of LF that ended in either intrauterine fetal death after 28 weeks, stillbirth, or neonatal death in the first week of life. Overall maternal mortality rate was defined as the proportion of women who died while pregnant or within 42 days of the end of pregnancy, irrespective of the duration or site of the pregnancy, from causes related to or aggravated by pregnancy or its management, but not from accidental or incidental causes . When discussing maternal mortality rates related to LF in the context of this study, we refer to the proportion of women in whom LF was diagnosed during pregnancy and who did not survive that pregnancy.\n\n【9】For statistical analysis, we calculated the mean \\+ SD for quantitative variables, whereas we used prevalence to characterize qualitative variables. We calculated the univariate correlation between clinical features (signs and symptoms) with maternal and fetal death and then ranked these features by the p value of their association with maternal outcome (either death or survival) by using a χ 2  test with Yates correction. Tests of statistical significance (defined as p < 0.05) were based on a 95% CI.\n\n【10】### Results\n\n【11】Forty-four cases of LF in pregnancy were managed at ISTH during the study period, out of a total of 5,048 pregnant women admitted to ISTH during that period. The 30 women for whom we were able to recover complete records were 16–39 years of age (mean \\+ SD 28.1 \\+ 5.1 years). These 30 women were admitted at gestational ages of 5–39 weeks (mean 21.6 \\+ 10.6 weeks) . Parity ranged from 0–6 (mean 3.0 \\+ 1.6). One woman had a twin pregnancy.\n\n【12】We divided clinical signs and symptoms into 2 broad patterns. First, 16 women had complications, defined as either coma, convulsions, irrational behavior, extravaginal bleeding, or oliguria. All of these 16 women were also found to have intrauterine fetal death or an abortive process. Maternal death ensued within 24–48 hours in 10/16 (.8%) of these cases and, in most cases, uterine evacuation was not completed before death. The second clinical pattern was observed in 14 women who had with milder, nonspecific symptoms, including fever, malaise, cough, and sore throat. Thirteen of these 14 women had a live fetus on ultrasound, and only 1 (.1%) of the 14 women died. Irrespective of initial signs and symptoms, in all cases of intrauterine fetal death, uterine evacuation was initiated using misoprostol vaginal tablets as we described. Four of these patients died before complete evacuation, and 5 women required manual vacuum aspiration for evacuation of retained products of conception.\n\n【13】All 30 women had fever. Overall, 19 women survived and 11 died, resulting in an overall LF-related maternal mortality rate of 36.7% in this cohort. Seven of the 11 deaths occurred within 24 hours of admission, and 4 occurred within 24–48 hours of admission. Fetal death in utero, retrosternal chest pain, vomiting, and cough were the most frequent clinical features apart from fever, whereas deafness was the least frequent . Extravaginal bleeding, convulsions, oliguria, fetal death in utero, and cough were each significantly associated with maternal death. In contrast, admission with a live fetus and breast pain or engorgement were each significantly associated with survival.\n\n【14】The mean \\+ SD duration of symptoms before admission was 6 \\+ 3.1 days among the survivors and 10 \\+ 3.5 days among those who died. When divided by trimester of pregnancy, maternal mortality rate was 50.0% (/10) in the first, 75.0% (/4) in the second, and 18.7% (/16) in the third trimester. We noted a trend toward a lower maternal mortality rate in the third trimester, but this difference did not reach statistical significance (p = 0.06). Overall, deaths attributed to LF accounted for 13.1% (/84, out of 5,048 total admissions of pregnant women) of maternal deaths at ISTH during the study period, second only to postpartum hemorrhage (.5%) and eclampsia (.6%).\n\n【15】Overall, 17/31 (.7%) of fetuses had died by the time of admission. Of the 13 women with viable pregnancies at admission, 1 had a twin gestation and subsequently delivered 1 live baby (Apgar score 4 at 1 minute, 8 by 5 minutes) without apparent abnormality and 1 stillbirth at term. Another patient returned with delayed intrauterine fetal death at 38 weeks’ gestation, despite having cleared her viremia after initial examination 9 weeks earlier, and died shortly after the return admission; a repeat LASV RT-PCR test was not performed. Five of the women with viable pregnancies at discharge had full-term deliveries, 5 had preterm deliveries at 33–36 weeks’ gestation, and 1 had early neonatal death on the third day of life. The rate of perinatal death or spontaneous abortion in women who were admitted with a live fetus was 21.4% (/14), compared with an overall rate of fetal and perinatal loss of 64.5% (/31). In relation to the fetus, the mortality rate was significantly lower when the mother was admitted to the hospital in the third trimester; the rate was 31.2% (/17) for the third trimester compared with 92.9% (/14) for the first and second trimesters combined (p<0.001). Maternal signs and symptoms that included extravaginal bleeding, convulsions, oliguria, or vaginal bleeding were each associated with fetal death (p<0.01) .\n\n【16】Finally, we noted that 17 (.7%) of the 30 women had been to other health facilities before admission, whereas the remaining 13 (.3%) patients sought care at our hospital as their first point of call. Among the 17 women from other facilities, LF was suspected in 8 cases; differential diagnoses included eclampsia, malaria, pyelonephritis, miscarriage, puerperal sepsis, and placenta previa.\n\n【17】### Discussion\n\n【18】LF is endemic to several states in Nigeria, and the number of women at risk for LASV infection during pregnancy is high. Our study highlights the contribution of LF to maternal mortality rates at a teaching hospital in Nigeria, where LF was diagnosed in 44/5,048 (.87%) of all admitted pregnant women but accounted for 11/84 (.1%) of maternal deaths at the facility during the study period. The 36.7% LF-related maternal mortality rate we report is roughly in the range of a case series described in Sierra Leone in 1988 (%)  and of limited data from Liberia (where the LF-related mortality rate reported before 1973 was 33%–75%) , although detailed comparison is difficult given the sample sizes involved, decades of separation, and different geographic and genetic contexts.\n\n【19】Most of the deaths in our case series occurred during 2009–2013 (/19, 47.4%); the mortality rate decreased by more than half for 2014–2018 (/11, 18.2%). Despite the relatively small numbers in our study, a few key changes occurred in Nigeria, and specifically at ISTH, that are consistent with dropping mortality rates. First, testing of LF and other viral hemorrhagic fevers increased after the 2013–2016 Ebola virus disease outbreak. Second, ISTH has dramatically increased resources devoted to LF research and clinical care. Third, public health activities at ISTH and throughout Edo State increased, including a series of community campaigns and seminars for healthcare workers, with the goal of raising awareness of LF and improving management strategies.\n\n【20】Ongoing public health efforts presume the importance of timely referrals and quick diagnosis of LF, given that poor health-seeking behaviors resulting from lack of awareness are believed to delay medical care and worsen outcomes . Furthermore, in our study, 64% of maternal deaths occurred within 24 hours of admission and 100% within 48 hours of admission, suggesting that the disease was often advanced before referrals were made, a finding consistent with previously described delays in care for other obstetric complications . This observation further emphasizes the need for continuous community engagement, healthcare worker sensitization, an increased index of suspicion in LF-endemic areas, and ready availability of rapid diagnostic tools. The actual contribution of LF to maternal mortality rates is likely underreported in Nigeria, and we suspect LF constitutes a hidden cause of maternal deaths in several LF-endemic communities in a country that already has one of the highest maternal mortality rates in the world .\n\n【21】Extravaginal bleeding, convulsions, and oliguria have been found to be associated with poor outcomes in nonpregnant adults , and we have found similar associations in maternal and fetal prognosis. In addition, we report the concerning finding that more than half of mothers who were admitted to our hospital with a deceased fetus eventually died themselves. This finding is consistent with the experience reported from Sierra Leone in 1988 , although the causal relationship between intrauterine death and severe LF disease is unknown. The number of cases in our retrospective series was insufficient to compare strategies of evacuation alone, evacuation and ribavirin, or ribavirin alone; further studies are needed to determine the best approach to obstetric care.\n\n【22】Although in Sierra Leone a major correlate of poor outcome was the diagnosis of LF in the third trimester , we did not observe the same correlation. Many factors might have contributed to this difference; however, we note that the Sierra Leone study endorsed active uterine evacuation in the third trimester and deferred ribavirin , whereas ISTH uses ribavirin as early therapy in all cases. Because ribavirin has been reported to be teratogenic in animal studies and data on its safety in pregnant women are insufficient , our decision to use this treatment was not taken lightly. However, although high-quality efficacy trials are lacking and safety data are pending, ribavirin is one of few therapeutic modalities available to us in this situation. The results in terms of maternal and fetal outcomes have been positive in our experience.\n\n【23】The favorable maternal outcome associated with admission with a live fetus suggests the importance of early detection, and we believe the results acquired through our conservative management of these patients challenge the view that active uterine evacuation is required, at least in our setting. Although we did find a significant risk of fetal death overall, we are encouraged because 12 of the 13 women admitted with live fetuses survived the illness, and 11 live births resulted. Our single experience with late uterine death of unexplained cause did increase our surveillance of pregnant survivors of LF, and we now admit such women for more frequent fetal monitoring (biometric and biophysical surveillance) and the option to induce labor in case of fetal compromise.\n\n【24】The generalizability of our findings is limited by a relatively small sample size at a single tertiary-care hospital and the exclusion of several cases because of incomplete records. We further recognize that we lack clinical laboratory data, quantitative RT-PCR assays needed to assess viral load, and pathologic confirmation of cause of death. Although availability of these data points has improved over time at our hospital, we note that they are not always routinely available in our resource-limited setting. We have instead highlighted key clinical findings associated with poor prognosis, including extravaginal bleeding, convulsions, oliguria, and fetal death, that do not require infrastructure for laboratory or pathologic confirmation. In our hospital, women with any of these signs or symptoms receive emergent and aggressive supportive care, including ribavirin therapy.\n\n【25】In conclusion, LF in pregnancy has a high case-fatality rate and is an important and likely underreported cause of maternal death in LF-endemic areas of Nigeria and in other West African countries. Admission to care with a live fetus is predictive of improved maternal outcome, and conservative obstetric management with early ribavirin (instead of evacuation) is producing good outcomes in our hands. Further studies are needed to confirm these conclusions and to provide an updated, data-driven algorithm for clinical management of pregnant women with LF.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6cb55ba2-079a-4fd0-b44e-d99019223ac5", "title": "Human Herpesviruses 6 and 7 and Central Nervous System Infection in Children", "text": "【0】Human Herpesviruses 6 and 7 and Central Nervous System Infection in Children\nHuman herpesvirus (HHV)-6 and HHV-7 are ubiquitous T-lymphotropic viruses that infect most humans. Infections with either agent occur primarily during childhood. Seroprevalence of HHV-6 reaches >80% in children >2 years . Antibody prevalence for HHV-7 reaches 75% in 3- to 6-year-old children and 98% in adults . HHV-6 and HHH-7 have been associated with a variety of clinical manifestations, including fever, rash, and seizures . Immunocompromised hosts, particularly transplant recipients, are at increased risk for symptomatic primary or reactivation disease associated with HHV-6 or HHV-7 .\n\n【1】The role of HHV-6 and 7 in central nervous system (CNS) disease is an area of ongoing investigation. The range of CNS manifestations ascribed to these viruses includes asymptomatic infection, febrile convulsions, seizure disorders, meningitis, meningoencephalitis, facial palsy, vestibular neuritis, demyelinating disorders, hemiplegia, and, rarely, fatal encephalitis . Investigators have been unable to culture HHV-6 or HHV-7 from cerebrospinal fluid (CSF) . However, HHV-6 and HHV-7 DNA have been detected in CSF and other body fluids by polymerase chain reaction (PCR), which implicates these viruses in neurologic disorders. HHV-6 DNA was identified in CSF of 14.8% of children evaluated for fever, sepsis, or seizures, with higher prevalence found among children with seizures . HHV-6 DNA was also detected in CSF of 70% to 90% of children who had neurologic symptoms during their primary HHV-6 infection, with a disproportionate association with recurrent febrile seizures . In a case-control study, HHV-6 DNA was found in CSF of 23% of patients who received an allogeneic bone marrow transplant who had CNS symptoms; it was found in <1% of patients with hematologic malignancies without neurologic symptoms . Other investigators have found a much lower prevalence (%–4%) of HHV-6 DNA in CSF of AIDS patients with neurologic symptoms and in CSF of children with febrile seizures . Similarly, although HHV-7 DNA has been detected in CSF of as many as 8.8%–14% of children with neurologic symptoms , other studies have found a lower prevalence (%–2%) in CSF of AIDS patients with neurologic symptoms and in children with febrile seizures .\n\n【2】Because of the conflicting results in the medical literature, the frequency at which HHV-6 and HHV-7 are associated with neurologic disease is unclear. The goal of this study was to further define the role of HHV-6 and HHV-7 as causes of CNS disease in children.\n\n【3】### Materials and Methods\n\n【4】##### Study Design\n\n【5】The study, approved by the University of Colorado Multiple Institutional Review Board, was conducted with all CSF clinical samples from pediatric patients submitted for herpes simplex virus (HSV) PCR to the Clinical Virology Laboratory at the University of Colorado from December 1998 through February 2000. When multiple specimens were submitted for one patient, only the first one was tested. Specimens positive for other microorganisms were not excluded. Peripheral blood specimens from these patients were not available to study. Information regarding demographics, clinical manifestations, diagnostic studies, management, discharge diagnosis, and outcome was gathered by retrospective chart review for patients seen at The Children’s Hospital, Denver. CNS diagnoses and classification of seizures were based on the assessments of the primary treating physicians.\n\n【6】##### Definitions\n\n【7】Infectious and postinfectious encephalitis were defined as the presence of encephalopathy or focal neurologic abnormalities, an abnormal CSF profile but negative CSF microbiologic studies, and a history or serologic result consistent with a current or preceding acute infectious illness. CSF pleocytosis was defined as >25 leukocytes x 10 6  /L for preterm neonates, >22 leukocytes x 10 6  /L for term neonates, and >7 leukocytes x 10 6  /L for all other patients. Infections not involving the CNS were classified as other infections.\n\n【8】##### HHV-6 PCR\n\n【9】HHV-6 PCR was performed  with the following primers and probes : 5′ AAG CTT GCA CAA TGC CAA AAA ACA G , 5′ AAC TGT CTG ACT GGC AAA AAC TTT T , and 5′ AAC TGT CTG GCA AAA ACT TTT . DNA was extracted from 50-μL aliquots of CSF previously stored at –70°C with Chelex purification matrix (Bio-Rad Laboratories, Inc, Hercules, CA). PCR was carried out in 50-μL mixtures containing 20 μL of extracted DNA, 1.25 units of PfuI (Stratagene, La Jolla, CA), 200 μM of each of four deoxynucleoside triphosphates, and 0.5 μM of each primer in PfuI buffer (Stratagene). The samples were amplified in duplicate for 45 cycles. The amplified DNA was separated according to molecular weight by using 3% agarose gel electrophoresis and transferred to nylon membranes. The identity of the DNA band (bp) was confirmed by detection with a digoxigenin-conjugated probe, antidigoxigenin antibody conjugated to alkaline phosphatase (Boehringer Mannheim Biochemicals, Indianopolis, IN), and chemiluminescent substrate (Tropix, Bedford, MA). Each patient sample was run in duplicate. Each assay included two negative controls (water) and two positive controls (genomic copies of HHV-6 DNA/reaction tube). The tests were considered valid if the controls yielded the expected results, and a specimen was considered positive if both replicates tested positive. Specimens with discordant replicates were reanalyzed in an independent run and considered positive if \\> 50% of replicates gave positive results. The ability to detect HHV-6 DNA after at least four freeze-thaw cycles remained intact.\n\n【10】The analytical sensitivity of HHV-6 PCR was 8–10 copies of genomic DNA per reaction tube, as determined by serial dilutions of a sample with known DNA copy number. To rule out the presence of inhibitors in the extracted DNA, 20 HSV-negative CSF samples were spiked with the equivalent of 2 HSV DNA copies per PCR reaction tube. All samples yielded a positive result for HSV, whereas the unspiked specimens remained HSV-negative. The ability to detect HHV-6 strains from different geographic regions was confirmed by using 14 HHV-6–containing specimens obtained from diverse laboratories with recognized expertise in diagnosing HHV-6 infections as well as our laboratory. No cross-reactivity occurred with other DNA viruses, including other herpesviruses, parvovirus, adenovirus, and polyomavirus, thereby establishing the specificity of the HHV-6 PCR.\n\n【11】##### HHV-7 PCR\n\n【12】HHV-7 PCR was performed by using the methods described for HHV-6 with modified sample preparation. The primers were 5′ TAT CCC AGC TGT TTT CAT ATA GTA AC and 5′ GCC TTG CGG TAG CAC TAG ATT TTT TG, and the probe was 5′ AGA ATT CTG TAC CCA TGG GCA CAT TTG TAC . The PCR product was 186 bp long. The assay included two negative controls (water) and two positive controls (genomic copies of HHV-7 DNA per reaction tube).\n\n【13】The sensitivity of HHV-7 PCR was 8–10 copies of DNA per reaction tube, as determined by detecting HHV-7 DNA from samples extracted from infected cells. Specimens were prepared by boiling aliquots of nonhemorrhagic CSF for 55 s. If CSF contained blood, DNA was extracted with Qiagen DNA extraction kit (Qiagen, Valencia, CA). Testing with control viral DNA indicated that boiling did not affect the sensitivity of the PCR. The presence of inhibitors was ruled out by spiking 20 HSV-negative CSF samples with the equivalent of two HSV DNA copies per reaction tube and showing that all spiked samples became HSV-positive, whereas unspiked samples remained negative. The ability to detect HHV-7 DNA after at least three freeze-thaw cycles remained intact.\n\n【14】### Results\n\n【15】##### Study Population\n\n【16】CSF samples obtained from 245 patients were tested for HHV-6 and HHV-7 DNA by PCR. Clinical data were available for the 218 patients hospitalized at The Children’s Hospital, Denver . The median age was 43 days; 68% were < 2 years of age; 13% were 2–6 years, and 19% \\> 6 years. Fever occurred in 56% of patients, rash in 11%, and seizures in 25%. CNS disorder diagnoses at discharge included meningitis in 61 patients (enteroviral, 21; bacterial, 4; aseptic/indeterminate etiology, 36), encephalitis in 21 patients (HSV, 4; acute disseminated encephalomyelitis, 4; infectious/postinfectious, 13), febrile seizures in 7 patients, and seizure disorder in 29 patients. A non-CNS disease had been diagnosed in the remaining 100 patients, which included a diagnosis of other infections in 60. Nineteen patients were immunocompromised (leukemia, 6; posttransplant, 3; premature, 3; HIV, 2; others, 5), of whom 1 had a febrile seizure, 1 had encephalitis, 2 had meningitis, and 5 had other infections. CSF pleocytosis was found in 90 (%) of 209 patients for whom data were available.\n\n【17】##### HHV-6 Infections of the CNS\n\n【18】PCR identified HHV-6 DNA in CSF of 3 of 245 patients . Patient 1, a 5-day-old full-term male infant of a diabetic mother, had a generalized seizure on the day he was born. His seizure was attributed to hypoglycemia because his concurrent glucose level was 0 mmol/L. On day 5 of life, a fever of 38.3ºC developed for which he underwent a complete workup for sepsis. Results of a physical examination were normal with no signs or symptoms of neurologic dysfunction. His blood count was remarkable for a leukocyte count of 8,600 x 10 6  /L (% neutrophils and 21% bands) and a platelet count of 40,000 x 10 6  /L. His CSF showed 40 leukocytes x 10 6  /L (% neutrophils, 9% bands, 31% lymphocytes, 33% monocytes, and 6% macrophages), 4,110 erythrocytes x 10 6  /L, protein 169 g x 10 -2  /L, and glucose 2.3 mmol/L. CSF HSV PCR, viral culture, and bacterial cultures of CSF and blood and urine were negative. Diagnosis at discharge was aseptic meningitis. The patient recovered spontaneously without sequelae.\n\n【19】Patient 2 was a 38-day-old full-term female infant who had a fever of 39.2ºC and irritability. The physical examination showed a few vesicular lesions on the tonsillar pillar as well as a diffuse, erythematous, macular blanching rash. CSF studies found 19 leukocytes x 10 6  /L (% lymphocytes, 12% monocytes, and 55% macrophages), 1 erythrocyte x 10 6  /L, protein 48 g x 10 -2  /L, and glucose 2.3 mmol/L. PCRs of CSF for HSV and enterovirus and bacterial cultures of CSF, blood, and urine were negative. She recovered spontaneously without sequalae. Her discharge diagnosis was aseptic meningitis.\n\n【20】Patient 3 was a male infant of 24 weeks’ gestation with respiratory distress syndrome. A blood culture was taken on day 10 of life because of worsening respiratory distress; results were positive for _Candida_ . A lumbar puncture was performed when the blood grew _Candida_ . The patient did not have any neurologic symptoms or signs. CSF had 1 leukocyte x 10 6  /L, 1,780 erthrocytes x 10 6  /L, protein 105 g x 10 -2  /L, and glucose 8.1 mmol/L. CSF studies, including PCRs for HSV and enterovirus, as well as bacterial, viral, and fungal cultures, were negative. He subsequently died of the complications of prematurity and candidal sepsis, which were unrelated to the CNS.\n\n【21】All 3 were <2 months of age, and 2 had CSF pleocytosis. Overall, HHV-6 DNA was detected in 2 (.2%) of 90 specimens from patients with pleocytosis and in 2 (.3%) of 61 patients with a discharge diagnosis of meningitis. HHV-6 DNA was not found in any patients who had encephalitis, febrile seizures, or a seizure disorder. None of the 19 immunocompromised patients had HHV-6 DNA in CSF.\n\n【22】##### HHV-7 Infections of CNS\n\n【23】HHV-7 DNA was not found in any of the 245 CSF samples tested by PCR. Spiking random samples with HHV-7 DNA generated positive PCR results.\n\n【24】### Discussion\n\n【25】HHV-6 DNA was uncommon in CSF of children (.2%), and HHV-7 DNA was not detected in any CSF specimen in this study. HHV-6 DNA was found in a small percentage of meningitis patients (.3%), but neither HHV-6 nor HHV-7 DNA was detected in samples from any patients with encephalitis, febrile seizures, or seizure disorders. Our limited detection of HHV-6 and HHV-7 contrasts with results of other studies, such as the report of Caserta et al. in which HHV-6 DNA was found in CSF of 14.8% of children evaluated for fever, sepsis, or seizures, and studies by Pohl-Koppe et al. and Yoshikawa et al. in which HHV-7 DNA was found in CSF of 8.8% to 14% of children with neurologic symptoms. Our findings may have differed from previous reports for the following reasons: 1) differences in PCR sensitivity; 2) differences in degree of intactness of DNA in specimens that were thawed after being frozen for prolonged periods; 3) geographic variation; or 4) differences in patient populations and, particularly, differences in age groups.\n\n【26】Technical laboratory problems were unlikely. Our assays are as sensitive or more sensitive than those previously reported that use plasmid-encoded DNA, and we detected a panel of samples identified by other laboratories, which use different primers or probe sets. However, these latter experiments were limited in number and could not completely rule out sensitivity differences related to the primers and probe sets. The presence of inhibitors was ruled out by spiking experiments. We tested the effect of freezing and thawing by subjecting several samples to three to four freeze-thaw cycles and found good preservation of both HHV-6 and HHV-7 DNA. Also, the PCR method used in this study detected cytomegalovirus DNA in CSF specimens stored at –70°C for up to 6 years . Yoshikawa et al. who reported a 16% incidence of HHV-6 and HHV-7 in their pediatric patients with neurologic disorders, detected the viral DNA in the CSF cell pellet, but not in the liquid phase. Other investigators, however, who found a high incidence of HHV-6 and HHV-7 in pediatric patients with neurologic disorders , recovered viral DNA from liquid CSF.\n\n【27】Our results are not likely to be biased by the study population. Although lower seroprevalence rates have been found in some regions of the world, for example, Morocco (% for HHV-6) and northern Japan (% for HHV-7) , no evidence has been found of low seroprevalence of HHV-6 and HHV-7 infections in specific regions of the United States. Also, the demographic characteristics of our study population were similar to those of the 487 children described by Caserta et al. including a median age similar to the median age of 1.5 months (range 0.1–36) in the latter series . The populations evaluated by Yoshikawa et al. and Pohl-Koppe et al. were somewhat older, with mean ages of 6 and 2.8–6 years, respectively. However, the number of patients >2 years of age included in our study population was similar to the numbers of participants enrolled in these two studies (and 68, respectively). Therefore, age differences do not appear to explain the discrepant findings.\n\n【28】The clinical features of patients 1 and 2 and the absence of another identified infectious agent suggest that HHV-6 may have been the cause of fever, pleocytosis, and the clinical aseptic meningitis diagnosed in these two infants. Other studies have not found an association between HHV-6 DNA in CSF and CSF pleocytosis. However, we note that the CSF of patient 1 was contaminated by blood. Therefore, the presence and magnitude of pleocytosis are difficult to state with certainty; moreover, we cannot be certain whether HHV-6 DNA was truly present in CSF or derived from contamination with peripheral blood.\n\n【29】In patient 3, the identification of a well-established pathogen and the normal results of neurologic and CSF examinations suggest that HHV-6 DNA in the CSF may have represented an asymptomatic infection or contamination with peripheral blood. The cases of HHV-6 infection of CNS occurred in infants <2 months of age, and two occurred in the early neonatal period. Infections in newborns have been previously described. Although horizontal transmission through saliva is postulated to be the most common method of transmission of HHV-6 and HHV-7 , infection in neonates as well as detection of HHV-6 DNA in aborted fetuses’ cord blood and in the female genital tract suggest the that transplacental or perinatal transmission is possible .\n\n【30】Our study included 29 patients with CNS disorders caused by a known pathogen other than HHV-6 and HHV-7 (enterovirus, herpes simplex virus, bacterial pathogen); none of these patients had HHV-6 or HHV-7 DNA in CSF. This finding suggests that HHV-6 or HHV-7 DNA is not frequently detected in CSF in association with other CNS infections, as might be expected if these viruses established latency in CNS and reactivated nonspecifically with other pathogens. Conversely, this finding supports the notion that detecting HHV-6 or HHV-7 DNA in CSF of patients with neurologic disorders is consistent with a pathogenic role. Our study population included a modest number of immunocompromised patients of whom a small subset had neurologic diagnoses. These numbers are insufficient to ascertain the roles of HHV-6 and HHV-7 in neurologic disease in immunodeficient patients.\n\n【31】In conclusion, our data indicate that HHV-6 and HHV-7 are uncommon causes of CNS infection in children. HHV-6 may occasionally cause meningitis in young infants. Further studies are required to define the role of HHV-6 and HHV-7 in neurologic disorders in immunocompromised hosts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51ea0287-7275-4bff-9406-6b61290b6dfe", "title": "Outbreaks of Tilapia Lake Virus Infection, Thailand, 2015–2016", "text": "【0】Outbreaks of Tilapia Lake Virus Infection, Thailand, 2015–2016\nViral diseases are common causes of illness and death in cultured fish; such viruses include infectious salmon anemia virus, infectious hematopoietic necrosis virus, and viral hemorrhagic septicemia virus . With regard to tilapia, some viral pathogens, including betanodavirus, iridovirus, and herpes-like virus , reportedly cause severe disease. In recent years, Thailand has experienced extensive losses of tilapia; most losses occurred 1 month after transfer of fish from hatchery to grow-out cages in public rivers or reservoirs (month mortality syndrome). During routine investigation of this syndrome, multiple bacterial and parasitic infections were identified. However, no association was established between the outbreaks and any primary causative agent(s). Most deaths occurred within 2 weeks after the first dead fish were found. Similar observations of extensive losses of raised tilapia and wild fish in Israel and Ecuador have been reported . These outbreaks led to identification of a virus affecting tilapia, called tilapia lake virus (TiLV). The epidemiologic pattern and clinical signs for infected fish in Thailand led to suspicion of an illness of unknown etiology that was similar to TiLV infection.\n\n【1】During 2015–2016, we investigated 32 outbreaks involving a large number of deaths of unknown cause among Nile tilapia (Oreochromis niloticus_ ) and red hybrid tilapia (Oreochromis_ spp.). The outbreaks occurred at fish farms in central, western, eastern, and northeastern Thailand . Affected fish were commonly found within 1 month after transfer from the hatchery facility to grow-out ponds or cages. In general, clinical signs and high mortality rates were associated with fish weighing 1–50 g . Mortality rates among tilapia farms were 20%–90%; higher rates were associated with secondary bacterial and parasitic infections. Mortality rates peaked within 14 days after the first dead fish were found.\n\n【2】As part of the outbreak investigation, samples of brain tissue were taken from fish at each of the 32 outbreak locations (each with a mortality rate >1%/day for 3 consecutive days): 10–30 moribund fish and 5–10 apparently healthy fish from the same culture areas. In total, 325 samples were collected and tested for etiologic agent(s) . Samples from fish involved in 22 of the 32 outbreaks were positive for TiLV .\n\n【3】For our study, we selected a field sample positive for TiLV (designated TiLV/Tilapia/Thai/TV1/2016) and processed it for whole-genome sequencing. Another 6 TiLVs were selected for sequencing of the putative polymerase basic 1 (PB1) gene . TiLV genome sequencing was conducted by using newly designed primers based on reference TiLVs available in the GenBank database . Nucleotide sequences of 7 TiLVs from Thailand were submitted to GenBank (accession nos. KX631921–36).\n\n【4】Comparison of the TiLVs from Thailand with those from Israel showed high nucleotide and amino acid identities (.18%–99.10%). Among TiLVs from Thailand, nucleotide and amino acid identities for segment 1 or the putative PB1 gene of the virus were high (.61%–100%) . Genetic analysis of the putative PB1 protein of TiLVs from Thailand and the viruses of the family _Orthomyxoviridae_ showed that TiLVs from Thailand possessed motifs preA, A, B, C, D, and E similar to those of _Orthomyxoviridae_ viruses, including influenza A, B, and C viruses; infectious salmon anemia virus; Dhori virus; and Thogoto virus  . Phylogenetic analysis showed that TiLVs from Thailand were closely related to TiLVs from Israel and grouped with the viruses of the family _Orthomyxoviridae_ but not _Arenaviridae_ and _Bunyaviridae_ . This result suggests that the genetic composition of this emerging virus was similar to that of orthomyxoviruses and homologous with previously published TiLV sequences.\n\n【5】Our PCR and whole-genome findings demonstrate genetic homology between TiLV from Thailand and the etiologic agent of a novel RNA virus infection of tilapia in Israel and Ecuador . Furthermore, the clinical signs and pathological presentation of infection with TiLV from Thailand are similar to those of infection with TiLV from Israel . The clinical signs, gross lesions, and histopathologic lesions combined with virus identification and characterization highlight emerging TiLV in Thailand as the primary cause of the outbreaks. We also found that fish that survived massive die-offs rarely showed clinical signs, suggesting the development of specific immunity against the virus. It should be noted that the TiLVs from Thailand possessed 10 gene segments encoding 10 proteins, including segment 1 or putative PB1 protein. The pattern of protein motifs for this putative PB1 was similar to that for influenza viruses. To our knowledge, TiLV has infected tilapia only, no other aquatic or terrestrial animals.\n\n【6】Our results emphasize that the virus isolated from Thailand shares high sequence similarity with TiLV from Israel, suggesting that this virus spreads across continents. Given that tilapia are the main aquaculture species, control of TiLV will be improved by further efforts such as strict biosecurity, vaccine development, and selection of resistant tilapia breeds.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "463b527f-0094-499d-993e-305a5cbcfdb2", "title": "Plague Epidemics and Lice, Democratic Republic of the Congo", "text": "【0】Plague Epidemics and Lice, Democratic Republic of the Congo\n**To the Editor:** Plague, a zoonotic disease caused by the gram-negative bacterium _Yersinia pestis_ , is transmitted to humans by the bites of infected fleas (such as _Xenopsylla cheopis_ ), scratches from infected animals, and inhalation of aerosols or consumption of food contaminated with _Y. pestis_ . Decades ago, Blanc and Baltazard proposed that human-to-human transmission of _Y. pestis_ could be mediated by human ectoparasites, such as the human body louse . This hypothesis was further supported by experimental data from animal models .\n\n【1】To further test this hypothesis among humans, we conducted a field assessment in April 2010, in which we collected body and head lice from persons living in a highly plague-endemic area near the Rethy Health District, Province Orientale, Democratic Republic of the Congo. This health district has 157,000 inhabitants, and during 2004–2009 it had more suspected plague cases (cases of suspected plague, 39 deaths) than any other health district in the Democratic Republic of the Congo. In April 2010, we visited the dwellings of 10 patients for whom suspected cases of plague had been diagnosed during January–April 2010. All patients had symptoms typical of bubonic plague, and their illnesses were reported as suspected bubonic plague. However, because of the lack of laboratory facilities in Rethy, none of these diagnoses could be microbiologically confirmed.\n\n【2】A total of 154 body lice and 35 head lice were collected from clothes and hair of persons living in or near the patients’ dwellings. Body lice were preserved in ethanol before being sent to the laboratory. Total DNA was extracted by using an EZ1 automated extractor (QIAGEN, Courtaboeuf, France) and subjected to parallel real-time PCRs selective for the _Y. pesti_ s _pla_ gene, the _Rickettsia prowazekii omp_ B gene, a _Borrelia recurrentis_ noncoding genomic fragment, and the _Bartonella quintana_ internal transcribed spacer. _B. quintana_ PCR primers and probe have been shown to be specific for _B. quintana_ . Primers and probe sequences and experimental conditions have been reported . Negative controls contained PCR buffer without DNA, as described . Any amplification with a cycle threshold (C t  ) <40 was regarded as positive.\n\n【3】Negative controls remained negative in all PCR-based experiments, which were not prone to in-laboratory contamination, and all samples were negative for _R. prowazekii_ and _B. recurrentis_ . Conversely, _B. quintana_ was detected in 50 (.5%) of the 154 body lice (C t  18.62–38.45) and 6 (.1%) of the 35 head lice (C t  29.48–38.68). The _Y. pestis pla_ gene was detected in 1 head louse (C t  38), which was negative for the other pathogens, and in 2 body lice (C t  37.36 and 36.97, respectively), which were positive for _B. quintana_ .\n\n【4】_B. quintana_ has been detected in head louse specimens collected in Ethiopia and Senegal  and in body louse specimens collected in Burundi, Rwanda, Zimbabwe , and Ethiopia ; we add Democratic Republic of the Congo to the list. Body lice are acknowledged vectors for human-to-human transmission of _B. quintana_ . Detection of _Y. pestis_ in head and body lice has been reported . Detailed observations in south Morocco showed that body lice collected from blood culture–negative bubonic plague patients were negative for _Y. pestis_ , whereas body lice collected from septicemic patients were positive according to guinea pig inoculation results . Further experiments in a rabbit experimental model demonstrated the possibility of direct louse-bite transmission of _Y. pestis_ . A recent search for _Y. pestis_ in head lice in Ethiopia found none .\n\n【5】Our detection of _B. quintana_ and the plague agent _Y. pestis_ in modern head and body lice is similar to findings of a paleomicrobiological investigation at a medieval plague site near Paris . There, high-throughput real-time PCR investigation of dental pulp collected from 14 teeth from 5 skeletons detected _B. quintana_ DNA in teeth from 3 skeletons and _Y. pestis_ DNA in teeth from 2, including 1 with co-infection. Altogether, these data suggest that transmission of _B. quintana_ and _Y. pestis_ has been ongoing for centuries in populations in which louse infestation is prevalent. This finding indicates that lice might play a role in transmission of _Y. pestis_ and that preventing and controlling louse infestations might help limit the extension of plague epidemics in louse-infested populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "82f6469b-0c54-47ee-bb57-cdfb27c18118", "title": "One Flu for One Health", "text": "【0】One Flu for One Health\n**To the Editor:** The emergence and spread of influenza A pandemic (H1N1) 2009 virus from the animal reservoir to humans raise questions about the future approach to influenza virus infections. The scientific community has evidence demonstrating that influenza virus genes migrate across continents and animal species and assemble themselves in combinations that are a threat to animal and human health, resulting in panzootics like that caused by influenza A virus (H5N1) or pandemics like that caused by pandemic (H1N1) 2009 virus. The latter virus emerged from the animal reservoir, containing a unique combination of genes donated by viruses originating from from 3 species and 2 hemispheres. In a globalized environment, mapping gene movement across species and national borders and identifying mutations and gene constellations with pandemic potential or virulence determinants are essential to enact prevention and control strategies at a global level. This conclusion is in agreement with, and possibly the best example of, the One Health  vision: a multidisciplinary collaborative approach to improving the health of humans, animals, and the environment. One Health is endorsed by the United Nations Food and Agriculture Organization, the World Organisation for Animal Health, and the World Health Organization.\n\n【1】Vast improvements in capacity building have been achieved as a result of the influenza A (H5N1) global crisis. Thousands of viral isolates with zoonotic potential have been obtained through surveillance efforts, although the genetic information has not been exploited fully. In addition, investigating how influenza viruses circulate in certain species, including dogs, pigs, and horses, has been neglected. This neglect is evidenced by the fact that, at the time of this writing, GenBank contained 4,001 full genome sequences of influenza viruses isolated from humans, 2,590 of viruses isolated from birds, and only 325 from swine, 85 from horses, 2 from mink, 4 from dogs, 2 from cats, 2 from tigers, and 3 from seals.\n\n【2】We invite donors and international agencies to invest in a novel approach to influenza virus infections, to abandon prefixed compartments linked to geographic origin or species of isolation, and to analyze the influenza gene pool as one entity. We propose capitalizing on existing achievements and investments to develop an international network and a permanent observatory, which will improve our understanding of the dynamics of the influenza virus gene pool in animals and humans. A greater understanding will generate important information to support both public and animal health. Ideally, a small consortium, including representatives of major international organizations, could take leadership and liaise with major institutions involved in influenza surveillance and research to develop a feasibility study and roadmap to achieve this goal. The One Flu initiative could result in international synergies, the bridging of gaps between medical and veterinary scientists, permanent monitoring of virus evolution and epidemiology, and the best exploitation of investments in capacity building. Above all, this collaboration could be a challenge and opportunity to implement the One Health vision, and possibly act as a model for other emerging zoonotic diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d50decf8-aa12-4fc3-9bd6-fe2be5416362", "title": "The Role of Scientific Collections in Scientific Preparedness", "text": "【0】The Role of Scientific Collections in Scientific Preparedness\nDuring the past decade, public health emergencies have challenged the preparedness and response of government agencies, hospitals and clinics, public health professionals, and academic researchers in the United States and abroad. Several devastating infectious diseases have been transmitted to human populations from different animal species, including severe acute respiratory syndrome, pandemic influenza A(H1N1), and, most recently, Ebola virus disease in West Africa. According to the World Health Organization and the United States Centers for Disease Control and Prevention, as many as 60% of emerging infectious diseases originated in animals , and another 17% originated from insects or other types of vectors .\n\n【1】When such outbreaks occur, epidemiologists, public health workers, researchers, and clinicians begin research that is tied to the infectious disease outbreak cycle . When they isolate and identify the infectious agent, they perform genetic analyses, use diagnostic methods, develop and use potential medical countermeasures, and guide practices for best medical treatment. Even though the infectious disease outbreak lifecycle  might seem routine, each outbreak is different and presents unique research challenges to the mitigation of the spread of disease and to the protection of human lives.\n\n【2】With this understanding, the US Department of Health and Human Services (HHS) Office of the Assistant Secretary for Preparedness and Response (ASPR) has undertaken a science preparedness initiative. ASPR’s experiences have emphasized the importance of the ability to perform rapid scientific research during a limited timeframe when responding to emerging infectious disease outbreaks or other public health emergencies . This office has to be prepared to answer timely questions during a response, and research results can enable a more educated and informed response to similar future events, maximizing recovery. The science preparedness effort is designed to ensure that such research needs are prioritized and to support needed infrastructure for such research.\n\n【3】Knowing what tools and resources are available at any given point during a response and making them available to researchers is one of the goals of science preparedness. Scientific collections span a wide range of scientific study and disciplines and include various objects from lunar rocks to bacteria. Collections are widely distributed across federal agencies and throughout the country at state and local levels. These collections form a base of support for scientific study that informs regulatory, management, and policy decisions. In fact, the Office of Science and Technology Policy in the department of the Executive Office of the President created the Interagency Working Group on Scientific Collections to facilitate policy development and identify a systematic approach to safeguarding these valuable scientific resources, making them more readily available and accessible to the research community .\n\n【4】Building on the findings and recommendations of the Interagency Working Group on Scientific Collections, Scientific Collections International  aims to improve the rapid access to science collections across disciplines, not just within the federal government but also globally, between government ministries and private research institutions. Through a joint partnership, the Interagency Working Group on Scientific Collections, SciColl offered a novel opportunity for ASPR to explore the value of scientific research collections under the science preparedness initiative and integrate it as a research resource at each stage in the emergence of infectious diseases cycle.\n\n【5】Under the leadership of the SciColl executive secretariat at the Smithsonian Institution, and with multiple federal and international partners, attendees of a workshop held on October 23 and 24, 2014, fully explored the intersections of the infectious disease cycle and the role scientific collections could play as an evidentiary scientific resource to mitigate the risks associated with emerging infectious diseases. During this dynamic and collaborative forum, the case studies presented exemplified how specific collections of mammals, parasites, and other vectors and reservoirs of pathogens provide evidence and understanding of disease emergence in human populations. Participants articulated needs and possible methods to capitalize on the use of collections, including their practical research applications and also policy issues surrounding their use, during outbreaks. As highlighted in the published workshop report , specific recommendations for data management, innovative approaches to cross-disciplinary research, and communication and sample sharing were identified. The achievement of these critical milestones will maximize the use of scientific collections to their greatest benefit.\n\n【6】The findings from the workshop highlight the importance of management practices, access, and use of scientific collections in detecting, characterizing, mitigating, and predicting emerging infectious diseases. These findings were identified in the midst of one of the most severe Ebola outbreaks in history, and applying their utility will benefit future efforts to respond to public health emergencies and save lives. Employees of ASPR will continue to work toward the greater collaboration and integration of scientific collections in mitigating, preventing, responding to, and preparing for emerging infectious diseases. By emphasizing the importance of science-supported research and strengthening initiatives like science preparedness, we can develop policies and systems that fully realize the practical application of scientific collections.\n\n【7】We encourage private and public research institutions to get involved with SciColl and engage in the global effort to work toward a more systematic approach for sharing, managing, and using scientific collections. Similar to that of the work of the recently launched Network Integrated Biocollections Alliance, many initiatives all over the world are being created where scientists and researchers have articulated a need for greater collaboration in digitally capturing and sharing scientific specimens . For this growing movement to be successful, it needs to be one that incorporates a whole of community strategy, where every institution, agency, and scientific discipline is vested in the improvement and development of this vast network. The staff of ASPR look forward to our continued collaboration with SciColl and other partners in the future to strengthen engagement with the stewards of scientific collections at home and abroad.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7a8fdb59-e969-45de-834f-fdf8b5e3ff06", "title": "Ebola Risk Perception in Germany, 2014", "text": "【0】Ebola Risk Perception in Germany, 2014\nMisperceptions of risk can lead to inappropriate reactions during epidemics , such as stigmatization of those who are perceived as possible sources of infection . With regard to Ebola virus disease (EVD) in the West African countries most affected by the outbreak in 2014, indications are strong that societal misperceptions contributed to the outbreak spread . Public perceptions even in countries not directly affected by the EVD outbreak might influence outbreak response (e.g. by the priorities governments will set or by the willingness of persons to volunteer for aid missions in the affected countries) .\n\n【1】National authorities in countries outside of Africa responded differently to the potential risks of importing EVD into their countries. In November 2014, Australia and Canada imposed entry restrictions for persons from Guinea, Sierra Leone, and Liberia . At the same time, the United Kingdom introduced entry screening for international flight and train passengers . Because evidence for these public health actions is difficult to evaluate , public opinions might have played a role in political decision making. As of November 2014, Germany had not implemented any travel or entry restriction. As of October 14, 2014, a total of 3 patients who had acquired EVD in West Africa have been evacuated to hospitals in Germany for treatment. These evacuations to Germany were intensively covered by the media in Germany.\n\n【2】Several previous EVD outbreaks have occurred, but none was comparable in size and spread to the 2014 epidemic in West Africa and none directly or indirectly affected European countries, until now. To understand public reactions during an emerging epidemic in a country not directly affected by EVD, but one that is exposed to media coverage of the epidemic and involved in actions to contain the epidemic, we conducted an online survey about EVD for residents of Lower Saxony, Germany. Our goal was to improve our understanding of risk perceptions and potential changes in behavior during epidemics.\n\n【3】### Methods\n\n【4】##### Participants\n\n【5】We implemented this survey by using a longitudinal online panel, which was created in March 2014 to address human hygiene and preventive behavior regarding infectious diseases . The panel consists of 1,376 persons 15–69 years of age, who complete short, online questionnaires once a month. Panel members come from 4 districts in Lower Saxony, Germany (Braunschweig, Salzgitter, Vechta, and Wolfenbüttel). The districts were chosen by convenience: Braunschweig is the location of our research institute (the Helmholtz Centre for Infection Research), Vechta is its rural counterpart, and Salzgitter and Wolfenbüttel are 2 neighboring districts of Braunschweig. In each district, potential participants were invited to the panel by means of proportional stratified random sampling from the population registry. Of 26,895 invited, 9% were successfully recruited.\n\n【6】##### Questionnaire\n\n【7】We used an open-source online survey application (Limesurvey; _15_ ) to develop a knowledge-attitude-practice survey for online use . In November 2014, the questionnaire was about EVD . The EVD questionnaire consisted of 27 questions with 2–11 items each, totaling 123 items. The questions covered 7 topics: worries about EVD and perceived personal probability of infection, knowledge about transmission routes of Ebola virus, media use to obtain information about EVD, personal reactions to the EVD outbreak, attitudes toward specific measures to prevent the spread of EVD to Europe, willingness to volunteer to fight EVD in West Africa, and attitudes toward vaccination against EVD.\n\n【8】Risk perceptions were operationalized by asking participants if they worry about EVD (“yes” or “no”) and how they perceive their personal probability of acquiring EVD in the following 9 scenarios: at work, in public transport, in public places, at an airport in Germany, as a patient in a hospital in Germany, at a doctor's office in Germany, during travel to affected countries, by food imported from West African countries, or by other products originating in West Africa. Responses were chosen from a Likert scale with 5 options: “yes,” “rather yes,” “rather no,” “no,” and “does not apply.” “Worry about EVD” describes thinking about threatening scenarios in the absence of actual danger , and “perceived personal probability of infection” describes participants’ estimation of the actual risk for infection.\n\n【9】Knowledge about the transmission of Ebola virus was assessed with regard to the following 11 potential transmission routes: by direct contact with bodily fluids of infected persons, dead or living; through material heavily contaminated with such fluids; by direct contact with infected but asymptomatic persons; through air; through material that has been heavily contaminated with bodily fluids of infected persons, dead or living; through drinking water; through food produced in Germany; by casual contact with someone already sick, such as sitting next to someone (and without any direct contact of bodily fluids); by wild animals in Africa; by insects in Africa; or by wild animals/insects in Germany. Response choices were “true,” “false,” and “don’t know.” We computed a cumulative knowledge score (point for each answer in agreement with current scientific knowledge, range 0–11). In addition, participants were invited to rate their personal knowledge of EVD as “very good,” “good,” “moderate,” or “not good.” They were also asked whether they increased their use of media to inform themselves about EVD.\n\n【10】To assess behavioral implications, participants were asked if they had changed their behavior as a result of the EVD outbreak, how they would change behavior if an EVD patient were flown from Africa to Germany for treatment in a nearby hospital, and whether they would cancel an already booked flight to Africa. Participants were also asked if they thought that specific prevention measures should be introduced to prevent the spread of EVD to Europe . The survey also included questions about aid missions in affected countries and about the potential vaccine against EVD.\n\n【11】This study was approved by the Ethics Committee of the Hannover Medical School and the Federal Commissioner for Data Protection and Freedom of Information. All participants gave written informed consent before entering the study.\n\n【12】##### Statistical Analyses\n\n【13】To test differences among groups, we used the χ 2  test for categorical variables and the Wilcoxon test for continuous variables. To assess associations of sociodemographic factors with worrying about EVD, we used the Spearman correlation coefficient. In addition, we performed explorative multivariable logistic regression analyses to assess the effect of knowledge about transmission routes and sociodemographic factors on worries about EVD and on willingness to volunteer for aid missions. Analyses were performed by using Stata 12 (StataCorp LP, College Station, TX, USA).\n\n【14】### Results\n\n【15】##### Risk Perceptions\n\n【16】A total of 974 participants, 15–69 years of age, completed the questionnaire. Sociodemographic characteristics of participants who completed the questionnaire  did not differ from those of other panel members who did not (data not shown). In response to the question about whether they worried about EVD, 29% of participants answered in the affirmative; of those, 79.0% rated the strength of their worries as average (score < 3 on a scale of 1 \\[a little\\] to 5 \\[very strong\\]) (data not shown). In response to another question, 68% of the participants reported that they perceived acquiring EVD as possible in at least 1 of the 9 scenarios specified (data not shown). In response to a question asking whether in the next 6 months EVD could spread to the general population of Germany in a similar way as occurred in some West African countries, 8% of participants worried about EVD and 1.6% of those not worried about EVD answered in the affirmative .\n\n【17】##### Knowledge\n\n【18】Although 25% of participants rated their personal knowledge about EVD as good or very good, only 3.9% correctly answered all questions about transmission routes. The most common misperception (by 73.7% of participants) was that airborne transmission of Ebola virus is possible; moreover, 74.0% believed that human-to-human transmission by infected but asymptomatic persons is possible. Among those who specified airborne transmission as being possible, 18.5% reported that they perceived that acquiring EVD while using public transportation was possible compared with 9.4% of those who did not consider airborne transmission as being possible (p = 0.001). Education was positively associated with knowledge scores about Ebola virus transmission routes (Spearman correlation coefficient 0.18, p<0.001) and rating of personal knowledge about EVD (Spearman correlation coefficient 0.39, p<0.001). After controlling for the rating of personal knowledge about EVD, education was no longer associated with the score for knowledge about Ebola virus transmission routes (partial correlation coefficient −0.003, p = 0.91).\n\n【19】##### Media Use\n\n【20】Increased use of media to learn about EVD was reported by 43% of participants. These participants most commonly used the Internet (.5%), television (.1%), and print media (.7%). Increased use of television was more common among participants with a low level of vocational or secondary education than among participants with a higher level of education (data not shown). Increased media use was not associated with a higher knowledge score (median score for both groups = 7, p = 0.37). Personal knowledge about EVD was self-rated as good or very good by 28.7% of those who increased their media use and by 21.8% who did not (p = 0.01).\n\n【21】Multivariable logistic regression analyses that included age, sex, education, increased media use, and knowledge score showed that those who increased their media use were more likely to be worried about EVD than were those who did not increase their media use and that knowledge about Ebola virus transmission routes was negatively associated with being worried about EVD . Worrying about EVD was not affected by age, sex, or education .\n\n【22】##### Personal Reactions\n\n【23】Among all participants, 7% changed behavior in response to the EVD outbreak . Among those, 68.8% avoided contact with African persons in public places and 26.6% avoided using public transportation.\n\n【24】If an EVD patient were to be flown from Africa to Germany and treated in a nearby hospital, 86.9% of all participants stated that they would change their behavior. Of these, 16.4% would avoid using public transportation, 74.9% would increase their hygiene behavior (e.g. washing hands more often), and 30.2% would not visit friends admitted to the same hospital.\n\n【25】Participants were also asked about travel to Africa. As many as 95% of all participants would cancel an already booked flight to affected countries in West Africa, and 35.6% would cancel a flight to nonaffected countries in Africa.\n\n【26】##### Attitudes toward Specific Measures to Prevent the Spread of EVD to Europe\n\n【27】Asked about specific measures to prevent the spread of EVD to Europe, 97.0% of participants replied that all travelers from affected areas should receive information about EVD and advice on what to do if signs and symptoms of EVD developed . Entry restrictions for persons from affected countries were supported by 17.0% of participants.6% of participants; the difference between those worried about EVD (.6%) and those not worried (.9%) was significant (p<0.001).\n\n【28】##### Willingness to Volunteer to Fight EVD in West Africa\n\n【29】Of all participants, 38.7% would volunteer to fight EVD in West Africa if their experience and their knowledge were needed and if their personal situation and their health allowed them to do so. Multivariable logistic regression analyses including age, sex, education, increased media use, and knowledge score showed that older persons were less likely than younger persons to volunteer for aid missions and that women were less likely than men to volunteer . Willingness to volunteer was not associated with education level.\n\n【30】##### Vaccination against EVD\n\n【31】If a vaccine against EVD existed, 18.3% would opt for vaccination even if they did not plan to visit affected countries in West Africa and did not have contact with EVD patients. Of those who wanted to get vaccinated, 41.1% would still do so if the vaccine were associated with occasional mild side effects and 15.2% if it were associated with rare but severe side effects.\n\n【32】Of all participants, 85.9% stated that compulsory vaccination against EVD should be implemented in affected countries. A total of 36.4% would support compulsory vaccination against EVD for medical staff in Germany, and 51.5% would support compulsory vaccination against EVD for the general population of Germany if the number of EVD cases in Germany increased.\n\n【33】### Discussion\n\n【34】We report public perceptions of EVD in Germany, a country not directly affected by the current epidemic. Among the participants of our study, a substantial proportion were worried about EVD; however, among those worried, most did not report strong worries. Only one quarter of participants rated their knowledge of Ebola as good or very good. In addition, a large majority had poor knowledge about the transmission routes of the virus. A particularly common misperception was that Ebola virus can be transmitted by the airborne route or that it can be transmitted from human to human by infected but asymptomatic persons. These misperceptions were strongly associated with perceived personal probability of becoming infected while using public transportation. At the peak of the epidemic (November 2014), we identified inappropriate, unjustified, and stigmatizing attitudes in only a small proportion of participants. In contrast, treatment of a patient flown from Africa to a nearby hospital would induce worrying and inappropriate behavior in most participants. This response might be attributable to the fact that persons intuitively overestimate the risk for rare events . Our findings indicate a potential for inappropriate reactions to the epidemic should cases of EVD occur in Germany or should evacuations of EVD patients to Germany increase . For either of these 2 scenarios, trusted institutions (e.g. government) should spread information on the cause and the risk for infection .\n\n【35】As expected, participants who were worried about EVD were more likely to support measures preventing its spread to Europe. The difference between those worried and those not worried was particularly large for measures that can be considered inappropriate or even counterproductive to fighting the epidemic. For example, the stigmatization of returning health care workers and other volunteers can lead to fewer persons being willing to volunteer for aid missions . It is crucial that those worried about EVD remain a minority so that society will not be paralyzed by worries. Thus, misperceptions regarding transmission routes of Ebola virus should be resolved, and the media should contribute to a balanced, rational response rather than fuel worries. The observation that increased media use was not associated with better knowledge of transmission routes indicates the need for qualitative improvement of media reporting of such situations. However, the direction of the association between increased media use and worries cannot be determined from our data, so conclusions on worries and increased media use should be made cautiously. Not only the media but also public health experts might have contributed to mixed messages regarding airborne transmission of Ebola virus .\n\n【36】Almost 39% of participants indicated that they would volunteer to fight EVD in West Africa, but some of those participants would at the same time support prevention measures that are likely to negatively affect willingness to participate in aid missions. The high percentage of volunteers might result from the specific question that the participants were asked. The question included 2 preconditions that would qualify persons to volunteer: having the required experience and having a personal situation that would enable going to Africa. Most participants probably did not fulfill these preconditions, so their willingness to volunteer was only hypothetical. Therefore, they might not have realized that the restriction regarding return of volunteers would hamper their own return.\n\n【37】The changes in personal daily behavior reported or forecasted by the study participants (change of contact structure and mode of transportation, support of rapidly introduced vaccines) have consequences for understanding future emerging epidemics. Mathematical models constructed on the basis of contact structures and health perceptions obtained outside an epidemic setting will not be able to provide helpful insights if they do not take these factors into account. Problems in modeling the further course of the influenza A(H1N1)pdm09 outbreak might be attributable to these factors , and the experience with models made for the current EVD epidemic might be similar.\n\n【38】A large majority of participants supported compulsory vaccination against EVD for persons in affected countries. About half also stated that EVD vaccination should be compulsory for the general population should the number of cases in Germany increase. This finding is astonishing because no compulsory vaccination exists in Germany, and during the 2009 influenza A(H1N1)pdm09 pandemic, it was regarded as completely unacceptable . It is possible that the acceptability of drastic and compulsory measures is high only if the likelihood that such measures will be implemented is low, as is now the situation for EVD. On the contrary, the perception of associated risks might be scored much higher for EVD than for influenza, thereby increasing the acceptance of compulsory vaccination.\n\n【39】This study has some limitations. Regional data collected in an online survey might not represent perceptions of the general population in Germany. Furthermore, because the respondents in our survey were participating in a study on hygiene and behavior regarding infectious diseases, their level of motivation and knowledge about health-related topics might be higher than that of the general population. The education level of participants was also higher than that of the general population (.2% of the study participants had university training compared with only 17.2% of the general population of Germany; _24_ ). The panel members were also older, and the percentage of female panel members was higher than that of the general population.\n\n【40】For some characteristics in our analyses we did not have baseline data. For example, we did not have baseline information about which types of media are generally used by participants, so we cannot tell whether participants increased their media use or whether they used additional media sources that they did not use before. Because we do not have information about participants’ professions, we cannot assess whether risk groups for exposure to EVD (e.g. medical staff) are overrepresented in the study sample.\n\n【41】The reported risk perceptions and attitudes are conditional for the situation in Germany as of November 2014 and assume no transmission of Ebola virus in Germany. In the case of real exposure, persons might not act as they predicted they would. We also cannot assess how much the responses are influenced by the current status and how persons would react when media attention is less. However, having access to the study population of the larger infectious diseases study will enable us to ask the same persons again several months later and to examine temporal changes of risk perceptions.\n\n【42】In conclusion, a substantial proportion of the study population demonstrated poor knowledge about the transmission modes of Ebola virus and about the actual risks in a European country during the 2014 EVD epidemic in West Africa. Increased media use was not associated with better knowledge, underscoring the need to improve quality of content reported by the media. Although inappropriate or unjustified attitudes in the current situation were not demonstrated by most participants, the treatment of flown-in EVD patients in a nearby hospital would trigger inappropriate behavioral changes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "97ffa0f0-f04a-4a1a-8f8a-e52df5a0bf9b", "title": "Endemic Hantavirus in Field Voles, Northern England", "text": "【0】Endemic Hantavirus in Field Voles, Northern England\nRecently a new vole-associated hantavirus (Tatenale virus) was discovered in northern England , but only from an individual _Microtus agrestis_ field vole _._ Previously only hantaviruses from murine-associated lineages (Seoul virus \\[SEOV\\] and SEOV-like viruses) had been reported in the United Kingdom, despite the abundance of potential vole hosts in the mainland United Kingdom and the endemicity of vole-associated hantavirus lineages (Puumala virus \\[PUUV\\] and Tula virus) in mainland Europe . Here we present data suggesting that the Tatenale virus lineage is endemic in northern England.\n\n【1】European hantaviruses are of public health significance because they are a causative agent of hemorrhagic fever with renal syndrome (HFRS). In the United Kingdom, HFRS cases have primarily been attributed to SEOV-like viruses on the basis of serologic tests. SEOV antibodies have been detected in both humans and Norway rats (Rattus norvegicus_ ) in Northern Ireland and Yorkshire , and seropositivity in humans correlates with domestic or occupational exposure to rats . However, in the United Kingdom, HFRS cases with serologic cross-reactivity to PUUV , which might share antigenic determinants with Tatenale virus, have occurred.\n\n【2】To investigate the endemicity of hantavirus in field voles in the United Kingdom, we surveyed the extensive field vole population in the Kielder Forest, Northumberland (≈230 km distant from the locality where Tatenale virus was discovered). All sampled sites were grassy, clear-cut areas (adjacent to forest stands) where field voles were prevalent. Fieldwork was approved by the University of Liverpool Animal Welfare Committee and conducted subject to UK home office project license PPL 70\\_8210. Following the capture and processing of animals as previously described , we extracted viral RNA from 48 livers using a QIAamp Viral RNA Mini Kit (QIAGEN, Manchester, UK) and converted to cDNA using a High-Capacity RNA-to-cDNA Kit (Applied Biosystems, Warrington, UK). Hantaviruses were detected by PCR amplification of a fragment of the genomic L segment encoding RNA polymerase, following the strategy outlined by Klempa et al.\n\n【3】PCR-positive results were recorded for 16.7% of voles (/48) in total; positive voles were recorded at 3 of the 5 sampled sites and throughout the survey period, March–September 2015 . Three positive samples from different individual voles were sequenced (in both directions from independent replicate PCRs) by Sanger sequencing (Source BioScience, Rochdale, UK). The 380-bp sequence was identical in all 3 positive vole samples, with the exception of a single nucleotide polymorphism at position 145 (adenine, 2 voles; guanine, 1 vole; GenBank accession nos. KY751731, KY751732). The recovered sequences were similar to but divergent from Tatenale virus (.0%–86.3% identity at the nucleotide level and 95.9%–96.7% identity at the amino acid level, 2). Phylogenetic analysis of the L segment demonstrated this level of divergence was comparable to the divergence among many western European lineages of PUUV .\n\n【4】Taken together with the original report, these data are sufficient to suggest that Tatenale-like hantavirus lineages are widespread and common in northern England. Furthermore, the considerable sequence divergence between samples in Cheshire and Northumberland is consistent with a long-standing endemicity in northern England. Given that PUUV has never been recorded in the United Kingdom , the possibility should be considered that a Tatenale-like virus could have been responsible for some of the HFRS cases that have occurred here. More studies are needed to confirm whether other common rodents in the United Kingdom are hosts for this virus and to further characterize its phyletic relationships and zoonotic potential. Cross-reactivity of the sera from Tatenale-like virus–infected individuals to antigens of other relevant hantaviruses should be determined to inform future serologic surveys and the diagnosis of human HFRS cases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7c61bf30-b581-4fa0-8d65-81818112586c", "title": "Potential Threats to Human Health from Eurasian Avian-Like Swine Influenza A(H1N1) Virus and Its Reassortants", "text": "【0】Potential Threats to Human Health from Eurasian Avian-Like Swine Influenza A(H1N1) Virus and Its Reassortants\nSince emerging in 2001, Eurasian avian-like (EA) swine influenza A(H1N1) virus has gradually become the predominant lineage and continues to circulate among pigs in China . Introduction of the 2009 pandemic H1N1 virus (pH1N1) among pigs has increased its reassortment with EA H1N1 swine influenza A viruses (IAVs), and several reassortant variants with the potential to infect humans have been detected in China .\n\n【1】Multiple genotypes have been identified in EA H1N1 swine IAVs from pigs in China, and recent data suggest that the potentially pandemic genotype 4 (G4) reassortant has predominated among swine populations in China since 2016 . To clarify their prevalence and genotype characterizations, we isolated 32 swine IAVs in China during 2018–2020, including 6 novel reassortant H3N1 viruses that carry the hemagglutinin (HA) gene derived from human H3N2 lineage, and conducted phylogenic analysis of 8 gene segments from these viruses.\n\n【2】### The Study\n\n【3】During January 2018–December 2020, we collected 1,006 swab samples from pigs with symptoms typical of swine influenza, such as fever and cough, on pig farms across 6 provinces (Shanghai, Jiangsu, Zhejiang, Tianjin, Hebei, and Shandong) in China. We isolated viruses using MDCK cells and determined viral whole-genome sequences by Sanger sequencing. We isolated a total of 32 swine IAVs from the 1,006 swab samples, an isolation rate of 3.18% . To determine the phylogenetic evolution of the 32 isolates, we performed genetic analyses using available sequences of related viruses from the GenBank and GISAID  databases.\n\n【4】Phylogenetic analysis revealed that the HA genes of 26 viruses isolated in the study were grouped within clade 1C.2.3 of EA H1N1 lineage . However, the HA genes of the 6 novel reassortant H3N1 viruses were located in the recently circulating human-like H3N2 lineage and shared the highest genetic identity (.7%–99.9%) with a swine H3N2 virus (A/Swine/Guangdong/NS2701/2012) in China . The neuraminidase (NA) genes of all 32 isolates were grouped within the EA H1N1 lineage . On the basis of sequence analysis of the HA and NA genes, we identified the 32 swine IAVs isolated in this study as EA H1N1 (n = 26; isolation rate: 2.58%) and H3N1 (n = 6; isolation rate: 0.6%), indicating that EA H1N1 was the predominant virus subtype circulating among the sampled pig population in China.\n\n【5】Origins of the 6 internal gene segments, polymerase basic (PB) 1 and 2, polymerase acidic (PA), nucleoprotein (NP), matrix (M), and nonstructural (NS) genes, were remarkably diverse: EA H1N1, pH1N1, and TRIG (triple-reassortant internal gene) lineages . Among 6 EA H1N1 swine viruses, all internal gene segments were of EA H1N1 lineage. Among 26 reassortant EA H1N1 and H3N1 viruses, the PB2, PB1, PA, and NP genes all originated from the pH1N1 lineage; the M genes were mainly from both the pH1N1 and EA H1N1 lineages. Almost all NS genes originated from the TRIG lineage, but 2 originated from the pH1N1 lineage. On the basis of phylogenetic analyses of the 8 gene segments, including from HA and NA genes, we identified the viruses isolated in our study as G1 (n = 6), G2 (n = 2), G4 (n = 10), G5 (n = 8), and novel H3N1 (n = 6) viruses, according to the genotype classification existing at that time . We isolated viruses year-round to capture seasonal strains. Over the 36-month survey period, G1 viruses disappeared after 2018, G2 viruses were sporadically detected in 2018–2019, and G4 viruses, with an isolation rate of 1.0%, became a predominant genotype beginning in 2018. In our annual surveillance, we found another predominant virus genotype, G5, that had an isolation rate of 0.8%. These results suggest that the internal genes of the pH1N1 lineage had become predominant in contemporary swine IAVs among the pigs in the survey region.\n\n【6】Of note, in late 2020, we detected the H3N1 swine IAVs in 6 isolates (all from Zhejiang Province), indicating that this is a novel emerging recombinant genotype. Phylogenetic analyses demonstrated that the 6 novel H3N1 reassortant swine IAVs contained NA genes from the EA H1N1; PB2, PB1, PA, NP, and M genes from pH1N1; and NS genes from TRIG swine lineages. This combination is similar to the potentially pandemic G4 viruses except for the HA genes, suggesting that the emergence of novel H3N1 reassortant swine IAVs was a natural reassortant event that derived from G4 and H3N2 swine IAVs.\n\n【7】### Conclusions\n\n【8】Because of their susceptibility to avian, swine, and human IAVs, pigs are regarded as a mixing vessel for generating novel reassortant influenza viruses capable of replicating and spreading among humans . Implications for human health reinforce the importance of continuous surveillance of swine IAVs in the pig population. China has the most varied swine influenza virus ecosystem in the world and different subtypes simultaneously circulate among pigs . The emergence of potentially pandemic G4 EA H1N1 virus has increased the chances of reassortment with enzootic swine IAVs and the subsequential emergence of novel reassortant swine IAVs.\n\n【9】We isolated 6 EA H1N1 swine viruses and 26 reassortant EA H1N1 and H3N1 swine viruses in this study. Analysis results indicated that the reassortment of gene segments between EA H1N1 swine viruses and other enzootic swine viruses occurred frequently, and the reassortant swine viruses became established among the sampled pig population. Previous studies have reported several cases of human disease from EA H1N1 swine IAV or its reassortant viruses in Europe and China .\n\n【10】Our study, based on swine epidemiologic data from China, demonstrates that EA H1N1 swine influenza virus and its reassortant viruses circulate in swine populations and pose potential threats to human health. Furthermore, we isolated and documented the genetic evolution of novel reassortant H3N1 viruses between potentially pandemic G4 EA H1N1 and H3N2 swine IAVs. These findings highlight the need for surveillance for novel H3N1 viruses in swine and human populations to enable early interventions to avert outbreaks and protect animal and human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d792b7db-21bb-4df6-ae70-b303d1c9e5cc", "title": "Antiviral Drug–Resistant Influenza B Viruses Carrying H134N Substitution in Neuraminidase, Laos, February 2016", "text": "【0】Antiviral Drug–Resistant Influenza B Viruses Carrying H134N Substitution in Neuraminidase, Laos, February 2016\nInfluenza B viruses cause annual epidemics and contribute to ≈30% of influenza-associated deaths among children in the United States . Two lineages, B/Victoria/2/87 and B/Yamagata/16/88, have been co-circulating globally in recent years . Neuraminidase (NA) inhibitors (NAIs) are the only drugs available for treating influenza B virus infections, but NA mutations that emerge during treatment or due to natural variance can diminish the usefulness of NAIs.\n\n【1】### The Study\n\n【2】For this study, the National Center for Laboratory and Epidemiology in Vientiane, Laos, a member of the World Health Organization Global Influenza Surveillance and Response System, provided influenza A and B viruses to the World Health Organization Collaborating Center at the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, USA; the viruses had been collected during October 1, 2015–February 29, 2016. We propagated the viruses and then used the CDC standardized NA inhibition assay to assess their susceptibility to NAIs . Compared with the median 50% inhibitory concentration (IC 50  ) values for B-Victoria lineage viruses, IC 50  values for 2 of the 24 B-Victoria lineage viruses, B/Laos/0406/2016 and B/Laos/0525/2016, were elevated for zanamivir (to 158-fold), oseltamivir (fold), peramivir (to 74-fold), and laninamivir (to 42-fold) . These results were interpreted as highly reduced inhibition by zanamivir, normal inhibition by oseltamivir, and reduced inhibition by peramivir and laninamivir  .\n\n【3】This interpretation is useful but obscures the higher median oseltamivir IC 50  value (.67 nmol/L vs. 0.42–1.47 nmol/L for other NAIs) and the lower potency of oseltamivir in inhibiting NA activity of influenza B viruses . Moreover, reports from clinical studies indicate a lesser susceptibility of influenza B viruses to oseltamivir than to zanamivir . Although the laboratory criteria defining clinically relevant NAI resistance are not established, the inhibitory profiles of these 2 viruses suggest resistance to \\> 1 antiviral drugs. NA sequence analysis revealed that both viruses had an amino acid substitution, histidine (H)→asparagine (N), at the highly conserved residue 134 (NA-H134N) ; the presence of H134N in the respiratory specimens was confirmed by pyrosequencing  . NA-H134Y was previously reported in influenza B virus displaying reduced inhibition by peramivir . The inhibition profile of influenza B viruses bearing NA-H134N resembles that of influenza A(H1N1) viruses carrying NA-Q136R (residue 134 in influenza B NA corresponds to 136 in N1 numbering) . Residue 134  has been implicated in the conformational change of the 150-loop, which may adversely affect the interaction between the NA active site and NAIs, especially those containing the guanidyl group .\n\n【4】To expand testing, the Laos National Center for Laboratory and Epidemiology provided 40 additional specimens that were positive for B-Victoria lineage virus by real-time reverse transcription PCR , bringing the total number tested to 64. The specimens were collected during October 2015–April 2016 in Champasack (n = 41), Vientiane (n = 12), Luangprabang (n = 7), and Saravanh (n = 5) Provinces from 28 male and 37 female patients (median age 7 \\[range 0–67\\] years). Pyrosequencing revealed NA-H134N in 1 specimen; the respective isolate, B/Laos/0654/2016, displayed the expected NA inhibition profile . In total, we found the NA-H134N substitution in 3 (.6%) of the 65 tested B-Victoria viruses. Analysis of NA sequences deposited to the GISAID database  revealed that among 8,601 sequences of influenza B viruses collected worldwide during October 2014–September 2016, only 3 other sequences contained a substitution at H134 (harbored H134Y and 1 H134L); the 3 sequences were for B-Victoria lineage viruses.\n\n【5】Epidemiologic data revealed that the NA-H134N viruses were collected from a young woman, a young man, and a 3-year-old girl residing in 2 distant provinces . The 3 infections occurred 6–10 days apart in February 2016, and 1 of the patients received medical care for severe acute respiratory illness. No epidemiologic links were identified among the 3 patients infected with the drug-resistant viruses, and patients had no documented exposure to NAIs.\n\n【6】The 3 drug-resistant viruses were genetically similar to other B-Victoria lineage viruses circulating in Laos during 2015‒2016. Besides having the NA-H134N amino acid substitution, these viruses also shared the M1-H159Q amino acid substitution not identified in other virus sequences . Also, these viruses have 3 synonymous nucleotide mutations: PB1-c93t, PB1-g1930a, and HA-g1520a. In addition, B/Laos/0406/2016, B/Laos/0525/2016, and B/Laos/0654/2016 harbored substitutions NA-D390D/E, HA-V225A, and NS1-V220I, respectively . An analysis of influenza B NS1 sequences available in the GISAID database (as of September 12, 2016) indicated that NS1-V220I is rare, present in only 7 (.1%) of 10,405 sequences. Taken together, the geographic distance between the sites where the drug-resistant viruses were collected and the differences in their genomes point toward the possibility of influenza NA-H134N viruses circulating in Laos communities.\n\n【7】Results of the NA inhibition assay showed that NA-H134N impairs binding of NAIs to the active site of the enzyme. To determine whether this change also affects other properties (e.g. thermostability) of the enzyme, we incubated 3 H134N viruses at elevated temperatures for 15 min and then assessed their NA activity . The H134N substitution reduced the thermostability of the enzyme. This was evident from the undetectable activity levels starting at 47.5°C, which was 7.5°C lower than that for the control virus, B/Laos/0880/2016, with H134 (p<0.001) .\n\n【8】To assess the replicative fitness of NA-H134N viruses, we used primary human differentiated normal human bronchial epithelial (NHBE) cells, a cell culture system that morphologically and functionally recapitulates the human airway. The NA-H134N viruses displayed ≈1–2 log 10  lower titers at 24–72 h after inoculation . Although, the virus yield reduction (area under the curve) was evident for 2 of the NA-H134N viruses (AUC 2  – 72  \\[p<0.05\\]) , the difference was not statistically significant for B/Laos/0654/2016 . The growth kinetics data in differentiated NHBE cells indicate an attenuated phenotype for NA-H134N viruses in vitro. Unlike the other 2 drug-resistant viruses, B/Laos/0654/2016 had substitution NS1-V220I, which resides at the recently discovered second RNA binding site of the NS1 protein of influenza B viruses . This finding suggests a possible compensatory effect of NS1-V220I on the in vitro replicative capacity of B/Laos/0654/2016.\n\n【9】We assessed the replicative fitness of drug-resistant B/Laos/0654/2016 in three 4- to 6-month-old male ferrets (Mustela putorius furo_ ) (Triple F Farms, Sayre, PA, USA) that were serologically negative by HI assay for currently circulating influenza A(H1N1)pdm09, A(H3N2), and B viruses. At 48 h after inoculation with virus (% tissue culture infectious dose/mL), ferrets displayed fever (> 1.5°C above baseline) that lasted 21.8 ± 5.1 h on average. Virus shedding lasted 6 days; nasal wash virus titers, which were determined daily, were 4.2 ± 0.4; 6.0 ± 0.2; 4.8 ± 0.4, 4.7 ± 0.4, 4.7 ± 0.6, and 2.8 ± 0.4 log 10  50% tissue culture infectious doses/mL, respectively. These data suggest that the drug-resistant virus can replicate to high titers in the upper respiratory tract of ferrets and induce persistent fever.\n\n【10】### Conclusions\n\n【11】In February 2016, we detected 3 influenza B viruses in Laos bearing a rare NA-H134N substitution. Current antiviral medications may not effectively control infections caused by such viruses. Virus harboring NA-H134N and NS1-V220I replicated efficiently in NHBE cells and in the ferret upper respiratory tract. Studies to ascertain the effect of NA-H134N and NS1-V220I on influenza B virus virulence and transmissibility in a mammalian host are needed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c27702b5-b1a4-4081-9b0d-f657110290ab", "title": "Heterogeneity of Dengue Illness in Community-Based Prospective Study, Iquitos, Peru", "text": "【0】Heterogeneity of Dengue Illness in Community-Based Prospective Study, Iquitos, Peru\nDengue classically presents as an acute febrile illness lasting ≈5 days and accompanied by headache, musculoskeletal pain, and rash . A minority of infected persons show development of plasma leakage syndrome, intravascular volume loss, or major bleeding, which can lead to shock and death . There are 4 serotypes of dengue (DENV), and persons show development of long-lasting immunity to the specific serotype after infection. Cross-reactive immunity provides short-term protection against other serotypes. However, under some circumstances, previous infection with a different serotype increases the risk for severe disease . The World Health Organization (WHO) classification of dengue focuses on distinguishing between mild cases (classic dengue) and persons with or at risk for major adverse outcomes or death (severe disease) . Moreover, most literature describing the clinical manifestations of dengue evaluates the healthcare-seeking population whose symptoms are likely to be more severe. DENV infections associated with milder illness have not been subjected to similar systematic analysis or characterization.\n\n【1】Although the focus on severe disease is an obvious priority, there is value to characterizing the subjective illness experience in persons who have milder disease. As dengue vaccine development evolves, one of the challenges will be to accurately measure the effect of vaccination on the severity of illness. Measuring the rates of severe disease in vaccine trials is an insensitive approach and addresses only the small fraction of cases meeting these criteria, leaving open the possibility that vaccinated persons not meeting the criteria for severe disease had a meaningfully different disease experience than unvaccinated persons. Behavioral responses and reactions to illness depend on the illness experience of a person and will determine whether they attend work or school, self-medicate, seek medical attention, and move around their neighborhood, potentially infecting mosquitoes at other sites . Quantifying these relationships will help identify the human factors essential for virus transmission, guide the design of improved control strategies, assist policy makers in assessing the burden of dengue illness and healthcare needs, and guide allocation of resources.\n\n【2】As part of a larger epidemiologic study we developed the dengue Illness Perceptions Response (IPR) survey to gather data to characterize the dengue illness experience of a person, including the range and intensity of symptoms, and to measure the response of a person to their illness. We outline the development and application of the IPR, and to illustrate its potential value, we describe and quantify the heterogeneity of dengue illness and its associations with behavior changes.\n\n【3】### Methods\n\n【4】##### Ethics\n\n【5】The study protocol was approved by the Naval Medical Research Unit No. 6 (NAMRU-6) Institutional Review Board (IRB) (protocol #NAMRU6.2014.0028) in compliance with all applicable federal regulations governing the protection of human subjects. IRB relying agreements were established between NAMRU-6, the University of California Davis, Tulane University, Emory University, and the University of California, San Diego. The protocol was reviewed and approved by the Loreto Regional Health Department, which oversees health research in Iquitos.\n\n【6】##### Field Site\n\n【7】We conducted the study in an established research unit in the Amazonian city of Iquitos, Peru . Based in the department of Loreto, Iquitos has a population of ≈400,000 and mostly relies on tourism and extractive industries . More than half of the population of Loreto depend on government health insurance, which is available for persons living in poverty . Dengue is endemic to Iquitos, and 1 serotype typically dominates at any one time; all 4 DENV serotypes have circulated in Iquitos over the past 3 decades . The force of infection for DENV in Iquitos was calculated to vary from 0 to 0.33 infections/susceptible person/year during 1999–2010 . In March 2016, Zika virus was detected in Iquitos, and its transmission dominated for ≈1.5 years . Since 2017, the Asian-American strain of DENV-2 has been the dominant circulating serotype (A.C. Morrison, unpub. data).\n\n【8】##### Development of IPR Survey\n\n【9】On the basis of the experience of our team in collecting dengue symptom data and the available literature, we developed a focus group guide used to facilitate 6 mixed-sex focus groups to assess how persons who had recently had dengue illness (or an adult family member of a child who was infected) described the experience, including the range, duration, and precise location of symptoms; ways to describe the severity of the symptoms; and word choices related to the symptoms. Focus group participants (n = 52) were persons who had laboratory-confirmed dengue (positive result on DENV reverse transcription PCR \\[RT-PCR\\]) during the previous 3 months and who were recruited by ongoing community or clinic-based febrile illness surveillance. Using the range of symptoms and descriptions elicited through the focus groups, the research team developed a first version of the IPR, which was reviewed by collaborating experts and 3 local clinicians experienced in managing dengue to ensure its medical relevance. These data informed the IPR development, helping to define the symptoms to be included and descriptive terms used for symptoms and determine how to measure symptom intensity (there was almost unanimous support for scales using facial expressions to grade intensity).\n\n【10】The IPR survey that was implemented collected data on 36 symptoms; depending on the specific symptom, these data included presence, duration, intensity, character, frequency and location of symptoms . We learned about descriptive terms for various symptoms: musculoskeletal pain was most commonly described as “beaten up,” affecting the whole body. Headaches were most commonly described as a “generalized pressure.” Abdominal pain was most commonly described as “cramping” and most frequently located in the epigastrium. The survey also asked to what extent symptoms had affected daily activities: no change, minor change, or major change. The IPR was then piloted on 54 persons: 7 children <10 years of age, 10 persons 10–20 years of age, and 37 persons >20 years of age. Feedback from this pilot testing was used to guide final modifications of the survey; data from these persons are not included in the main analyses.\n\n【11】##### Study Design\n\n【12】The study followed a contact-cluster design. Persons positive for serum DENV RNA by RT-PCR (index case-participants) were identified through community- or clinic-based febrile illness surveillance . At the time of blood collection, we administered a retrospective movement survey to the index case-participants to identify locations visited in the previous 15 days. As soon as the initial PCR result was available, usually the next weekday, persons (contacts) from the home of the index case-participants and any residential locations visited by the index case-participants were then invited to provide a blood sample, regardless of the presence of symptoms; we tested consenting persons for serum DENV RNA by using RT-PCR. The protocol enabled requesting follow-up samples from PCR-negative contacts at intervals of no less than 2 days; we tested a median of 2 (interquartile range 2–3) blood samples from contacts by using PCR.\n\n【13】Index case-participants and any contacts with positive RT-PCR results  for DENV RNA were invited to respond to a series of surveys relating to symptoms (IPR), movements throughout the city, health related qualify of life, and illness-related expenditures. The IPR survey was applied daily (where possible), starting from the day of the positive RT-PCR result until there were no reported symptoms for 2 days, and then again 30 days later. Inclusion criteria for the study were an age \\> 5 years, DENV viremia documented by RT-PCR, and willingness to provide informed consent or assent for persons 5–17 years of age.\n\n【14】##### Data Analysis and Statement\n\n【15】We used CommCare , an open-source software platform, to develop a digital version of the IPR, which we administered by using handheld tablets . Survey data were uploaded to CommCare secure server where it could be reviewed by senior project members for discrepancies and corrected if necessary. We forwarded data to a PostgreSQL database  and directly accessed this database and analyzed the data by using R version 3.5.1 . We categorized symptoms into the following clinically defined groups: constitutional, fever, headache, musculoskeletal, abdominal, cutaneous, respiratory, bleeding, and other . The final dataset included the first 14 days of illness for each participant, indicating for these days symptom intensities from 0 (absence) to 10 (most intense). The survey solicited the maximum symptom intensity experienced between the day of collection and either the day the symptom started (in the first IPR) or the previous survey (in subsequent IPRs). Intensities recorded in the first IPR were assigned to the first date that the specific symptom was reported and any gaps in intensity data were imputed with linear interpolation. For days after the final survey, intensities and frequencies of symptoms were assumed to be 0. From this dataset we calculated the duration of illness and of specific symptoms, and the proportion of symptoms that were reported on each day of illness. Suspected dengue was defined following the 2009 WHO guidelines as fever and \\> 2 of the following symptoms: headache, retroorbital pain, nausea/vomiting, muscle/joint pains, and rash .\n\n【16】 To compare index and contact cases, we first compared the mean number of reported symptoms by using a 2-sided Student _t_ \\-test, and then compared the proportions of persons reporting specific symptoms by using the Fisher exact test. Because there were 36 comparisons, we applied a Bonferroni correction to the α value.\n\n【17】To explore the relationship between major activity change and individual symptom intensity, we performed logistic regression by using the glm function in the stats package in R, designating major activity change (present or absent) as the dependent variable and symptom intensity, age, and sex as independent variables. To evaluate the benefit of collecting intensity data versus only symptom presence and absence, we performed 2 logistic regression models for each symptom by using major activity change as the response variable and either symptom intensity or binary symptom presence as the explanatory variable. We used the difference in the model Akaike Information Criteria (Δ-AIC) as a means to compare the 2 models, with a positive AIC favoring the use of intensity over presence or absence. All data and R code used for this analysis are available .\n\n【18】### Results\n\n【19】We enrolled 79 persons who completed a total of 429 IPR surveys (median 5 surveys/person) . A total of 55 persons were enrolled through febrile illness surveillance (index case-participants), 42 through community-based surveillance, and 13 through clinic-based surveillance. The remaining 24 persons were enrolled through cluster investigations (contact case-participants); these case-participants were identified from the total of 408 contacts tested (% of the 567 eligible contacts). Index and contact case-participants were similar in age and sex. The first survey was completed a median of 3 days after the onset of symptoms (range −1 to 8 days). A total of 75% of participants completed the follow-up survey at a median of 36 days after the onset of symptoms (range 21–82 days). Seven (%) participants were hospitalized during the course of their acute infection .\n\n【20】##### Frequency and Duration of Symptoms\n\n【21】We summarized the overall frequency  and duration  of symptoms. The frequency of individual symptoms did not differ by sex or age group (younger vs. older than 18 years of age). All symptoms occurred more frequently in index case-participants than in contact case-participants, with the exception of vaginal bleeding. These differences were only significant for bad taste and chills (p<0.01 with Bonferroni correction) .\n\n【22】Participants reported a mean symptom duration of 7.37 days. One person (a contact case-participant) experienced no symptoms. A total of 5 persons experienced \\> 1 symptoms between the follow-up visit and the last form in the acute phase of illness, including nausea , malaise , headache , congestion , itching , and fainting .\n\n【23】##### Timing and Characterization of Symptoms\n\n【24】We report the timing of each of 13 symptoms for which duration data were collected . Malaise preceded other symptoms by 1 day in a substantial fraction of cases and was still reported by >30% of persons at day 7. Fever, headache, and pain (body/muscle/bone/joint) were most frequently reported on days 1–3, whereas abdominal pain was most frequently reported on days 3–5 .\n\n【25】We found substantial heterogeneity in reported maximum intensity per symptom by participants. We compiled the distribution of the maximum intensity reported by each person during the illness period for 12 key symptoms on a 10-point scale . Symptoms with the highest median values for maximum intensity (excluding those that did not report the symptom at all) were malaise and fever , body pain, headache, muscle pain, and weakness  .\n\n【26】We report the trajectories of symptom intensity over the course of the illness for 6 symptoms ; if the symptom was absent, an intensity of 0 was assigned. For the study population as a whole, the intensity of individual symptoms followed a similar timing as the presence or absence of each symptom. However, there was substantial variation in the trajectories of symptom intensity by participant .\n\n【27】We report correlations between the intensities of individual symptoms and the hierarchical clustering dendrogram of symptom intensities . Pairwise correlations ranged from 0.12 (sore throat vs. weakness) to 0.81 (body pain vs. muscle pain). Symptom intensity scores clustered into distinct groups (i.e. constitutional \\[malaise and weakness\\], fever/chills, headache/retroorbital pain, and musculoskeletal \\[body, muscle, bone, and joint pains\\]). Abdominal pain and sore throat did not cluster with other symptoms in this analysis.\n\n【28】##### Symptom Intensity and Activity Change\n\n【29】A total of 48 (%) participants reported a major change in their daily activities on \\> 1 days, 25 participants (%) reported a minor change in daily activities, and only 6 participants (%) reported no change in daily activities during their illness. On the basis of logistic regression models analyzing major activity change as a function of individual symptom intensities, corrected for age and sex, weakness had the strongest association with major activity change (odds ratio 1.48, 95% CI 1.36–1.63), followed by malaise (odds ratio 1.36, 95% CI 1.25–1.48) .\n\n【30】To assess the added value of measuring symptom intensity versus only symptom absence or presence, we compared logistic regression models that used major activity change as the dependent variable (compared with minor or no activity change as reference) and either the presence of a symptom or the symptom intensity as the independent variable. The difference in the model (Δ-AIC) was used to compare the 2 models, in which a positive Δ-AIC would favor the use of symptom intensity over presence/absence alone. Symptoms with the greatest positive Δ-AIC were malaise (Δ-AIC 42.1), weakness (Δ-AIC 35.8), and fever (Δ-AIC 20.8) . These data indicate that symptom intensity is more valuable than symptom presence or absence alone as a predictor of major activity change during DENV infection.\n\n【31】### Discussion\n\n【32】Symptoms reported most frequently by study participants were consistent with classical descriptions of dengue illness, other cohort studies, and WHO guidelines, as well as the key symptoms reported by participants in our focus groups , although a large fraction of participants reported less typical gastrointestinal or respiratory symptoms. Participants enrolled as contact case-participants reported fewer symptoms, similar to the findings in cluster investigations performed in Thailand . Some of these persons might otherwise not have sought medical attention. Only 1 (contact case-participant) person reported no symptoms. This finding conflicts with evidence suggesting that only 12%–25% of DENV infections are apparent . However, it is likely that administering the IPR survey encouraged reporting of symptoms that would not have been recalled in the context of a retrospective questionnaire, possibly explaining the relatively high symptomatic to asymptomatic ratio in our sample. We only collected data for persons who had a positive RT-PCR result (persons with only immunologic evidence of seroconversion to DENV were not included). This limitation has been associated with a higher frequency of symptoms .\n\n【33】Although most persons had mild dengue illness, our data demonstrate a range of intensity levels for individual symptoms. The IPR survey also enabled us to identify symptom groups within which daily intensities were highly correlated (r >0.70): constitutional (malaise and weakness), fever (fever and chills), headache (headache and retroorbital pain), and musculoskeletal pain (body, muscle, bone, and joint pain).\n\n【34】Abdominal pain was reported by 59% of participants, but only 13% reported severe abdominal pain (intensity >6). Abdominal pain followed a somewhat different time course from and showed substantially lower correlation with the symptom groups listed above. This finding is consistent with a distinct physiologic mechanism for abdominal pain. It also supports guidelines classifying severe abdominal pain as a warning sign, although few of our participants had evidence of plasma leakage or severe bleeding.\n\n【35】Although our study population consisted primarily of persons with mild dengue illness, participants still reported a substantial impact of illness on daily activities. Use of intensity scores for the major symptoms substantially improved the assessment of the effects of illness on daily activities when compared with use of symptom presence/absence alone. Therefore, the ability of the IPR survey to capture this aspect of heterogeneity in nonhospitalized persons with dengue could help improve assessment of either beneficial or detrimental effects of interventions, as proposed by Thomas et al. Moreover, the causes of specific symptoms in dengue remain poorly defined ; instruments such as the IPR survey could potentially be used to explore these underlying mechanisms.\n\n【36】Recently, a group of experts proposed a data collection tool to capture the overall experience of a person with dengue based on how their symptoms affect general wellness and functionality . The Dengue Illness Index (DII) records the presence or absence of symptoms daily. Our IPR survey has similarities to the DII, but a major difference is that the IPR solicited the assessment of the intensity of key symptoms of a participant. Our data suggest that persons are able to provide such an assessment and that intensity data add information relevant to the overall assessment of illness impact.\n\n【37】Thomas et al. proposed a strategy for tabulating the DII to yield a single illness score. We did not assign weights a priori for the different symptom intensities. Our data showing high correlations within symptom groups suggests that each symptom should not be given equal weight. We are exploring approaches to express the symptom severity data to a single or small number of the most informative parameters (e.g. principal component analysis). Regardless of the specific approach used to score dengue symptom severity, it will be essential to define the relationships of severity score to other external measures of illness impact. In addition to data on change in daily activities, described here, persons also provided data on movement  and on a health-related quality of life survey, which we are incorporating into future analyses.\n\n【38】Our findings should be interpreted in light of several additional limitations. The IPR survey was administered to participants by research staff using a tablet-based application. Some choices in the design of the tablet-based survey addressed operational needs or preferences of the research team. These considerations created some unanticipated challenges and required minor modifications to the tool during the course of our study. For example, as a result of delays in receiving RT-PCR results or missed follow-up assessments, it was difficult to accurately assign a start and end date of some symptoms for some persons. Imputation of missing data introduces error into our dataset that is difficult to quantify. Our sample size is relatively small and homogenous in host and viral populations. Our study was focused on evaluation of persons with acute DENV infection and did not include participants with nondengue febrile illnesses for comparison. Persons who participated in the focus groups or the main study are not representative of the overall population of Iquitos (e.g. greater time availability or willingness to engage with medical personnel). That said, the instrument development process, which engaged participants recently given a diagnosis of DENV infection and clinical experts who reviewed the literature, resulted in a tool that assessed a wide range of symptoms and potential behavioral responses, which we believe could be applied in other settings, although piloting the tool before use elsewhere is advisable. Febrile illness surveillance was limited to selected neighborhoods, and contacts were identified on the basis of social proximity. Given the long-standing interactions of the research team with the local population and the demographics of study participants, we do not expect these considerations to have introduced major bias in our results.\n\n【39】Our data support the feasibility and rationale of efforts to quantify dengue illness in future natural history and intervention studies. Our experience should be useful to guide development of reliable and validated tools for this purpose. We anticipate that the IPR survey could be adapted to other formats, including self-administration by research subjects, and to other languages, but these efforts would require modifications and further validation. Further studies are needed to test our results across other populations, to assign appropriate weights to individual symptom scores, and to correlate with other biologic and epidemiologic measures of disease impact.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5ff0af95-5adf-4d8a-9906-0c53d4a59c33", "title": "Avian Influenza and US TV News", "text": "【0】Avian Influenza and US TV News\nTo the Editor: Scholars have routinely noted ways in which scientific inquiry is isolated from public life and popular attention and have bemoaned relatively low levels of scientific literacy among lay audiences . While public understanding of science in the United States and elsewhere undoubtedly is not at the level desired by most scientists, apparent interest and hunger to learn are high for certain issues. These issues represent public communication opportunities.\n\n【1】Avian influenza is now such an issue. Although the risk for pandemic human influenza stemming from the avian influenza H5N1 virus is thought to be relatively low , media coverage of the disease, at least superficial and episodic coverage of disease incidence, has been dramatic. Aside from existing coverage, however, what type of coverage should the issue receive according to viewers? Are they interested in the issue, if at all, as a matter of scientific inquiry or simply as a sensational threat to individual survival?\n\n【2】We report here relevant results from a national survey of local television news viewers in the United States. Evidence from an Internet-based survey conducted in May 2006 suggests that viewers not only think that the potential direct impact of avian influenza on their own lives should be covered by reporters but also have interest in scientific investigation of the disease.\n\n【3】Working with Survey Sampling International , we recruited by email a nationally representative sample of regular television news viewers. Potential respondents were offered the chance to win a cash prize. Only those \\> 18 years of age and those who watched local television news show at least twice a week in recent months contributed to the final survey. We report here data from the 2,552 respondents who met those criteria and who answered all relevant questions.\n\n【4】Participants represented a reasonable cross-section of the general US population of television news viewers. Participants were 18–90 years of age (mean age 52, SD = 15.45). Educational attainment was mixed: 37% reported having completed at least a 4-year undergraduate degree, and 63% had completed <4-year degree. The final sample was 87% Caucasian, 8% African American, and 2% Asian; 8% also identified themselves as Hispanic or Latino. Approximately 54% of the sample was female, and 11% reported that they work for an organization directly involved in science.\n\n【5】When offered a 7-point scale that ranged from \"not at all important\" to \"very important\" to describe the priority that local television news should assign to addressing the \"direct impact\" of avian flu on one's own life and the lives of others, ≈80% chose \\> 5\\. Approximately 42% of respondents chose the highest level, indicating it was very important for local television news to cover this angle of the story. Regarding deeper perspectives on the story, ≈81% of respondents chose \\> 5 on the 7-point scale of importance when asked about potential coverage of how avian flu spreads and why scientists are finding it difficult to contain; 41% of respondents thought that it was \"very important\" that television reporters explicitly discuss that aspect of the issue. Moreover, 69% of respondents, by offering \\> 5 on the 7-point scale, thought the television news should focus on the connection of avian flu to other issues, such as business and travel. Clearly, we are living in a time in which news audiences would tolerate much more than the soundbites and superficial coverage often offered with regard to infectious disease research.\n\n【6】Equally as striking are the demographic characteristics of those who believe that local television news should cover the process of scientific discovery in this arena. We conducted a simple regression analysis to predict 1 of the items noted above, i.e. perceived importance of television news discussion of how avian flu spreads and of the efforts of scientists. We used formal employment with a scientific institution, level of educational attainment (a 5-level variable treated here as interval), and reported conversation with others about science in recent months as predictors. Educational attainment actually bore a negative relationship to interest in such coverage, β = -0.14, p<0.01, and formal affiliation with a scientific institution bore no statistically significant relationship, p>0.10. (Past conversation about science bore a positive relationship, β = 0.06, p<0.01.)\n\n【7】Results suggested a prime opportunity for public communication efforts not just because of issue timeliness but also because of apparent widespread hunger for information among the US television news viewers. Health and science communication professionals could address this interest and desire to boost popular awareness of epidemiologic and medical inquiry.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4847c61a-d180-4050-95f2-7c6fb5bb1e42", "title": "Two Related Occupational Cases of Legionella longbeachae Infection, Quebec, Canada", "text": "【0】Two Related Occupational Cases of Legionella longbeachae Infection, Quebec, Canada\nSeveral _Legionella_ species can cause legionellosis, which results in influenza-like illness (Pontiac fever) or pneumonia (Legionnaires’ disease) . _L. pneumophila_ , which is mainly transmitted from aerosolized water, has been the principal _Legionella_ species reported from Canada . Unlike _L. pneumophila_ , _L. longbeachae_ is highly adapted to the soil environment and primarily transmitted from potting soils and compost .\n\n【1】During summer 2015, a regional public health authority in Quebec, Canada, received reports of 2 cases of pneumonia attributable to _L. longbeachae_ infection. These cases occurred 1 month apart in persons who shared the same workplace. We conducted epidemiologic and environmental investigations to identify the source of infection and propose appropriate control measures.\n\n【2】### The Study\n\n【3】On July 3, 2015, the provincial public health laboratory (Laboratoire de santé publique du Québec \\[LSPQ\\], Sainte-Anne-de-Bellevue, Quebec, Canada), informed the regional public health authority (Centre intégré de santé et de services sociaux de la Montérégie-Centre, Longueuil, Quebec) about a case of _L. longbeachae_ serogroup 1 infection. The investigation team included members with expertise in infectious diseases and in occupational and environmental health. Public health experts from the Institut national de santé publique du Québec (Quebec City, Quebec), the Institut de recherche Robert-Sauvé en Santé et en sécurité du travail (Montreal, Quebec), and the LSPQ joined the investigation team of the regional public health authority. The investigators questioned the patient by using a standardized epidemiologic questionnaire and explored potential relationships between the patient’s illness (i.e. clinical manifestations, laboratory results, and diagnosis) and personal factors (i.e. demographic, behavioral, and medical risk factors) and possible exposure sources. During the investigation, another worker from the same workplace was hospitalized with severe pneumonia, and the public health team recommended testing for _L. longbeachae_ . On July 20, 2015, the LSPQ confirmed _L. longbeachae_ serogroup 1 infection for the second patient, and the investigation team questioned this patient by using the same standardized epidemiologic questionnaire answered by the first patient. No other causal organism was identified for either patient.\n\n【4】A lag of 1 month separated onset of symptoms in the 2 patients. Both had severe pneumonia that required admission to intensive care. They recovered and returned to work a few months later. Both had personal risk factors for Legionnaires’ diseases. However, neither had a history of travel, gardening, visits to gardening centers, or exposure to hanging plant pots or compost. Both worked at the same metal recycling plant for many years and shared no nonprofessional activities. One was a shredder operator at a fixed work station; the other was responsible for machinery maintenance throughout the plant (m 2  in size); the only shared spaces were the locker and lunch rooms. Their work shifts overlapped for a few hours. The company, which employed ≈25 workers, has been in operation for >40 years and had no prior case of legionellosis.\n\n【5】On July 8, 2015, the regional public health authority investigated the workplace and assessed the industrial processes. Trucks containing cars and other bulk metal materials unload at the site. An industrial grapple clamps the materials and feeds them to a shredder. Any overload is stacked until it can be processed. Diverse metals are then sorted out and sold. The business operates during April–December.\n\n【6】The investigation identified different sources of soil exposure. First, most of the site lies on bare ground. A tanker truck regularly sprinkles water to control dust. Second, a conveyor belt with foam residue and other debris generates aerosols that contain soil particles. However, employees are not allowed near the conveyor belt when it is operating. Third, some cars are reportedly filled with soil by suppliers to increase weight and raise selling value.\n\n【7】On July 31, 2015, multiple soil samples were taken from a workplace area where the soil could have been at higher risk for _L. longbeachae_ contamination (i.e. because of greater-than-usual humidity, less exposure to wind, and less ultraviolet exposure from the sun). A control sample was taken from an area of undisturbed soil in this workplace. Proper sterilization of equipment was ensured between collections of samples. Sixteen randomly located sites (each 4 m 2  ) were sampled, including the control site.\n\n【8】The Institut de recherche Robert-Sauvé en santé et en sécurité du travail obtained an isolate from 1 soil sample and used PCR for identification. The LSPQ obtained isolates from bronchoalveolar lavages and, in collaboration with Canada’s National Microbiology Laboratory, confirmed their identity by using the 16S sequencing method. Pulsed-field gel electrophoresis was used to investigate concordance of the outbreak strains and to compare the isolates’ patterns with those obtained from previous _L. longbeachae_ isolates from Quebec. An adaptation of the full pulsed-field gel electrophoresis _Sfi_ I protocol developed for _L. pneumophila_  was also conducted by using _Asc_ I for _L. longbeachae_ isolates.\n\n【9】All soil samples were positive for _Legionella_ spp. an expected outcome because these bacteria are ubiquitous in the environment. PCR and cultures conducted on the soil sample taken near the truck-unloading station were positive for _L. longbeachae._ By using the 2 enzymes (Asc_ I and _Sfi_ I) protocol, laboratory findings showed that the strains from the 2 patients and from the positive soil sample were concordant , except for 1 difference, and were closely related, according to Tenover’s criteria .\n\n【10】### Conclusions\n\n【11】_L. longbeachae_ infections are rarely reported in Quebec. During 2003–2014, the LSPQ identified only 7 sporadic cases and no geographic clustering. In 2015, 2 severe _L. longbeachae_ pneumonia cases occurred 1 month apart. The determination that the only common temporospatial exposure for the 2 patients was the workplace constitutes a strong epidemiologic link. Furthermore, _L. longbeachae_ of the same genotype was isolated in the workplace soil samples. Although the diversity and distribution of _L. longbeachae_ strains in Quebec soils are unknown, finding the same _L. longbeachae_ genotype in the workplace soil suggests a causal link between the 2 case-patients and their workplace.\n\n【12】Unlike clusters of _L. longbeachae_ described in the literature , these 2 patients did not come into contact with potting soils or compost during the exposure period. However, several sources of soil were found in their work environment. Although _L. longbeachae_ usually is found in highly organic soil , the positive soil sample in this investigation came from poor soil. Until now, no Legionnaires’ disease case has been linked to _L. longbeachae_ in this type of soil. Possibly, _L. longbeachae_ traveled from the environment surrounding the plant or from soil trapped in trunks of wrecked cars. Also, soil analysis results might not reflect the conditions that prevailed during the exposure period.\n\n【13】This outbreak resolved spontaneously. The regional public health authority recommended preventive measures, such as handwashing, reinforced personal hygiene, avoidance of soil dumping from car trunks, and dust control.\n\n【14】The small number of cases in this outbreak and a general paucity of knowledge about _L. longbeachae_ limited this investigation. The precise mechanism leading to infection has not yet been identified. Whereas hand-to-mouth contamination followed by microaspiration  seems the most probable route of exposure, dust inhalation cannot be ruled out. Early mobilization of experts and good collaboration with the implicated company facilitated the outbreak investigation.\n\n【15】These cases highlight the need to search for _L. longbeachae_ in cases of severe pneumonia by performing appropriate cultures and to consider the risk for occupational exposure when soil is present. Environmental investigation appears useful to understand _L. longbeachae_ transmission and ecology in Canada’s soils. Additional research is needed to improve understanding of sources of exposure, the pathogenesis of this species, and appropriate control measures.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "52ad4e1c-71fa-417e-941f-135d4d15bef2", "title": "New Variant of Porcine Epidemic Diarrhea Virus, United States, 2014", "text": "【0】New Variant of Porcine Epidemic Diarrhea Virus, United States, 2014\n**To the Editor:** Porcine epidemic diarrhea (PED) was first reported in the United Kingdom in 1971 . The disease was characterized by severe enteritis, vomiting, watery diarrhea, dehydration, and a high mortality rate among swine. Subsequently, the causative agent of PED was identified as porcine epidemic diarrhea virus (PEDV), which belongs to the family _Coronaviridae_  and contains an enveloped, single-stranded positive-sense RNA genome. PEDV has been reported in many other countries, including Germany, France, Switzerland, Hungary, Italy, China, South Korea, Thailand, and Vietnam  and was first identified in the United States in May 2013. By the end of January of 2014, the outbreak had occurred in 23 US states, where 2,692 confirmed cases  caused severe economic losses. Recent studies have shown that all PEDV strains in the United States are clustered together in 1 clade within the subgenogroup 2a and are closely related to a strain from China, AH2012 .\n\n【1】In the state of Ohio, the first PED case was identified in June of 2013; since then, hundreds of cases have been confirmed by the Animal Disease Diagnostic Laboratory of the Ohio Department of Agriculture. In January of 2014, samples from pigs with unique disease, suspected to be PED, were submitted to this laboratory. Sows were known to be infected, but piglets showed minimal to no clinical signs and no piglets had died.\n\n【2】According to real-time reverse transcription PCR, all samples from the piglets were positive for PEDV. Subsequently, the full-length genome sequence of PEDV (OH851) was determined by using 19 pairs of oligonucleotide primers designed from alignments of the available genomes from PEDVs in the United States . On the basis of BLAST  searches, strain OH851 showed 99% and 97% nt identity to PEDVs currently circulating in the United States (Colorado, Iowa, Indiana, Minnesota) for the whole genome and the full-length spike (S) gene, respectively. By distinct contrast, strain OH851 showed only 89% or even lower nucleotide identity to PEDVs currently circulating in the United States in the first 1,170 nt of the S1 region. In that region, nucleotide similarity to that of a PEDV strain from China (CH/HBQX/10, JS120103) was 99%, suggesting that strain OH851 is a new PEDV variant. Phylogenetic analysis of the complete genome indicated that the novel OH851 PEDV is clustered with other strains of PEDV currently circulating in United States, including another stain from Ohio, OH1414 . However, phylogenetic analysis of the full-length S gene showed that strain OH851 is clustered with other strains of PEDV from China and most closely related to a PEDV strain from China, CH/HBQX/10 , but distantly related to other PEDV strains currently circulating in the United States and strain AH2012 . This finding strongly suggests that strain OH851 is a variant PEDV. In comparison with the S gene of other strains from the United States, the S gene of strain OH851 has 3 deletions (a 1-nt deletion at position 167, a 11-nt deletion at position 176, and a 3-nt deletion at position 416), a 6-nt insertion between positions 474 and 475, and several mutations mainly located in the first 1,170 nt of the S1 region.\n\n【3】It is highly possible that the sequence deletions, insertion, and mutations found in variant strain OH851 might have contributed to the reduced severity of the clinical disease in the piglets. More animal studies are needed to test this hypothesis. The unique deletion and insertion feature also represents a target for diagnostic assays to differentiate between currently circulating PEDV strains and new variants.\n\n【4】The low nucleotide identity in the 5′-end S1 region (first 1,170 nt) region and high nucleotide identity in the non-5′-end S1 region of the variant strain, compared with that of the PEDVs currently circulating in the United States, suggest that this new PEDV variant might have evolved from a recombinant event involving a strain from China. Because the new variant does not cause severe clinical disease, including death, the novel virus is a potential vaccine candidate that could protect the US swine industry from the infection caused by the virulent strain of PEDV currently circulating in United States.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f846e63d-b991-49a8-ac08-254b6f5c8b0b", "title": "Excess Mortality Associated with Antimicrobial Drug-Resistant Salmonella Typhimurium", "text": "【0】Excess Mortality Associated with Antimicrobial Drug-Resistant Salmonella Typhimurium\nFoodborne _Salmonella_ infections have become a major problem in most industrialized countries. Of particular concern is the increasing number of infections with antimicrobial drug-resistant _Salmonella_ , including the recent emergence of drug-resistant _Salmonella enterica_ serotype Typhimurium (S._ Typhimurium) definitive phage type 104 (DT104). This strain is usually resistant to at least five drugs: ampicillin, chloramphenicol, streptomycin, sulfonamides, and tetracycline (R-type ACSSuT) and has become a predominant _Salmonella_ type in many countries, including the United States, United Kingdom, Germany, and France . In spite of its rapid international dissemination  and the fact that antimicrobial drug-resistant _Salmonella_ was associated with human infections before the recent spread of DT104, the available data are inconclusive regarding a possible increased virulence of DT104. Whether antimicrobial drug resistance in DT104 contributes to enhanced illness or death is unclear . Few studies have addressed the health impact of drug resistance in types of zoonotic _Salmonella_ other than DT104 , and these studies suggest that drug resistance may be associated with increased illness and death rates.\n\n【1】Excess mortality associated with drug resistance in zoonotic _Salmonella_ is difficult to quantify. Death is a relatively rare event and may not occur until months after the initial diagnosis. Furthermore, a number of factors, including chronic and malignant diseases, may contribute to death from salmonellosis. The objective of this study was to determine death associated with antimicrobial drug resistance in _S._ Typhimurium. The study was based on a large, unbiased sample of Danish patients registered in a national database. We linked these data with those in the Danish civil registry, which has complete information about survival status. Furthermore, by completing the data with information from hospital discharge registries, we were able to adjust for comorbidity.\n\n【2】### Materials and Methods\n\n【3】##### Surveillance\n\n【4】In Denmark the diagnosis of human _Salmonella_ infections is made at Statens Serum Institut (SSI) or at 10 clinical microbiology laboratories. The SSI receives notifications of positive findings as well as isolates from the microbiology laboratories. If a specific _Salmonella_ serotype is found more than once from the same person during a period of up to 6 months, only the first positive sample is registered. As a part of this laboratory-based surveillance system, monitoring for antimicrobial resistance in _S._ Typhimurium was initiated in 1995. In 1995 and 1996, a sample of strains was tested, but from 1997 on, all _S._ Typhimurium strains received at SSI were tested for antimicrobial susceptibility. This study included all isolates of _S._ Typhimurium examined from January 1, 1995, through October 31, 1999.\n\n【5】Isolates were tested by tablet diffusion on Danish Blood Agar (SSI Diagnostica, Hillerød, Denmark) with the use of Rosco Neosensitabs (Rosco, Roskilde, Denmark). The panel included 13 drugs from the Danish Integrated Antimicrobial Resistance Monitoring and Research Programme  . Because reduced susceptibility to ciprofloxacin is difficult to detect by the tablet diffusion test, the E-test (Biodisk, Solna, Sweden) was used as well whenever the tablet diffusion test identified nalidixic acid resistance. In this paper, quinolone resistance refers to strains resistant to the first-generation quinolone nalidixic acid  .\n\n【6】##### Registry Linkage Study\n\n【7】All live-born children and citizens of Denmark are assigned a personal identification number, uniquely identifying every person the Danish Civil Registration System  . Demographic data, including vital status, marriage status, emigration/immigration, and address of residence, are kept in this Civil Registration System.\n\n【8】The matched cohort study used the data from the Civil Registration System to compare the death rates of patients with culture-confirmed _S._ Typhimurium infections to the death rates of persons in the general Danish population. For each patient, we randomly selected 10 people matched by age, sex, and county of residence. People who were born during the same month and year as the patient and were alive on the date of sample receipt were eligible for the reference group. From the Danish Civil Registration System, we obtained information on vital status, date of change of vital status, (i.e. date of death or emigration) and area of residence (county level) for the patients and the persons included in the reference group.\n\n【9】Data on admissions to hospital and discharge diagnosis were obtained by using the data from the Danish National Patient Registry  and the Cancer Registry for all persons included in this study, thereby allowing us to control for preexisting illness (comorbidity). Danish National Patient Registry contains data on all patients discharged from non-psychiatric departments since January 1, 1977. Diagnoses and procedures are coded according to the International Classification of Diseases 8 or International Classification of Diseases 10 (from 1993). Diagnoses obtained during 10 years before infection were used to calculate the comorbidity index.\n\n【10】##### Statistical Methods\n\n【11】The comorbidity index used the principles described by Charlson et al. This index is a sum of severity scores (weights) corresponding to the number and severity of comorbidity conditions. In the first step, we analyzed the data from the background population to calculate the relative rate associated with each of the diagnostic groups summarized in Table 1 . These relative rates served as the weights in the further survival analyses. The index was calculated by adding log-transformed weights, thus taking into account multiple hospital discharges. Diagnostic groups associated with a relative mortality rate <1.2 were not included in the models. By comparing this index with the survival analyses, any difference between the death rates of _Salmonella_ patients and the general population quantifies excess mortality beyond what is attributable to underlying illness.\n\n【12】To compare mortality rates of _S._ Typhimurium patients with those of the general population, the data were stratified so that each stratum contained 1 patient and 10 persons from the reference group. To control for age, sex, and county of residence, we used conditional proportional hazard regression. Death up to 2 years after infection was determined, after adjusting the data for comorbidity as described. To assess death rates associated with antimicrobial drug resistance, interaction by drug resistance on _Salmonella_ deaths was determined. We used the Wald test to test for homogeneity of the rate ratios. The analyses were conducted by the use of the PHREG procedure of the SAS system (Version 6.12, SAS Inst. Inc. Cary, NC). Death rate ratios (RR) are expressed as the relative death rates of patients compared with the matched sample of the general Danish population, and the term “referents” refers to this unexposed matched sample.\n\n【13】### Results\n\n【14】Of 4,075 cases of _S._ Typhimurium infections reported in Denmark from January 1995 to October 1999, the antimicrobial-drug susceptibility was determined in isolates from 2,059 cases, and a successful link to the Civil Registry System was obtained for 2,047 (.4%). In the period up to 2 years after entry in the study, 59 deaths were identified in _S._ Typhimurium patients and 221 deaths among 20,456 referents. The median age of the 59 persons were 74.1 years (range 18.1 to 90.1). In the first 30 days after entry in the study, the cumulative mortality proportion (Kaplan-Meier estimate) was 0.73% for _S._ Typhimurium patients and 0.04% for the referents (RR 15.4, 95% confidence interval \\[CI\\] 6.1 to 39.2). In the period 30 to 720 days after entry, cumulative mortality was 2.75% in _S._ Typhimurium patients and 1.51% in referents (RR 1.8, 95% CI 1.3 to 2.6). On this basis, we used the period 0 to 720 days in the remaining analyses.\n\n【15】Overall, patients with _S._ Typhimurium were 3.0 times (% CI 2.2 to 4.0) more likely to die than referents in the 2 years following infection. After the data were adjusted for comorbidity, the relative rate was 2.3 (% CI 1.7 to 3.2). This relative death rate was independent of age (p=0.84).\n\n【16】A total of 631 (.8%) patients were hospitalized in connection with the _S._ Typhimurium infection. In the reference group, 577 (.8%) were hospitalized within 60 days of entry. Five of those had gastroenteritis as their primary diagnosis.\n\n【17】Two hundred seventeen (.6%) of _S._ Typhimurium patients and 954 (.7%) persons from the referent group had at least one of the diagnoses listed in Table 1 , which summarizes the various diagnostic groups and their weights in relation to the comorbidity index. A total of five HIV infections were found, three among patients and two in the reference group. All five were still living at the end of the study.\n\n【18】In the 2,047 strains, 953 (.6%) were pansusceptible, 1,094 (.4%) resistant to at least one drug in the panel, and 639 (.8%) were resistant to at least two drugs. Resistance to sulfonamides was found in 47.3% of the patient isolates, tetracycline in 25.1%, streptomycin in 22.4%, ampicillin in 19.2%, chloramphenicol in 17.0%, kanamycin in 9.6%, quinolone in 4.1%, trimethoprim in 3.0%, gentamicin in 2.2%, and ceftriaxon in 1.4%. No ciprofloxacin-resistant strains were found. The MIC of ciprofloxacin in the quinolone-resistant isolates ranged from 0.06 to 0.38 mg/L (median 0.09 mg/L).\n\n【19】R-type ACSSuT was found in 283 (.8%) isolates, and patients infected with this type were 6.9 times more likely to die than the general population, compared with a RR of 2.6 in patients with strains of other R-types (p =0.02). Also, chloramphenicol (.4 vs. 2.4, p=0.003), quinolones (.9 versus 2.8, p=0.05), and ampicillin (.1 versus 2.7, p=0.09) were associated with higher death rates in resistant than sensitive strains.\n\n【20】A total of 270 of the isolates with R-type ACSSuT were phage-typed, and 217 (.4%) were DT104, 18 (.7%) DT12, 11 (.1%) DT120, and the rest were other or unknown phage types. Strains with other R-types were distributed over a number of different phage types. A total of 1,667 were examined, and the three most common were DT12 (.8%), DT66 (.0%), and U288 (.9%). Thirty-nine (.3%) were DT104. In the patients with R-type ACSSuT, no difference in the death rate between persons infected with DT104 (relative death rate 4.4, 95% CI 1.7 to 11.6) and other phage types (relative death rate 6.4, 95% CI 1.3 to 32.4) was found; both estimates were adjusted for comorbidity.\n\n【21】No difference in age and sex distribution between patients infected with R-type ACSSuT and other antibiograms were found. The median age in both groups was 33 years (range 1 to 87 and 0 to 95, respectively, p=0.89).\n\n【22】Finally, we analyzed a model with three levels of resistance: non-ACSSuT, R-type ACSSuT (Nx-sensitive), and R-type ACSSuTNx. The figure shows the survival curve of the referents and patients according to these three groups. In the group of 40 cases with R-type ACSSuTNx, we identified five deaths within the 2-year period after infection, one of those within the first month of infection, three within 6 months, and one within 18 months. The relative risk associated with an infection with R-type ACSSuTNx was 12.4 without adjusting the data for comorbidity. After adjustment, the RR associated with this resistance pattern was 13.1. The median age in this group was 43 years (range 1 to 89), 10 years higher than the R-type ACSSuT quinolone-sensitive group.\n\n【23】### Discussion\n\n【24】Since the 1990s, the frequency of antimicrobial drug resistance in zoonotic _Salmonella_ and the number of drugs to which the strains are resistant has increased, primarily as a consequence of antimicrobial use in food production . The recent development of fluoroquinolone resistance is of particular concern . At present, a fluoroquinolone is the drug of first choice for extraintestinal and serious intestinal _Salmonella_ infections in adults, and resistance to this drug may potentially reduce the efficacy of early empirical treatment. The health impact of antimicrobial drug resistance in zoonotic _Salmonella_ needs to be determined . We used data from registries created for other purposes to avoid bias and were able to explore long-term death rates and adjust the data for comorbidity.\n\n【25】The comorbidity index was based on discharge diagnoses from patients admitted to hospitals in Denmark and to a lesser degree on data from outpatient clinics but did not include data from general practitioners. Any patient with a coexisting disease severe enough to alter the outcome of a _Salmonella_ infection is likely to have had contact with a hospital or an outpatient clinic within the 10-year period before infection. The backbone for the construction of the comorbidity index was the National Discharge Registry. A validation of this registry showed that there was agreement between the registry and hospital records of 75% to 90%, using 3-digit level International Classification of Diseases diagnoses  .\n\n【26】In general, patients with _S._ Typhimurium infections were 2.3 times more likely to die than the matched sample of the Danish population during a 2-year follow-up. This figure is likely to reflect both long-term consequences of _S._ Typhimurium as well as underlying diseases and conditions not fully described by our comorbidity score based on hospital discharge diagnosis. The excess mortality was independent of age, a finding which warrants further studies. The cumulative mortality in the first 30 days, 0.7%, is comparable with the case-fatality rate of 0.8% for all nontyphoidal _Salmonella_ serotypes found in data from FoodNet 1996-97  .\n\n【27】We found that _S._ Typhimurium with R-type ACSSuT was associated with higher death rates than other strains. Similar tendencies were found for chloramphenicol and ampicillin, both being markers for R-type ACSSuT. Patients infected with R-type ACSSuT were seven times more likely to die than the general population, but when the data were adjusted for underlying illness, this figure was reduced to fivefold higher mortality. This reduction was expected; a part of the excess mortality associated with R-type ACSSuT was attributable to underlying illness. However, the excess mortality still tended to be elevated after adjustment. Patients with quinolone-resistant strains had a marked and substantial excess mortality, which could not be explained by imbalances in comorbidity. All the quinolone-resistant strains in this study were designated as fluoroquinolone-susceptible by NCCLS cut-offs for ciprofloxacin. Several patients in the study were part of an outbreak of _S._ Typhimurium DT104 R-type ACSSuTNx traced back to swine herds in the Danish island of Zealand  .\n\n【28】Most deaths occurred in relation to infections with _S._ Typhimurium DT104, and we were not able to demonstrate any statistically significant variation among different phage types. In our initial model we took age into account, expecting a relatively higher mortality among the elderly. But again, we could not demonstrate such an effect. In other words, no additive effect was found between age and drug resistance compared with age and being infected by sensitive strains of _S._ Typhimurium.\n\n【29】A study from England suggests that the isolation rates of drug-resistant DT104 from blood cultures are not higher than those of other _S._ Typhimurium phage types and that the frequency is comparable with the incidence of blood culture isolates of _Salmonella_ Enteritidis  . The study suggests that _S._ Typhimurium of R-type ACSSuT does not cause invasive disease more often than _Salmonella_ Enteritidis. However, the overall mortality in relation to _S._ Typhimurium infection is higher. Two studies based on outbreaks of resistant _Salmonella_ in the United States and the United Kingdom have found case fatality rates of 4.2% and 3.0% respectively . Even though they were based on outbreak investigations, the cumulative death rate is comparable to our results (.9% after 6 months of infection).\n\n【30】Antimicrobial drug resistance in zoonotic _Salmonella_ may be associated with adverse consequences in several ways, including treatment failures. However, treatment failures have, until now, been infrequently reported . We had no data on treatment with antimicrobial drugs. Therefore, exploring the extent to which the excess mortality of patients infected with quinolone-resistant strains was caused by reduced efficacy of drugs was impossible. We estimate that approximately 20% of the patients were prescribed empiric treatment in connection with the collection of specimens and that some of the deaths may have been associated with reduced efficacy of flouroquinolones, as described in Mølbak et al.\n\n【31】Resistant bacteria have a selective advantage in ecosystems where antimicrobial drugs are used. Studies have shown that treatment with antimicrobial drugs (for any reason) is a major risk factor for infections with antimicrobial drug-resistant bacteria, and that this association may result in increased incidence and illness severity . Infection with drug-resistant _S_ . Typhimurium in patients treated for other infections may contribute to the excess mortality we found.\n\n【32】Infections with resistant _Salmonella_ may be associated with increased severity for reasons that are poorly understood. An increased virulence of drug-resistant _Salmonella_ has not been well characterized. Two earlier studies found increased rates of hospitalizations  and death  , but these studies had limitations. Lee et al. were only able to control for comorbidity in a limited way, and none of the earlier studies were restricted to a single serotype and able explore the impact of specific resistance patterns as we did.\n\n【33】The use of antimicrobial drugs in food production is one of the major factors in the emergence and dissemination of antimicrobial drug-resistance in foodborne bacterial pathogens. We were able to determine death rates in a large sample of patients with _S._ Typhimurium and to control for confounding factors in the analyses. We associated resistance in _S._ Typhimurium with excess mortality, and the demonstration of a hazard to human health underscores the need for restrictions in the use of antimicrobial drugs in the production of food from animals. A particular risk was associated with quinolone resistance, indicating that the use of fluoroquinolones for food production animals should be discontinued.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1f94b265-3f2e-466d-a814-e647da5db9c9", "title": "Novel Cetacean Morbillivirus in Guiana Dolphin, Brazil", "text": "【0】Novel Cetacean Morbillivirus in Guiana Dolphin, Brazil\n**To the Editor:** Since 1987, morbillivirus (family _Paramyxoviridae_ , genus _Morbillivirus_ ) outbreaks among pinnipeds and cetaceans in the Northern Hemisphere have caused high rates of death . Two morbillivirus species are known to affect aquatic animals: _Phocine distemper virus_ (PDV) and _Cetacean morbillivirus_ (CeMV). PDV has been isolated from pinnipeds, and 3 strains of CeMV (porpoise morbillivirus \\[PMV\\], dolphin morbillivirus \\[DMV\\], and pilot whale morbillivirus \\[PWMV\\]) have been isolated from dolphins and whales .\n\n【1】Serologic surveys indicate that morbilliviruses infect marine mammals worldwide ; however, only 1 fatal case in a bottlenose dolphin (Tursiops truncatus_ ) has been confirmed in the Southern Hemisphere (in the southwestern Pacific Ocean) . Positive DMV-specific antibody titers in 3 Fraser’s dolphins (Lagenodelphis hosei_ ) stranded off Brazil and Argentina in 1999 indicate the exposure of South Atlantic cetaceans to morbillivirus . We report a case of lethal morbillivirus infection in a Guiana dolphin (Sotalia guianensis_ ), a coastal marine and estuarine species that occurs off the Atlantic Coast of South and Central America.\n\n【2】A female Guiana dolphin calf (cm in total body length)  was found stranded dead in Guriri (°44’S; 39°44’W), São Mateus, Espírito Santo State, Brazil, on November 30, 2010; the dead calf was severely emaciated. Postmortem examination of the animal showed multifocal ulcers in the oral mucosa and genital slit, diffusely dark red and edematous lungs, and congested and edematous brain. Samples of selected tissues were collected, fixed in buffered formalin, and processed according to routine histopathologic methods. By microscopy, the most noteworthy lesions included marked lymphoplasmacytic and neutrophilic meningoencephalitis, optic nerve perineuritis, and hypophysitis. Lungs showed moderate acute diffuse lymphoplasmacytic and neutrophilic interstitial pneumonia; severe multicentric lymphoid depletion and multifocal necrotizing hepatitis were also observed.\n\n【3】Immunohistochemistry was performed by using CDV-NP MAb (VMRD, Inc. Pullman, WA, USA), a monoclonal antibody against the nucleoprotein antigen of canine distemper virus that cross-reacts with cetacean morbilliviruses . Known positive and negative control tissues and test sections with omitted first-layer antibody were included. Viral antigen was detected in neurons in the brain, bronchiolar epithelium and macrophages in the lungs, bile duct epithelium in the liver, and macrophages and lymphocytes in lymph nodes.\n\n【4】We extracted RNA from frozen lung samples by using TRIzol Reagent (Life Technologies Corporation, Carlsbad, CA, USA) according to the manufacturer’s instructions and amplified a 374-bp conserved fragment of the phosphoprotein (P) gene by reverse transcription PCR. The following _Morbillivirus_ spp.–specific primers were used for PCR: 5′-ATGTTTATGATCACAGCGGT-3′ (forward) and 5′-ATTGGGTTGCACCACTTGTC-3′ (reverse) . MEGA5  was used to construct a neighbor-joining phylogenic tree based on the sequenced amplicon from this study  and 12 other GenBank sequences that represent the 6 morbillivirus species already described in the literature. The analysis placed the Guiana dolphin strain at the CeMV clade, but segregated it from the already described dolphin morbillivirus strains PMV, DMV, and PWMV . The sample shared 79.8% nt and 58.4% aa identity with PMV, 78.7% nt and 56.6% aa identity with DMV, and 78.7% nt and 57.1% aa identity with PWMV. Within the _Morbillivirus_ spp. PDV shared the lowest sequence identity (.1% nt and 26.8% aa).\n\n【5】In summary, sequence analysis of the morbillivirus from the dead Guiana dolphin suggests that the virus is a novel strain of the CeMV species; this conclusion is supported by phylogenic analysis and geographic distribution of the virus and by its distinct host. Emaciation, marked lymphoid depletion, interstitial pneumonia, and meningoencephalitis are common findings in morbillivirus-infected animals . Together with antigenic and genomic evidence, our findings indicate that morbillivirus infection is extant in Guiana dolphins in the waters off Brazil.\n\n【6】Morbillivirus outbreaks have caused a high number of deaths among pinnipeds and cetaceans and are a major risk to previously unexposed nonimmune populations of aquatic mammals . A high number of morbillivirus-related deaths have not yet been reported among aquatic mammals in the waters off Brazil, but our findings shows that Guiana dolphin calves are susceptible to infection. Subclinical morbillivirus infection with immune suppression has been reported in bottlenose dolphins in Florida . It is unknown whether subclinical infection occurs in this host population or whether the virus has undergone species-adaptive changes, as proposed for PWMV . The sequence data from our study suggest that the virus from the Guiana dolphin calf is the fourth member of the CeMV group and is closer to the root of the CeMV clade than to that of DMV, PMV, or PWMV. Further studies are required to determine the epidemiology of morbillivirus infection in this and other cetacean species and to assess the risk for epizootic outbreaks among South Atlantic cetaceans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "88e1450d-170a-4d13-ad6a-2c884a1b367c", "title": "Fatal Respiratory Infections Associated with Rhinovirus Outbreak, Vietnam", "text": "【0】Fatal Respiratory Infections Associated with Rhinovirus Outbreak, Vietnam\nThe World Health Organization estimates that ≈2 million children die each year from acute respiratory tract infection (ARI), and most live in developing countries . Human rhinovirus (HRV), a common cause of mild upper respiratory tract infections, may also cause severe ARI in children. We report on an outbreak of severe ARI caused by HRV in children living in orphanages in Vietnam.\n\n【1】### The Study\n\n【2】During December 2007–February 2008, twelve infants <6 months of age with severe ARI infection were admitted to 2 hospitals in Hanoi. Because all 12 infants lived in 2 orphanages in Hanoi, the National Hospital of Pediatrics (NHP) initiated an outbreak investigation. Data on demographic characteristics, clinical features, and outcomes were collected for the 12 infants. Researchers visited 1 of the outbreak orphanages and 1 control orphanage (km apart, no severe ARI observed). Patient histories were obtained from orphanage staff; all infants were examined, and nasal and pharyngeal swab specimens were collected. Respiratory specimens were tested by bacterial culture and multiplex reverse transcription PCR (RT-PCR, Seeplex RV12 Kit; Seegene, Seoul, South Korea) for the following respiratory viruses: influenza A/B viruses, respiratory syncytial virus (RSV), rhinovirus, coronavirus (OC43/HKU1 and 229E/NL63), adenovirus, parainfluenzavirus, and human metapneumovirus . This outbreak investigation was approved by the Scientific Committee of NHP.\n\n【3】Twelve patients with severe ARI were admitted to the NHP intensive care unit over 38 days during the cool months of December 2007–February 2008 (temperature 5°C–14°C). The 12 hospitalized infants (female) were 2- to 4-months-old. Most (/12) were from 1 orphanage, which was selected for investigation. Two hospitalized children had a known underlying condition: congenital hypothyroidism (n = 1) and HIV infection (n = 1). Seven infants were underweight (sex-specific weight-for-age; _z_ score <2 SDs). All exhibited cough, coryza, wheezing, and dyspnea, and 6 (%) had a documented fever.\n\n【4】Acute respiratory distress syndrome developed in all 12 a mean of 8.3 (% CI ± 5 days) days after onset. Mean pressure of arterial oxygen/fractional inspired oxygen ratio was 66.0 (% CI ± 30.6, range 25.1–131.0). Chest radiographs showed extensive bilateral infiltrates. Blood cultures for bacteria were negative. Despite mechanical ventilation and administration of intravenous broad-spectrum antimicrobial drugs, 7 patients died and 3 patients recovered (were lost to follow up). HRV was detected by RT-PCR in all infants; 2 patients were co-infected with RSV and adenovirus . In addition, 1 bronchoalveolar lavage specimen from 1 patient was HRV positive.\n\n【5】The outbreak orphanage was visited within 1 week of outbreak detection. The visit revealed that several other infants had been hospitalized elsewhere, but we could not obtain detailed data about them. We tested nasal–pharyngeal swab specimens (pooled) from all 43 infants (% <12 months of age) living in the outbreak orphanage and nasal swab specimens from the 40 youngest children (.5% <12 months of age) at the control orphanage within 2 weeks of the outbreak. In both orphanages, 5–9 children lived in 1 room and shared the same bed, wiping cloths, basic utensils such as cups, and clothing.\n\n【6】Most children in the outbreak orphanage were female (/43 \\[72%\\]), compared with 50% (/40) in the control orphanage. Of children whose specimens were tested by RT-PCR, 98% (/43) of the infants from the outbreak orphanage had at least 1 symptom of respiratory tract infection compared with 14 (%) of 40 infants at the control orphanage. Among children from the outbreak orphanage (both hospitalized and nonhospitalized \\[n = 55\\]), a single pathogen was identified in 31 (.4%) infants and ≥2 pathogens were found in 9 (.4%) children . The most frequently detected pathogen was HRV (n = 38). In the control orphanage, 9 (.5%) children tested positive for HRV, and 5 children were infected with 2 pathogens. Only nonpathogenic bacteria were cultured from the respiratory specimens (data not shown).\n\n【7】Because HRV was the predominant pathogen detected, we genotyped all HRV isolates directly from the specimens using a molecular typing assay based on phylogenetic comparisons of a 260-bp variable sequence (P1–P2) in the 5′-noncoding region with homologous sequences of the 101 known serotypes . We were able to sequence the P1–P2 fragment of HRV genome from 23 specimens positive for HRV: 2 were from hospitalized patients (R and 08043R) , 19 were from the outbreak orphanage (identification numbers starting with DA), and 5 were from the control orphanage (identification numbers starting with BaVi). The P1–P2 phylogenetic tree showed that sequences obtained from isolates from the 2 hospitalized infants were closely related to a subclade of cluster A sequences from nonhospitalized infants at the outbreak orphanage . HRVs from the control orphanage are also part of the A cluster but formed a distinct subcluster. Several children (not hospitalized) from the outbreak orphanage were also infected with HRVs from the C cluster. No children had B cluster strains.\n\n【8】### Conclusions\n\n【9】We found that HRV was the main pathogen detected in an outbreak of severe ARI in children living in an orphanage in Vietnam. Our findings support recent studies showing that HRV may be associated with severe respiratory tract infection in infants and children . A study in central Vietnam also showed that HRV is a notable cause of ARI in children in Vietnam . Because we detected rhinovirus in all hospitalized infants, we believe that HRV was the main causal agent in this outbreak, although 2 hospitalized infants were co-infected with RSV, an unambiguous respiratory pathogen. A deep lung specimen from 1 hospitalized infant was also positive for HRV, which supports a causal relationship. Furthermore, the outbreak orphanage had significantly more HRV-positive patients than did the control orphanage, and sequence analysis showed that the outbreak isolates formed a distinct cluster, which also supports a causal role for HRV.\n\n【10】In addition, we found that several infants from the outbreak orphanages were infected with HRV strains belonging to the C cluster, according to P1-P2 sequence analysis. HRV-C has been associated with more severe infections and circulates worldwide . We may have missed the HRV-C strains in the clinical case-patients, because we were only able to sequence the virus from samples from 2 patients. We had to sequence directly from the specimens because we had no viral culture facility at the time of the outbreak. Phylogenetic analysis of P1–P2 sequences can distinguish HRV-B rhinoviruses, but it is limited in distinguishing HRV-A and HRV-C. Full HRV sequence analysis would be able to provide this detail, but it was not feasible for this investigation.\n\n【11】This outbreak investigation has some limitations. Only a small number of controls were selected from a single orphanage, and the controls were sampled later than the hospitalized patients and children from the outbreak orphanage. These limitations may have led to underdiagnosis of HRV in controls. Bacterial co-infection cannot be ruled out as a cause of more severe infection in the hospitalized infants because most were receiving antimicrobial drugs at the time respiratory specimens were collected.\n\n【12】Viral respiratory infections can be more severe in malnourished infants, which was likely the case for the hospitalized infants in this outbreak. In this study, co-infections (.4%) with other respiratory viruses were detected at a similar rate as in another study in central Vietnam (.5%) . This finding is also consistent with previous work indicating that HRV infections can occur with other respiratory viruses and lead to more severe disease . This outbreak illustrates that HRVs can cause severe pneumonia, leading to acute respiratory distress syndrome in young, vulnerable infants. HRV remains an underappreciated cause of severe pneumonia in vulnerable groups.\n\n【13】Professor Le Thanh Hai is a pediatrician and head of the Emergency Department of the National Hospital of Pediatrics in Hanoi, Vietnam, and a lecturer at Hanoi Medical University. His research interests are respiratory infections, tuberculosis, and pediatric emergency medicine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5ef85205-252e-40d2-9f7b-202aff4db683", "title": "Pharmacy Data for Tuberculosis Surveillance and Assessment of Patient Management", "text": "【0】Pharmacy Data for Tuberculosis Surveillance and Assessment of Patient Management\nControlling and preventing tuberculosis (TB) continue to be major public health challenges in the United States . Information obtained through TB surveillance ensures that TB-control activities are appropriate and can be used to evaluate the effectiveness of public health programs . Because TB surveillance relies heavily on laboratories and providers to report cases to local health departments, surveillance data can be compromised by underreporting, particularly by private-sector clinicians who treat TB infrequently. Pharmacy data, often available in automated form, may supplement traditional TB reporting, especially because anti-TB medications are rarely used to treat other conditions.\n\n【1】A Massachusetts study found that persons with TB who were identified through pharmacy dispensing records and who had not been previously reported to the state health department represented 16% of all new cases . In that study, receipt of two or more anti-TB drugs identified most cases of active TB. These results suggested that pharmacy dispensing information could supplement traditional TB surveillance. In addition, pharmacy dispensing information for persons with active TB provided useful information about appropriateness of prescribed treatment regimens and adherence to therapy .\n\n【2】We therefore evaluated the contribution of pharmacy data to overall TB surveillance and to assessing the quality of TB management. We performed this study through health plans to facilitate access to pharmacy dispensing data and medical records.\n\n【3】### Methods\n\n【4】##### Study Population\n\n【5】Members of three different health plans in Michigan , Missouri , and Tennessee  were included in the study population. Study periods were based on availability of pharmacy data.\n\n【6】All health plans met our basic criteria of providing most of the medical care to defined populations, providing prescription drug benefits, having automated pharmacy claims files, and having accessible full-text medical records. The health plans differed in some ways. Most importantly, plan C  routinely delegated care of recognized TB patients to local health departments or had members obtain their anti-TB drugs from public health programs separate from the plan’s regular pharmacy programs and data systems. Additionally, the structure of the three health plans and their populations differed; they consisted of the following: a mixed staff and group model that included a large urban population (plan A); an independent practice association (IPA) health plan affiliated with a managed-care organization, principally serving an employed population (plan B); and a mixed IPA and staff model, principally serving Medicaid enrollees (plan C). Staff-model health plans employ providers who practice in common facilities. IPA-model health plans contract with providers who practice in their own offices . Prior institutional review board approval was obtained from participating health plans.\n\n【7】##### Pharmacy Screening\n\n【8】Health plan pharmacy dispensing data were screened to identify all members who received two or more anti-TB medications during the study period. For plans A, B, and C, we screened, respectively, approximately 1.3 million, 1.0 million, and 1.6 million health plan person-years. The anti-TB medications included in the screening were isoniazid, rifampin, pyrazinamide, ethambutol, streptomycin, ethionamide, kanamycin, cycloserine, capreomycin, para-aminosalicylic acid (PAS), and drugs containing any combination of these medications. Although at least 90% of health plan members had some form of pharmacy benefit, the plans varied considerably in directing their members to public health facilities to obtain anti-TB medications.\n\n【9】##### Identifying TB Cases\n\n【10】Reporting confirmed or clinically suspected TB to local or state health departments by providers, laboratories, boards of health, or administrators of hospitals is mandatory in Michigan, Missouri, and Tennessee, which maintain registries of all verified cases. State health department staff in all three states determined whether health plan members identified as having received two or more anti-TB medications had been reported previously to the health departments by matching to the state TB registries by using previously described methods .\n\n【11】For all plan members who received two or more anti-TB medications and who were not previously reported to the state health departments, information was obtained through review of medical records. A case of TB was defined according to the Centers for Disease Control and Prevention (CDC) surveillance definition . In a culture-positive case, _Mycobacterium tuberculosis_ was isolated from a clinical specimen. In a smear-positive case, acid-fast bacilli (AFB) were demonstrated in a specimen in the absence of a culture. A clinical case-patient met all of the following criteria: a positive tuberculin skin test, signs and symptoms compatible with TB, and treatment with two or more anti-TB drugs. Case-patients without a positive culture for _M. tuberculosis_ that were not known to the health departments were verified by review with clinicians experienced in diagnosing and treating TB.\n\n【12】To estimate the number of TB cases not detected by using pharmacy data, each health plan’s membership during the study period was matched to the state health department’s TB registry entries during the same period by using minimal disclosure methods . Potential matches were confirmed with full identifiers. To determine the source of care for patients not identified through pharmacy screening, health department records of all such patients in plans A and B and a random sample in plan C were reviewed.\n\n【13】##### Assessing TB Management\n\n【14】Automated pharmacy dispensing records were used to characterize TB therapy for persons with active TB who met pharmacy screening criteria in plans A and B. Plan C did not participate in the assessment of TB management because members were routinely referred to public health clinics for treatment, and information about medications was unavailable from the health plan pharmacy database. In addition, pharmacy dispensing records from two additional health plans affiliated with plan B were screened, and TB cases verified through medical record review were included in the analysis.\n\n【15】All filled prescriptions were identified for isoniazid, rifampin, pyrazinamide, ethambutol, streptomycin, ethionamide, kanamycin, cycloserine, capreomycin, PAS, and drugs containing a combination of these medications. Initial regimens, i.e. those dispensed at the start of therapy before susceptibility results were known, and final treatment regimens were graded for consistency with American Thoracic Society (ATS) and CDC guidelines in effect at the time of diagnosis . The appropriateness of doses based on patient weight was not evaluated.\n\n【16】Two measures were calculated for therapeutic adequacy. The standard regimen dispensed is a percentage calculated by comparing the cumulative dose of each drug dispensed with the total recommended. Each drug received equal weight to a maximum of 100% per drug, as noted in the following formula for a three-drug regimen: percent standard regimen = ([D 1  /SR 1  \\] + \\[D 2  /SR 2  \\] + \\[D 3  /SR 3  \\]) x (/3), where D X  is the cumulative dose for drug X and SR X  is the recommended total dose. Patients with a score ≥80% were considered to have received an appropriate amount of anti-TB medication. The days without medication for isoniazid or another drug required for the duration of treatment are calculated by dividing the total number of days without medication (based on medication refill intervals and quantities dispensed) by the number of days between the first and last dispensing .\n\n【17】##### Analysis\n\n【18】The sensitivity of pharmacy data was defined as the number of verified TB cases detected by pharmacy screening divided by the total number of verified TB cases identified through the TB registry, pharmacy data, or both methods. The positive predictive value (PPV) of pharmacy screening was defined as the number of verified TB cases detected by pharmacy data divided by the total number of persons meeting pharmacy screening criteria; persons with undetermined case status were excluded. Exact binomial confidence intervals were calculated for sensitivity and PPV .\n\n【19】### Results\n\n【20】##### Dispensing Anti-TB Drugs\n\n【21】A total of 244 patients received two or more anti-TB drugs . Of these, 13 (%) met the TB case definition and had not been previously reported to their respective state health departments. Another 61 (%) were active TB case-patients. Sixty-three percent did not meet the TB case definition, and the status of the remaining 7% could not be determined because the medical records were either unavailable or insufficient.\n\n【22】Of 153 patients who received at least two anti-TB medications but did not meet the CDC TB case definition, 62 (%) were treated for suspected active TB. Of these, 15 (%) received a full course of therapy for suspected active TB. Twenty-one (%) received more than one drug during treatment for latent TB infection, 63 (%) were treated for non-TB mycobacterial infections, and 7 (%) were treated for noninfectious conditions or for unknown reasons .\n\n【23】The overall rate of initiating two or more anti-TB drugs was 6 per 100,000 person-years, ranging from 3 to 11 per 100,000 person-years in the three health plans. Confirmed case rates ranged from 0.9 to 4.3 per 100,000 person-years screened. The 1998 TB incidence for the three states ranged from 3.4 to 8.1 cases per 100,000 persons . For persons meeting pharmacy screening criteria, the proportion confirmed as new case-patients did not vary significantly among the three plans .\n\n【24】##### Newly Identified Cases of TB\n\n【25】A total of 207 health plan members meeting TB case definitions (in plan A, 22 in plan B, and 132 in plan C) were identified through pharmacy data or health department records . Among these, 13 case-patients (%) were unknown to the respective state health departments. Two persons with TB unknown to one health department had been reported to Mississippi State Department of Health. None of the 13 were culture-positive for _M. tuberculosis_ ; one lacked a microbiology culture but met the smear-positive case definition, and the remaining 12 met the CDC TB clinical case definition. All except one involved active pulmonary disease.\n\n【26】One hundred thirty-three TB cases were known to the state health departments but were not identified through pharmacy databases. We reviewed the records of 81 of these patients, of whom 61 (%) received their anti-TB medications from public health clinics; this proportion ranged from 58% (/38) in plan A to 93% in plan C (/28). An additional 3 (%) were treated at Veterans Administration (VA) facilities or were diagnosed with TB during hospitalization and died before discharge. Health plan medical records did not include information about TB diagnosis and treatment for 17 (%) patients. Reasons for this may include TB treatment exclusively by other providers and incomplete documentation in accessible medical records.\n\n【27】The overall sensitivity of the pharmacy screening method to identify persons with active TB was 36% (% in plan A, 32% in plan B, and 39% in plan C). However, the overall sensitivity was 80% after the extrapolated number of persons who received their TB medication from public health clinics rather than the health plans was excluded . The positive predictive value of the pharmacy screening method to identify persons with active TB was 33% (% in plan A, 50% in plan B, and 36% in plan C) .\n\n【28】##### Assessing Management of TB\n\n【29】Of the 29 plan A (n = 15) and plan B (n = 14) members with active TB identified through pharmacy screening, health plan and health department records indicated that 17 (%) did not receive treatment in public health clinics and were likely to have received their anti-TB medications through health plan–reimbursed pharmacies. Twenty-eight (%) patients received initial regimens through pharmacies reimbursed by the health plan. In all instances, the initial regimen dispensed was appropriate. For all 17 patients not treated in public health clinics, the final regimen described in the medical record was adequate with regard to the agents used, doses prescribed, and intended duration of treatment.\n\n【30】Fifteen of the 17 health plan–treated patients received anti-TB medications for at least 70 days (compared to 3 of 13 who were treated outside the health plans \\[relative risk = 3.8, p < 0.01\\]), with a median dispensing duration of 180 days (interquartile range 150–324 days). The median standard-regimen-dispensed score was 100% (interquartile range 93%–100%) . Based on health plan pharmacy data, one patient received an inadequate treatment regimen, with a standard-regimen-dispensed percentage of only 48%. Another health plan–treated patient received a standard-regimen-dispensed score of 100% but had a days-without-medication score of 51%, because of a gap in anti-TB therapy of 143 days. One additional patient with culture-positive _M. tuberculosis_ infection received a standard-regime-dispensed score of 68% and a 60-day duration of dispensing. In all of these cases, the treating physician did not describe noncompliance or document a non–health plan source of anti-TB medications.\n\n【31】### Discussion\n\n【32】TB surveillance has traditionally depended on reporting by laboratories, public health clinics, hospitals, and private practitioners. Several retrospective studies  indicate that TB cases may be underreported, particularly those without positive cultures. In this study, we found that 6% of all TB cases in the three participating health plans had not been reported to state health departments. Most cases missed by traditional surveillance were culture- and smear-negative; however, nearly all patients with missed cases had clinical evidence of pulmonary disease and were therefore of public health interest.\n\n【33】The recent shift of populations at risk for TB, including Medicaid recipients, into managed care raises concerns about reporting. As the proportion of patients with TB who are cared for outside traditional public health–funded clinics grows, the benefit of adjunct surveillance methods based on pharmacy data is likely to increase, since these data are available for a large segment of the U.S. population.\n\n【34】Although we used health plan data for this study, health departments could more efficiently obtain this information directly from pharmacy benefits management companies (PBMs) that act as intermediaries between managed-care organizations and pharmacies, because they administer and manage the prescription drug benefit programs for these organizations. Working directly with PBMs has two advantages. First, PBM information is accessible in real time. Second, since the three largest PBMs in the United States manage the pharmacy claims of approximately 200 million persons, information available from a small number of PBMs could provide a rich resource for public health screening .\n\n【35】The percentage of cases in these three health plans that were missed by traditional surveillance (%) was lower than the 16% missed in the Massachusetts study . This difference may reflect the fact that public health clinics cared for more patients in these health plans than in Massachusetts, where 60% of patients were treated solely by health plan providers, compared to about 40% for these health plans.\n\n【36】Patients who received their anti-TB medications from public health clinics were not identified through health plans’ pharmacy data. However, because these patients are already known to the public health system, supplemental surveillance methods are unnecessary. Pharmacy screening identified 80% of the patients with TB who were not treated outside the health plan. This estimate is conservative; we probably underestimated the number of persons receiving anti-TB medications from public health clinics or other healthcare systems because we based this assessment on the private providers’ records. Intermittent enrollment may also have compromised the sensitivity of pharmacy-based screening. Larger databases that include pharmacy information from multiple health plans within a geographic area, such as those maintained by PBMs, are likely to improve case-finding.\n\n【37】The most common reasons for dispensing two or more anti-TB medications to persons who did not meet the case definition were 1) more than one drug used to treat latent TB infection; 2) suspected active TB; 3) treatment of other mycobacterial infections; and 4) treatment for suspected active TB and receiving full courses of therapy, despite not meeting the CDC surveillance definition for TB, based on information available from their medical records. Persons in the last category may warrant additional evaluation by health departments because the case definition may not detect all patients who meet clinical standards for treatment.\n\n【38】The PPV of pharmacy screening criteria may be lower in clinical settings where treatment of non-TB mycobacterial infections is common. One strategy to increase the efficiency of pharmacy-based screening would be to use microbiologic culture information to quickly identify and exclude from further follow-up any persons with results indicating mycobacterial species other than _M. tuberculosis_ . Complete laboratory reporting for _M. tuberculosis_ is an important prerequisite for efficiently implementing this surveillance strategy.\n\n【39】In routine practice, pharmacy data might be used for active TB case-finding, with direct reporting from organizations dispensing drug information, such as health plans or PBMs, to local or state health departments. These data are typically available from health plans within 1 or 2 months and from PBMs within a day. Such reporting would require verifying case status by health department personnel.\n\n【40】Obtaining and reviewing medical records for this study were labor-intensive, but collecting this information from providers in real time should be more efficient. The cost of reporting would be relatively small for health plans or pharmacy benefits managers, and the cost per person identified would be small for large organizations. The additional costs for health departments to evaluate the status of persons not already identified will vary considerably across health departments. Despite the increased emphasis on privacy, current laws specifically allow reports of protected health information to support public health activities.\n\n【41】Although pharmacy data may be useful, they will not replace traditional surveillance of suspected and confirmed TB cases. Because rapidly following-up suspected TB cases is essential to prevent the spread of _M. tuberculosis_ , educating providers to report suspected cases promptly to public health officials will continue to be important.\n\n【42】Automated pharmacy data also provided useful information about physicians’ management of TB and about patients’ adherence to prescribed therapy. Monitoring these aspects of TB care is particularly important when care is decentralized or when patients receive care from more than one provider. Pharmacy information demonstrated that, in nearly all cases, appropriate empiric regimens were prescribed. In most cases managed by health plan providers, full ATS/CDC-recommended regimens were dispensed. Consistent with the Massachusetts study results, using a cutoff value of at least 70 days of therapy identified most patients treated solely within the health plan. This practice is important in monitoring adherence to therapy, since automated pharmacy information is complete only for these patients. Pharmacy data also identified several persons with evidence of suboptimal adherence to therapy. Pharmacy information on anti-TB drugs could thus be used for monitoring the appropriateness of case management and to evaluate the program.\n\n【43】This study and our earlier work demonstrate that pharmacy data may be useful in settings where TB care is provided by the private healthcare system. Centralized repositories of pharmacy data, such as those maintained by PBMs, may facilitate even more efficient application of this surveillance strategy to find TB cases and assess TB management for large patient populations. Similar studies in other settings could expand our understanding of current surveillance limitations and provide better estimates of the true burden of TB in the United States. Similar strategies could also be considered to augment traditional surveillance for other diseases of public health importance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "23644816-3e59-40cc-92ef-cf1cec18b389", "title": "New Reassortant Clade 2.3.4.4b Avian Influenza A(H5N6) Virus in Wild Birds, South Korea, 2017–18", "text": "【0】New Reassortant Clade 2.3.4.4b Avian Influenza A(H5N6) Virus in Wild Birds, South Korea, 2017–18\nClade 2.3.4.4 H5 highly pathogenic avian influenza viruses (HPAIVs) have evolved by reassortment with different neuraminidase (NA) and internal genes of prevailing low pathogenicity avian influenza viruses (LPAIVs) and other HPAIVs to generate new genotypes and further evolved into genetic subgroups A–D since 2014 . Among these, subgroups A and B viruses were disseminated over vast geographic regions by migratory wild birds . Subgroup B influenza A(H5N8) viruses were detected in Qinghai Lake, China, and Uvs-Nuur Lake, Russia, during May–June 2016 (Qinghai/Uvs-like), followed by the identification of reassortant viruses in multiple Eurasian countries . Recently, subgroup B H5N6 viruses were isolated from birds in Greece during February 2017 and England, Germany, the Netherlands, Japan, and Taiwan during winter 2017–18 .\n\n【1】During December 2017–January 2018 in South Korea, we isolated 6 H5N6 HPAIVs from 231 fecal samples of wild birds collected from the banks of the Cheongmi-cheon River (°06′56.9′′N, 127°25′18.3′′E) and 34 from 222 fecal samples collected from the banks of the Gokgyo-cheon River (°45′12.3″N, 127°07′12.7″E) . These wild bird habitats are wintering sites of migratory waterfowl, including mallard (Anas platyrhynchos_ ), spot-billed duck (Anas poecilorhyncha_ ), Mandarin duck (Aix galericulata_ ), and common teal (Anas crecca_ ). The Gokgyo-cheon River is a major habitat site for Mandarin ducks, and numerous HPAIVs were detected in fecal samples from Mandarin ducks during 2011, 2015, and 2016 . We identified avian influenza virus–positive fecal samples from 38 Mandarin ducks and 2 mallards, based on DNA barcoding technique . We performed full-length genome sequencing and comparative phylogenetic analysis on 19 of the 40 isolates .\n\n【2】All H5N6 isolates shared high nucleotide sequence identities in all 8 gene segments (.58%–100%) and were identified as HPAIVs based on the presence of multiple basic amino acids at the HA proteolytic cleavage site (PLREKRRKR/G). Searches of the GISAID  and BLAST  databases indicated that all 8 genomes had the highest nucleotide identity with A/Great\\_Black-backed\\_Gull/Netherlands/1/2017 (Netherlands/1) clade 2.3.4.4 subgroup B H5N6 strain from December 2017 (.17%–99.79%), rather than subgroup B H5N6 viruses from Japan and Taiwan collected during December 2017 (.18%–99.27%).\n\n【3】In phylogenetic analysis, we identified 2 genotypes of subgroup B H5N6 viruses : genotypes B.N6.1 and B.N6.2. The genotype B.N6.1 viruses were identified from South Korea, Japan, Taiwan, Greece, and the Netherlands (Netherlands/1 strain), and the genotype B.N6.2 viruses were detected from England, Germany, and the Netherlands. For genotype B.N6.1, all genes except NA clustered with H5N8 HPAIV of previously reported genotypes, H5N8-NL cluster I in the Netherlands , Ger-11-16 in Germany , and Duck/Poland/82a/16-like in Italy . The NA gene clustered with LPAIVs circulating in wild birds in Eurasia and separated into 2 clusters, suggesting the potential for >2 independent reassortment events between H5N8 virus and unidentified wild bird origin N6 segments. Consistent clustering of South Korea isolates with the Netherlands/1 strain in maximum-likelihood (ML) phylogenies for each gene supported by high ML bootstrap values  suggests their close relationship. The genotype B.N6.2 viruses had different polymerase basic 2 (PB2) and polymerase acidic (PA) genes from genotype B.N6.1. The polymerase basic 2 gene probably originated from other LPAIVs, and a polymerase acidic gene originated from H5N8-NL cluster II genotype . The phylogenetic network and ML analysis suggest that H5N6 viruses have evolved from subgroup B H5N8 viruses into 3 independent pathways, detected in Greece, Europe/South Korea, and Japan/Taiwan .\n\n【4】The time of most recent common ancestry (tMRCA) for each gene of genotype B.N6.1 H5N6 viruses isolated during winter 2017–18 in Eurasia, except for the NA gene, ranged from January 2016 to October 2016, suggesting that genotype B.N6.1 viruses diverged during the previous year. The tMRCA of the NA gene was September 2015 (% highest posterior density August 2014–August 2016). The tMRCA of the NA gene has wide 95% highest posterior density range because only a few recent N6 genes of LPAIVs were available in databases for analysis. The tMRCA for each gene of H5N6 HPAIVs identified in South Korea ranged from July through September 2017, suggesting that ancestors of these viruses emerged among wild birds during or after summer 2017, possibly at the breeding and molting sites in the Palearctic region . Detection of H5N6 HPAIV from fecal samples of wild birds in South Korea during the 2017–18 wintering season and our phylogenetic analysis suggest that the viruses had moved through wild birds during the fall migration season.\n\n【5】On the basis of our data and migratory pattern of birds, we estimate that H5N6 viruses possibly descended from H5N8 viruses circulating during 2016–17, reaching breeding regions of wild birds during early 2017, followed by dissemination into Europe and East Asia during the fall migration. Enhanced surveillance in wild birds is needed for early detection of new introductions of HPAIV and to trace the transmission route of HPAIV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "19ae36d6-91fc-4637-bd06-549c9f25816a", "title": "Restaurant-Based Measures to Control Community Transmission of COVID-19, Hong Kong", "text": "【0】Restaurant-Based Measures to Control Community Transmission of COVID-19, Hong Kong\nAs of April 14, 2021, a total of 11,608 cases and 207 deaths from coronavirus disease (COVID-19) had been reported in Hong Kong . A series of community epidemics have occurred, the largest of which have been the third wave in June–October 2020, which had 3,978 cases, and the fourth wave in November 2020–March 2021, which had 6,048 cases. To suppress local transmission of COVID-19, the government implemented a combination of public health and social measures (PHSMs): bar closures, restaurant capacity restrictions and opening hour restrictions, bans on live music performances and dancing, and work-from-home advisories . Ongoing assessment of the effect of these measures on transmission can guide evidence-based policy. One type of location in which COVID-19 transmission is known to occur is restaurants . Earlier studies have evaluated the impact of PHSMs, including restrictions on large group gatherings , but the specific effect of restaurant measures was not studied. Here we focus on the effect of restaurant measures on transmission in Hong Kong.\n\n【1】We collected details and time of implementation of each intervention of all the PHSMs applied during the third and fourth waves from the official reports of the Hong Kong government  . In wave 3, a ban on dine-in service after 6:00 pm was in force during July 15–August 27, 2020 . Other PHSMs were implemented on the same day and kept in place for longer. Wave 4 was initiated by multiple superspreading events in a network of dancing venues. A ban on dine-in service after 6:00 pm was implemented on December 10, 2020, which was a week to a month later than the implementation of other PHSMs . Hence, we could disentangle the effect of shortened dine-in hours from other measures. No other PHSMs were implemented before the study period.\n\n【2】To determine the effect of the ban on dine-in services after 6:00 pm , we applied a previous approach to estimate time-varying reproduction number (R t  ) . Then, we fitted LASSO regression models to log(R t  ) to assess the effect of the ban on dine-in services after 6:00 pm on R t  , accounting for the effect from other PHSMs . We allowed for a 7-day lag between implementation of a measure and its effect on incidence, to account for the incubation period. In both waves, we grouped the PHSMs other than ban on dine-in services after 6:00 pm into a single variable to indicate the period when \\> 3 of these other PHSMs were in place.\n\n【3】We estimated that the ban on dine-in services after 6:00 pm did not reduce R t  in both waves, but other PHSMs were associated with substantial reductions in R t  . In wave 3, R t  rose rapidly to 4.5 on June 27, 2020, but ≈1 week after measures were applied it was <1.0 . Implementation of \\> 3 other PHSMs was associated with a 53% (% CI 44%–59%) decrease in R t  .\n\n【4】In wave 4, R t  increased to 3.1 on November 16, 2020, and then decreased to ≈1.0 after PHSMs began . Implementation of \\> 3 other PHSMs was associated with a 40% (% CI 28%–47%) decrease in R t  . Another model that excluded basic civil service arrangement in other PHSMs showed that a ban on dine-in service beginning at 6:00 pm did not have an effect . We performed sensitivity analysis to remove the effect of superspreading in wave 3 by changing the start date to July 1, 2020; we found the ban on dine-in service from 6:00 pm did not have an effect .\n\n【5】Our analysis suggested that the PHSMs were critical for suppressing the third and fourth waves of COVID-19 in Hong Kong. However, we found that a ban on dine-in hours after 6:00 pm might not have had an effect in both waves when capacity was already reduced. A complete closure of restaurants in Hong Kong would have considerable social impact because dining out is very common. We hypothesize that encouraging restaurants to extend dine-in hours, but with capacity restrictions to reduce crowding, could be a reasonable approach to reduce transmission.\n\n【6】A limitation of our analysis is that we cannot distinguish the effect of some PHSMs because they began simultaneously. We cannot rule out that a ban on dine-in service after 6:00 pm might have an effect if it began earlier than other PHSMs or in regions with high incidences. In addition, changes in R t  are a consequence of individual behavioral changes such as avoiding crowded areas; increasing incidence and implementation of multiple PHSMs could raise the public’s perception of risk. Determining the effectiveness of alternative PHSMs would provide evidence-based guidance on control strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b6778432-d155-4382-824d-6298c2872030", "title": "Acrylamide", "text": "【0】*   Overview\n*   NIOSH Chemical Resources\n*   Related NIOSH Resources\n*   Selected Publications\n*   Related Resources\n*   International Resources\n\n【1】### Overview\n\n【2】CAS No. 79-06-1\n\n【3】Acrylamide (CH 2  \\=CHCONH 2  ) is a white, crystalline solid. It can be harmful to the eyes, skin, and nervous and reproductive systems. Workers may be harmed from exposure to acrylamide.  The level of harm depends upon the dose, duration, and work being done.\n\n【4】Acrylamide is used in many industries. It is used to treat waste water from treatment facilities, and in the manufacture of paper, contact lenses, some dyes, chemicals, and fabrics  Examples of workers at risk of being exposed to acrylamide include the following:\n\n【5】*   Workers who manufacture certain cosmetics and toiletries\n*   Employees exposed to certain types of dyes and organic chemicals\n*   Manufacturing workers who make polyacrylamide\n*   Construction workers involved in constructing tunnels, wells, and sewers\n\n【6】NIOSH recommends that employers use Hierarchy of Controls to prevent injuries. If you work in an industry that uses acrylamide, please read chemical labels and the accompanying Safety Data Sheets for hazard information. Visit NIOSH’s page on Managing Chemical Safety in the Workplace to learn more about controlling chemical workplace exposures.\n\n【7】The following resources provide information about occupational exposure to acrylamide. Useful search terms for acrylamide include “acrylamide monomer,” “acrylic amide,” “propenamide,” and “2-propenamide.”\n\n【8】### NIOSH Chemical Resources\n\n【9】NIOSH Pocket Guide to Chemical Hazards\n\n【10】The NIOSH Pocket Guide to Chemical Hazards (NPG) helps workers, employers, and occupational health professionals recognize and control workplace chemical hazards.\n\n【11】Manual of Analytical Methods\n\n【12】The NIOSH Manual of Analytical Methods (NMAM) is a collection of methods for sampling and analysis of contaminants in workplace air, and in the blood and urine of workers who are occupationally exposed.\n\n【13】Health Hazard Evaluations\n\n【14】The Health Hazard Evaluation Program (HHE) conducts onsite investigations of possible worker exposure to chemicals. Search the HHE database for more information on chemical topics.\n\n【15】### Related NIOSH Resources\n\n【16】*   NIOSHTIC-2 search results on acrylamide —NIOSHTIC-2 is a searchable database of worker safety and health publications, documents, grant reports, and journal articles supported in whole or in part by NIOSH.\n*   Immediately Dangerous to Life or Health (IDLH) Profile Value: Acrylamide —NIOSH reviews relevant scientific data and researches methods for developing IDLH values.\n*   NIOSH Worker Health Study Summaries —NIOSH conducts research to prevent illnesses and injuries in the workplace. The NIOSH Worker Notification Program notifies workers and other stakeholders about the findings of these research studies.\n\n【17】### Selected Publications\n\n【18】*   Criteria for a Recommended Standard: Occupational Exposure Standard for Acrylamide —DHHS (NIOSH) Publication No. 77-112. This report increases awareness and recommends work practices to reduce exposures to acrylamide.\n*   NIOSH Skin Notation (SK) Profiles: Acrylamide\n*   NIOSH and NIOSH Basis for an Occupational Health Standard – Acrylamide: A Review of the Literature\n*   Occupational Safety and Health Guideline for Acrylamide\n\n【19】### Related Resources\n\n【20】*   ATSDR ToxFAQs for Acrylamide\n*   ATSDR Toxicological Profile for Acrylamide\n*   EPA Chemistry Dashboard: Acrylamide external icon\n*   EPA Hazard Summary: Acrylamide external icon\n*   EPA Hazardous Air Pollutants: Acrylamide external icon\n*   EPA Integrated Risk Information System (IRIS): Acrylamide external icon\n*   IARC Monographs (Vol. 60): Acrylamide external icon\n*   NTP-CERHR Monograph (Reproductive Hazards): Acrylamide external icon\n*   NTP Report on Carcinogens (Fourteenth Edition): Acrylamide external icon\n*   NTP Substance Profile: Acrylamide external icon\n*   OSHA Hazard Communication external icon\n*   OSHA Occupational Chemical Database: Acrylamide external icon\n*   OSHA Sampling and Analytical Methods: Acrylamide external icon\n*   New Jersey Hazardous Substance Fact Sheet: Acrylamide external icon\n*   National Cancer Institute: Acrylamide external icon\n\n【21】### International Resources\n\n【22】*   Canadian Centre for Occupational Health and Safety (CCOHS): Acrylamide external icon\n*   European Chemicals Agency (ECHA): Acrylamide external icon\n*   Gestis Substance Database external icon\n*   International Chemical Safety Card: Acrylamide external icon\n*   IPCS INCHEM: Acrylamide external icon\n*   OECD Global Portal to Information on Chemical Substances external icon", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bb598f51-18ee-4511-97bd-3207ea31cf63", "title": "Household Effects of School Closure during Pandemic (H1N1) 2009, Pennsylvania, USA", "text": "【0】Household Effects of School Closure during Pandemic (H1N1) 2009, Pennsylvania, USA\nSome studies have suggested that school-age children are influential in the ongoing transmission of influenza . Closing schools may potentially reduce the spread of influenza . In mid-May 2009, an elementary school (kindergarten-4th grade) in a semirural area of Pennsylvania closed for 1 week after an abrupt increase in absenteeism due to influenza-like illness (ILI) and the confirmation of influenza A pandemic (H1N1) 2009 virus infection in 1 student. Other schools in the district remained open. From May 26 through June 2, 2009, investigators from the Pennsylvania Department of Health and the Centers for Disease Control and Prevention surveyed households with students at the school by telephone to assess influenza symptoms, childcare arrangements, movements of affected children during the school closure period, and household demographics and socioeconomic status. This study did not address the transmission effects, but assessed the potential disruption to households resulting from school closure.\n\n【1】### The Investigation\n\n【2】The survey was considered a public health response. School administrators provided contact information for households with children attending the school. Investigators asked to speak to an adult in the household. If an adult was available and consented, the survey was administered. For each day of school closure, respondents were asked for the following information: where the student spent most of the day; whether the student went elsewhere (prompted by specific venues), who watched the student; and whether the person watching the student missed work. Questions were asked regarding the oldest student if multiple children attended the school.\n\n【3】Respondents were also asked, for each household member, whether the person had symptoms of ILI (defined as fever with cough and/or sore throat) between May 1, 2009, and the time of the survey. Children were defined as persons <18 years of age, and those \\> 18 years of age were considered adults. The Technical Appendix describes the process followed to calculate variables used in the analysis.\n\n【4】The locations where students spent most of the day and other venues visited were tabulated. Significant differences in venues visited by students with and without ILI were determined by using the Fisher exact test. We computed unadjusted and adjusted odds ratios (ORs) for the following characteristics versus whether the household reported missing \\> 1 workdays: whether the oldest student reported ILI (repeated for whether any adult, any student at the closed school, or any child in the household reported ILI), whether the household had a single child, whether the household had just 1 adult, whether all adults in the household worked outside the home, and whether household income was above the median . Adjusted ORs were computed in a logistic regression model for variables that had unadjusted ORs significant at p<0.10 by the Fisher exact test.\n\n【5】Surveys were completed for 214 (%) of 364 households (%), and accounted for 269 (%) of the 456 students enrolled at the school. Table 1 shows the demographics of surveyed households. Most households had at least 2 adults, at least 2 wage earners, and \\> 2 children. Households with incomes \\> $60,000 were at or above the median income. Because some of the oldest students spent days in multiple locations during the 5 days of school closure, we calculated the number of student-days at each venue (number of students at each type of venue multiplied by the number of days spent there). Home was the primary location during the school closure for 77% of the student-days . The next most common location was another family member’s home.\n\n【6】Sixty-nine percent of students visited other venues during school closure . Those reported as having ILI were more likely to have visited a healthcare provider than those without ILI (p<0.01), but no other statistically significant differences were found in terms of venues visited between those with ILI and those without ILI. Seventy-nine percent of households reported zero missed workdays ; of the remaining households in which work was missed, ≈40% missed work during all 5 days of school closure.\n\n【7】The only household characteristics for which the OR for missing any workdays was significantly different from 1 at p<0.10 were single child, all adults work, and household income is greater than or equal to median income . When adjusted ORs were calculated, household income greater than or equal to median was significant at p<0.05, but because income data were only available for 184 households (vs. 214 for the other factors), the sample on which the adjusted ORs were calculated was somewhat different. All adults in the household working was significantly associated with household income greater than or equal to the median (p<0.01).\n\n【8】### Conclusions\n\n【9】Estimating the economic effects of school closure can provide useful information to aid in estimating whether it is likely to achieve the intended goals. Households that reported missed work incurred costs, even if those costs were only in terms of lost vacation or sick time.\n\n【10】The data show that most of the oldest students spent the days of school closure at home. However, most students left the home at least once during the closure period to visit routine venues (stores, locations of sports events or practices, restaurants). Few differences were found for reported ILI (with the obvious exception that students with ILI had significantly more visits to healthcare providers). These latter 2 findings are similar to those found in a 2006 study of an influenza B–related school closure in North Carolina, USA . This behavior, particularly by students who reported ILI, may increase the risk for onward transmission. A survey of 2 school districts in Kentucky that experienced a seasonal influenza–related school closure also found that students engaged in many activities outside the home , as did a survey of households affected by pandemic (H1N1) 2009 school closure in Australia .\n\n【11】In our study, only 22% of households reported missing any work to watch the students, fewer than during the closure in Australia . However, in ≈40% of households in which work was missed, an adult missed work for all 5 days of closure, indicating a relatively large effect on those households . A limitation is that the question regarding missed work was narrowly worded  and did not explore whether an adult missed work for other reasons. As shown in Table 2 , adult ILI was not significantly associated with missing work. Some adults with ILI may have stayed at home to watch students but determined that they would have stayed home because of their own illness had the school not been closed and answered “no.” In the Kentucky school closure situation, 29% of households had working adults who provided childcare. In 16% of households, adults missed work and lost pay . Closures for >1 week may result in more households that report missing work days. The factors “all adults working” and “having a household income equal to or greater than the median” were associated with missed workdays, as were fewer children (other children in the home may have made it possible for some households to avoid having an adult miss work to watch students whose school was closed).\n\n【12】These findings add to the body of literature on the effects of school closure on households. They can be used by decision makers, as well as parents, to assess the potential social disruption of school closure in the context of future influenza outbreaks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5351bb3-7dc2-4c2d-a54e-ae432b261b7a", "title": "Pig Herds Free from Human Pathogenic Yersinia enterocolitica", "text": "【0】Pig Herds Free from Human Pathogenic Yersinia enterocolitica\n_Yersinia enterocolitica_ is a major cause of foodborne disease in the industrialized world . The emergence of _Y. enterocolitica_ O:3 and O:9 in Europe and Japan in the 1970s and in North America by the end of the 1980s has been characterized as an example of a global pandemic . Outbreaks of _Y. enterocolitica_ O:3 have occurred among black US infants due to cross-contamination during household preparation of raw pork intestines (chitterlings) , and the main reservoir for _Y. enterocolitica_ O:3 in Europe is the domestic pig population . A case–control study conducted by the US Centers for Disease Control and Prevention (CDC) and the Norwegian Institute of Public Health (NIPH) indicated pork products as a major source of yersiniosis in humans in Norway . As a result of this and other epidemiologic studies , improved slaughtering and dressing procedures of pigs  were implemented in Norwegian abattoirs in 1994. The decline in the incidence of human yersiniosis , which started in 1995, is most likely the result of these preventive measures. Among the Nordic countries, Denmark, Norway, and Sweden started to improve slaughter hygiene by implementing the plastic bag technique during 1990–1995; however, Finland did not implement this technique, which may have contributed to the higher level of human yersiniosis in this country than in the other Nordic countries .\n\n【1】During an outbreak in January and February 2006, 11 human cases of _Y. enterocolitica_ O:9/biovar 2 infection were identified in Norway; 2 patients died and reactive arthritis developed in 1 . A case–control study and microbiologic findings indicated a processed pork product (julesylte; Christmas brawn) as the probable source. Another, smaller, family outbreak of yersiniosis occurred, caused by _Y. enterocolitica_ O:3/biovar 4 in brawn and was registered in the outbreak database at NIPH in 2006 .\n\n【2】Most Norwegian pig production is organized in a closed breeding system in which primary nucleus-herd farms sell breeding animals to secondary multiplying-herd farms. These multiplying-herd farms sell breeding animals to conventional-herd farms (farrowing to finishing herds or young pig production). In turn, animals from young pig–production farms are sold to fattening-herd farms. These breeding pyramids are kept free from animal diseases such as sarcoptic mange, swine dysentery, and enzootic pneumonia. If successful elimination of human pathogenic _Y. enterocolitica_ could be accomplished on the top levels of the breeding pyramids, prevalence of human pathogenic _Y. enterocolitica_ might be lowered in the general pig population. Previously, Skjerve et al. indicated that intervention at herd level is a possible strategy for maintenance of _Y. enterocolitica_ O:3/biovar 4–free pig herds in Norway. Serologic analysis showed 182 (.4%) of 287 herds to be positive for _Y. enterocolitica_ O:3. Among the seropositive herds in this study, significantly fewer were mixed herds of piglets and fatteners (.1%) than fattening herds (%). Mixed herds represent a significant protective factor against infection with _Y. enterocolitica_ O:3/biovar 4 because the herd is not supplemented by animals brought in from outside sources. Thus, reducing the herd prevalence of _Y. enterocolitica_ O:3/biovar 4 may be possible by minimizing contact between infected and noninfected herds.\n\n【3】The ability to create pig herds free of human pathogenic _Y. enterocolitica_ has been evaluated. We report that a specific pathogen–free (SPF) breeding pyramid with focus on animal disease can be established and maintained free from _Y. enterocolitica_ O:3/biovar 4.\n\n【4】### Material and Methods\n\n【5】##### Herds\n\n【6】In 1996, the first SPF nucleus herd (herd 1; 100 breeding sows) was established by hysterectomy, and the piglets were reared without contact with other pigs. In 1999, a second nucleus SPF herd (herd 2; 65 breeding sows) was established with gilts from herd 1. These 2 herds have been totally isolated from other herds, except for artificial insemination. Since 1997, 14 new SPF herds have been established with gilts from 1 or both of the above-mentioned SPF nucleus herds; each has been maintained as a closed herd (or supplemented with replacement gilts from 1 of the 2 SPF nucleus herds). Each of these 14 new SPF herds had an average of 60 animals (range 20–150). All SPF herds are housed, the water supply is potable, and pest control systems are established. Pets and wild animals cannot enter the pig house. The owner, herdsmen, veterinarians, and technicians must shower and change clothes before entering the pig housing. Many pig herds organized in the general closed breeding system have also implemented many of these preventive measures.\n\n【7】**Testing of pigs**\n\n【8】Previously, Nesbakken et al. have shown that _Y. enterocolitica_ O:3/biovar 4 can be detected in different age groups of pigs by 1) serologic testing of pigs at all ages from ≈100 days, including at slaughter when the pigs are 150–180 days old; and 2) bacteriologic examination of feces from pigs of all ages from 85 days until ≈135 days. In most instances, the testing of pigs in our study has been in accordance with the conclusions of Nesbakken et al .\n\n【9】##### Collection of Blood Samples\n\n【10】After the original 54 samples were tested in 1996, blood samples from 30–60 pigs in herd 1 were tested for antibodies against _Y. enterocolitica_ O:3 every year from 1998 through 2007, and samples from 30 pigs in herd 2 were tested each year from 2001 through 2006. Periodically, from 2002 through 2007, blood samples from 19–60 pigs from the 14 secondary SPF herds were tested . Most blood samples were collected from 4- to 6-month-old fatteners or gilts. Through 2001, some samples from pigs in the 2 nucleus herds were from sows. In total, blood samples from 1,083 pigs from 16 different herds were tested for antibodies against _Y. enterocolitica_ O:3.\n\n【11】##### Collection of Fecal Samples\n\n【12】Each herd was sampled once. In total, 286 samples were collected from 18–24 animals from each of 4 herds in 2005 and 10 herds in 2006 . Fecal samples were not collected from herds 5 (the owner did not give permission) and 9 (no longer registered as an SPF herd since 2006). Fecal samples weighed 0.1–36.8 g. The average amounts per herd tested varied from an average of 0.8 g (range 0.1–3.3 g) to an average of 23 g (range 8–31 g). The fecal samples were aseptically collected from the rectum of the pigs (days of age) by use of a clean plastic glove.\n\n【13】##### Serologic Methods\n\n【14】Serum samples were analyzed for antibodies against _Y. enterocolitica_ O:3 by using an indirect pig immunoglobulin lipopolysaccharide ELISA  at the Danish Veterinary Institute, Technical University of Denmark, Copenhagen. A basic cut-off of optical density (OD) 20% was used to maximize the specificity of the ELISA.\n\n【15】##### Isolation and Characterization of _Y. enterocolitica_\n\n【16】_Y. enterocolitica_ were cultured and isolated according to the International Organization for Standardization  with modifications . Colonies characteristic for _Yersinia_ were confirmed biochemically, first by selecting only lactose-negative, urease-positive colonies and later with Vitek (BioMerieux Limited, Marcy l’Etoile, France) by using the revised biogrouping scheme for _Y. enterocolitica_  as a key, and serologically for O:3 and O:9 reactivity (and 63502; Sanofi Diagnostics-Pasteur, Marnes la Coquette, France).\n\n【17】### Results and Discussion\n\n【18】The serologic and the bacteriologic results showed a low rate of exposure to _Y. enterocolitica_ O:3/biovar 4 in the pigs from the closed SPF herds . During the first 5 years, 10 of 174 blood samples from pigs in herd 1 had low levels of antibodies against _Y. enterocolitica_ O:3; however, because some of these pigs were old sows, the low titers (OD >20% but <31%) are consistent with past exposure to the organism or nonspecific cross-reaction rather than active infection. Bowman et al. report that gestating sows had the second highest prevalence of human pathogenic _Y. enterocolitica_ among the different age categories at herd level; _Y. enterocolitica_ was never detected in the farrowing sows. Gürtler et al. did not detect human pathogenic _Y. enterocolitica_ among sows. However, according to these 2 reports, the sows were investigated by culture and not by serologic testing . In the past 5 years , none of the 223 blood samples taken from pigs in this herd has been positive for _Y. enterocolitica._ Although some of the blood samples from the 2 nucleus herds were from old sows, most were from fattening pigs at slaughter. If nucleus herd 1 had been truly positive, pigs purchased from this herd would probably have infected the other herds because this herd was at the top of the breeding pyramid. In herds 3 and 10, 1 of 61 animals was positive. When a herd has a history of infection with _Y. enterocolitica_ O:3/biovar 4, antibodies are widely distributed among the animals . Accordingly, it is not likely that herds 1, 3, and 10 were infected by _Y. enterocolitica_ O:3/biovar 4. The specificity of the serologic ELISA used is not fully known; false positives might appear. Only 1 of the 16 herds examined (herd 14) was classified as serologically positive for antibodies against _Y. enterocolitica_ O:3. Among the 30 animals tested, 15 were positive (OD average 39%; range 0%–109%). This herd was also the only one that was positive for _Y. enterocolitica_ O:3/biovar 4 according to culture result. The isolation method used in our study has proven to be sensitive for isolation of _Y. enterocolitica_ O:3/biovar 4 even when the fecal samples are small . On the basis of intestinal tract content samples (n = 120), there was no statistical difference between the isolation method used in our study and the BUGS’n BEADS (Genpoint, Oslo, Norway) detection method (PCR) for virulent _Y. enterocolitica_ .\n\n【19】According to serologic testing results, 15 of the 16 SPF herds examined were free from _Y. enterocolitica_ O:3/biovar 4. The first basic nucleus herd at the top of this breeding pyramid has remained free from this pathogenic variant since the herd’s establishment in 1996. A total of 13 herds were confirmed negative for _Y. enterocolitica_ O:3/biovar 4 by culture of feces. Broadly, these findings show that clusters of pig herds free from _Y. enterocolitica_ O:3/biovar 4 can be established and kept free from this human pathogenic variant for many years. Christensen  also documented a low level of human pathogenic _Y. enterocolitica_ in 4 SPF herds examined by tonsil swabs in Denmark during 1978–1979. From 99 pigs he found only 1 isolate of _Y. enterocolitica_ serovar O:3/biovar 4.\n\n【20】The low prevalence of human pathogenic _Y. enterocolitica_ observed in the herds’ immediate environment (e.g. water, rodents, flies) by Pilon et al. suggests that the environment does not represent the main source of contamination of pigs by human pathogenic _Y. enterocolitica._ Rather, transmission is more likely from other infected pigs. Thus, mixed herds in closed health and breeding pyramids represent an important barrier against infection with _Y. enterocolitica_ O:3/biovar 4. Reduction in prevalence of human pathogenic _Y. enterocolitica_ at the top levels of the health and breeding pyramids may also reduce the prevalence of _Y. enterocolitica_ O:3/biovar 4 in the general pig population. The meat industry could then categorize herds by serologic or bacteriologic methods and use these results in its strategy to reduce the risks for consumers. Serologic testing is preferable to bacteriologic methods on the basis of practicality, time-saving aspects, and costs. If human pathogenic _Y. enterocolitica–_ free segments of the pig population could be established, preharvest risk management might be possible by using serologic methods to categorize herds. If this experience is used in the general health and breeding pyramids of pig herds, the Norwegian meat industry could provide pork from pigs raised in herds free from human pathogenic _Y. enterocolitica_ , which might be the starting point for providing human pathogen–free (HPF) pork on the market. The following facts should be considered in discussions of the possibility of establishing HPF herds: 1) <0.1% of the pigs in Norway harbor _Salmonella_ ; 2) the most recent case of _Trichinella_ infection in pigs was in 1994 ; 3) 2.6% of 1,605 pigs from 321 herds had antibodies against _Toxoplasma gondii_ , and only 1.3% of the mixed herds had antibodies against _T. gondii_ according to the data on which this article is based; and 4) ≈100% of the pigs harbor _Campylobacter_ spp.\n\n【21】Closed SPF pig herds are probably nearly free from _Salmonella, Trichinella,_ _T. gondii,_ and, according to our findings, even human pathogenic _Y. enterocolitica_ . Freedom from _Campylobacter_ spp. in pigs is probably impossible. However, blast chilling after the slaughtering process seems to reduce the number of _Campylobacter_ spp. ≈100% (Nesbakken et al. unpub. data). Thus, in the future, pork from Norwegian SPF pig herds and even mixed herds in closed breeding pyramids might be marketed as HPF.\n\n【22】Another aspect to consider is the environment. Usually manure from pig farms is spread in fields and may contaminate wild animals, lakes, and rivers. Drinking water may thereby be contaminated with pathogenic _Y. enterocolitica._ This contamination has a human health aspect because one of the risk factors for human yersiniosis might be drinking water that has not been disinfected . Thus, in addition to their public health benefits, human pathogenic _Y. enterocolitica–_ free herds might have a positive environmental effect.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "49735e99-8e02-4f4c-ae99-04860f4a3d73", "title": "Reassortment of Human Rotavirus Gene Segments into G11 Rotavirus Strains", "text": "【0】Reassortment of Human Rotavirus Gene Segments into G11 Rotavirus Strains\nGroup A rotaviruses are the most frequently detected viral cause of diarrhea in children worldwide and cause ≈600,000 deaths in children <5 years of age annually, mainly in developing countries . Rotaviruses have a genome composed of 11 segments of double-stranded RNA that encodes 6 structural (VP) and 5 or 6 nonstructural (NSP) proteins . The 2 outer capsid proteins VP7 and VP4 are the basis for a widely used dual classification system defining G-types and P-types, respectively. Currently 23 G-genotypes and 31 P-genotypes have been described, of which 12 of each type have been found in human rotavirus isolates . However, only a limited number of G/P-genotype combinations are found frequently in humans, such as G1P, G2P, G3P, G4P, and G9P, and, more recently, the G12 genotype in combination with P or P . It has been hypothesized that the G9 and G12 genotypes have been able to successfully infect, spread, and persist in humans because of reassortment events with human Wa-like rotavirus strains, which has resulted in the G9 and G12 rotaviruses combining with P and the 9 remaining gene segments belonging to genotype 1 .\n\n【1】A nucleotide sequence–based classification and nomenclature system encompassing the 11 rotavirus genome segments has been introduced recently, and it defines genotypes for each of the 11 gene segments . This system has been useful in investigating reassortment events and interspecies transmission of rotaviruses and their interhost relationships . In this system, the VP1–VP3, VP6, and NSP1–NSP5 genotypes comprising Wa-like strains have been designated as genotype 1 (R1, C1, M1, I1, A1, N1, T1, E1, and H1, respectively), and DS-1 and AU-1-like strains have been designated as genotypes 2 and 3, respectively . A Rotavirus Classification Working Group (RCWG) was formed to maintain and update this system and to assign successive genotype numbers to newly discovered rotavirus genotypes .\n\n【2】G11 rotaviruses are believed to be circulating in pigs, albeit in low numbers. Only 2 G11P porcine strains have been isolated, strain YM in Mexico in 1983 and strain A253 in Venezuela in 1989 . Each of these porcine strains was identified as a single isolate in large strain collections obtained during epidemiologic surveys. In subsequent years, no additional G11 strains were detected in the same or nearby pig farms. However, more than a decade after the detection of these 2 porcine G11 strains, several reports have described the isolation of G11 rotavirus strains from humans. Three G11 rotaviruses, Dhaka6, KTM368, and CRI 10795, have been found in combination with the rare human genotype P in India , Bangladesh , and Nepal . Furthermore, G11 human rotaviruses have been found in combination with P (Matlab36–02 and Matlab22–01), with P in Ecuador (EC2184)  and Bangladesh (Dhaka13–06)  and with P (CUK-1) in South Korea .\n\n【3】It was recently suggested that human rotavirus strains belonging to the Wa-like genogroup and porcine rotaviruses have a common origin . This report prompted us to investigate the level of genetic relatedness, and possibly evolutionary origins, between these unusual human G11 rotavirus strains and porcine rotavirus strains by complete genome analyses of 2 human G11P strains (KTM368 and Dhaka6), 2 human G11P strains (Matlab36–02 and Dhaka22–01), and 3 cell culture–adapted porcine strains YM (G11P), Gottfried (G4P), and OSU (G5P).\n\n【4】### Methods\n\n【5】##### Strain Collection\n\n【6】Strain KTM368 (G11P) was isolated in Nepal in 2004 , and rotavirus strains Dhaka6 (G11P), Dhaka22–01 (G11P), and Matlab36–02 (G11P) were isolated in Bangladesh in 2001, 2001, and 2002, respectively . Double-stranded RNA of tissue culture–adapted reference porcine strains YM (G11P), isolated in Mexico in 1983 , and OSU (G5P) and Gottfried (G4P) isolated in the United States in 1976 , was used in our study.\n\n【7】##### RNA Extraction and Reverse Transcription–PCR\n\n【8】Virus RNA was extracted by using a QIAamp Viral RNA Mini Kit (QIAGEN, Leusden, the Netherlands) according to the manufacturer’s instructions. Extracted RNA was denatured at 97°C for 5 min, and reverse transcription–PCR was performed by using the OneStep RT-PCR Kit (QIAGEN). Forward and reverse primers used for amplification of different gene segments were synthesized on the basis of alignments of known 5′ and 3′ sequences of respective gene segments found in GenBank (primers are available upon request from J.M.). PCRs were performed by using an initial reverse transcription step at 50°C for 30 min, followed by PCR activation at 95°C for 15 min, 40 cycles of amplification, and a final extension at 72°C for 10 min in a BiometraT3000 Thermocycler (Biometra, Westburg, the Netherlands). Cycle conditions for the amplification of VP1, VP2, VP3, and VP4 were 30 s at 94°C, 30 s at 50°C, and 6 min at 70°C. For other gene segments, conditions were 30 s at 94°C, 30 s at 45°C, and 3 min at 72°C.\n\n【9】##### Nucleotide Sequencing\n\n【10】PCR products were purified with the MSB Spin PCRapace Kit (Invitek, Berlin, Germany) and sequenced by using the dideoxy-nucleotide chain termination method with the ABI PRISM BigDye Terminator Cycle Sequencing Reaction Kit (Applied Biosystems, Foster City, CA, USA) in an ABI PRISM 3100 automated sequencer (Applied Biosystems). Complete 5′ and 3′ terminal nucleotide sequences of the 11 gene segments were determined by using a modified rapid amplification of cDNA ends technique as described .\n\n【11】##### Nucleotide and Protein Sequence Analysis\n\n【12】Chromatogram sequencing files were analyzed by using Chromas 2.3 (Technelysium, Helensvale, Queensland, Australia), and contigs were prepared by using SeqMan II (DNASTAR, Madison, WI, USA). Multiple sequence alignments were constructed in ClustalX 2  and subsequently edited in MEGA4 .\n\n【13】##### Phylogenetic Analysis\n\n【14】Phylogenetic and molecular evolutionary analyses were conducted by using MEGA4 . Genetic distances were calculated by using the Kimura-2 correction parameter at the nucleotide level, and phylogenetic trees were constructed by using the neighbor-joining method with 500 bootstrap replicates.\n\n【15】##### Assignment of Newly Identified Genotypes\n\n【16】Genotypes of each of the 11 genome segments for all the rotavirus strains under investigation were determined according to the genotyping recommendations of the RCWG by using the RotaC rotavirus genotyping tool . Because the sequence of the VP6 gene segment of strain KTM368 did not belong to any of the established VP6 I genotypes, it was submitted to the RCWG for appropriate genotype assignment. GenBank accession numbers for each of the gene segments of strains Dhaka6, KTM368, Dhaka22–01, Matlab36–02, YM, OSU, and Gottfried are shown in Technical Appendix 1 .\n\n【17】### Results\n\n【18】Complete genome sequences of 3 human rotavirus strains (Dhaka6, KTM368, and Matlab36–02) were determined. For strain Dhaka22–01, only short nucleotide sequences ranging from 199 to 962 nt per segment could be determined because of insufficient sample. For porcine rotavirus strains YM, OSU, and Gottfried, variable amounts of gene sequences have been reported. We sequenced the remaining porcine rotavirus gene segments .\n\n【19】##### Genotyping\n\n【20】According to guidelines of the RCWG, all gene segments belonged to established genotypes, except for the VP6 gene segment of KTM368 . This sequence was submitted to the RCWG, accepted as a new VP6 genotype, and designated I12. Complete genotype assignments of the 6 strains fully sequenced and reference strain Wa are shown in Table 1 . All 3 human G11 strains have gene segments belonging to genotype 1 for the gene segments encoding VP1–VP3 and NSP1–NSP5. The VP4-encoding gene segment of human strains KTM368 and Dhaka6 had the unusual P genotype, and human strains Matlab36–02 and Dhaka22–01 had the typical human P genotype. With regard to VP6-encoding gene segments, only the human strain KTM368 from Nepal had the I12 genotype, and other human rotavirus strains isolated in Bangladesh had the typical human Wa-like VP6 genotype I1. The 3 porcine rotavirus strains YM, Gottfried, and OSU all had genotype 1 gene segments for VP1–VP3 and NSP2–NSP5 and the expected different G (G11, G4, and G5) and P (P and P) genotypes. The VP6 and NSP1 genotypes were either I1 or I5 and A1 or A8, respectively .\n\n【21】For several gene segments of the partially sequenced strains Dhaka22–01, insufficient sequence data were available for a definitive classification according to the guidelines of the RCWG . However, when available sequence data of strain Dhaka22–01 were compared pairwise with those of the other human G11 strains, sequences of VP7, VP6, and VP4 of strain Dhaka22–01 were nearly identical with those of the strain Matlab36–02 (.8%, 99.0% and 98.4% identity at the nucleotide level, respectively) . In addition, for the VP1–VP2 and NSP1–NSP5 gene segments of strain Dhaka22–01, high identities (range 97.9%–100%) at the nucleotide level were found between strain Dhaka22–01 and the other human Wa-like G11 strains. For the VP3 gene segment, identities between strain Dhaka22–01 and strains KTM368, Dhaka6, and Matlab36–02 were low (range 86.3%–86.7%), and the VP3 gene segment of strain Dhaka22–01 was closely related to strains in the human M1 subcluster (.6% identity with strain Dhaka12–03). These findings suggest a typical human Wa-like origin for Dhaka22-01 .\n\n【22】##### Phylogenetic and Pairwise Identity Analyses\n\n【23】To study the relationships between human G11 and porcine rotavirus strains in greater detail, we constructed phylogenetic trees by using entire open reading frame nucleotide sequences for the 11 gene segments . For VP7, strains KTM368, Dhaka6, and Matlab36–02 cluster closely within the G11 genotype, together with the G11P strain CUK1 from South Korea; the porcine G11 rotavirus strains YM and A253 are more distantly related . For VP6, strain KTM368 (genotype I12) was only distantly related to strains belonging to genotype I1. Human strains Dhaka6 and Matlab36–02 cluster in a large I1 subcluster, which contains mainly human and a few porcine strains isolated in Bangladesh, Belgium, the United States, Thailand, India, Australia, and Japan . For VP4, strains KTM368 and Dhaka6 cluster closely in the rare P genotype, whereas strain Matlab36–02 is closely related to recently isolated P human strains from Bangladesh, Belgium, South Korea, and the Democratic Republic of the Congo .\n\n【24】In the phylogenetic trees of the remaining 8 gene segments (VP1–VP3 and NSP1–NSP5), at least 1 major human monophyletic subcluster could be distinguished within genotype 1. We also observed 1 major porcine genotype 1 subcluster, a finding that is consistent with the assumption that Wa-like human rotavirus strains and porcine rotaviruses have a common ancestor  . Several gene segments of human G11 strains cluster closely in human genotype 1 subcluster, but a few of them did not cluster closely with any known human or porcine rotavirus strains and formed a distinct (nonhuman, nonporcine) branch or subcluster within genotype 1.\n\n【25】For the VP2 and NSP1 gene segments, human strains KTM368, Dhaka6, and Matlab36–02 clustered closely within the genotype 1 (C1 and A1, respectively) human subcluster . Regarding the VP1, NSP2, NSP3, NSP4, and NSP5 gene segments, human rotavirus strains Dhaka6 and Matlab36–02 are localized within the human subcluster, whereas strain KTM368 formed a distinct branch inside genotype 1 and did not cluster with any known human or porcine rotavirus strain . For VP3, all 3 human G11 strains (KTM368, Dhaka6, and Matlab36–02) belonged to a distinct subcluster inside the VP3 M1 genotype .\n\n【26】##### Acquisition of Human Rotavirus Genes\n\n【27】The distinct genotype constellations  differed in their degree of relatedness to typical human Wa genogroup rotavirus strains. With the exception of the G11 genotype, human strain Dhaka22–01 is nearly indistinguishable from other locally or more distantly circulating human Wa-genogroup rotavirus strains, such as strains Dhaka16–03 (G1P) and Dhaka12–03 (G12P) from Bangladesh or strain B4633–03 (G12P) from Belgium, all isolated in 2003  . Human strain Matlab36–02 is closely related to strain Dhaka22–01, except for the VP3 gene , for which human strains KTM368, Dhaka6, and Matlab36–02 form a distinct subcluster inside the M1 genotype. This finding suggests a recent reassortment event. Strain Dhaka6 is closely related to strain Matlab36–02 and only differs in the VP4 genotype (P versus P), respectively . This finding also suggests a recent reassortment event. Strain KTM368 from Nepal has VP2 and NSP1 gene segments that belong to the typical human subcluster within the C1 and A1 genotypes, respectively. All other gene segments belong to non–Wa-like genotypes (VP7: G11, VP4: P, and VP6: I12) or to a distinct subcluster or branch inside genotype 1 with an unknown origin.\n\n【28】There are 2 possible hypotheses for the observed acquisition of human rotavirus genes by G11 strains. The first hypothesis is that different human G11 virus strains described in this study may have originated from several unrelated interspecies transmission events of animal G11 strains to humans, followed by reassortment events that involved Wa-like human strains. The second hypothesis is that a gradual acquisition of human rotavirus genes occurred after 1 interspecies transmission event, followed by multiple successive reassortment events. The second hypothesis is that a currently unknown ancestral rotavirus, of probable porcine origin and having the G11-P-I12 genotypes in a nonhuman Wa genogroup background, might have undergone multiple reassortment events with co-circulating human rotavirus strains, resulting in the different natural reassortant rotavirus strains described in this study. These reassortments resulted in a human G11P rotavirus composed entirely of typical human genotype 1 (Wa-like) RNA segments.\n\n【29】### Discussion\n\n【30】G11 rotaviruses are considered porcine rotaviruses because they were first isolated from pigs in Venezuela and Mexico in the 1980s . Although G11 porcine rotaviruses have been detected infrequently on pig farms , these viruses have been recently detected in humans in several locations (India, Bangladesh, Nepal, South Korea, and Ecuador) . Our data show that multiple reassortment events have occurred between porcine or human G11 rotaviruses and co-circulating human Wa-like rotavirus strains (all human G11 strains were isolated during 2001–2006). In addition to G11 strains described in this study, another G11P human rotavirus strain (CRI 10795) has been isolated in India, but only partial VP7, VP4, VP6, and NSP4 gene sequences of this strain are available . The CRI 10795 strain is yet another G11P human rotavirus variant with a VP6 gene of the human I1 genotype and an NSP4 gene of the nonhuman subcluster of the E1 genotype. This finding suggests that additional reassortments have occurred between G11 and Wa-like strains.\n\n【31】Because the few human G11 strains investigated most likely represent only a small part of a complex set of events, our primary hypothesis of a linear stepwise acquisition of human rotavirus genes through successive reassortment events, which result in a human G11 rotavirus with an entire human Wa-like genomic background, may be oversimplified. An alternative hypothesis is that rare (porcine?) G11-P-I12 progenitor strains are more widely spread and that genes of such viruses were introduced into the human population by interspecies transmissions, followed by multiple independent reassortment events with human rotavirus strains (G1P, G3P, G4P, G9P). These reassortments could have resulted in different numbers of porcine gene segments being transferred to human Wa-like rotaviruses, as described for the G11 strains used in this study. Reassortment of individual or small numbers of porcine rotavirus genes into human rotaviruses, which are well adapted to propagation in the human host, can result in a virus with a genetic makeup that is optimal for replication in the human host and spread in the human population. However, close phylogenetic clustering of most genes of different G11 strains  suggests that these strains have a recent common ancestor and that gradual acquisition of human rotavirus genes is a plausible hypothesis.\n\n【32】The 2 hypotheses are not mutually exclusive, and a combination of both cannot be ruled out or proven at this time. In addition to G11P and G11P human rotaviruses, G11 rotaviruses in combination with P and P genotypes have been isolated in Bangladesh  and South Korea . However, additional sequence data are not available for these strains, which are suggestive for additional reassortment events with P (DS-l like) and P genotype rotaviruses. A recent report describes another human G11P strain in Ecuador, which was found to be the likely result of reassortment between a typical porcine rotavirus and a human Wa-like rotavirus .\n\n【33】Early detection in ongoing surveillance programs and detailed analyses of G11 strains might provide unique insights into adaptation mechanisms of nonhuman rotaviruses to the human host through reassortment. G11 rotaviruses appear to be acquiring genotype 1 genome segments through multiple reassortment events (or Wa-like strains are acquiring genes of G11 rotavirus strains) in a short period. It will be useful to monitor whether new G11P human rotavirus strains, which carry mainly human Wa-like genes, will be as successful as G9 and G12 rotaviruses in finding a niche in the human population, and whether the currently licensed rotavirus vaccines will afford protection against rotavirus disease caused by G11P human rotavirus strains. Given that available rotavirus vaccines contain the virus P component, it is more likely that they will also protect humans against G9, G11, and G12 strains with the VP4 genotype P.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "853b1c46-19d1-433e-8178-077a3aab798d", "title": "Fluid Motion and Frozen Time", "text": "【0】Fluid Motion and Frozen Time\n**Thomas Red Owl Haukaas (−), **_More Time Expected_ , 2002.** Hand-made ink and pencil on antique ledger paper, 16.5 in × 27.5 in/41.9 cm × 69.9 cm. Tacoma Art Museum, 1701 Pacific Avenue, Tacoma, WA 98402, United States. Gift of Greg Kucera and Larry Yocoreem in honor of Rock Hushka.\n\n【1】In June 1981, five cases of _Pneumocystis_ pneumonia in gay men were described in CDC’s _Morbidity and Mortality Weekly Report_ . Those cases signaled the start of the AIDS pandemic, which now enters its fifth decade and has to date resulted in more than 75 million HIV infections and 32 million deaths worldwide. UNAIDS estimates that in 2019, 38 million persons were living with HIV, 1.7 million became newly infected, and 690,000 died with HIV disease.\n\n【2】Since the beginning of the HIV/AIDS pandemic, artists―some involved in AIDS activist organizations and others working independently―have applied their talents and skills to create, share, and deliver works that depict messages calling for political action and scientific research, documenting the impact of AIDS among various and diverse communities and groups, and celebrating medical breakthroughs and advances in treating AIDS. In 2016, a traveling exhibition entitled _Art AIDS America_ examined the ongoing influence and impact of the AIDS pandemic on American art.\n\n【3】Among the more than 125 works featured in that traveling exhibition was _More Time Expected_ by Lakota artist Thomas Red Owl Haukaas, displayed on this month’s cover. Haukaas conveys both a sense of fluid motion and frozen time in this image, a modern example of Native American ledger art (a genre of narrative drawing or painting on paper or cloth) that developed during the Indian Wars era and continued in the forced relocations of Plains tribes to government reservations from the 1860s through the 1920s. Widespread hunting had depleted buffalo and other game animals that provided hides the tribes traditionally used as canvases for recording events, ceremonies, and exploits. As a result, Native Americans began using paper taken from ledgers and other sources and employing ink, pencils, and watercolors rather than bone or wooden implements dipped in mineral and other natural pigments.\n\n【4】In this work, Haukaas shows a group of Native Americans and horses sweeping from right to left across the paper. The figures and horses are crowded, flattened, and overlapping each other. Riders and horses are looking straight ahead and moving in unison toward a destination beyond the edge of the image. The artist carefully depicts his figures of Native Americans dressed in traditional garb and wearing an array of bright colors and patterns. The horses are also stylized and individualized: many are boldly colored, and others are sketched with repeating patterns and rows of stripes. Half of the horses carry either a single rider or a pair of riders, but the other half are riderless, including the cobalt blue horse that draws attention to the center of the image.\n\n【5】Michelle Reynolds, Associate Director of Marketing and Communications at the Tacoma Art Museum, which organized _Art AIDS America_ in partnership with the Bronx Museum of the Arts, explains how this work is related to HIV/AIDS: “The imagery, specifically the riderless horse, explores the complicated issues of stigma surrounding HIV/AIDS and the Native American experience with the disease. Historically, instances of HIV on the reservation are virtually unmentioned, a silence that only worsens an already high rate of infection. The horse with no rider, often used as a symbol for a warrior who fell in battle, represents individuals on the reservation who have died of AIDS-related causes. By focusing on absence within a group, Haukaas plays to the importance of community and families within these settings.”\n\n【6】In a 2018 interview, Haukaas told writer and editor Emily Withnall, “My pieces are meant for dialogue, for discussion, for thinking about.” Despite the fluidity and motion of his work, the underlying message carried by his portrayal of these riderless horses is the palpable sense of what has been lost. Haukaas, whose works are featured in many museum collections and have been part of numerous exhibitions, does not have formal training in art, and he credits his family and friends for teaching him traditional skills and practices. Known for his talents as a ledger, beadwork, and doll artist, he trained as a psychiatrist.\n\n【7】Through this image, Haukaas reconnects with the traditional ledger art form and uses it as a platform to engender thought and discussion about the ongoing medical and social effects of HIV within reservation communities, throughout other marginalized racial and ethnic communities around the world, and in those regions most affected by this ongoing pandemic. As De Cock, Jaffe, and Curran state in their EID article Reflections on 40 Years of AIDS, “Although initially slow, the HIV/AIDS response over the years has been a beacon in global health for respect for individuals and their rights and for health equity. More reflection is required with regard to what the responses to HIV and Ebola have taught us and how they might be relevant to COVID-19 and other future epidemics.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4d160f96-ea42-4149-be87-92dee22e2d23", "title": "Role of Food Insecurity in Outbreak of Anthrax Infections among Humans and Hippopotamuses Living in a Game Reserve Area, Rural Zambia", "text": "【0】Role of Food Insecurity in Outbreak of Anthrax Infections among Humans and Hippopotamuses Living in a Game Reserve Area, Rural Zambia\nDuring August–September 2011, a total of 85 hippopotamuses (Hippopotamus amphibious_ ) died of suspected anthrax (Bacillus anthracis_ ) infection in a game management area along the South Luangwa River near the district of Chama in northeastern Zambia  . At least 521 suspected human anthrax cases and 5 deaths were reported near this area during this period. Residents of the area near the river had reportedly found dead hippopotamuses and subsequently butchered, cooked, and consumed meat from the dead animals, which was thought to be the cause of the human outbreak. As previously reported, most human cases of anthrax infection were cutaneous infections, with most patients initially having papular lesions (%) and the rest having lymphadenopathy and gastrointestinal symptoms . Most cases resolved after the patient received a course of oral ciprofloxacin. Some additional animal species were reported by local wildlife staff to have been affected, but no empiric data were found to corroborate those reports.\n\n【1】Anthrax outbreaks associated with animals are common and reported worldwide. Herbivores are thought to have onset of disease after ingesting spores in soil, water, or on vegetation. Reports of anthrax outbreaks occurring in wild and domestic animals in Africa have usually been associated with the dry season and have stopped with the onset of the rainy season . Outbreaks can begin with wildlife, expand into domestic livestock, and ultimately affect humans . When anthrax outbreaks occur in national parks in Africa and are limited to the wildlife, the outbreak is usually allowed to run its natural course without any human intervention. Multiple challenges make it impractical to vaccinate free-ranging wildlife populations; sometimes vaccine programs are initiated but usually only to protect endangered species or populations at high risk .\n\n【2】Anthrax outbreaks associated with hippopotamuses have been reported previously in Zambia, Uganda, Zimbabwe, and South Africa . Human cases associated with wildlife outbreaks in Africa are generally not well-documented but are known to occur .\n\n【3】Zambia’s Chama District (population 103,894) borders Malawi, in what is currently known as Muchinga Province  . Nearly 93% of the district’s residents live in rural areas, and overall population density is 5.9 persons/km 2  . The rural population resides in small villages largely accessible only by all-terrain vehicles. Communication in many parts of the district is only possible through 2-way radios. Chama is within a game management area that includes the South Luangwa River and contains rich flora and fauna, including hippopotamuses but also other foragers and predators. Because of Chama’s status as a game management area, residents are not permitted to protect crops from foragers or hunt on area grounds, which are overseen by the Zambia Wildlife Authority (ZAWA). Food and water are scarce for animals and humans during Zambia’s dry season, generally May–November. A delay in the annual rainy season, usually December–March, can put farmers at risk for low crop production, as was the case in 2011 . During this period, animals forage deep into riverbeds in search of water and food, digging up and activating dormant anthrax spores. Residents, for whom food can also be scarce under these conditions, have been known to consume animals they find dead in their area .\n\n【4】In September 2011, a few months after the first human anthrax cases were reported in the district, a team of epidemiologists, health officers, and environmental health technicians from the Zambia Central Ministry of Health, the Eastern Provincial Health Office, and Chama District Health Office conducted a preliminary investigation of the possible outbreak under their jurisdiction. Shortly thereafter, upon formal request, epidemiologists from the US Centers for Disease Control and Prevention joined the Zambia team to conduct this outbreak investigation in an effort to further inform prevention activities.\n\n【5】Specifically, we aimed to shorten the outbreak by identifying and eliminating any remaining exposures and by recommending mitigation and prevention strategies for this and future outbreaks. We also wanted to determine the riskiest exposures so that educational messages could be properly tailored. Here we report the results of a household survey used to identify the most common exposures associated with human anthrax and to determine how food insecurity contributed to consumption of the anthrax-contaminated meat and anthrax infection among residents of this game management area.\n\n【6】### Methods\n\n【7】We conducted a cross-sectional, interviewer-administered household survey in 3 villages with access to riverbeds where hippopotamuses died: Chikwa, Chigoma, and Chimpamaba. These villages’ estimated combined population of adults \\> 15 years of age is 6,553 . A questionnaire  was developed in English and translated into Senga, the local language spoken in these areas. The questionnaire was designed to complement and expand on the data initially collected in the district by local public health officials. It captured age, sex, and occupation and yes/no responses to several questions about symptoms, exposure to hippopotamuses, food sources, and whether respondents would eat meat from an animal they found dead. Those who responded that they would eat animals they found dead were asked an open-ended question to elucidate their reasons for doing so. Interviewers and supervisors were trained on procedures for conducting the survey before fieldwork commenced. Interviewers and supervisors tested questions on each other, which resulted in improvements to question wording. They also practiced administering the survey on one another before interviewing community participants.\n\n【8】Three teams administered the questionnaire, each team made up of a trained District Health Office staff supervisor and 6 trained volunteer interviewers. Each team was assigned to 1 of the 3 communities. Interviewers selected every fifth household they encountered in each village for inclusion and interviewed all adults \\> 15 years of age living in selected households. No incentives were offered for participation. Interviewers read each prospective participant a description of the survey, highlighting that it was voluntary and could be stopped at any time by request. The Zambia Ministry of Health deemed this outbreak investigation to be exempt from Research Ethics Committee review. The investigation protocol was reviewed by the CDC-Zambia office and CDC’s National Center for Emerging and Zoonotic Infectious Diseases, in accordance with institutional review policies. The protocol was determined to be nonresearch under 45 CFR 46 §102(d) and therefore did not require Institutional Review Board review.\n\n【9】Data from completed questionnaires were entered into an Access database (Microsoft, Redmond, WA, USA) and analyzed by using Epi Info version 3.5.1 (CDC, Atlanta, GA, USA). We defined a case as illness in a respondent who reported having had anthrax infection diagnosed by a healthcare worker since July 2011 and compared demographic characteristics and risk factors by case status by using χ 2  and _t_ tests. Associations were quantified with simple and multivariable logistic regression.\n\n【10】The second part of the investigation involved going into the field to view 3 of the areas where most of the infected hippopotamuses were found. The site visits occurred within about a month after the outbreak and were conducted to better understand the topography and the range of the animals and to identify any additional animal species affected by anthrax. All sites were areas frequented by the exposed human populations and were <1 km from residents’ homes. The site surveys were not exhaustive; they focused on areas with known human-animal interaction  and recently discovered dead hippopotamuses . N95 masks, Tyvek suits, and rubber boots were worn during site visits when members exited vehicles. We used these observations and the results of the household-level surveys to develop recommendations to mitigate and control future spread of the infection to humans, animals, and the environment.\n\n【11】### Results\n\n【12】All 284 household members (≈4% of the population of the villages) in the 87 households selected agreed to be interviewed (mean 3 participants per household). In total, 31 (%) of participants reported having anthrax infection diagnosed by a healthcare worker since July 2011; another 137 (%) reported not having anthrax infection diagnosed, and 116 (%) did not know whether they had anthrax infection diagnosed. We assumed that those who did not know did not have anthrax infection diagnosed. Male respondents accounted for 48% (n = 136) of total participants but 68% (n = 21) of those reporting having anthrax infection diagnosed, compared with 36% of cases occurring among female respondents (p<0.001) . Of the 96 persons who answered the occupation-related question, 91 (%) listed their occupation as farmer. This finding was not surprising given the rural setting of the outbreak. Because the responses for occupation were so homogenous, occupation was not evaluated as a risk factor. The median age of persons having received an anthrax diagnosis since July 2011 was 33 years, similar to the median age of all participants (years) . The most common signs and symptoms reported by those reporting having been diagnosed with anthrax included myalgia, skin lesions, fatigue, diarrhea, and fever .\n\n【13】Most participants ([84%\\]) reported having eaten hippopotamus meat at the time of the outbreak. Participants who ate the meat were 9 times (% CI 1.3–369.3 times) more likely to report having had anthrax than those who did not eat the meat. Carrying hippopotamus meat (odds ratio 5.3, 95% CI 2.0–15.4) and preparing it for cooking (odds ratio 3.3, 95% CI 1.1–13.7) were also significantly associated with anthrax infection. After controlling for having eaten the hippopotamus meat, 3 activities (skinning, carrying, and cutting the meat) were all still significantly associated with reported anthrax infection .\n\n【14】Most people surveyed ([76%\\]) reported they would not eat meat from a dead hippopotamus knowing now that it can cause anthrax infection, but 65 (%) of all respondents and 5 (%) of the 31 respondents in whom cases were reported said that they would eat meat from a dead hippopotamus despite this knowledge. Of the 65 participants saying they would eat the meat if given the chance again, reasons given were because they lacked other options for a side dish (“relish”) to Nshima, the maize-based staple food ([73%\\]); lacked meat ([22%\\]); suffered from hunger ([7%\\]); or lacked protein ([5%\\]), in addition to other less commonly reported reasons .\n\n【15】The investigation team also visited several field sites. At 1 of the sites, previous human interaction with dead hippopotamuses was evident. Bones and hides were strewn across a large area. Evidence of multiple campfires were found in the vicinity of the hippopotamus remains . According to a Zambia Ministry of Health official and others on the investigation team who had visited the site earlier, the strewn animal parts appeared to have been from the initial human contact with the dead hippopotamuses. Residents appeared to stop handling dead hippopotamuses after human anthrax cases were detected and linked to contact with hippopotamus carcasses.\n\n【16】### Discussion\n\n【17】A large outbreak of cutaneous anthrax among humans in the Chama District of Zambia was associated with physical contact with meat from hippopotamuses that had died of anthrax, specifically skinning, carrying, or cutting the meat. Food insecurity was thought to have been the major factor driving the local population to consume meat from dead animals.\n\n【18】Large outbreaks affecting hippopotamus herds occurred within the Luangwa River Valley during 1987–1988 . Although no human infections were reported in relation to these outbreaks, positive results with low antibody titers against _B. anthracis_ were obtained during a 1989 follow-up study from half of the subjects in a small sample of unvaccinated healthy volunteers from Luangwa River Valley villages, suggesting previous exposures in those persons through handling or consumption of meat from anthrax-infected animal carcasses .\n\n【19】Our findings are subject to a few limitations. In the household surveys, we used self-report of anthrax diagnosis to define a case; however, 41% of participants indicated they did not know whether they had received an anthrax diagnosis. We assumed that these respondents might not have understood the question and had probably not had anthrax diagnosed. Although this assumption is a limitation, it would probably bias our associations toward the null. We also had to use self-report of diagnosis rather than laboratory confirmation. However, separately some hippopotamus and human samples were confirmed in the laboratory as positive for _B. anthracis_ , which does strengthen the epidemiologic linkage . Finally, slight discrepancies can be noted in the number of cases and hippopotamus deaths in this and the 2 other reports describing other aspects of the outbreak and response that have been published; specifically, the numbers of human cases vary from 511 to 521, and the numbers of hippopotamus deaths vary from 81 to 85 . This discrepancy likely illustrates the difficulty in describing events in very remote areas.\n\n【20】From this investigation we found that the greatest risk for having anthrax diagnosed came from carrying, skinning, or butchering hippopotamuses. This finding is consistent with other anthrax outbreaks associated with contaminated meat .\n\n【21】Recommendations from this investigation built on the initial response of the Zambia Ministry of Health and included community education, enhanced surveillance in human and animal populations, and resolution of food insecurities by working with governmental and nongovernmental agencies. The message to not eat meat from animals found dead was communicated at the time of the initial investigation. On the basis of survey responses indicating persons were no longer touching the meat and that carcasses were no longer being butchered, we think the messages were received and understood by the communities affected. Because the handling of the carcasses proved to be the most important risk factor for anthrax infection, future education campaigns should also focus on avoiding handling animals that have died of unknown causes. In rural Zimbabwe communities where anthrax awareness was high (.5%) in a 2013 survey, 41% of persons surveyed reported “forgetting about anthrax,” a major reason for consuming meat from anthrax-infected animals . Residents should be reminded by community-based awareness campaigns or other means of the hazards of consuming meat from animals that have died of unknown causes .\n\n【22】To inform planning, wildlife authorities should identify high-risk periods and locations for naturally occurring animal outbreaks through ecologic studies that identify conditions favoring anthrax infection among animal populations . Wildlife and public health authorities should work together to ensure that community-based campaigns proactively prepare communities for possible outbreaks according to their risk profile . Community-based interventions should involve residents in addressing communitywide food insecurity and in educating neighbors on the hazards of consuming meat from animals that die of unknown causes . These interventions should begin before the dry season in outbreak-prone areas.\n\n【23】Questionnaire responses showed that food insecurities appear to be the primary reason for handling and consuming meat from animals found dead. Other countries in Africa have undertaken successful programs to distribute meat from trophy animals to feed communities with limited access to protein while also reducing poaching by local communities . Such an approach might be considered as a component of a multisectoral solution to address food insecurity and consumption of unsafe foods in Zambia.\n\n【24】Overall, food insecurity throughout sub-Saharan Africa has improved throughout recent years; however, hunger and malnutrition continue to be concerns in many sub-Saharan countries including Zambia. Zambia maintains a food reserve of maize, and it was suggested throughout the investigation that officials should provide additional corn meal as a possible solution for the food shortage. However, populations at risk for food insecurity need better access to balanced diets rather than more carbohydrates . Our survey respondents highlighted the desire for more fresh fruits and vegetables, which suggests more balanced diets would be welcomed.\n\n【25】Most of the crops grown in this region were cotton and other nonedible, exportable crops. Assistance is needed to help the population better balance subsistence farming with cash crops on small family farms to improve the overall diversity of crops and ultimately mitigate the risk for food insecurity .\n\n【26】Our household survey aimed to determine the main risk factors for anthrax transmission and the underlying factors driving those infected to risk exposure. Our results suggest the need to address long-standing political and economic issues related to food insecurity in protected areas, as well as an urgent need for better coordination between wildlife management and public health authorities. A more proactive approach could help prevent future outbreaks.\n\n【27】At the time of this investigation, Dr. Lehman was an Epidemic Intelligence Officer assigned to the Bacterial Special Pathogens Branch, Division of High-Consequence Pathogens and Pathology, National Center for Emerging and Zoonotic Infectious Diseases, Centers for Disease Control and Prevention, Atlanta, Georgia, USA. He is currently a public health officer in the US Air Force, assigned to the US Air Force School of Aerospace Medicine, Wright-Patterson Air Force Base, Ohio, USA. His primary research interests include food protection, food security, and international public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c18a79c3-62c2-407a-943f-9e6f27b17c71", "title": "Full-Genome Deep Sequencing and Phylogenetic Analysis of Novel Human Betacoronavirus", "text": "【0】Full-Genome Deep Sequencing and Phylogenetic Analysis of Novel Human Betacoronavirus\nThe ability of coronaviruses (CoVs) to infect multiple species and to rapidly change through recombination presents a continuing human health threat. The epidemic of severe acute respiratory syndrome (SARS) during 2003–2004 during which a CoV transmitted from bats to civet cats and then to humans demonstrated this potential (reviewed in ). Recently, a novel human betacoronavirus (betaCoV) was found to be associated with at least 13 human infections, 7 of which were fatal . One of the viruses, EMC/2012, has been sequenced, and its sequence similarity to several bat CoVs suggested an animal origin, but a definitive bat species of origin has not yet been identified . Additional genome sequences from this virus are needed to aid diagnostics, monitor population dynamics, identify the animal source, and characterize mechanisms of pathogenesis. The large size (nt) and high variability of CoV RNA genomes present a challenge for sequencing.\n\n【1】We describe a strategy for rapidly designing the primers necessary for reverse transcription and cDNA amplification of such diverse RNA viruses and report the full-genome determination of the novel CoV directly from patient sputum using next-generation short-read sequencing. Full genomes from 2 epidemiologically unlinked novel CoV infections separated in time by >2 months were analyzed to gain clues to 2 major questions: what are the precursors of the virus, and how long has the virus been circulating in its current form?\n\n【2】### Materials and Methods\n\n【3】##### Primer Design\n\n【4】Fifteen betaCoV full genomes with the closest homology to EMC/2012  were analyzed to identify all possible primer-like sequences (melting temperature 58°C–60°C, guanine plus cytosine content 35%–60%) yielding 357,198 potential primers. The 30-kb JX869059 CoV genome was divided into fifteen 2.5-kb overlapping amplicons, and the 2 highest frequency sequences mapping in the 5′ and 3′ 250 bp of each amplicon were selected. Reverse complements of the 3′ mapping sequences were prepared, resulting in a set of 60 primers for the 15 amplicons (primers/amplicon). Three additional primers were added for the extreme termini of the genome. The algorithm also prepared a virtual PCR map of the predicted binding and amplicon size for the primer set on CoV genomes. A map of the primer mapping positions and the predicted PCR products using EMC/2012 as the target is shown in Figure 1 , panel A. The primer sequences and details of their use are listed in the Technical Appendix Table.\n\n【5】##### Sample History\n\n【6】The patient, originally from Qatar, visited Saudi Arabia, where he became unwell with severe respiratory disease and renal failure and was transferred to a hospital in London, UK . Viral RNA was extracted from a sputum sample collected on September 19, 2012, sixteen days after symptom onset. Real-time PCR was performed by using an assay targeting the _upE_ gene  and confirmed by using primers and probe targeting the RNA-dependent RNA polymerase (RdRp_ ) gene. RdRpTaq1: (′-GAC TAA TCG CCA GTA CCA TCA G-3′), RdRpTaq2: (′-GAA CTT TGT AGT ACC AAT GAC GCA-3′), RdRpProbe: (′-FAM-ATG CTT AAG TCC ATG GCT GCA ACT CGT GGA G–BHQ-3′). The assays gave cycle threshold values of 17.83–18.01 for the _upE_ and _RdRp_ targets. A 100-μL sample of sputum was lysed with the addition of an equal volume of Qiagen Lysis Buffer AL (QIAGEN, Hilden. Germany). RNA was extracted from 200 µL of this material (final elution volume 60 µL) by using the EZ1 Virus Mini Kit v2.0 and EZ1 instrument (QIAGEN).\n\n【7】##### Reverse Transcription and PCR Amplification\n\n【8】Reverse transcription was performed at 50°C for 60 min by using the amplicon reverse primers. PCRs were then performed with forward and reverse primers for each amplicon (reactions) for 35 cycles (°C, 10 sec; 54.5°C, 30 sec; 72°C, 2.5 min) with a final 10-min 72°C extension. A 3-µL aliquot of each reaction was analyzed by electrophoresis, and each reaction showed the expected 2–2.5-kb products . A detailed protocol is available on request from the authors.\n\n【9】##### Sequencing\n\n【10】The PCR products of amplicons were combined into 4 pools of approximately equal molarity and processed into standard Illumina multiplex libraries (Illumina, San Diego, CA, USA) with each pool bearing a unique bar code sequence. Libraries were sequenced by Illumina MiSeq generating 150-bp paired end reads. The resulting Illumina read sets were processed to remove adaptor and primer sequences and quality controlled to ensure a median read Phred quality score of 30 by using QUASR . The 5′- and 3′-most primers were derived directly from the first 27 nt or last 31 nt of the EMC/2012 genome. These primer sequences were not trimmed from the reads, which might mask sequence difference between EMC/2012 and England/Qatar/2012 at these positions.\n\n【11】##### Genome Assembly\n\n【12】Reference-based mapping and de novo assembly methods were applied to the data for assembly into viral genomes. Reference-based mapping was performed against the EMC/2012 genome by using the Burrows-Wheeler Aligner software package ). For de novo assembly, maximum contig lengths were obtained by using subsets of 30,000–60,000 reads. Therefore, random subsets of reads were extracted from the readset and assembled by using Velvet version 1.2.07  and VelvetOptimiser . The de novo assembled sequences were used to confirm the validity of the reference-based sequence and showed that the de novo assembly and the reference-based mapping produced identical sequences, apart from several small gaps near the termini of the de novo assembled sequence (results not shown). The complete genome sequenced here is named England/Qatar/2012 and is available in GenBank .\n\n【13】##### Sequence Alignment\n\n【14】The complete England/Qatar/2012 genome was combined with 46 previously published complete genomes of the α-(group 1), β-(group 2), γ-(group 3), and δ-(group 4) CoVs. The betaCoV complete genomes encompassed the a, b, c, and d lineages, as well as the 2 genomes of the novel human betaCoV, EMC/2012  and England1 , the latter from the same patient as studied here. The sequences were aligned by using MUSCLE  within MEGA5 . Poorly aligned regions and the genes absent in some CoV genotypes were excluded from the alignment, resulting in a virtual concatenation of the open reading frame (ORF) 1ab, S, E, M, and N genes. A second alignment using a 396-bp region of the _RdRp_ was generated similarly. This shorter alignment included strains that are genetically close to England/Qatar/2012 but lack complete genome sequences in GenBank.\n\n【15】##### Phylogenetic Methods\n\n【16】Maximum-likelihood (ML) phylogenetic trees were inferred from the sequence alignments by using PhyML version 3.0 . Phylogenies inferred from nucleotide and amino acid sequences used the general-time reversible and Whelan-and-Goldman substitution models, respectively. A discrete-Γ distribution of 4 rate categories (Γ 4  ) was used to model among-site heterogeneity. The robustness of the tree topology was assessed by bootstrap analysis of 1,000 pseudo-replicates of the sequences.\n\n【17】##### Molecular Clock Dating\n\n【18】The evolutionary time scale of 2 novel human betaCoVs, EMC/2012 and England/Qatar/2012, was estimated by using a strict clock model under the Bayesian Markov Chain Monte Carlo framework in BEAST version 1.7.4 . The Hasegawa-Kishino-Yano nucleotide substitution model was used with a discrete-Γ distribution of 4 rate categories (Γ 4  ) to enable among-site heterogeneity. A simple constant coalescent prior on the age of the divergence was used. With only 2 samples, estimating a rate of evolution was not possible, so we conditioned our date estimates on a range of fixed rates from 1.0 × 10 −4  to 5.0 × 10 −3  substitutions/site/year to enable comparison with plausible values. For each fixed rate value, a total of 10 7  Bayesian Markov Chain Monte Carlo states were computed, with the first 10% discarded as burn-in.\n\n【19】### Results\n\n【20】##### Comparison of England1, England/Qatar/2012, and EMC/2012\n\n【21】The index genome for this virus, EMC/2012, was originally obtained from a Saudi Arabian patient in July 2012 after 6 passages in cell culture . Our England/Qatar/2012 sequence was derived directly from the sputum of a Qatari patient receiving care in London in September 2012. A consensus genome from the same Qatari patient, but from a lower respiratory tract sample obtained later in the infection, is also available . The 3 genomes were aligned to determine sequence differences.\n\n【22】The England/Qatar/2012 and England1 genomes show only nucleotide differences at the genome termini, with the England1 genome lacking 46 nt at the 5′ end and 42 nt at the 3′ end . The England1 genome was generated by using 80 PCR products sequenced by Sanger dideoxy methods , and the high level of sequence identity between the England/Qatar/2012 and England1 sequences strongly validates the novel rapid sequencing methods described here.\n\n【23】The England/Qatar/2012 consensus genome has 30,021 (.67%) of 30,119 nucleotides identical to the EMC/2012 genome. In addition to the single nucleotide differences, EMC/2012 shows an insertion of 6 nt starting at position 29639 and a single A insertion at 30661 relative to the England/Qatar/2012 genome. The sequence differences are evenly spread across the genome .\n\n【24】Deep sequencing data enable the generation of a consensus sequence from the majority nucleotide at each genome position and the identification of nonconsensus nucleotides at each position. The England/Qatar/2012 sample was sequenced to a high level of coverage (mean coverage = 4,444) compared with the previously reported England1 genome (fold), enabling insights into the variation consequent on within-host evolution. Setting a conservative estimate for total sequencing errors at 1% enables nucleotide variants present at >1% frequency to be considered true variants in the virus genome . Variation is clearly detected across the virus genome with certain regions, such as nonstructural protein (NSP) NSP3, NSP5, NSP6, NSP12, and NSP14 showing increased levels of nonconsensus nucleotides .\n\n【25】The predicted ORFs in England/Qatar/2012 are shown in Figure 2 , panel C, Appendix. The ORF pattern is identical to EMC/2012, with 1 exception: England/Qatar/2012 has a G at position 27162 and a longer ORF 5, whereas EMC/2012 has an A (and the predicted ORF 5 is truncated). Van Boheemen et al. had commented on mixed nucleotides (A–G) observed at this position. In addition, EMC/2012 had variation at position 11623 (U or G), whereas the England/Qatar/2012 genome has a U at this position.\n\n【26】An assessment of specific ORFs of the 2 viral genomes was performed because evolution of specific proteins is a key determinant of host range and defines cross-species transmission events that may be recent for this virus. As expected from the overall close homology of the 2 viruses, several ORFs show little or no change between the 2 genomes . However, several ORFs show higher amino acid differences, including the nucleocapsid ORF N, and ORFs 3, 4a, and 8b .\n\n【27】Of the 16 predicted nonstructural proteins encoded by ORF 1a and ORF 1b, 11 show no change at the amino acid level , whereas 5 (NSP2, NSP3, NSP4, NSP13, and NSP15) show >0.3% difference . As more sequences become available from this virus, such comparisons will yield clues about the adaptation to humans.\n\n【28】##### Close Phylogenetic Relationship with European Bat CoVs\n\n【29】A ML phylogenetic tree inferred from the whole genome alignment indicated that the 3 novel human betaCoVs sequences (England1, England/Qatar/2012, and EMC/2012) clustered closely, forming a monophyletic lineage that falls into group 2c . This novel human betaCoV lineage shares common ancestries with other group 2c bat CoV variants, including HKU4 and HKU5 strains isolated in southern People’s Republic of China ; however, the genetic distances between them remain substantial (≈70% similarity in nucleotide level). The novel human betaCoVs have even less similarity with group 2a, 2b, and 2d CoVs (<60% nt). Phylogenies constructed from individual ORFs including ORF 1ab, S, E, M, and N demonstrated a largely consistent phylogenetic position of the novel human betaCoVs, except for a small discordance in the order of branching between novel human betaCoV, HKU4, and HKU5 lineages . Such phylogenetic incongruence could result from several possible evolutionary features of the virus, including evolutionary rate variation, homoplasy, and recombination, as well as from uncertainty in the alignment of distant sequences. Additional related sequences are needed to clarify this issue.\n\n【30】Because more CoV strains are available in GenBank for partial genomic sequences, we performed extensive BLAST searches  and identified several additional European bat CoV sequences sharing higher nucleotide sequence similarity (.0%–87.7%) with the novel human betaCoVs. Because only partial _RdRp_ sequences were available for these European bat CoVs, another ML phylogeny was constructed from this short region to examine their evolutionary relationships . As noted, P.pipi/VM314/2008/NLD  identified in a _Pipistrellus pipistrellus_ bat from the Netherlands  shows the closest sequence similarity (.7%) and phylogenetic relationship to the novel human betaCoVs. In addition, 2 CoV sequences, H.sav/J/Spain/2007  and E.isa/M/Spain/2007 (HQ184062; from _Eptesicus isabellinus_ ), obtained from bats from Spain  are also closely related to the novel human betaCoVs .\n\n【31】##### Timing the Origin of Zoonosis\n\n【32】The time to the most recent common ancestor (tMRCA) of EMC/2012 (isolated June 13, 2012) and England/Qatar/2012 (isolated September 19, 2012) was estimated with a strict molecular clock model. A plot of estimated tMRCA of the 2 human strains as a function of the fixed rate of molecular evolution shows that with the fastest measured rate of human CoVs (× 10 −3  substitutions/site/year for SARS-CoV ) the tMRCA (i.e. October 2011; 95% highest posterior density: August–December 2011) is older than the start of 2012, but with slower rates it is calculated to be much older . However, for evolutionary rates as high as those of human influenza A virus and HIV (e.g. 3–5 × 10 −3  substitutions/site/year), the estimated tMRCA becomes compatible with the earliest disease report in Jordan in April 2012 .\n\n【33】### Discussion\n\n【34】The reports of several human infections by similar strains of a novel betaCoV have raised global concern about a new SARS-like outbreak. Such worry is not misplaced, particularly when little is known about the origin and transmissibility of the virus and the natural history of the disease it causes. In this study, we presented a rapid deep sequencing method to obtain the complete genome sequence of the novel human betaCoV and its minor variants from a patient’s sputum sample, which provides a useful tool to study the origin, evolution, and detection of this novel CoV. Our detailed phylogenetic analyses of the viral genomes also provide additional clues to the emergence and evolution of this virus.\n\n【35】Mammalian zoonoses are often identified as the origin of new human CoV infections including OC43, NL63, and SARS-CoVs . In particular, bats, which maintain the largest diversity of CoVs , are thought to be a natural reservoir of the virus and thus a probable origin of the novel human betaCoV studied here. Indeed, the initial genetic study of the novel human betaCoV demonstrated its phylogenetic relatedness to the HKU5 and HKU4 bat CoVs in southern China . van Boheemen et al. found a bat CoV sequence from the Netherlands that clusters closely with EMC/2012 . In addition, most bat CoVs in the group 2c lineage, including the sequence from the Netherlands, were isolated from bat species of the Vespertilionidae family, whereas the SARS-like CoVs in group 2b lineage were mainly from bats from the Rhinolophidae family . In this study, we showed that 2 other CoVs from bats from Spain of Vespertilionidae again are also clustered near the novel human betaCoVs , which further supports a European Vespertilionidae bat ancestry for this virus. However, such a genealogic link may be indirect and distant because a similar hypothesis for SARS-CoV has been under dispute: whether the CoV jumped from bats directly to humans or through civets or even through some other animals as intermediate hosts . Therefore, like SARS-CoV, the high dissimilarity between the bat CoVs and novel human betaCoVs makes it difficult to rule out the presence of other intermediate hosts transferring the virus from bats to humans and to confirm the geographic origin of the direct predecessor. With the current data, Europe and the Middle East would be plausible regions for more intensive pilot surveillance studies on bats and other animal reservoirs for this virus.\n\n【36】In the interest of public health, it is critical to determine whether these CoV infections in humans are the consequence of a single zoonotic event followed by ongoing human-to-human transmissions or whether the 3 geographic sites of infection (Jordan, Saudi Arabia, and Qatar) represent independent transmissions from a common nonhuman reservoir. The large genetic diversity of CoV maintained in animal reservoirs suggests that viruses that independently moved to humans from animals at different times and places are likely to be reasonably dissimilar in their genomes, possibly making the multiple transmission events model less likely. Further information is needed to confirm this point because the currently available data are limited. If we calibrate our molecular clock analysis using the evolutionary rate of Zhao et al. estimated for SARS-CoV, we dated the tMRCA of EMC/2012 and England/Qatar/2012 viruses to early 2011. Therefore, if both sequenced viruses and the other cases descended from a single zoonotic event, then this tMRCA suggests that the novel virus has been circulating in human population for >1 year without detection and would suggest most infections were mild or asymptomatic. The rate would have to be considerably faster, of a magnitude observed for human influenza A virus, for the tMRCA to be compatible with the earliest known cases in April 2012. Perhaps more probable, therefore, is that the 13 known cases of this disease represent >1 independent zoonotic transmission from an unknown source. Viral sequence data from other patients infected with this novel human betaCoV will help to more accurately estimate the estimate a genomic evolutionary rate specific to this virus, which will then yield a tMRCA estimate closer to the actual time.\n\n【37】A major determinant of SARS pathology is the distribution of the host receptor that the virus has evolved to use for entry. A detailed analysis of EMC/2012 receptor use in cell culture  revealed that EMC/2012 was capable of infecting a range of mammalian cells, including human, pig, and bat cells, and the virus entry was independent of the ACE2 receptor. Comparing the S genes of England/Qatar/2012 predicts only 2 aa changes (in a 1,367-residue protein). Thus, this broad infection capability of EMC/2012 is likely to be valid for England/Qatar/2012 and would imply a higher possibility for the existence of other intermediate hosts transferring the virus from bats to humans.\n\n【38】Precise identification of the origin of this virus, defining its mode of evolution, and determining the mechanisms of viral pathogenesis will require full-genome sequences from all cases of human infection and substantially more sampling and sequencing from Vespertilionidae bats and other related animals. The sequencing method reported here markedly shortens the time required to process the clinical sample to genome assembly to 1 week and will provide a useful tool to study this novel virus.\n\n【39】The current number of confirmed novel CoV cases is 13, including 7 deaths . A third novel CoV genome sequence was posted on the Health Protection Agency website on February 18, 2013 . An analysis of the novel CoV genomes from the 3 dates suggests an evolution rate of 4 × 4e −4  , which would give a tMRCA of early 2009.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1a54002e-2e75-419d-880a-6c1179d2cc84", "title": "Behind the Mask: How the World Survived SARS, the First Epidemic of the 21st Century", "text": "【0】Behind the Mask: How the World Survived SARS, the First Epidemic of the 21st Century\nBehind the Mask: How the World Survived SARS, the First Epidemic of the Twenty-First Century recounts the outbreak of Severe Acute Respiratory Syndrome (SARS) that swept through much of the world, especially Asia, in 2003. The author does a superb job of telling the reader about what was occurring before SARS appeared, what happened during the outbreak, and what efforts are underway to prevent its return. The author has blended research results and interviews with frontline staff, particularly healthcare providers, into 20 nicely interlaced chapters. The stage for this commentary is a timeline of events, starting November 16, 2002, with the first known case of SARS in Guangdong Province, China, and ending in December 2003–January 2004 with 4 cases of SARS and the slaughtering of ≈10,000 civets in Guangdong Province. Where possible, the author avoided the use of medical terms and jargon and provides helpful lay translations where their use was unavoidable. As a result, the book is accessible to readers both inside and outside the healthcare arena.\n\n【1】The information presented in the book is, for the most part, current and accurate; different views and beliefs are presented when necessary. There are minor typographical mistakes as well as a few incorrect statements, such as in chapter 1, page 6, where the author refers to past public health efforts to eradicate viruses. The author states that smallpox virus and poliovirus have both been eradicated and that both are now bioterrorism agents. However, despite tremendous progress through efforts of many governments and public and private entities, poliovirus has yet to be eradicated and is not regarded as a bioterrorism agent at this time.\n\n【2】With regard to SARS, however, the author successfully portrays the human side of the outbreak response—a response heralded as unparalleled by many of the involved officials. Dr Carlo Urbani, the World Health Organization (WHO) physician in Vietnam who worked tirelessly and who was an eventual casualty of SARS, is among the many heroes who are featured in this book. Lesser-known facts, such as the thought processes that led to identifying SARS, are also provided. For example, before the SARS coronavirus was shown to be the causative agent, the outbreak was thought to be caused by either avian influenza or chlamydia. The reader is made aware of all of the challenges posed by the SARS virus, such as the delay in recognizing that there was an outbreak, the difficulties in diagnosing and reporting the disease and obtaining specimens, the breadth and scope of national and international collaboration and coordination, and not knowing the causative agent.\n\n【3】This book does a nice job of giving readers a flavor of the experiences faced by persons at WHO, persons at the country ministry level, individual healthcare providers, and SARS patients. I highly recommend this book, especially to anyone who was not directly involved in the SARS outbreak response; they too can share the experience of the global community response to a disease that was first recognized in 1 province of China.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3c6ac5d4-6836-46f3-988e-6b6b1d9d4fa3", "title": "Heterophyiasis", "text": "【0】Heterophyiasis\n_\\[Heterophyes heterophyes\\]_\n\n【1】*   Parasite Biology\n*   Image Gallery\n*   Laboratory Diagnosis\n*   Treatment Information\n\n【2】### Causal Agents\n\n【3】The trematode _Heterophyes heterophyes_ , a minute intestinal fluke.\n\n【4】### Life Cycle\n\n【5】Adults release embryonated eggs each with a fully-developed miracidium, and eggs are passed in the host’s feces  . After ingestion by a suitable snail (first intermediate host), the eggs hatch and release miracidia which penetrate the snail’s intestine  . Genera _Cerithidia_ and _Pironella_ are important snail hosts in Asia and the Middle East respectively. The miracidia undergo several developmental stages in the snail, i.e. sporocysts  , rediae  , and cercariae  . Many cercariae are produced from each redia. The cercariae are released from the snail  and encyst as metacercariae in the tissues of a suitable fresh/brackish water fish (second intermediate host)  . The definitive host becomes infected by ingesting undercooked or salted fish containing metacercariae  . After ingestion, the metacercariae excyst, attach to the mucosa of the small intestine  and mature into adults (measuring 1.0 to 1.7 mm by 0.3 to 0.4 mm)  . In addition to humans, various fish-eating mammals (e.g. cats and dogs) and birds can be infected by _Heterophyes heterophyes_  .\n\n【6】### Geographic Distribution\n\n【7】Egypt, the Middle East, and Far East.\n\n【8】### Clinical Presentation\n\n【9】The main symptoms are diarrhea and colicky abdominal pain. Migration of the eggs to the heart, resulting in potentially fatal myocardial and valvular damage, has been reported from the Philippines. Migration to other organs (e.g. brain) has also been reported.\n\n【10】##### Adult of _Heterophyes heterophyes_ .\n\n【11】Adults of _Heterophyes heterophyes_ are minute flukes, measuring 1-2 mm in length. The tests are large and paired, and are situated near a small ovary. The surface of the worm is covered with minute spines. Adults reside in the small intestine of the definitive host.\n\n【12】##### Snail intermediate hosts of _Heterophyes heterophyes_ .\n\n【13】Like all trematodes, _Heterophyes heterophyes_ requires a snail as an intermediate host. _Cerithideopsilla cingulata_ is the main first intermediate host for _H. heterophyes_ in southeast Asia.\n\n【14】### Laboratory Diagnosis\n\n【15】The diagnosis is based on the microscopic identification of eggs in the stool. However, the eggs are indistinguishable from those of _Metagonimus yokogawai_ and resemble those of _Clonorchis_ and _Opisthorchis_ .\n\n【16】### Treatment Information\n\n【17】For information about treatment please contact CDC-INFO .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3168d78f-9125-42d6-baec-d6b0ceef8338", "title": "Preventing Zoonotic Influenza Virus Infection", "text": "【0】Preventing Zoonotic Influenza Virus Infection\nIn the United States, influenza viruses are estimated to cause 36,000 human deaths and 200,000 hospitalizations annually . The current outbreaks of avian influenza in Asia and Eastern Europe remind us of the zoonotic potential of these viruses. Swine cells express sialic acids that can be receptors for swine, human, and avian influenza strains and facilitate cross-species influenza transmission and the genesis of novel influenza strains. Reported cases of human-to-swine and swine-to-human influenza transmission illustrate this potential .\n\n【1】Persons who work in enclosed livestock buildings (confinement workers) have among the highest risk of becoming infected with swine influenza virus. Their work involves close contact with many swine, including sick ones. The purpose of this cross-sectional study was to learn if these workers had evidence of previous swine influenza virus infection and, if so, to determine factors that cause them to be at increased risk.\n\n【2】### The Study\n\n【3】Iowa is the top swine-producing state in the United States and markets ≈25 million swine a year. From November 2004 to March 2005, we recruited confinement workers. Site selection was based on the authors' community contacts and opportunities to invite workers to participate. Local veterinary clinics advertised the study and permitted enrollment at their facilities. This study was approved by the University of Iowa's institutional review board.\n\n【4】Persons were eligible to participate in the study if they had worked in a swine confinement facility in the past 12 months. Participants completed a questionnaire and permitted blood sample collection on enrollment. The questionnaire captured demographic, medical, and occupational data including influenza immunization history, swine occupational exposures, and use of protective equipment (gloves and masks). Nonexposed controls were enrolled during a concurrent study of University of Iowa faculty, staff, and students .\n\n【5】Serum samples were studied by using a hemagglutination inhibition (HI) assay against 2 recently circulating swine strains, A/Swine/WI/238/97 (H1N1) and A/Swine/WI/R33F/01 (H1N2), and 1 human influenza virus strain, A/New Caledonia/20/99 (H1N1). The swine H1N1 strain represents a lineage of virus that has been circulating among US swine for 70 years. The swine H1N2 strain first appeared in US swine in 1999. HI titer results are reported as the reciprocal of the highest dilution of serum that inhibited virus-induced hemagglutination of a 0.65% solution of guinea pig erythrocytes for human influenza and 0.5% solution of turkey erythrocytes for swine influenza.\n\n【6】Specimen laboratory results were examined for their statistical association with demographic, immunization, occupational, and other behavioral risk factors. Confinement workers were queried about the nature of their work and whether they had used protective equipment. Because incidence of high titers was low or nonexistent in most groups, H1N1 titers >10 were grouped. The resulting categories were <10, 10, and >10. Wilcoxon rank sum and χ 2  statistic or 2-sided Fisher exact test were used to access bivariate risk factor associations. Depending on the nature of the data and modeling assumptions, proportional odds modeling or logistic regression was used to adjust for multiple risk factors. Final multivariate models were designed by using a saturated model and manual backwards elimination. Analyses were performed by using SAS version 9.1 (SAS Institute, Inc. Cary, NC, USA). Questionnaires were made available in both English and Spanish. Site selections were based on personal contacts in 3 completely different areas.\n\n【7】Forty-nine confinement workers and 79 nonexposed controls were enrolled in the study. The distribution of ages was similar for the 2 groups, but the confinement workers were more likely to be male and Hispanic and less likely to have received influenza vaccination .\n\n【8】Swine confinement workers were categorized by type of work, frequency of contact with swine, use of gloves, and use of masks. The question \"When working with sick or diseased swine, how often do you wear gloves?\" explained the most variation in swine H1N1 antibody titers and was included in the best fit model. Workers who sometimes or never used gloves were significantly more likely (odds ratio \\[OR\\] 30.3, 95% confidence interval \\[CI\\] 3.8–243.5) to have elevated titers than the nonexposed controls . These workers also were significantly more likely (OR 12.7, 95% CI 1.1–151.1) (data not shown) to have elevated titers than the other confinement workers who used gloves most of the time or always. Workers who reported smoking also had high OR (data not shown) for elevated titers.\n\n【9】Multivariate analysis also showed that persons who had received the 2003–04 influenza vaccine were significantly more likely to have elevated titers (>10) against swine H1N1 virus  as well as swine H1N2 (data not shown). Although cross-reaction with 1 of the viruses in the 2003–04 vaccine or a circulating influenza virus may explain this occurrence, higher titers would have been expected for all vaccinated persons (including controls), but such higher titers were not observed . We suggest that this result represents other behavior or health-related confounders not identified in the questionnaire for this study.\n\n【10】### Conclusions\n\n【11】These data suggest, like previous studies , that swine confinement workers are at increased risk for zoonotic influenza infection. However, our data are among the first to evaluate swine confinement workers, our sample size was small (not likely representative of all swine workers), and exposure data were self-reported. Confinement workers, in contrast to other swine occupations, are difficult to reach because of language barriers, on-farm policies regarding visitors (biosecurity protocols), and lack of trust in the public health sector.\n\n【12】Several studies have documented smoking as a risk factor for human influenza virus infection . However, we believe our data are the first evidence that smoking also increases the risk for swine influenza virus infections. We believe that this increased risk may be because the workers' oral mucosa are exposed to swine influenza virus after handling pigs.\n\n【13】This study's chief unique contribution is the evidence that use of gloves during swine confinement work noticeably decreases the risk for swine influenza virus infection. Thus, a simple personal protective measure might do much to reduce swine-to-human virus transmission. Future larger studies of swine confinement workers are needed to validate our findings and to better quantify risk factors for this population.\n\n【14】Individual behavior strongly influences influenza virus transmission . The national strategy for pandemic influenza highlights worker education and emphasizes individual responsibilities in preventing the spread of infection . Should a virulent, novel zoonotic influenza virus enter swine confinement facilities and spread among concentrated swine populations, the impact would be grave. Surveillance for zoonotic influenza virus therefore must be routinely conducted among agricultural workers. Also, use of personal protective equipment, frequent hand washing, and restrictions on smoking in or around swine facilities should be encouraged. Further, such workers should be included in state and federal pandemic plans as a high-risk group designated to receive annual influenza vaccines and antiviral drugs during pandemics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6a750713-4101-40b8-88b8-81444006eb21", "title": "Blastomyces gilchristii as Cause of Fatal Acute Respiratory Distress Syndrome", "text": "【0】Blastomyces gilchristii as Cause of Fatal Acute Respiratory Distress Syndrome\nDifferences in virulence have long been observed between different strains of _Blastomyces dermatitidis_ . Following the 2013 description of _B. gilchristii_ as a cryptic species of _Blastomyces_ spp. _B. dermatitidis_ and _B. gilchristii_ have been recognized as etiologic agents of blastomycosis . However, research evaluating the differences in clinical outcome between _B. dermatitidis_ and _B. gilchristii_ infection has been lacking. Here we demonstrate that _B. gilchristii_ infection can cause fatal acute respiratory distress syndrome (ARDS) in humans.\n\n【1】### The Study\n\n【2】A 27-year-old woman sought care at an emergency department in a rural community in northwestern Ontario, Canada, for a nonproductive cough, right-sided chest heaviness, nausea, and abdominal pain that had developed over the previous week. She had a previous diagnosis of type 1 diabetes mellitus and a history of intravenous drug use.\n\n【3】On examination, deep and labored breathing was observed, and decreased air entry in the left upper lung was heard on auscultation. A chest radiograph revealed left upper lobe consolidation . Blood chemical analysis revealed diabetic ketoacidosis (.4 mmol/L glucose, 3.2 mmol/L K+, 6.0 mmol/L HCO 3  −, anion gap 27 mg/dL, and pH 7.0 venous blood gas). Intravenous fluoroquinolone was administered for suspected left upper lobe bacterial pneumonia, and fluid resuscitation and intravenous insulin were administered for diabetic ketoacidosis.\n\n【4】During the first 2 days in hospital, the patient was stable and afebrile, despite remaining tachycardic and tachypneic. From day 3 to day 4, the patient became febrile (.3°C) and tachypnea worsened to the point that >4 L of O 2  was required to maintain O 2  saturation >90%. The patient’s leukocyte count was elevated at 13.5 × 10 9  cells/L.\n\n【5】On day 5, the patient was observed to be using accessory muscles of respiration. A chest radiograph was performed and showed a complete whiteout of the left lung . Air transfer to a tertiary hospital was requested but unavailable. The patient’s mean arterial blood pressure decreased from 80 to 65 mm Hg, leukocyte count increased to 18.8 × 10 9  cells/L, and hemoglobin decreased from 140 to 102 g/L. A chest radiograph revealed patchy opacification of the right lung that was not observed several hours earlier . After intubation and sedation, the patient was successfully transferred to the intensive care unit of a tertiary hospital.\n\n【6】On arrival at the intensive care unit, the patient was put on a mechanical ventilator requiring positive-end expiratory pressure to oxygenate. Antimicrobial drugs (vancomycin, piperacillin/tazobactam, and azithromycin), an antifungal drug (amphotericin B), and a vasopressor (norepinephrine) were administered, and sputum cultures were sent for bacterial and fungal culture. One day after entering the intensive care unit, the patient became bradycardic and went into cardiac arrest. Cause of death was determined to be ARDS .\n\n【7】A sputum specimen was prepared for direct microscopic examination for fungal elements by staining with Calcofluor White (Sigma-Aldrich, St. Louis, MO, USA) and for fungal culture by plating onto BHICCGE (brain heart infusion + 5% sheep blood + 50 μg/mL chloramphenicol, 0.01% cycloheximide, 20 μg/mL gentamicin, and egg white) agar and inhibitory mold agar plates and incubated at 28°C. Direct microscopic examination of the sputum specimen revealed broad-based budding yeast with refractile cell walls characteristic of _Blastomyces_ . After appropriate culture and incubation, small colonies of mold were observed after 1 week. The identification of a _Blastomyces_ sp. was confirmed with a molecular probe (AccuProbe _Blastomyces dermatitidis_ Culture Identification Test; Gen-Probe, San Diego, CA, USA) and by conversion of the mold-form to the yeast-phase using Blasto-D medium and incubation at 37°C . The culture was preserved at −80°C as a glycerol stock with the designation 13BL347.\n\n【8】The isolate was later resurrected and incubated as previously described . Using a similar method described in Brown et al. fungal DNA was extracted, the internal transcribed spacer 2 (ITS2) region was amplified by PCR using the primers described in White et al. and the resulting product was sequenced. Sequences were analyzed, and the single nucleotide polymorphism in ITS2 at position 19 (used for differentiation of _B. dermatitidis_ from _B. gilchristii_ ) was noted . The sequence of 13BL347 ITS2 possessed a cytosine at position 19, which is diagnostic for _B. gilchristii,_ whereas thymine at that position is conserved in _B. dermatitidis_ . By using a multiple sequence alignment tool, we aligned the 13BL347 ITS2 sequence to sequences of several well-characterized representative sequences of both species ; it clustered with other _B. gilchristii_ strains  .\n\n【9】Variation in clinical presentation among different genetic groups of _B. dermatitidis_ has long been observed . However, since the 2013 identification of _B. gilchristii_ , it is unclear as to whether the previous reports were actually reporting differences in virulence between _B. dermatitidis_ and _B. gilchristii_ . Therefore, we refer to the fungi that were the subject of previous blastomycosis studies as _Blastomyces_ spp. rather than _B. dermatitidis_ .\n\n【10】Known virulence factors of _Blastomyces_ spp. include the cell surface carbohydrate polymer alpha-1,3-glucan and the _Blastomyces_ adhesion 1 protein (formerly WI-1) surface adhesion molecule . To our knowledge, differences in expression of these virulence factors in _B. dermatitidis_ and _B. gilchristii_ have not been compared. It has been demonstrated previously that African strains of _B. dermatitidis_ differ from North American strains in their growth and morphology and are thought to cause less severe disease . Genetic analysis has identified 2 distinct genetic groups of African _B. dermatitidis_ that differ in expression of the _Blastomyces_ adhesion 1 protein surface adhesion molecule . Whether the differences in growth, morphology, and apparent virulence in the African _Blastomyces_ sp. is attributable to _B. gilchristii_ and _B. dermatitidis_ , intraspecies variation, or a separate undescribed genetic group remains unexplored.\n\n【11】There are several challenges in evaluating the virulence of _B. dermatitidis_ and _B. gilchristii_ . First, the clinical course of blastomycosis has been found to be correlated with the amount of inoculum (conidia) initially acquired (typically through inhalation) . In the context of human infection, it is difficult to determine if differences in disease process are attributable to differences in fungal virulence or the magnitude of inoculum. Furthermore, host factors such as immunodeficiency and variation in human leukocyte antigen profile have been shown to cause variation in immune response to _Blastomyces_ spp.\n\n【12】_Blastomyces_ spp. have also been observed to lose virulence during laboratory processing, making the results of previous laboratory-based virulence studies questionable . Another challenge is that the commercially available molecular probe that is often used to confirm the identification of _Blastomyces_ spp. cannot differentiate between _B. dermatitidis_ and _B. gilchristii_ . Currently, _B. dermatitidis_ and _B. gilchristii_ cannot be distinguished based on phenotype; PCR followed by sequence analysis is the only method of differentiating these species.\n\n【13】### Conclusions\n\n【14】Most cases of blastomycosis-induced ARDS are preceded by a prodrome of pneumonia weeks to months before development of ARDS . However, in a minority of cases, such as the one we describe, the clinical course rapidly progresses to fatal ARDS . This patient died within 7 days of hospital admission and <24 hours after intubation. In this case, it is difficult to comment on the virulence of _B. gilchristii_ because the patient’s immune status is uncertain. The patient had uncontrolled hyperglycemia secondary to type 1 diabetes mellitus, history of intravenous drug use, and an unknown HIV status; these factors are known to cause immune dysfunction and might have contributed to the acuity and severity of disease. Nevertheless, our findings demonstrate that _B. gilchristii_ infection can cause fatal ARDS in humans and form a basis for further virulence and epidemiologic studies of _B. dermatitidis_ and _B. gilchristii_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "40803dd9-f7b9-45e2-88eb-305cdf8c0bad", "title": "An Immortal Hero, an Enduring Challenge", "text": "【0】An Immortal Hero, an Enduring Challenge\n**Artist Unknown. Hercules and the Erymanthian Boar, mid-17th century. Bronze, with red-brown lacquer patina. Height: 17 1/2 in/44.5 cm. Metropolitan Museum of Art, New York, New York, USA; The Jack and Belle Linsky Collection, 1982.**\n\n【1】Hercules 1  has endured as perhaps the most popular figure from Greek mythology for nearly 3 millennia. Central to his myth is the story cycle about the 12 presumably impossible labors he carried out for the loathed King Eurystheus. Sometimes overlooked is that Hercules performed those tasks as penance for having murdered his own wife and children during a fit of madness.\n\n【2】For the fourth labor, Eurystheus ordered Hercules to capture the vicious Erymanthian Boar, a menacing beast that would descend from its lair on the mountain of Erymanthus each day, trampling the farmlands and attacking man and beast. Surprising the boar in its lair, Hercules drove his quarry into the deep snow, where he subdued the exhausted boar and then carried it to the king’s court. According to the myth, when the cowardly King Eurystheus heard the snorting, grunting beast and realized that Hercules had succeeded, the king hastily hid in a buried pithos jar, imploring Hercules to remove the boar.\n\n【3】Hercules’ capture of the Erymanthian Boar, a tempting subject for artists, has been immortalized on coins and amphora; on a Roman sarcophagus; in paintings, films, and sketches; and through myriad sculptures. The identity of the sculptor who created the elegant bronze statue reproduced for this month’s cover is not known. This work is thought to be modeled on earlier works by an imitator or student of the Flemish sculptor Giambologna , the court sculptor to the Medici grand dukes in Florence. He had produced a set of bronze statuettes of the 12 labors and supervised the work and training of numerous assistants.\n\n【4】This bronze portrays the aftermath of the chase. The figure of Hercules resolutely strides toward the court, clasping the huge boar over his shoulder with his left arm, balancing it with the club he holds aloft in his right hand. The textures of the bronze offer visual and tactile contrasts. Hercules’ body is cast in smooth bronze, which heightens his physical prowess and musculature. The struggling boar’s bristly hide, Hercules’ hair and beard, and the patterns in his wooden club reveal the artist’s nimble touch in working with bronze. A Metropolitan Museum of Art commentary notes that “The present statuette is extremely light in weight, with a dark but warm brown patina and richly variegated tool marks, such as the punch marks that articulate the club.”\n\n【5】Although his strength and vigor are realized through this statue, Hercules would not have redeemed himself through his struggles and suffering alone, and ultimately become immortal, without also using his cunning and skill. He frequently sought the counsel of others in the course of his undertakings. During this labor, it was the centaur Chiron (though some versions say it was a different centaur named Pholus) who advised Hercules to drive the boar into the snow.\n\n【6】While completing his fourth labor, Hercules was potentially exposed to dangers that also threatened mortal men of his time and ours: possible exposure to zoonotic diseases. Zoonotic diseases are caused by any of more than 200 pathogenic agents, including bacteria, viruses, parasites, and fungi, transmitted directly or indirectly from animals to humans. For instance, the Erymanthian Boar could have felled Hercules via microbiology instead of muscle and tusk.\n\n【7】Hercules’ risks did not stop with the Boar. In slaying and then skinning the formidable Nemean Lion and stalking and capturing the sacred Hind of Ceryneia, Hercules was at risk for various zoonotic diseases. Cleaning the Augean stables exposed him to enteric pathogens from the vast quantities of manure generated by teeming herds of cows, sheep, horses, and goats. Additional exposures occurred when he wrestled the Cretan Bull and stole the Cattle of Geryon. Hercules’ efforts to dispel the Stymphalian Birds and capture the Mares of Diomedes may have exposed him to zoonoses acquired from birds and equids. And finally, why was the great 3-headed dog Cerberus so furious and out of control—rabies perhaps?\n\n【8】The human web of daily activities crosses many ecosystems. Animals provide food, transportation, companionship, and if you are Hercules, occasionally a wrestling adversary. Managing and limiting infectious risks associated with the intricate connections among humans, animals, and our environments prove to be challenging, complex endeavors. It’s not hyperbole to label this undertaking a Herculean task.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71852f6a-425b-48d5-ae59-eea69b43d119", "title": "Limited Spread of Penicillin-Nonsusceptible Pneumococci, Skåne County, Sweden", "text": "【0】Limited Spread of Penicillin-Nonsusceptible Pneumococci, Skåne County, Sweden\nIncreasing frequencies of penicillin-nonsusceptible pneumococci (PSNP) (MIC of penicillin ≥0.12 mg/L) became a worldwide problem in the 1980s . The rapid increase in frequency is likely caused by intercontinental spread of a few PNSP clones for which MICs of penicillin are high . For a long period, very low PNSP frequencies were noted in Sweden (%–3%) . However, in the early 1990s, the incidence of PNSP increased from 8% to 10% in Skåne, the southernmost county, while the rest of the country reported unchanged low frequencies . When considered from the perspective of international experience, in which the rate of PNSP rapidly increased when this level of resistance was reached , the increasing frequencies of PNSP in Skåne County led to the formation of an expert committee, appointed by the National Board of Health and Welfare. This committee proposed a national strategy in 1995, based on reducing unnecessary use of antimicrobial agents and applying infection control measures to limit the more immediate spread of PNSP for which MICs of penicillin were high (≥0.5 mg/L, penicillin-resistant pneumococci \\[PRP\\]), especially in preschool children ages 1 to 6 years. In 1996, infection and carriage with PRP became notifiable by the Swedish Communicable Disease Act. The reasons to choose MIC ≥0.5 mg/L as a limit for intervention were that epidemiologic data on pneumococci suggested that an increased prevalence of strains for which the MIC of penicillin was ≥1.0 mg/L most often was caused by spread of a few already resistant clones, so that those strains could lead to treatment failures, and that those strains were relatively rare in Sweden in 1994 . Since the Etest can be hard to interpret, MIC ≥0.5 mg/L was chosen so strains for which the MIC of penicillin was 1.0 mg/L would not be missed. The decision of whether to follow the recommendations of the expert committee is made by each county department for communicable disease control. The recommendations were strictly applied in Skåne County (South Swedish Pneumococcal Intervention Project, SSPIP), while some Swedish counties either did not follow the recommendations at all or only applied parts of the recommendations. For a long period, Skåne County had the highest use of antimicrobial agents in Sweden, especially of macrolides and broad-spectrum antibiotics , and in the SSPIP traditional communicable disease control measures are combined with actions aimed at reducing the use of antimicrobial drugs. Experiences from the first 2 years of the SSPIP from parts of Skåne County have previously been reported . For example, data on individual risk factors for carriage of PNSP have been evaluated. We discuss the overall results of the SSPIP and evaluate the effects of the recommendations in Skåne County during the first 6 years they were implemented.\n\n【1】### Materials and Methods\n\n【2】##### Study Area\n\n【3】On January 1, 2000, the population of Skåne County was 1,129,424 inhabitants; 93,051 were < 6 years of age. The county is divided into 33 municipalities. The largest municipality, Malmö, had 259,579 inhabitants and of those, 20,325 were < 6 years of age. The smallest municipality had 6,745 inhabitants; 581 were < 6 years. During the study period, ≈76% of the children 1 to 6 years of age were enrolled in day care.\n\n【4】##### SSPIP\n\n【5】The principles of the SSPIP have been described earlier . In brief, all persons in Skåne County with a culture that yields penicillin-nonsusceptible pneumococci with an MIC ≥ 0.5mg/L for penicillin (PRP), regardless of resistance to any other antibiotics, have since 1995 been reported to the Regional Center of Communicable Disease Control (CCDC). Whenever a person with a clinical infection caused by PRP (index case-patient) is identified, nasopharyngeal cultures are obtained from family members and other close contacts to identify asymptomatic PRP carriers (contact case-patients). All carriers are followed weekly with nasopharyngeal cultures at the local primary health care center until two consecutive cultures that yield no growth of PRP (PRP-negative) have been obtained. If the index case is in a child attending any form of group day care, nasopharyngeal cultures are obtained from the other children and staff members at that day care group. Preschool children are restricted from attending day care until they are PRP-negative. In selected cases, eradication therapy with antimicrobial drugs is considered after 2 to 3 months of carriage, or earlier when strong social reasons exist .\n\n【6】##### Microbiologic Methods\n\n【7】The pneumococcal strains considered in this study were recovered from cultures analyzed at the Departments of Clinical Microbiology in Lund, Malmö, Helsingborg, and Kristianstad from July 1, 1995, through June 30, 2001. These four laboratories served the entire population of Skåne County during the study period. The specimen were cultured on blood agar plates, and the isolates were identified as _Streptococcus pneumoniae_ on the basis of colony morphology and susceptibility to optochin . The strains were screened for penicillin-resistance by using the disk-diffusion method, according to the Swedish Reference Group for Antibiotics (SRGA). The strains were injected onto Iso Sensitest agar (Oxoid Ltd, Basingstoke, UK), supplemented according to the recommendations, and the antibiotic disks (Oxoid Ltd) were applied. Inhibition zones were read to the nearest millimeter and interpreted according to SRGA guidelines . For pneumococci with an oxacillin 1 μg inhibition zone <20 mm, the MIC of penicillin was determined by the Etest (AB Biodisk, Solna, Sweden) . The susceptibility to other antimicrobial agents (erythromycin, tetracycline, trimethoprim-sulfamethoxazole, and clindamycin) was determined by using the disk diffusion method . Strains from each patient were registered only once per season, even if multiple cultures were positive for pneumococci. Whether the strains were recovered at a visit to the doctor or by contact tracing or screening was noted. Serotyping to the group level was performed by the quellung reaction, by using antisera from the Statens Seruminstitut, Copenhagen, Denmark .\n\n【8】##### Antimicrobial Drug Use\n\n【9】Data were collected regarding antimicrobial drug prescriptions for outpatient care, served at Swedish pharmacies from July 1, 1995, through June 30, 2001, that were issued for children ages < 6 years who lived in Skåne County; these data were obtained from the Corporation of Swedish Pharmacies, which owns all Swedish pharmacies and collects and compiles information on all drugs sold in the country . The municipality of the patient was registered. The prescribed antimicrobial agents were given as prescriptions per 1,000 inhabitants ages < 6 years per season, and all prescriptions of phenoxymethylpenicillin (PcV), ampicillin/amoxicillin (including amoxicillin+clavulanic acid), cephalosporins, trimethoprim-sulfamethoxazole, macrolides, clindamycin, and other antimicrobial drugs (grouped together) were registered.\n\n【10】### Results\n\n【11】During the project, the frequency of PNSP carriers from clinical nasopharyngeal cultures has been evaluated. Cultures taken at contact tracing were excluded. From July 1, 1995, until June 30, 2001, the average frequency of PNSP carriers has been stable, approximately 7.4% (PNSP/34,745 pneumococci) . The highest frequencies (.1%) were seen in 1997 to 1998. During the 6 years of the recommendations, the levels of PNSP have been unevenly distributed over the county; higher and stable levels were found in the city of Malmö in the southwestern part of the county, (.3%, 976 PNSP/8,651 pneumococci) and lower, but stable, levels were found in the northeastern part of the county (.3%, 395 PNSP/9,109 pneumococci). The frequency of PNSP among invasive pneumococcal strains (blood and cerebrospinal fluid cultures) in Skåne County has been low, ≈2.5% (PNSP/858 pneumococci). As with the PNSP in nasopharyngeal cultures, the highest frequencies of invasive PNSP were seen in 1997 to 1998.\n\n【12】The average frequency of index case-patients with PRP (that is, the number of clinical case-patients with a culture with growth of PRP, divided by the number of pneumococci found in nasopharyngeal cultures) has been rather stable since the start of the project, ≈2.6% (PRP/34,745 pneumococci) . In the southwestern part of the county, in the cities of Malmö and Lund and surrounding municipalities, where most cases were found, PRP have been constantly present. However, in several of the smaller municipalities, PRP were prevalent during one or two seasons but then disappeared. In two municipalities, both situated in the northeast, no cases of PRP infection have been found. The proportion of cases with PRP with high MICs (≥2 mg/L) of penicillin has not increased during the 6 years . Carriers of PRP with MIC ≥4.0 mg/L have all been adopted children or immigrants from Eastern Europe or from countries outside Europe.\n\n【13】During the 6-year period, 2,269 PRP carriers (persons) have been registered at the CCDC in Skåne County. Of the 2,269 PRP carriers, 40% were index case-patients, and 60% were contacts. Few contact cases have been found in persons at the extremes of ages (≤1 year and >65 years). The number of contact cases per index patients in children < 6 years of age has, on an average, been 1.6 (.1 to 2.2 per season) and has not increased. The index patients have most often been found through positive nasopharyngeal cultures, and only a minority (n = 13, 1.4%) have been found through positive blood or cerebrospinal fluid cultures. More men carried PRP overall, but in the group ages 18 to 64 years, PRP carriage was more common among women. A clear seasonal variation in the incidence of PRP was found; considerably more cases occurred during the winter months (October–March) than during summer. The median duration of PRP carriage was 21 days (range 2–368 days); 1,704 (%) of the PRP patients were children 1 to 6 years of age. Seventy-one percent of these children attended day care centers; 6% attended family day care, and 23% were cared for at home. The other contact case-patients have primarily been siblings, parents, or grandparents of young children.\n\n【14】Two hundred and twenty-seven children with a clinical culture showing growth of PRP, led to screening of children and staff at 227 of the county’s 1,250 day care centers. The number of contact cases per index case-patient among all children attending these day care centers has, on an average, been 3.46 (.6–4.2 per season) and has not increased, and the number of contact cases in each day care center has varied from 0 to 25 (median 2). In 24% of the day care center interventions, no contact cases were found. In 45 of the 227 day care centers, more than one PRP serotype was discovered, and 65 day care centers have been investigated twice or more (median 2, range 2–6). Twenty-two of these day care centers had two outbreaks with the same serotype within 6 months. Most of the day care interventions took place in the southwestern part of the county. Only 20 of all screened staff at day care centers were PRP-positive.\n\n【15】Serotyping to the group level was performed for 2,131 (%) of the 2,269 PRP strains. Twenty-five serotypes of PRP were found. Six serotypes comprised 93% of the strains (serotypes 9, 19, 6, 23, 15, 14). The most prevalent strain, serotype 9, represented an average of 48% of the PRP strains (%–66% per season). This strain has spread over the county from municipality to municipality in a clear pattern, whereas the other common serotypes have spread more randomly.\n\n【16】The use of antimicrobial agents in outpatient care in Skåne County decreased from the first to the last season in all age groups, but especially in children aged < 6 years . The main reduction among children ages < 6 years was caused by a decrease in number of prescriptions for phenoxymethylpenicillin, and the use of macrolides was reduced by 50%. Even though the decreased use of antimicrobial drugs was seen in all municipalities, the municipalities with the highest utilization in 1995–1996 still had the highest use in 2000–2001. Those municipalities were all located in the southwestern part of the county.\n\n【17】### Discussion\n\n【18】In Skåne County, where PNSP (pneumococci with an MIC for penicillin ≥0,12 mg/L) have been prevalent for >10 years, the frequency of PNSP in clinical nasopharyngeal cultures (cultures taken at contact tracing excluded) remained at the same level in 2000 to 2001 as in 1995 to 1996, on average, 7.4%. In the city of Malmö, where the spread of PNSP appears to have started, the average level of PNSP has been higher (.3%) during the project period. In contrast, the average level of PNSP remained low (.3%) during the study period in the northern and eastern parts of the county. Furthermore, the number of PRP contact cases per index case among children attending day care centers did not increase during the 6-year period. These data indicate that the rapid spread of PNSP that started in the southwestern part of the county in the beginning of the 1990s  has been limited. These results are unique in comparison with experience in other countries, where PNSP rates have rapidly increased after reaching an 8% PNSP level . The proportion of case-patients with PRP with high MICs of the PNSP in Skåne County has not increased during the 6-year period, contrary to results from most other countries, which report increasing frequencies of PNSP with high MICs .\n\n【19】One of the project’s aims was to decrease unnecessary use of antimicrobial agents, especially of macrolides and trimethoprim-sulfamethoxazole, which in studies have been shown to promote spread of PNSP . During the study period, the use of antimicrobial drugs decreased in all municipalities in the county, and the decrease was most prominent in children ages < 6 years. In this age group, the use of macrolides was cut in half. The use of long-acting macrolides has been extremely low and has comprised <3% of the total use of macrolides among children. Even though the use of antimicrobial agents decreased in all municipalities of Skåne County, those with high usage in 1995 to 1996 also had high usage in 2000 to 2001, and those with low usage in 1995 to 1996 had an even lower usage in 2000 to 2001. Mirroring the use of antimicrobial agents, the occurrence of PNSP has been unevenly distributed in the county, with constant high rates in the southwestern part and low levels in the northern and eastern parts. In cities in the northeastern parts and in the smaller municipalities, with lower antimicrobial agent use, single outbreaks have occurred without any more cases being reported later. A possible explanation for this might be that municipalities in which antibiotics are frequently used have a higher risk of spreading PNSP in the community, as indicated by previously published data from the project as well as by other studies .\n\n【20】Many PRP serotypes were found in the county during the six seasons. However, one serotype, serotype 9, has dominated during all six seasons and has spread in a clear pattern from municipality to municipality throughout the county , while other strains have spread more randomly in time and geographically. Why this strain has been persistently present for such a long time is unclear.\n\n【21】We chose to present the figures for the frequency of PNSP in clinical cultures over time, excluding cultures taken at contact tracing, since the contact case-patients were not found at random, but rather represented a selected population around an index patient. The exclusion of cultures taken at contact tracing ought to give more fair figures when comparing data from the study period with the “resistance situation” before the project started or with data from other studies. The possibility of excluding all cultures taken at contact tracing retrospectively was not 100% guaranteed, but cultures taken at day care center screenings were easily found and excluded. The rest of the cultures obtained because of contact tracing constituted only a small proportion of cultures, and thus they should not be a source of error.\n\n【22】Although the indication for nasopharyngeal sampling was not changed during the study period, the number of obtained samples has decreased since the start of the project. However, the number of nasopharyngeal cultures increased in 1995 to 1996 compared to 1993 to 1994. One possible explanation for this might be that the health authorities encouraged doctors to take nasopharyngeal samples more frequently during the first years of the project.\n\n【23】Exactly comparable data on PNSP frequencies from Swedish counties or comparable survey data (nasopharyngeal cultures) from other countries are hard to find. However, since 1996, Swedish law has mandated the reporting of all PRP strains to the Swedish Institute for Infectious Disease Control. Data from this national register contains information on reported PRP from all over Sweden from 1997 to 2002. Of all reported PRP, the PRP from Skåne County comprised 40% in 1997, but only 10% in 2002 . Furthermore, the recommendations are strictly applied in Skåne County, while some Swedish counties either do not follow the recommendations at all or only apply parts of the recommendations. In some counties, where the recommendations were not applied or not as strictly applied as in Skåne County, the frequencies of PRP have increased from 2000 to 2002 . Moreover, according to data from the annual Resistance Surveillance and Quality Control Programme, the frequency of PNSP in clinical nasopharyngeal cultures in Sweden has increased from 3.8% in 1994 to 6.2% in 2002 . In addition, the frequency of PNSP among invasive pneumococcal strains in Sweden has, on average, increased from 1.4% in 1999 to 2.4% in 2002 , but the frequency of invasive PNSP during the same period in Skåne County has been approximately 2.5% and stable. The numbers of invasive PNSP isolates in Skåne County are very low, and therefore drawing any reliable conclusions from these data is difficult. However, the frequency of PNSP has not increased, as it has in most other countries, and the trends of invasive PNSP seem to follow the trends for PNSP from nasopharyngeal cultures. Compared to the other Nordic countries, who still report low frequencies of invasive PNSP, Sweden has had the lowest frequencies of invasive PNSP between 1999 and 2002 .\n\n【24】Although the frequencies of PNSP in Skåne County were not reduced, the spread of PNSP seems to have been limited. Whether this is a result of the actions of the SSPIP among children or of the decreased use of antibiotics in the county, or both, is impossible to tell since these actions were started simultaneously. In other countries, actions against unnecessary use of antimicrobial drugs has been the only measure of combating the spread of PNSP, in most cases with little success, probably in part because the actions have been initiated in a much later phase, when frequencies of PNSP were higher . Still, further efforts are necessary to reduce the prescribing of antimicrobial agents in the southwestern areas, since the use of antimicrobial agents is still high there and poses the greatest risk factor for PNSP carriage.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "37e1bd09-2169-4e25-8556-992e7885aaab", "title": "Outbreak of Leptospirosis after Flood, the Philippines, 2009", "text": "【0】Outbreak of Leptospirosis after Flood, the Philippines, 2009\nLeptospirosis is highly endemic to the Philippines. Outbreaks usually occur during the typhoon season (July–October) . On September 26, 2009, a typhoon caused serious flooding in Metro Manila . Starting in the first week of October, the number of patients with suspected signs and symptoms of leptospirosis increased sharply. Until mid-November, 2,299 patients, including 178 who died (case-fatality ratio \\[CFR\\] 8%), in 15 hospitals in Metro Manila were reported to the Department of Health . For this outbreak, we conducted a hospital-based investigation to describe the characteristics of hospitalized patients, investigate risk factors for death, and identify the causative _Leptospira_ species and serogroups.\n\n【1】### The Study\n\n【2】We conducted our investigation at San Lazaro Hospital, a national 500-bed referral infectious disease center for Metro Manila and neighboring provinces, which mostly serves economically disadvantaged persons. On October 11, 2009, this hospital initiated prospective surveillance and retrospective data collection. The study ended on October 31, when cases had decreased to baseline level (<1 case/d). The list of patients was obtained from the inpatient database with International Classification of Diseases, 10th revision, coding. Vital sign information and laboratory findings at admission were collected from medical charts.\n\n【3】Eligible were all hospitalized patients who had 1) fever plus at least 2 other signs and symptoms of leptospirosis (headache, myalgia, eye pain, nausea, vomiting, abdominal pain, diarrhea, conjunctival suffusion, jaundice, tea-colored urine, oliguria, anuria, or unusual bleeding) ; 2) history of wading in floodwater; and 3) symptoms that started September 26–October 31. We evaluated prognostic factors and the effect of therapeutic factors by using univariate and multivariate Poisson regression analyses with robust SEs .\n\n【4】A total of 486 cases met clinical criteria for leptospirosis; 15 were excluded because of insufficient data. Patients were predominantly young and male . The most common clinical features were conjunctival suffusion and myalgia, followed by abdominal pain and oliguria. Among 471 patients, 51 died (CFR 10.8%); 12 (.6%) were discharged before improvement; and 7 (.5%) were transferred to other hospitals, mainly for dialysis. Primary causes of death were pulmonary hemorrhage ([35%\\]) and acute respiratory distress syndrome/severe respiratory failure ([24%\\]), followed by acute renal failure ([20%\\]) and multiple organ failure/disseminated intravascular coagulation ([16%\\]). Mean time ± SD from illness onset to death was 7.7 ± 5 days; 36 (%) died within 2 days of admission.\n\n【5】Univariate analysis showed the following to be associated with death: older age, jaundice, anuria, and hemoptysis . Of the initial laboratory findings, neutrophilia, thrombocytopenia, increased blood urea nitrogen, and increased creatinine levels were associated with death. Most patients received antimicrobial drugs (mainly penicillin G) as first-line treatment. Delayed initiation of treatment increased risk for death . No patient received prophylactic antimicrobial therapy. Administration of rapid volume replacement therapy and diuretics was associated with death but probably reflected severe renal disease. Only 1 patient received peritoneal dialysis and 1 received mechanical ventilation.\n\n【6】During the outbreak, 2 kinds of rapid diagnostic tests for leptospirosis—Leptospira Serology Kit (Bio-Rad, Marnes-la-Coquette, France) and PanBio IgM ELISA (Panbio Diagnostics, Brisbane, Queensland, Australia)—were available in the hospital, although the number of kits was limited. Plasma collection for additional laboratory confirmation started October 11. Samples were initially stored at –4°C in the hospital laboratory and then frozen at –40°C in the National Reference Laboratory, STD/AIDS Cooperative Central Laboratory, San Lazaro Hospital. In June 2010, microscopic agglutination test (MAT) and PCR were performed in the National Institute of Infectious Diseases, Tokyo, Japan, as described . A case was defined as laboratory confirmed if 1) specific antibodies were detected with titer \\> 400 or at least a 4-fold increase in reciprocal MAT titer between paired samples, 2) PCR for _Leptospira flaB_ gene was positive for at least 1 blood sample, or 3) rapid diagnostic test result (either test) was positive for at least 1 blood sample.\n\n【7】A total of 134 plasma samples were collected from 93 patients. From 25 patients, 2–4 samples were collected; mean time from first to last collection was 6.1 (range 1–18) days. Among the 93 patients, 15 had a single MAT titer \\> 400 and 4 showed a 4-fold increase in the MAT titer in paired samples. Most antibodies were against _L. borgpetersenii_ serovar Tarassovi, followed by serovars Poi and Sejroe and _L. interrogans_ serovars Losbanos and Manilae. PCR produced positive results for 4 patients, 2 of whom had a negative MAT titer. PCR-detected _Leptospira_ strains were phylogenetically characterized by the _flaB_ sequence ; 3 were identical to those of isolates from rats in Metro Manila, but 1 was distinct .\n\n【8】Rapid-test results were available for 85 case-patients; 48 of 78 samples were positive by Leptospira Serology Kit and 5 of 7 were positive by ELISA. Of 27 cases tested by MAT and Leptospira Serology Kit, 6 were positive by both tests. One sample was negative by MAT and ELISA. Taken together, 149 (%) of all cases underwent any diagnostic testing for leptospirosis, and results for 67 (%) of these were laboratory confirmed. Clinical and laboratory findings of laboratory-confirmed and suspected cases, respectively, were almost identical except for the presence of jaundice (.2% vs. 45.5%; p = 0.018), convulsions (.5% vs. 0%; p = 0.003), and death (% vs. 12.1%; p = 0.03). No plasma sample test results have been validated; thus, deduced incidence should be considered with caution. To explore the possibility of hantavirus co-infection contributing to the high CFRs, we screened samples for antibodies against hantavirus , but all were negative.\n\n【9】### Conclusions\n\n【10】Risk factors for fatal leptospirosis were jaundice, anuria, and hemoptysis at admission. These are typical signs of the severe form of leptospirosis called Weil disease , confirming earlier work . Hemoptysis with high CFR ([47%\\] of 15) may have represented leptospirosis-associated severe pulmonary hemorrhagic syndrome . Some clinical features, including cough, seemed to be associated with lower risk for death, but minor symptoms in dying patients might have been overlooked. High leukocyte counts, blood urea nitrogen and creatinine levels, and lower platelet counts were also associated with death.\n\n【11】Deaths could be reduced if these indicators are detected and appropriate management introduced early. Laboratory data such as potassium level might also be helpful, but these tests are often not available for all suspected cases in resource-poor settings, especially during outbreaks. The trend of most patients being male is compatible with previous reports and thought to reflect higher exposure to contaminated water .\n\n【12】Although all patients received antimicrobial therapy, a considerable proportion died of acute respiratory distress syndrome and acute renal failure within only 2 days of admission, indicating that most patients sought care too late. The World Health Organization recommends starting antimicrobial therapy before the fifth day of disease onset , but for half of the case-patients reported here, it was started after the fifth day. Our results support the benefits of early initiation of antimicrobial therapy.\n\n【13】Renal failure was clearly a cause of death, but only 1 patient received peritoneal dialysis and another 7 were transferred to other hospitals for hemodialysis after only 3.4 days of hospitalization. This lack or delay of dialysis might have affected outcomes. San Lazaro Hospital is not sufficiently equipped for intensive care. Expanded access to peritoneal dialysis might reduce deaths from severe leptospirosis complicated by acute renal failure.\n\n【14】Leptospirosis is typically seen in resource-poor settings, where costly medical equipment such as dialyzers and ventilators are rarely accessible. For leptospirosis outbreak control and CFR reduction in leptospirosis-endemic regions, continuous case and environmental monitoring and early introduction of appropriate treatment for suspected cases are warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e5cc9422-b2a8-4f82-92dd-92e9210d9116", "title": "Moxifloxacin Prophylaxis against MDR TB, New York, New York, USA", "text": "【0】Moxifloxacin Prophylaxis against MDR TB, New York, New York, USA\nLimited data exist on safety of prophylaxis for contacts to persons with multidrug-resistant tuberculosis (MDR TB). All MDR TB strains are resistant to at least isoniazid and rifampin, precluding the use of these drugs for MDR TB prophylaxis. Current local, national, and international guidelines suggest using antibiotics to which the strain from the index case-patient is susceptible ( ); however, no randomized controlled trial has been conducted to support this recommendation. Global spread of MDR TB necessitates identification of treatment options with acceptable safety and tolerability for persons infected with drug-resistant strains. Choosing appropriate treatment for HIV-positive persons exposed to TB is even more crucial considering the increased risk among these persons for progression from TB infection to active disease .\n\n【1】In 2005, two TB outbreaks occurred in New York City (NYC) among HIV-positive persons with 2 distinct MDR TB strains. In both outbreaks, contacts were defined as 1) residents of a building on the floor on which a case-patient resided or visited during the infectious period and 2) health care staff members who provided direct care to case-patients. Eligible contacts were treated with moxifloxacin to prevent progression from TB infection to disease. We present the 9-year follow-up from these exposures and the outcomes of the treated contacts.\n\n【2】### The Investigation\n\n【3】The first outbreak we investigated occurred in a facility that provided housing and harm-reduction services to a predominantly HIV-positive, homeless, and drug-using population (site A). The first TB case-patient identified was a 53-year-old HIV-positive man residing there, in whom pulmonary TB was diagnosed by a positive (+) acid-fast bacilli (AFB) sputum smear and positive culture. His chest radiograph showed extensive bilateral infiltrates and a large pulmonary cavity; he died 4 days after initiating treatment. Subsequent drug-susceptibility results indicated the strain was resistant to isoniazid, rifampin, ethambutol, pyrazinamide, streptomycin, rifabutin, and kanamycin. Within 3 months, TB was diagnosed in 2 additional HIV-positive residents of site A; genotype and drug-resistance phenotype matched those of the index case-patient. A contact investigation and active case finding were initiated at site A, and 3 additional MDR TB cases with matching genotype were identified.\n\n【4】Of 105 close contacts identified, 84 (%) were HIV-positive, 16 (%) were HIV-negative, and 5 (%) had unknown HIV status . Among the 21 contacts not known to be HIV-positive, 1 person had a positive tuberculin skin test (TST) result, had normal chest radiograph results, and started moxifloxacin prophylaxis; however, the patient was lost to follow-up after 2 months. Among the 84 HIV-positive contacts, TST results of 2 were positive and that of 1 other contact was positive after a negative result documented 3 years before. Fifty-one (%) HIV-positive contacts were lost to follow-up or refused evaluation or prophylaxis. Before being tested, 1 (%) contact died as a result of HIV-related causes. Of the remaining 32 (%) HIV-positive persons, 26 (%) started moxifloxacin prophylaxis; 16 (%) completed treatment, 5 (%) were lost to follow-up within 2 months (including the 3 who tested TS positive), 3 (%) were discharged from treatment because of adverse reactions, and 2 (%) were either medically discharged for unknown reasons or refused to continue treatment.\n\n【5】The second outbreak occurred at a long-term care facility housing HIV-positive, previously homeless persons (site B). The index case-patient was a 49-year-old HIV-positive man for whom smear-positive (+), culture-positive pulmonary TB was diagnosed. A TB strain resistant to isoniazid, rifampin, and rifabutin was identified; the patient died 1 month later. Contact investigation and active case finding were initiated at site B. Within 6 months of the index case-patient’s diagnosis, 5 additional TB cases were identified in 4 HIV-positive residents and 1 HIV-negative staff member. On the basis of genotype, the strain the index case-patient was diagnosed with matched the strain of the 3 residents and staff member. All isolates also had the same drug resistance phenotype. The other HIV-positive resident had a clinical diagnosis of TB meningitis (no culture results available).\n\n【6】In the site B outbreak, 136 close contacts were identified : 83 (%) HIV-positive residents and 53 (%) staff members with unknown HIV status. Of the 53 staff members, 22 (%) were previously TST-positive but had normal chest radiograph results during this evaluation; 25 (%) tested negative, and 6 (%) were not evaluated. No staff members were eligible for prophylaxis. Of the 83 HIV-positive residents, 3 (%) had positive TST results; 2 had documented negative results within the year before their positive result, strengthening evidence of TB transmission in site B.\n\n【7】Considering the drug susceptibility pattern of this strain, a combination of moxifloxacin and pyrazinamide was recommended for all HIV-positive contacts once active disease was ruled out. Among exposed residents, 40 (%) either died of non–TB-related causes or were lost to follow-up before completing TB evaluation, and 12 (%) either refused treatment or were not started on treatment because of physician decision. Of the remainder, 24 initiated moxifloxacin and pyrazinamide treatment; 14 (%) completed treatment, and 10 (%) refused or were lost-to-follow up after a median 3 (range 1–5) months of treatment. The 2 contacts whose TST results were converted were placed on alternative regimens.\n\n【8】To determine whether TB symptoms subsequently developed in any contact in either outbreak, we compared them to cases identified in the NYC TB registry. As of March 2014, after a maximum of 8.5 years of follow-up at site A and 9 years at site B, 1 contact, a resident at site B who completed 1 month of moxifloxacin and pyrazinamide treatment in 2006 had TB disease caused by a different drug-susceptible strain develop during 2009.\n\n【9】### Conclusions\n\n【10】Globally, an estimated 480,000 persons were infected with MDR TB in 2013 . Although the current priority is accurate diagnosis and treatment of persons with active disease, preventing MDR TB in infected contacts is also crucial. Programs with capacity for contact investigation are faced with the question of what prophylaxis to recommend for persons who have contact with persons who are positive for MDR TB because data are scarce on efficacy and safety of treatment options for these persons.\n\n【11】Currently recommended treatment regimens  are not without risk. Studies demonstrate serious adverse effects associated with the use of pyrazinamide in combination with either ethambutol  or a fluoroquinolone, including ofloxacin  and levofloxacin . However, a recent study of contacts treated with moxifloxacin- or levofloxacin-based prophylaxis found no serious adverse events and fewer cases of disease among those treated than those untreated .\n\n【12】Of the 50 contacts initiating moxifloxacin-based prophylaxis in the 2 outbreaks, 30 (%) completed treatment. Therapy was generally well-tolerated; 3 contacts discontinued treatment because of gastrointestinal symptoms, (nausea, vomiting, diarrhea). None of the contacts manifested TB symptoms regardless of treatment status. Those contacts at greatest risk for development of disease may have done so during the outbreak investigation.\n\n【13】Because of the lack of safety information for moxifloxacin-based prophylaxis, we reported these findings despite key limitations. These outbreak investigations were conducted as part of routine TB control activities; thus, treatment regimens were not standardized across the study populations, and contacts were not actively followed. However, these outbreaks demonstrate known exposure to infectious MDR TB case-patients with strong evidence of transmission to a high-risk HIV-positive population. Because of robust disease reporting and broad surveillance coverage, we are able to report on outcomes in NYC within < 9 years of follow-up time. This study demonstrates that moxifloxacin-based regimens can be used to treat HIV-positive persons exposed to MDR TB with few serious adverse effects, although we were unable to assess efficacy of the regimen. More research is needed to evaluate the effectiveness of prophylaxis for MDR TB.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "156d0f3e-2279-4f96-af6d-766938e23f10", "title": "Vertical Transmission of Zika Virus by Aedes aegypti and Ae. albopictus Mosquitoes", "text": "【0】Vertical Transmission of Zika Virus by Aedes aegypti and Ae. albopictus Mosquitoes\nFollowing the 2007 outbreak in Micronesia, Zika virus (Flaviviridae, Flavivirus_ ) has continued to expand its distribution throughout the Pacific region and, since 2014, the Americas . The virus is primarily maintained by horizontal transmission between _Aedes aegypti_ mosquitoes and humans, yet other _Aedes_ spp. are also competent vectors . The extent to which Zika virus can utilize vertical transmission between mosquitoes (i.e. transmission from an infected adult female mosquito to her progeny) has not been adequately assessed after peroral infection. Such studies are required to accurately determine the potential role of vertical transmission in Zika virus expansion and maintenance.\n\n【1】Although previous studies have found that other flaviviruses, including West Nile , dengue , yellow fever , and St. Louis encephalitis , can undergo vertical transmission, such transmission is generally relatively inefficient, with filial infection rate (FIR) estimates ranging from 1/36 to 1/6,400 . A previous study estimated rates for Zika virus vertical transmission in _Ae. aegypti_ mosquitoes to be 1/290, yet a reliable estimate for transmission in _Ae. albopictus_ mosquitoes was not achieved . In addition, these estimates were based on intrathoracic inoculation of Zika virus rather than assessment after infectious blood meal acquisition.\n\n【2】We exposed laboratory colonies of _Ae. aegypti_ mosquitoes (collected in Posadas, Argentina, or Poza Rica, Mexico) and _Ae. albopictus_ mosquitoes (obtained from Suffolk County, New York) to Zika virus through infectious blood meals and evaluated the mosquitoes’ capacity to transmit the virus to progeny. For this study, we used the Zika virus strain ZIKV HND , passaged once on C6/36 cells, and Zika virus PR , passaged 4 times on Vero cells and twice on C6/36 cells. Zika virus was propagated on C6/36 cells for 4 days, and freshly harvested supernatant was mixed 1:1 with sheep blood (Colorado Serum Company, Denver, CO, USA) and 2.5% sucrose.\n\n【3】Infectious blood meals were offered to 4- to 7-day-old female mosquitoes, and weekly noninfectious blood meals were offered after the first oviposition. Eggs laid during the second oviposition and beyond were collected and hatched for subsequent testing. Third- to fourth-instar larvae were collected in pools of 5 and processed by homogenization and centrifugation. After RNA extraction, we used Zika virus–specific quantitative reverse transcription PCR  to determine adult infection (indicated by positive bodies), dissemination (indicated by positive legs), viral load, and FIR, which was calculated by using a maximum-likelihood estimate (PoolInfRate 4.0; Centers for Disease Control and Prevention, Atlanta, GA, USA).\n\n【4】We tested 104 _Ae. aegypti_ pools; 6 were Zika virus–positive, indicating a FIR of 11.9 (range 4.9–24.6). This value equates to a ratio of ≈1:84, which is substantially higher than that found by Thangamani et al. as well as ratios historically measured for flaviviruses . Although just 17 pools of _Ae. albopictus_ were tested, 1 pool was positive, which equates to a similar FIR (.8 \\[range 1.7–134.8\\]) and establishes that _Ae. albopictus_ mosquitoes are capable of vertical transmission of Zika virus in the laboratory.\n\n【5】Although the bypassing of the midgut during inoculation generally results in higher levels of vertical transmission, we fed mosquitoes high virus doses (.9–9.3 log 10  PFU/mL), resulting in >93% of disseminated infections and development of high viral titers in individual mosquitoes, averaging 7.1 (Ae. albopictus_ ) to 7.5 (Ae. aegypti_ ) log 10  PFU/mosquito . Although the likelihood that eggs were derived from mosquitoes with disseminated infections is high, the rate of vertical transmission (proportion of infected mosquitoes transmitting to progeny) could not be determined. Future studies assessing infection status and FIR of individual mosquitoes will help clarify the extent of individual variability in vertical transmission efficiency. In addition, we tested larvae rather than adults, and it is likely that transtadial transmission is not completely efficient, so further studies are required to fully evaluate transmission potential of adults infected via vertical transmission. We observed a trend of increasing vertical transmission with time and additional egg laying, similar to what has been reported for West Nile virus . This finding suggests that survival and gonotropic cycles could be key determinants of success of vertical transmission in nature. Finally, our results demonstrate population-specific differences, with the FIR of the population from Argentina more than twice that of the population from Mexico , suggesting that particular populations may have increased capacity for maintenance through vertical transmission. Although we did not measure differences between ZIKV HND and ZIKV PR, evaluating additional strains could help clarify the influence of viral genotype on vertical transmission efficiency.\n\n【6】Together, these results indicate that Zika virus has a relatively high capacity for being transmitted vertically by both _Ae. aegypti_ and _Ae. albopictus_ mosquitoes. Although the mechanism of vertical transmission with flaviviruses is generally thought to be infection of eggs during oviposition, rather than transovarial transmission , these rates suggest that further investigation into Zika virus tropism in mosquitoes is warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4835c7d3-46c7-4056-ac30-64bb67fbbc01", "title": "Evaluating Temperature Sensitivity of Vesicular Stomatitis Virus–Based Vaccines", "text": "【0】Evaluating Temperature Sensitivity of Vesicular Stomatitis Virus–Based Vaccines\nEbola virus (EBOV; family _Filoviridae_ , genus _Ebolavirus_ ) and Lassa virus (LASV; family _Arenaviridae_ , genus _Mammarenavirus_ ) are prominent etiologic agents of viral hemorrhagic fever diseases in humans that have variable but typically high death rates . At least 20 major EBOV outbreaks have occurred in sub-Saharan Africa, including 2 during 2018–2019. During 2013–2016, the largest documented EBOV outbreak caused ≈28,000 cases and ≈11,000 deaths in several countries in West Africa . In response to the extent of the outbreak, the development of experimental vaccines was accelerated, culminating in a ring vaccination trial of the live-attenuated vesicular stomatitis virus–based EBOV vaccine (VSVΔG/EBOVGPC) . Based on the success of the trial, a similar strategy is being used in the 2018–2019 outbreak in the Democratic Republic of the Congo; preliminary results from the World Health Organization have identified the vaccine as >97% effective .\n\n【1】LASV, a rodentborne virus, is endemic to most of West Africa. Annually, ≈300,000–500,000 LASV infections occur. Most are acquired after direct contact with infected rodents or their contaminated excreta or secreta . Several large LASV outbreaks have occurred, most recently in Nigeria. During 2015–2018, prolonged and severe Lassa fever outbreaks were documented across most of the country .\n\n【2】VSVΔG/EBOVGPC and a similar VSV-based LASV vaccine, VSVΔG/LASVGPC, are leading candidates to help reduce illnesses and death from EBOV and LASV infections . Under manufacturer recommendations, these products are intended to be maintained at –80°C. Given the often remote locations where EBOV and LASV emerge, concern exists about maintaining such a rigorous cold chain to deliver the vaccines to areas where they are most needed. We evaluated the temperature sensitivity of VSVΔG/EBOVGPC and VSVΔG/LASVGPC in vitro and their ability to provide protection against lethal LASV infection.\n\n【3】### The Study\n\n【4】To evaluate the effects of prolonged and multiple breaks in the cold chain on titer and protective efficacy of VSVΔG/EBOVGPC and VSVΔG/LASVGPC, we thawed cryopreserved stock vials of experimental-grade VSVΔG/EBOVGPC and VSVΔG/LASVGPC and maintained them for 7 d at 4°C, room temperature (≈21°C), or 32°C. In addition, we freeze–thawed (°C to room temperature) a vial of each stock 3 times over 7 days (× freeze–thaw group). After incubation, we diluted the test vaccines according to the original stock titer and vaccinated groups of 9 outbred female Hartley guinea pigs (g) by the intraperitoneal route. A control group (animals per vaccine) comprising vaccination with 1 × 10 6  PFU of stock unmanipulated VSVΔG/EBOVGPC or VSVΔG/LASVGPC was included with each condition. A mock vaccination control group (animals per vaccine) received the equivalent dose of an irrelevant VSV-based Andes virus vaccine (VSVΔG/ANDVGPC). For each group of vaccinated animals, we randomly selected 6 for monitoring survival and 3 for timed necropsy and analysis when control animals demonstrated advanced signs of disease. All animal work was approved by the Canadian Science Centre for Human and Animal Health’s Institutional Animal Care and Use Committee and was conducted according the guidelines of the Canadian Council on Animal Care. All work with infectious materials was conducted in the National Microbiology Laboratory’s Biosafety Level 4 laboratory. After vaccination, the test and control vaccines were titered on Vero E6 cells using 10-fold serial dilutions and standard plaque assay methods.\n\n【5】Twenty-eight days after vaccination, we challenged the guinea pigs with 1,000 times the median lethal dose (LD 50  ; equivalent to 22 PFU) of guinea pig–adapted (GPA) EBOV, or 10 × LD 50  (equivalent to 1 × 10 4  50% tissue culture infectious dose \\[TCID 50  \\]) of GPA-LASV. After challenge, we monitored animals daily, recording body weights and monitoring temperatures using previously implanted IPTT-300 temperature transponders using a DAS 6002 hand-held scanner .\n\n【6】The in vitro titer resulting from vaccination with VSVΔG/EBOVGPC was relatively stable across all conditions; only the 32°C test group showed a significant decrease . This finding did not affect vaccine-derived protection from a GPA-EBOV challenge; we observed 100% survival across all conditions . After challenge, all vaccinated animals demonstrated no weight loss or increased body temperature . In contrast, sham-vaccinated animals (VSVΔG/ANDVGPC) demonstrated increased temperatures and decreases in body weights before reaching the study humane endpoints. Once the control animals achieved the humane endpoint, 3 animals per test group were euthanized to compare viremia and tissue titers. Vaccinated animals demonstrated significantly lower, and in many cases no, detectable infectious EBOV in tissue and blood samples .\n\n【7】VSVΔG/LASVGPC was similarly stable across all experimental groups but to a lesser degree in the 32°C group than in VSVΔG/EBOVGPC . Despite the relative stability, deaths occurred in most groups, including the positive control vaccinated animals . The exception was in the 3× freeze–thaw group, which maintained 100% survival. Consistent with these findings, we noted weight loss and increased body temperatures in all but the 3× freeze–thaw groups . Sham-vaccinated animals progressed as previously described after challenge, and infection was uniformly lethal within 18 days after challenge. Similarly, we noted infectious virus at varying levels in some or all samples tested in all groups .\n\n【8】### Conclusions\n\n【9】VSVΔG/EBOVGPC was surprisingly durable despite 7 days of suboptimal storage with no obvious clinical signs of disease and no deaths among animals across the treatment groups. We detected low-level viral titers in liver samples collected when the control animals were experiencing terminal disease. The 3× freeze–thaw condition resulted in detectable virus in both lung and spleen, although 100% of animals still survived. Curiously, VSVΔG/LASVGPC demonstrated improved performance under the same condition; however, a small sample size precludes any definitive conclusion.\n\n【10】The other temperature conditions had little effect on VSVΔG/LASVGPC in vitro titers. However, in most animals we noted clinical signs of disease, including lethargy, inappetence, moderate to severe increases in body temperature, and mild to moderate decreases in body weight. Consistent with these findings, experimental animals sampled when control animals were perimortem had infectious viral titers that were not significantly different from those of the control animals. Nevertheless, statistically significant increases in survival rates occurred in all experimental groups. Although 32°C proved more deleterious than other temperature conditions, the efficacy was similar to the control vaccination group. The GPA-LASV model is relatively new and has been adapted for maximum lethality in Hartley guinea pigs. VSVΔG/LASVGPC has proved to be extremely effective in both nonhuman primates and strain 13 guinea pigs when tested against several genetically and geographically distinct wild-type LASVs . This slight reduction in efficacy has been noted previously and might speak to the aggressiveness of the outbred guinea pig Lassa fever model . Follow-up studies in the inbred strain 13 guinea pig model, which is better characterized, could be considered .\n\n【11】Every effort needs to be made to ensure optimal storage and dosages of medical countermeasures to treat and prevent human disease. However, in remote and often tropical areas, maintaining these standards can be challenging, particularly if, in addition to climatic conditions, civil conflict is ongoing. Vaccine shortages are problematic, particularly during outbreaks, as demonstrated by the recent yellow fever vaccine shortage . Enhanced knowledge of vaccine stability under suboptimal storage conditions can help mitigate shortage issues by ensuring breaks in the cold chain do not necessarily translate into unusable vaccine lots. Our data demonstrate that the current –80°C cold chain condition might tolerate significant variances without affecting efficacy, at least in animals.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9fe5cbec-7aa0-42d8-8fde-5e43f962467c", "title": "Risk for Avian Influenza Virus Exposure at Human–Wildlife Interface", "text": "【0】Risk for Avian Influenza Virus Exposure at Human–Wildlife Interface\nThe emergence of highly pathogenic avian influenza virus (AIV) (H5N1) in domestic poultry in Asia with spillover infections in humans has raised concerns about the potential for a human pandemic . Although subtype H5N1 is the most well-known infecting strain, evidence of direct bird-to-human transmission has been documented for several other AIV subtypes .\n\n【1】Little is known about the types of exposure that result in human infections, especially with AIV being transmitted from wild birds and animals because only a few cases of transmission to humans have been documented . Overall, the types of exposures associated with the transmission of AIV to humans have been ingestion, inhalation of aerosolized virus, or direct contact through mucous membranes . The probability of infection with AIV varies with the activity and depends on the contact type (duration and route) and dose. Contacts for the general public are likely short and indirect, often occurring through outdoor activities, such as hiking, picnicking, or feeding birds. Contact for waterfowl hunters is especially intense and direct during bird-cleaning activities. Biologists and workers at wildlife hospitals have frequent and direct contact with wild birds and mammals. Biologists trap apparently healthy free-ranging animals and perform field necropsies, and rehabilitation workers handle sick and injured wild animals. In this study, we tested wild birds and marine mammals for AIV to determine the exposure risks associated with specific casual, recreational, and occupational activities that result in contact with wildlife.\n\n【2】### The Study\n\n【3】Human risk categories were created based on a typical contact type with wildlife: 1) casual (the general public), 2) recreational (waterfowl hunters), and 3) occupational (wildlife biologists, wildlife hospital workers, and veterinarians). Frequency of contact with AIV was estimated for each risk group by evaluating the prevalence of AIV among animals sampled opportunistically in each category. Surveillance for AIV was conducted from October 2005 through August 2007.\n\n【4】For casual contact, wild bird species (mostly periurban passerines such as sparrows, finches, and crows) were sampled to reflect typical daily exposures for the public . For recreational contact, birds were assessed by sampling hunter-killed waterfowl (mostly mallards, northern shovelers, gadwalls, green-winged teals, northern pintails, and American widgeons) at check stations in the Sacramento National Wildlife Refuge. For occupational contact, wild birds (seabirds, wading birds, waterfowl, raptors, and passerines) and marine mammals (seals and sea lions) admitted to 3 northern California wildlife hospitals were sampled. Cloacal samples were taken from birds and nasal and rectal samples from marine mammals with rayon-tipped swabs (MicroPur; PurFybr, Inc. Munster, IN, USA). Birds in recovery also had oropharyngeal samples taken. Swab samples were placed in viral transport media, transported within 24 hours from the site of collection to the University of California, Davis, in a cooler with ice packs and then transferred to a –70°C freezer for storage. A total of 9,157 samples were tested for AIV. Of these, 2,346 were screened by virus isolation in embryonating chicken eggs , and 6,811 were screened by real-time reverse transcription–PCR (RT-PCR) . All positive samples were tested for Eurasian H5 viruses .\n\n【5】The prevalence of AIV in each group was low (ranged 0.1%–0.9%) , and no samples were positive for Eurasian H5. We found that risk of contact with AIV-infected wildlife was 8 times higher for the recreational group compared to either the occupational or the casual group (p<0.0001; EpiInfo, Centers for Disease Control and Prevention, Atlanta, GA, USA).\n\n【6】### Conclusions\n\n【7】We did not detect AIV (H5N1) in California during October 2005–August 2007 nor did other surveillance efforts in the United States . We did detect other AIVs, although at a low prevalence (<1%). The prevalence of AIV in California wildlife was substantially lower than the prevalence reported in Alaskan wildlife in the same flyway . AIV prevalence may decrease with latitude , or this opportunistic sample design may have resulted in testing of species with a natural low prevalence. Although overall prevalence was low, it was highest in the recreational category and, coupled with the directness and intensity of the contacts especially during bird cleaning, this group would be expected to have the highest risk for infection. However, emergence or introduction of a virus that causes disease in wild birds or animals would likely result in a disproportional shift in prevalence of infection in wildlife brought to rehabilitation hospitals, thus making occupational contact more risky. As a recent example, 1 stork and 2 buzzards that were infected with AIV (H5N1) were brought to a wildlife hospital in Poland, which potentially exposed staff .\n\n【8】Novel transmission pathways are possible in places like wildlife hospitals because wild species that do not meet in nature are brought into close and extended contact with each other and humans. For example, marine mammals are susceptible to infection with AIV  and human influenza viruses  and have been documented as intermediate hosts . Other species may also be intermediate hosts for AIV, although they have not been identified. Those working in wildlife occupations should be encouraged to wear personal protective equipment when handling wildlife because of the types of contacts they can have and the potential for viruses to emerge in this setting. Similarly, personal protection should be recommended for waterfowl hunters because of the relatively higher prevalence of AIV in the birds with which they have contact.\n\n【9】We assessed the risk for human exposure to AIV by opportunistically sampling wildlife at the human–wild animal interface. A better measure of human risk would be to directly assess human exposure by testing for antibodies to all AIV subtypes that could occur in nature. Although it is not practical to simultaneously test for 144 virus subtypes, 2 serologic studies of persons exposed to wildlife showed antibodies to a limited number of AIVs . Since these exposures did not cause discernable illness, diagnosis based on clinical signs would likely underestimate infection.\n\n【10】Although our methods enabled us to compare exposure risk among different groups, the testing methods we used likely did not estimate the true AIV prevalence in wildlife. The real-time RT-PCR used in this study and in national surveillance efforts  has not been validated in wildlife , nor has virus isolation in embryonating chicken eggs, and it may be that neither method is perfect in detecting AIV in species that are only distantly related to chickens . Improved diagnostic methods are needed to assess AIV infections in wildlife species, and close monitoring of persons with the highest level of exposure to AIV is a necessary component of an early warning system to detect transmission from animals to humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "89280c67-e6c1-4b7f-b47f-b9ba5f753cef", "title": "Phylogenetic Analysis of Spread of Hepatitis C Virus Identified during HIV Outbreak Investigation, Unnao, India", "text": "【0】Phylogenetic Analysis of Spread of Hepatitis C Virus Identified during HIV Outbreak Investigation, Unnao, India\nHepatitis C virus (HCV) infection is a major public health concern worldwide and recognized as the leading cause of chronic liver disease, cirrhosis, and hepatocellular carcinoma (HCC). India, Pakistan, Georgia, and many countries in the western world have recorded high HCV prevalence among persons who inject drugs (PWID) . In addition, unsafe injection practices in therapeutic settings have been responsible for spread of HCV infection in the general population . HCV shares the same routes of transmission as HIV, although heterosexual sex is a less efficient one. Owing to the shared routes of transmission, about one third of persons living with HIV have been estimated to be coinfected with HCV in the United States, Europe, and some developing countries .\n\n【1】Increased detection of HIV among the attendees of the Integrated Counselling and Testing Center, located in the district hospital of Unnao, in the northern state of Uttar Pradesh, India, was reported in July 2017. A case–control investigation was initiated while 2 patients from Premganj Township of Bangarmau Block, Unnao, raised concerns that a local doctor (frequented mostly by persons from lower socioeconomic status) had been using a single syringe and needle to inject different persons. That investigation indicated that unsafe injecting practices in therapeutic settings were associated with HIV transmission and identified clustering around a monophyletic HIV-1 clade C. Serologic analyses revealed very high prevalence of antibodies to HCV among HIV-infected persons (%) and controls (%), suggesting a concerning level of spread of HCV infection in the study community, which was a serendipitous finding . Detection and molecular analysis of community HCV outbreaks is important because they can inform targeted public health interventions to interrupt HCV transmission. These initiatives are even more imperative as India aims to achieve the Sustainable Development Goal 3.3, which seeks to end viral hepatitis by 2030, as articulated in the National Health Policy . In this article, we describe the findings of molecular and phylogenetic analysis of HCV infections discovered during the HIV outbreak investigation in Unnao, India.\n\n【2】### Methods\n\n【3】##### Ethics Statement\n\n【4】We obtained approval for the investigation from the Ethics Committee of the Indian Council of Medical Research–National AIDS Research Institute (Pune, India). All study participants provided written informed consent.\n\n【5】##### Clinical Specimens and Data\n\n【6】We conducted molecular and phylogenetic analyses for 98 HCV seroreactive participants, including 70 with HCV and 28 with HIV–HCV co-infection. We enrolled these participants as part of our previous case–control investigation, which was undertaken to identify the factors associated with increased detection of HIV in Unnao District Hospital, Unnao, India, during September–December 2018, details of which are published elsewhere . In brief, case-patients (n = 33) were persons found to be HIV seroreactive during a 6-month period (November 2017–April 2018) from 3 locations (Premganj, Karimuddinpur, and Chakmeerapur) in the Bangarmau Block of Unnao District. Controls (n = 125) were persons who lived in the same geographic locale as case-patients and tested HIV serononreactive either in health camps or at the Integrated Counselling and Testing Centers where the cases were detected. Eighty-five percent (/33) of HIV-positive and 56% (/125) of HIV-negative study participants were HCV seroreactive. We used the HCV seroreactive specimens stored at −80°C for the molecular analysis for HCV infection, along with corresponding metadata pertaining to sociodemographic and laboratory data.\n\n【7】##### Viral Load Estimation\n\n【8】We performed HCV viral load testing by using the Xpert HCV viral load assay . In brief, we placed 1 mL serum into the Xpert cartridge, scanned it, and loaded it into the GeneXpert IV instrument. We recorded the results according to the manufacturer’s instructions.\n\n【9】We estimated HIV-1 viral load by using the Abbott Real Time HIV-1 assay , which uses reverse transcription PCR (RT-PCR) with homogenous detection of real-time fluorescence. We carried out automated sample preparation on the m2000sp instrument for RNA isolation by using magnetic microparticle technology and dispensed it to a PCR tray along with the amplification reagents. We then transferred the PCR tray to the Abbott m2000rt instrument for amplification and real-time detection according to the manufacturer’s instructions.\n\n【10】##### HCV RNA Extraction, NS5B and Core Gene Amplification, and Sequencing\n\n【11】We extracted viral RNA by using the NucliSens EasyMag total nucleic acid extraction system . We amplified the extracted RNA for the core and NS5B region of HCV. We amplified the core region with outer sense primer 5′-ACTGCCTGATAGGGTGCT TGC-3′; outer antisense primer 5′-ATGTACCCCATGAGGTCGGC-3′; inner sense primer 5′-AGGTCTCGTAGACCGTGCA-3′; and inner antisense primer 5′-CATGTGAGGGTATCGATGAC-3′ . We amplified the NS5B region with outer sense primer 5′-CNTAYGGITTCCARTACTCICC-3′, antisense primer 5′-GAGGARCAIGATGTTATIARCTC-3′, inner sense primer 5′-TATGAYACCCGCTGYTTTGACTC-3′ and inner antisense primer 5′-GCNGARTAYCTVGTCATAGCCTC-3′ . We used the Onestep RT-PCR kit  for cDNA synthesis. We carried out the outer PCR in 25 µL reaction containing 4 µL 5× PCR buffer, 1 µL 2.0 mmol/L dNTP, 4.5 µL RNase-free water, 2 µL each sense and antisense primers, 1 µL RT-PCR enzyme mix, 0.5 µL RNase out , and 10 µL extracted RNA (treated with heat at 65°C for 30 s for core and 42°C for 5 min for NS5B) with PCR conditions: 50°C for 35 min; 95°C for 15 min; 95°C for 20 s, 55°C for 45 s, 72°C for 2 min, 35 cycles; and 72°C for 10 min. We carried out the inner PCR in 25 µL reaction containing 2.5 µL 10× PCR buffer, 1 µL 2.5 mmol/L dNTP, 11 µL water, 1 µL each sense and antisense primers, 0.5 µL Taq enzyme, and 5 µL template cDNA with PCR conditions: 95°C for 1 min, 95°C for 15 s, 56°C for 45 sec, 72°C for 1 min, 30 cycles; and 72°C for 5 min. We verified the PCR products by gel electrophoresis. We sequenced all amplified products on the Applied Biosystems 3130XL genetic analyzer . We then performed sequence assembly and base-calling with SeqScape 2.7 software (ThermoFisher), using the FASTA files generated for further analysis. We submitted nucleotide sequences to GenBank (accession nos. MW675966–MW676030 for the core gene and MW675899–MW675965 for the NS5B gene).\n\n【12】##### HCV Genotyping and Phylogenetic Analysis\n\n【13】We genotyped partial sequences of core and NS5B genes of HCV samples from persons with HIV–HCV coinfection (labeled as i) and HCV monoinfection (labeled as ic) by using GenomeDetective  and HCV-Blast . We performed multiple sequence alignments by using the prototype sequence of H77 isolate  belonging to genotype 1, using MAFFT . We determined the exactly identical sequences by using the CD-HIT web server . We then extracted core and NS5B genes belonging to India and global HCV isolates from GenBank  by using blastn . We carried out recombination detection by using RDP4  and then reconstructed phylogenetic trees for core and NS5B sequences by using the maximum-likelihood method as implemented in IQTREE . We used partial sequences of core and NS5B genes belonging to other India and global isolates, which clustered with Unnao samples for evolutionary analysis. We used BEAST version 1.10.4 to determine time to most recent common ancestor (tMRCA) .\n\n【14】We used the generalized time reversible model as a nucleotide substitution model with relaxed clock and log-normal distribution and coalescent constant growth as demographic model for tMRCA calculation. We carried out Markov Chain Monte Carlo simulations for 100 million steps and sampled every 10,000 steps. We used Tracer 1.6  for assessing convergence and iTOL  and FigTree  for visualization of phylogenetic trees. To generate information on drug-resistant mutation substitutions observed in NS5B protein sequences, we mapped to sofosbuvir and ribavirin drug-resistant variants .\n\n【15】##### Statistical Analysis\n\n【16】We compared the means between 2 samples by using an independent _t_ test. We used Pearson’s correlation analysis to determine whether the values of 2 variables were associated. We considered a probability level lower than the conventional 5% (p<0.05) as statistically significant. We assessed the association between dependent variable and cofactors by using Pearson's χ 2  or Fisher exact tests as appropriate.\n\n【17】### Results\n\n【18】##### Participant Profile\n\n【19】We profiled persons with HCV monoinfection (n = 70) and those with HIV–HCV co-infection (n = 28) ; sex, area of residence, occupation, and sexual risk distribution were not significantly different between the 2 groups. We observed statistically significant differences between the 2 groups with regards to age and receipt of injection during treatment-seeking within the last 5 years.\n\n【20】We compared the characteristics of HCV reactive (n = 98) and nonreactive (n = 60) persons and observed that a statistically significant higher proportion of HCV reactive persons reported being exposed to used syringes and needles while seeking treatment and were HIV seropositive compared with HCV nonreactive persons. We did not observe significant differences with regards to age, sex, area of residence, occupation, and sexual risk distribution between the 2 groups . None of the study participants reported ever injecting drugs for nonmedicinal or recreational purposes.\n\n【21】##### Association between HCV and HIV Viral Load\n\n【22】We tested all HCV antibody-positive specimens for HCV viral load. Samples collected from 2 participants with HIV–HCV co-infection (identified by presence of antibody) and 3 with HCV monoinfection (seroreactive) had insufficient volume. Thus, we used specimens from 26 participants with HIV–HCV co-infection and 67 participants with HCV monoinfection for viral load testing. Eighty-eight percent (/26) of specimens from HIV–HCV coinfected persons and 73% (/67) specimens from HCV monoinfected participants had detectable HCV RNA (chronic infection). HCV RNA load in participants with HIV–HCV co-infection was (mean \\+ SD log IU/mL) 5.93 \\+ 0.91 and HCV monoinfection was 5.46 \\+ 1.09; the difference was not statistically significant (p = 0.07). HCV RNA load did not differ significantly in HIV-positive persons with detectable (.06 \\+ 1.15) and undetectable (.85 \\+ 0.76) viral load (p = 0.65) . We did not observe any correlation between HIV and HCV viral load (r = 0.0567; p = 0.88).\n\n【23】##### Genetic Variability in the HCV Core and NS5B Genes and HCV genotypes\n\n【24】We obtained a total of 67 (with HIV–HCV coinfection and 44 with HCV monoinfection) and 65 (with HIV–HCV coinfection and 43 with HCV monoinfection) sequences belonging to core and NS5B regions. The partial core sequence mapped to 394–722 (bases) and NS5B gene mapped to 8,196–8,647 (bases) of H77 prototype. We observed the extent of sequence similarities in the core HIV–HCV coinfection was 98.15%, in HCV monoinfection was 96.9%, and in combined datasets of the sequence similiarity was 96.6%. Similarly, for the NS5B sequence similarities in HIV–HCV coinfection was 98.9%, HCV monoinfection was 98.03%, and in combined datasets of the sequence similarity was 97.36%. All the isolates from Unnao belonged to genotype 3a regardless of HIV co-infection status. No recombination was observed in the regions of NS5B and core genes sequenced.\n\n【25】With reference to the H77 prototype, we obseved a total of 71 nucleotide and 12 amino acid substitutions in the core protein, whereas the NS5B protein showed 70 nucleotide and 43 amino acid substitutions in Unnao isolates. Of the 12 amino acid substitutions in the core protein, 6 (N16I, L36V, R70Q, P71S, T75S, and T110N) were part of highly variable sites in other India isolates. Of the 43 NS5B substitutions, 6 were associated with resistance to the known drugs ribavirin and sofosbuvir. The substitutions Q309R (observed in all Unnao isolates) and R345S (observed in 4 isolates in the HCV monoinfection group) are known to be associated with resistance to ribavirin. We also observed the substitutions A207M, S218F, C289F, and A333R (associated with sofosbuvir resistance) in all isolates.\n\n【26】##### HCV Phylogenetic and Evolutionary Analysis\n\n【27】The phylogenetic trees derived using partial sequences of NS5B and core genes of the Unnao isolates showed clustering of isolates with HIV–HCV co-infection and HCV monoinfection, indicating lack of distinct clusters pertaining to HIV status . In the phylogenetic tree derived for NS5B gene using global sequences that include India HCV isolates from other studies as well as the Unnao isolates, Unnao isolates were observed to form a monophyletic cluster . The closest branch joining the Unnao isolates includes 1 India isolate , 3 isolates from the United States (GenBank accession nos. DQ430819–20, isolated during 2000, and AY956467, isolated in 2002), and 2 isolates from the United Kingdom (GenBank accession nos. GQ356207 and GQ356217, isolated during 2006). In the phylogenetic analysis using the core gene, Unnao isolates were part of 3 clusters. Most Unnao isolates (of the total 65) formed a distinct cluster. The second cluster included 1 Unnao isolate ic31460 and 5 HCV isolates (GenBank accession nos. MT953835 and MT953776, isolated during 2019 from India, and GenBank accession nos. KY620638, KY620807, and KY620806, isolated during 2014 \\[country of isolation is not available\\]). The third cluster included Unnao isolates i10711, i30501, i10911, i30071, i20851, i10811, ic31220, and ic31370 and other India isolates (GenBank accession nos. MN697780, isolated during 2016, and MN697854 and MN697827, isolated during 2017) from a previous study  .\n\n【28】We estimated tMRCA for Unnao isolates by using the partial sequences of NS5B genes of HIV–HCV co-infection and HCV monoinfection, together with the India and global isolates that are observed in close proximity in the phylogenetic tree. tMRCA for Unnao was observed to be 2012 . The nucleotide substitution rate of partial sequences of the core gene belonging to Unnao and global isolates is 2 × 10 −3  substitutions/site/year (% highest posterior density interval 1.16–3.54 × 10 −3  ) and for the NS5B gene (that clustered with Unnao isolates) is 6 × 10 −3  substitutions/site/year (% highest posterior density interval 1.28–8.46 × 10 −3  ).\n\n【29】### Discussion\n\n【30】We describe the investigation of a HIV–HCV dual outbreak in a general community setting from India by means of sequencing of partial regions of 2 HCV genes; namely, NS5B (nonstructural) and core (structural). The NS5B is relatively conserved and used as a marker for genotyping . The isolates sequenced from participants with HCV monoinfection and co-infection (HIV–HCV) were characterized by using phylogenetic analysis to understand the circulating genotypes, time of introduction of HCV in the community, and the functional significance of substitutions, if any.\n\n【31】Phylogenetic analysis revealed the presence of genotype 3a in all samples. This genotype is the most commonly reported from the state of Uttar Pradesh and the neighboring regions . All HCV sequences from Unnao shared high sequence similarity (>96%) regardless of HIV infection status. The clustering pattern observed in the phylogenetic tree derived from NS5B suggests monophyly and thereby indicates that the Unnao isolates are descendants of 1 (or many) closely related 3a isolates that are circulating in the community. Similarly, the estimated nucleotide substitution rates reported in our study are not only in agreement with previous reports  but also corroborate with the timeline of the HIV outbreak reported. Thus, the phylogenetic clustering supports the observation pertaining to the use of unsterile injection equipment as a potential route of entry of HCV in the study setting . The phylogenetic tree of core sequences from Unnao showed that 1 of the 3 observed clusters consisting of 8 sequences (i and 2 ic) group with isolates reported in a previous study describing HCV diversity among HIV-infected persons . However, clustering proximity of Unnao isolates with isolates from recreational drug users (based on core region) is insufficient  and therefore the use of recreational drug abuse can be ruled out in the Unnao outbreak . None of the study participants reported injecting drugs for nonmedicinal or recreational purposes. Thus, the exceptionally high rate of HCV infection in Unnao might be explained by exposure of the local community (mostly agrarian and belonging to lower socioeconomic status) to unsafe injection practices during healthcare seeking; an assertion supported by the observation of 2 patients, which prompted the case–control study described previously and earlier reports documenting reuse of injection equipment in nearby rural and urban settings .\n\n【32】HIV is reported to accelerate the progression of liver disease in HCV-infected persons because HCV replication increases in the presence of HIV, resulting in elevated HCV RNA levels . We did not observe correlation between HIV and HCV RNA load, nor was any specific clustering of HCV sequences based on HIV serostatus noted. Thus, it appears that the influence of duration of HIV infection was not sufficiently strong and probably succeeded (rather than preceded) the highly evolved or adapted HCV that circulated in defined clusters.\n\n【33】We observed certain mutations associated with ribavirin and sofosbuvir resistance; these drugs form the main component of HCV treatment regimens in India . The low detection rate of resistance mutations in the isolates corroborates with the HCV treatment–naive status of the community in concern. Knowledge of baseline mutations is important for predicting potential response to antiviral therapy and will help to determine local policies for HCV treatment.\n\n【34】The first limitation of our study is that it was not primarily designed as an HCV outbreak investigation and hence was restricted in terms of the samples tested for HCV phylogeny. The nature of the investigation did not enable us to link HCV infection to any specific source. HCV RNA testing was performed after the detection of the HIV outbreak in Unnao. Hence, we could not define the time between infection and the onset of viremia. A similar outbreak investigation in the Roka rural commune in Cambodia showed that concurrent spread of HIV and HCV was linked with unsafe injecting practices in therapeutic settings .\n\n【35】The ongoing HCV epidemic we uncovered in our investigation highlights the need for HCV surveillance in the area and in neighboring districts. Furthermore, after our investigation, an antiretroviral treatment center was established at the Bangarmau Community health center, the coordination site for the case–control investigation described previously, to ensure access of local residents to testing, treatment, and follow-up services.\n\n【36】In conclusion, characterization of NS5B and core sequences with HCV monoinfection and HIV–HCV co-infection using phylogenetic analysis not only substantiates our previous results  but also indicates a potential HCV outbreak in the community from a single isolate or its closely related descendants in Unnao, India. The HIV–HCV dual epidemic in a general community setting highlights the need for systematic surveillance and integrated approach combining hepatitis testing along with HIV for early detection and timely interventions to arrest transmission. We reiterate the need for adhering to good infection prevention and control practices, deploying single-use injection equipment (preferably autodisabled ones), strengthening healthcare delivery systems, and empowering the community to reduce the further spread of HIV and HCV. Molecular analyses of all HCV outbreaks is warranted to decipher the circulating genotypes, genetic diversity, and transmission dynamics. These findings in turn will provide valuable inputs to the National Viral Hepatitis Control Program in planning targeted interventions for mitigating the spread of HCV in India.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b3dd201e-bb9f-4516-8f21-357b1b3d9158", "title": "Parallels and Mutual Lessons in Tuberculosis and COVID-19 Transmission, Prevention, and Control", "text": "【0】Parallels and Mutual Lessons in Tuberculosis and COVID-19 Transmission, Prevention, and Control\nIn addition to having devastating effects on the economies of the world, the pandemic of coronavirus disease (COVID-19) itself and the responses entailed in containment and mitigation efforts could have disastrous consequences for existing public health programs, with the impacts being most pronounced in high-burden, low-income settings . Modeling of the impact of the COVID-19 pandemic conducted by Imperial College London (London, UK) suggests that in high-burden settings, disease-related deaths over 5 years might be increased by up to 10% for HIV, 20% for TB, and 36% for malaria .\n\n【1】To minimize the adverse consequences of COVID-19 on overall public health services, synergies between COVID-19 response and traditional public health programs should be sought and the lessons and resources developed in any of the programs should be used for the benefit of the others. In this regard, approaches to TB control might hold lessons for the public health response to COVID-19 and vice-versa.\n\n【2】### Synergies and Commonalities for COVID-19 and TB\n\n【3】Several commonalities exist between COVID-19 and TB, most notably transmission of their etiologic agents, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and _Mycobacterium tuberculosis_ . Both pathogens are transmitted through secretions from the respiratory tract . Moreover, protecting healthcare workers and other susceptible patients, and contact identification and evaluation are key components of the public health response to both infections. An understanding of the routes of and factors influencing transmission is necessary to develop effective and efficient measures to control the diseases. For TB, many years of clinical and experimental studies have provided a wealth of information on which to base contact identification, prioritization, and evaluation . Investigations of TB outbreaks have been especially informative . Not surprisingly, this level of understanding of SARS-CoV-2 transmission does not exist, and the relative contributions to transmission of large respiratory droplets, fomites, and aerosols remain controversial . Notably, transmission of both pathogens has been associated with superspreader events .\n\n【4】The clinical manifestations of COVID-19 were initially described as mainly involving the respiratory tract, with cough as a predominant symptom along with fever, but knowledge of its full natural history, with both immediate and potential long-term consequences, is still increasing rapidly . Both the degree of infectiousness and the severity of SARS-CoV-2 infection dictate rapid and effective implementation of healthcare facility infection prevention and control to minimize transmission. These measures include administrative, engineering, and personal measures (i.e. personal protective equipment) and community-based public health activities, even without strong empirical evidence on which to base these interventions.\n\n【5】### Seeking COVID-19 Mitigation and Control Strategies\n\n【6】Unquestionably, the package of community-based mitigation measures put into place for the current pandemic has had a major effect in reducing cases and deaths, as shown by Hsiang et al. However, uncertainties remain concerning the most effective individual or combinations of measures. These uncertainties preclude the ability to readily identify more targeted and efficient control strategies. Thus, an urgent need exists for a more detailed understanding of SARS-CoV-2 transmission routes and patterns.\n\n【7】Of particular importance is the implementation of monitoring and rapid case identification as current mitigation measures are relaxed. General agreement exists that rapid case identification through PCR-based testing quickly followed by contact identification and evaluation (generally called contact tracing in the context of COVID-19) is the key strategy in reducing transmission in settings where the epidemic curve is flattened or declining . Earlier in the pandemic, after the spring 2020 surge subsided, this approach, which closely resembles strategies used for TB, was being scaled up and implemented rapidly. The core actions involve identifying persons with the disease (index case-patients) and identifying and evaluating persons exposed to the index case-patient (contacts) to find additional cases and offer contacts preventive interventions. However, rapid increases in cases in late fall and winter 2020 made contact tracing impractical, simply because of volume. Now, as the pandemic wanes, a trend that we hope will continue, contract tracing is again becoming feasible.\n\n【8】### Value of Contact Identification and Evaluation\n\n【9】Contact identification and evaluation have been key components of TB-control measures in most low TB–incidence countries for at least the past 75 years, and a strong scientific basis exists for most, but not all, elements of this activity . Although the same information and approaches apply in generally resource-poor, high TB–incidence countries and although international guidelines exist, implementation of routine contact investigations has been very limited . In the setting of TB, effective contact investigations have addressed stigma, community engagement, training of interviewers, and use of specific operational guidelines . These same elements will likely prove crucial to the effectiveness of COVID-19 contact tracing.\n\n【10】At least 3 important differences exist between factors that should be considered when engaging in contact identification and evaluation for COVID-19 compared with TB. First, because of the short interval between exposure and disease onset, estimated to be a median of 4.1 days for COVID-19, the timeframe for contact identification and evaluation is much shorter than for TB . In addition, infection with _M. tuberculosis_ in immunocompetent hosts most commonly results in latent infection, which can last decades and in most cases never progresses to active TB disease. Second, persons with COVID-19 are most infectious in the immediate presymptomatic and early symptomatic phases, when the viral titers are at their peak, again indicating the need for speed in the contact process for maximal effectiveness . Third, SARS-CoV-2 clearly is transmitted from person to person predominantly through respiratory secretions that may be inhaled, settling on the mucosal lining of large airways, or be self-inoculated onto nasal mucosa or into the eyes . Unlike TB, the droplets with the SARS-CoV-2 viral cargo might also contaminate and persist on surfaces, although the role played by surface or fomite transmission is not well-quantified . However, increasing controversies and concerns exist as to the relative contribution of aerosols to overall transmission .\n\n【11】### The Role of Droplet Nuclei and Acquisition of Infection\n\n【12】_M. tuberculosis_ is transmitted nearly exclusively by aerosolized droplet nuclei, particles <5 µm in aerodynamic diameter . Large droplets per se are not effective vehicles for transmission of _M. tuberculosis_ ; however, as the water content of large droplets evaporates, droplet nuclei are formed. The closeness and duration of exposure to a person with infectious TB, as well as the ventilation of the space in which the exposure occurs, influence the likelihood of transmission. Nevertheless, TB outbreaks have been documented with more casual exposures in churches, schools, nursing homes, prisons and jails, and long airplane flights, as well as in other congregate settings, many of which have also been locations of documented SARS-CoV-2 transmission .\n\n【13】Direct and indirect evidence that SARS-CoV-2 may also be transmitted by aerosols with droplet nuclei (i.e. fine particles that remain suspended in air) carrying infectious particles  is increasing. A description of an outbreak of COVID-19, associated with a restaurant in Guangzhou, China, strongly suggested transmission through an airborne route , as did case distribution and additional studies of air circulation, also in this restaurant in Guangzhou .\n\n【14】For both TB and COVID-19, cough is a predominant symptom, and airborne droplets are produced by any forced expiratory maneuver, especially coughing; at least for TB, the severity of cough is an indicator of transmission risk. For TB, several additional indicators assist in quantifying the risk for transmission from the index case and, thus, in assigning priority to a contact investigation. These indicators include the bacillary burden, as indicated by the radiographic extent of the disease in the lungs and the presence or absence of cavitary lesions and qualitative sputum smear positivity . No such assessment is routinely used for COVID-19, although quantification of viral load in nasal or pharyngeal swab specimens and an assessment of the severity and duration of respiratory symptoms could provide such information . Reduction in viral inoculum by widespread wearing of masks has been postulated to result in less severe manifestations of SARS-CoV-2 infection .\n\n【15】For TB, because of the increasing risk for acquisition of infection with the closeness and duration of exposure to persons with this disease, contact evaluation can be structured, beginning in the home, workplace, or school, and places of leisure and working outward in a manner that conceptually resembles concentric circles. The number and percentage of close contacts with evidence of disease, or recent infection, inform the need to expand the investigation to contacts in outer ring circles. This iterative approach optimizes the use of resources for investigations and testing . For SARS-CoV-2, data strongly suggest that the virus is highly transmissible even with casual contact, so the duration of exposure might not be relevant .\n\n【16】All of the foregoing indicates that in conducting contact identification and evaluation for persons exposed to persons with COVID-19, a wide net must be cast. Moreover, given the incubation period and pace of the disease, the process must be accomplished much more quickly than is necessary for TB. Unfortunately, much of the knowledge base that is used to guide TB contact identification and evaluation does not yet exist for COVID-19. To generate the necessary information, investigators studying the epidemiology of COVID-19 and, in particular, those charged with investigating outbreaks and conducting contact tracing, should be certain that the data being collected will enable analyses directed toward identifying factors that influence viral transmission. A recent report of nationwide contact tracing for COVID-19 in South Korea indicated both the need to investigate »10 contacts per index case and that 11.8% of household contacts had COVID-19, >6 times the 1.9% prevalence of COVID-19 in nonhousehold contacts .\n\n【17】### Using the Investigation of TB on the USS Byrd as a Template\n\n【18】Essentially all infection control and public health measures for TB are based on the understanding, backed by strong empirical and experimental evidence, that _M. tuberculosis_ is transmitted nearly exclusively by aerosols . Some of the strongest evidence of _M. tuberculosis_ transmission through aerosols has been derived from several TB outbreak investigations. Perhaps the most notable and informative outbreak investigation was conducted in response to a single crew member who was found the have cavitary pulmonary TB during the course of a long sea tour by the US Navy vessel the USS Richard Byrd in 1965 . A thorough assessment of the patterns of air circulation and their relationship to new cases and infections was conducted aboard the ship. The investigation found that all new cases and infections occurred in crew members who had either direct personal contact with the index case-patient or were exposed through recirculated air in a closed ventilation system. The investigators were able to establish what might be viewed as a dose-response curve based on the exposure to different amounts of recirculated air and the proportion exposed crew members who were infected . Of particular note, several of the newly infected sailors (indicated by a new positive tuberculin skin test) who were asymptomatic and had negative chest radiographs were found to have _M. tuberculosis_ in their sputum, raising the possibility of transmission from persons without the usual symptoms of TB, as is the case with COVID-19 . This finding is consistent with findings from national TB prevalence surveys of a substantial proportion of study subjects who were found to have _M. tuberculosis_ in their sputum but had no symptoms (e.g. cough >2 weeks) .\n\n【19】Outbreaks of COVID-19 on a cruise ship (Diamond Princess) in late January 2020 and the USS Theodore Roosevelt in March 2020 provide unique opportunities, similar to those provided by the USS Byrd, to gain a more detailed understanding of transmission patterns for SARS-CoV-2. To date, published assessments of COVID-19 outbreaks in these 2 separate settings consist of initial assessments, 1 documenting the occurrence of 700 cases of COVID-19 among nearly 3,700 passengers and crew members in the cruise ship . The investigation identified that 15 of 20 cases in crew members were in food workers, and 16 of these 20 persons slept in cabins on deck 3. No details were provided for the distribution of COVID-19 cases in passengers, nor of the ventilation system in this cruise ship . A follow-up assessment was limited to 215 Hong Kong passengers after quarantine and disembarkation; 9 tested positive for SARS-CoV-2 . No berthing information is available for those passengers. The USS Roosevelt outbreak investigation was a serostudy of a convenience sample of 382 crew members . Although the sample was not representative of the entire crew, 60% of the participants had antibodies to SARS-CoV-2, indicating prior infection. Notably, 20% of the seropositive group denied having symptoms. Also, as is the case with asymptomatic TB, the degree to which these asymptomatic persons transmitted the infection is not known. Examination of crew member duty rosters and assessment of ventilation patterns in areas inhabited by infected and noninfected persons could provide important information concerning aerosol transmission and the role of spread of the virus by asymptomatic persons. Although the outbreak on the USS Byrd occurred >50 years ago, its assessment is a model for advancing knowledge by thorough investigations, including environmental studies to examine the role of air circulation. With increasing speculation and uncertainty about basic questions such as relative importance of different transmission modes for SARS-CoV-2 , the Diamond Princess and USS Roosevelt outbreaks present opportunities, similar to that provided by the USS Byrd, that should not be overlooked.\n\n【20】As noted, although contact identification and evaluation are widely used in high-income, low TB–incidence countries, implementation is limited in low- and middle-income countries. Given the experience with TB, considerable patience, skill, and ingenuity are needed in the implementation of contact tracing for COVID-19. Digital and other automated technologies have been applied to COVID-19 contact tracing in different country settings . This new thinking, coupled with innovative tools, will likely hold lessons and examples for improvements in TB prevention and control.\n\n【21】### Avoiding Past Mistakes and Seizing Present Opportunities\n\n【22】In response to COVID-19, countries are having to reassign or recruit and train staff, as well as to establish a robust laboratory diagnostic testing capacity to deliver timely quality-assured results. Early reports from the United States have documented that the COVID-19 response has diverted resources away from essential TB services . This scenario must be avoided; investments required should be used to improve all public health programs and be sustained over time. Thirty-five years ago, TB provided a dramatic example of the impact of inattention to, and disinvestments in, basic public health programs. During 1985–1992, a reversal of longstanding downward trends occurred as well as and 20% increase in cases .\n\n【23】We now have a rare opportunity to seize the moment and use the attention garnered by this novel virus pandemic to ensure that new investments contribute not only to the control of COVID-19, but also to the strengthening of older, yet very relevant public health programs, and to recognize that lessons learned from those programs benefit those at risk for COVID-19. In the United States and in other parts of the world, TB served as the impetus for the establishment of public health programs, and these programs were geared to deal with TB as a public health problem . Public health approaches to COVID-19, relying as they do on accelerated responses, digital technologies, and large numbers of trained community-based contact investigators, could establish a new more comprehensive paradigm for the public health programs of the future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5c71a59c-dcee-4aba-b31a-3a3a8ebedf47", "title": "Nipah Virus Infection Among Military Personnel Involved in Pig Culling during an Outbreak of Encephalitis in Malaysia, 1998-1999", "text": "【0】Nipah Virus Infection Among Military Personnel Involved in Pig Culling during an Outbreak of Encephalitis in Malaysia, 1998-1999\n**To the Editor:** An outbreak of severe encephalitis affecting 265 patients, 104 (%) of whom died, occurred during 1998-1999 in Malaysia. It was linked to a new paramyxovirus, Nipah virus, which infects pigs, humans, dogs, and cats . Nipah virus is most closely related to Hendra virus , which was discovered in Australia in 1994 during an outbreak of severe respiratory disease among horses and humans . Most patients in Malaysia were pig farmers, and human infection was linked to exposure to pigs . An operation to cull the approximately 1 million pigs on farms in the outbreak-affected areas was carried out, primarily by 1,638 military personnel. After two soldiers involved in culling became ill with Nipah encephalitis, we conducted a cross-sectional survey of military personnel participating in culling activities in the outbreak-affected states of Malaysia (Negeri Sembilan and Selangor) to assess the prevalence of Nipah infection.\n\n【1】The survey was conducted approximately 2-4 weeks after the end of all culling activities to control the outbreak. All military personnel, enlisted and officers, who had been assigned to culling duty in the states of Negeri Sembilan and Selangor were invited to participate, regardless of specific job assignment. Study teams visited the military bases in each state and administered a survey asking about illness, specific exposures and activities during culling, use of protective equipment such as gloves, and other pig exposures not associated with culling. A single serum specimen obtained at the time of the interview was tested for the presence of immunoglobulin (Ig) M and IgG antibodies against Nipah virus by enzyme immunoassays (EIA). IgM antibodies were detected by an IgM-capture EIA and IgG antibodies by an indirect EIA. Hendra virus antigens, which cross-react with antibodies against Nipah virus, were used in the serologic assays. In limited laboratory comparisons with Hendra virus and Nipah virus antigens, a good correlation was observed between the IgG and IgM EIA results.\n\n【2】Of 1,474 military personnel listed in the records of the military bases where the survey was performed, 1,412 (%) responded to the survey and provided serum specimens. The mean age of the participants was 28.3 years (range 19 to 50 years). On average, the soldiers participated in culling for 8.2 days (range 1 to 60 days), for 7.4 hours per day (range 0.5 to 18 hours per day), and at 86 farms (range 1 to 696 farms). During culling, 63% reported physical contact with live pigs and 30.9% with dead pigs. The most common activities reported by the soldiers included shooting pigs (.2%); herding, hitting, or carrying live pigs (.5%); changing rifle magazines (.4%); carrying dead pigs (.8%); and spraying lime over pig burial sites (.6%). More than 80% reported wearing gloves, masks, and boots, and 31% reported wearing goggles at all times during culling.\n\n【3】Six (.4%) of the 1,412 personnel studied had detectable antibody against Nipah virus. All six had IgM antibody, and one also had IgG antibodies. Two of the six antibody-positive persons had been hospitalized with encephalitis during the culling operation. All the antibody-positive personnel were involved in culling in Negeri Sembilan state and reported direct physical contact with live pigs; none reported obvious contact with secretions or body fluids (e.g. blood and urine) of infected pigs. Four of the six antibody-positive persons also reported direct physical contact with dead pigs. All six reported wearing gloves, masks, and boots at all times while working, and three reported wearing goggles.\n\n【4】Comparison of exposures and activities among personnel involved in culling in Negeri Sembilan state (N = 960) and Selangor state (N = 497) showed that the former were more likely to report direct physical contact with both live pigs (.4% vs. 57.0%, respectively, p < 0.001) and dead pigs (.7% vs. 11.2%, respectively, p < 0.001). However, the reported prevalence of sick pigs on farms where pigs were culled did not differ between the two groups (.6% versus 72.9%, respectively, p = 0.50). No significant differences were observed in use of personal protective equipment (gloves, boots, and mask) for those in Negeri Sembilan compared with Selangor, except for wearing goggles (.8% versus 76.4%, respectively, p < 0.001).\n\n【5】Our findings indicate that transmissibility of Nipah virus to military personnel involved in pig culling was low. Four of the six infected persons were apparently well; follow-up of these and the other infected soldiers will be important to determine if any symptoms of disease or long-term sequelae develop. The observation that all the infected persons reported direct contact with live pigs is consistent with the hypothesis that transmission of Nipah virus to humans most likely occurs from close contact with infectious secretions or body fluids of pigs. Respiratory secretions and urine of infected pigs have been shown to contain Nipah virus and may be vehicles of transmission .\n\n【6】We could not document the route of infection for the antibody-positive personnel. Although all six antibody-positive persons reported wearing gloves, masks, and boots while culling, three did not report wearing goggles. Exposure may have occurred through inoculation of the conjunctiva with infectious secretions of pigs; however, bias in reporting use of other protective equipment should also be considered. Given the great severity of Nipah encephalitis and the possible, although small, risk of transmission of virus to military personnel involved in culling, we recommend great care in handling potentially Nipah-infected pigs during such operations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db8268a0-ed23-4f8b-8536-5e839cb4d519", "title": "Chronic Genotype 3 Hepatitis E in Pregnant Woman Receiving Infliximab and Azathioprine", "text": "【0】Chronic Genotype 3 Hepatitis E in Pregnant Woman Receiving Infliximab and Azathioprine\nHepatitis E virus (HEV) genotype 1 causes a high number of deaths of pregnant women in developing countries . The few reported cases of HEV during pregnancy in industrialized countries  mainly relate to acute genotype 3 infection. We report the course of autochthonous chronic genotype 3c  hepatitis E in a pregnant woman in France.\n\n【1】The patient, 27 years of age, was receiving immunosuppressive therapies for ulcerative colitis and became pregnant during the infection and treatment. At symptom onset, she had received infliximab and azathioprine for >5 years and reported eating undercooked meat; she had not traveled abroad. Prolonged elevated alanine aminotransferase (ALT) since May 2014 led her physician to suspect viral hepatitis; HEV infection was later diagnosed in September 2014 by detection of HEV IgM and RNA in plasma . We retrospectively tested previous blood samples from this patient, routinely stored in the hospital virology laboratory, and found them to be negative for HEV IgM and RNA.\n\n【2】Persistence of HEV has not been reported among patients receiving infliximab or azathioprine. However, HEV persistence was reported in a patient receiving azathioprine combined with oral steroids  and in a pig model of HEV chronicity under combined cyclosporine/azathioprine/methylprednisolone . On the basis of those reports, we reduced the patient’s dose of azathioprine to 100 mg/d and that of infliximab to 5 mg/kg/d every 8 weeks in November 2014 , but infection did not resolve. She became pregnant shortly thereafter.\n\n【3】During the patient’s pregnancy, viral loads ranged from 5.7 to 6.8 log 10  copies/mL, and ALT returned to reference range . We discontinued infliximab at the beginning of the third trimester . Viral load increased by >1 log 10  copies/mL, and ALT remained within reference range. She gave birth by vaginal delivery at 40 weeks of amenorrhea. On the day of delivery, 3 months after infliximab discontinuation, viral load peaked (.9 log 10  copies/mL, panel A). Although viral load was high during pregnancy, the infant was not infected and was in good health; HEV RNA was undetectable in cord blood (the placenta was not available for evaluation), and neither HEV IgM nor RNA were found in the newborn’s plasma 2 days after birth.\n\n【4】After delivery, testing of the mother’s plasma showed cytolysis (ALT >3× upper limit of reference range) and a >3-log decrease of HEV RNA . We reintroduced infliximab 3 weeks after delivery, at which time HEV RNA was lower than during pregnancy but still detectable (.5 log 10  copies/mL, panel A). At 2 months after delivery, hepatic cytolysis resolved; 2 months later, HEV became undetectable. No relapse was noted during subsequent follow-up (the last PCR performed in August 2017 was negative).\n\n【5】Because of the high rate of severe acute hepatitis E reported in pregnant women in developing countries, we monitored the patient for negative outcomes during gestation but found none. This finding is consistent with the small number of reported HEV infections during pregnancy in industrialized countries , despite high seroprevalence in the general population .\n\n【6】Innate immunity has been suggested as essential for severe outcomes of acute HEV infection during pregnancy . In our study, HEV infection had moved toward chronic infection before pregnancy, which may have reduced the role of innate immunity. T-cell responses are decreased in immunosuppressed patients and in pregnant women, particularly when term approaches. The imbalance in T-cell immunity (Th1/Th2) has been proposed to be implicated in the progression of chronic HEV infection in immunocompromised pigs . This imbalance may explain the absence of cytolysis during pregnancy and the increased viral load observed despite discontinuation of infliximab. Conversely, after delivery, restoration of cellular immunity is commonly observed  and may have contributed to efficient clearance of the virus by hepatic cytolysis along with the reduced immunosuppression resulting from infliximab discontinuation. Despite reintroduction of infliximab when HEV RNA was still detectable, we observed spontaneous resolution of chronic hepatitis E, although immunosuppressive treatment at that time was identical to that previously implicated in the chronicity of infection.\n\n【7】The risk for HEV vertical transmission seems dependent on viral load . In a model of HEV infection in pregnant rabbits, Xia et al. reported severe outcomes and a high level of transmission to offspring . In the case we report, despite high viral loads in the mother’s plasma throughout pregnancy, we found no HEV RNA in the newborn’s plasma. Of note, although mothers in the rabbit model were negative for HEV IgG throughout pregnancy, in the case we report, the mother was IgG positive before pregnancy, which may have helped protect the fetus from infection, although this protective role is inconsistent in previous reports of HEV genotype 3 (HEV3) infection of humans . Furthermore, despite a high sequence similarity to HEV3, rabbit HEV cross-species infections are restricted to nonhuman primates, and pathogenesis may differ from that of HEV3. In conclusion, our results and those reported by Mallet et al. indicate that chronic HEV3 infection in pregnant women might resolve after pregnancy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fb0d2533-186c-410e-9f42-145c0e8771f4", "title": "Possible Increased Pathogenicity of Pandemic (H1N1) 2009 Influenza Virus upon Reassortment", "text": "【0】Possible Increased Pathogenicity of Pandemic (H1N1) 2009 Influenza Virus upon Reassortment\nThe influenza virus A (H1N1) that caused the first influenza pandemic of the 21st century, pandemic (H1N1) 2009, continues to be detected worldwide . The pandemic overall has been relatively mild; disease has ranged from subclinical infections to sporadic cases of severe pneumonia and acute respiratory distress syndrome . The virus responsible is a unique reassortant virus containing neuraminidase (NA) and matrix genes from the Eurasian swine influenza virus lineage, and the other 6 gene segments are derived from the North American triple reassortant swine influenza virus lineage . From the pandemic’s start, there have been concerns the virus may mutate or reassort with contemporary influenza viruses and give rise to more pathogenic viruses.\n\n【1】Cocirculation of multiple strains of influenza virus A in humans provides an opportunity for viral genetic reassortment (mixing of genes from \\> 2 viruses) . Genetic reassortment of pandemic (H1N1) 2009 virus with seasonal influenza A (H3N2) or seasonal influenza A (H1N1) viruses might thus represent a route to enhanced pathogenicity. No reassortment events between pandemic (H1N1) 2009 and seasonal viruses have been reported in humans. However, a triple-reassortant swine influenza virus A (H1N1), distinct from pandemic (H1N1) 2009 virus and containing the hemagglutinin (HA) and NA genes of seasonal influenza virus A (H1N1), was described recently . Dual infections by seasonal influenza A (H1N1) and seasonal influenza A (H3N2) viruses have been reported , as well as mixed infections of pandemic (H1N1) 2009 and seasonal influenza A (H3N2) viruses , highlighting the potential for reassortment of currently circulating influenza viruses. Subtype H1N2 reassortant influenza viruses that contain the HA of seasonal influenza A (H1N1) and the NA of seasonal influenza A (H3N2) viruses have been isolated from humans during previous influenza seasons, confirming that such HA/NA combinations can emerge in humans .\n\n【2】To investigate the potential for reassortment between seasonal influenza A and pandemic (H1N1) 2009 viruses, we used an in vitro selection method using reverse genetics and serial passaging under limited dilution conditions. Pathogenicity and transmission of these viruses were tested by using a ferret model. We report here the identification of 4 reassortants with different gene constellations.\n\n【3】### Materials and Methods\n\n【4】##### Cells and Viruses\n\n【5】MDCK cells were cultured in Eagle minimum essential medium as described . Influenza virus A/Netherlands/602/2009 was isolated from the first patient with pandemic (H1N1) 2009 virus infection in the Netherlands . Influenza virus A/Netherlands/213/2003 (seasonal influenza A \\[H3N2\\]) and influenza virus A/Netherlands/26/2007 (seasonal influenza A \\[H1N1\\]) were isolated from patients during epidemics in the Netherlands. After these viruses were passaged in MDCK cells 2×, all 8 gene segments were amplified by reverse transcription–PCR, cloned in a modified version of the bidirectional reverse genetics plasmid pHW2000 , and subsequently used to generate recombinant virus by reverse genetics as described elsewhere .\n\n【6】##### Generation of the Reassortant Viruses\n\n【7】Mixtures of reassortant viruses were generated in 293T cells by using reverse genetics, by co-transfecting 8 plasmids that encode the pandemic (H1N1) 2009 virus genome together with 7 plasmids encoding the seasonal influenza A (H3N2) or seasonal influenza A (H1N1) virus genome. We omitted HA of the seasonal viruses to ensure that only reassortants containing the pandemic (H1N1) 2009 virus HA could arise, against which a large proportion of the human population is still immunologically naïve . The 293T-cell supernatants were passaged in quadruplicate under limiting dilution conditions by using 10-fold serial dilutions in MDCK cells 3× to enable selective outgrowth of viruses with high in vitro replication rates. After 3 passages, the genome composition of these viruses was determined by sequencing with conserved primers targeting noncoding regions of each gene segment. Reverse genetics was also used to produce specific reassortant viruses (pandemic \\[H1N1\\] 2009–seasonal influenza A \\[H1N1\\] basic polymerase \\[PB\\] 2, pandemic \\[H1N1\\] 2009–seasonal influenza A \\[H1N1\\] PB2 acidic polymerase \\[PA\\], pandemic \\[H1N1\\] 2009–seasonal influenza A \\[H3N2\\] NA, and pandemic \\[H1N1\\] 2009–seasonal influenza A \\[H3N2\\] NAPB1) by transfection of 293T cells and subsequent virus propagation in MDCK cells.\n\n【8】##### In Vitro Characterization of Viruses\n\n【9】Multicycle replication curves were generated by injecting MDCK cells at a multiplicity of infection of 0.01 50% tissue culture infective dose (TCID 50  ) per cell in 2-fold . Virus titers from samples of inoculated MDCK cells, as well as nasal and throat swabs or homogenized tissue samples from inoculated ferrets, were determined by endpoint titration in MDCK cells, as described .\n\n【10】##### Ferret Experiments\n\n【11】All animal studies were approved by an independent animal ethics committee. Experiments were performed under animal BioSafety Level 3+ conditions. The ferret model to test pathogenicity and transmission of pandemic (H1N1) 2009 virus was described previously . To study pathogenicity, 5 groups of 6 influenza virus–seronegative female ferrets (Mustella putorius furo_ ) were inoculated intranasally with 10 6  TCID 50  of wild-type pandemic (H1N1) 2009 virus, or the reassortant viruses pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2, pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2PA, pandemic (H1N1) 2009–seasonal influenza A (H3N2) NA, and pandemic (H1N1) 2009–seasonal influenza A (H3N2) NAPB1, divided between both nostrils (× 250 μL). In the transmission experiment, 4 female ferrets for wild-type pandemic (H1N1) 2009 virus and 2 ferrets for each reassortant virus were individually housed in transmission cages and inoculated intranasally with 10 6  TCID 50  of virus divided between both nostrils (× 250 μL). Animal daily weights were used as an indicator of disease.\n\n【12】##### Immunohistochemistry and Histopathology\n\n【13】Immunohistochemical testing and pathologic examination were performed by using lungs of inoculated ferrets. For each virus, 3 ferrets were euthanized at 3 and 7 days postinoculation (dpi) by exsanguination. Necropsies and tissue sampling were performed according to standard protocol. After fixation in 10% neutral-buffered formalin and embedding in paraffin, samples were sectioned at 4 μm and stained with an immunohistochemical method by using a mouse monoclonal antibody against the nucleoprotein of influenza virus A . Influenza virus antigen expression in lung sections was scored for bronchial surface epithelium, bronchial submucosal gland epithelium, bronchiolar epithelium, alveolar type I pneumocytes, and alveolar type II pneumocytes. Scoring was categorized as 0, no positive cells; 1, few positive cells; 2, moderate number of positive cells; and 3, many positive cells. Serial lung sections were stained with hematoxylin and eosin for detection and description of pathologic changes. Samples were scored for influenza virus–associated inflammation in bronchi (bronchitis), bronchial submucosal glands (bronchoadenitis), bronchioles (bronchiolitis), and alveoli (alveolitis). Scoring of severity of inflammation was 0, no inflammation; 1, mild inflammation; 2, moderate inflammation; and 3, marked inflammation. Researchers who examined the sections had no knowledge of the identity of the ferrets.\n\n【14】### Results\n\n【15】##### In Vitro Selection of Reassortants\n\n【16】The proportion of gene segments 1–8 (except HA) analyzed was ≈60%, 60%, 65%, 90%, 95%, 100%, and 100%, respectively. Minor virus variants were not detected. No point mutations were observed in the proportions of the genome analyzed. Upon pandemic (H1N1) 2009–seasonal influenza A (H1N1) transfection and passaging, 3 reassortants contained the PB2 gene of seasonal influenza virus A (H1N1), 2 of which had also incorporated the seasonal (H1N1) PA gene. All 4 pandemic (H1N1) 2009–seasonal influenza A (H3N2) virus reassortants had the NA gene of seasonal influenza A (H3N2), and 3 of 4 reassortants also incorporated the seasonal influenza A (H3N2) PB1 gene .\n\n【17】##### In Vitro Characterization of Pandemic (H1N1) 2009–Seasonal Influenza A (H1N1) and Pandemic (H1N1) 2009–Seasonal Influenza A (H3N2) Reassortants\n\n【18】The replication kinetics of pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2 and pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2PA were similar to those of wild-type pandemic (H1N1) 2009 virus, and the pandemic (H1N1) 2009–seasonal influenza A (H3N2) NA and pandemic (H1N1) 2009–seasonal influenza A (H3N2) NAPB1 reassortant viruses displayed slightly higher virus titers at 24 and/or 48 h after inoculation, with a maximum difference in virus titer of 1.0 log 10  TCID 50  . The fact that each reassortant virus replicated at least at the same rate as the wild-type pandemic (H1N1) 2009 virus agrees with results from the in vitro selection experiment described above.\n\n【19】##### Pathogenicity of the Reassortant Viruses in Ferrets\n\n【20】The mean maximum weight loss was 7% for animals inoculated with the pandemic (H1N1) 2009 virus. Animals inoculated with pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2PA, pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2, pandemic (H1N1) 2009–seasonal influenza A (H3N2) NAPB1 and pandemic (H1N1) 2009–seasonal influenza A (H3N2) NA had a maximum weight loss of 4%, 2%, 2%, and 6%, respectively (data not shown).\n\n【21】Nose and throat swabs were collected daily, and virus titers were determined. Infectious virus shedding continued until 6–7 days dpi from noses  and throats  of most inoculated animals. Total virus shedding from the nose, as calculated from the area under the curve for ferrets in the experiment for 7 days (n = 3), was significantly lower in animals inoculated with the pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2 reassortant virus (p = 0.003 by _t_ test) and significantly higher in the animals inoculated with the pandemic (H1N1) 2009–seasonal influenza virus A (H3N2) NA (p = 0.023 by _t_ test), than in animals inoculated with wild-type pandemic (H1N1) 2009 virus. Total virus shedding from the throat, as calculated from the area under the curve for ferrets in the experiment for 7 days, was not significantly different between the groups of ferrets.\n\n【22】At 3 and 7 dpi, 3 ferrets from each group were euthanized, and samples from nasal turbinates, trachea, and lungs were collected for virologic examination. At 7 dpi, virus was undetectable or detected at only very low levels in these samples from all groups of ferrets. At 3 dpi, no virus or relatively low virus titers were detected in the lungs and trachea respectively, of ferrets inoculated with pandemic (H1N1) 2009–seasonal influenza A (H1N1) reassortant viruses , and titers in the nasal turbinates were similar to those for wild-type pandemic (H1N1) 2009 virus . These data indicate that both pandemic (H1N1) 2009–seasonal influenza A (H1N1) viruses were attenuated with respect to replication in the lower respiratory tract of ferrets.\n\n【23】At 3 dpi, virus was detected in the lungs, trachea, and nasal turbinates of ferrets inoculated with pandemic (H1N1) 2009–seasonal influenza A (H3N2) NAPB1 and pandemic (H1N1) 2009–seasonal influenza A (H3N2) NA viruses at approximately the same levels as upon inoculation with wild-type pandemic (H1N1) 2009 virus . Virus titers detected in the lungs and trachea of animals inoculated with pandemic (H1N1)–seasonal influenza virus A (H3N2) NA at 3 dpi were 1.0 log 10  TCID 50  higher than those in animals inoculated with wild-type pandemic (H1N1) 2009 virus (not statistically significant). These data indicate that both pandemic (H1N1) 2009–seasonal influenza A (H3N2) viruses tested were not attenuated in ferrets. If anything, shedding of pandemic (H1N1)–seasonal influenza A (H3N2) NA virus from the nose , lungs , and trachea  was higher than shedding of wild-type pandemic (H1N1) 2009 virus.\n\n【24】##### Pathologic Changes in the Respiratory Tract of Ferrets Inoculated with Pandemic (H1N1) 2009 and Reassortant Viruses\n\n【25】At 7 dpi, virus antigen expression was undetectable in lung tissue of any of the euthanized ferrets, and lesions were absent or resolving. At 3 dpi, neither viral antigen expression nor lesions were detected in lungs of ferrets inoculated with pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2. Only 1 of 3 ferrets inoculated with pandemic (H1N1) 2009 had scant virus antigen expression and mild associated lesions in bronchial submucosal glands and bronchioles at 3 dpi . In contrast, all 3 ferrets inoculated with pandemic (H1N1) 2009–seasonal influenza A (H3N2) NA had moderate to abundant virus antigen expression in bronchial submucosal glands, bronchioles, or both, associated with moderate to marked inflammation . Virus antigen expression and associated lesions in the lungs of ferrets inoculated with pandemic (H1N1) 2009–seasonal influenza A (H3N2) PB1NA were intermediate between those of wild-type pandemic (H1N1) 2009 and pandemic (H1N1)–seasonal influenza A (H3N2) NA .\n\n【26】Cell types in which virus antigen expression was detected were ciliated epithelial cells of bronchi, epithelial cells of bronchial submucosal glands, ciliated and nonciliated cells of bronchioles, and both squamous and cuboidal epithelial cells (interpreted as type I and type II pneumocytes, respectively) of alveoli . Virus antigen expression was also seen in desquamated epithelial cells and cell debris in lumina of above tissues.\n\n【27】Lesions associated with virus antigen expression can be categorized as acute, focal or multifocal, necrotizing bronchitis, bronchoadenitis, bronchiolitis, and alveolitis. These lesions were characterized by degeneration and necrosis of epithelial cells, infiltration of the affected tissues and their lumina by many neutrophils and few eosinophils, and exudation of edema fluid and fibrin into tissue lumina.\n\n【28】##### Transmission of Reassortant Viruses in Ferrets\n\n【29】Transmission of pandemic (H1N1) 2009 and reassortant influenza viruses through aerosol or respiratory droplets was tested in the ferret model. Ferrets in groups of 4 for pandemic (H1N1) 2009 virus and 2 for the reassortant viruses were inoculated intranasally with 10 6  TCID 50  of virus. At 1 dpi, an uninfected ferret was placed in a cage adjacent to each inoculated ferret. All viruses were transmitted from the inoculated to the uninfected ferrets in 4/4 ferrets for pandemic (H1N1) 2009 virus and 2/2 ferrets for each of the reassortant viruses. The first day of virus detection in the previously uninfected animals was 2 days post exposure, similar for all viruses tested.\n\n【30】### Discussion\n\n【31】We used an in vitro selection method to identify reassortant viruses between pandemic (H1N1) 2009 virus and seasonal influenza A (H1N1) and influenza A (H3N2) viruses of interest for testing in a ferret model. Studying the effects of reassortment on changes in influenza virus phenotype is cumbersome because the number of reassortants that can be generated between 2 viruses is high; 2 8  \\= 256 different viruses. After 3 passages, a limited number of specific virus populations were selected in vitro. Minor virus variants representing <20% of the virus population would remain undetected in our approach of PCR amplification and direct determination of the consensus sequence of the amplicons. However, upon repeating the procedure 4 times for both reassortment combinations, the seasonal influenza virus genes that were selected in the pandemic (H1N1) 2009 virus backbone were more or less consistent, with NA of seasonal influenza virus A (H3N2) being selected in 4/4 attempts, PB1 of seasonal influenza A (H3N2) and PB2 of seasonal influenza virus A (H1N1) in 3/4 attempts, and PA of seasonal influenza virus A (H1N1) in 2/4 attempts. Replication in MDCK cells may not be the best selection criterion for the identification of reassortants of interest to human health. Nevertheless, we chose this in vitro selection method because previous work has shown that pandemic (H1N1) 2009 outcompetes seasonal influenza A (H1N1) and seasonal influenza A (H3N2) viruses rapidly, reducing the opportunity for reassortment . This growth advantage over seasonal viruses was in agreement with the fact that selected viruses mostly contained pandemic (H1N1) 2009 genes. The use of reverse genetics enables production of all gene segments at approximately similar copy numbers on transfection, whereas upon double infection with 2 viruses, in vitro or in ovo viruses may differ in replication capacity, resulting in a bias of reassortants produced.\n\n【32】Notably, the polymerase gene segments of seasonal influenza A (H1N1) and seasonal influenza A (H3N2) viruses frequently substituted for the polymerase genes of the pandemic (H1N1) 2009 virus in vitro _._ In minigenome assays, the polymerase complex activity of the wild-type pandemic (H1N1) 2009 virus was relatively low, and replacement of various polymerase genes of the pandemic (H1N1) 2009 virus increased this activity. However, polymerase complexes with the highest activity in minigenome assays were not necessarily the ones detected in the reassortant viruses (data not shown). This apparent discrepancy is probably a result of the different parameters under investigation in the 2 assays, in particular, the production of mRNA vs. all viral RNAs.\n\n【33】Virus titers for pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2PA and pandemic (H1N1) 2009–seasonal influenza A (H1N1) PB2 in the lungs and trachea of ferrets were lower than titers in ferrets inoculated with wild-type pandemic (H1N1) 2009 virus, suggesting that both pandemic (H1N1) 2009–seasonal influenza A (H1N1) reassortant viruses were attenuated in ferrets, at least for replication in the lower respiratory tract. The reassortants between pandemic (H1N1) 2009 and seasonal influenza A (H3N2) viruses replicated at slightly higher rates than wild-type pandemic (H1N1) 2009 virus in vitro. Moreover, virus shedding of pandemic (H1N1) 2009–seasonal influenza virus A (H3N2) NA from the nose, lungs, and trachea of inoculated ferrets was slightly higher than wild-type pandemic (H1N1) 2009 virus. Although the differences in replication and shedding were small and not statistically significant because of the small numbers of animals in each group, the pandemic (H1N1) 2009–seasonal influenza A (H3N2) viruses were not attenuated in ferrets.\n\n【34】Inoculation of the reassortant pandemic (H1N1)–seasonal influenza A (H3N2) NA (either with or without the PB1 of seasonal influenza virus A \\[H3N2\\]) resulted in higher expression of virus antigen and more severe lesions at all levels of the lower respiratory tract compared with inoculation of wild-type pandemic (H1N1) 2009 virus . In a previous study, the wild-type pandemic (H1N1) 2009 virus was detected more abundantly in the lower airways of ferrets than in the present study . We attribute this difference to the use of a virus isolate rather than a virus generated by reverse genetics and to a different batch of ferrets in the previous study. In the present study, all viruses were produced with reverse genetics and can thus be compared directly. Moreover, the reassortant pandemic (H1N1) 2009 virus with the NA of the seasonal influenza virus A (H3N2) was more pathogenic than both sources of pandemic (H1N1) 2009 virus, either the wild-type isolate or the virus derived by reverse genetics. Increased severity of lesions may be related to higher virus replication in the lung, to stronger host immune responses, or both .\n\n【35】We conclude that the pandemic (H1N1) 2009 virus has the potential to reassort with seasonal influenza virus A (H1N1) and influenza virus A (H3N2) and that such reassortment events could result in viruses with increased pathogenicity in ferrets. Although increased pathogenicity in ferrets cannot be extrapolated directly to increased pathogenicity in humans, ferrets are susceptible to natural infection and respiratory disease and lung pathology develop in a manner similar to that in humans infected with seasonal, avian, or pandemic influenza viruses. Thus, the ferret model is generally thought to be a good animal model for influenza in humans . Patterns of influenza virus attachment to cells of the respiratory tract are also similar in ferrets and humans , and the ferret model has further been used successfully for studies on virus transmission through respiratory droplets or aerosols .\n\n【36】All reassortants were transmitted between ferrets through aerosol or respiratory droplets. These results demonstrate that some reassortants between pandemic (H1N1) 2009 and seasonal influenza A (H3N2) were viable, remained transmissible, and were more pathogenic than the wild-type pandemic (H1N1) 2009 virus and emphasize the importance of monitoring reassortant viruses in surveillance programs because reassortment events may affect pathogenicity.\n\n【37】Although viruses with the NA gene (with or without the PB1 gene) of seasonal influenza A (H3N2) were identified here as potentially fit virus reassortants, reassortant viruses with other gene constellations may have selective advantages in humans as well. The 1968 influenza virus A (H3N2) pandemic also continued to reassort after the pandemic year, resulting in viruses during 1969–1971 with a different N2 gene than those earlier in the pandemic . Reassortants of influenza virus A (H1N2) with the HA of seasonal influenza A (H1N1) and the NA of seasonal influenza A (H3N2) viruses have been isolated from humans during previous influenza seasons, thereby confirming that reassortant influenza viruses with such an HA/NA combination can emerge in humans . Moreover, influenza (H1N2) viruses frequently have been detected in pigs around the world . Therefore, we recommend that reassortant of pandemic (H1N1) 2009 influenza viruses be monitored closely in surveillance programs, particularly when changes in pathogenicity or transmission in humans become apparent.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "84fc2c1a-a414-4a74-9090-ac81b722e292", "title": "European Bat Lyssavirus Infection in Spanish Bat Populations", "text": "【0】European Bat Lyssavirus Infection in Spanish Bat Populations\nRabies is a worldwide zoonosis due to _Lyssavirus_ infection; multiple host species act as reservoirs. This disease infects the central nervous system of humans and other mammals. Bats are no exception, as proved by the 630 positive cases detected in Europe from 1977 to 2000 . Recent molecular studies have shown genetic differentiation in lyssaviruses that cause rabies among European bats, leading to a classification into two new genotypes, 5 and 6, which correspond to _European bat lyssavirus 1_ (EBL1) and EBL2, respectively . As a result of a recent molecular study, two new lineages within genotype 5 have been identified—EBL1a and EBL1b; the latter is potentially of African origin, which suggests south-to-north transmission  . However, despite molecular advances and many European cases verified to date, knowledge of the prevalence and epidemiology of EBL is limited. Of the 30 insectivorous bat species present in Europe, approximately 95% of cases occur in the species _Eptesicus serotinus_  . This species, which is nonmigratory, cannot be linked to all the different foci of positive cases in Europe  . In Spain, the first case of bat lyssaviruses was recorded in 1987 in Valencia. Sixteen more cases were reported in _E. serotinus_  . The distribution of positive cases in Spain is indicated in Figure 1 .\n\n【1】Recently, clinically silent rabies infection has been reported in zoo bats (Rousettus aegyptiacus)_ in Denmark and the Netherlands  . This observation, together with the results of an experimental challenge, suggests that this frugivorous bat species of African origin can survive EBL1 infection or inoculation  . Silent infection has also been described in the American bat (Tadarida brasiliensis mexicana)_  and suggests an alternative viral strategy for _Lyssavirus_ infection of European insectivorous bats compared with the terminal infection commonly associated with rabies infection.\n\n【2】To investigate these observations, a 9-year study was undertaken in Spain to locate and determine the colonies and species of bats carrying EBL or _Lyssavirus_ antibodies, monitor the prevalence of seropositive bats, and characterize circulating lyssaviruses.\n\n【3】### Material and Methods\n\n【4】##### Selection of Bat Colonies and Banding\n\n【5】The study area consisted mainly of the Spanish Autonomous Regions of Aragon, Balearic Islands, Catalonia, and Valencia  . The region of Ceuta (North Africa, near the Straits of Gibraltar) was also studied because of its proximity to Europe. Bat colonies were selected according to the following criteria: colony behavior (anthropophilic, migratory, gregarious) and proximity of the colonies to urban areas. The Valencia bat colony was widely sampled because the first case of bat _Lyssavirus_ in Spain was reported there  . Colonies exhibiting positive sera were more intensively explored during the years after the first detection. From 1996 to 2000, bats from the locations Nos. 4, 5, and 7 were banded in the forearm to facilitate monitoring of their movements between colonies  .\n\n【6】##### Blood Sampling\n\n【7】To draw blood, we set the bat face upward with a stretched wing. The patagium was wiped clean and locally disinfected with a sanitary towel soaked in 96% alcohol to prevent infections. Immediately afterwards, a small puncture was made next to the radius proximal epiphysis. Blood was collected in an Eppendorf vial by using a Pasteur pipette. The amount of blood sampled varied from 0.2 mL to 0.5 mL, according to the size of the animal. A sterilized absorbent hemostatic sponge impregnated with gelatin was administered to prevent bleeding and facilitate healing. Pressure was applied to the wound with a sanitary towel for 30 seconds. The bats were given 10% glucose water to drink to prevent dehydration and provide rapidly assimilated compounds for energy. Once bleeding ceased, the bat was released. Vials containing blood were stored at 4ºC for a few hours. Samples were centrifuged for 20 minutes at 5,000 rpm, and the serum was extracted with a pipette. Serum samples and blood pellets were stored at –20°C.\n\n【8】##### Detection of EBL Antibodies\n\n【9】The technique used for the detection of EBL antibodies is an adaptation of the Rapid Fluorescent Focus Inhibition Test  . A constant dose of a previously titrated, cell culture-adapted EBL1 challenge virus 8918FRA  was incubated with threefold dilutions of the sera to be titrated. After incubation of the serum/virus mixtures, a suspension of BSR (a clone of BHK-21) cells was added. After 24 hours’ incubation, the cell monolayer was acetone-fixed and stained with a fluorescent anti-nucleocapsid antibody (Bio-Rad, Marnes-la-Coquette, France) to detect the presence of non-neutralized virus (fluorescent foci). Titers are presented as an arithmetic mean of two independent repetitions. Serum samples with antibody titers <27 are considered negative for EBL1-neutralizing antibodies. The percentages of seropositive bats and the years in which bats were analyzed (from 1996 to 2000) were correlated, and regression curves were obtained. To confirm the specificity of the reaction, the same test was performed on selected sera by using the challenge virus strain (CVS)  and 9007FIN EBL2 challenge viruses  .\n\n【10】##### Brain Sampling\n\n【11】Brain samples were obtained from dead bats, submitted by citizens. Dead bats found in the studied refuges were also gathered. The bats found dead from 1994 to 1996 were analyzed by direct immunofluorescence technique . The bats found dead from 1997 to 2000 were analyzed by nested reverse transcription-polymerase chain reaction (RT-PCR)  . To eliminate cross-contamination at necropsy, sterilized instruments were used.\n\n【12】##### Detection of EBL Antigens\n\n【13】The standard fluorescent antibody test (FAT) was performed on brain tissue specimens of the bats by using the polyclonal fluorescein isothiocyanate-labeled rabbit anti-rabies nucleocapsid immunoglobulin G, as described by the manufacturer (Bio-Rad). Brain smears obtained from noninfected and CVS-infected mice were incorporated as controls in each FAT test run.\n\n【14】##### Detection of EBL1 RNA\n\n【15】Total RNA was extracted from tissue samples (mg -100 mg) by using the TRIzol method (Invitrogen, Groningen, the Netherlands), purified with chloroform and precipitated with iso-propanol (Merck, Darmstadt, Germany). After being washed with 70% ethanol, the RNA pellet was dried, resuspended in a volume of 50 μL bidistilled water and stored at –20ºC. cDNA synthesis of the genomic and antigenomic sense of the EBL1a nucleoprotein RNA was performed by annealing, at 70ºC for 3 minutes, 2 μL of total RNA extract with 15 pmol of primers N60 (’-TCCATAATCAGCTGGTCTCG-3’, positions 98-117, relative to rabies genome)  and N41, as described previously  .\n\n【16】Amplification of 5 μL of the cDNA template was performed in a final volume of 50 μL containing 1x magnesium-free PCR buffer (Invitrogen), 5 mM deoxynucleoside triphosphate (NTP) mix (containing 1.25 mM each of dATP, dCTP, dGTP, and dTTP), 5 mM magnesium chloride (Invitrogen), 2 U _Taq_ DNA polymerase (Invitrogen), and 30 pmol of primers N60 and N41. The amplification was performed on a GeneAmp PCR System 9700 Thermal cycler. The program started with one denaturation step at 94ºC for 5 minutes, followed by 30 cycles of 94ºC for 30 sec, 60ºC for 30 sec, and 72ºC for 40 sec. The amplification was finalized by an ultimate elongation step at 72ºC for 5 min. The primary amplification products were stored at –20ºC. For nested RT-PCR, the amplified product was diluted 10 times in distilled water. Then the second amplification was performed as described above with the following modifications: 30 pmol of primers N62 and N63 (N62: 5’-AAACCAAGCATCACTCTCGG-3’, position 181-200; N63: 5’-ACTAGTCCAATCTTCCGGGC-3’, position 342-323 relative to the _Rabies virus_ genome)  were used, and the elongation steps were performed at 72ºC for 30 sec. Aliquots (µL) of nRT-PCR products were analyzed by horizontal agarose (.5%) gel electrophoresis. Gels were stained with 1 µg/mL ethidium bromide and photographed under UV light.\n\n【17】Extraction of RNA was performed in a level-2 biosafety laboratory. Then we prepared the template and RT-PCR mix and added DNA to the mix with aerosol-resistant tips in two different rooms. We also performed nRT-PCR on tissue RNA, omitting reverse transcriptase. Positive  and negative (H 2  O) controls were incorporated into each of the following steps: total RNA extraction, cDNA synthesis, and each of the two steps of the amplification program. To avoid false-positive results, usual precautions for PCR were strictly followed in the laboratory .\n\n【18】The threshold of detection of the nRT-PCR method was determined by preparing 10-fold dilutions of a pretitrated suspension of Strain 8918FRA  in TRIzol (GIBCO-BRL). Total RNA extraction, cDNA synthesis, and the RT-PCR procedures were performed as described above.\n\n【19】Sequencing of amplified products was performed by using the primers N62 and N63 and an Applied Biosystems 373A sequencer (Foster City, CA), according to the Applied Biosystems protocol. Multiple sequence alignments were generated with the Clustal W 1.60 program  .\n\n【20】### Results\n\n【21】##### Presence of EBL1 Antibodies in Six Bat Colonies\n\n【22】We describe here a very efficient technique of blood collection, which is more humane than collection by cardiac puncture . The bats recaptured 1 week after the blood extraction did not show any trace of a scar. Furthermore, our technique is easier than collection by puncture of the uropatagium or the propatagium cardiac veins  . To eliminate any false- or doubtful positive reactions in seroneutralization, the threshold of positivity (titer=27) was chosen higher than the one adopted by other authors  . Two independent repetitions of the seroneutralization also reinforced the accuracy of our results.\n\n【23】Throughout the 9-year study, 976 sera obtained from 14 bat species in 37 different locations were analyzed ; 76 (.8%) were positive . _Lyssavirus_ antibodies were detected in four bat species (Myotis myotis,_ _Miniopterus schreibersii, Tadarida teniotis,_ and _Rhinolophus ferrumequinum_ ). Sixteen positive sera and 5 negative sera against EBL1 (genotype 5 of lyssaviruses) were further tested against standard strains of genotypes 1 (CVS), and 6 (EBL2). These sera were obtained from the four EBL1-seropositive bat species and from another bat species that remained negative (R. euryale_ ). None of them reacted positively against CVS and EBL2, confirming the specificity of the positive reactions against EBL1 obtained in these species .\n\n【24】The highest percentages of seropositive bats, 22.7% and 20.8%, were observed in the Balearic Islands in the locations of Inca  and Llucmajor , respectively . From spring to autumn, location No. 4 shelters a plurispecific colony of approximately 1,000 bats belonging to the following species: _M. myotis_ (% of seropositives), _M. schreibersii_ , _R. ferrumequinum_ , _M. capaccinii,_ and _M. nattereri_ . At the beginning of summer, _M. myotis, M. nattereri,_ and _M. schreibersii_ species form breeding pairs. Location No. 5 shelters a summer-breeding colony of approximately 500 bats of the species _M. myotis_ (.5% of seropositives), _M. schreibersii_ (.1% of seropositives), and _M. capaccinii_ . In both sites the most abundant species is _M. myotis_ .\n\n【25】Seropositive bats were also found in four other locations, Nos. 1, 3, 6, and 7. Location No. 1 (.5% of seropositive _R. ferrumequinum_ ) shelters a breeding colony of _R. ferrumequinum_ . In spring, the colony also includes some _M. schreibersii_ . Location No. 3 (.9% of seropositive _M. schreibersii_ ) is a hibernation refuge for approximately 2,200 _M. schreibersii;_ some _M. capaccinii_ are also present. Location No. 6 (.8% of seropositive _T. teniotis_ ) is a big sinkhole with a resident bat colony belonging to the following species: _T. teniotis_ , _M. blythii_ , _M. daubentonii_ , _Pipistrellus pipistrellus_ , _Pipistrellus kuhlii_ , _Hypsugo savii_ , _E. serotinus_ , _Plecotus austriacus,_ and _Barbastella barbastellus_  . Location No. 7 (% of seropositive _M. schreibersii_ ) shelters a colony of _M. schreibersii_ , _M. capaccinii,_ and _M. myotis_ .\n\n【26】##### Evolution of the Percentage of Seropositive Bats in Colonies Nos. 4 and\n\n【27】In Location 4, the percentage of seropositive bats rose from 3.3% in 1995 to 59.3% in 1996 . Then it decreased significantly (Y=-15.6X + 31,196.5, r=-0.989, p<0.05) until 1999, when it reached 10%. This percentage remained stable in 2000. The percentage of seropositive bats remained stable in Location No. 5 from 1995 to 2000.\n\n【28】##### Exchange of Animals Between Colonies and Survival of Seropositive Bats\n\n【29】During the period 1996-2000, 355 and 87 _M. myotis_ were banded in Locations Nos. 4 and 5, respectively . Recapture of the banded _M. myotis_ allowed us to prove a few exchange of bats between the colonies. Two percent of _M. myotis_ banded in Location No. 5 moved to Location No. 4 (the refuges are about 35 km apart). During the same period, 13 and 33 _M. schreibersii_ were banded in Locations Nos. 5 and 7, respectively. One of the 33 _M. schreibersii_ moved to Location No. 5 (the refuges are approximately 47 km apart); another moved to Location No. 4 (a distance of 11 km) .\n\n【30】Banding also allowed us to follow the seroneutralization titer of some bats during the study period. The serum of a _M. schreibersii_ captured in Location No. 7 in 1996 was negative; another serologic sample obtained from the same bat 2 years later in Location No. 5 yielded a titer of 8,508. During spring 2000, 12 _M. myotis_ previously banded and analyzed were recaptured in Location No. 4. Four (%) of them had already been shown to be seropositive in preceding years: two in summer 1997 (titers 29 and 145, respectively), one in summer 1998 (titer 303), and one in summer 1999 (titer 95). This indicates that some seropositive bats may survive at least 3 years after _Lyssavirus_ infection.\n\n【31】##### Detection and Characterization of EBL1 RNA in Bats\n\n【32】During 1995 through 1996, 12 brain samples were only analyzed by FAT. After 1996, the brain samples (n=79) were also analyzed by nested RT-PCR . All brains (n=91) analyzed by FAT were negative. In contrast, brains of 1 _M. myotis_ , 1 _M. nattereri,_ and 1 _M. schreibersii_  of Location No. 4 and 1 _R. ferrumequinum_  of Location No. 1 (all collected in 2000) were positive by nested RT-PCR. Four animals  were completely necropsied. Various organs and tissues (medulla, liver, kidney, spleen, heart, tongue, esophagus-larynx-pharynx, and lung) were collected and subjected to nRT-PCR. Esophagus-larynx-pharynx and lung of bat No. 135 and tongue, lung, and heart of bat No. 128 were positive .\n\n【33】Twenty-seven blood pellets of bats collected in 2000 were also analyzed by nRT-PCR. These samples were obtained from 8 _R. ferrumequinum_ , 1 _R. ferrumequinum_ , 1 _M. myotis_ , 14 _M. myotis_ , and 3 _M. schreibersii_ . The blood pellets of three _M. myotis_ from Location No. 4 were found positive by nRT-PCR. None of the blood samples showing positive RT-PCR results on the pellet were found positive by seroneutralization.\n\n【34】The threshold of detection of the nRT-PCR for the amplification of the EBL1a genomic and antigenomic RNAs of the N gene was 5 x 10 -2  fluorescent forming units of EBL1a/mL. In all these experiments, negative controls performed individually for each step (extraction, RT, primary, and secondary PCR) were negative. Furthermore, nRT-PCR performed on positive tissues without previous reverse transcription gave negative results, demonstrating the absence of complementary DNA contamination.\n\n【35】Nucleotide (nt) sequences were determined by using the positive nRT-PCR products obtained from the four brains and from one blood sample. These 122-nt long sequences of the nucleoprotein gene were strictly similar to the sequence of two EBL1b Spanish isolates (SPA and 9483 SPA) described previously  , except that the sequence obtained from the positive blood pellet exhibited a T→A mutation in position 145 of the coding region of the nucleoprotein gene. Four mutations distinguished the sequence of the positive control corresponding to a French bat  from the different sequences obtained from Spanish bats (not shown). This further confirms the specificity of the products amplified from the Spanish bat samples.\n\n【36】### Discussion\n\n【37】This is the first report of the presence of EBL1-specific neutralizing antibodies in four European insectivorous bat species (M. myotis,_ _M. schreibersii, T. teniotis,_ and _R. ferrumequinum_ ). These findings lead to the following observations on the circulation and possible bat species involved in the dispersion of EBL1 in southern Europe. First, the identification of EBL1 antibodies in 24% of the _M. myotis_ analyzed in Locations No. 4 and No. 5 in 1995 through 2000 (n=276) indicates that bats of this genus are infected with EBL1. Second, the distribution of _T. teniotis_ and _M. schreibersii_ in southern Europe and northern Africa  could contribute to the dispersion of EBL1 in southern Europe and is concordant with the possible African origin of EBL1, as suggested by Amengual et al.\n\n【38】Although the seasonal movements of _T. teniotis_ are scarcely known, the quick, straight flight of this species suggests that such movements are long, as is the case with the American bat (T. brasiliensis mexicana),_ which is capable of performing annual migrations of more than 1,000 km. Since _M. schreibersii_ makes seasonal migrations (some of them >350 km)  , this species could also be one of the dispersion vectors of the disease in southern Europe, where it abounds. _M. schreibersii_ dwells in five out of the six sites where seropositive bats have been found. In three of them, _M. schreibersii_ forms mixed colonies with _M. myotis_ , in one it shelters next to _R. ferrumequinum,_ and in the fifth it shelters alone. _M. schreibersii_ and _M. myotis_ have direct physical contact in the mixed colonies. However, it is unlikely that _Pipistrellus nathusii_ is a dispersion vector of the lyssaviruses in Spain, as Brosset  suggests, since this is a very rare bat in the Iberian Peninsula.\n\n【39】The results obtained in 1995-2000 in Location No. 4 show that the evolution in the number of seropositive bats after a _Lyssavirus_ infection corresponded to an asymmetrical curve, with a sudden initial increase reaching more than 60% of the colony and a gradual decline over subsequent years  —unless a new episode took place . Because of the gregarious behavior of this species, a quick increase and a high seropositive percentage (almost 60% in this location) after a _Lyssavirus_ episode are not unusual. The intimate contact that always exists among bats must facilitate viral transmission and antibody development. A high seropositive percentage also occurs in colonies of _T. brasiliensis mexicana_ , where percentages >80% have been observed . The transmission of lyssaviruses between bats from mixed colonies could take place through breathing or biting but is currently not documented.\n\n【40】The low prevalence (of 91, <1.1%) of active infection as determined by FAT is concordant with previous results obtained in America, which show a prevalence of active rabies infection in bats between 0.1 and 2.9% . However, we report the first detection of EBL1 RNA by nRT-PCR in several tissues (brain, blood pellet, lung, heart, tongue, and esophagus-larynx-pharynx) of four _M. myotis_ , one _M. nattereri_ , one _M. schreibersii,_ and two _R. ferrumequinum_ . These isolates show the existence of a low or nonproductive infection in these species, although some small remnant of RNA remaining in a clinically normal bat as a result of an earlier nonlethal exposure to a _Lyssavirus_ is also possible _._ This low amount of viral DNA present in the tissues underscores the need to use nRT-PCR as a very sensitive technique for epidemiologic studies of EBL1 in bat populations. Rønsholt et al. also comment on the difficulty of detecting _Lyssavirus_ infection by immunofluorescence in bats when a clinically silent infection exists.\n\n【41】EBL1 are known to actively infect the brain, lung, and tongue of _E. serotinus_  . However, this is the first report that EBL1 RNA can be detected in various organs and tissues in the absence of active infection, as demonstrated by negative results obtained by FAT. Most of these bats were dead when collected but were kept in conditions that allowed the classic diagnosis by FAT to be performed properly. These negative FAT results indicate that these bats died of causes other than their low productive _Lyssaviru_ s infection. The recapture of seropositive bats over several years also shows that some of these bats survived EBL1 infection. The detection of EBL1b sequences in the blood pellet of bats (/27) is also a new finding. This technique would be an easy test for screening positive bats. However, further studies are needed to establish the interest and sensitivity of this sample.\n\n【42】The sensitivity of the different European bat species to EBL infection probably varies according to the animal and virus species involved. Therefore, we have summarized in Table 5  the bat species in which either _Lyssavirus_ or antibodies against _Lyssavirus_ have been detected. Further studies are needed to determine which of the European bat species are the reservoir of EBL infection and if different species act as sentinels for the presence of the virus in the colony.\n\n【43】The presence of EBL1 RNA and immunity to EBL1 in several wild bat colonies also has important implications for bat management and public health. The probability of humans’ having contact with these colonies should be reduced and controlled. In our study, most bat colonies were found in sites that are frequently visited by speleologists, tourists, and bat-lovers. As a consequence of our findings, the entry to these caves is now controlled and limited during the periods when bats are present . Entry is limited by horizontal bars that allow the bats to fly across them but prevent access to people without obscuring the view.\n\n【44】Jordi Serra-Cobo is a member of the Quality Research Team (Biology of Vertebrates, 96-SGR0072) of the Universitat de Barcelona and a contracted doctor by the Instituto Pirenaico de Ecología (CSIC). His areas of expertise are vertebrates, population ecology, and bat lyssaviruses. Since 1990 he has been working in the research of Spanish bat lyssaviruses for the Ministerio de Sanidad y Consumo and the Conselleria de Sanitat of the Balearic Autonomous Government.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "433b4692-8bbb-4a59-87c8-48b750fd23a3", "title": "Detection of Infectious Poxvirus Particles", "text": "【0】Detection of Infectious Poxvirus Particles\nAfter the attacks with anthrax spores in the fall of 2001 in the United States, the potential abuse of variola virus or genetically engineered orthopoxviruses in bioterrorist plots has been intensely discussed . To date, several diagnostic assays have been developed to rapidly and reliably detect poxvirus particles or poxvirus genomes in suspected samples. Electron microscopy (EM) can also identify poxvirus particles . However, it cannot differentiate between orthopoxvirus species and has limited sensitivity because reliable detection is only possible with particle concentrations >10 6  /mL .\n\n【1】Molecular methods such as real-time polymerase chain reaction (PCR) are more sensitive, detecting <10 genome equivalents per PCR, but PCR can only identify short stretches of poxvirus DNA . Nevertheless, since EM and PCR cannot discriminate between infectious and noninfectious virus particles or nucleic acids, they are not satisfactory when an evaluation of the infectious capacity of viral particles is required.\n\n【2】Identifying viral particles by EM is usually sufficient to diagnose a poxvirus infection in clinical samples from patients with typical symptoms of this infection. Virus concentration should exceed 10 6  particles/mL; however, even at these concentrations only the virus family can be determined, and no additional classification is possible. Detection of poxvirus nucleic acids is sensitive and permits identification of virus-specific sequences and differentiation of a variola virus infection from an infection with other orthopoxviruses. Thus, a combination of both methods is recommended for frontline diagnostic procedures, and a positive result obtained by 1 of these methods would initiate a confirmation diagnosis.\n\n【3】If symptoms in clinical cases are unambiguous, they can usually be attributed to a replication-competent infectious virus. In contrast, in environmental samples, including samples from suspected parcels, a positive EM or PCR result would also require virus isolation to prove that particles could replicate to make a reasonable risk assessment .\n\n【4】With environmental samples, the unknown factor is to what extent the sample matrix influences the ability of the virus to replicate, and detecting particles by EM or DNA by PCR does not necessarily indicate infectious particles. The only diagnostic approach to identify replication-competent poxvirus particles is their propagation in a suitable cell culture system. With this system, it takes >1 day to reliably detect poxvirus proteins with specific antibodies.\n\n【5】We combined a cell culture approach that identifies virus replication with the speed and sensitivity of real-time PCR. To this end, we changed the target of real-time PCR from poxvirus DNA to poxvirus mRNA genes that are highly expressed during the first few hours of the infection cycle. Expression levels of these genes enable sensitive detection 1–2 hours after infection. The complete diagnostic approach can be performed in 96-well plates and provides results within 5 hours of receipt of a sample.\n\n【6】### The Study\n\n【7】Briefly, 1.5 × 10 4  HEpG2 cells were infected with 150 PFU of vaccinia virus strain Lister Elstree. A 15-minute centrifugation step at 1,000 × _g_ increased the efficacy of infection by a factor of 10 compared with regular infection at 37°C (data not shown). Virus-containing supernatant was removed, and virus was allowed to replicate for 4 h. Every 30 minutes an aliquot of cells was harvested, and RNA and DNA were isolated by standard procedures (RNAeasy kit and Blood DNA kit, Qiagen, Hilden, Germany). RNA was subjected to 1-step real-time reverse transcription–polymerase chain reaction (RT-PCR) (QuantiTect Probe RT-PCR kit, Qiagen) in a real-time PCR 7700/7900/7500 sequence detection system (Applied Biosystems, Foster City, CA, USA). Amplification of fragments of the _F1L_ gene, an apoptosis modulator, and the _rpo_ 18 gene, the small subunit of viral RNA polymerase  (both genes are encoded by all poxviruses including variola virus), was monitored by gene-specific 5´-nuclease probes.\n\n【8】Expression of the _F1L_ and _rpo_ 18 genes could be detected 30 minutes and 1 hour after infection, respectively. The copy number of the transcripts was determined by comparison with in vitro translated RNA molecules that were generated according to standard procedures. Briefly, RNA was transcribed in vitro by T7 RNA polymerase (RiboMax RNA production system, Promega, Madison, WI, USA) from plasmids containing the respective PCR target region, and plasmid DNA was digested with DNase.\n\n【9】During the first 4 hours after infection _F1L_ mRNA increased 2.7 × 10 4  \\-fold, indicating early expression of viral genes in the cells analyzed. The _rpo_ 18 mRNA showed a 410-fold increase after 4 hours. Quantification of viral DNA showed a slight decrease in DNA during the same period, and the ratios of RNA to DNA increased substantially, as shown in Figure 1 . This high ratio of poxvirus RNA to poxvirus DNA demonstrates that a possible background of genomic viral DNA, which is derived from poxvirus particles that are noninfectious or from traces of poxvirus genomic DNA in the RNA preparation, does not result in false-positive results in real-time RT-PCR.\n\n【10】To evaluate the detection limit of our approach, a probit analysis was performed by repetition of the detection (N = 12) of vaccinia virus strain Lister Elstree. Vaccinia virus stocks were titrated according to standard procedures. The virus load used varied from 1.5 × 10 3  PFU to 0.1 PFU, which is equivalent to a multiplicity of infection of 0.15 to 1 × 10 -5  . As shown in Figure 2 , after 2 hours of incubation, real-time PCR analysis showed that the _F1L_ assay detected 3 PFU of vaccinia virus, and the _rpo_ 18 detected 6 PFU of vaccinia virus with a confidence interval of 95%.\n\n【11】### Conclusions\n\n【12】The extremely low detection limit of the new assay indicates that environmental samples, which may contain cell culture inhibitory substances and are routinely subjected to crude separation steps such as low-speed centrifugation before analyses, can be diluted by several orders of magnitude to dilute inhibitors while maintaining the viral load at detectable levels. The time frame required for the individual steps of the diagnostic approach is 15 minutes for sample infection, 2–4 hours for virus propagation, 30 minutes for RNA preparation, and 2 hours for real-time RT-PCR. Use of alternative, more rapid real-time PCR platforms further reduces the time required to complete an assay. For poxvirus-positive results, fluorescence melting curve analysis of the _rpo_ 18 PCR product allows rapid and reliable differentiation of variola virus . Under optimal conditions, results can be obtained <5 hours after the sample has arrived in the laboratory.\n\n【13】In summary, the combination of cell culture and real-time RT-PCR detection of early, highly expressed viral genes permits detection of minute quantities of infectious poxvirus particles in a suspected sample. Identification of variola virus can be performed by fluorescence melting curve analysis, therefore permitting a reliable risk assessment of a suspect parcel.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cf00bb3a-4116-4e61-93b8-b945b9e59cca", "title": "Chikungunya Virus, Cameroon, 2006", "text": "【0】Chikungunya Virus, Cameroon, 2006\nChikungunya virus (CHIKV), formerly only an anecdotally described arbovirus, is now a worldwide public health problem . Recently, numerous cases of CHIKV infection have been reported from a major outbreak of febrile illness around the Indian Ocean, which included Comoros, Mauritius, Réunion Island , and southern India .\n\n【1】CHIKV is widely distributed in tropical Africa  and in Asia . In Africa, until 2000, the virus was described as endemic, perpetuated through a sylvatic cycle involving wild primates, humans, and mosquitoes of the genus _Aedes_ . During the past 6 years, the urban cycle has also tended to play a role in Central Africa . Nevertheless, although recent serologic surveys suggest a high prevalence of _Togaviridae_ , _Flaviviridae,_ and _Bunyaviridae_ , understanding of the circulation and effects of arboviruses in Cameroon remains imprecise. This lack of understanding may reflect confusion between arboviral infections and hyperendemic _Plasmodium falciparum_ infection.\n\n【2】We report the first isolation, to our knowledge, of CHIKV in Cameroon. The virus was identified during an outbreak of a febrile syndrome in French soldiers in Douala and in patients from an urban medical center in Yaoundé. We also found evidence of cocirculation of CHIKV and dengue virus (DENV).\n\n【3】### The Study\n\n【4】In Douala, Cameroon, 2 sporadic cases of a denguelike syndrome were recorded in French soldiers (patients 1 and 2) on April 3 and May 22, 2006, respectively . From the end of May through the end of July 2006, more cases of denguelike syndrome, which included fever, asthenia, maculopapular rashes, and arthralgia, were observed in Yaoundé. The number of patients who sought treatment at the Yaoundé Medical Center peaked in mid-June 2006. Blood samples were collected from 30 of the 40 patients who visited the medical center. The 30 patients’ ages ranged from 1 to 54 years. The delay between the onset of symptoms and the sampling ranged from 0 to 39 days with a median of 4 days . All but 1 patient lived in Yaoundé, and none of these patients had a history of travel abroad or from Yaoundé. Nine patients were Cameroonian, and all other patients were from other countries; 15 patients were female. A blood sample from a 53-year-old woman who returned to France from Yaoundé was also received. All patients had negative results for _P. falciparum_ according to rapid test (Core Malaria Pf, Core Diagnostics, Birmingham, UK) and thick smear examination.\n\n【5】Serum specimens were tested for immunoglobulin M (IgM) and IgG antibodies specific for DENV, West Nile virus (WNV), Wesselsbron virus, Rift Valley fever virus, Bunyamwera virus, and CHIKV by IgM-antibody capture (MAC-ELISA) and IgG sandwich ELISA, respectively . A serum sample was consider positive if the optical density (OD) ratio of viral antigen to uninfected cells was >3. The presence of CHIKV, DENV, and WNV genomes was tested for by specific real-time reverse transcription PCR (RT-PCR) . Virus isolation on C6/36 and Vero cells was attempted on samples that were positive by RT-PCR .\n\n【6】The serologic follow up of patient 1  for a 3-week period detected seroconversion to a virus antigenically related to CHIKV virus (the OD ratios obtained with the second sample were >3) for IgM and IgG. A sample from patient 2 was obtained the day after the onset of symptoms, and no antibodies to all tested arboviruses were detected. However, the specimen was positive by real-time RT-PCR for CHIKV. The patient’s sample yielded CHIKV when cultured, and the envelope gene was partially sequenced . The 1.2-kb sequence genetic analysis did not show any codon deletion or insertion when compared with other African CHIKV sequences available in the GenBank database . A high degree of identity was observed when the sequence was compared with the Democratic Republic of the Congo (DRC) strains isolated in 2000 . Paired identity ranged from 97% to 98.1% at the nucleotide level and from 98.7% to 99.3% at the amino acid level. The Cameroon isolate displayed a higher nucleotide divergence (paired identity ranging from 95% to 95.5%) when compared with the 2006 Réunion Island strains . However, amino acid sequences were highly conserved (%–99.5%). The sequence identity among these isolates highlights their common origin and particularly the genetic stability of CHIKV despite the 6 years and the geographic distance from the DRC outbreaks. As shown in the phylogenetic tree , the CHIKV Cameroon strain clustered with DRC CHIKV strains with a high bootstrap value of 100. This genotype of CHIKV was closely related to strains from the Central African Republic and the 1982 Uganda isolate . The close genetic relationship suggests a continuous circulation of a homologous CHIKV population in Central Africa with a high degree of genetic stability. The genetic stability of the Central African CHIKV strains during 24 years, whether associated with epidemic or sporadic cases, highlights the peculiar importance of the few mutations detected in the recent Réunion Island isolates . This also suggests that the Central African strain CHIKV zone of circulation now includes India , the Indian Ocean, and Cameroon.\n\n【7】The phylogenetic tree also illustrates the differences between the Cameroon isolates and the Asian subgroup isolates. Moreover, when compared with Asian CHIKV, including the 2006 isolates, the Cameroon strain showed 91%–91.9% and 96.8%–98.8% identity at the nucleotide and amino acid levels, respectively. Despite the similarity, cross-neutralization experiments must be conducted to confirm the protective effect of the Asian CHIKV-based vaccine against Central African strains .\n\n【8】Among patients from Yaoundé, 1 (patient 11) had only IgM antibodies specific to CHIKV, while patients 18 and 28 had both IgM and IgG antibodies specific to CHIKV . One patient from Cameroon (patient 20) had IgG specific to both CHIKV and flavivirus. Three patients (nos. 6, 13, and 15), 2 of whom were Cameroonian, had antibodies specific for flavivirus. All samples were negative for WNV and CHIKV by RT-PCR. One sample (from patient 31) was positive for DENV; however, no virus was detected by cell culture. These results suggested a cocirculation of CHIKV and dengue virus during the same period, which is consistent with the suspected circulation of dengue virus, CHIKV, and yellow fever virus observed in a study from 2000 through 2003 in Cameroon .\n\n【9】In Cameroon, as in DRC , patients were likely infected in urban or periurban centers (Yaoundé, the capital of Cameroon; Douala, a major city). These infections occurred in a context where _Aedes albopictus_ tends to replace indigenous _Ae. aegypti_ in rural and urban Cameroonian environments . This finding suggests that urban cycles and urban vectors, in addition to the traditional forest-dwelling vectors, may play an important role in the maintenance and amplification of CHIKV in Africa.\n\n【10】### Conclusions\n\n【11】Since its first isolation in 1953 , CHIKV has been isolated in different Central African countries . Until now, only 2 alphavirus strains antigenically suspected to be CHIKV had been isolated from human patients in Cameroon . Recent serosurvey studies suggested a possible CHIKV circulation in Cameroon . Our Cameroon CHIKV isolate confirmed its circulation in this country. Our study suggests a 6-year continuous circulation of genetically stable and indigenous strains in Central Africa rather than importation of CHIKV from the recent Indian Ocean or Asian outbreaks. Moreover, the genetic stability of the Central African CHIKV highlights the importance of the unique molecular features that was shown in Réunion Island isolates .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f766be9a-17ee-4eba-a624-874a6c773415", "title": "Human Co-Infection with Avian and Seasonal Influenza Viruses, China", "text": "【0】Human Co-Infection with Avian and Seasonal Influenza Viruses, China\n**To the Editor:** In April 2013, a case of co-infection with avian-origin influenza A(H7N9) virus and seasonal influenza A(H3N2) virus was reported in Jiangsu Province, China . This case raised concern over the possible occurrence of new reassortants with enhanced transmissibility among humans. Because of the nature of the dynamic reassortment of A(H7N9) virus with A(H9N2) virus in the environment and in poultry , close surveillance for possible new reassortment in human patients with A(H7N9) infection is needed. We report co-infection in 2 patients in Hangzhou, the capital Zhejiang Province, China, in January 2014. The co-infections involved influenza A(H7N9) virus and a seasonal A(H1N1)pdm09 virus (patient) or a seasonal influenza B virus (patient).\n\n【1】Of 60 patients with laboratory-confirmed influenza A(H7N9) infections in Hangzhou in April 2013 and in January–February 2014, testing of pharyngeal swab samples indicated that 2 patients were also positive for seasonal influenza virus. The pharyngeal samples were tested by real-time reverse transcription PCR according to protocols provided by the Chinese National Influenza Center. Informed consent for this study was provided by each patient’s spouse.\n\n【2】On January 6, 2014, patient 1 (male, 58 years of age), a resident of Xiaoshan District, had a high fever (.6°C) and a cough; at a hospital, he received a diagnosis of severe acute interstitial pneumonia. The patient had a history of chronic myelogenous leukemia; his history of exposure to live poultry was not clear. On January 13, infection with influenza A(H7N9) virus was laboratory confirmed; viral RNA from a pharyngeal swab sample collected before oseltamivir treatment was positive for the following: influenza A virus (cycle threshold \\[Ct\\] = 26), H7 (Ct = 27), N9 (Ct = 26), influenza A(H1N1)pdm09 virus H1 (Ct = 30), and N1 (Ct = 30). The 2 viruses were named A/Hangzhou/10–1/2014(H7N9) and A/Hangzhou/10–2/2014(H1N1)pdm09. The patient received oseltamivir while in the hospital but died on January 18.\n\n【3】On January 5, patient 2 (male, 54 years of age), also from Xiaoshan District, had fever and a cough; at a hospital, he received a diagnosis of severe acute pneumonia. He had a history of aplastic anemia and had been exposed to live poultry 1 week before symptom onset. On January 18, infection with influenza A(H7N9) virus was laboratory confirmed. Viral RNA from a pharyngeal swab sample collected before oseltamivir treatment was positive for the following: influenza A virus (Ct = 22), H7 (Ct = 23), N9 (Ct = 22), and influenza B virus (Ct = 22). The viruses were named A/Hangzhou/17–1/2014(H7N9) and B/Hangzhou/17–2/2014. This patient received oseltamivir but died on January 22.\n\n【4】The hemagglutinin (HA) and neuraminidase (NA) sequences of viruses from these 2 patients were determined by Sanger sequencing. The specific primers used are listed in Technical Appendix Table 1. The accession numbers of these sequences and the reference sequences for phylogenetic analyses are listed in Technical Appendix Table 2. Phylogenetic analyses  revealed that these 2 influenza A(H7N9) viruses were clustered into the clade of A/Shanghai/2/2013(H7N9)-like viruses . Some amino acid features within the HA and NA of these 2 viruses were the same as those in the A/Shanghai/2/2013(H7N9) strain: L226 and G228 in HA, believed to control host receptor specificity; the cleavage site in HA, relevant for virulence; a deletion in NA stalk (position 69–73), associated with the adaption to gallinaceous hosts; and R294 in NA, related to virus sensitivity to oseltamivir . The HA and NA sequences of A/Hangzhou/10–2/2014(H1N1)pdm09 and B/Hangzhou/17–2/2014 were very close to those of A(H1N1)pdm09 virus and B/Yamagata-lineage viruses that had recently circulated in China .\n\n【5】Co-infection with A(H7N9) virus and seasonal influenza viruses is probably associated with the overlap of A(H7N9) virus and seasonal virus circulation in both time and space and with increased prevalence of influenza virus infections within the population. From November 2012 through March 2014, outbreaks of A(H7N9) infection (in April 2013 and in January–February 2014) were concurrent with increases in seasonal influenza virus infections in Hangzhou . Prompt control of A(H7N9) infection outbreaks and vaccination against seasonal influenza viruses could reduce the potential for co-infections with A(H7N9) virus and seasonal viruses.\n\n【6】Taken together with the previous finding of human co-infection with A(H7N9) virus and A(H3N2) virus , our results show that human co-infection with A(H7N9) virus and each of the 3 seasonal influenza viruses currently circulating worldwide can occur. Avian influenza viruses, including A(H7N9), preferentially replicate in the lower respiratory tract of humans . In contrast, seasonal influenza viruses preferentially infect the upper respiratory tract of humans . Coexistence of A(H7N9) virus with either A(H1N1)pdm09 virus or influenza B virus in the pharyngeal swab samples from 2 patients suggests that the upper respiratory tract could provide a location for the A(H7N9) virus to reassort with other influenza viruses. The possibility that seasonal influenza viruses might provide some gene segments that increase the human-to-human transmissibility of possible new reassortants is cause for concern. For detection of such new influenza virus reassortants, extensive surveillance to identify influenza virus co-infections is necessary.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "10afb704-2c64-4271-bd30-11a7fe2e65b8", "title": "Specimen Collection for Electron Microscopy", "text": "【0】Specimen Collection for Electron Microscopy\n**To the Editor:** As virologists whose specialties include diagnostic electron microscopy (EM), we read with interest the discussion on bioterrorism scenarios  and the subsequent note by Marshall and Catton  on the rapid EM diagnostic process used for smallpox . EM diagnostics for viral agents offer an open, undirected view; a catch-all method; and speed. A negative stain preparation may be made, and a result could be obtained within 5 minutes of the specimen's arrival in the EM laboratory. As suggested by Marshall and Catton, however, success depends as much on the quality of the sample collected as on the method of preparation and skill of the microscopist.\n\n【1】The Konsilarlaboratorium für die Elektronenmikroskopische Erregerdiagnostik in the Robert Koch Institut, Berlin, Germany, provides EM viral diagnostic services for up to 800 specimens per year and counsels other German diagnostic units. The Electron Microscope Unit for the Department of Medical Microbiology and Infectious Diseases, University of Manitoba, is used for EM viral diagnostics by both the major health-care facility in Manitoba, Canada, and the Manitoba Provincial Laboratories; it examines approximately 2,300 clinical specimens annually. Our two facilities examine 70 to 90 vesicular specimens of suspected viral origin annually. In our experiences, the most effective methods of specimen collection from virus-induced blisters (or ulcers) involve opening the vesicle with a 26-gauge needle. The exudate may then be collected and prepared for examination in one of three ways: 1) Draw lesion aspirates into the barrel of the needle with a tuberculin syringe and cap the needle ; 2) touch a light microscope slide to the vesicle fluid; or 3) touch a 400-mesh, plastic-coated specimen grid directly to the base of the lesion . The samples may then be transported to an EM facility for preparation and examination. With the first two sample types, the sample is resuspended in approximately 20 µL of 0.2-µ pore-filtered, bidistilled water; this suspension is used to prepare a standard drop preparation on a 400-mesh, carbon-reinforced, plastic-coated grid. In all cases, the specimens are then negatively stained and examined.\n\n【2】Because of safety concerns about HIV infection, many health officials view transport of vesicle aspirates in capillary pipettes or needles as unacceptable. Glass slides are considered more acceptable, but still a risk. Since examination facilities or wards usually do not have the material to do direct touch preparations onto EM grids, many health officials advocate placing samples into transport medium. Alternatively, swabs may be used to prepare smears on glass slides for subsequent EM examination . Swabs in transport medium may be of value for culture or polymerase chain reaction procedures. However, in our experience these samples are not acceptable for EM diagnostics. Marshall and Catton suggest skin scrapings as an alternative to swabs . We find that these samples are preferential to swab specimens but not ideal. Our success rates in identifying herpesvirus and orthopoxvirus by drop method preparation  of vesicle aspirates are 62% to 80%, annually. The advent of sample transport as swabs has made additional procedures necessary to improve sensitivity and has delayed results. In Manitoba, direct centrifugation of samples to EM grids with the Beckmann Airfuge (Palo Alto, California, USA) is used as a nonspecific method of concentrating virus in sample preparations. This method increases the yield of viral particles by three or more orders of magnitude . In spite of this concentration method, the success rate in EM diagnostics using swab specimens has declined to below 10%, while viral agents continue to be identified in >60% of lesions in submitted aspirates.\n\n【3】Because concentration methods are not always available, and in view of the sample problems identified by Marshall ,we reviewed, in Winnipeg, whether collection of lesion fluids directly onto EM sample grids  improved sensitivity over aspiration into 26-gauge needles on tuberculin syringes . While neither method increased the number of cases identified in matched samples, the yield of virus seen in samples taken by touching the EM sample grid directly to the base of the lesion did increase, making it easier to identify viral agents in the samples (Hazelton and Louie, unpub. data). In Berlin, we also routinely find higher particle numbers on grids that have been prepared by the direct touch method. Transport of samples on EM sample grids is conducive to prolonged storage and transport of samples over long distances  and removes the risk of needle-stick accidents.\n\n【4】We continue to recommend examining grids touched directly to the lesion or vesicle aspirates. Where possible, infectious diseases and infection control staff contact the EM unit when a sample needs to be collected to receive instructions about methods and ensure that staff are available to conduct the examination. When the specimen needs to be transported some distance, such as between cities, smears on individually packaged glass slides or on sample grids are an alternative method for submitting vesicle aspirates. Glass slides allow the collection of samples for both polymerase chain reaction and EM examination (Charles Humphrey, personal communication). An additional advantage of smears is that interfering background proteins can be removed by drying the sample on the slide and then resuspending the viral agent. Proteins such as mucus, which interfere with staining and visualization, remain insoluble. We understand that other major viral EM diagnostic units also prefer aspirates, smears on glass slides, or lesion exudate on the final sample grid as preferred methods of submission of suspected blister material because of ease in handling and higher efficiency in examination.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dedcb9a3-f175-4f98-9251-ccc998dbcb85", "title": "Toscana Virus Infection in American Traveler Returning from Sicily, 2009", "text": "【0】Toscana Virus Infection in American Traveler Returning from Sicily, 2009\n**To the Editor:** Since the discovery of Toscana virus (TOSV) in 1971 in Tuscany , sandfly-borne TOSV has become recognized as a leading cause of acute meningitis in central Italy during the summer . France, Spain, Portugal, Greece, and Cyprus have also reported cases of TOSV infection . Although TOSV has been detected in sandflies in Sicily , we are not aware of any historically documented human infection with TOSV in this southernmost region of Italy.\n\n【1】We report TOSV infection of an American male physician, 65 years of age, who traveled to Sicily for 3 weeks and returned to the United States in October 2009. Two days after his return, he awoke with a headache, and hours later he noticed difficulty finding words. His headache progressed, and during the next few hours, he experienced severe expressive dysphasia. At admission to the hospital, he denied having fever, nuchal rigidity, photophobia, nausea, vomiting, or diarrhea.\n\n【2】Other than changing planes in Milan, the patient had remained in Sicily during the entire 3 weeks of his visit. He had sustained both mosquito and what he thought were flea bites while in Sicily. He had no known exposure to bats, rabid animals, or ticks.\n\n【3】Computed tomographic scan and magnetic resonance imaging of the brain showed no mass lesions or abnormality of the cerebral vessels. A sample of cerebrospinal fluid (CSF) obtained at admission showed 14 leukocytes/mm 3  (reference range 0–5 leukocyte/mm 3  ) with 100% lymphocytes, a protein level of 126 mg/dL (reference range 15–45 mg/dL), and a glucose level of 63 mg/dL (reference range 50–80 mg/dL). A nasopharyngeal swab specimen was negative for influenza A and B virus antigens. Other than a decreased thrombocyte count and an elevated serum glucose level, the results of complete blood count, comprehensive chemical panel, and coagulation studies were within normal limits. PCR results for CSF were negative for herpes simplex virus, enterovirus, and parechovirus. Test results for acute-phase and convalescent-phase serum specimens performed at the Washington State Department of Health Laboratory were negative for West Nile virus and St. Louis encephalitis virus immunoglobulin M.\n\n【4】Serum and CSF were sent to the Centers for Disease Control and Prevention in Fort Collins, Colorado. TOSV RNA was detected in a CSF sample collected on day 1 of illness by using reverse transcription–PCR . Plaque-reduction neutralization assays demonstrated a >4-fold rise in TOSV neutralizing antibodies between paired serum specimens collected on days 1 (titer <1:10) and 21 (titer 1:320) of illness. No similar rise in neutralizing antibodies to serologically related phleboviruses (e.g. sandfly fever Naples virus and sandfly fever Sicilian virus) was detected. The patient received supportive care only. He had a complete neurologic recovery in 10 days and was able to return to work.\n\n【5】Phylogenetic analyses indicate that 2 geographically distinct genotypes, the Italian and Spanish lineages of TOSV, circulate throughout the Mediterranean region . To determine the lineage of the infecting strain, we performed advanced molecular analyses of TOSV RNA isolated from the infected traveler’s CSF. These analyses used published consensus primers that target the small (S) segment  as well as primers newly designed to target the medium (M) segment: M 851F, 5′-ACCAAATACAACCATAGCCCC-3′ (forward) and M 1327c, 5′-ATACAATTCCCACAGTCGTTAG-3′ (reverse) of the multisegment TOSV genome. Reverse transcription–PCR amplification and nucleotide sequencing generated 2 nt sequences of 332 (S segment) and 424 (M segment) nucleotides in length. Phylogenetic analyses of the newly determined sequences and sequences previously determined for Mediterranean TOSV isolates of diverse origin were carried out by using MEGA version 4 . According to phylogenetic inference, the TOSV RNA identified in the returning traveler is of the Italian lineage . Of interest, the TOSV M segment sequence generated from this patient aggregates with extreme bootstrap support along with that generated previously from a strain of TOSV that was isolated from sand flies in Palermo, Sicily, in 1993 , indicating that the infecting strain is likely representative of strains that have circulated in Sicily for years.\n\n【6】This case represents the third report of meningitis or meningoencephalitis caused by TOSV infection in a US traveler to the Mediterranean (all acquired in Italy) . As is shown by this and other recent reports of TOSV infections in the Mediterranean islands surrounding Italy , the geographic range of TOSV human infections is larger than previously known. Reports of TOSV infection among European travelers returning from disease-endemic regions have provided additional evidence of the emergence of TOSV-related illness on a global scale .\n\n【7】Although the clinical course varies from asymptomatic infection to severe meningoencephalitis, TOSV should be included in the differential list of viral pathogens among patients who seek treatment with symptoms consistent with meningitis or encephalitis if the patients have recently traveled to Mediterranean areas, including Sicily. Because neither a vaccine nor specific antiviral drug treatment is available to prevent or treat TOSV infection, travelers to TOSV-endemic areas should be advised to take all precautions to prevent insect bites.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "adea6415-1d12-47dd-b112-275b220b7352", "title": "An Online Geographic Data Visualization Tool to Relate Preterm Births to Environmental Factors", "text": "【0】An Online Geographic Data Visualization Tool to Relate Preterm Births to Environmental Factors\nAbstract\n--------\n\n【1】Preterm birth (<37 weeks gestation) continues to be a significant cause of disease and death in the United States. Its complex causes are associated with several genetic, biological, environmental, and sociodemographic factors. Organizing and visualizing various data that may be related to preterm birth is an essential step for pattern exploration and hypothesis generation and presents an opportunity to increase public and stakeholder involvement. In this article, we describe a collaborative effort to create an online geographic data visualization tool using open software to explore preterm birth in Fresno County, where rates are the highest in California. The tool incorporates information on births, environmental exposures, sociodemographic characteristics, the built environment, and access to care. We describe data sets used to build the tool, the data-hosting platform, and the process used to engage stakeholders in its creation. We highlight an important example of how collaboration can increase the utility of geographic data visualization to improve public health and address health equity in birth outcomes.\n\n【2】Preterm Birth and the Need for Data Visualization Tools\n-------------------------------------------------------\n\n【3】Preterm birth (<37 weeks of gestation) contributes significantly to disease and death in the United States, both in the short term and long term. It is associated with higher death rates through infancy and childhood, decreased reproduction, increased risk of having preterm offspring, increased risk of high blood pressure, and symptoms of metabolic syndrome . The causes of preterm birth are complex and vary for early gestation (weeks) and late gestation (weeks) as well as spontaneous (eg, sudden or unplanned preterm birth) and medically indicated (eg, planned and induced preterm birth to minimize other health risks of the baby or mother) subtypes . Understanding the causes of preterm birth is vital to informing overarching risk reduction strategies and to developing early detection methods and interventions and can lead to new discoveries in subtype and population-specific risks. However, exploring the myriad of risks for preterm birth — from a woman’s health history, to biomarker data, to behaviors — is challenging for researchers, clinicians, and community health organizations seeking to understand preterm birth and work with women to reduce their risks. These challenges increase as the importance of environment and context become increasingly relevant in preterm birth research and clinical care. Factors such as air pollution, neighborhood environment, and socioeconomic status introduce new data and analytic challenges derived from geographic data formats, which must be integrated with traditional clinical data . Furthermore, new sources of open-source data are becoming increasingly available, leading to new research and data integration opportunities for better understanding preterm birth .\n\n【4】At a population level, health data can be linked geographically through address, census tract, and zip code information. Exploratory spatial data analysis (ESDA) uses geographic linkages to explore patterns, compare nearby geographic regions, and analyze spatial clusters , which can indicate underlying place-based variables important to a health outcome, such as crime, pollution, or unexplored environmental factors . Visualization of such data is essential for pattern exploration and hypothesis generation. Data visualization offers a field of research and developed tools for exploring patterns, identifying relationships, and synthesizing information in large, multiscale, and multivariate data sets . Being able to explore and visualize multilevel and multifactor risks for preterm birth may lead to new mechanistic hypotheses , and allow researchers, clinicians, and community health organizations to work with patients in the context of population-level patterns . For such a visualization tool to be useful it must be easy to use and immediately accessible, preferably through an online platform; it must leverage ESDA and geographic data science exploratory tools  and must have an integrated data structure that includes medical, behavioral, social, and environmental factors.\n\n【5】In this article we discuss the collaborative efforts of several organizations in Fresno County and the state of California to create an online data visualization and exploration tool by using open software  to describe preterm birth in Fresno County. We describe data sources and data collection, data features, and mapping functions of the tool. We highlight the utility of online geographic data visualization to explore possible causes of preterm birth and interventions to address it. We also detail how such tools can be built in collaboration with on-the-ground organizations and stakeholders.\n\n【6】Setting and Partners\n--------------------\n\n【7】Fresno County, California, exemplifies the many challenges presented by the complex and multiple-pathway mechanisms of preterm birth. According to vital statistics for 2007 through 2012 of the California Department of Public Health, Fresno County had the highest overall preterm birth rate in the state, 9.9%, representing 3.7% of California’s preterm births. At a finer geographic scale, 41.4% of all preterm births in that time frame occurred in the south and west-central areas of the city of Fresno. These are the most populated areas of the county, and more than 70% of pregnant women residing there receive Medi-Cal health insurance for prenatal care or delivery on the basis of low-income status. For these reasons, Fresno County is one location of focus for the University of California–San Francisco (UCSF) California Preterm Birth Initiative, a multiyear interdisciplinary research effort with the goal of reducing the prevalence of preterm birth. The Fresno County part of the initiative is a Collective Impact effort that brings together strategic partners from different sectors to focus on the prevention of preterm birth and to address racial and ethnic disparities in its prevalence. Members include local leaders representing public institutions (Fresno County Office of Education, Fresno Housing Authority, Fresno Police Department), public health (Fresno County Department of Public Health; Special Supplemental Nutrition Program for Women, Infants, and Children; First 5 of Fresno), health systems and hospitals (Cal Viva Health, Valley Children’s Healthcare, Community Medical), higher education (Fresno State University, UCSF–Fresno), health clinics (eg, Clinica Sierra Vista), community benefit organizations (eg, AMOR Foundation, Every Neighborhood Partnership), and mothers who experienced preterm birth. Collectively, these groups have committed to address and reduce preterm birth.\n\n【8】A research team from the University of California–San Diego and UCSF composed of geographers, computer scientists, research experts on preterm birth, and a practicing obstetrician was formed in 2016. The team partnered with the Collective Impact effort in Fresno County to begin planning an online geographic data visualization tool that could aid in the assessment, exploration, and discovery of patterns in preterm birth and other social, environmental, and hazard factors. The team met several times with members of the Fresno County Preterm Birth Initiative to obtain feedback about data to include in the tool and the tool’s usability and design.\n\n【9】Data Inputs and Collection\n--------------------------\n\n【10】Finding, formatting, and amalgamating data inputs is one of the largest tasks of any data visualization project. A core goal of this project was to collect data resources that may not be traditionally associated with preterm birth to aid in research discovery and relationship exploration. As a start to the project, various preterm birth researchers were invited to brainstorm variables and data of interest ranging from birth outcomes to environmental factors to supportive resources. From those lists, the core team worked to collect as many data sources as possible . Data were formatted at 2 geographic levels: census tracts, when possible, and Medical Service Study Areas (MSSAs). MSSAs are geographical analysis units defined by the California Office of Statewide Health Planning and Development (OSHPD) and are based on census tracts. Numerous health-related state data sets are provided only at the MSSA unit. Three main types of software were used to amalgamate and format data: ArcMap, version 10.5 (Esri), Microsoft Excel (Microsoft Corp), and SPSS (IBM Corp). All data were manually entered into master CSV (comma-separated values) spreadsheets by geographic unit with data dictionaries. The process included manual data curation decisions, for example, deciding what variables to keep or discard because of redundancy, or methods of aggregation such as counts versus averages.\n\n【11】Data about births were obtained from a birth cohort database maintained by OSHPD. These data are shared through data use agreements and were obtained through collaboration with the California Preterm Birth Initiative under an institutional review board–approved protocol from the California Committee for the Protection of Human Subjects, protocol no. 12–09-0702, ongoing since 2009. The birth cohort file contains detailed information on maternal and infant characteristics derived from linked hospital discharge, birth certificate, and infant death records. Included in the file were all singleton births in Fresno County from 2007 through 2012 with an obstetrician’s best estimate of a gestation at delivery of 20 to 44 weeks and with no known chromosomal abnormalities or major structural birth defects . Births were categorized by weeks of gestation based on best obstetric estimates (early preterm birth, gestational age <32 wk; late preterm birth, gestational age 32 to ≤36 wk; term birth, gestational age ≥37 wk) and spontaneous and medically indicated subtypes of birth. Additional data derived from the linked birth certificate and hospital discharge data were race/ethnicity, age, parity, country of birth, pre-pregnancy weight, height, insurance status, smoking, diabetes, hypertension, anemia, maternal education, reported drug use, diagnosed infection, mental illness, and for multiparous women, known number of previous caesarean sections and interpregnancy interval. Data files provided diagnoses and procedure codes based on the International Classification of Diseases, 9th Revision, Clinical Modification . The final data set included 81,021 women. Data were aggregated to MSSAs and census tracts with a minimum of 16 women per geographic unit to preserve privacy. In addition to the OHSPD data set of women giving birth, several other indicators were collected from a variety of data sources including the US Census, the Environmental Protection Agency, California Health and Human Services, Fresno County, Esri (Environmental Systems Research Institute), Google, and other California State agencies . Almost all data sets included in the tool are open source and available online, except for data about crime. Not all the indicators align in date with the birth data  because of availability of online data sets. This is a limitation of the tool, which will be improved when newer birth data can be incorporated and as more data are made available online to include past years. Data ingestion is ongoing, and new data sets will be added as they are made available and processed.\n\n【12】Online Infrastructure and Key Features\n--------------------------------------\n\n【13】The online platform has 2 sides with a user-friendly front end featuring simple click and view options for preterm birth–related topics, and a password protected back end for more advanced users with more data and complex visualization options. Software to develop the tool are all open source framework environments and libraries built with shareability and reproducibility in mind. Using the open source frameworks can allow the tool to be repurposed not only for health but also for other data applications. Code developed for the front-end infrastructures is available online through our GitHub repository , with back-end code to be added in the near future. We encourage users to use these codes for their own health data visualization projects and to contribute new visualization features back to the repository as they are developed. The front-end development was a direct result of input from stakeholders who, early in the process, voiced a need for a user-friendly and guided data experience for people unfamiliar with data exploration and visualization techniques. Thus, the 2 applications were designed differently to provide their targeted audiences with the data visualization and analytic tools that support knowledge discovery for preterm birth.\n\n【14】The topic-driven front end is intended to create a guided experience to browse data by predefined topics (eg, demographics, environmental pollution) with curated data selected by the team. Each topic has no more than 15 variables per screen, thus limiting the scope of data exploration and making it more manageable for lay users. The data-driven back end application allows users to examine preterm birth–related variables by region and by their relationships with other variables with no limit to the quantity or type of variables included in an analysis. For example, a researcher can explore associations between very preterm birth (between 28 and 32 wk), environmental pollution, socioeconomic disadvantage, and access to care simultaneously.\n\n【15】When logging onto the public site for the first time , the user is welcomed with a video briefly explaining the California Preterm Birth Initiative. The 8 topical areas (birth data, health care, demographics, environmental pollution, socioeconomics, pregnancy-related factors, built environment, and health risk) encourage users to begin their exploration of preterm birth and selected geographically organized data. These topics are built with a module tab design pattern where content for each topic is constructed in a separate tab panel and only 1 topic is viewable at a time. When a topic is clicked, the first window to appear in the new page is information about the sources of data being displayed. Within each topic tab, data visualization components are laid out in a side-by-side grid system. These data visualization components are modularized map or graph items that support interactive data visualization by itself or with intercomponent highlighting (across map and graph) and synchronization (between maps). We selected the combination of data visualization components for each topic on the basis of the data type and the spatial resolution of data. For example, pollution data are presented solely with map representations , whereas birth data include graphical representations of indicators .  \n\n【16】A. Demonstration of the environmental pollution topic of an online geographic data visualization tool for exploring preterm birth in Fresno County, California, using linked maps. Different indicators can be selected for each map. B. The birth topic with overall preterm birth shown on the map and spontaneous preterm birth shown on the indicator histogram. Data elements are linked so that as selection of 1 data value occurs, the location of that data value is displayed on the map. \n\n【17】Underlying the back end of the site is the National Science Foundation–supported DELPHI (Data E-Platform Leveraged for Patient Empowerment and Population Health Improvement) developed at UC San Diego, which was implemented to design an asthma management system in San Diego County  and can support data-driven public health discoveries from multiple approaches. The site is organized by 4 main visualization functionalities to allow for data exploration by geographic region, by indicator , by indicator relationships, and by correlation matrixes . These different functions give the user multiple options to explore data relationships through dynamic pie chart visuals, histograms, ranked associations with other indicators, and heat-map matrix visuals. All functions are linked and highlighted when hovering over them so that as 1 data element or geographical unit is selected, the corresponding units are highlighted. All data are included in the back-end site and are organized by topic area. This organization does require the user to filter through and select variables of interest, but is an important aspect of the data exploration process. In numerous pages the option to save data selections is included so that users can go back and reload previous visualizations. Users can also export various results and outputs into Excel, CSV, and PDF. In future iterations we plan to add ability to download spatial data formats as well.  \n\n【18】**  \nA. Demonstration of the indicator explorer feature of an online geographic data visualization tool to explore preterm birth in Fresno County, California. The indicator explorer feature allows mapping and side-by-side histogram evaluation of any of the hundreds of variables included in the back-end site. As one variable is selected, the relationship between that indicator and others can be viewed in alternate tabs. B. The relationship explorer tab, which builds correlation matrixes for selected indicators. Users can change color, specify the minimum number of geographical units that must be included, and specify the minimum correlation value. \n\n【19】We used several popular programming framework environments and libraries to develop the applications. Both the front-end and back-end site were developed with the Bootstrap framework (Bootstrap) and the Node.js (Joyent, Inc) environment. The visualization components are built around the D3.js (Mike Bostock) library in the back-end site, and the front-end site mixes up D3.js and its extensions for the chart items and the open-source Leaflet.js (Vladimir Agafonkin) for some mapping features. The data framework transfers processed data between the PostgreSQL (PostgreSQL Global Development Group) database and the visualization platforms through Node.js routes and server-side SQL functions.\n\n【20】Stakeholder Involvement\n-----------------------\n\n【21】The design team followed a user-centered design model for facilitating stakeholder involvement and designing the tool for optimal interface success, which proved successful for designing web-mapping, visualization, and data exploration projects . We used the user–utility–usability loop developed by Roth, Ross, and MacEachren, in which we collected input and feedback on needs and designs from preterm birth researchers and stakeholders (user), prompting revisions to the conceptualization and functional requirements of the tool (utility), leading to new versions of the data visualization tool (usability) for additional evaluation by our target users, thus restarting the loop .\n\n【22】An initial draft of the visualization tool using the DELPHI platform was presented to the Fresno County Preterm Birth Initiative in early 2017. Feedback obtained from stakeholders included comments about variables to be used, geographic resolution, and the need for a more user friendly and simple site that would allow less advanced users to explore and visualize key data sets. The public-facing side of the site was presented again to the Fresno County Preterm Birth Initiative in early 2018 to obtain further input and to narrow down the key topics of interest. The design team then worked with the Fresno County Preterm Birth Initiative Shared Measures Committee, a subgroup of the Fresno initiative that focuses on data and measurement issues, over several meetings to come up with 8 topics to feature on the public-facing side of the tool. The Shared Measures Committee comprises cross-sector leaders and experts in measurement and evaluation and a mother who experienced preterm birth. This committee helps set goals, inform strategies, and establish or develop measures of progress for the Fresno initiative. The iterative feedback between the design team and the Shared Measures Committee was critical for the design of the tool and for determining how the 8 topics should be populated and visualized. Having the design team attend multiple meetings of the Fresno initiative gave additional context to challenges surrounding preterm birth, such as social disadvantage and health care access, and the need to represent such phenomena in maps in a transparent way. Further discussions highlighted how the tool needed to be easy to use for nonprofit organizations to create figures for grant applications and accessible to the public and elected officials. In addition, the tool needed to be bilingual, for both English and Spanish speakers.\n\n【23】The tool was presented to the public and other Fresno initiative stakeholders in July 2018 at the Fresno County Preterm Birth Initiative Forum. The purpose of the event was to convene community members and stakeholders to raise awareness about preterm birth and communicate Preterm Birth Initiative strategies, successes, and challenges. As an example, the team walked through an assessment of environmental pollution as related to preterm birth from the viewpoint of a concerned community member and then from the viewpoint of a public health researcher. Beginning with the front-end, demographics around a neighborhood of interest as related to preterm birth were examined using pie charts and histograms with linked maps (eg). Moving to the environmental pollution tab, rates of ozone, PM2.5 (atmospheric particulate matter with a diameter of less than 2.5 micrometers), traffic density, and toxic release were examined in relation to 3 neighborhoods in downtown Fresno (eg). Resulting figures could be used in public presentations, community discussions, or public grant applications. The demonstration then turned to the back-end site where a researcher might want to examine all possible pollution factors in association with not only preterm birth rates, but also mothers’ rates of hypertension, community asthma rates, and poverty indicators by using a correlation matrix (eg). The tool was presented alongside a poster outlining “Unequal Neighborhoods: Fresno” research on historical zoning and land use policies influencing health disparities. At the event, attendees were encouraged to interact with the tool and with the design team to ask questions and provide feedback. They were given a link to submit further feedback through an online survey and to stay engaged with the project. Feedback is considered and incorporated into the tool design.\n\n【24】Challenges, Next Steps, and Final Thoughts\n------------------------------------------\n\n【25】The project faced numerous challenges. Data collection, formatting, and updating from various agencies and resources will continue to be a significant task moving forward. The issue of updating data in future years as funding for the project expires is a current topic of discussion for the Fresno County initiative. Our current funding from the California Preterm Birth Initiative supports travel to Fresno and meeting support with stakeholders, a part-time developer, and a part-time–equivalent data-focused researcher. At the very least, a sustainable data updating plan will need to be enacted so that the tool can become a staple feature of the Fresno County Preterm Birth Initiative. As new and updated data are pulled into the tool, the team will also have to face the challenge of demonstrating change from time-based data. The collaborative nature of the project, while an essential feature of making the tool a success, is also not without challenges. When soliciting feedback from stakeholders, the design team had to be cognizant of what was possible in terms of data limitations, the design work involved, and timeline management. Balancing what is possible and what is feasible within a project scope and budget is a challenge for any collaborative project.\n\n【26】The next step in the process will be further dissemination of the tool, because use of the tool by stakeholders thus far has been limited. To this end, the authors are planning a series of workshops focused on local policy makers and staff members, nonprofit organizations, and academic researchers. The tool will also be presented to obstetrics and gynecology residents and students during a UCSF–Fresno grand rounds. The team is also creating a set of how-to videos with examples and trainings to explain how to use tool components and featuring specific use cases such as generating visuals for a grant application or for presentation to a community group, drawing on feedback and previous research showing the need for training across health and geospatial visualization tools . We are working to create an Esri story map as an example of how a narrative concerning preterm birth might be created using data on the tool site, and we will also explore open source visualization publishing tools to allow advanced users to create their own story-like narratives from the data visualizations. In the future, a major goal will be to allow users to securely upload and analyze their own collected data sets against the backdrop of public data amalgamated in the tool. We are also in the process of developing similar tools for the San Francisco and San Diego regions.\n\n【27】The Fresno Preterm Birth Initiative’s data visualization tool provides information on births, environmental exposures, sociodemographic characteristics, the built environment, and access to care in a format that makes the information more accessible than it has ever been before. The front-end site design was customized to suit each topic and made as intuitive as possible, allowing community users to engage with preterm birth data, and allowing stakeholders from nonprofit organizations to use figures and data in the tool for grant writing, communication, and policy discussions about preterm birth. The back end offers significantly more data for exploration, along with more powerful tools for data visualization and relationship discovery. The design process, with significant input from the Fresno initiative, preterm birth researchers, and other stakeholders demonstrates the power of working with health and community experts who work daily to improve public health. The original concept of the tool focused solely on using the DELPHI platform and would have been inaccessible to many potential users. Through iterative feedback the design team was able to create a user-friendly front-end site and had significant assistance in identifying and obtaining data sets for inclusion in the tool. Furthermore, because of the collaboration, dissemination of the tool is increased by public events such as the Fresno County Preterm Birth Initiative Forum and publicity of the tool through the initiative’s platforms. Lastly, our feedback to date is unanimously positive. Users of the tool report powerful value for the community at large and express hope to see it grow.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f3c37dd9-4984-4c8d-9217-41e6c95ea1a9", "title": "Outbreak of Haff Disease along the Yangtze River, Anhui Province, China, 2016", "text": "【0】Outbreak of Haff Disease along the Yangtze River, Anhui Province, China, 2016\nHaff disease is an unexplained rhabdomyolysis that occurs within 24 hours after consumption of certain types of freshwater or saltwater fish . It was first reported in 1924 in the vicinity of Königsberg along the Baltic coast near Frisches Haff . Over the next 9 years, an estimated 1,000 persons were affected by similar outbreaks, occurring seasonally in the summer and autumn in this area . Although subsequent outbreaks were identified in several other countries, such as Sweden , the former Soviet Union , Brazil , Japan , and China , the etiology has not yet been determined. An unidentified heat-stable toxin similar to cyanotoxins or palytoxin, but primarily myotoxic and not neurotoxic, is thought to be the cause of Haff disease ; however, evidence supporting this hypothesis has been scant.\n\n【1】In July 2016, the number of rhabdomyolysis cases reported to the National Foodborne Disease Surveillance System (NFDSS) in China dramatically increased in Anhui Province compared with previous years. Most of the cases were reported in Wuhu and Ma’anshan, cities in Anhui Province in eastern China. Epidemiologic features were compatible with Haff disease . Preliminary investigation implicated crayfish as the vector. On August 5, the number of cases surpassed 200, prompting an emergency investigation by the Chinese Field Epidemiology Training Program, together with the Anhui Province Center for Disease Control and Prevention (CDC). The objectives of the investigation were to describe the epidemiologic and clinical characteristics, trace back the implicated vectors, identify possible risk factors, and recommend control measures.\n\n【2】### Methods\n\n【3】##### Case Definition and Finding\n\n【4】We defined a case of rhabdomyolysis as any person with elevation in creatine kinase (CK) value plus clinical manifestations of myalgia or limb weakness . We defined a Haff disease case as illness in any person with acute onset of rhabdomyolysis after ingestion of freshwater fish or seafood within 24 hours in Anhui Province during June–August 2016. We searched for physician-diagnosed rhabdomyolysis cases from the NFDSS, an internet-based, passive surveillance system for foodborne diseases searchable by food source in China. We also reviewed the outpatient and inpatient medical records in hospitals in Wuhu and Ma’anshan during the outbreak period to search for potential rhabdomyolysis cases.\n\n【5】Local Anhui Province CDC staff or Chinese Field Epidemiology Training Program trainees interviewed all rhabdomyolysis case-patients, either in-person or by telephone, using a structured questionnaire. Information collected included age, sex, date of onset, disease duration, clinical symptoms, potential risk factors (e.g. food, drugs, alcohol consumption, intense exercise, allergy history, underlying chronic illness), and the quantity of crayfish consumed. The researchers also obtained laboratory test findings from hospital medical records. They applied the Haff disease case definition to rhabdomyolysis cases to identify Haff disease cases and collected blood and urine specimens from Haff disease case-patients for further analysis.\n\n【6】In total, 673 rhabdomyolysis cases were identified in Anhui Province during June–August 2016. Of these, 99.9% (/673) were compatible with the definition of Haff disease. All but 1 patient consumed cooked crayfish before symptom onset. The patient who did not eat cooked crayfish was a steelworker who had been working in the factory before onset, suggesting his illness might have been caused by heatstroke.\n\n【7】##### Case–Control Study\n\n【8】Although nearly all Haff disease case-patients ate cooked crayfish, we did not know what percent of persons who did not become ill also ate meals containing crayfish, given that crayfish were widely available during June–August 2016. Therefore, we conducted a matched case–control study to assess the association between eating crayfish and Haff disease. Cases in this study were persons who met the definition for Haff disease, had shared a meal with someone else before symptom onset, and consented to participate in the study. We identified \\> 1 control per case; controls were selected among persons who shared the suspected meal of exposure with the case-patient before symptom onset. Controls had no clinical symptoms compatible with rhabdomyolysis and consented to participate in the study. Persons with other illnesses (e.g. fever, cold, injury, etc.) were disqualified as controls. In total, 67 cases and 108 controls were enrolled in the case–control study. Trained investigators conducted telephone-based interviews August 7–15, 2016, using a standardized questionnaire.\n\n【9】##### Traceback of Food and Environmental Investigation\n\n【10】For all Haff disease cases, we conducted a traceback investigation for the source of the implicated food by interviewing case-patients, restaurant owners, fishermen, and crayfish sellers. We conducted an environmental investigation of potential contamination along the distribution chain or unusual events during the outbreak period. We investigated the restaurants where case-patients had a meal before onset to find out where the crayfish came from and how they were cooked. We also visited crayfish farms, settings where crayfish were caught, and factories along the Yangtze River to identify whether the implicated crayfish or the environment in which the crayfish were raised had been contaminated.\n\n【11】##### Data Analysis\n\n【12】We performed statistical analysis using SPSS Statistics 20 . We compared cases and controls by χ 2  test. Significant risk factors (p<0.05) in the χ 2  tests were included in a multivariate Cox proportional hazard model to determine the odds ratio (OR) and 95% CI for the potential risk factors associated with Haff disease. All of the p values were 2-sided, and p<0.05 was considered significant.\n\n【13】### Results\n\n【14】##### Confirmation of the Outbreak\n\n【15】In total, we verified 672 Haff disease cases in Anhui Province during June–August 2016. All cases occurred in 7 cities along the Yangtze River in Anhui Province; 83.3% (/672) of the cases occurred in Wuhu (cases) and Ma’anshan (cases). We focused our investigation on the cases that occurred in Wuhu and Ma’anshan. Of the 560 case-patients in Wuhu and Ma’anshan, 495 (.4%) completed the questionnaires; all 495 had consumed crayfish within 24 hours before symptom onset. The epidemic curve suggested a continuing common-source outbreak .\n\n【16】##### Descriptive Epidemiology\n\n【17】The outbreak started at the end of June, peaked in mid-July to early August, and lasted through August 17. Of the 495 case-patients, 197 (.8%) were hospitalized; mean length of hospital stay was 7.3 ± 3.2 days. No deaths were reported. The mean age of the case-patients was 38.7 ± 13.5 years; 323/495 (.3%) patients were female. Although cases were widely distributed in the 2 cities, 87.7% (/495) were in residents from urban areas close to the Yangtze River. Each case-patient consumed a mean of 11.6 ± 6.1 crayfish pieces. The mean incubation period was 6.2 ± 3.8 hours.\n\n【18】##### Clinical Characteristics\n\n【19】All 495 case-patients experienced myalgia that was local or diffuse, involving the back, waist, whole body, neck, limbs, and chest . A total of 271/495 (.7%) experienced muscle weakness. Additional symptoms included brown urine, dyspnea, vomiting, abdominal pain, dizziness, and headache. Symptoms of nerve paralysis and fever were rare. Acute renal failure was not observed.\n\n【20】##### Laboratory Characteristics\n\n【21】We reviewed the laboratory test findings of blood and urine for some cases. The mean value of myoglobin was 330.0 ± 121.2 ng/mL, and mean CK level was 5,439.2 ± 4,765.1 U/L. In >80% of the cases, the levels of muscle-type CK and aspartate aminotransferase were abnormally elevated. In addition, 50.0% of case-patients were positive for urinary occult blood and proteinuria .\n\n【22】##### Case–Control Study\n\n【23】In the case–control study, 100% of the 67 cases and 93.3% (/108) of controls ate crayfish during their shared meal (OR = ꝏ, 95% CI 0.92–ꝏ). We observed a significant dose-response relationship between the number of pieces of crayfish eaten and Haff disease (χ 2  \\= 29.225; p<0.001) . Further analysis showed that eating crayfish liver was associated with increased disease risk (OR = 4.0, 95% CI 1.2–12.7).\n\n【24】##### Traceback and Environmental Investigation\n\n【25】Wuhu and Ma’anshan are located in the middle to lower reaches of the Yangtze River. Crayfish is a popular dish for residents of these 2 cities. Before the Haff disease outbreak, Anhui Province experienced heavy rainfall, which caused the largest flood disaster in decades. Consequently, rain or floodwater was retained in irrigation ditches and detention ponds for an extended time, and the amount of crayfish caught on the shores of the Yangtze River or its connected ditches was 5–10 times more during the outbreak period. However, no industrial or chemical contamination along the Yangtze River was reported.\n\n【26】The only common risk factor for all cases was eating crayfish, which were cooked thoroughly. We conducted a traceback investigation of the source for the implicated crayfish in Ma’anshan and Wuhu by interviewing persons in markets, restaurants, fisheries, and settings where crayfish were caught, as well as fishermen; we were able to trace 50.1% (/495) of the implicated crayfish to their sources. Of these, 96.8% (/248) were wild crayfish caught on the shores of the Yangtze River or its connected ditches. When we consulted with crayfish biologists, we found that the species of crayfish implicated during this outbreak was _Procambarus clarkii_ .\n\n【27】##### Public Health Measures\n\n【28】Local governments issued a warning about the dangers of eating crayfish. In addition, public health departments instituted continuous surveillance and investigation of the outbreak.\n\n【29】### Discussion\n\n【30】The epidemiologic and traceback investigations of a large outbreak of Haff disease in Anhui Province, China, indicated that all case-patients consumed crayfish within 24 hours before symptom onset; the implicated crayfish were caught on the shores of the Yangtze River or its connected ditches. The case–control study revealed that eating the liver of crayfish was associated with an increased risk for disease; the risk increased as the quantity of crayfish eaten increased.\n\n【31】In China, the earliest reported outbreak of Haff disease was in Beijing in 2000 and involved 6 cases . An epidemiologic study revealed that all patients ate crayfish before onset, suggesting a link between crayfish and Haff disease . Although the literature shows that eating several species of fish, such as buffalo fish , salmon , freshwater pompano , marine boxfish , and pomfrets , could trigger Haff disease, almost all Haff disease cases in China were associated with eating crayfish . In recent years, Haff disease outbreaks have been reported in other cities in China . These outbreaks prompted the China CDC to conduct a thorough investigation of Haff disease. Crayfish have become a popular seafood for residents in central and eastern China, especially in June–September. Previous studies have reported that Haff disease shows a seasonal pattern, and outbreaks usually occur in the summer and fall months . Although a large Haff disease outbreak caused by eating freshwater pomfret occurred in October 2009 in southern China , most crayfish-related outbreaks , clusters , and sporadic cases  occurred predominantly in the summer. Seasonal crayfish harvest and consumption in June–September likely increases the opportunities for exposure, which may partially explain the seasonal pattern of Haff disease in China .\n\n【32】The most commonly reported clinical features in this outbreak were myalgia and muscle weakness, as well as abnormal levels of myoglobin and CK. Increased serum myoglobin concentration is the basis for early diagnosis of rhabdomyolysis ; however, myoglobin concentrations tend to normalize within 6–8 hours following exposure. Thus, the window of opportunity for diagnosis is short . Of note, elevated myoglobin concentrations were observed in all case-patients who were tested in this study; this may be due to prompt medical care and timely laboratory testing in the hospital.\n\n【33】Rhabdomyolysis is a common life-threatening syndrome characterized by the injury of skeletal muscle resulting in the leakage of intracellular contents into the circulatory system . Patients with rhabdomyolysis usually experience myalgia, muscle weakness, raised serum CK, and brown urine . The etiologic spectrum of rhabdomyolysis is extensive, including crush injuries, ischemia, strenuous exercise, extreme body temperatures, drugs, toxins, infections, hereditary causes, and inflammatory or autoimmune muscle disease . A substantial number of patients may have no cause identified. We found that nerve paralysis and fever were rare symptoms, all crayfish were cooked thoroughly, and no industrial or chemical contamination was identified; therefore, this outbreak was unlikely to have been caused by infectious or chemical etiologies. Diaz et al. reported that an unidentified, heat-stable, algal toxin with primarily myotoxic rather than neurotoxic properties in seafood has been proposed as a cause of Haff disease ; whether this toxin also exists in crayfish remains unknown.\n\n【34】Although many Haff disease cases have occurred in cities located in the middle to lower reaches of the Yangtze River, the association between Haff disease and crayfish caught from Yangtze River has not been elucidated in the published literature . In recent years, 3 other large Haff disease outbreaks have been reported in Nanjing and Tongling, 2 other cities located in the middle to lower reaches of the Yangtze River . The fact that these outbreaks all occurred in the middle to lower reaches of the Yangtze River suggests that crayfish could be their common etiology.\n\n【35】Studies using a mouse model have found that the hazardous substance from crayfish could cause rhabdomyolysis . This hazardous substance is specific to certain batches of crayfish. A dose-response relationship has also been observed. These findings in laboratory animals were consistent with the results of human epidemiologic investigation  and with our case–control study findings.\n\n【36】Our study had several limitations. First, because we lacked data on how many persons ate crayfish in the 2 study cities, we could not calculate the attack rates. Second, not all crayfish were traced back to their sources. Third, we were unable to conduct animal experiments to prove causation.\n\n【37】In conclusion, during this outbreak, the risk for Haff disease was associated with eating crayfish along the Yangtze River. The etiology of Haff disease remains elusive due to lack of knowledge of the underlying disease mechanism of rhabdomyolysis. Our findings might help researchers isolate the toxin that causes this disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7e9cf804-4f77-4061-adcf-883f3fe68917", "title": "Matrix Protein 2 Vaccination and Protection against Influenza Viruses, Including Subtype H5N1", "text": "【0】Matrix Protein 2 Vaccination and Protection against Influenza Viruses, Including Subtype H5N1\nYearly development of influenza vaccines that are antigenically matched to circulating strains pose extraordinary challenges. A rapidly developing pandemic would shorten the time for strain identification and vaccine preparation; meanwhile, antigenic changes would continue. Moreover, the need to immunize an entirely naive population would exacerbate problems with vaccine production and supply.\n\n【1】Vaccines based on conserved antigens would not require prediction of which strains would circulate during an approaching season and could avoid hurried manufacturing in response to outbreaks. Test vaccination with DNA constructs that express conserved influenza A nucleoprotein (NP) or NP plus matrix (M) induced antibody and T-cell responses and protected against heterosubtypic viruses . Despite the virulence and rapid kinetics of challenge infection, DNA vaccination with NP and M achieved limited protection against an H5N1 virus strain isolated from the 1997 human outbreak in Hong Kong .\n\n【2】The M gene of influenza A encodes 2 proteins, both highly conserved: M1, the capsid protein, and M2, an ion channel protein. M2 contains a small ectodomain , M2e, which makes it a target for antibody-based immunity. The ability of anti-M2 monoclonal antibody (MAb) to reduce viral replication  implicates M2, in particular M2e, as a vaccine target. M2 vaccine candidates that have been explored include peptide-carrier conjugates , baculovirus-expressed M2 , fusion proteins , multiple antigenic peptides , and M DNA constructs that potentially express M2 . In those studies, mice were protected against challenge with homologous or heterosubtypic viruses, but even the heterosubtypic viruses had an M2e sequence identical to the vaccine constructs or differed by only 1 amino acid.\n\n【3】Although most human influenza viruses of H1, H2, or H3 subtypes share identity with the M2e consensus sequence (M2e-con) , some influenza A viruses do not. In a study of M2e-carrier conjugate vaccines, serum antibodies specific for M2e-con or M2e-A/PR/8/34 (H1N1) did not cross-react with M2e peptides from H5 and H7 subtype avian viruses that have 3 or 4 mismatches . In another study, monoclonal and polyclonal antibodies reacted with a subset of avian sequences . Although a recent study used M2e peptide-liposome vaccines of subtypes including H5N1 with matched challenge viruses , no prior work has documented protection against challenge with influenza viruses in which M2e sequences differed substantially from those of the immunizing antigen.\n\n【4】Priority is being given to developing vaccines that offer broad protection against multiple influenza subtypes, including H5N1. Indeed, development of conserved-antigen vaccines, and specifically M2-based vaccines, is part of the US Department of Health and Human Services Pandemic Influenza Plan . We therefore evaluated M2-based vaccine efficacy against divergent challenge viruses.\n\n【5】### Methods\n\n【6】#### Mice\n\n【7】Female BALB/cAnNCR mice were purchased from Division of Cancer Treatment, National Cancer Institute, Frederick, Maryland, USA. The institutions’ Animal Care and Use Committees approved all protocols for animal experiments.\n\n【8】##### Viruses\n\n【9】Influenza viruses used were A/PR/8/34 (H1N1) , A/FM/1/47-MA (H1N1) , and A/Thailand/SP-83/2004 (H5N1) . Some virus stocks were propagated in the allantoic cavity of embryonated hen eggs at 34°C for 48–72 h (A/PR/8) or 37°C for 24 h (SP-83). A/FM was prepared as a pooled homogenate of lungs from BALB/c mice infected 4 days previously. All experiments with H5H1 subtypes were conducted under biosafety level 3, enhanced containment.\n\n【10】##### Peptides and Peptide Conjugates\n\n【11】M2e 2–24 peptides (no NH2-terminal methionine) were synthesized with COOH-terminal cystine residue and conjugated to maleimide-activated keyhole limpet hemocyanin (KLH) for vaccines. The same peptides were also synthesized without COOH-terminal cystine and used for antibody and T-cell assays. Influenza A NP147–155 and M2e peptides were synthesized in the core facility of the Center for Biologics Evaluation and Research, US Food and Drug Administration. Severe acute respiratory syndrome (SARS) matrix peptide  was provided by the National Institutes of Health.\n\n【12】##### Vectors\n\n【13】Plasmid and recombinant adenoviral (rAd) vectors that express B/NP and A/NP have been described , as has the plasmid containing the entire M gene of A/PR/8 . The plasmid VR1012-M2 (termed M2-DNA above) was generated as follows. The plasmid pCR3-M2 was derived by PCR from the vector pCR3-M previously generated from A/PR/8 virus by reverse transcription–PCR . To modify the sequence to the widely shared M2e sequence, full-length consensus M2 cDNA with Kozak sequence at its 5′ end was generated from 2 overlapping M2 DNA fragments and subcloned into VR-1012, obtained under material transfer agreement from Vical, Inc. San Diego, CA, USA. The sequence of the M2 insert was confirmed by restriction digestion and sequence analysis. The replication-incompetent adenovirus that expressed the M2 protein with the consensus sequence (M2-Ad) was constructed by using Gateway cloning and the ViraPower Adenoviral Expression System (both by Invitrogen, Carlsbad, CA, USA) according to manufacturer’s instructions. Briefly, the M2 cDNA from VR1012-M2 was cloned by PCR into the pENTR/D-TOPO Gateway vector and then transferred into the pAd/CMV/V5-DEST adenoviral Gateway vector by LR Clonase (Invitrogen) reaction to give pAd/CMV-M2. Integrity and proper insertion of the cloned M2 cDNA were confirmed by sequencing. M2-Ad was generated by transfection of 293A cells with pAd/CMV-M2. M2 expression was confirmed by immunohistochemical staining of M2-Ad-infected Madin-Darby canine kidney cells with M2-specific polyclonal sera (data not shown). High-titered stocks of rAd were prepared by ViraQuest, Inc. (North Liberty, IA, USA). Adenovirus stocks were stored in 3% sucrose/phosphate-buffered saline (PBS) at 1–2×10 12  particles/mL and confirmed as negative for replication-competent adenovirus by passage on nonpermissive cells.\n\n【14】##### Immunization\n\n【15】Mice were given an intraperitoneal injection of 40 μg peptide-KLH or unconjugated KLH in complete Freund’s adjuvant (emulsified 1:1 with antigen in PBS). Three weeks later, the mice were given an intraperitoneal booster injection with peptide-KLH in incomplete Freund’s adjuvant; 13 days later blood was collected. Injections were started at 8–10 weeks of age for peptide and 6 weeks of age for DNA. DNA vaccination at doses of 50 μg/mouse (unless noted otherwise in a figure legend) in low-endotoxin PBS (AccuGENE, Cambrex, East Rutherford, NJ, USA) was given intramuscularly in the quadriceps, half to each leg, in 3 doses 2 weeks apart. In some experiments, mice were given a booster injection of rAd intramuscularly at a dose of 10 10  particles/mouse, 2–3 weeks after the last dose of DNA.\n\n【16】##### Challenge\n\n【17】Challenge virus in 50 μL of PBS was administered intranasally to anesthetized mice. Isoflurane or ketamine/xylazine was used for mice challenged with H1N1 subtype. Reported 50% lethal dose (LD 50  ) for H1N1 subtype was determined for 8-week-old naive BALB/c mice for each anesthetic (and may vary from the actual LD 50  for the older vaccinated mice that were challenged). Subtype H5N1 was administered intranasally to mice anesthetized with 2,2,2-tribromoethanol in tert-amyl alcohol (Avertin; Aldrich Chemical Co. Milwaukee, WI, USA). Some mice were humanely killed so their lungs could be harvested; others were monitored for body weight and death. Monitoring continued until all animals died or were recovering, as indicated by body weight.\n\n【18】##### In Vivo T-Cell Depletion\n\n【19】Acute depletion of lymphocyte populations by MAb treatment on days –3, +2, +8 relative to day of challenge was performed as described previously  and used MAbs GK1.5, specific for mouse CD4; 2.43, specific for mouse CD8; and SFR3-DR5, specific for a human leukocyte antigen as a negative control. Splenocytes were analyzed 2 days after challenge (before the next injection) by flow cytometry to confirm completeness of in vivo T-cell depletion, as described .\n\n【20】##### ELISA\n\n【21】ELISA for M2-specific antibodies was performed on plates coated with 15 µg/mL of synthetic peptides in 0.007 mol/L borate buffer and 0.025 mol/L saline; the rest of the procedure was as described .\n\n【22】##### Passive Serum Transfer\n\n【23】Naive mice were given intraperitoneal injections of pooled serum, 1 mL per mouse, from mice immunized with M2-DNA plus matched Ad booster or from control mice (B/NP-DNA, M2-H5(HK) peptide-KLH conjugate, or A/PR/8 virus). Mice were challenged with a moderate dose of A/PR/8 virus the day after serum transfer; antibody levels in recipients were not measured.\n\n【24】##### Spleen Cell Fractionation\n\n【25】T cells were enriched by negative selection that used magnetic beads. Briefly, splenocytes were depleted of erythrocytes and labeled with biotinylated antimouse B220, CD11b, and PanNK antibodies (BD Pharmingen, San Diego, CA, USA). After labeling, cells were incubated with Streptavidin MicroBeads (Miltenyi Biotec, Auburn, CA, USA). Unlabeled T cells and labeled non–T cells were separated through the Miltenyi AutoMACS system according to the manufacturer’s instructions.\n\n【26】##### Enzyme-linked Immunosorbent Spot (ELISPOT) Assay\n\n【27】This assay detected T-cell responses to M2 peptides. ELISPOT IP plates (Millipore; Billerica, MA, USA) were coated with 50 µL of Hank’s balanced salt solution (Hyclone, Logan, UT, USA) containing 5 µg/mL of anti–interferon-γ (IFN-γ) MAb AN18 (BD Pharmingen) and incubated overnight at 4°C. The membrane was washed and then blocked with medium containing 10% fetal bovine serum for 60–90 min at room temperature. Splenocytes depleted of erythrocytes were added to wells in 2-fold dilutions, starting at 250,000 cells/well in 50 µL. Peptides (SARS-M-209–221, NP147–155 of A/PR/8, or M2–2-24 of A/PR/8) were added at a final concentration of 1 µg/mL. After incubation for 36–48 h at 37°C, bound IFN-γ was detected with 50 µL of biotinylated MAb R4–6A2 (BD Pharmingen) at 1 µg/mL. Spots were developed by using alkaline phosphatase–labeled streptavidin and 5-bromo, 4-chloro, 3-indolylphosphate/nitroblue tetrazolium substrate (Kirkegaard and Perry Laboratories, Gaithersburg, MD, USA) and counted with an ELISPOT reader (Zeiss; Thornwood, NY, USA).\n\n【28】##### Virus Quantitation\n\n【29】Lungs were homogenized in 1 mL of sterile PBS, clarified by centrifugation, and titrated for virus infectivity by 50% egg infectious dose (EID 50  ) assay as described . The limit of virus detection is 1.2 log 10  EID 50  /mL. Challenge virus stocks were titrated on Madin-Darby canine kidney cells as described previously .\n\n【30】##### Statistical Analysis\n\n【31】The serologic assays and detection of M2-specific T cells were performed multiple times with comparable results. Some vaccinations were repeated with independent groups (as noted in the figure legends); those performed once used numbers of animals per group adequate for statistical significance. Lung virus titers were compared by using 1-way analysis of variance on log-transformed data, followed by pairwise multiple comparison (Holm-Sidak method). Weight loss after challenge was compared for survivors on each day by also using 1-way analysis of variance followed by pairwise multiple comparison (Holm-Sidak). This method overestimates body weight in groups with deaths because the animals that died would have had very low body weights, affecting the average, especially in the negative control groups. Nonetheless, differences between vaccinated groups were significant in the instances stated. Comparison of cumulative survival rates used the log-rank test, followed by pairwise multiple comparison, again by using the Holm-Sidak method. Overall significance level for Holm-Sidak tests was p = 0.05. All statistical analyses were performed with SigmaStat Software v3.11 (Systat Software, Point Richmond, CA, USA).\n\n【32】### Results\n\n【33】#### M2e-KLH Vaccination\n\n【34】For proof-of-concept studies, peptides representing M2e-con  and additional viral M2 ectodomains  were conjugated to KLH and used to immunize BALB/c mice. Immune serum samples were analyzed for M2-specific antibodies by ELISA on plates coated with synthetic M2e peptides with sequences from 2 viruses of H1N1 and 2 of H5N1 subtype. Serum from KLH-immune mice did not react with any of the peptides, but each M2e-immune serum sample reacted with M2e-PR8, M2e-FM, and M2e-H5(SP-83) peptides . Cross-reactions were lower on the M2e-H5(HK) peptide .\n\n【35】M2e-vaccinated mice were then challenged with either A/PR/8 or A/FM and monitored for weight loss (as a measure of illness) and death. Weight losses were consistent with a hierarchy of protection based on sequence similarity (data not shown), and differences from control mice were statistically significant. In the same groups, 100% of mice vaccinated with M2e-con or M2e-FM survived challenge with A/PR/8 and A/FM, but M2e-H5(HK)/KLH vaccination provided incomplete protection .\n\n【36】##### M2-DNA Vaccination\n\n【37】To investigate whether, like M2e peptide, DNA vaccination could protect against viruses with quite divergent M2e sequence, we tested M- and M2-DNA for efficacy. M-DNA with the A/PR/8 sequence protected against challenge with A/PR/8 . M2-DNA with the consensus sequence also protected against this A/PR8 challenge; however, M1-DNA did not (data not shown). For challenge with A/FM, M2-DNA was more efficacious than M-DNA .\n\n【38】##### M2-DNA Vaccination Followed by M2-Ad Boost\n\n【39】Previously, we have shown for NP vaccination that boosting with rAd induces more potent antibody and T-cell (especially CD8 +  ) responses than does DNA vaccination alone and can protect against challenge with highly pathogenic H5N1 subtype . We investigated whether boosting with rAd would also enhance immunity to M2. The A/M2 gene with the consensus sequence was cloned into a replication-deficient Ad construct (M2-Ad). Mice were primed with DNA as before and boosted with M2-Ad or control B/NP-Ad. Two weeks later, serum samples were collected and assayed for M2-specific immunoglobulin (Ig) G by ELISA on peptides as above . Mice given the booster of M2-DNA+M2-Ad had dramatically greater M2e-PR8–specific IgG antibody responses than mice given either component alone (M2-DNA+B/NP-Ad or B/NP-DNA+M2-Ad). Moreover, IgG cross-reactivity with M2e-FM (amino acid differences) and M2e-H5(SP-83) (amino acid differences) was found with serum from mice given M2-Ad; cross-reactivity was even greater with serum from mice given M2-DNA+M2-Ad. These serum samples, however, did not cross-react with M2e-H5(HK) (amino acid differences), although serum from mice immunized with M2e-H5(HK)-KLH as positive controls reacted strongly (data not shown).\n\n【40】Protective immunity due to vaccination with M2-DNA+M2-Ad was tested by challenge with A/PR/8 (high-dose) or A/FM (moderate dose) virus. Of mice vaccinated with M2-DNA+M2-Ad, 100% survived challenge with A/PR/8 and A/FM virus; of mice vaccinated with B/NP-DNA+B/NP-Ad, 20% survived challenge with A/PR/8 and none survived challenge with A/FM . Thus, the prime-boost vaccination protected against challenge viruses in which M2e sequences were similar to or divergent from those of the vaccine.\n\n【41】##### T-cell Response\n\n【42】T-cell responses to M2 have been observed . Immunization with cDNA expressing full-length M2 protein might induce, in addition to antibody, M2-specific T-cell responses not induced by peptides. To address the contribution of T-cell responses, we immunized mice to M2 by prime-boost and acutely depleted them of T cells just before and during the challenge period. Lymphocyte depletion was confirmed to be complete; residual CD4 +  or CD8 +  cells were <1% (data not shown). Of the B/NP control mice, 100% died of A/PR/8 infection by day 8 after challenge, while 100% of M2-immune mice treated with a control MAb (SFR) survived . Individual depletion of CD4 +  or CD8 +  T cells did not abrogate protection. Depletion of CD4 +  and CD8 +  T cells together partially, but statistically significantly, abrogated M2-induced protection  but left some protection significantly different from that in the B/NP control mice. Thus, under these challenge conditions, T cells are important. M2e-specific immunity has been reported to be natural killer (NK)-cell dependent . We found that mice depleted of NK cells with anti–asialo-GM1 antibody were protected similarly to controls (data not shown). Thus, while NK cells may play a role, they were not required under the conditions we studied.\n\n【43】Given the effect of T-cell depletion, we tested for in vitro M2-specific IFN-γ–producing T cells. The IFN-γ ELISPOT assay showed positive responses to an amino-terminal M2 peptide in spleen cells from mice immune to M2-DNA+M2-Ad  and to A/PR/8 (data not shown). After spleen cells were fractionated by magnetic bead separation , the non–T-cell fraction did not respond, while the unfractionated cells and T-cell fraction maintained a strong M2-specifc IFN-γ response .\n\n【44】##### Role of M2-specific Antibody\n\n【45】On the basis of information from previous studies , we tested the ability of antibodies induced by DNA prime–Ad boost to passively transfer protection. All (%) mice given serum from M2-immune or A/PR/8-infected mice survived, while only 50% given control B/NP-immune serum survived . Passive serum antibody from M2 prime–boost immune mice also conferred significant protection against weight loss .\n\n【46】##### Heterologous Challenge, Including SP-83 (H5N1)\n\n【47】Because protection against challenge with A/FM virus that has an M2e sequence quite divergent from that of the immunizing sequence was encouraging, we tested whether M2-DNA+M2-Ad vaccination could protect against challenge with H5N1 subtype. Mice were immunized 3× with B/NP-DNA (negative control), A/NP-DNA (positive control), or consensus M2-DNA, and boosted with matched rAd. Mice were challenged with a lethal dose of A/Thailand/SP-83/2004 (H5N1) virus in which M2e differed from the consensus by 3 amino acids. On day 5 after infection, a random subset of animals was killed and their lung virus titers were measured. Virus titers of mice vaccinated with A/NP or with M2 were significantly reduced compared with those of control mice . The remaining mice were monitored for weight loss and survival. Weight loss was less in mice vaccinated with A/NP and M2 than in control mice . All the B/NP-immune mice died of SP-83 infection by day 11 postchallenge. All (%) A/NP-immune mice and all but 1 M2-immune mouse survived .\n\n【48】### Discussion\n\n【49】Our results indicate that M2 vaccination can induce cross-reactive antibody responses, virus-specific T-cell responses, and protection against challenge with lethal heterologous virus. As has been found in previous studies of M2-based vaccines, we found strong antibody responses to the conserved M2e region. Anti-M2 antibodies in the serum of mice immunized with M DNA suggest expression of M2 from this plasmid (not shown). However, reactivity could be due to the 9 amino acid portion shared with the M1 sequence, and we did not explore this possibility. Fan et al. used A/PR/8 and Aichi M2e-carrier conjugates for immunization, and the resulting antibodies did not cross-react with the avian M2e sequences tested . We found cross-reactivity with M2e peptides that had considerable sequence divergence from the human influenza M2 consensus. Immunization with M2e-con/KLH induced antibodies that were reactive with M2e-H5(SP-83) peptide but less reactive with M2e-H5(HK). However, immunization with M2e-H5(HK)/KLH induced antibodies that were reactive with all the M2e peptides. This pattern parallels the results of Liu et al. However, neither Fan et al. nor Liu et al. investigated protection against challenge with H5N1 subtype.\n\n【50】In our lethal challenge studies, M2e peptide conjugates protected against not only a 1934 subtype H1N1 virus (A/PR/8) but also a 1947 subtype H1N1 virus (A/FM). The latter virus, which is virulent in mice, has an M2e sequence with 3 amino acid differences from the consensus and thus is as divergent from the consensus sequence as some M2-H5 sequences. Encouraged by this broad cross-reactivity and cross-protection, we expanded the study to DNA vaccination and DNA prime–Ad boost regimens. These approaches have the advantage of providing more epitopes than peptide immunization and relevant T-cell immunity.\n\n【51】Using M2 consensus DNA vaccination with or without Ad boost, we again saw cross-reactivity on avian peptides M2e-H5(SP-83) and M2e-H5(HK), although cross-reactivity was low on the HK peptide. T-cell responses to M2 peptides were detected by ELISPOT.\n\n【52】Several studies have shown that M2e-specific antibodies can mediate protection against influenza infection in vivo . In agreement with those studies, we found that serum antibodies induced by peptide conjugates or by prime-boost vaccination could transfer protection to naive recipients. We found that T cells were also important because depletion of CD4 +  and CD8 +  T cells during the challenge period reduced protection against a higher challenge dose. This could reflect M2e-specific memory T cells, which we have demonstrated in spleen and peripheral blood by ELISPOT, or a concurrent T-cell response to challenge virus supplementing the protective effects of antibodies.\n\n【53】In lethal challenge studies, the M2 consensus DNA and rAd constructs could protect against not only A/PR/8 but also against A/FM, a virus quite divergent in the M2e sequence. Furthermore, they could protect mice against challenge with SP-83 (H5N1) isolated from a fatal human case, at a dose lethal to control mice. Virus replication in lungs and illness reflected by loss of body weight were also reduced by M2 immunization. Protection against challenge with other H5N1 subtypes remains to be explored, and serologic results on M2e-H5(HK) peptide suggest results of such studies might differ on the basis of sequence variations.\n\n【54】M2 expression constructs with various M2e sequences could be used as vaccines. Our observation of protection across substantial sequence divergence means that H5-derived vaccines might also protect against circulating H1N1 and H3N2 subtypes. An additional advantage of protection across substantial divergence is potential protection by an M2 vaccine against an unexpected subtype that could cause a pandemic.\n\n【55】One concern about M2 vaccines is the possibility of escape mutants. A study of forced escape mutants found limited diversity , which indicates that structural constraints, perhaps due to requirements of the M1 structure encoded by the same segment, may limit drift.\n\n【56】The cross-reactivity and protective efficacy of M2-specific antibodies suggest that M2-specific MAbs could be useful for antiviral therapy. These features, combined with constraints on M2 structure, highlight the potential of M2-specific MAbs to inhibit replication of influenza viruses, including some H5N1 strains. Although traditional M2-directed drugs (e.g. amantadine) have led to drug resistance, the mutations that confer resistance are within the transmembrane region , which may have fewer structural constraints than the ectodomain.\n\n【57】An M2 prime-boost regimen is intended to be combined with vaccination against additional antigens rather than acting as a standalone vaccine. For example, prime-boost vaccination against conserved NP is highly protective (). The use of multiple antigens has several advantages: reduced likelihood of escape mutants, better coverage of human leukocyte antigen haplotypes in the genetically diverse human population, and a broader spectrum of immune response mechanisms (with antibodies perhaps dominating for M2 and cytotoxic T lymphocytes for NP).\n\n【58】Vaccines based on conserved antigens are not intended to replace strain-matched vaccines that induce neutralizing antibodies and thus prevent infection. However, strain-matched vaccines may be difficult to produce in adequate quantities in short time periods, and continued antigenic drift may render them ineffective. Vaccinations as described here, based on M2, might reduce deaths and severity of disease while strain-matched vaccines were being prepared and could enhance protection afforded by inactivated vaccines. Immunogenicity and safety studies in people are needed to evaluate this approach.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5445e0fe-51f3-4079-97d6-7276733a2954", "title": "Genetic Characterization of Nipah Virus, Bangladesh, 2004", "text": "【0】Genetic Characterization of Nipah Virus, Bangladesh, 2004\nNipah virus (NV) and Hendra virus (HV), the only members of the genus _Henipavirus_ within the family _Paramyxoviridae_ are different from most other paramyxoviruses because they have a broad host range in vivo and in vitro. Although HV and NV have genetic characteristics and replication strategies similar to those of other paramyxoviruses, the henipaviruses have several unique genetic features .\n\n【1】The first outbreak of NV occurred between late 1998 and early 1999 in peninsular Malaysia and Singapore and was associated with respiratory disease in swine and acute and febrile encephalitis in humans. Direct contact with sick pigs was the primary source of human infection . Of 265 human cases, 108 were fatal. Although NV is excreted in respiratory secretions and urine of patients , a survey of healthcare workers in Malaysia showed no evidence of human-to-human transmission . The reservoir of NV is presumed to be fruit bats, primarily of the genus _Pteropus_ , and humans are infected through intermediate hosts such as pigs.\n\n【2】Recently, NV has been established as the cause of fatal, febrile encephalitis in human patients in Bangladesh during the winters of 2001, 2003, and 2004 . An NV-like virus was identified as the cause of the outbreaks in 2001 and 2003 on the basis of serologic testing . Two outbreaks consisting of 48 cases of NV were detected in 2004 in 2 adjacent districts (km apart) of central Bangladesh (Rajbari and Faridpur) with a case-fatality rate of nearly 75%. Because of heightened surveillance, other small clusters and isolated cases (n = 19) were identified during the same period in 7 other districts in central and northwest Bangladesh. Although antibodies to NV were detected in fruit bats from the affected areas in 2004, an intermediate animal host was not identified, which suggests that the virus was transmitted from bats to humans. Human-to-human transmission of NV was also documented during the Faridpur outbreak . We describe the genetic characteristics of 4 NV isolates from the outbreak in Bangladesh in 2004.\n\n【3】### The Study\n\n【4】Virus isolation was performed in the BSL-4 laboratory at the Centers for Disease Control and Prevention in Atlanta. Vero E6 cells were inoculated and observed for characteristic cytopathic effect, syncytium formation . NV was isolated from 2 oropharyngeal swabs (SPB200401066, SPB200406506), 1 cerebrospinal fluid (SPB200401617), and 1 urine specimen (SPB200405758) from human patients, and isolation was confirmed by reverse transcription polymerase chain reaction (RT-PCR). Two isolates were from Rajbari, and 1 was from Faridpur; the fourth isolate, from the Rajshahi district (km from Rajbari), was not linked to the other 2 outbreaks. The complete genomic sequence of the first viral isolate (SPB200401066) from Rajbari was derived and submitted to GenBank  as NV-Bangladesh (NV-B). The sequences of the open reading frame (ORF) coding for nucleoprotein (N) were obtained for the other 3 isolates. The methods used for RT-PCR, sequencing, cDNA cloning, rapid amplification of cDNA ends (RACE), and sequence analysis were previously described .\n\n【5】The genome of NV-B is 18,252 nt in length, 6 nt longer than NV-Malaysia (NV-M), the prototype strain of NV (SPB199901924). The additional 6 nt map to the 5´ nontranslated region of the fusion protein (F) gene. The length of the NV-B genome is evenly divisible by 6, suggesting that NV-B follows the \"rule of six\" . The gene order and sizes of all the ORFs except V are conserved between NV-B and NV-M . The overall nucleotide homology between the genomes of NV-B and NV-M is 91.8%, but the changes are not uniformly distributed throughout the genome. Nucleotide homologies are higher in the protein coding regions than in the noncoding regions, although the sizes of the nontranslated regions remain highly conserved . The predicted amino acid homologies between the proteins expressed by NV-M and NV-B are all >92% .\n\n【6】Overall, the predicted amino acid homologies of the surface glycoproteins, F and G, of NV-B and NV-M are high . In the F protein, the predicted cleavage site, F1 amino-terminal domain, transmembrane domain, and predicted N-glycosylation sites are identical in NV-B and NV-M . Four of the 9 predicted amino acid changes occur in the first 11 amino acids (aa) of the precursor of the F protein, F0, which fall within the predicted signal peptide and would be cleaved from the mature protein. Within the G proteins, the predicted transmembrane domains, and the positions of all 17 cysteine residues are conserved between NV-B and NV-M. Of 8 predicted N-linked glycosylation sites in the G protein of NV-M, 6 are conserved in NV-B and in HV .\n\n【7】The coding strategy of the P gene is identical in NV-B and NV-M. In these viruses, the P gene contains the C, V, and W ORFs in addition to the P ORF. Like most other paramyxoviruses, the henipaviruses have a conserved AG-rich region that acts as an editing site to facilitate the addition of nontemplated G residues into the transcripts of the P gene. The edited transcripts encode 2 proteins, V and W, which are co-amino-terminal with P but have unique carboxy termini . The addition of 1 G residue generates the mRNA for the V protein, and the addition of 2 G residues produces the mRNA for the W protein. Sequence analysis of multiple cDNA clones containing the editing site of NV-B identified edited transcripts that encoded both the V and W proteins (data not shown). The conserved 20-nt region encompassing the editing site is identical in NV-M, HV, and measles virus (MV) ; however, the editing of site of NV-M (UGGGUAAUUUUUCCCGUGUC) differs from NV-B (GGGAUAAUUUUUCCCGUGUC) at 2 nt positions (underlined). The functional significance of these 2 substitutions is under investigation. All of the cysteine residues are conserved in the V proteins of NV-B and NV-M; however, the unique portion of the V protein of NV-B is predicted to be 55 aa, 3 aa longer than the V protein of NV-M. The predicted W protein of NV-B is identical in size and sequence to the W protein of NV-M. Recently, aa 100–160 and 230–237 of the V protein of NV-M have been identified as necessary for inhibition of interferon signaling . These regions are highly conserved in the V protein of NV-B, which has 4 predicted amino acids substitutions (conservative) between positions 100–160 and no predicted substitutions between positions 230–237. In addition, the V protein of NV-B has 3 predicted amino acid substitutions in the CRM1-dependent nuclear export signal that was identified between aa 174–193 in the V protein of NV-M . The biologic effects of these amino acid substitutions are under investigation.\n\n【8】The L proteins of NV-B and NV-M had a high level of predicted amino acid conservation . The 6 highly conserved domains of viral polymerases, originally described by Poch et al. and delineated for NV-M by Harcourt et al. remain largely unchanged between NV-B and NV-M. Domains 1, 2, and 5 have 1 conservative amino acid change each and domain 3, which is considered the most conserved domain within the L proteins of paramyxoviruses, has 2 aa changes. The 4 motifs identified in domain 3, including the QGDNE motif, which is assumed to be the active site of the polymerase, are identical between the NV-B and NV-M, as is the predicted nucleotide-binding motif in domain 6.\n\n【9】The cis-acting control sequences are highly conserved in the genomes of NV-B and NV-M. As in NV-M, the intergenic sequences in NV-B are GAA, with the exception of the sequence between the G and L genes, UAA, which is unique among the henipaviruses. However, the intergenic sequence between the G and L genes is GAA in the second isolate from Bangladesh. The transcriptional start and stop signals of each gene of NV-B are highly conserved in relation to the other henipaviruses. The 3´ leader sequence of NV-B is identical in length to those of all other paramyxoviruses and has nucleotide changes at positions 14 and 47 compared to NV-M. The 5´ trailer of NV-B is identical in length and sequence to NV-M.\n\n【10】Phylogenetic analysis was used to compare the sequence of the N ORF of NV-B to the sequences of the N ORFs from other members of the subfamily _Paramyxovirinae_ . The results confirmed the results of the sequence comparisons, which show that NV-B is most closely related to the henipavirus NV-M, and support the conclusion that NV-B should be regarded as new strain of NV . Phylogenetic analyses conducted with the sequences of the other genes produced similar results (data not shown). The sequences of the N ORFs of 4 NV isolates from Bangladesh share 99.1% nt homology  but exhibited more interstrain nucleotide heterogeneity than the sequences of the human isolates in Malaysia, which were nearly identical . These varying amounts of genetic variability may reflect differences in the mode of transmission of NV in the 2 countries. In Malaysia, molecular evidence suggests that at least 2 introductions of NV into pigs occurred . However, the nearly identical sequences of human and pig isolates from the later phase of the outbreak suggest that only 1 of the variants spread rapidly in pigs and was associated with most human cases . In contrast, the sequence heterogeneity observed in Bangladesh may be the result of multiple introductions of NV into humans from different colonies of fruit bats.\n\n【11】### Conclusions\n\n【12】This first look at strain variation in NV indicates that viruses circulating in different areas have unique genetic signatures and suggests that these strains may have coevolved within the local natural reservoirs. Until 2004, identification of NV outbreaks in Bangladesh had been based only on serologic testing. The isolation and genetic characterization of NV-B confirm that NV was the etiologic agent responsible for these outbreaks.\n\n【13】Note: After this article was accepted for publication, Nipah virus was isolated from _Pteropus lylei_ in Cambodia . Phylogenetic analysis of the N gene sequences demonstrated that this virus is more closely related to Nipah-Malaysia than to Nipah-Bangladesh and represented another lineage of Nipah virus.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3258584b-7ed0-40eb-aaba-f7e91fd8693a", "title": "Bartonella spp. Infections, Thailand", "text": "【0】Bartonella spp. Infections, Thailand\n**To the Editor:** _Bartonella_ are fastidious hemotropic gram-negative bacteria with a worldwide distribution. In Thailand, _Bartonella_ species have been demonstrated in mammalian hosts, including rodents, cats and dogs, and in potential vectors, including fleas . However, data on human infection have been limited to case reports  and 1 seroprevalence survey, which found a 5.5% prevalence of past _B. henselae_ infection . No studies have systematically assessed the frequency, clinical characteristics, or epidemiology of human _Bartonella_ infections in Thailand.\n\n【1】We conducted a prospective study to determine causes of acute febrile illness in 4 community hospitals, 2 in Chiang Rai (northern Thailand) and 2 in Khon Kaen (northeastern Thailand). We enrolled patients \\> 7 years of age with a temperature >38°C who were brought to study hospitals for treatment from February 4, 2002, through March 28, 2003. Patients were excluded if they had a history of fever for \\> 2 weeks or an infection that could be diagnosed clinically. Acute-phase serum samples were collected at the time of enrollment and convalescent-phase serum samples 3–5 weeks later. We enrolled nonfebrile control patients \\> 14 years of age who had noninfectious conditions; acute-phase serum samples were collected. Clinical information was abstracted from patient charts. Nurses conducted physical examinations and personal interviews to collect information on patients’ demographic characteristics, exposures to animals, and outdoor activities.\n\n【2】Serum samples were tested for immunoglobulin (Ig) G antibodies to _Bartonella_ spp. by immunofluorescent antibody assay at the Bartonella Laboratory of the Centers for Disease Control and Prevention, Fort Collins, CO, USA. Strains used for antigen production were: _B. elizabethae_ (F9251), _B. henselae_ (Houston-1), _B. quintana_ (Fuller), and _B. vinsonii_ subsp. _vinsonii_ (Baker). Homologous hyperimmune serum specimens were produced in BALB/c mice as previously described . _Bartonella_ infection was considered confirmed in febrile patients who had a \\> 4-fold rise in IgG antibody titers and a convalescent-phase titer >64. Probable infection was defined as 1) a 4-fold antibody titer rise but convalescent-phase titers of 64, or 2) high and stable titers (> 512 in acute-phase and convalescent-phase serum samples), or 3) acute-phase titer \\> 512 with a \\> 4-fold titer fall. Paired serum samples from febrile patients were also tested for serologic evidence of other common causes of febrile illness in Southeast Asia.\n\n【3】Febrile patients with acute-phase and convalescent-phase IgG antibody titers <128 were considered not to have _Bartonella_ infection; we compared demographic and clinical characteristics of these patients to _Bartonella_ \\-infected patients. To evaluate potential risk factors, we compared _Bartonella_ \\-infected case-patients \\> 14 years of age without serologic evidence of other infections (n = 20) to nonfebrile controls with IgG to _Bartonella_ <128 (n = 70). Age adjusted odds ratios (AORs) with 95% confidence intervals (CIs) were calculated.\n\n【4】Serologic testing was completed on paired serum samples for 336 (%) of 732 febrile patients enrolled; 92 (%) had serologically confirmed  or probable  _Bartonella_ infections. Thirty-five (%) of these 92 had serologic evidence of infection with another pathogen. The remaining 57 _Bartonella_ \\-infected case-patients (confirmed, 23 probable) had a median age of 19 years (range 7–72 years); 65% were males, 47% were students, and 35% were rice farmers. Common clinical characteristics of _Bartonella_ \\-infected patients included myalgias (%), chills (%), and headache (%). Thirty (%) patients had anemia (hemoglobin level <13 mg/dL); 18 (%) had a hemoglobin level <12 mg/dL, and 4 (%) had <11 mg/dL. When compared with 193 febrile patients without _Bartonella_ infection, the 57 _Bartonella_ \\-infected patients were similar in age and sex but were more likely to be rice farmers and were more likely to have leukocytosis . Compared with the 70 nonfebrile controls, _Bartonella-_ infected case-patients were more likely to report tick exposure (% vs. 7.9%; AOR = 5.6, 95% CI 1.5–21) and outdoor activities (% vs. 31%; AOR = 2.7, 95% CI 1.0–7.4) during the 2 weeks before illness onset. Prevalence of reported rat exposure and animal ownership (cats, dogs, pigs, cows, or buffaloes) was similar among case-patients and controls.\n\n【5】We describe the frequency and clinical characteristics of acute _Bartonella_ infection among febrile patients in Thailand. Over 25% of patients with undifferentiated febrile illness had serologic evidence of _Bartonella_ infection (including 15% serologically confirmed). Our findings indicate that _Bartonella_ infections may be common and underrecognized causes of acute febrile illness in rural Thailand. Although our results are limited by lack of culture confirmation, we used conservative case definitions for serologic diagnosis and therefore believe that most cases represent true _Bartonella_ infections. The common clinical features of anemia and leukocytosis and the frequent tick exposure and outdoor activity are consistent with known features of _Bartonella_ infections and lend support to serologic findings. Because of the potential for serologic cross-reactivity between _Bartonella_ species, we did not attempt species identification. The case-control study was therefore limited by grouping case-patients that were likely infected with different _Bartonella_ species for which risk factors may differ. Such studies could lead to meaningful recommendations for prevention and control of _Bartonella_ infections. Additional epidemiologic and transmission studies are needed to improve understanding of risk factors, identify key animal reservoirs and vectors, and ascertain transmission dynamics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dfdc3c83-5696-4934-8b0b-45278bae3ffc", "title": "Feasibility of Increasing Childhood Outdoor Play and Decreasing Television Viewing Through a Family-Based Intervention in WIC, New York State, 2007-2008", "text": "【0】Feasibility of Increasing Childhood Outdoor Play and Decreasing Television Viewing Through a Family-Based Intervention in WIC, New York State, 2007-2008\nAbstract\n--------\n\n【1】**Introduction**Active Families is a program developed to increase outdoor play and decrease television viewing among preschool-aged children enrolled in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). Our objective was to assess its feasibility and efficacy. **Methods**We implemented Active Families in a large WIC clinic in New York State for 1 year. To this end, we incorporated into WIC nutrition counseling sessions a community resource guide with maps showing recreational venues. Outcome measures were children’s television viewing and time playing outdoors and parents’ behaviors (television viewing, physical activity), self-efficacy to influence children’s behaviors, and parenting practices specific to television viewing. We used a nonpaired pretest and posttest design to evaluate the intervention, drawing on comparison data from 3 matched WIC agencies. **Results**Compared with the children at baseline, the children at follow-up were more likely to watch television less than 2 hours per day and play outdoors for at least 60 minutes per day. Additionally, parents reported higher self-efficacy to limit children’s television viewing and were more likely to meet physical activity recommendations and watch television less than 2 hours per day. **Conclusion**Results suggest that it is feasible to foster increased outdoor play and reduced television viewing among WIC-enrolled children by incorporating a community resource guide into WIC nutrition counseling sessions. Future research should test the intervention with a stronger evaluation design in multiple settings, with more diverse WIC populations, and by using more objective outcome measures of child behaviors.  \n\n【2】Introduction\n------------\n\n【3】The prevalence of obesity is increasing among children in the United States, particularly among low-income children . In 2003, the prevalence of obesity among young children living in low-income families was almost 15%  and in New York State exceeded 16% . Although obesity prevention programs targeting low-income preschool-aged children are needed, few such programs have been implemented . The Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) presents an ideal opportunity for health promotion efforts that target preschoolers and their families. WIC provides direct contact with parents, who are fundamental in shaping children’s dietary and physical activity (PA) behaviors . Fit WIC is a WIC-based obesity prevention program . In New York State, Fit WIC incorporates obesity prevention into the re-education of WIC staff. The program provides training and resources for clinic staff in how to incorporate PA into nutrition assessment and education, offers WIC staff opportunities for wellness at work, and encourages staff to model healthy behaviors. From 2007 through 2008, the New York State WIC program implemented a pilot project called Active Families to enhance the Fit WIC initiative. Given that at least 50% of preschoolers do not meet recommended levels of PA  and television viewing , Active Families sought to increase the time that children spend playing outdoors  and decrease the time they spend watching television . Outdoor play was a target behavior because it is consistently linked with higher PA among young children . Furthermore, it is challenging to quantify PA in preschool children because of the intermittent nature of their activity patterns . To achieve the goals of increasing outdoor play and reducing television viewing, Active Families addressed community-based barriers to children’s PA and outdoor play  by integrating into WIC counseling sessions a community resource guide linking families with local resources for PA. The specific objective of our study was to assess the feasibility and efficacy of the Active Families program.  \n\n【4】Methods\n-------\n\n【5】### Study design and setting\n\n【6】We used a pre-post (nonpaired) quasiexperimental design with a nonequivalent comparison group. We implemented the Active Families program in a WIC clinic, in a metropolitan area in central New York State, from August 2007 through August 2008. We selected this clinic as the intervention site because it has a large and diverse client base and the staff had previously participated in Fit WIC training. The institutional review boards of the New York State Department of Health and the University at Albany, State University of New York, approved the study protocol. We collected baseline data from 422 families at the target clinic during June, July, and August in 2007 and follow-up data from 442 families during July and August in 2008, representing approximately one-third of families with enrolled children during each time period. It was not possible to link the baseline and follow-up data because of budgetary, time, and logistical constraints associated with working in a community setting. Participants therefore reflect a sample of the clinic population at each time point. During both assessment periods, parents (defined in this article as biological parents or other home-based caregivers) of children aged 2 to 5 years completed a self-report survey, with reference to their oldest child enrolled in WIC, in the clinic waiting room. A trained research assistant provided assistance as necessary. Surveys were available in English and Spanish, although most parents completed the survey in English. The survey measured the primary outcomes of interest and theory-based mechanisms  expected to explain intervention effects: 1) demographic factors, including child and parent age and sex, parent race/ethnicity, and parent education; 2) child television viewing, including hours per day the child watched television on a typical day and the presence of a television in the child’s bedroom; 3) child outdoor play or the time the child spent playing outdoors on a typical day, during the morning, afternoon, and early evening ; 4) parent behaviors and parenting practices including hours per day the parent watched television, days per week the parent participated in at least 30 minutes of moderate PA or 20 minutes of vigorous PA, and whether the parent limited the child’s television viewing to less than 2 hours per day; and 5) parent self-efficacy to reduce the child’s television viewing time and encourage the child to be physically active. We modeled survey questions after previous statewide WIC surveys and validated surveys . The follow-up survey included process-related questions to examine whether parents received the guide, how many copies they received, whether they read the guide, and how they used the guide. We drew comparison data from 3 matched WIC agencies in upstate New York. Eligibility criteria were 1) the agency had previously participated in Fit WIC training, 2) the agency was in an area geographically similar to the target site, and 3) the agency served a demographic population similar to the target site. At the comparison clinics, parents of children aged 2 to 5 years completed a survey during May, June, and July in 2008 (n = 458) as part of the Fit WIC evaluation (items matched those from the Active Families survey). This timeframe matched that of the follow-up assessment at the target clinic. Parents at the comparison sites (n = 398) also completed the survey during September, October, and November in 2006. Although the timing of this survey does not match that of the baseline assessment for the intervention site, we reviewed the earlier sets of data from the target and comparison sites for preexisting differences in the outcome variables. With 1 exception, we identified no differences that could bias results in favor of the target site. The exception was children’s outdoor play; children from the target site were more active at baseline than were their counterparts at the comparison sites. \n\n【7】### Program description\n\n【8】The fundamental component of the Active Families program was a community resource guide linking families with local resources for PA. The guide included an extensive list of outdoor recreation venues, such as parks and playgrounds, with maps identifying the specific location of each venue along with information on hours of operation, contact details, costs, and available facilities. A calendar of community events, updated every 2 to 3 months, was included as an insert in the back of the guide to encourage outdoor family recreation (eg, local family festivals). We developed winter and summer versions of the guide. The winter guide outlined opportunities for indoor as well as outdoor recreation. Before program implementation, we conducted 2 focus groups, with 5 and 8 people per group, to get feedback from parents on the usefulness of the guide, other places in their community for active recreation, and the topics that should be included in the guide. The first author (K.K.D.) and a public health dietitian experienced in moderating focus groups conducted each focus group. Based on parent feedback, we incorporated additional content in the guide, including information on the benefits of increasing children’s PA and reducing television viewing, suggestions for non–screen-based indoor activities, where to find free or low-cost winter clothing, and responses to frequently asked questions that emerged during the focus groups. \n\n【9】### Program implementation\n\n【10】In July 2007, the authors (K.K.D. L.S.E. L.M.Y.) trained the clinic staff to use the guide as a tool during counseling sessions. In particular, we encouraged counselors to 1) review the guide with parents, 2) point out the goals of the program and the benefits of increasing PA and reducing television viewing, 3) show parents the maps and help them locate their home, and 4) bring their attention to the calendar of local events at the back of the guide. We repeated the training session midway through the year for any new staff. From August 2007 through August 2008, WIC counselors at the target site distributed the community guide to parents of children aged 18 months or older during WIC counseling sessions. Guides were also available in the clinic waiting room. During the intervention, families received up to 4 copies of the guide. To ensure the program was implemented as intended, we conducted periodic site visits and observed the extent to which counselors used the guides during counseling sessions and whether guides were available in the counseling areas and waiting room. \n\n【11】### Statistical analysis\n\n【12】To facilitate data analysis and interpretation, we dichotomized the outcome variables. This structure allowed us to use the same analytic method for all the primary analyses. We coded child television viewing and PA to reflect whether the child met recommendations, including less than 2 hours per day of television viewing  and 60 minutes or more per day of playing outdoors, which we used as a surrogate for 60 minutes of PA . Similarly, we coded parent PA to reflect whether parents met recommended levels of PA . We coded questions specific to parent self-efficacy in reducing child television viewing and increasing child PA such that parents who strongly agreed or agreed with the statement “I am confident in my ability to reduce my child’s television viewing time” or “I am confident in my ability to encourage my child to be physically active” were classified as having high self-efficacy. Those who indicated that they did not know, disagreed, or strongly disagreed were coded as having low self-efficacy. We coded race/ethnicity as white, black, Hispanic, or other/multiracial and parent education as some high school or less, high school graduate or equivalent, or at least some college. In preliminary analyses, we examined differences in sample characteristics for the target (baseline and follow-up) and comparison sites using _χ_ 2 analysis or _t_ tests, as appropriate. In the primary analyses, we assessed differences in outcome variables at baseline versus follow-up at the target site and at follow-up for the target versus comparison sites by using logistic regression analysis. We conducted all analyses using SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina) and controlled for differences in parent characteristics identified in the preliminary analyses. Significance was established at _P_ < .05.  \n\n【13】Results\n-------\n\n【14】### Sample characteristics\n\n【15】Significant baseline versus follow-up differences at the target site were observed for parent race/ethnicity and education . Similarly, significant differences at the target site versus comparison sites at follow-up were identified for race/ethnicity and parent education. We observed no differences in the age or sex of children or the age of parents. Across intervention and comparison sites and regardless of timing of the assessment, the majority of children watched less than 2 hours of television each day and played outdoors for at least 60 minutes each day . The majority of parents watched television for more than 2 hours each day and were confident that they could limit their child’s television viewing. Approximately half of parents met PA recommendations. Finally, approximately one-third of parents reported limiting children’s television, and approximately half of children had a television in their bedroom. \n\n【16】### Program evaluation\n\n【17】Parents at follow-up compared with baseline were significantly more likely to report that 1) their child watched less than 2 hours of television per day, 2) they themselves watched television less than 2 hours per day, and 3) they felt confident they could limit their child’s television viewing . No significant group differences were identified for television in the child’s bedroom and limits parents placed on the child’s television viewing. For the PA outcomes, compared with baseline, parents at follow-up were significantly more likely to report sufficient PA to meet recommendations and that their children played outside for at least 60 minutes per day. Compared with parents from the comparison clinics, parents from the target clinic were significantly more likely to report that they 1) watched television less than 2 hours per day, 2) were confident in their ability to limit their child’s television viewing, and 3) limited their child’s television viewing to less than 2 hours per day. With regard to PA, parents at the target site at follow-up were more likely to report that their children played outdoors for at least 60 minutes per day than were parents at comparison clinics. \n\n【18】### Process evaluation\n\n【19】Most of the parents who recalled receiving the guide (n = 86) reported that they read the guide (n = 62). The most frequently used section of the guide was the list of community events; approximately half of these parents indicated that they used this information. In addition, approximately 1 in 3 parents reported that they used the guide to be more active themselves, to help their child to be active, or to reduce their child’s television viewing time. Parents also reported that they used the maps in the guide to find places to take their child (%) and to find winter clothing for their child (%). In terms of specific venues visited, 60% to 80% of parents who used the guide indicated that they visited parks, playgrounds, swimming pools, fairs, and festivals listed in the guide.  \n\n【20】Discussion\n----------\n\n【21】Our results suggest that building on existing nutrition counseling services in the WIC program by incorporating a community resource guide can increase the number of children meeting PA and television viewing recommendations. In 3 of 4 comparisons, the intervention was associated with an increased proportion of children playing outdoors for at least 60 minutes or watching television for less than 2 hours per day. Furthermore, in most instances we observed anticipated differences in potential mechanisms of effect , including parents’ own behaviors and their confidence in their ability to influence their child’s behavior. Results from this study are consistent with previous obesity prevention programs implemented in a WIC setting  and research showing that enhanced access to places for PA combined with information about possible activities is effective in increasing levels of PA . To our knowledge, this is the first study to evaluate a community resource guide in a WIC setting to improve parent and child outcomes specific to PA and television viewing. Although we were unable to use a rigorous experimental design, the consistency of results supports the integrity of the observed intervention effect. Half of the comparisons were significant and in the anticipated direction. Additionally, no effects were in the opposite direction to those expected. Social desirability bias is a concern when relying on self-reported data, but it is unlikely that such a bias entirely explains the results because we would expect a similar bias in the comparison sample. It is also unlikely that preexisting group differences in the outcome variables are responsible for our findings. We identified few preexisting group differences between the target and comparison sites that would bias the results in favor of the intervention group. One exception is children’s outdoor play. At baseline, parents at the target site reported higher scores for children’s outdoor play than did parents at the comparison sites. Although this difference could indicate that children at the target site were more active overall at baseline, it more likely reflects a seasonal effect on PA  because baseline data were collected in summer for the target site and in fall for the comparison sites. As a result, it is difficult to interpret site differences in outdoor play for the target versus comparison sites. We did not observe intervention effects for “television in the child’s bedroom.” Approximately 50% of children had a television in their bedroom, which is linked with more hours watching television  and is contrary to American Academy of Pediatrics recommendations . The common presence of a television in children’s bedrooms could reflect sleeping arrangements in low-income families. For example, informal observations suggest that children in low-income families may be more likely to sleep in shared or communal areas where a television is often located. If this is the case, it has implications for how we define and measure the presence of a television in the child’s “bedroom” and the associated intervention strategies. Unfortunately, although a substantial proportion of children across all income levels have a television in their bedroom, few interventions with known efficacy directly address this practice. Additional formative research is needed to understand factors that lead parents to allow a television in the area where children sleep and why it might be difficult to later reverse this decision. This study has several limitations. Given the programmatic, community-based context in which we implemented the study and the limited funds available, it was not feasible to randomly assign clinics to a condition or to follow clients over time. The samples that we surveyed at baseline and follow-up, however, were representative of the clinic’s population. Therefore, the results represent a site-level change in the outcomes of interest. Moreover, we distributed follow-up surveys at the same time of the year as baseline surveys to increase the likelihood that those surveyed at baseline would participate in the follow-up assessment (ie, WIC visits occur midway through a 6-month recertification period). Still, the lack of a randomized control group presents several threats to the internal validity of the study. Findings are also limited by the use of self-report measures of parenting practices and parent and child television viewing and PA behaviors. Finally, results are generally limited to English-speaking WIC-enrolled families. Despite these limitations, results provide preliminary evidence for the feasibility and efficacy of the Active Families program and suggest that incorporating a community resource guide to promote PA into WIC counseling sessions is a strategy worthy of further investigation. Future research could expand on this initial work by using a more rigorous design with multiple WIC sites, using more objective outcome measures, and quantifying the resources necessary to develop resource guides on a larger scale.  \n\n【22】Tables\n------\n\n【23】#####  Table 1. Characteristics of Participants in the WIC Active Families Target and Comparison Sites, New York State, 2007-2008\n\n【24】CharacteristicTarget SiteP ValueComparison Sites, n = 458P Value aBaseline, n = 422Follow-Up, n = 442ChildrenAge, mean (SD), mo41.0 (.5)41.1 (.5).8440.5 (.2).40Girls, %50.452.7.5049.8.38ParentsRace/ethnicity, %White33.330.4.0143.1&lt;001Black38.347.246.2Hispanic15.414.86.2Other/multiracial13.07.54.4Education, %Some high school or less37.938.5.0423.8&lt;001High school graduate or equivalent27.333.745.7At least some college34.827.830.5Age, mean (SD), y28.8 (.9)29.4 (.9).2629.8 (.3).45Abbreviations: WIC, Special Supplemental Nutrition Program for Women, Infants, and Children; SD, standard deviation.a Values indicate comparison sites vs follow-up for the target site. Data collection for the 2 groups corresponded in time period._χ_ 2 tests were used for categorical variables and _t_ tests for continuous variables. \n\n【25】#####  Table 2. Television Viewing and Physical Activity Outcomes for Participants in the WIC Active Families Target and Comparison Sites, New York State, 2007-2008\n\n【26】OutcomeTarget SiteComparison Sites, a %, n = 458Baseline, %, n = 422Follow-Up, %, n = 442Television viewingChild watches &lt;2 h/d596667Parent watches &lt;2 h/d254430Parent confident can limit child’s television709078Child does not have television in bedroom515141Parent limits television to &lt;2 h/d303926Physical activity (PA)Child plays outdoors ≥60 min/d748367Parent meets PA recommendations b505757Parent confident can encourage PA929595Abbreviation: WIC, Special Supplemental Nutrition Program for Women, Infants, and Children.a Comparison site data collection corresponded in time period to target site follow-up data collection.b 150 min/wk moderate-intensity or 75 min/wk vigorous-intensity aerobic PA . \n\n【27】#####  Table 3. Odds of Television Viewing and Physical Activity Outcomes for Participants in the WIC Active Families Target and Comparison Sites, New York State, 2007-2008\n\nOutcomeTarget Site at Follow-Up vs Baseline (Reference)Target Site at Follow-Up vs Comparison Sites a (Reference)Adjusted OR b(% CI)P ValueAdjusted OR b(% CI)P ValueTelevision viewingChild watches &lt;2 h/d1.40 (.05-1.86).021.15 (.83-1.57).39Parent watches &lt;2 h/d2.79 (.03-3.83)&lt;0012.37 (.69-3.33)&lt;001Parent confident can limit child’s television4.14 (.81-6.11)&lt;0013.32 (.17-5.07)&lt;001Child does not have television in bedroom1.08 (.83-1.43).540.81 (.60-1.09).17Parent limits television to &lt;2 h/d1.25 (.93-1.67).141.98 (.43-2.75)&lt;001Physical activity (PA)Child plays outdoors ≥60 min/d1.68 (.19-2.37).0032.79 (.94-4.02)&lt;001Parent meets PA recommendations c1.32 (.01-1.73).041.20 (.89-1.62).22Parent confident can encourage PA1.67 (.95-2.95).071.61 (.82-3.16).16Abbreviations: WIC, Special Supplemental Nutrition Program for Women, Infants, and Children; OR, odds ratio.a Comparison site data collection corresponded in time period to target site follow-up data collection.b All ORs were adjusted for child and parent age, parent race/ethnicity, and parent education. All variables were coded such that higher ORs reflect a stronger intervention effect. Values <1 indicate a result in the opposite direction to that anticipated.c 150 min/wk moderate-intensity or 75 min/wk vigorous-intensity aerobic PA .   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1d897dd0-9b2c-4be7-9fe4-e582889b64db", "title": "Mycobacterium bovis Infection in Free-Ranging African Elephants", "text": "【0】Mycobacterium bovis Infection in Free-Ranging African Elephants\nTuberculosis (TB), caused by the human pathogen _Mycobacterium tuberculosis_ , is a recognized disease in human-managed and wild Asian elephants (Elephas maximus_ ) and African elephants (Loxodonta africana_ ) . Previous findings demonstrate the importance of human-elephant interfaces for transmission. However, range countries for African and Asian elephants also have high burdens of bovine TB, caused by _M. bovis_ . The World Organisation for Animal Health (OIE) records cases of bovine TB; in the 49 elephant range countries in Africa and Asia, only Namibia is declared free of _M. bovis_ . Therefore, the paucity of cases of _M. bovis_ infection in elephants is unexpected. The lack of _M. bovis_ cases in elephants may be caused by rare or sporadic exposure, innate resistance of the species, or limited surveillance, especially in environments to which bovine TB is endemic.\n\n【1】Kruger National Park (KNP) in South Africa has recorded _M. bovis_ infection in >20 wildlife species and is considered a bovine TB−endemic area. Although cases of _M. bovis_ infection have been reported in other large herbivores, such as black rhinoceros (Diceros bicornis_ ) and white rhinoceros (Ceratotherium simum_ ) , only 1 case of _M. tuberculosis_ infection has been found in an elephant in KNP , despite hundreds of individual animals examined during 1967–1994 when elephants were harvested . After the discovery of an _M. tuberculosis_ –infected adult bull elephant in 2016 , opportunistic sampling of elephants was implemented by park veterinarians.\n\n【2】In May 2018, a young bull elephant (E1; estimated age 18–20 years) was fatally shot in the southern part of KNP. In addition, a young bull elephant (E2; estimated age 3 years) in KNP was euthanized in October 2019 after being found moribund. Postmortem examination of E1 revealed rare small, consolidated masses in the lung. Elephant 2 had several focal firm masses (cm 2  ) scattered in the lung containing caseous material and some mineralization. We took representative samples from the peripheral (prescapular, inguinal, popliteal), head (parotid, retropharyngeal), thoracic (tracheobronchial), and abdominal (mesenteric) lymph nodes; lung lesions were also sampled. We froze samples at −20°C and transported them for mycobacterial culture and speciation in the Biosafety Level 3 laboratories at Stellenbosch University (Cape Town, South Africa) and ARC-Onderstepoort Veterinary Institute (Pretoria, South Africa).\n\n【3】We prepared tissues for mycobacterial culture as previously described  using the BD BACTEC MGIT 960 Mycobacterial Detection System . We performed mycobacterial speciation by region-of-difference PCR  and spoligotyping  on Ziehl-Neelsen stain positive cultures. We isolated _M. bovis_ from both tracheobronchial lymph node cultures from E1. Culture results from E2 confirmed the presence of _M. bovis_ in 2 lung samples in 2 different laboratories, and in a tracheobronchial lymph node cultured in the second laboratory. We characterized the _M. bovis_ spoligotype pattern from E1 as SB0121, which is the most common strain found in KNP  . In contrast, in E2 we found a novel spoligotype pattern (SB1681) not previously reported in KNP . However, this pattern varied by 1 spacer, and it is possible that this strain evolved from the common SB0121 strain in KNP.\n\n【4】The finding of _M. bovis_ infection in 2 free-ranging African elephants in KNP has significance for other elephant populations in bovine TB–endemic areas. In South Africa, the Department of Agriculture, Land Reform, and Rural Development has the authority to place _M. bovis_ –infected premises under quarantine. Previously, elephants and rhinoceros were excluded from movement restrictions placed on other species in bovine TB–endemic populations. However, our findings require revisiting this assumption and investigating disease transmission risks for these species. Kerr et al. in a retrospective study investigating antibodies to _M. tuberculosis_ complex antigens in KNP elephants, has estimated seroprevalence at 6%–9% , suggesting that _M. bovis_ and possibly _M. tuberculosis_ infection is more common in KNP than previously thought. However, no additional samples were available to confirm infection in the individual elephants studied. It is more likely that these seropositive responses in elephants represent infection with _M. bovis_ because all the cases of TB in rhinoceros to date have been due to _M. bovis_ infection . Isolation and speciation of pathogenic mycobacteria are essential for understanding the epidemiology of these infections, especially in areas where human–livestock–wildlife interfaces occur.\n\n【5】Our findings emphasize the importance of surveillance using molecular and bacteriological tools for detection of bovine TB in elephants because serologic assays and visual assessment of gross and histopathological lesions cannot differentiate between infection with _M. bovis_ and _M. tuberculosis_ . In areas where elephants may have indirect (through shared forage and water sources) or direct contact with animal and human populations with high burdens of _M. bovis_ and _M. tuberculosis_ , such as African and Asian range countries, it is crucial that surveillance and diagnostic tools are readily available to distinguish these pathogens to improve our understanding of epidemiology of TB at human–livestock–wildlife interfaces.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "563779b0-d6e8-4e66-99c4-2413ffbc5cc0", "title": "Neutralizing Antibody Response and SARS Severity", "text": "【0】Neutralizing Antibody Response and SARS Severity\nSevere acute respiratory syndrome (SARS) is a newly emerged infectious disease. Its etiologic agent is a novel coronavirus (SARS-CoV) , which can readily infect a variety of wild and laboratory animals without causing apparent clinical symptoms , making the existence of an animal reservoir possible. In humans, SARS appears with a wide clinical spectrum, ranging from self-limited pneumonia to acute respiratory distress syndrome (ARDS) and death . Anecdotally, asymptomatic infection has also been reported .\n\n【1】Autopsies of SARS patients have found the virus to be widespread throughout a variety of tissues and organs . During the acute phase, the virus is found in the excreta of infected persons  and is thought to be transmitted by direct contact, droplets, or contaminated environmental surfaces. Infection can be prevented largely by good hand hygiene, although some healthcare settings and communities may be prone to the aerosolization of contaminated human excreta, and in these cases, precautionary measures should be instigated accordingly . The chain of human transmission has been successfully interrupted by public health measures, but potential reintroduction of the virus from an unidentified natural reservoir remains a concern. A wealth of clinical and epidemiologic observations have emerged and contributed to the successful control of the SARS epidemic . However, information on immunity and pathogenesis is insufficient to provide a comprehensive basis for specific drug or vaccine design. Nor have animal pathogenic models been established that adequately resemble the pathogenesis of SARS in humans. Without a good experimental model to study the biologic basis for human disease, the observational data collected from reported SARS case-patients, along with the associated laboratory diagnostic tests, will continue to provide essential leads in controlling a possible reemergence of SARS. To gain a better insight into the humoral responses in the context of epidemiologic and clinical settings, we analyzed the neutralizing antibody data, along with a variety of epidemiologic elements in the database.\n\n【2】### Material and Methods\n\n【3】This retrospective analysis is based on Taiwan's nationwide database on SARS cases reported from March to July 2003 to the Center for Disease Control in Taiwan (Taiwan-CDC). The criteria for reporting SARS patients evolved over time but were principally adopted from the World Health Organization, and the total reported probable SARS patients in Taiwan were 665.\n\n【4】##### Data\n\n【5】The epidemiologic database contains basic demographic information (age, sex, city/county of residence); symptoms at onset; date of onset of first symptoms; date of diagnosis; dates of hospitalization, discharge, or death; results of all epidemic investigations on contact tracing; travel history; and results of laboratory tests of reverse transcription–polymerase chain reaction (RT-PCR) on SARS-CoV and other pathogens in the differential diagnosis of atypical pneumonia. The analysis of epidemiologic data has been reported previously . The detailed laboratory data taken from molecular and serologic tests of SARS-CoV infection were compiled in a separate file that could be linked to the epidemiologic data. The concordance and discordance between various serologic tests and molecular diagnostic methods of SARS have also been reported previously . The serum neutralizing antibody was measured by microtiter assay and by enzyme-linked immunosorbent assay (ELISA) (Centers for Disease Control and Prevention, Atlanta, GA, USA) as described .\n\n【6】##### Severity of Illness\n\n【7】Hospitalization served the dual purposes of isolating patients and providing health care; therefore, criteria for discharging patients, i.e. being afebrile for 5 days and clinical improvement, were stringently adhered to by the clinicians as a part of public health practice. Since no antiviral drug was known to effectively shorten the clinical course of SARS, the duration of illness, defined as the number of days between onset of fever and time of discharge from the hospital, can be assumed to reflect the clinical severity of SARS manifested by the patient. To validate the consistency of the interhospital practices in patient care in relation to the severity of patients, we collected and analyzed anonymous and computerized clinical data, focusing on oxygen supplementation and respiratory therapy, on a sample of SARS patients from 3 hospitals that represented 3 healthcare accreditation levels in Taiwan: a major medical center (National Taiwan University Hospital), a regional teaching hospital (Taipei Mackay Memorial Hospital), and a district hospital (Taipei Hospital). Regardless of hospital, duration of illness correlated highly with the supplementation of oxygen, which is a good surrogate for the level of pulmonary dysfunction (p for trend <0.001) . Thus, in our analysis, duration of illness was used as a surrogate for clinical severity among the surviving SARS patients, and death rate was also used as severity index. For the convenience of discussion, a duration of illness <2 weeks was considered mild, 2–4 weeks as intermediate, and >4 weeks as severe. A fatal case, regardless of the length of survival, was considered severe. Our data corroborate the report that SARS patients with a severe clinical course mainly had a slower and prolonged recovery .\n\n【8】##### Statistical Analysis\n\n【9】Data were analyzed with SAS software (Version 8, SAS Institute Inc, Cary, NC, USA). Differences in frequencies or proportions were tested using a χ 2  test and by risk ratios. The continuous variables, i.e. age distribution or titers of neutralizing antibody, were compared by using the Wilcoxon rank-sum nonparametric method. Multivariate logistic regression  was used to analyze factors that can affect seropositivity, including demographic information, source of infection, and duration of illness. To adjust the time effects and other covariates of interest, the relationship between antibody titer, based on logarithmic transformation of base 2 (serum dilution) and other potential factors, i.e. age, sex, infection source, and duration of illness, was quantified by linear mixed models , which took into account the correlation between repeated measurements of each study participants.\n\n【10】### Results\n\n【11】##### Participants\n\n【12】Specimens from all patients with probable SARS in Taiwan were serologically tested for case confirmation, with a particular focus on the convalescent-phase serum specimens, as previously reported . Positive neutralizing antibody results, which correlated well with those of ELISA , were used as the standard assay for case confirmation. Thus, 347 of 665 reported probable SARS cases were confirmed. These included cases in 126 patients whose diagnoses were based on serologic testing alone, 121 whose results were positive by both tests, and 100 patients whose diagnoses were based on the RT-PCR alone. Of these 100 diagnoses based on RT-PCR alone, 32 had convalescent-phase serum specimens that tested negative, and 68 did not have appropriate convalescent-phase serum specimens for antibody testing because of death, loss to follow-up, or inappropriate timing of serum collection .\n\n【13】##### Seronegative Results\n\n【14】We examined whether the seronegative results of the 32 patients were false-positive instances of virus detection by RT-PCR, most commonly caused by laboratory error or contamination. Cross-contamination in the laboratory should occur without any correlation with the patients' demographic or clinical parameters. The seronegative rate (.1%, 18/94) was significantly higher in men than in women (.5%, 14/185) (Mantel-Haenszel test, p = 0.004), but the effect of age was not statistically significant (p = 0.07 in men, χ 2  test) . Based on the transmission risk of known or unknown sources, patients whose sources could not be ascertained, i.e. had no apparent history of having contact with SARS patients, were significantly more likely to be seronegative (.1%, 23/51) than those with known sources of infection (.7%, 8/212) (χ 2  , p<10 –7  ) . Patients with a shorter duration of illness were more likely to be seronegative; 14 (.4%) of 46 patients with a duration of illness <14 days, 8 (.8%) of 81 patients with illness durations of 15 and 21 days, 5 (.8%) of 64 patients for those with 22 and 28 days, and none of those who survived for >28 days (χ 2  for trend = 20.5, p = 0.00001). A logistic regression model confirmed that patients with a known source of infection (odds ratio \\[OR\\] = 15.6, p<0.0001) and a longer duration of illness (OR = 1.08 for each additional day of illness, p = 0.004) were more likely to possess a detectable level of neutralizing antibody than those with no discernible infection source and shorter duration of illness .\n\n【15】##### Antibody and Duration of Illness\n\n【16】The total number of serum specimens collected from each patient ranged from 1 to 4, including >1 convalescent-phase serum sample collected after week 4. Of the 247 seropositive SARS patients, 217 (.8%) of the patients were seropositive by week 3; 27 (.4%) of 155 patients who were seropositive within the first 2 weeks of illness and are called early responders hereafter. On rare occasions (.21%, 3/247), seroconversion occurred after week 6 of symptoms onset.\n\n【17】Because of the differences in the distribution of age and sex among patient groups and the differences in the number and timing of specimens collected for antibody measurement, we used regression-based modeling approach to examine these factors simultaneously and tried to analyze the relationship between their potential interactions and antibody titers . This model was based on the neutralizing antibody titer of the 312 convalescent-phase serum assays, representing 194 patients who had had 1 convalescent-phase serum sample collected between weeks 3 to 12 after onset of fever, 41 patients who had 2 convalescent-phase samples, and 12 patients who had 3. The number of serum specimens collected ranged from 21 to 43 per week from week 4 of illness through week 12. The model suggested that neutralizing antibody rose and diminished during the follow-up period between weeks 3 and 13 after onset of illness (p = 0.026 for linear term and p = 0.042 for quadratic term); the estimated half life was ≈6.4 weeks. Patients with a more protracted clinical course tended to have a higher antibody titer than patients with a shorter clinical course (p = 0.008). Antibody in patients with more severe clinical courses tended to decay at a faster rate than in patients with shorter clinical course (the interaction between duration of illness and time of serum collection, p = 0.037). This pattern of decay followed a half-life of ≈6.4 weeks after reaching the peak, which occurred between weeks 5 and 8 after infection . The time that the blood was collected for each patient was examined, and an equally dispersed pattern of blood collection was found in all clinical groups .\n\n【18】Of the 53 patients who had a second convalescent-phase specimen collected after 6 weeks, 16 (.2%) of 53 showed a 4-fold (dilution) drop by week 12 postinfection. Three patients had a negative seroconversion during the same period. Conversely, a measurable neutralizing antibody persisted in 12 (.7%) of 14 patients who had been followed up between weeks 13 and 16. To better examine the dynamics of the antibody profile, cross-sectional views of Figure 2A were extracted . The model suggested that antibody response was higher and occurred earlier in patients with a more severe clinical course than in those with a shorter clinical course.\n\n【19】##### Early Responders and Deaths\n\n【20】In the model, patients with a more severe clinical course had earlier and higher antibody responses; we then examined the death rate of the early responders . These early responders had a significantly higher mortality rate (.6% vs. 7.8%) than others who did not undergo seroconversion until week 3 of illness or later (p = 0.004, χ 2  test). These early responders also tended to die early during the acute phase: 6 of 8 died during the first 2 weeks of illness, and the other 2 died on days 15 and 17 of illness, respectively (Fisher exact test, p = 0.028). Of the 10 patients who died and who seroconverted after the second week of symptom onset, only one died during the first 2 weeks of illness (Wilcoxon rank sum, p = 0.007). Among the 27 early seroresponders, the antibody titer of those who died (n = 8) (median titer = 48) was not significantly higher than that of those who survived (median titer = 32) (Wilcoxon rank sum, p = 0.79). However, the early seroresponders were significantly older (mean age 43.7 years) than the 128 case-patients who seroconverted after week 2 of illness (mean age 37.3 years) (Wilcoxon rank sum, p = 0.028); i.e. older patients were more likely to be early responders, 60% of patients >60 years of age versus 16.3% of patients <60 years (Fisher exact test, p = 0.004).\n\n【21】### Discussion\n\n【22】Neutralizing antibody plays an integral role in immunoprotection from viral diseases, and serologic tests are important to their diagnosis. This report relates SARS neutralizing antibody profiles to clinical outcomes. The lack of a readily available, well-characterized diagnostic assay that could be used as a standard and the additional lack of a well-established typical clinical description that encompasses all clinical syndromes are intrinsic difficulties of working with a new infectious disease. Therefore, analysis of the interrelation between clinical, epidemiologic, and laboratory data might provide further insight into these elements. Results of our analysis, although they passed a certain level of statistical scrutiny, should be interpreted with caution.\n\n【23】##### Antibody Titer and Seronegativity\n\n【24】The 32 seronegative SARS patients whose diagnoses were based on positive RT-PCR results of nasopharyngeal swab specimens warrant further discussion concerning whether they were indeed SARS patients or were merely misdiagnosed by the false-positive RT-PCR of SARS-CoV. The false-positive RT-PCR is most commonly due to cross-contamination, which pertains to the nature and quality of a laboratory procedure and should be independent of patient's profile. However, we found that seronegative patients were more likely to have a short duration of illness and no clear source of infection. Lack of specificity of the test is another reason for having a false-positive RT-PCR, but none of the commercial tests we used have been reported to have nonspecific cross-reactivity with other known pathogens. Furthermore, after May 1, SARS diagnosis required positive results of >2 specimens collected at different time or from different sites, or tested by >1 RT-PCR method (Centers for Disease Control and Prevention, Atlanta, GA, USA; Roche Diagnostics GmbH, Mannheim, Germany; Artus GmbH, Hamburg, Germany) if only 1 specimen was available. Therefore, a specimen that yields false-positive results with 2 different test methods is deemed unlikely.\n\n【25】Alternatively, these 32 patients were indeed SARS patients, but the negative neutralizing antibody reading was due to patent's low antibody level in combination with the low sensitivity of the antibody test. The sensitivity of our neutralizing assay is comparable to that of ELISA , but the possibility that our assay had a low sensitivity remains because the neutralizing antibody test is based on the reading of a complete inhibition of cytopathic effect. Thus the absolute titer is expected to be lower than the results, based on reading of 50% inhibition. The neutralizing antibody of SARS patients has been reported in only 1 other study, in which a pseudovirus containing the S protein of SARS-CoV was used; antibody titers were found to be low . A low antibody response may be associated with a primary infection of SARS-CoV, as seen with primary infection of respiratory syncytial virus (RSV) , and infection through the respiratory tract was shown to stimulate a less vigorous immune response than infection by an invasive intravenous rejection of RSV . Furthermore, a robust humoral immune response requires antigen in sufficient doses through a proper route; this fact has been demonstrated in vaccine studies, including research on several live vaccines . The lack of detectable antibody among patients without history of contact with a known SARS patient might be associated with a low inoculum of the virus because of incidental exposures, in contrast to patients who acquired SARS in hospitals under circumstances assumed to have a high virus density. When systematically screened during the SARS outbreak, some healthcare workers and public health personnel who had a history of direct contact with patients were shown to harbor nasopharyngeal SARS-CoV. Subsequently, however, they did not show seroconversion (Y.-T. Lu et al. unpub. data), which raises the possibility of asymptomatic mucosal epithelial colonization by SARS-CoV. Our seronegative patients with mild symptoms might fit into the spectrum between the seronegative asymptomatic colonizers and the severe SARS patients with high neutralizing antibody response. All considerations appear to favor the possibility that seronegative patients indeed had acquired SARS. Since the natural reservoir of SARS-CoV has not been clearly identified, and reintroduction of SARS-CoV to humans is possible, the short duration of having detectable antibody should be considered when a vaccine against SARS-CoV is developed.\n\n【26】##### Severity and Pathogenesis\n\n【27】The clinical course of SARS patients with severe infection is described as follows: pulmonary functions worsen during week 2 of illness , while the virus load in the airway decreases , and patients with mild disease would begin to stabilize clinically. Those in whom ARDS later develops usually show pulmonary decompensation during week 2. Severity was intensified by a slower and prolonged recovery with complications of pulmonary fibrosis occurring in week 3 in some patients . Results of a high-resolution computed tomographic scan in follow-up of SARS patients corroborates this observation by showing a high correlation between bilateral fibrotic lung changes and clinical severity . Findings of these studies, in conjunction with clinical study on cytokines during the acute phase , suggest that activation of Th1 cell–mediated immunity and a hyperinnate inflammatory response, rather than direct damages from uncontrolled virus growth, are responsible for the pathogenic process in severe infection . In previous vaccine studies, immunization conditions that could induce a stronger activation of Th1 response would concurrently result in a higher antibody response . Thus, the high antibody response and a strong cell-mediated Th1 response may reasonably be understood as concurrent events, and the latter may be causally related to a severe clinical course of SARS. Neutralizing antibody is unlikely to be causally related to the pathogenesis of SARS because treating SARS patients with convalescent-phase serum collected from patients who had recovered from SARS showed no adverse effect and probably had beneficial effects . Thus, for all the reasons stated above, our finding that high neutralizing antibody correlating with clinical severity should not be interpreted to mean that neutralizing antibody is harmful.\n\n【28】##### Death Rate and Early Responders\n\n【29】Having a detectable neutralizing antibody during the first 2 weeks of illness, in our analysis, coincides with a high and an early SARS mortality rate. The basis for early antibody response is not apparent, but 1 possibility is the priming effect of a previous non–SARS-CoV infection. Indeed, antibody against SARS-CoV has been shown to cross-react with human coronavirus 229E . The finding that early responders are older than other SARS patients is in agreement with the priming effect since cumulative infection rate increases with increasing age. The priming effect of a previous viral infection can induce cross-reactive but nonneutralizing antibody, as well as neutralizing antibody to SARS. Furthermore, the nonneutralizing antibodies are known to facilitate viral infection, termed antibody-dependent enhancement (ADE), which is the pathogenic basis of feline infectious peritonitis virus (FIPV, a type II coronavirus), dengue hemorrhagic fever, and other viruses . In the case of FIPV, ADE can occur even with neutralizing antibody . However, this type of ADE resulted directly from neutralizing antibody is unlikely to occur with SARS-CoV infection because a number of SARS patients have been treated with convalescent-phase serum of SARS patients and show no adverse effect .\n\n【30】The hypothesis that the early responders may have experienced a priming effect could be verified by demonstrating that a significantly higher proportion of early responders than other SARS patients possess antibody against non-SARS coronavirus during the acute phase. Early death occurring within the first 2 weeks of illness is also associated with high nasopharyngeal virus load among a subset of SARS patients with information on nasopharyngeal virus load (J.-Y. Yang et al. unpub. data). Unfortunately, the number of early responders for whom information on virus load was available was too few to yield a meaningful statistical analysis on whether high virus load is correlated with an early humoral response. While antibody induced by a variety of SARS-CoV antigen preparations protects against SARS-CoV infection in mice and ferrets , these animals do not develop clinical symptoms resembling that of SARS-CoV infection in humans and thus are not models of pathogenesis. Since ADE can occur through a number of mechanisms and is not completely understood, clinical trials of vaccine against SARS-CoV should be conducted with caution.\n\n【31】In summary, SARS neutralizing antibody level is positively correlated with clinical severity, and in a portion of the patients with mild infection, a detectable neutralizing antibody response may not develop. All published clinical and immunologic data on SARS patients suggest that a strong cell-mediated Th1 response is causally related to a severe clinical outcome, whereas high neutralizing antibody is probably a concurrent event of a strong Th1 activation. Early neutralizing antibody responders are more likely to be older, to have a higher case-fatality rate, and to survive for a shorter time. These observations, if corroborated with further analysis of data collected in other countries, should raise the concerns of possible ADE in the pathogenesis of SARS-CoV infection in humans and should be considered in the process of vaccine development.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ac809d2a-5403-492c-8c92-dbb8f833f8fb", "title": "Unsuspected Dengue and Acute Febrile Illness in Rural and Semi-Urban Southern Sri Lanka", "text": "【0】Unsuspected Dengue and Acute Febrile Illness in Rural and Semi-Urban Southern Sri Lanka\nDengue virus (DENV), with 4 antigenically distinct serotypes (DEN1–4), is the most common cause of arboviral disease; ≈50 million cases of dengue occur annually in >100 countries . Manifestations of dengue infection range from asymptomatic or mild febrile illness to circulatory failure and death from dengue hemorrhagic fever (DHF). Urbanization and geographic expansion of the primary vector for DENV, _Aedes aegypti_ , have fueled the current global dengue pandemic .\n\n【1】DENV has been documented in the Indian subcontinent since the 1960s, and there are recent reports of DENV in new areas and of severe disease . Epidemic dengue was first recognized in Colombo, the capital of Sri Lanka, in 1965–1966 . Outpatient, clinic-based surveillance at Colombo’s Lady Ridgeway Children’s Hospital during 1980–1984 found dengue accounted for 16% of acute febrile illness, among which 66% were secondary (recurrent) dengue cases. A 1980–1985 school-based study found a baseline DENV seroprevalence of 50% in Colombo and a 6-month dengue incidence of 15.6%, of which 37% were secondary cases . In the early 1980s, severe dengue was rare in Sri Lanka: <10 reported cases were DHF . However, since 1989, many cases of DHF have been reported from the heavily urbanized western coastal belt of Sri Lanka, which includes Colombo , and cases have recently been reported elsewhere in the country.\n\n【2】DENV began emerging in southern Sri Lanka recently. To define the epidemiology of dengue in this previously unstudied region, we prospectively enrolled patients seeking care for acute febrile illness at a local hospital. Study participants lived in Galle, a seaport city (population 100,000), and the surrounding coastal plain and heavily vegetated foothills at the southernmost tip of the island nation. During the study months, the temperature ranged from highs of 27.5°C to 32°C and lows of 24°C to 26°C, and rainfall was variable (mean 301 mm, range 36–657 mm).\n\n【3】### Materials and Methods\n\n【4】##### Febrile Cohort\n\n【5】During March–October 2007, we recruited patients in the emergency department, acute care clinics, and adult and pediatric wards of Teaching Hospital Karapitiya in Galle, the largest (bed) hospital in southern Sri Lanka. Consecutive febrile (> 38°C tympanic) patients \\> 2 years old without trauma or hospitalization within 7 days were eligible for study participation. Study doctors verified eligibility and willingness to return for convalescent-phase follow-up and obtained written consent from patients or from parents (for patients <18 years old) and assent from those \\> 12–17 years old. The institutional review boards of the University of Ruhuna, Johns Hopkins University, and Duke University Medical Center approved the study.\n\n【6】Study personnel recorded epidemiologic and clinical data on a standardized form. Study doctors obtained blood for on-site, clinician-requested testing and off-site, research-related testing. Patients returned for clinical and serologic follow-up 2–4 weeks later or were visited in their home if unable to return and their address was known. Blood and serum samples were stored promptly at −80°C and shipped on dry ice to the University of North Carolina School of Medicine (laboratory of A. de S.), where paired serum samples were tested by ELISA and acute-phase serum samples were cultured and tested by PCR.\n\n【7】##### Serologic Testing for Dengue\n\n【8】##### IgM ELISA\n\n【9】We performed dengue IgM capture ELISA as described , except we used anti-flavivirus monoclonal antibody (mAb) 4G2 followed by enzyme-conjugated goat anti-mouse IgG to detect captured DENV antigen. In brief, 96-well plates were coated (overnight, 4°C) with 100 μL/well (ng/μL) of goat anti-human IgM (Sigma, St. Louis, MO, USA) at a concentration of 0.1 mol/L in carbonate buffer (pH 9.6). Plates were washed 3× in Tris-buffered saline with 0.2% Tween 20 (TBST) and blocked with 200 μL/well of 1× Tris-buffered saline with 0.05% Tween 20 and 3% nonfat dry milk. Paired serum samples were tested on the same plate. Diluted serum (:50) was loaded in duplicate and incubated (°C, 1 h) to capture IgM antibody. Unbound antibody was washed, and wells were successively incubated with DENV antigen (mix of serotypes DEN1–4), mouse anti-flavivirus 4G2 mAb, and human-absorbed alkaline phosphatase (AP)–conjugated goat anti-mouse IgG antibody (Sigma). Optical density (OD) was measured at 405 nm after final incubation with AP substrate.\n\n【10】##### IgG ELISA\n\n【11】Dengue IgG ELISA was performed as described . Plates were coated overnight (°C) with 100 μL/well of mouse anti-flavivirus 4G2 mAb at a concentration of 0.1 mol/L in carbonate buffer (pH 9.6) and then washed 3× in TBST. Plates were then blocked with standard diluents and successively incubated (°C, 1 h) with DEN1–4 antigen, diluted serum (:100) in duplicate wells, and AP-conjugated goat anti-human IgG (Fc portion), with 3 washings (TBST) between incubations. Plates were read at 405 nm after a final incubation with AP substrate (min, room temperature, in the dark).\n\n【12】##### Serologic Interpretation\n\n【13】We defined acute dengue as IgG seroconversion (acute-phase OD <0.20 and convalescent-phase OD \\> 0.20) or as a substantial increase in antibody titer (convalescent-phase IgG OD \\> 0.30 or IgM OD \\> 0.20 than acute-phase). Acute primary (first episode) and acute secondary (recurrent) dengue were distinguished by the absence or presence of IgG (OD <0.35 and \\> 0.35, respectively) in acute-phase serum samples. The presence of IgG without a substantial increase in titer defined past dengue infection. Seroprevalence was defined as the presence of IgG (OD >0.20) in acute-phase serum samples; other samples were seronegative. For each ELISA, we used 2 negative control human serum samples, each tested in duplicate. Positive cut-off values were determined during assay validation and were based on the mean OD +2 SD for negative control serum specimens.\n\n【14】##### Isolation of Dengue\n\n【15】Individual 15-μL aliquots of undiluted acute-phase serum (from all patients with confirmed acute dengue as well as negative and positive controls) were each mixed with 185 μL of Eagle minimal essential medium supplemented with 2% fetal bovine serum, and then each was added to C6/36 cells (°C, 5% CO 2  ). After 7 and 10 days, cells were fixed and a direct immunofluorescent antibody assay was performed by using anti-dengue mAb (H2-Alexa488) . Positive samples were centrifuged (× _g_ , 5 min). Supernatants containing virus were supplemented with 20% fetal bovine serum, divided into aliquots, and preserved at −80°C.\n\n【16】##### Dengue Virus Neutralization\n\n【17】We tested the convalescent-phase serum samples from 35 patients with serologically defined acute dengue. Dengue neutralizing antibodies were detected by using a flow cytometry–based assay as described . In brief, serially diluted immune serum samples were incubated with each of the 4 dengue serotypes (°C, 1 h). Next, the virus and serum mixture was added to a human monocyte cell line (U937DC-SIGN) that was engineered to express the dengue receptor DC-SIGN and incubated with 5% CO 2  (°C, 24 h). The cells were washed, fixed, and stained with Alexa 488–conjugated anti-dengue mAb 2H2. The percentage of infected cells was measured by flow cytometry. GraphPad Prism 4 for Windows  and nonlinear regression analysis were used to calculate 50% neutralization values.\n\n【18】##### PCR for DENV\n\n【19】To yield cDNA, we used RNA eluted serum samples (QIAmp Viral RNA Mini Kit; QIAGEN, Valencia, CA, USA) from 25–140 µL of acute-phase serum samples, reverse transcriptase (RT), and DENV downstream consensus primer D2 or random primers. To confirm and serotype DENV, we used consensus primers D1 and D2 to amplify cDNA by standard PCR; positive first-round PCR products were diluted 1:100 and used as template in second-round PCR with consensus primer D1 and nested serotype DEN1–4 type-specific primers . Negative controls were included for the RT and PCR steps. The negative control for the RT step consisted of a sample to which all reagents, except RNA, were added. The negative control step for PCR consisted of a sample to which all reagents, except cDNA, were added.\n\n【20】##### Statistical Analysis\n\n【21】Proportions were compared by the χ 2  test or Fisher exact test and continuous variables by Student _t_ test or the rank sum test, if not normally distributed. Analyses were performed by using Stata IC 11.0 (StataCorp LP, College Station, TX, USA).\n\n【22】### Results\n\n【23】##### Febrile Cohort\n\n【24】Paired serum samples to identify acute and past dengue were available for 859 (.6%) of 1,079 patients enrolled. The likelihood of follow-up did not differ by age (p = 0.30), sex (p = 0.22), or level of education (p = 0.74). Most (.2%) patients reported rural residence, and follow-up was more likely among rural (.6%) than urban (.5%) dwellers (p = 0.008). The reported duration of fever and illness was similar in patients who did and did not return for follow-up.\n\n【25】Among the 859 patients with paired serum samples, 61.2% were male, and the median age was 30.7 years (interquartile range \\[IQR\\] 19–48 years), which did not differ by sex (p = 0.97). The median reported duration of fever and of illness before seeking medical care was 3 days (IQR 2–5 and 2–7 days, respectively). Many patients (.3%) reported having taken an antimicrobial drug for the illness. The median time between acute-phase and convalescent-phase follow-up was 21 days (IQR 15–32 days).\n\n【26】##### Acute Dengue\n\n【27】Acute dengue occurred during each month of the study and accounted for 3.0% (May) to 11.1% (October) of acute febrile illnesses. Fifty-four patients (.3%) had acute dengue (primary and 27 secondary infections). Of patients \\> 18 and <18 years old, 48 (.0%) and 6 (.5%), respectively, had acute dengue (p = 0.09). The age distribution of patients with and without acute dengue is shown in Figure 1 . The median age of patients with acute dengue was 27.6 years (IQR 22–45 days), and a similar proportion were male versus female (p = 0.26).\n\n【28】The clinical features of patients with and without acute dengue are detailed in Table 1 . Headache was the most frequent (.9%) symptom, and lethargy and muscle and joint pain were also reported by >50% of patients with dengue; however, these symptoms were just as frequent in patients without dengue. Patients with dengue were less likely than those without it to report cough and sore throat and to have lymphadenopathy, but they were more likely to have conjunctivitis. Although gastrointestinal symptoms and signs were uncommon overall, diarrhea, jaundice, hepatomegaly, and abdominal tenderness occurred more frequently in dengue patients. Patients with acute dengue had statistically significantly lower leukocyte and platelet counts than patients without dengue. No dengue patients had petechiae.\n\n【29】Only 3 acute dengue patients (men 23–27 years old; 2 with secondary and 1 with primary infection) had platelet counts consistent with DHF (<100,000/μL) . At admission, 2 of the 3 men reported myalgia and arthralgia; all 3 had headache, leukopenia (leukocytes 2,400–3,600 cells/μL), severe thrombocytopenia (platelets 19,000–47,000/μL), and compensated shock (blood pressure 100–110/70–80 mm Hg, with heart rate 88–100 beats/min). Additional laboratory testing only included determination of transaminase levels, which were elevated in the 1 patient in whom they were evaluated. All 3 patients were well at follow-up.\n\n【30】Patients with acute dengue were more likely than those with other causes of fever to be hospitalized (.6% vs. 71.3%; p = 0.001), but their duration of stay was similar (median 5 vs. 4 days \\[IQR 3–7 and 3–6 days, respectively\\]; p = 0.23). A presumptive clinical diagnosis was available for 791 patients, including 50 with acute dengue. Few adults and children were suspected to have acute dengue on the basis of clinical features (.3% vs. 1.2%, respectively; p = 0.16), and only 7 of the 23 patients (all adults) with clinically suspected dengue had laboratory-confirmed infection (positive predictive value 30.4%, 95% CI 27.2–33.6). Clinical diagnosis also had poor sensitivity (.0%; 95% CI 11.6–16.4) and high specificity (.8%; 95% CI 96.8–98.9). Compared with other patients, those with suspected dengue were more likely to be admitted to the hospital (.4% vs. 95.7%; p = 0.008) and less likely to be treated with antimicrobial drugs (.6% vs. 20.0%; p = 0.02). However, 50.0% of those with confirmed acute dengue reported taking antimicrobial drugs before seeking medical care.\n\n【31】Clinical illness was similar in patients with primary and secondary dengue. Median platelet counts were 186,000/μL (IQR 153,000–234,000/μL) and 198,000/μL (IQR 160,000–270,000/μL) for patients with primary and secondary dengue, respectively (p = 0.60). Diarrhea was more common in patients with secondary than with primary dengue (.3% vs. 11.1%; p = 0.05), as were jaundice (.8% vs. 0.0%; p = 0.04) and hepatomegaly (.2 vs. 0.0%; p = 0.01). The duration of illness before patients sought care was similar for those with primary and secondary dengue. Patients with primary dengue were as likely as those with secondary dengue to be admitted to the hospital (both 92.6%; p = 1.0) and to remain for a similar duration (median 5 days for both \\[IQR 3–6 and 3–8 days, respectively\\]; p = 0.89).\n\n【32】##### Acute or Past Dengue\n\n【33】Fifty-four percent (/859) of the study cohort had either acute or past dengue . The overall seroprevalence was 50.9% (/859 patients) because the 27 patients with acute primary infections were seronegative at enrollment. The proportion of patients who were seropositive at enrollment increased in each older age group, from 9% in those <5 years old to 72% in those 40–44 years old . Overall, seropositivity was more likely in male than in female patients (.9% vs. 42.9%; p<0.0001), and in urban than in rural dwellers (.3% vs. 52.0%; p<0.0001).\n\n【34】##### Confirmation of Acute Dengue and Serotyping of Virus\n\n【35】We isolated DENV from 12 (.2%) of the 54 acute dengue patients. The likelihood of isolating DENV was higher when the patient had fewer reported days of fever (median 3 vs. 4 days; p = 0.04), and DENV was isolated almost exclusively from patients with primary versus secondary dengue (.7% vs. 8.3%; p = 0.001).\n\n【36】Dengue PCR results were positive in 19 (.2%) patients; positive results were higher among patients with positive versus negative culture results (.0% vs. 23.8%; p = 0.001). Fever duration was similar in patients with dengue-positive and -negative PCR results (median 4 days; p = 0.22). A positive PCR result was not statistically more likely in patients with primary versus those with secondary dengue (.9% vs. 42.1%; p = 0.39). PCR testing confirmed serotype DEN2 (primary cases), D3 (primary, 7 secondary), and D4 (primary, 1 secondary); no cases of serotype DEN1 were identified. DENV neutralization testing confirmed the specific presence of dengue antibodies in 33 (.3%) of 35 IgG-positive serum samples (% CI 80.8–99.3).\n\n【37】### Discussion\n\n【38】We sought to define the epidemiology of dengue in the southern tip of Sri Lanka. We prospectively enrolled patients with a reproducible criterion (documented fever) and rigorously distinguished acute from past infections by using paired serum samples. The seroprevalence of dengue in our cohort increased with age; however, half of those 20–25 years old were seronegative. In contrast, 50% of children in Colombo are positive for dengue IgG antibody by 5 years of age, and >70% have been found to be seropositive by 12 years of age . The high proportion of susceptible adults in Galle suggests the recent emergence of DENV in this area .\n\n【39】Dengue is considered an urban and peri-urban disease, and the current pandemic is partly attributed to increasingly populous cities . We documented cases of acute dengue during March–October in rural and semi-urban areas. In rural Cambodia, dengue is a major cause of hospitalization and death among children, with delays in recognition and care-seeking contributing to its impact in rural areas . Discarded water storage jars and concrete water tanks are commonly used breeding sites for the vector , and mosquito populations may be higher in rural than urban areas . Dengue has also been documented in rural Vietnam . Control efforts in urban Sri Lanka have included covering cement water tanks, which are established breeding sites for _Aedes_ spp ; however, data to direct control efforts in Southern Sri Lanka are lacking.\n\n【40】We describe the clinical manifestations of endemic dengue in an unselected febrile cohort in southern Sri Lanka. Dengue accounted for 6.3% (/859) of acute febrile illnesses in this cohort; serotypes DEN2–4 caused illness, with DEN3 being the most frequent. In the cohort (predominantly adults who sought care early in illness), manifestations of acute dengue were similar to those of other causes of fever, except for more diarrhea, jaundice, and abdominal tenderness and less cough and sore throat. Ramos et al. similarly identified the presence of diarrhea and absence of respiratory symptoms as early features of confirmed acute dengue infection in adults; however, 53.5% of the patients in that study were excluded from analyses because only single serum samples were tested.\n\n【41】We identified a disease spectrum different from that of classic severe dengue reported in hospital-based studies during epidemics in urban centers (Colombo and Kandy) in Sri Lanka ; the spectrum we report is similar to that described in studies of acute febrile illness elsewhere in Asia. In a report from Singapore, headache, retro-orbital pain, arthralgia, anorexia, nausea, and vomiting were symptoms statistically associated with dengue, but, as in our study, the symptoms were common in their entire febrile cohort . In the same study, signs more suggestive of dengue were infrequent: red eyes (.6% of patients), rash (.2%), and bleeding (.5%). The only signs we found statistically associated with acute dengue were jaundice, abdominal tenderness, and hepatomegaly. Others have found abnormal test results for liver function to be common among patients with secondary dengue . That we observed jaundice only in secondary cases lends credence to the hypothesis that host response to the virus may be a major factor in the pathogenesis of dengue .\n\n【42】Our finding that only 14% of patients with dengue were identified clinically is consistent with the recent emergence of dengue in southern Sri Lanka. In addition, the finding emphasizes the difficulties with clinical diagnosis, particularly in unselected patients with recent onset of fever (median 3 days) in the absence of a recognized epidemic. Clinical acumen is difficult to develop when confirmatory testing is not available, even in a subset of patients. This fact highlights the need for rapid, accurate point-of-care diagnosis , which could also limit the frequent use of unnecessary antimicrobial drugs.\n\n【43】A limitation of many studies is that diagnosis of acute dengue was made on the basis of a single serum sample. In studies of severe dengue in urban Sri Lanka  and in a more recent case-control study of dengue , acute dengue was defined by a positive result from a PanBio dual IgM/IgG rapid strip test (PanBio Pty Ltd. Brisbane, Australia) , which was performed on a single serum sample obtained on day 7 of illness. This strategy may be adequate during an epidemic in a patient with classic severe dengue; however, in a setting of high dengue seroprevalence and lower pretest probability (undifferentiated fever, no recognized epidemic), a single positive serologic test result may more often denote past dengue, and the true cause of illness might remain unrecognized and untreated.\n\n【44】Unique strengths of our study include rigorous confirmation of acute infection by World Health Organization criteria , a large sample size with 80% follow-up (critical to reference-standard diagnosis and assessment of outcomes), and prospective clinical correlation. To confirm dengue, the World Health Organization requires detection of virus by isolation or PCR or a 4-fold rise in antibody in paired serum samples. Cases with supportive serologic test results (single high antibody titer) are considered probable cases because antibody may not be present early (poor sensitivity) or may represent past infection (poor specificity) . In our study, we would have identified only secondary cases if a probable case definition had been used because IgG is initially absent in primary dengue. We assessed paired serum samples by using an ELISA comparable to the more difficult and time-consuming hemagglutinin inhibition test used for determining dengue seroprevalence in populations  and the ELISA used for dengue surveillance in Colombo, Sri Lanka . A recent systematic review scrutinized clinical studies designed to aid clinicians in resource-poor settings by identifying features predictive of acute dengue . Most studies had myriad methodologic flaws, including inadequate diagnosis, which made prediction impossible. We prospectively studied unselected patients by using rigorous diagnostic criteria to minimize recall, selection, and diagnostic verification bias.\n\n【45】A limitation of serologic testing is that serologic cross-reactions occur between dengue and other flaviviruses, but we believe this is unlikely to bias our results. West Nile virus has not been described in the region, yellow fever is not present in Asia, and Japanese encephalitis is of low endemicity, with no recent outbreaks reported in the area. Furthermore, we used dengue virus neutralization testing to confirm the specific presence of dengue in a subset of IgG-positive serum samples in addition to performing PCR and viral isolation on the acute-phase serum samples of patients with serologically confirmed acute dengue. The median duration of fever in those in whom virus was isolated is consistent with the reported persistence of viremia for 5–6 days after onset of symptoms . Because most patients were adults, we could not compare the clinical features of symptomatic dengue in adults versus those in children.\n\n【46】We have not delineated the full clinical spectrum of dengue, which would require a prospective population-based study. However, we believe that the population studied is representative of patients in the region with clinically noteworthy symptomatic dengue: Teaching Hospital Karapitiya has a large catchment area and is 1 of 2 public teaching hospitals in the Southern Province, and there are no large private hospitals in the area. Patients with fulminant dengue may die before hospital evaluation, but most, including indigent patients from outlying areas, seek care because of free access.\n\n【47】In summary, we found that dengue is responsible for 6.3% of undifferentiated febrile illnesses in southern Sri Lanka. We identified relatively mild disease, despite identifying mostly serotype DEN3 infection, which is difficult to distinguish from other acute febrile illnesses, and we found acute infections were mostly in young adults. We hypothesize that the burden and spectrum of acute dengue is under-recognized in other regions because of limited study and limited tools for rapid diagnosis. Clinical recognition is poor for less severe dengue, particularly, perhaps, when endemicity is low or transmission is sporadic. Assays to measure antibody in single serum samples may be misleading, especially early in illness, and PCR and viral isolation are often unavailable and may be insensitive. Other causes of acute febrile illness require antimicrobial drug therapies, but dengue only requires supportive care; thus, improved low-cost diagnostic tools are urgently needed to guide clinical management.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5d567d4a-eaca-4b55-a5f7-86fd3d06dde5", "title": "A Framework for Implementing the National Diabetes Prevention Program in Los Angeles County", "text": "【0】A Framework for Implementing the National Diabetes Prevention Program in Los Angeles County\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Preventing type 2 diabetes is a public health priority in the United States. An estimated 86 million Americans aged 20 years or older have prediabetes, 90% of whom are unaware they have it. The National Diabetes Prevention Program (NDPP) has the potential to reduce the incidence of type 2 diabetes; however, little is known about the best way to institutionalize such a program in a jurisdiction with a racially/ethnically diverse population. The objective of this study was to develop a practice-grounded framework for implementing the NDPP in Los Angeles County.\n\n【3】**Methods**\n\n【4】In 2015, the Los Angeles County Department of Public Health (LACDPH) partnered with Ad Lucem Consulting to conduct a 3-stage formative assessment that consisted of 1) in-depth interviews with key informants representing community-based organizations to learn about their experiences implementing the NDPP and similar lifestyle-change programs and 2) 2 strategic planning sessions to obtain input and feedback from the Los Angeles County Diabetes Prevention Coalition. LACDPH identified core activities to increase identification of people with type 2 diabetes and referral and enrollment of eligible populations in the NDPP.\n\n【5】**Results**\n\n【6】We worked with LACDPH and key informants to develop a 3-pronged framework of core activities to implement NDPP: expanding outreach and education, improving health care referral systems and protocols, and increasing access to and insurance coverage for NDPP. The framework will use a diverse partner network to advance these strategies.\n\n【7】**Conclusion**\n\n【8】The framework has the potential to identify people with prediabetes and to expand NDPP among priority populations in Los Angeles County and other large jurisdictions by using a diverse partner network.\n\n【9】Introduction\n------------\n\n【10】Preventing type 2 diabetes is a public health priority in the United States . Prediabetes occurs when a person’s blood glucose level is higher than normal (fasting blood glucose level of 100–125 mg/dL \\[5.6 to 7.0 mmol/L\\]), putting the person at increased risk for heart disease, stroke, and developing type 2 diabetes . An estimated 86 million Americans aged 20 years or older have prediabetes, and 90% of those do not know they have it . In 2012, diabetes and prediabetes were estimated to cost $245 billion nationally and $32.3 billion in California, through direct medical spending and lost productivity .\n\n【11】Prediabetes can be reversed through lifestyle modifications . For example, the National Diabetes Prevention Program (NDPP), an intensive lifestyle-change program focused on improving diet and physical activity, can delay the onset of diabetes among those at risk . In response, the Centers for Disease Control and Prevention (CDC) made significant investments in state and local efforts to translate NDPP into community settings and to grow the program in the Los Angeles County . In 2014, the Los Angeles County Department of Public Health (LACDPH) was selected as a large-city participant in CDC’s cooperative agreement 1422, State and Local Public Health Actions to Prevent Obesity, Diabetes, and Heart Disease and Stroke. In Los Angeles this program is called the Chronic Disease Prevention Strategy. One of the program’s primary aims is to expand access to and participation in NDPP in Los Angeles. To advance this goal, LACDPH partnered with the YMCA of Metropolitan Los Angeles to co-lead the Los Angeles County Diabetes Prevention Coalition (LACDPC), which was established in 2012 to help the YMCA and others expand enrollment in the NDPP.\n\n【12】Although LACDPC could serve as a powerful vehicle in Los Angeles to implement NDPP, the actions the coalition should take to institutionalize the program across this large, racially/ethnically diverse jurisdiction were unclear. In particular, little is known about the best ways to implement such a program among high-risk, high-burden priority populations (ie, those who have prediabetes or uncontrolled high blood pressure, those who live in low-income communities, and those in racial/ethnic minority populations who experience disparities in access to and quality of care) . To develop a practice-grounded framework for the Los Angeles County NDPP, we conducted a 3-stage formative assessment. This assessment sought to answer the following questions: 1) what core activities are needed to identify, refer, and enroll eligible participants in the NDPP, including establishing payment options to offset costs? and 2) what key partners are needed to advance this work?\n\n【13】Methods\n-------\n\n【14】In summer 2015, LACDPH partnered with Ad Lucem Consulting (ALC) to conduct a 3-phase formative assessment to inform objectives for the Chronic Disease Prevention Strategy, including identifying a strategic plan for the LACDPC. The assessment team, which included staff members from LACDPH and ALC, used an outcomes-focused approach , first defining the desired goal — increasing identification, referral, and enrollment of eligible populations into the NDPP — and then working backward to identify key activities and partners. In phase 1, ALC conducted in-depth interviews with key informants to learn about their experiences implementing NDPP and similar lifestyle-change programs. In phase 2, ALC and LACDPH presented results from these interviews to LACDPC for input and feedback. In phase 3, LACDPH synthesized results from the interviews and coalition dialogues into a practice-based framework. The project was reviewed and considered exempt by the LACDPH internal review board.\n\n【15】### Phase 1: Qualitative key informant interviews\n\n【16】Eligible participants were recruited through a multistage sampling process that combined snowball and maximum variation techniques . The goal was to identify leaders in prevention and management in the United States working in various roles (ie, practitioners, policy makers, researchers, and funders) and organizations (eg, health care agencies, health plans, health departments, community-based organizations \\[CBOs\\]). First, LACDPH, along with contacts at the American Diabetes Association, American Association of Diabetes Educators, and CDC, provided ALC with a list of potential participants who were individuals or organizations that had field experience implementing the NDPP or other chronic disease prevention or management programs, or who had conducted NDPP-focused research and evaluation. An initial round of interviews was carried out with those listed: 1) organizations that were CDC-recognized NDPP providers (if applicable), and 2) individuals or organizations that had experience serving priority populations. Interviewees in the initial round were then asked to identify other individuals or organizations meeting the inclusion criteria. Recruitment continued until the sample included a balance of participants in terms of roles and organizational types. ALC recruited all potential interviewees through email, with follow-up telephone calls, as needed.\n\n【17】Of the 45 experts identified, 33 consented to participate. ALC conducted interviews with representatives from health departments and government agencies (respondents), health care providers and health plans (respondents), nonprofit and CBOs (respondents), academic institutions (respondents), and funders (respondents). Most participants were from Los Angeles (respondents), and the others (respondents) were from large metropolitan cities (populations of 8 million or more) or large states (populations of more than 19 million).\n\n【18】Four trained ALC interviewers conducted all interviews by telephone in August and September 2015. The interview guide included 7 primary open-ended questions and associated probes focused on understanding 1) how to increase referrals to NDPP, 2) how to increase enrollment in NDPP, 3) models for NDPP delivery, 4) barriers to implementing NDPP, 5) models for reimbursement and coverage of NDPP, 6) expanding the pool of NDPP providers, and 7) the role of health departments in implementing NDPP. Each interview was conducted by one interviewer and lasted approximately one hour, during which time the interviewer typed notes (transcripts) to record responses verbatim. The transcripts were then uploaded into Atlas.ti (Scientific Software Development, GmbH) for qualitative analysis. First, 2 interviewers developed a list of thematic codes based on the interview questions. Second, the 4 interviewers independently coded transcripts in batches, meeting 4 times as a full group to reconcile coding, refine the coding scheme, and group codes into themes in 4 predefined areas relevant to developing a strategic plan for LACDPC 1): ways to increase demand for the NDPP among patients and providers, 2), ways to engage the health care system, 3) ways to increase the supply and capacity of NDPP providers, and 4) ways to conduct NDPP implementation research and evaluation.\n\n【19】### Phase 2: Input from the Los Angeles County Diabetes Prevention Coalition\n\n【20】ALC and LACDPH presented the themes developed from the interviews during phase 1 to LACDPC during 2 in-person strategic planning sessions in December 2015 and February 2016. Sixteen coalition members, representing 11 institutions, participated. The sessions, which lasted approximately 3 hours each, focused on systematically generating input on key activities that were most important and relevant to advancing diabetes prevention in Los Angeles. We solicited reactions from coalition members with questions such as, Is this activity important? What partners are needed to implement these activities? What else could complement these activities? and How can these activities be applied to diabetes prevention work locally in Los Angeles? During each session, one trained note taker recorded participants’ discussion. ALC and LACDPH staff members then met to review and summarize the notes, which were then shared via email with the full coalition for final input and confirmation.\n\n【21】### Phase 3: Framework development\n\n【22】Two LACDPH team members conducted a holistic analysis of the data generated during phases 1 and 2 and synthesized these into a framework that identified the core activities needed to increase identification, referral, and enrollment of eligible populations into the NDPP in Los Angeles. The 2 team members were guided by, but not bound to, analyses conducted by ALC in phase 1 to identify the themes that were presented to the coalition. LACDPH staff compiled interview transcripts and notes from coalition sessions into a master document. Two team members reviewed this document independently, working inductively to assign descriptive codes to segments of text . After independent review, the 2 team members met to discuss codes, reconcile differences in interpretation, and develop a core set of activities. The team prioritized activities that were identified by both multiple interview participants and members of LACDPC. The team grouped the activities into overarching domains and developed a visual representation to show how they relate to one another and the desired outcomes.\n\n【23】Results\n-------\n\n【24】Key informant interviews generated themes in phase 1, and the coalition generated additional input in phase 2 . The framework developed in phase 3 consists of 3 domains 1): expanding outreach and education, 2) improving health care referral systems and protocols, and 3) increasing access to and insurance coverage for NDPP. The framework relies on a diverse partner network for advancing these strategies .  \n\n【25】Framework for implementing the National Diabetes Prevention Program (NDPP) in Los Angeles County. \n\n【26】**Expand outreach and education.** The first domain emphasizes the importance of providing education and training to the public and community-based partners and health care providers to increase knowledge and awareness of prediabetes and of the NDPP. LACDPH identified 3 activities to expand outreach and education. First, to help increase knowledge among the general public, public education campaigns such as the Ad Council’s Diabetes Prevention Campaign  are needed to emphasize the importance of preventing diabetes and the value of lifestyle-change programs. Participants recommended creating and disseminating a national message that includes an appropriate local-level focus to help empower those most affected by prediabetes to talk to their health care providers (ie, helping to increase demand for the NDPP). Second, participants recommended developing educational resources targeting community partners and health care providers (ie, physicians, community health workers, _promotoras_ , health navigators, and large employers). Participants emphasized the importance of training community-based partners and health care providers on the content and scope of NDPP, on tools to screen people for prediabetes, and on integrating screening and referral tools into practice. Existing training resources, such as the American Medical Association–CDC Prevent Diabetes STAT toolkit , were identified as important tools for increasing knowledge and enhancing the referral process. Third, participants recommended creating an NDPP resource inventory (ie, informational resource lists and databases) as a complementary activity to increase knowledge and awareness of programs, including where, when, and in what languages classes are offered.\n\n【27】**Improve health care referral systems and protocols** . The second domain emphasizes the need to create referral systems and protocols for health care providers to refer and identify at-risk patients to NDPP. LACDPH identified 3 activities to improve health care referral systems and protocols. First, participants recommended enhancing the existing electronic medical record (EMR) system to identify patients with prediabetes and refer those eligible to local health care providers participating in NDPP. Participants recommended developing mechanisms to conduct regular queries of EMRs to identify patients at risk for prediabetes and link them to NDPP providers through an automatic referral process. CBOs could use similar electronic processes to screen people for diabetes risk and refer those eligible directly to local NDPP providers. Second, participants recommended modifying EMRs to create feedback loops between the health system and local NDPP providers. These feedback loops would help enhance bidirectional communication between health systems and community-based NDPP providers to more effectively manage patient care. Third, participants recommended expanding the use of team care and nonphysician providers, especially community health workers, to identify and refer patients to NDPP. Participants emphasized the relevance of using team-based approaches to reduce provider burden and enhance coordination of care for patients to improve processes for identifying and referring patients to NDPP.\n\n【28】**Increase access to and insurance coverage of the NDPP.** The third domain emphasizes the importance of increasing access to NDPP and insurance coverage for health care associated with participation in NDPP. Participants described the lack of program options (eg, delivery formats, language options) and insurance coverage for NDPP as significant barriers to enrollment, especially for priority populations, including those in low-income communities and those who speak languages other than English. LACDPH identified 2 activities to increase access and coverage. First, participants recommended increasing the availability of NDPP providers in diverse settings and expanding the network of NDPP providers by 1) providing technical assistance to new organizations to administer the NDPP and 2) helping current providers increase their reach in priority areas. Participants identified the need to improve the cultural relevance of NDPP, including training lifestyle coaches that represent the cultures and languages of high-risk populations and developing and disseminating a culturally diverse resource guide for participants to augment NDPP and support the adoption of healthy behaviors. Recommendations were made to offer the program in identified priority languages: Spanish, Chinese, and Korean. Second, participants described the need to partner with large worksites and insurers to offer NDPP as a covered insurance benefit. Working directly with employers can help facilitate access to NDPP and insurance coverage for NDPP health care services. Interview participants felt that employer-based NDPP programs (ie, offering NDPP directly at targeted worksites) were a convenient way to engage potential program participants, implement screening protocols, and facilitate coverage of the program. Insurance providers were identified as another key partner in helping to remove cost barriers to participation in the NDPP. The need to conduct additional research and evaluation to identify NDPP models that meet the need of payers by demonstrating return on investment is a high priority. In addition, creating financial and quality incentives, such as a Healthcare Effectiveness Data and Information Set measure for prediabetes, might facilitate increased access to and insurance coverage of NDPP-related health care.\n\n【29】**Partner network.** The framework relies on a diverse partner network to implement NDPP. Participants described diverse partnerships to facilitate capacity building among providers, assist with the development of educational resources for training, increase awareness of NDPP, and provide resources for increasing access to the program. Participants stressed the importance of partnerships among health care organizations, local and national government entities, nonprofit organizations, CBOs, payers, local funding organizations, and NDPP provider organizations. Additionally, participants emphasized the need to work with local NDPP providers to pilot programs and test payment models to build the case for insurance coverage. Participants from LACDPC recognized the importance of their role in facilitating many of these partnerships by convening key stakeholders (ie, NDPP providers, insurers, academic partners, health care providers, government, CBOs) and working to grow the coalition to increase the diversity of organizations and member expertise.\n\n【30】Discussion\n----------\n\n【31】We described a 3-pronged framework to increase the identification, referral, and enrollment of participants in NDPP: expanding outreach and education, improving health care referral systems and protocols, and increasing access to and insurance coverage of the NDPP. The framework relies on a diverse partner network in advancing this work. The framework provides a roadmap for the work of LACDPH and LACDPC.\n\n【32】Increasing uptake of the NDPP in Los Angeles will require the use of a multipronged approach that simultaneously focuses on increasing availability of and demand for the program while reducing potential barriers to program participation. Such an approach echoes calls to action from leaders in community translation and in clinical prevention; these calls have separately included recommended actions to increase awareness among patients and health care providers about the risk of prediabetes , to enhance clinical systems to institutionalize the novel prevention approach , or to implement varied and sustainable program and payment models to ensure that the NDPP is available and accessible to the full population in need . The framework developed in this study synthesized key informant recommendations into a single practice-based model that emphasizes the importance of advancing the 3 prongs of the framework concurrently so that they are mutually reinforcing.\n\n【33】Our study suggests that diverse partners are needed to implement the framework. Best-practice recommendations to implement evidence-based programs reinforce the importance of early and meaningful involvement from a full range of stakeholders ; our study suggests that key stakeholders in the implementation of NDPP should include representatives from business, health systems, NDPP providers, government, community, education/academia, and philanthropy. A coordinated, collaborative effort that includes these groups (the foundations of this effort were developed locally by LACDPC ) will be needed to advance the multifaceted and mutually reinforcing strategies necessary to implement NDPP in Los Angeles County. We anticipate that LACDPC can build on this study’s framework with input from community members to address the complex health problem that diabetes poses .\n\n【34】The formative assessment process and resulting framework described in this study has been useful in Los Angeles for organizing and developing plans to implement NDPP . The framework is currently being implemented by LACDPC, which has adopted a subcommittee structure to advance activities in each of the framework’s 3 domains. Other evidence-based health promotion programs have suggested the need for additional actions, such as assessing local conditions and capacity . However, this type of planning action did not emerge as a priority in our study. One potential reason for this is that the multistage process of vetting broader national perspectives (collected in phase 1) with local stakeholders (through the planning sessions held in phase 2), resulted in a framework that reflects existing conditions in Los Angeles County and steps needed to implement the framework. Although more work is needed to systematically examine the framework’s local usefulness and impact, other interested jurisdictions may wish to adapt the formative assessment process used in this study to develop practice-based frameworks that reflect their own local needs.\n\n【35】This study has several limitations. First, although the strategic planning process offered an opportunity to confirm and enrich interview data, the scope of the information presented during these sessions was limited by the initial interview guide. A more open-ended process could have acquired more information. Similarly, soliciting the perspective of potential NDPP participants could have provided information on barriers and facilitators to program enrollment; however, because our study focus was to identify key actions organizations could take expand NDPP, collecting such data from potential participants was beyond the scope of this assessment. Second, the assessment was guided by an outcomes-focused, practice-grounded approach , which sought to identify concrete action steps to increase the identification, referral, and enrollment of eligible populations into NDPP in Los Angeles. A theoretical model was not used to guide the assessment. Finally, although key informant interviews were conducted with various local and national experts, viewpoints from Los Angeles were heavily represented in the development process. Additional efforts are needed to determine whether our framework can be useful for other jurisdictions.\n\n【36】A comprehensive framework that identifies the core activities and partners needed to implement the NDPP regionally can provide a useful platform to organize collaborative efforts. Other jurisdictions can use the processes and results in this study to help advance evidence-based, lifestyle-change programs such as the NDPP in their communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "191decd1-220d-4d77-968e-9a11a7a074f2", "title": "A Rondelay (Without Cadenza) By The Virion Of Influenza", "text": "【0】A Rondelay (Without Cadenza) By The Virion Of Influenza\nNow you have me―sucrose-banded,\n\n【1】Enveloped and negative-stranded\n\n【2】Spiked and cleaved and slightly dented\n\n【3】Into pieces eight, segmented―\n\n【4】Without mercy I’ve been strained\n\n【5】Through filters―then been bromelained, and\n\n【6】Torn apart by each detergent\n\n【7】With a haste unseemly, urgent―\n\n【8】All my helices displayed\n\n【9】Just to show you how I’m made.\n\n【10】My polypeptides have been mapped\n\n【11】My hemagglutinin unwrapped―\n\n【12】All my sequences are clear―\n\n【13】All the way from Arg to Ser .\n\n【14】With techniques sharp and newly honed\n\n【15】My very genes have now been cloned―\n\n【16】Transplanted to an alien host\n\n【17】Wherein my evanescent ghost\n\n【18】As solitary as an elf\n\n【19】Has managed to express myself.\n\n【20】And scientists have labored nights\n\n【21】To probe my antigenic sites\n\n【22】Some fresh from school with new diplomas\n\n【23】(Aided by their hybridomas)\n\n【24】Scramble up trimeric slopes\n\n【25】Counting all my epitopes\n\n【26】And every Ph.D. or pupil\n\n【27】Utterly devoid of scruple\n\n【28】Mates me with complete abandon―\n\n【29】Asks, then, why my genes are random\n\n【30】Can I kiss and never tell\n\n【31】When genotyped upon a gel?\n\n【32】Which, if inspected with acuity\n\n【33】Will document my promiscuity―\n\n【34】Must you write, in fat reports\n\n【35】How flagrantly I reassort?\n\n【36】No boundaries have my misbehavin’\n\n【37】Horsey set or duck or avian―\n\n【38】In other moments less sublime\n\n【39】You’ve put my perils before swine!\n\n【40】And yet in\n\n【41】Despite the work that has been done\n\n【42】The epidemics come and go\n\n【43】As regular as winter snow.\n\n【44】And people cough and people die\n\n【45】And all of you still wonder why.\n\n【46】I’m so perverse and ever mutable\n\n【47】And so eternally unscrutable.\n\n【48】But think about just what you’d do\n\n【49】If there were really\n\n【50】No more flu!", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6ebcac82-04d7-4bcf-b963-11f4724e264e", "title": "Population-Based Estimate of Melioidosis, Kenya", "text": "【0】Population-Based Estimate of Melioidosis, Kenya\n_Burkholderia pseudomallei_ , the causative agent of melioidosis, is a gram-negative bacillus endemic particularly in northern Australia and South and Southeast Asia. Worldwide, _B. pseudomallei_ causes ≈165,000 cases of disease and ≈89,000 deaths annually . The presence of _B. pseudomallei_ in Africa has been demonstrated by sporadic cases of melioidosis reported in travelers returning from countries including Kenya . Indigenous culture-confirmed cases have been reported in only 4 countries in Africa, mainly from research centers with diagnostic laboratory facilities .\n\n【1】The first case of melioidosis linked to Kenya was diagnosed in 1982 in a tourist from Denmark who had visited Nyali (an area of Mombasa City), ≈50 km south of the town of Kilifi . Follow-up clinical surveillance in Nairobi and environmental surveillance from other regions in Kenya yielded no _B. pseudomallei_ isolates  _._ However, growing concerns over possible underestimation of the disease in potentially endemic areas, including in tropical Africa, have led to calls for improved surveillance .\n\n【2】In 2010, at Kilifi County Hospital (KCH), we isolated _B. pseudomallei_ from the blood culture of a 3-year-old child after a near-drowning accident in a seasonal river. The identity of the isolate was confirmed by real-time PCR targeting the type III secretion system genes of _B. pseudomallei_ , and the isolate was later sequenced for a study of geographic dissemination of _B. pseudomallei_ . After this identification, we conducted a retrospective analysis of archived blood culture isolates collected during 1994–2012 to investigate possible missed cases of invasive _B. pseudomallei_ infection.\n\n【3】### The Study\n\n【4】During 1994–1998, blood culture was performed on all febrile patients admitted to the pediatric wards at KCH. Since 1998, all pediatric patients <15 years of age admitted, except those having trauma, burns, or elective surgery, have had blood samples drawn for culture. Surveillance for patients \\> 15 years of age began in 2007; blood samples are drawn at admission for cultures on patients meeting clinical criteria for possible invasive bacterial disease. Since 2002, hospitalization events have been linked to the Kilifi Health and Demographic Surveillance System (KHDSS), which monitors the population of ≈280,000 over an area of 891 km 2  . Informed consent is obtained from all patients participating in the surveillance, including for storage of isolates and future use of clinical data.\n\n【5】Blood samples for bacterial cultures were collected in BACTEC Peds Plus or BACTEC Plus Aerobic/F bottles  and incubated on a BACTEC FX 9050 Automated Blood Culture instrument (Becton Dickinson). Nonfastidious, oxidase-positive, gram-negative bacilli were identified by using API 20NE test kits . We reviewed all gentamicin-resistant, glucose-nonfermenting, gram-negative rods, with the exception of _Pseudomonas aeruginosa_ , even if the API 20NE identification was acceptable, to account for difficulties in speciating _Burkholderia_ spp. with biochemical methods.\n\n【6】A total of 86,582 patients <15 years of age were admitted during 1994–2012 and 18,864 patients ≥15 years of age during 2007–2012. Surveillance identified 33 gentamicin-resistant, glucose-nonfermenting bacilli in 14,235 positive blood cultures from patients <15 years of age and 5 gentamicin-resistant, glucose-nonfermenting bacilli in 705 positive blood cultures from patients ≥15 years of age . We retrieved all 38 isolates from storage for PCR, which we performed using published primer and probe sequences .\n\n【7】We identified 4 isolates as _B. pseudomallei_ by PCR, including the index isolate from 2010 . One isolate was previously identified as _B_ . _cepacia,_ and 2 were previously labeled as _Pseudomonas_ species. We identified a fifth _B. pseudomallei_ case in July 2014 in a 68-year-old female patient with diabetes mellitus and bilateral cervical abscesses . Blood culture results were negative, but aspirated pus grew _B. pseudomallei_ , identified by API 20NE and confirmed by PCR.\n\n【8】None of the case-patients had any history of travel outside Kilifi County. Three died during the course of their admission. No further information is available for the 2 case-patients who survived because they were not residents of the area surveyed by KHDSS.\n\n【9】To estimate the incidence of melioidosis bloodstream infection, we divided the number of invasive _B. pseudomallei_ cases among KHDSS residents by the sum of the annual midyear population counts during 2002–2012 for those <15 years of age and during 2007–2012 for those \\> 15 years of age. We also adjusted for the sensitivity of the surveillance to account for the proportion of patients not consenting to the surveillance study and those who did not have a blood culture drawn. For the period before 2002, we extrapolated age-specific population estimates by using a log-linear model of age-specific population data based on subsequent enumerations. The estimated incidence was 1.3 cases/1 million person-years of observation for those <15 years of age and 2 cases/1 million person-years of observation for those ≥15 years of age .\n\n【10】### Conclusions\n\n【11】We identified 5 cases of melioidosis from a single surveillance site in Kenya. Despite reports suggesting that melioidosis is endemic but underdetected in the region , we demonstrated low incidence in this part of Kenya. Even so, _B. pseudomallei_ has emerged as an underdiagnosed cause of sepsis in Kilifi County. The empirical treatment used for sepsis, ampicillin and gentamicin, does not cover _B. pseudomallei_ . The lack of pathognomonic clinical features makes it difficult to detect melioidosis clinically, especially in areas to which the disease is not endemic. In the series we report, 2 case-patients died before receiving definitive treatment, and only 1 case-patient received antimicrobial drugs recommended to treat melioidosis.\n\n【12】The integrated, population-based bacterial surveillance system in Kilifi County provides a unique opportunity to estimate incidence. Routine blood culture sampling of all admitted patients <15 years of age and eligible patients \\> 15 years of age eliminates reliance on clinical suspicion for bacteremic melioidosis. The use of molecular methods on isolates suspected to be _B. pseudomallei_ will probably enhance case detection because _B. pseudomallei_ is commonly misidentified or unidentified by culture . Only 2 isolates in our study were identified by using standard techniques, despite the reported good discriminatory performance of API 20NE in distinguishing _B. pseudomallei_ and _B. cepacia_ .\n\n【13】Our reported incidence rates might still be underestimated. Our data do not account for KHDSS residents who do not go to KCH. For example, ≈64% of deaths in children <5 years of age in the KHDSS area occur at home or in other healthcare facilities . Furthermore, as demonstrated by the fifth case, the incidence of nonbacteremic infection might be higher because non–blood culture samples are not systematically collected. Only 50%–75% of patients with melioidosis are bacteremic , and culture has an estimated sensitivity of 60.2% for melioidosis . In addition, our screening method excluded gentamicin-susceptible isolates. If gentamicin-susceptible _B. pseudomallei_ is as common in Kenya as reported in other areas , additional surveillance that includes these organisms could increase the reported incidence rates. Finally, melioidosis often is unevenly distributed within endemic areas, as noted in Thailand . Despite these factors, our results suggest that, although _B. pseudomallei_ is present in tropical Africa, the incidence of invasive melioidosis is surprisingly low.\n\n【14】The differences in disease incidence in Africa and Asia are striking. Host factors, such as diabetes mellitus, might contribute, but environmental factors and agricultural practices, such as rice farming, are probably more important in permitting exposure to and environmental persistence and proliferation of the organism. Nonetheless, Kenya has been identified as environmentally suitable for _B. pseudomallei_ because of its soil type, agricultural practices, and rainfall . Our study demonstrates the presence of _B. pseudomallei_ in Kenya. Changes in climate and agricultural practices might lead to future increases in melioidosis, and ongoing surveillance is necessary.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8532fec7-35bf-4174-9e61-aac2844cd73f", "title": "Using PCD’s First-Ever External Review to Enhance the Journal’s Worldwide Usefulness to Researchers, Practitioners, and Policy Makers", "text": "【0】Using PCD’s First-Ever External Review to Enhance the Journal’s Worldwide Usefulness to Researchers, Practitioners, and Policy Makers\n*   Refining PCD's Vision and Mission Statement\n*   Focusing on Topics Areas of Greatest Interest\n*   Revisiting Article Types\n*   Securing Scientific and Programmatic Expertise\n*   Complementing Our Work on Epidemiological Studies With Increased Attention to Evaluating Population-Based Interventions and Policies\n*   Providing Transparent Information on the Journal's Impact\n*   Conclusion\n*   Author Information\n\n【1】Leonard Jack Jr, PhD, MSc\n\n【2】_Preventing Chronic Disease_ (PCD_ ) was established in 2004 by the Centers for Disease Control and Prevention (CDC) to enhance the science base on effective public health approaches to prevent and control chronic disease. After 14 years of progress, _PCD_ conducted its first-ever external review to identify ways for the journal to continue to enhance its usefulness for its audience of researchers, practitioners, and policy makers. In June 2017, _PCD_ invited a panel of 7 nationally recognized experts in scientific publishing  to respond to key questions about _PCD_ ’s mission, quality of scientific content, scope of operation, intended audience, and future direction.\n\n【3】The panel’s overall assessment of _PCD_ was that it is well-positioned to continue its trajectory of growth and success. In particular, the panel complimented the journal’s leadership, innovation in scientific publishing, increase in national visibility, and superior customer service provided to its editorial board, associate editors, authors, readers, and peer reviewers. On the basis of the panel’s recommendations and in consultation with the journal’s editorial board, associate editors, and National Center for Chronic Disease Prevention and Health Promotion (NCCDPHP) leadership, I am pleased to share these enhancements to the journal.\n\n【4】Refining PCD’s Vision and Mission Statement\n-------------------------------------------\n\n【5】_PCD_ was encouraged by the external review panel to refine its vision and mission statement. _PCD_ will maintain the journal’s focus on providing current, top-quality content to public health practitioners, researchers, and policy makers. _PCD_ vision and mission statements now clearly reflect the journal’s commitment to disseminating respected content worldwide.\n\n【6】**Vision.** _PCD_ will serve as an influential journal in the dissemination of proven and promising public health findings, innovations, and practices with editorial content respected for its integrity and relevance to chronic disease prevention.\n\n【7】**Mission statement.** The mission of _PCD_ is to promote dialogue among researchers, practitioners, and policy makers worldwide on the integration and application of research findings and practical experience to improve population health.\n\n【8】Focusing on Topics Areas of Greatest Interest\n---------------------------------------------\n\n【9】_PCD_ has been in existence long enough to better focus the primary topic areas of greatest interest to the journal. _PCD_ has refined its primary focus to 4 main areas:\n\n【10】*   Development, implementation, and evaluation of population-based interventions to prevent chronic diseases and control their effect on quality of life, illness, and death.\n\n【11】*   Behavioral, psychological, genetic, environmental, biological, and social factors that influence health.\n\n【12】*   Interventions that reduce the disproportionate incidence of chronic diseases among at-risk populations.\n\n【13】*   Development, implementation, and evaluation of public health law and health-policy–driven interventions.\n\n【14】These 4 areas of interest represent areas in which the journal now seeks to expand its content. On its About the Journal webpage, _PCD_ will provide examples of the types of articles and content of greatest interest to the journal under each of these topic areas. Submissions to _PCD_ that merely describe programs, theoretical frameworks, or research or evaluation methods without providing findings supported through sound research or evaluation will not be of primary interest to the journal. In addition, submissions that focus solely on describing partnerships, collaborations, and coalition-building efforts will not be of interest to the journal.\n\n【15】Revisiting Article Types\n------------------------\n\n【16】Moving forward, _PCD_ will strongly encourage the submission of manuscripts that align with the journal’s revised mission and areas of interest. Doing so will put the journal’s editorial resources to best use. Given that _PCD_ has not received a meaningful number of book reviews for consideration, the Book Review article type has been eliminated. _PCD_ also will eliminate the Special Topics and Community Case Study article types. In their place, _PCD_ will introduce a new article type, Program Evaluation Brief. Program Evaluation Briefs will allow authors to share promising preliminary data and findings based on the use of sound evaluation methods and approaches. An important goal of this new article type is to encourage more submissions from organizations and institutions (eg, state and local health departments, community-based organizations) with findings from well-delivered and evaluated public health programs. _PCD_ also has renamed the Tools and Techniques article type to Tools for Public Health Practice. _PCD_ will keep the following article types: Original Research, Research Brief, Systematic Review, Implementation Evaluation, Essay, and Letter to the Editor. _PCD_ will place greater emphasis on publishing Program Evaluation Brief, Implementation Evaluation, Original Research, Research Brief, and Systematic Review article types. _PCD_ will maintain the highest ethical standards in scientific publishing to promote a transparent review and decision-making process for all manuscripts submitted to the journal.\n\n【17】Securing Scientific and Programmatic Expertise\n----------------------------------------------\n\n【18】To emphasize _PCD_ ’s commitment to publishing quality articles from around the world, the journal will add the following statement to the journal’s About the Journal statement: “ _Preventing Chronic Disease_ (PCD_ ) is a peer-reviewed public health journal sponsored by the Centers for Disease Control and Prevention and authored by experts worldwide.” Although supported by CDC, the journal maintains its commitment to a broad representation of public health professionals on its editorial board; of _PCD_ ’s 23 editorial board members, only one is a CDC employee. Over the past year, _PCD_ has increased the number of associate editors to improve access to specific content areas. _PCD_ currently has 16 associate editors, 9 who are external to CDC and 7 who are CDC employees. The names, titles, affiliations, backgrounds, and appointment terms are available on the journal’s website . Term limits for all editorial board members and associate editors were established in 2016. We continue to identify new talent and improve succession planning to ensure the journal secures and maintains the necessary expertise.\n\n【19】Complementing Our Work on Epidemiological Studies With Increased Attention to Evaluating Population-Based Interventions and Policies\n------------------------------------------------------------------------------------------------------------------------------------\n\n【20】_PCD_ has developed an international reputation as an authoritative resource, publishing the latest information on the epidemiological effects of behavioral, psychological, genetic, environmental, biological, and social factors that influence health. _PCD_ will continue to serve as primary resource to the world in this area, and we will now increase our focus on disseminating articles that report findings beyond epidemiological studies. _PCD_ will emphasize identifying and securing articles from researchers and practitioners working in settings that improve health by using population-based interventions and policies. Researchers and practitioners are encouraged to submit Original Research, Implementation Evaluation, and Program Evaluation Brief articles to the journal for consideration.\n\n【21】Providing Transparent Information on the Journal’s Impact\n---------------------------------------------------------\n\n【22】_PCD_ will provide transparent information on its website that describes and reports measures used by the journal to determine the quality of the journal’s content and the journal’s global reach. Multiple metrics will be posted in the journal’s annual _Year in Review_ to provide readers with a sense of the journal’s relevance, resonance, reach, routine, and recognition:\n\n【23】1.  Relevance: Measure of publication impact by examining the number of _PCD_ citations.\n2.  Resonance: Measures of sharing activity that generates attention to create awareness and dissemination. This will include “likes,” bookmarks, and media coverage (Altmetric).\n3.  Reach: Measures that help examine how far information can travel to help determine popularity, affinity, and potential impact. _PCD_ will track the number of views, downloads, and visitors to its website, and for all published articles.\n4.  Routine: Measures that provide insight on processes that ensure publication content aligns with the journal’s mission and scope and with the highest publication standards. _PCD_ will report on the number of submissions, rejection rate, acceptance rate, and turnaround times in its annual _Year in Review_ .\n5.  Recognition: Measures that rely on citation history to determine the journal’s standing in scholarly literature using various systematic approaches. _PCD_ will use impact factor, Scopus, and Google Scholar to help determine the journal’s recognition.\n\n【24】Conclusion\n----------\n\n【25】_PCD_ ’s commitment to scientific quality and integrity, service to the public, and technological innovation over the past 13 years have led to its respected place in the field of public health. The external review panel was an initiative to continue this success and advance the journal. NCCDPHP’s leadership, along with _PCD_ ’s editorial board, associate editors, and staff remain committed to enhancing the journal’s focus, reach, and visibility. Its revised vision, mission statement, and areas of focus better emphasize the journal’s commitment to advancing the intersection of research, practice, and policy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5490ebd-97df-400f-b29a-930c473b7ccf", "title": "Methicillin-Resistant and -Susceptible Staphylococcus aureus Sequence Type 398 in Pigs and Humans", "text": "【0】Methicillin-Resistant and -Susceptible Staphylococcus aureus Sequence Type 398 in Pigs and Humans\nNasal _Staphylococcus aureus_ carriage has increased in pig farmers, and specific lineages of _S. aureus_ are shared by farmers and their animals . In addition, rates of nasal carriage of methicillin-resistant _S. aureus_ (MRSA) by veterinary personnel working with pigs is high . The pig-related MRSA appears to be clonal and was identified by multilocus sequence typing (MLST) as sequence type 398 . Such resistant bacterial strains can spread from animals to the environment, which may facilitate the colonization of persons who are not involved in animal husbandry . The porcine MRSA strain has been isolated from humans with invasive and superficial infections, and familial outbreaks of colonization and cross-colonization have been documented .\n\n【1】We sought to determine whether the clinical effect of the porcine ST398 MRSA strain can be substantiated by the existence of genetically homologous, methicillin-susceptible _S. aureus_ (MSSA) strains among healthy or infected persons. The international MLST database   listed only 1 ST398 MSSA nasal carriage isolate from a patient in Cape Verde. In addition, 1 ST398 MRSA strain was isolated from a woman living in Groningen, the Netherlands, without further clinical and epidemiologic data available. ST398 MSSA nasal carriage isolates were also identified in several pig farmers in a study by Armand-Lefevre et al. We describe the population genetic analysis of Dutch community-based and nosocomial MSSA isolates in comparison with pig- and pig farmer–derived ST398 MRSA isolates, performed by _spa_ \\-sequencing and amplified fragment length polymorphism (AFLP) analysis .\n\n【2】### The Study\n\n【3】Most of the ST398 MRSA strains studied were collected at the Dutch Institute for Public Health and the Environment (RIVM, Bilthoven, the Netherlands). A total of 20 strains were isolated from the nares of pigs in several slaughterhouses (RIVM 21–40) , whereas 18 additional strains were detected during in-hospital screenings for MRSA carriage among Dutch farmers from independent farms (RIVM 1–8, 10–12 and 14–20). In addition, 8 clinical and carriage isolates were obtained from the Veterinary Medical Diagnostic Centre in Utrecht .\n\n【4】Amplified fragment length polymorphism (AFLP) analysis was performed as described previously . A total of 147 marker fragments per strain were scored, and a binary table with marker absence  or presence  was constructed. A total of 30 fragments with differential occurrence, when genetically heterogeneous MSSA and ST398 MRSA fingerprints were compared, were reamplified and sequenced (Applied Biosystems, Foster City, CA, USA). Fragments were sequenced for 3 independent strains, and the consensus was analyzed by using BLAST . Typing of the staphylococcal chromosome cassette (SCC _mec_ ) and the presence of the Panton-Valentine leukocidin (PVL) genes was performed by PCR.\n\n【5】We embedded the genetic fingerprints of the 46 pig-related MRSA isolates in the population structure of _S. aureus_ as obtained before . These studies include high throughput AFLP fingerprints of 829 nonclinical _S. aureus_ human carriage isolates and 146 and 77 (including 2 MRSA isolates) clinical isolates of human and animal origin, respectively. All carriage strains were isolated from volunteers living in the Rotterdam region, where pig farms are absent.\n\n【6】Sequencing of the repetitive region of the protein A gene _spa_ was performed for all ST398 MRSA isolates . Data were analyzed by using the Ridom Staphtype software version 1.4 .\n\n【7】Analysis of the AFLP data was performed as described previously . Both hierarchical cluster analysis and principle component analysis were performed with Spotfire DecisionSite 7.2 software . We used the Fisher exact test to compare the distribution of strain categories in different phylogenetic lineages. A 2-sided p value <0.05 was considered significant.\n\n【8】The AFLP analysis of the ST398 MRSA strains derived from human and animal sources (n = 46) indicated that these strains are highly clonal. When the AFLP patterns for the ST398 strains were included in the overall population analysis for Dutch MSSA strains from carriage and infection, the distinct cluster was still observed . Few Dutch MSSA strains from the Rotterdam region coclustered with the ST398 pig-related MRSA isolates . In total, 6 (.6%) MSSA isolates coclustered with the ST398 MRSA isolates, of which 2 were nasal carriage isolates from healthy persons . Of the 6 strains, 3 were blood culture isolates taken from 3 elderly patients. All 3 patients had nosocomial bacteremia: 1 after inflammatory aneurism of the aorta, 1 during Fournier gangrene, and the last 1 after primary ventricular fibrillation. Epidemiologic data exclude a cluster of nosocomial infections; patients were not in direct contact (data not shown).\n\n【9】After principle component analysis , the ST398 MRSA strains still clustered as a separate group . The AFLP analysis did not distinguish strains from pigs or pig farmers, and only a limited number of polymorphic AFLP fragments were seen. AFLP markers that were positive for the ST398 MRSAs and absent from the other strains, or vice versa, were sequenced. Of 30 fragments analyzed, 9 were ≈100% specific for the pig-associated strains. Another 3 fragments were present in a subset of the pig-associated strains only. Of these 12 fragments, 4 were not homologous with current entries in the GenBank database, including the 10 _S. aureus_ full-genome sequences. Of the 12 pig-specific markers, 8 were homologous with known sequences, which suggests that these markers become pig-specific by point mutations in the AFLP primer annealing site(s) rather than by genomic rearrangement. Several of the sequences encode factors were associated with membranes or transport.\n\n【10】The preponderance of types t011 and t108 was confirmed by _spa_ sequencing . These made up >75% of all cases. However, the other types all belonged to the same family of _spa_ types, which suggests recent drift in the sequence motifs. The t011 types are primarily associated with SCC _mec_ IV and IVa, whereas the t108 type is nearly fully associated with SCC _mec_ V. This finding suggests that ST398 MRSA has arisen independently on at least 2 occasions. Finally, SCC _mec_ III is found in association with t108, t898, t567, t034, and t571. This finding suggests promiscuous dissemination of this cassette among the ST398 MRSA. Strain RIVM-17 harbored the PVL genes. Apparently, the bacteriophage carrying these genes found its way into the porcine ST398 MRSA lineage.\n\n【11】### Conclusions\n\n【12】The massive colonization of Dutch pigs with a single sequence type of MRSA was unexpected . Molecular strain typing was initially compromised because PFGE failed . _Spa_ gene sequencing  showed heterogeneity in the ST398 MRSA lineage with types t011 and t108, which are closely related, covering >75% of all isolates. Hence, 1 or 2 new MRSA lineages had been discovered. We found a degree of genetic association between _spa_ types and the presence of certain SCC _mec_ cassettes, which suggests bacterial evolution and horizontal DNA exchange in the zoonotic reservoir.\n\n【13】We found that ST398 is rare among Dutch MSSA strains colonizing healthy persons ([0.2%\\] of 829 strains). However, a relatively high number of MSSA isolates homologous to the ST398 MRSA were derived from bacteremic patients ([2.1%\\] of 146; p = 0.026). These 3 bacteremia isolates were not related epidemiologically; they were isolated from different patients in different medical departments over an extended period. This finding suggests that these MSSA strains are quite virulent. The strict segregation of ST398 strains  corroborates that the strains belong to a separate biotype associated with pigs .\n\n【14】Our findings pose a warning to public health surveillance: if the ST398 MSSA virulence toward humans would be maintained within the ST398 MRSA lineage from pigs, care should be taken not to introduce this strain into humans. We consider it to be likely that ST398 MRSA from pigs is capable of causing serious infection in humans even though its primary host seems to be pigs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "56a0b5ef-ac7f-42ee-b128-3c77637ea13b", "title": "Buruli Ulcer, Central African Republic", "text": "【0】Buruli Ulcer, Central African Republic\n**To the Editor:** Buruli ulcer, the third most common mycobacterial disease of humans after tuberculosis and leprosy, is an important disfiguring and disabling cutaneous infection disease caused by _Mycobacterium ulcerans_ . Buruli ulcer was declared an emerging skin disease of public health concern by the World Health Organization (WHO) in 1998. Although the disease is known to be associated with swampy areas and environmental changes, the mode of transmission is not yet clearly understood. A possible role for water bugs in the transmission has been postulated in the last 10 years. In this direction, several researchers have proposed that biting water bugs could be vectors for _M. ulcerans_ . _M. ulcerans_ produces a potent toxin known as mycolactone , which lyses dermal cells, leading to the development of continuously expanding ulcers with undermined edges. Surgery is the only treatment for late lesions, which involves excision of necrotic tissues, followed by skin grafting. After such treatment, patients suffer from functional limitations, social stigmatization, and the loss of livelihood . Antimicrobial drug treatment is available (a combination of rifampin and streptomycin), but it is effective only for early lesions .\n\n【1】The disease is endemic in rural wetlands of tropical countries of Africa, the Americas, and Asia. Over the past decade, the prevalence of Buruli ulcer was highest in western Africa , with an alarming increase in detected cases. In central Africa, foci of Buruli ulcer have been reported in Gabon, Equatorial Guinea, Cameroon, Congo, the Democratic Republic of Congo, and Sudan , which are all neighboring countries of the Central African Republic (CAR). Surprisingly, in CAR, no cases of Buruli ulcer have been reported so far, even though its presence in this country was suspected in 2006, although not confirmed. This situation motivated us to begin a passive survey in the hospitals of Bangui, the capital of CAR. We report here 2 confirmed cases of Buruli ulcer that were found through this survey. The 2 patients were admitted in April 2007 to Hôpital de l’Amitié, Bangui, CAR, with extensive skin ulcers, which might correspond to Buruli ulcer according to WHO guidelines . Both patients were farmers from the Ombella M’poko region. They lived on the border of the M’poko River and carried out daily activities in an aquatic environment.\n\n【2】The first patient was a 62-year-old man who had a large ulceration of the right limb . Differential diagnosis eliminated other ulcerative diseases such as drepanocytosis, and the patient was HIV negative. For bacteriologic diagnosis, 4 samples were taken with sterile cotton swabs from beneath the undermined edges of the ulcer. _Proteus mirabilis_ was isolated from the lesion, and a few acid-fast bacilli were shown by Ziehl-Neelsen (ZN) staining. Unfortunately, 1 week later, the patient died of an unknown cause.\n\n【3】The second patient was a man of the same age who had an ulceration 6.5 cm in diameter on the left ankle . His condition had been treated with various antimicrobial agents without any result. Blood testing showed minor anemia (hemoglobin 12.4 g/dL) and that the patient was HIV negative. Bacteriologic analysis found no gram-positive and gram-negative bacteria, and ZN staining showed the presence of acid-fast bacilli. He received the specific recommended treatment for _M. ulcerans_ infection (antimicrobial drug regimen: rifampin, 10 mg/kg, and streptomycin, 15 mg/kg), and the lesions had receded 2 months later .\n\n【4】The identification of _M. ulcerans_ was confirmed by PCR on the basis of the IS _2404_ repeated insertion sequences of _M. ulcerans_ as described by Stinear et al. The positive results were confirmed by quantitative real-time PCR, in the Laboratory of Bacteriology at Central Hospitalier Universistaire, Angers, France, on 2 specific sequences: IS _2404_ sequence and ketoreductase B domain of the mycolactone polyketide synthase gene from the plasmid pMUM001 .\n\n【5】According to WHO criteria, 2 confirmative test results should be obtained of 4 laboratory tests (ZN staining, positive culture of _M.ulcerans_ , specific gene amplification, pathognomonic histopathologic features) to establish a definitive diagnosis . Concerning the 2 patients in this study, results of ZN staining and PCR were positive, thus confirming the diagnosis of Buruli ulcer. Samples were inoculated on Löwenstein-Jensen (LJ) media and incubated at 30°C for 2 months, but the culture did not grow the organism. This result could be accounted for by the paucity of bacilli in the samples. In conclusion, our study confirms that, although infrequently diagnosed, Buruli ulcer is an endemic disease in CAR.\n\n【6】Identification and control of Buruli ulcer remain difficult in CAR, where this disease is often not considered. Even with evocative clinical signs, confirmation of diagnosis by biological analysis is still not easy. It is therefore of high importance that the public health authorities are fully informed and properly trained to identify this neglected disease in the early stages so patients can be cured before the onset of functional impairment and the appearance of extensive lesions. Further investigation to isolate strains present in CAR is also essential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9c5e83fe-95f3-4f6b-a534-d86fcaca7110", "title": "Contagious: Cultures, Carriers, and the Outbreak Narrative", "text": "【0】Contagious: Cultures, Carriers, and the Outbreak Narrative\nThe outbreak narrative, which tells the evolving story of disease emergence, is the central theme of this book by Priscilla Wald, an English professor at Duke University. Wald discusses and challenges outbreak narratives that use a formulaic plot line: identification of an emerging infection, discussion of global networks through which diseases travel, and a chronicle of the epidemiologic work that results in disease containment. The expectation created by that formula, Wald contends, has at times hampered our ability to address such emerging diseases as HIV infection because we distort or exaggerate the story to fit the formula. Possibly more important, she claims, is the development of the outbreak narrative in the late 19th century, which coincided with emergence of urban sociology and the concept of social contagion. During this time, circulation of ideas and attitudes turned individuals into social groups and cultures, and terms such as contagion and infection entered lay language.\n\n【1】Wald describes Mary Mallon, “Typhoid Mary,” as the prototypical healthy carrier whose outbreak narrative led to a new way of thinking about social relationships and social responsibility. Her status as an immigrant contributed to society’s blaming and stigmatizing immigrants because of their association with communicable disease (especially venereal disease), a stigma reanimated in the 1980s with the emergence of HIV and its most infamous carrier, Patient Zero.\n\n【2】Wald’s analysis of disease emergence also notes its simultaneous evolution with social, religious, and political changes. The viral metaphor was applied to biologic warfare in the 1950s and expanded to describe political contagion. According to J. Edgar Hoover, “the bloody virus of communism” was spread by agents (carriers) trying to “infiltrate and colonize this country.”\n\n【3】The microbe is the worthy foe in epidemiologists’ stories, which can be seen as analogous to good detective stories that have happy endings and draw attention to urgent problems. However, Wald warns, what makes the story appealing can distort articulation of the problem. The danger, she believes, lies in the storytelling, not in the epidemiology or laboratory science. Wald also notes that epidemiology’s shift away from infectious diseases beginning in the 1960s removed the heroic edge from the field. She writes, “Sociology did not make for risk, exciting disease detectives.” Wald is wary of the limitations of the outbreak narrative and advocates a model for global health based on social justice.\n\n【4】The book is academic in presentation, and Wald’s conclusions are not always compelling. For example, her discussion of epidemiologic horror in books and movies such as Invasion of the Body Snatchers will intrigue some readers and put off others who might find this analysis extreme and detracting from the serious tone of the rest of the book. Still, the thesis involving the outbreak narrative is interesting, and readers intrigued by the influence of epidemiology on emerging infections as well as its broader implications to society and the world will find this book worthwhile.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5db41692-1db1-415f-88c5-6e4f7edf35ea", "title": "New Dengue Virus Type 1 Genotype in Colombo, Sri Lanka", "text": "【0】New Dengue Virus Type 1 Genotype in Colombo, Sri Lanka\nDengue virus (DENV) is a flavivirus transmitted by _Aedes_ spp. mosquitoes. There are 4 distinct DENV serotypes (DENV-1–4). Infection with a single serotype leads to long-term protective immunity against the homologous serotype but not against other serotypes . Globally, dengue is an emerging disease that causes an estimated 50–100 million infections, 500,000 dengue hemorrhagic fever (DHF) cases, and 22,000 deaths annually .\n\n【1】Epidemiologic and other studies indicate that risk factors for severe dengue include secondary infection with a heterologous serotype, the strain of infecting virus, and age and genetic background of the host. Studies are under way to further explore the role of these factors in severe disease .\n\n【2】In Sri Lanka, serologically confirmed dengue was first reported in 1962 , but although all 4 virus serotypes were present and there were cases of DHF, only since 1989 has DHF been considered endemic to Sri Lanka . Dengue was made a reportable disease in Sri Lanka in 1996, and the largest epidemic (reported cases, 170 cases/100,000 population, and 346 deaths) occurred in 2009 . DHF epidemics in 1989 and 2002–2004 were associated with emergence of new clades of DENV-3 . We report a new DENV-1 genotype introduced to Sri Lanka before the 2009 epidemic.\n\n【3】### The Study\n\n【4】The study was approved by the Ethical Review Committee of the Faculty of Medicine, University of Colombo, Sri Lanka, and the Institutional Research Board of the International Vaccine Institute, Seoul, South Korea. Serum samples were obtained in 2009 and early 2010 from patients as part of a Pediatric Dengue Vaccine Initiative (PDVI) fever surveillance study in Colombo, Sri Lanka. Samples were originally tested for dengue by reverse transcription PCR at Genetech Research Institute (Colombo, Sri Lanka). A random subset of dengue-positive samples of all 4 serotypes was sent to the Program in Emerging Infectious Diseases Laboratory at Duke–National University of Singapore Graduate Medical School, Singapore, for virus isolation and sequencing.\n\n【5】RNA was extracted from virus isolates, subjected to standard reverse transcription PCR to confirm the presence of dengue virus, and serotyped as described . Samples processed at Duke–National University of Singapore underwent whole-genome sequencing as described . Using DENV-1 isolates from Sri Lanka obtained from dengue cases in 1983, 1984, 1997, 2003, and 2004  and representative DENV-1 sequences for the 4 genotypes, we constructed a phylogenetic tree by using MEGA5 software  .\n\n【6】The 4 DENV serotypes found in Sri Lanka have been classified into genotypes according to the nomenclature described by Rico-Hesse . The earliest isolates found in 1983 and 1984 belong to South Pacific genotype III. More recent isolates obtained during surveillance efforts during 1997–2004 belong to Africa/America genotype IV, indicating that at some point between the early 1980s and the mid 1990s, there was a DENV-1 genotype shift. Analysis of viruses isolated in 2009 indicated that another Asia genotype I of DENV-1 has been introduced into Sri Lanka  . This Asia genotype I virus appears to be responsible for the 2009 epidemic of dengue fever and DHF.\n\n【7】### Conclusions\n\n【8】A feature of the epidemiology of dengue in Sri Lanka was the lack of DHF in the early 1980s and the increase in the number of severe dengue cases since 1989, more so after 2000. This finding was observed despite seroprevalence rates remaining largely the same over time as reported in a previous study  and in the current PDVI study .\n\n【9】Previous epidemics (and 2002–2004) showed a correlation with evolution of DENV-3 genotype III in Sri Lanka, where emergence of new clades of DENV-3 genotype 3 showed a correlation with large increases in the number of reported cases and the geographic range of the virus . A similar observation was reported for Puerto Rico by Bennett et al. who compared data for DEN2 and DEN4 over 20 years and found that dominant clades were replaced by viral subpopulations existing within the population  and in the South Pacific region for DENV-2, where a similar clade replacement occurred . These clade changes were accompanied by positive selection in the nonstructural protein 2A (NS-2A) gene for DENV-4 and the envelope, premembrane, NS-2A, and NS-4A genes for DENV-2.\n\n【10】Our results indicate that introduction of a new DENV-1 genotype coincided with the 2009 dengue epidemic in Sri Lanka. Studies are underway to determine if the proportion of DENV-1 cases in 2009 was greater than in previous years and to assess the role of this new DENV-1 genotype in the severe epidemic of 2009. Further studies are needed to determine if this new genotype has spread to other countries in the region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fd6975c4-c5e5-49c0-85e4-6cfbf073e015", "title": "Characterization of Clinical Isolates of Bartonella henselae Strains, South Korea", "text": "【0】Characterization of Clinical Isolates of Bartonella henselae Strains, South Korea\nThe genus _Bartonella_ includes infectious, gram-negative, facultative intracellular bacteria of numerous species. Among the _Bartonella_ species, _B. henselae_ is known as one of the most noteworthy pathogens . _B. henselae_ causes cat-scratch disease, which is a common zoonosis and manifests various clinical symptoms .\n\n【1】A case of _B. henselae_ infection in South Korea was confirmed in 2005 by PCR . Although a few more studies have been published after this case of _B. henselae_ , only 2 cases were culture-proven: 1 from blood and 1 from bone marrow . Because of difficulties in cultivation and isolation, studies of the isolation of _B. henselae_ from clinical specimens remain scarce. In this study, we analyzed the characteristics of the isolated _B. henselae_ strains in South Korea and compared the clinical features of the patients.\n\n【2】### The Study\n\n【3】We conducted the study among patients who visited Inha University Hospital, a tertiary hospital in Incheon, South Korea, during 2009–2016. From these patients, we isolated 5 cases in which _B. henselae_ was identified from cultures of blood or bone marrow .\n\n【4】Case-patient 1 (IIBC1301) was a 22-year-old man hospitalized for left inguinal lymphadenopathy that had started 10 days earlier. His body temperature was 38.5°C, and he had rashes that started on the palms and soles and subsequently spread to his entire body. _B. henselae_ was isolated from the blood that was cultured on the second day of hospitalization.\n\n【5】Case-patient 2 (IIBC1302) was a 40-year-old woman hospitalized for fever and myalgia, symptoms that had lasted for 1 month. The patient had an erythematous papular rash on her face and extremities and tenderness in her abdomen. Computed tomography (CT) of the abdomen showed chronic cholecystitis; therefore, levofloxacin and metronidazole were prescribed . _B. henselae_ was identified from cultures of blood obtained on the first day of the hospitalization. The patient had not raised any animals. After discharge, the patient experienced continuous fever, poor oral intake, and weight loss. Reevaluation showed centrilobular ground-glass opacity in both lung fields on chest CT and growth of _Mycobacterium tuberculosis_ on sputum acid-fast bacilli culture . A pulmonary tuberculosis infection was diagnosed and treated with antituberculosis medication.\n\n【6】Case-patient 3 (IIBC1303) was a 52-year-old woman hospitalized for fever and left flank pain; her symptoms had persisted for 1 month. She also reported right-side neck swelling and pain at neck levels II, III, and VA. _B. henselae_ was isolated from cultures of blood collected on the 16th day of hospitalization. She had no contact with animals.\n\n【7】Case-patient 4 (IIBC1304) was a 42-year-old man we previously reported  whose main complaints were fever, rash, and arthralgia. _B. henselae_ was isolated from a bone marrow sample. The patient had no contact with or experience in raising pets.\n\n【8】Case-patient 5 (IIBC1305), also previously published , was a 73-year-old woman who had _B. henselae_ isolated from her blood. She also did not have any contact with animals.\n\n【9】_Bartonella_ species can be grown by blood agar–based culture systems. However, it is difficult to culture them this way because the growth of bacterial cells is slow, and obtaining colonies on the agar plate takes a long time. On the other hand, _Bartonella_ species grow more rapidly with cell culture–based systems . For testing of these patients, we grew ECV304 cells in M199 media containing 10% heat-inactivated fetal bovine serum and inoculated 1 mL of whole blood or other samples from the patients onto the cells. After 24 hours, we washed the cells with Dulbecco’s phosphate-buffered saline and maintained them in M199 media. We performed an immunofluorescence assay (IFA) with the patient’s own serum (:40 diluted) every week after the inoculation. When the growth of bacteria was observed, we scraped all cultured cells from the T25 flask. We then reinoculated 1 mL of infected ECV304 cells onto uninfected ECV304 cells in a T75 flask for expansion of bacterial cells.\n\n【10】To identify the bacterial isolates, we amplified and sequenced the 16S rRNA gene . The pathogens cultured from the specimens showed the highest sequence similarities with _B. henselae_ Houston-1 strain (GenBank accession nos. KY773227, KY773228, KY773229, KY773290, and KY885188). The similarity was >99%  . IFA results using a commercial _Bartonella_ IFA IgG kit (FOCUS Diagnostics, DiaSorin Molecular, Cypress, CA, USA) also showed positive results for all patients’ serum samples; titers ranged from 1:40 to 1:1,280 . We also performed multilocus sequence typing to determine the genotypes of _B. henselae_ isolates  and found that all isolates belonged to sequence type 1 .\n\n【11】### Conclusions\n\n【12】We cultured _B. henselae_ isolates from clinical samples and compared characteristics of 5 patients: 3 new cases and 2 previously reported cases from which _B. henselae_ was isolated . Because of the diverse manifestations of _B. henselae_ infection, the symptoms were similar to those of other bacterial infections. _B. henselae_ infections in 3 patients were initially misdiagnosed as other diseases: sexually transmitted disease (case-patient 1), enteric fever-like syndrome (case-patient 2), and acute pyelonephritis (case-patient 3). The diagnosis of _B. henselae_ infection was made even more difficult because none of these 5 patients reported a history of raising cats. However, the absence of contact with animals should not preclude infection; even though _B. henselae_ infection is ususally related to cat scratches or bites, it may also occur without animal contacts . It is also noteworthy that the patient described in case 2 was co-infected with pulmonary tuberculosis. Co-infection with _B. henselae_ and _Mycoplasma_ spp. has also been reported in previous studies . Co-infection with other bacteria suggests that infection with _Bartonella_ species may weaken the host’s immune system, leaving the host vulnerable to secondary infections. In addition, these co-infections may cause difficulty in diagnosing _Bartonella_ infection.\n\n【13】Multilocus sequence typing indicated that all isolates from this study belonged to _B. henselae_ sequence type 1. This result is consistent with previous studies, which showed relatively less diversity among human strains than among the feline reservoir .\n\n【14】In summary, the clinical features of _B. henselae_ infection are diverse and nonspecific, which could initially lead to misdiagnosis as other diseases. Physicians and patients should consider that _Bartonella_ infection presents various clinical symptoms and might be a common cause of fever of unknown origin, irrespective of exposure to cats. Once _Bartonella_ infection is suspected, cell culture should be considered to confirm the diagnosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1a916b2a-6499-4fef-bdd2-7ad302c09204", "title": "Avian Influenza Outbreaks in Chickens, Bangladesh", "text": "【0】Avian Influenza Outbreaks in Chickens, Bangladesh\nThe threat that highly pathogenic avian influenza (HPAI) A virus subtype H5N1 poses to poultry and public health has intensified . As the virus becomes established in poultry in developing countries, the number of human cases increases . Countries in the Asian Association for Regional Co-operation are especially vulnerable to virus perpetuation because of insufficient biosecurity, rearing of chickens and ducks together, selling of live birds, and deficient disease surveillance. To prevent human infection with avian influenza (H5N1), knowledge of avian influenza epidemiology is needed. We therefore describe the epidemiology of HPAI outbreaks in chickens in Bangladesh.\n\n【1】### The Study\n\n【2】Through July 10, 2007, we investigated 52 outbreaks caused by HPAI virus (H5N1) and 3 outbreaks caused by low-pathogenicity avian influenza (LPAI) virus (H9N2) in chickens in Bangladesh. After a high number of chicken deaths on a farm was reported to an _upazila_ (a lower administrative unit of Bangladesh) veterinarian, the sick chickens on the farm were examined. From each of 55 outbreaks, 2 dead chickens were sent to a field disease investigation laboratory or to the Central Disease Investigation Laboratory, where oropharyngeal swabs were tested for avian influenza A virus antigen. From cases with positive results, tracheal samples were referred to the National Reference Laboratory for Avian Influenza (NRL-AI) for viral RNA extraction and purification , reverse transcription–PCR that used a primer set of hemagglutinin (H) genes , and end-product visualization. When NRL-AI confirmed H5, the farm was considered HPAI affected and was reported to the Department of Livestock Services. Tracheal samples from chickens involved in 37 outbreaks, including those that were A-antigen positive but H5 negative, were sent to the Veterinary Laboratory Agency in the United Kingdom for confirmation. A farm on which influenza subtype H9N2 was found was considered LPAI affected. All farms affected with HPAI or LPAI virus were called avian influenza–affected farms. A district or _upazila_ with at least 1 avian influenza–affected farm was considered an infected district or infected _upazila._\n\n【3】To collect information about the farms, we used a pretested questionnaire administered by 2 veterinarians. The form had space where veterinarians could add additional comments on the probable virus sources for infections by backward tracing (window < 21 days of onset of clinical signs) and sources of spread by forward tracing (window between onset of clinical signs and culling), which they obtained by interviewing the affected farmers and allied personnel. Farm geographic coordinates were recorded. Through another questionnaire, we collected data on commercial and backyard farms and outbreaks from the _upazila_ livestock offices. All avian influenza data stored at the Department of Livestock Services head office, field disease investigation laboratories, the Central Disease Investigation Laboratory, and NRL-AI were also collected.\n\n【4】Summary statistics were computed and plotted by using Excel (Microsoft, Redmond, WA, USA), Arc View 9.1 (Environmental Systems Research Institute, Redlands, CA, USA), and STATA 7 (Stata Corp. College Station, TX, USA). A 7-day rolling mean of avian influenza–affected farms, according to date of clinical onset of disease, was calculated from January 12, 2007, and plotted as a bar chart on day 4 for each value. Attack rates of farms were calculated separately for _upazilas_ of every infected district .\n\n【5】We found that the H cleavage site of the selected influenza subtype H5N1 isolates (determined from the Veterinary Laboratory Agency) contained polybasic amino acids, which are characteristic of HPAI A viruses . According to categories established by the Food and Agriculture Organization , 5 breeder and 2 layer farms had production system 2, 28 layer farms and 1 broiler farm had system 3, and 20 backyard farms had system 4. The 7-day rolling means of the numbers of avian influenza–affected farms are shown in Figure 1 ; the temporal and spatial spreads, in Figure 2 . The index farm was recorded on January 15, 2007, at a local live bird market in Sarishabari _upazila_ in Jamalpur district. We hypothesized that the infection probably came from chickens in nearby backyard farms because high numbers of deaths in this population went uninvestigated.\n\n【6】The outbreaks peaked on March 26, 2007, when 11 affected farms in 3 districts—Dhaka, Gazipur, and Narayangogj—formed a cluster, indicating a common source. The source may have been larger live bird markets, which probably infected chickens of the 15 districts. The first avian influenza–affected backyard farm was reported on March 22, 2007, the date when avian influenza was confirmed in Bangladesh. Of the 20 backyard farms, 14 were in 7 northern districts.\n\n【7】The overall attack rates for the _upazilas_ of the infected districts were 6/1,000 commercial farms and 1/100,000 backyard farms . Uninvestigated deaths of backyard chickens could result in underestimation of the attack rate.\n\n【8】Among the 9 probable sources of infection, egg trays and contaminated vehicles from larger live bird markets and local live bird markets accounted for 47% of probable virus sources, eggs for 48%, and apparently healthy chickens for 5%. One avian influenza–affected farm disposed of ≈1,000 dead chickens in an open field before diagnosis was confirmed. For the backyard chickens, sources of spread were selling chickens (%), giving chickens to relatives or neighbors (%), moving birds through local poultry vendors, and hiding birds during culling operations (%).\n\n【9】On the index farm, chickens in 1 shed were infected, but chickens in 2 other sheds <40 yards away remained clinically unaffected through the time of culling (days after clinical onset). Although the media reported that a corporate-run poultry farm, Biman Poultry Complex, was the first avian influenza–affected farm in Bangladesh, our investigation found it to be the third. Of the 5 breeder farms, 2 had imported chicks from the United States and 1 from the United States and France, but these chicks had arrived >21 days before clinical onset of HPAI.\n\n【10】On May 22, 2008, the Directorate General of Heath Services, Bangladesh, declared that a sample collected from a child in January 2008 was diagnosed by the US Centers for Disease Control and Prevention as positive for influenza virus (H5N1). Before this time, no human infection with influenza virus (H5N1) had been reported in Bangladesh. Lack of human cases may have resulted from early immunologic response , genetic variation in receptors , poor surveillance of disease in humans, or using antiviral drugs during culling of birds.\n\n【11】### Conclusions\n\n【12】Our investigation showed that the epicenter of the HPAI outbreaks in Bangladesh was the Sarishabari _upazila_ of Jamalpur district and that the primary source of infection was backyard chickens. Phylogenetic analysis on 1 influenza virus (H5N1) isolate showed that it belongs to the subclade 2.2 of the Qinghai lineage , most closely related to viruses isolated from Afghanistan, Mongolia, and Russia . Therefore, the virus might have entered Bangladesh through migratory birds . The presence of influenza virus subtype H9N2 in chickens on 3 farms, however, raises the question of when this virus was introduced to Bangladesh. An earlier introduction or emergence of LPAI virus (H9N2) in backyard chickens cannot be ruled out because ≈18% of backyard chickens tested during 2000–2003 were seropositive for avian influenza virus .\n\n【13】This study illustrates the progression of HPAI in Bangladesh. Further study is needed to provided more evidence for the sources we have identified.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51df05f5-dd2c-4500-ba2f-d4e74aa8db58", "title": "Mycobacterium doricum Osteomyelitis and Soft Tissue Infection", "text": "【0】Mycobacterium doricum Osteomyelitis and Soft Tissue Infection\n**To the Editor:** Infections with nontuberculous mycobacteria (NTM) are being increasingly identified. Several factors may contribute to this finding, including increased awareness of these organisms as pathogens, improved ability of laboratories to isolate and identify these organisms, and increasing prevalence . We describe a case of osteomyelitis and soft tissue infection with _Mycobacterium doricum_ after trauma in a previously healthy adult.\n\n【1】A 21-year-old man sustained an open right femur fracture with gross contamination of the wound with dirt and gravel. The wound was irrigated and debrided, the fracture was fixed by intramedullary nailing, and the wound was closed.\n\n【2】Sixteen weeks later, pain, swelling, and erythema developed in the right thigh of the patient. Radiographs showed incomplete union of the fracture and possible loosening of the nailing hardware. Repeat irrigation and debridement of the right thigh was performed. Thirty milliliters of purulent material was drained. Bone was not visible, and the hardware was not removed or replaced. Gram staining of the purulent material showed 1 polymorphonuclear cell per oil-immersion microscopic field and no bacteria. Routine bacterial cultures were negative. Results of acid-fast staining were also negative. Three operative specimens were tested for acid-fast organisms. The patient was discharged and prescribed a 6-week course of vancomycin, 750 mg intravenously every 8 hours, and ciprofloxacin, 500 mg orally 2×/d.\n\n【3】Four weeks later, all 3 operative cultures grew an acid-fast bacillus, which was identified by DNA sequencing and high-performance liquid chromatography as _M. doricum_ (Centers for Disease Control and Prevention, Atlanta, GA, USA). While in vitro susceptibility testing results were pending , recurrent swelling, erythema, and warmth developed in the patient at the previous injury site. A computed tomography scan showed multiple small abscesses adjacent to the nonunited fracture, an irregular periosteal reaction around the fracture site, and lucency surrounding the medullary rod, suggestive of osteomyelitis .\n\n【4】The patient was treated with irrigation and debridement of the abscesses and exchange of hardware. Two specimens were tested for acid-fast culture, 1 of which grew _M. doricum_ after 3 weeks of incubation. The patient was discharged and empirically treated with amikacin, 1,250 mg/d intravenously for 3 weeks, and levofloxacin, 750 mg/d orally for 3 months, as therapy for infection with _M. doricum_ , pending susceptibility testing results.\n\n【5】In vitro susceptibility results showed susceptibility to all drugs tested: MIC <1 μg/mL for amikacin, 0.25 μg/mL for ciprofloxacin, 4 μg/mL for clarithromycin, 2 μg/mL for ethambutol, <0.12 μg/mL for rifampin, <0.25 μg/mL for rifabutin, <0.5 μg/mL for streptomycin, and <0.12/2.4 μg/mL for trimethoprim/sulfamethoxazole (Associated Regional and University Pathologists, Salt Lake City, UT, USA). The antimicrobial drug regimen was changed to trimethoprim/sulfamethoxazole (mg of trimethoprim and 400 mg of sulfamethoxazole) orally 2×/d, and doxycycline, 100 mg orally 2×/d. After 10 months of therapy, the patient stopped taking these antimicrobial drugs. Six weeks after discontinuation of therapy, he had no signs or symptoms of recurrent infection.\n\n【6】_M. doricum_ was first identified in a cerebrospinal fluid sample from a 50-year-old man with AIDS (CD4+ lymphocyte count 28 cells/mm 3  ). _Cryptococcus neoformans_ was also isolated from the same specimen. This patient was treated for _C. neoformans_ infection with amphotericin B and 5-fluorocytosine but died 6 weeks later, before isolation of _M. doricum_ . The isolate was susceptible to all antimicrobial drugs tested in vitro, including amikacin, azithromycin, clarithromycin, ciprofloxacin, clofazimine, ethambutol, isoniazid, ofloxacin, rifabutin, rifampin, sparfloxacin, and streptomycin .\n\n【7】The American Thoracic Society and Infectious Diseases Society of America have published guidelines on the diagnosis, treatment, and prevention of NTM diseases . However, there are no recommendations for treatment of _M. doricum_ infection. At least 2 antimicrobial drugs are recommended for treating infections with NTM, and surgery is indicated for extensive disease, abscess formation, or contraindications/intolerance to medical therapy. When possible, it is recommended that foreign bodies be removed. For skin and soft tissue infections, \\> 4 months of therapy is recommended, with extension to 6–12 months for cases of bone involvement . In this instance, trimethoprim/sulfamethoxazole and doxycycline were chosen because of availability of oral formulations, in vitro susceptibility testing results, and affordability given the patient’s lack of medical insurance.\n\n【8】Disease in the patient was eradicated by surgical debridement and prolonged antimicrobial drug therapy. We speculate that the organism was introduced from the soil at the time of the open fracture. Given increased awareness of NTM as pathogens and improvement in the ability of laboratories to isolate and identify these organisms from clinical specimens, this _Mycobacterium_ species might be increasingly identified as a cause of disease. Additional reports of treating disease caused by _M. doricum_ will be valuable so that in vitro susceptibility testing can be correlated with clinical responses, eventually enabling development of guidelines for therapy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f5f59476-9552-4651-bd88-49eae0685f06", "title": "Bombali Ebolavirus in Mops condylurus Bats (Molossidae), Mozambique", "text": "【0】Bombali Ebolavirus in Mops condylurus Bats (Molossidae), Mozambique\nSix viruses of the genus _Ebolavirus_ have been documented to date (Zaire, Sudan, Bundibugyo, Taï Forest, Reston, and Bombali), and some have caused outbreaks in Africa, resulting in high human fatality rates. Bombali virus (BOMV) was first identified in free-tailed bats of the family Molossidae, specifically the species _Mops condylurus_ and _Chaerephon pumilus_ , in 2016 in the Bombali District in Sierra Leone . This virus was later detected in _M. condylurus_ bats in Kenya  in 2018 and in Guinea  in 2019 . Human infections have not been documented, including in patients with febrile illness symptoms in areas where BOMV has been found in bats .\n\n【1】We detected BOMV RNA in 3 _M_ . _condylurus_ bats (all female) captured in Mozambique in the southeastern portion of this species’ geographic range . In May 2015, we obtained samples from 54 _M. condylurus_ bats residing in buildings in the Inhassoro District of southeastern Mozambique and from 211 other bats (representing 10 species), mostly from caves . We screened all samples for viruses belonging to the families _Astroviridae_ , _Coronaviridae_ , and _Paramyxoviridae_ . We performed RNA extraction with the QIAamp Viral RNA Mini Kit  and reverse transcription with the ProtoScript II Reverse Transcriptase and Random Primer 6 . We screened complementary DNA with 3 assays targeting the large (L) protein gene of Filoviriridae  and submitted PCR products of the expected size for direct Sanger sequencing . We did not attempt virus isolation in this study. We processed samples in a Biosafety Level 3 laboratory at the University of Reunion (Saint-Denis, Reunion Island, France) and transferred original samples to the Biosafety Level 4 laboratory at Inserm Jean Mérieux (Lyon, France).\n\n【2】To date, BOMV is the only ebolavirus that has been recurrently detected by PCR, across multiple years , and in bat populations located >5,000 km apart . Our study provides support for BOMV in the southern range of where _M_ . _condylurus_ bats are known to reside . Partial sequencing of the L protein gene revealed that the BOMV sequences detected in bats from Mozambique were closely related to those sequences reported in bats from Sierra Leone, Kenya, and Guinea . Although our findings are based on short sequences (bp), this finding could suggest a strong association between BOMV and _M. condylurus_ bats across their geographic range.\n\n【3】BOMV epidemiology in _M_ . _condylurus_ bats is unknown. Seasonal variation of environmental conditions and population structure are important drivers for the transmission dynamics of infectious agents in natural systems . For instance, pulses of Marburg virus, paramyxovirus, and coronavirus shedding have been shown to coincide with a seasonal increase of juveniles in bat populations . Although our study was based on a limited sampling, we detected BOMV only in female bats  and did not find differences between adults and subadults . Previous reports likewise reported BOMV more frequently in female bats  (⁠). All prior studies reported BOMV-positive bats during the month of May  (⁠). Whether these observations reflect a biologic phenomenon remains to be tested. Across their geographic range, female _M_ . _condylurus_ bats usually have 2 birthing periods that occur between September and early May, and some variation in virus shedding can be anticipated with each reproductive cycle, as documented for other bat–virus systems. Longitudinal studies are needed to investigate biologic and ecologic factors involved in the transmission dynamics of BOMV in _M. condylurus_ bats but also to fully assess virus spillover risk to other hosts, including humans.\n\n【4】In Mozambique, neither BOMV nor other species of ebolavirus have been detected in humans, highlighting that our findings should not be considered evidence of a major threat to local communities, but should be instead considered a catalyst for further investigation and surveillance. Additional studies should focus on other Molossidae bats, because BOMV was initially reported in another member of the family, _Chaerephon pumilus_ , and these bats commonly roost in synanthropic settings and therefore generate opportunities for spillover. Indeed, most BOMV-positive bats were captured in day-roost sites in buildings occupied by humans or livestock . Assessing livestock exposure to BOMV also would be prudent, given their key role as intermediate hosts in the emergence of zoonotic viruses.\n\n【5】The discovery of ebolavirus in wild animals raises questions regarding virus spillover and epidemic potential. In addition to the identification of molecular factors involved in the ability of the virus to replicate in human cells, risk assessment should include environmental factors across the local, social, and habitat landscape. Employing a One Health approach (i.e. collaborative, multisectoral, and transdisciplinary) might prevent future outbreaks, promote sustainable development of human communities, and offer protection for bats that play a key functional role in ecosystems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6efed78-ca13-4d05-bcbf-b0f1d898c1a2", "title": "Costing Framework for International Health Regulations (2005)", "text": "【0】Costing Framework for International Health Regulations (2005)\nIn 2005, the member states of the World Health Organization (WHO) recognized the need to overhaul international public health cooperation, and they revised the International Health Regulations (IHR). The IHR  focus on strengthening capabilities for confronting all potential public health emergencies of international concern when and where they occur. The 194 states parties made a commitment to develop core capacities to detect, assess, report, and respond to any public health event that might have international effects, regardless of type or origin of the event. The IHR  also conferred new responsibilities on WHO and the global health community to share resources, information, and expertise to help nations prepare for and respond to public health events .\n\n【1】The WHO checklist and indicators for monitoring progress in the development of IHR core capacities by states parties, also known as the IHR Monitoring Framework, details 8 core capacities plus activities at points of entry that must be developed to fully implement the IHR  . The IHR Monitoring Framework, first published in 2010, also defines country-level indicators within each core capacity. The regulations and the framework describe the core capacities needed for functional implementation of the IHR  but leave flexibility for nations to determine how best to structure and develop these capacities .\n\n【2】The IHR also direct countries to strengthen and integrate existing systems for public health surveillance and response, rather than to create new, vertical programs. Various national approaches to IHR implementation have emerged, depending on factors such as the sophistication of preexisting systems and infrastructure, past and present objectives of health ministries and their external partners, availability of resources, architecture of health systems, and strength of regional commitments to health cooperation and coordination. Examples of the latter are the Integrated Disease Surveillance and Response strategy previously adopted by the WHO Regional Committee for Africa and shared standards developed through a Latin American subregional trade alliance . Two WHO regional offices, Southeast Asia Regional Office (SEARO) and Western Pacific Regional Office (WPRO), collaboratively developed the Asia Pacific Strategy for Emerging Diseases, providing a framework for coordinated approaches to rapid disease detection and public health emergency responses across sectors, countries, and regions .\n\n【3】Even with regional support, achieving the IHR core competencies is challenging for many nations at high risk for epidemic-prone or emerging infectious disease outbreaks and other public health crises. Member states initially agreed to implement IHR  by June 2012, but a substantial proportion will clearly need at least one 2-year extension. Under Article 44 of the IHR, nations agreed to collaborate on developing and maintaining the public health capacities for IHR implementation by providing technical, logistical, and financial assistance to developing nations. The flexibility of the IHR framework, which enables national leaders to interpret the IHR requirements through mechanisms that are sensitive to local and regional contexts, makes it challenging to marshal such assistance effectively. The decision to measure IHR core capacity development in terms of functional outcomes rather than specific activities means that there could be 194 distinct but equally valid national approaches to fulfilling IHR  obligations. Consequently, many nations that could use help with IHR implementation are still in the process of identifying opportunities for cooperative capacity building with external partners, often without information on how much it will cost to implement their national IHR action plans.\n\n【4】We describe steps for estimating the costs of achieving IHR  implementation in countries with different economic climates by first identifying essential inputs. We identified functional pathways for implementing the 8 core capacities and actions at points of entry identified in the WHO 2010 IHR Monitoring Framework, on the basis of current and planned actions in 6 Southeast Asian case-study countries at different levels of economic and health systems development. We used this to develop a representative IHR implementation strategy to serve as a framework for a preliminary estimate of fixed and operating costs associated with developing and sustaining IHR core capacities across an entire public health system.\n\n【5】### Methods\n\n【6】##### Case-Study Countries\n\n【7】To develop an initial costing framework for IHR implementation, we sought case-study countries that could provide examples of field-tested strategies and practices in the 8 IHR core capacities and at points of entry, along with associated costs. On the basis of geographic proximity, recent responses to emerging infectious diseases of public health significance, economic development levels, and accessibility of financial and policy information, we identified 6 case-study countries in Southeast Asia: 1 low-income (Cambodia), 3 lower-middle income (Lao People’s Democratic Republic, Vietnam, and Timor-Leste), and 2 upper-middle income (Malaysia and Thailand) . These 6 countries fall into the SEARO and WPRO areas, which share the Asia Pacific Strategy for Emerging Diseases capacity-building strategy .\n\n【8】##### Core Capacities Matrix\n\n【9】The 2010 WHO IHR Monitoring Framework identified 20 country-level indicators that states parties could use to assess IHR core capacity development . The framework described levels of capability that could be used to evaluate progress toward each indicator, categorizing capabilities as prerequisites/foundational (level <1), inputs and processes (level 1), outputs and outcomes (level 2), and additional (level 3). Framework guidance specified that for all indicators to meet IHR requirements, countries must successfully demonstrate the attributes at levels 1 and 2.\n\n【10】For each country-level indicator, we identified specific activities and resources that could operationally achieve levels 1 and 2 attributes. Identification involved a 2-step process: 1) determining whether a technical standard exists for achieving each country-level indicator and 2) mapping activities and strategies among the case-study countries to the IHR Monitoring Framework.\n\n【11】To identify standards for building and sustaining the 8 core capacities, we reviewed guidance published by WHO and its regional offices, accrediting and professional organizations, and the US Centers for Disease Control and Prevention; consensus recommendations developed by expert working groups; and peer-reviewed publications, supplemented by additional input from subject matter experts in relevant disciplines.\n\n【12】To describe capabilities, activities, tools, and processes identified by decision makers in each case-study country as relevant to IHR core capacities, we reviewed published and unpublished government documents (e.g. legislation; regulations; national strategies; operational and programmatic guidance; training materials; self-assessments; proposed and enacted budgets; and plans for developing, strengthening, or maintaining IHR core capacities, pandemic preparedness, public health or emergency medical preparedness, indicator- and event-based surveillance, and laboratory systems) and materials prepared with or for development partners, technical partners, and nongovernmental stakeholders in each country. We supplemented the literature review through interviews with governmental and nongovernmental stakeholders in case-study countries. To determine requirements for diagnostic testing capabilities, we derived a priority disease list comprised of the endemic, epidemic-prone, and emerging infectious diseases specifically cited as always notifiable by the IHR  Annex 2 reporting algorithm plus those appearing on \\> 3 case-study country priority disease lists.\n\n【13】We mapped the activities and strategies identified through the reviews of technical guidance and case-study country activities to specific country-level indicators in the IHR Monitoring Framework, creating an operating core capacities matrix. To identify the practices and attributes common to some or all case-study countries for each core capacity, we compared these activities and strategies, distilling the strategies and practices into a representative Southeast Asian country, hereafter referred to as Country X, that has achieved levels 1 and 2 under each country-level indicator. Where no clear consensus emerged on strategies or practices, we selected the national approach that most closely resembled international or regional technical standards.\n\n【14】##### Costing Framework\n\n【15】For each activity or capability mapped to a specific country-level indicator in the Country X core capacities matrix, we extrapolated requirements for physical infrastructure (facilities, equipment, utilities), human capabilities (workforce, training, skills, and knowledge), and tools and processes (e.g. diagnostic platforms, materials, reagents, quality control and assurance, reporting systems), building on the foundations of existing public health surveillance costing platforms, such as the Integrated Disease Surveillance and Response SurvCost tool . Because of the integrated nature of the IHR core capacities, some physical infrastructure, human resources, and tools and processes might contribute to multiple core capacities. We sought to prevent overlap by including such elements only 1 time, under the most immediately relevant indicator.\n\n【16】To develop a preliminary cost estimate for developing and sustaining such infrastructure, human capabilities, and tools and process, we used the following: 1) costs calculated by case-study country government actors for procurement or national budgets; 2) estimates derived with or for international partners; 3) the WHO CHOICE (CHO_ osing _I_ nterventions that are _C_ ost _E_ ffective) database as a source of average salaries, per diem and travel compensation, physical infrastructure, and tradables specific to the subregions of SEARO B and WPRO B, into which the case-study countries fall; and 4) commercial price lists and supply schedules. Because the WHO CHOICE dataset expresses average costs in 2005 international dollars (defined as equivalent to $US in 2005 purchasing power parity), we likewise included or adjusted all costs in 2005 $US. We did not attempt to distinguish between the contributions of public, private, or international actors in mapping the surveillance, response, and laboratory systems for Country X. We thus assumed that the total costs of developing and sustaining each activity or capacity would be the same regardless of the payer. We did not attempt to calculate tariffs or other additional fees specific to each country.\n\n【17】##### Assumptions and Limitations\n\n【18】The Country X template represents a composite of demographic, political, and geographic attributes of 6 low- to middle-income case-study countries in 2 WHO sub-regions (SEARO B and WPRO B). All estimates for Country X assume a population of 60 million persons; 64 provinces with 600 functional districts; and 6 designated points of entry with a Ministry of Health responsible for public health surveillance, response, and laboratory capabilities at the national, provincial, district, and community levels .\n\n【19】Among the case-study countries, national strategies for public health surveillance depend heavily on facility-based surveillance. The reliability and timeliness of facilities-based reporting depend on population access to basic health services with trained health workers at peripheral, intermediate, and central levels. Such services are an absolute prerequisite to IHR implementation but are not explicitly included in the IHR  or associated guidance. Any estimates for costs of public health surveillance and response developed through the framework described here should therefore be considered additional to the costs of developing and sustaining adequate essential health services.\n\n【20】### Results\n\n【21】When we identified practices and strategies in 6 case-study Southeast Asian countries, referenced against regional/global technical standards, that could achieve the functional outcomes specified by each country-level indicator in the IHR Monitoring Framework, the resulting core capacities matrix created a detailed template for fully implementing IHR  in a model Southeast Asian country. We present this matrix as a framework for determining the inputs—physical infrastructure, human capabilities, and tools and processes—required to achieve each core capacity at the peripheral, intermediate, and central levels of the Country X template and for estimating the costs associated with these inputs .\n\n【22】##### Core Capacity 1: National Legislation, Policy, and Financing\n\n【23】Country-level indicators focus on adoption of budgetary and regulatory frameworks to support IHR implementation. We identified only 1 input with cost implications—support for legal expertise (domestic or external consultants) to review and, as needed, revise national public health laws, estimated at $75,000 (in 2005 $US)—which was based on past consulting costs for revising national regulations with regard to avian and human influenza.\n\n【24】##### Core Capacity 2: Coordination and National Focal Point Communications\n\n【25】IHR coordination, communications, and advocacy require designation of an IHR National Focal Point and mechanisms to identify, convene, and coordinate stakeholders in public health surveillance and response across sectors. Inputs include information and communications technologies equipment and services as well as office infrastructure, transportation, and salary support for the individual or office serving as National Focal Points .\n\n【26】##### Core Capacity 3: Surveillance\n\n【27】The IHR Monitoring Framework specifies that this core capacity encompasses indicator-based surveillance (the routine reporting of diseases or syndromes that meet specific case definitions) and event-based surveillance (the rapid detection and reporting of unusual or unexpected disease patterns, deaths, and exposure risks) . All case-study countries conduct national indicator-based surveillance for priority diseases and have developed strategies for combining routine surveillance data with reports from other sources to provide early warning of emerging public health events. The resources for detecting, reporting, and managing cases of priority diseases and unusual events overlap substantially in the Country X template, particularly at the community level.\n\n【28】For Country X, we developed a template for surveillance staffing structure based on a combination of health systems structure and population. We did not attempt to prorate the share of office space, utilities, transportation, etc. dedicated to surveillance for epidemic-prone or emerging infections versus other goals (such as tracking nonommunicable conditions or high-risk behavior).\n\n【29】##### Indicator-based Surveillance\n\n【30】The template for surveillance core capacities in Country X includes activities to support indicator-based surveillance at the central, intermediate, and peripheral levels of the national health system and includes additional capabilities for collecting and analyzing urgent reports for event-based surveillance. Figure 2 provides an overview of Country X inputs; Technical Appendix Table 2 provides a more detailed examination of the infrastructure, human capabilities, and tools and resources for supporting these inputs, including estimates intended to illustrate the approximate costs of implementing IHR core capacities according to this template.\n\n【31】The template uses the proposed minimum standard endorsed by the US Centers for Disease Control and Prevention: 1 field-trained epidemiologist per 200,000 population . To identify the full operating costs, the template assumes that Country X has achieved this population-based target (epidemiologists).\n\n【32】For Country X, numerous provinces with populations of ≈1 million persons serve as the hub for surveillance activities at the intermediate level, and districts (with catchment populations of ≈100,000) serve as the central hubs for surveillance activities at the peripheral level. The template for Core Capacity 3 includes dedicated personnel at the intermediate and peripheral levels to compile and report data on priority diseases and unusual events, to use and disseminate data and guidance issued from the national level, and to train local stakeholders. The template assumes that these functions are housed within existing health offices and health care facilities at the provincial and district levels. We developed the staffing model at each level on the basis of case-study country practices considered by interviewed experts to adequately serve case load.\n\n【33】According to experiences in the case-study countries, some mechanism is needed to extend government disease prevention and control programs to the community level. The inputs for the surveillance template include on-site training of community health center staff and for information and communications technologies equipment and services to facilitate the exchange of information between district health offices, basic health facilities, and communities. The template also includes monthly allowances for training, travel, and communications for 1 village or community health worker per 500 population (the approximate median among the case-study countries with established community health worker networks) to extend disease surveillance and prevention efforts to the household level.\n\n【34】##### Event-based Surveillance\n\n【35】We assume that the infrastructure, capabilities, systems, and processes developed for routine or indicator-based surveillance will also serve as the backbone for event-based surveillance, particularly at the community level where community health volunteers are likely to play a role in collecting reports of unusual disease clusters. The Country X template includes inputs for a center open 24 hours per day, 7 days per week, 365 days per year that would monitor and respond to urgent inquiries from health workers and the public and would collect and analyze structured and unstructured reports. To maximize the use of limited resources, the Country X event monitoring center shares physical infrastructure, information and communications technology resources, and other utilities with the Command and Control Center described in Core Capacity 4.\n\n【36】##### Hazards Mapping\n\n【37】The Country X template includes inputs to develop a baseline inventory of community and national health risks. It applies across all sectors through consultative workshops and field assessments.\n\n【38】##### Core Capacity 4: Response\n\n【39】Country X template inputs for response include a functional, dedicated command and control center with room and information and communications technology equipment and services to accommodate up to 40 personnel during an event . Other inputs include materials, supplementary compensation, training, and travel allowances for 2 trained, 5-member multidisciplinary rapid response teams per province, plus 2 central rapid response teams, to investigate and respond to at least 1 public health event per year, with logistical and risk communications support from the provincial level. Inputs also include development and dissemination of guidance for infection control and case management, related training, and systems for isolating and transporting potentially infectious patients.\n\n【40】##### Core Capacity 5: Preparedness\n\n【41】The inputs for the Country X preparedness template encompass development, planning, and testing of a national public health emergency response plan . This template is based on a comprehensive national risk assessment and on establishment of a national stockpile of materials to respond to priority events.\n\n【42】##### Core Capacity 6: Risk Communications\n\n【43】Inputs for risk communication include the development, printing, and dissemination of a national risk communications plan .\n\n【44】##### Core Capacity 7: Human Resources\n\n【45】The inputs for human resources in Country X include resources for developing a coordinated national strategy for public health workforce development . The inputs also include resources for fully supporting field epidemiology training (including travel support for field investigations) for as many as 16 trainees per year.\n\n【46】##### Core Capacity 8: Laboratories\n\n【47】For diagnostic testing capabilities, we identified the testing platforms, materials and reagents, and specimen collection and referral systems necessary to support detection and confirmatory testing of the Country X priority diseases as appropriate at each level of a tiered, integrated health system. Technical Appendix Table 7 represents costs only of infrastructure and human capabilities associated with laboratory capacity.\n\n【48】##### Points of Entry\n\n【49】The inputs for the Country X template include a health office at each designated point of entry . Each office is staffed by 4-person multidisciplinary public health response teams trained and equipped to respond to medical emergencies.\n\n【50】### Discussion\n\n【51】This study was designed to help decision makers understand the demands of implementing IHR  and to build the business case for strengthening global capacities to detect, assess, report, and respond to public health emergencies. The lack of standards in many areas of public health capacity-building and the many options at almost every step of investment allow for dozens of variations in each category of core capacity, with concomitant variations in costs. However, the framework described in this article illustrates the scope of IHR implementation demands and is intended to help national and international decision makers understand the inputs and associated costs of implementing the IHR .\n\n【52】The framework includes all inputs and associated costs of building capacity for IHR-relevant public health surveillance and response rather than the marginal costs of adding new features to existing surveillance capabilities. Many countries will build the necessary capacities incrementally, and most already have capacities in place. This framework presents a way to estimate one-time capital costs, plus recurrent costs calculated on an annual basis, assuming that the total costs of national implementation depend on variables such as population, existing infrastructure, and health status. For most countries, the first step in developing a national IHR action plan is assessing the gap between current status and their ultimate strategies for implementing IHR core capacities fully.\n\n【53】We believe that this framework serves as a first step in helping national health authorities define to their own governments the actions and investments required to meet their IHR obligations, to protect their populations during public health emergencies, and to build a business case for potential donors. Articulating the elements of IHR core capacity-building can also help the global health community better comprehend a complex obligation that, if implemented fully, will strengthen the public health diagnostic, analytical, and information-sharing capacities that underpin effective decision making across health systems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "34b8e4a6-e0db-40f1-9ed2-829c9fb7b278", "title": "Correction: Vol. 26, No. 12", "text": "【0】Correction: Vol. 26, No. 12\nThe Figure had an incorrect x-axis scale in Laboratory Features of Trichinellosis and Eosinophilia Threshold for Testing, Nunavik, Quebec, Canada, 2009–2019 (L.B. Harrison et al.). The article has been corrected online .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "900e963f-2a59-4e8a-aa0c-a86cf7fec326", "title": "European Bat Lyssavirus Type 2 RNA in Myotis daubentonii", "text": "【0】European Bat Lyssavirus Type 2 RNA in Myotis daubentonii\nBat-mediated rabies has been reported in Europe for more than 50 years. Two variants or genotypes are now recognized that are distinct from rabies viruses of terrestrial mammals and new world bats . These are known as European bat lyssaviruses (EBLVs). A third lyssavirus, West Caucasian bat lyssavirus, has been isolated in eastern Europe . EBLV type 1 (EBLV-1) is found throughout mainland Europe and principally associated with the Serotine bat (Eptesicus serotinus_ ) . EBLV-2 is found in _Myotis_ bats (Myotis daubentonii_ \\[Daubenton's bat\\] and _Myotis dasycneme_ ) and has been identified in 3 locations in Europe: the Netherlands, the United Kingdom, and Switzerland .\n\n【1】Two reports detail isolation of EBLV-2 from humans who died of rabies encephalitis in Finland and the United Kingdom . In addition, 4 isolations from Daubenton's bat have been reported in the United Kingdom since 1996. Seroprevalence studies suggest that EBLV-2 is maintained at certain sites in the United Kingdom at low levels . However, the small number of bats infected with EBLV-2 and the nocturnal habits of insectivorous bats have hampered attempts to understand the distribution, prevalence, and transmission of the virus. Biting by Daubenton's bats was suspected in the 2 human cases from Finland and the United Kingdom because both persons had handled this species before symptoms developed. However, like rabies virus, EBLV-2 does not persist in the environment outside of an infected host, and alternative routes of infection should be considered. Investigation of the second bat detected viable EBLV-2 in the brain, and genomic RNA in the heart, stomach, tongue, intestine, liver, and kidney by using a sensitive nested reverse transcription–polymerase chain reaction (RT-PCR) . This approach was unable to quantify viral RNA within particular tissues. Since this study, 2 additional cases have occurred in the United Kingdom . The investigation and quantification of viral load within the infected host could provide evidence for release of virus and methods of transmission.\n\n【2】### The Study\n\n【3】In 2004, two EBLV-2 cases were identified in Daubenton's bats . A diagnosis of EBLV-2 infection was confirmed on brain samples with a fluorescent-antibody test, the mouse inoculation test, and a rapid TaqMan assay . Attempts to culture EBLV-2 from organs in both cases failed because of cytotoxicity of the samples, which destroyed the cell monolayer. Sample dilution reduced the cytotoxic effects of the sample on the cell monolayer (used for virus isolation) and enabled the development of small foci of infection (bat 603/04). Heminested RT-PCR detected virus RNA in brain, tongue, thyroid gland, and bladder after the first round of amplification, and in salivary gland, heart, lung, intestine, and stomach after the second round of amplification. We suspect that inappropriate storage of bat 696/04 in a freezer with repeated freezing and thawing before submission resulted in inactivation of virus in this sample. Heminested RT-PCR detected virus RNA in samples of brain and stomach after the first round of PCR, and in samples of tongue, intestine, liver, and kidney after the second round of amplification.\n\n【4】An EBLV-2-specific real-time PCR was developed to measure virus genome to quantify the potential viral RNA load within organs. Analysis was only attempted on those organs with sufficient RNA within the sample . Primers EBLVNa (´-CCTGGCAGATGATGGGAC-3´) and EBLVNb (´-GCCTTTTATCTTGGATCACT-3´) are located within the nucleoprotein gene and amplify a 221-bp target. An amplified product from a previous case  was purified by using the RNeasy kit (Qiagen, Valencia, CA, USA) and quantified with a NanoDrop WD-1000 spectrophotometer (NanoDrop Technologies, Inc. Wilmington, DE, USA). This procedure enabled the absolute number of copies of the amplicon to be calculated by its approximate molecular weight and Avogadro's number, as previously described .\n\n【5】RNA was isolated from each organ with Trizol (Invitrogen, Carlsbad, CA, USA) and quantified. Dilutions were made to either 0.25 μg/μL (bat 603/04) or 1 μg/μL (bat 696/04) to standardize the quantity of RNA used for reverse transcription. Primer EBLVNa was used for cDNA synthesis from the genomic (negative) sense strand as previously described . All PCRs were performed by using SYBR Green JumpStart Taq ReadyMix (Sigma, Saint Louis, MO, USA) and an MX3000P real-time thermal cycler (Stratagene, La Jolla, CA, USA). A dilution series of the control amplicon was amplified simultaneously with the organ samples to create a standard curve for comparison of the threshold value (Ct) with target copy number .\n\n【6】A representative plot of amplification curves from organ samples taken from bat 696/04 is shown in the Figure, panel B, with 10 μL of product separated by electrophoresis on a 1% agarose gel included for comparison. The quantitative results for viral RNA load for both bats are shown in Table 2 . In both cases, the brain had the highest viral genome load. Virus RNA was consistently detected in the tongue, intestine, and stomach. EBLV-2 was also found in the bladder of bat 603/04 but not in the kidney of bat 696/04 from which the bladder was not recovered because of carcass decomposition. Virus was not detected in the liver of either bat.\n\n【7】### Conclusions\n\n【8】The detection and quantification of EBLV-2 RNA in bat organs by real-time PCR show the potential distribution of this virus. The choice of organ tested in both cases was severely limited by degradation of the carcass before investigation. Furthermore, live virus could not be recovered from many organs because of cytotoxicity of the samples and virus degradation caused by repeated freezing and thawing.\n\n【9】Viable virus was recovered from the brain of bat 603/04. Since the brain is the main site of EBLV-2 replication, this finding suggests that the virus displays a similar neurotropism to classical rabies virus. Rabies virus, especially in the late stages of disease, disseminates from the brain to other innervated sites within the host . For EBLV-2, the tongue was consistently found to contain detectable levels of viral RNA in this study and a previous study . Genomic RNA was also found in the stomach and intestines of 3 bats investigated (this study and ). All of these organs are highly innervated tissues, although virus RNA in the stomach could result from swallowing virus.\n\n【10】Dissemination of rabies virus to the salivary glands and subsequent virus shedding enables transmission through biting. Detection of EBLV-2 RNA in the tongue of infected bats leads us to conclude that transmission of EBLV-2 may occur through biting. However, since EBLV-2 genome was detected in a bladder sample, we cannot exclude the possibility of virus release from urine. In future cases, where possible, organs such as the salivary glands and lungs should be examined to provide further evidence for the route of virus transmission between bats.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
