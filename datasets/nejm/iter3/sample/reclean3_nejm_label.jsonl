{"seq_id": "2bc66b32-edc6-449d-92a9-ed2f2019d435", "title": "Renal Tumors and Hereditary Pheochromocytoma-Paraganglioma Syndrome Type 4", "text": "【0】Renal Tumors and Hereditary Pheochromocytoma-Paraganglioma Syndrome Type 4\nTo the Editor:\n--------------\n\n【1】The genes for the succinate dehydrogenase subunits A, B, C, and D ( _SDHA, SDHB, SDHC,_ and _SDHD,_ respectively) encode proteins that form part of the mitochondrial complex II, which links the Krebs cycle and the electron-transport chain. Heterozygous germline mutations of _SDHB, SDHC,_ and _SDHD_ cause the well-characterized familial pheochromocytoma-paraganglioma syndromes known respectively as PGL4, PGL3, and PGL1.  _SDHB, SDHC,_ and _SDHD_ mutations have also been linked to gastrointestinal stromal tumor, and _SDHB_ and _SDHD_ have been linked to renal-cell carcinoma. These kindreds are rarely recognized when they present with gastrointestinal stromal tumor and they are virtually never recognized when they present with renal-cell carcinoma. This lack of recognition is unfortunate, given the malignant potential of all three component tumors, particularly in PGL4 (caused by _SDHB_ mutation), which is especially associated with malignant pheochromocytoma-paraganglioma and renal-cell carcinoma. \n\n【2】When any component of the mitochondrial complex II is lost, evidence suggests that the complex becomes unstable and immunohistochemical analysis to detect SDHB becomes negative. A double-hit _SDH_ mutation in pheochromocytoma-paraganglioma is virtually always associated with a germline mutation rather than being caused by two somatic events. Therefore, negative staining for SDHB indicates the need for genetic testing for _SDHB, SDHC,_ and _SDHD_ in patients who present with pheochromocytoma-paraganglioma.  As of this writing, a single SDHB-negative renal-cell carcinoma in a patient with PGL4 has been reported. \n\n【3】Figure 1. Renal-Cell Carcinoma Specimens Obtained from Controls and Kindreds Who Were Subsequently Shown to Harbor a Germline _SDHB_ Mutation.\n\n【4】Routine hematoxylin and eosin staining is shown on the left of each panel, and IHC staining to detect SDHB is shown on the right of each panel. As shown in Panel A, specimens obtained from the non-neoplastic kidneys of carriers showed strong positive granular cytoplasmic staining for SDHB at high magnification. Identical features were seen in the non-neoplastic kidneys of controls. This positive staining is most striking in the mitochondria-rich proximal convoluted tubules (arrows). Panel B shows a tumor specimen obtained from one patient with PGL4 (low magnification). The completely negative staining of the tumor for SDHB (arrows) contrasts strongly with the positive staining of the non-neoplastic kidney. In Panel C, a tumor specimen obtained from a second patient also shows negative staining of neoplastic tumor cells at high magnification. The non-neoplastic endothelial cells (arrows) show positive staining and act as an internal positive control. In Panel D, a control specimen of a typical conventional clear-cell renal carcinoma (obtained from one of three patients with von Hippel−Lindau syndrome) is shown at high magnification. It shows positive staining in all neoplastic cells.\n\n【5】We performed immunohistochemical analysis to detect SDHB in renal-cell carcinoma specimens obtained from three kindreds who were subsequently shown to harbor a germline _SDHB_ mutation .  SDHB was completely negative in all three specimens of _SDHB_ mutation-related renal-cell carcinoma, but it was positive in non-neoplastic cells obtained from these patients and in 70 unselected renal tumors used as controls. The unselected renal tumors included 45 conventional clear-cell renal carcinomas (including 3 associated with the von Hippel-Lindau syndrome), 10 papillary renal carcinomas, 9 chromophobe carcinomas, 5 oncocytomas, and 1 hereditary leiomyomatosis renal-cell carcinoma-related tumor.\n\n【6】Given the parallels with pheochromocytoma-paraganglioma, it is possible that negative staining for SDHB may accompany _SDHD_ \\-mutated renal-cell carcinoma. Negative SDHB staining also occurs in a distinct subgroup of gastrointestinal stromal tumors (pediatric wild-type gastrointestinal stromal tumors, the gastrointestinal stromal tumors of the Carney triad, and the subgroup of adult gastrointestinal stromal tumors that these tumors resemble clinically and pathologically).  Not all of these gastrointestinal stromal tumors will be associated with identifiable mitochondrial complex II mutations. Therefore, it is likely that there are other as-yet-undescribed mechanisms of mitochondrial complex II instability and tumorigenesis that are also associated with negative staining for SDHB.\n\n【7】Immunohistochemical analysis to detect SDHB can be used to screen patients with renal tumors for underlying _SDHB_ germline mutations (and possibly other mitochondrial complex II-related syndromes) at a fraction of the time and cost of formal genetic testing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d6ff917b-25d9-4368-a315-3a6f1d54da50", "title": "Case 8-2004 — A 28-Year-Old Man with Abdominal Pain, Fever, and a Mass in the Region of the Pancreas", "text": "【0】Case 8-2004 — A 28-Year-Old Man with Abdominal Pain, Fever, and a Mass in the Region of the Pancreas\nA 28-year-old Moroccan man was admitted to the hospital because of a 10-day history of epigastric pain and a 2-day history of right-upper-quadrant pain and fever. On examination, there was right-upper-quadrant tenderness with guarding and rebound. Imaging studies disclosed a complex, cystic mass in or adjacent to the neck of the pancreas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "05d14e39-0fbf-4394-b9f1-43562939be2b", "title": "Transmission of Tuberculosis in New York City -- An Analysis by DNA Fingerprinting and Conventional Epidemiologic Methods", "text": "【0】Transmission of Tuberculosis in New York City -- An Analysis by DNA Fingerprinting and Conventional Epidemiologic Methods\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The incidence of tuberculosis and drug resistance is increasing in the United States, but it is not clear how much of the increase is due to reactivation of latent infection and how much to recent transmission.\n\n【3】Methods\n-------\n\n【4】We performed DNA fingerprinting using restriction-fragment-length polymorphism (RFLP) analysis of at least one isolate from every patient with confirmed tuberculosis at a major hospital in the Bronx, New York, from December 1, 1989, through December 31, 1992. Medical records and census-tract data were reviewed for relevant clinical, social, and demographic data.\n\n【5】Results\n-------\n\n【6】Of 130 patients with tuberculosis, 104 adults (80 percent) had complete medical records and isolates whose DNA fingerprints could be evaluated. Isolates from 65 patients (62.5 percent) had unique RFLP patterns, whereas isolates from 39 patients (37.5 percent) had RFLP patterns that were identical to those of an isolate from at least 1 other study patient; the isolates in the latter group were classified into 12 clusters. Patients whose isolates were included in one of the clusters were inferred to have recently transmitted disease. Independent risk factors for having a clustered isolate included seropositivity for the human immunodeficiency virus (HIV) (odds ratio for Hispanic patients, 4.31; P = 0.02; for non-Hispanic patients, 3.12; P = 0.07), Hispanic ethnicity combined with HIV seronegativity (odds ratio, 5.13; P = 0.05), infection with drug-resistant tuberculosis (odds ratio, 4.52; P = 0.005), and younger age (odds ratio, 1.59; P = 0.02). Residence in sections of the Bronx with a median household income below $20,000 was also associated with having a clustered isolate (odds ratio, 3.22; P = 0.04).\n\n【7】Conclusions\n-----------\n\n【8】In the inner-city community we studied, recently transmitted tuberculosis accounts for approximately 40 percent of the incident cases and almost two thirds of drug-resistant cases. Recent transmission of tuberculosis, and not only reactivation of latent disease, contributes substantially to the increase in tuberculosis\\.\n\n【9】Introduction\n------------\n\n【10】After decades of decline, the incidence of tuberculosis in the United States began to increase in 1986, resulting in 52,000 excess cases by 1992  . New York City accounted for 14 percent of all cases of tuberculosis in the United States in 1992; the number of cases reported in the city has increased by over 150 percent since 1979  . This increase has been especially dramatic among minorities and in specific areas.\n\n【11】The increase in tuberculosis has been attributed to coinfection with the human immunodeficiency virus (HIV),  deterioration of the public health infrastructure,  social disruption including homelessness and drug abuse,  and immigration  . A high reactivation rate of latent tuberculous infection in persons coinfected with HIV is thought to be the principal mechanism underlying this phenomenon  . It is estimated that 90 percent of tuberculosis cases nationwide are due to the reactivation of latent, remote infection  . To reduce the risk of reactivation, tuberculosis-control strategies have emphasized preventive therapy in populations at high risk for latent infection  .\n\n【12】Outbreaks of tuberculosis in a residence for patients with the acquired immunodeficiency syndrome (AIDS),  in shelters,  and in hospitals  show that transmission and rapid progression to disease can occur in institutional settings. Increases in tuberculosis among children  indicate that transmission is occurring in the community. There has been a growing recognition that social conditions in poor urban areas where tuberculosis remains prevalent, combined with high rates of HIV infection, may facilitate transmission of tuberculosis  . However, the relative contribution of recent transmission to the overall incidence of disease and the risk factors involved have not been established. In this study, we used molecular and epidemiologic methods to assess these factors in one medical center in New York City.\n\n【13】The presence of repetitive genetic insertional elements in Mycobacterium tuberculosis permits the identification of individual strains by DNA fingerprinting with restriction-fragment-length polymorphism (RFLP) analysis and can be used to demonstrate the transmission of particular M. tuberculosis strains in outbreaks  . This technique, combined with medical-record review and analysis of census data, allowed us to investigate the microbiologic, clinical, social, and demographic factors associated with recently transmitted disease.\n\n【14】Methods\n-------\n\n【15】Patient Population\n------------------\n\n【16】The study was performed at a 765-bed hospital that is the largest provider of primary care to the 1.2 million residents of the Bronx, a borough of New York City. The population served is largely poor, Hispanic, and black, but also includes middle-class whites and minorities. The hospital also serves as a referral center for nearby counties. There have been no recognized outbreaks of nosocomially transmitted tuberculosis.\n\n【17】Accrual of Patients and Mycobacterial Samples\n---------------------------------------------\n\n【18】All patients who had at least one positive culture for M. tuberculosis from December 1, 1989, through December 31, 1992, were enrolled in the study. Each patient's medical records were reviewed with a standardized form, and one initial M. tuberculosis isolate was selected for RFLP analysis and antimycobacterial-susceptibility testing. Thirteen isolates could not be subcultured: 11 were nonviable, 1 was contaminated, and 1 was lost. Patients were considered to be infected with HIV only if they met the 1987 definition of AIDS devised by the Centers for Disease Control and Prevention (CDC) or were documented to be seropositive, and they were considered to have AIDS only if they met the 1987 CDC case definition. A patient's country of origin was not always recorded; consequently, patients were classified as either foreign-born or not foreign-born (U.S.-born or of unknown origin). The hospital's medical data base was used to provide a complete listing of each patient's previous hospitalizations at the institution from mid-1984 until the end of the study period. The investigators were unaware of the individual RFLP patterns of the study patients when they reviewed the medical records. This study was approved by the institutional review board of the hospital.\n\n【19】Analysis and Mapping of Census Blocks\n-------------------------------------\n\n【20】Adults with tuberculosis who had confirmed Bronx addresses were located in specific census blocks and block groups (the smallest areas of census enumeration) with TIGER files, the comprehensive geographic locator system developed by the U.S. Census and U.S. Geological Survey. Demographic, social, and household characteristics were taken from the STF1 and STF3 reports, the major files of the 1990 U.S. Census. Mapping was done by the ATLAS/GIS mapping program. Fifteen patients were excluded from this analysis because their medical records listed an address outside the Bronx, and one was excluded because the address given could not be confirmed as an existing street address.\n\n【21】Susceptibility Testing\n----------------------\n\n【22】Susceptibility testing was done with the CDC version of the proportion method  . Susceptibility data were not available for three isolates; this was reflected in the denominator used in calculations involving drug resistance. A radiometric culture system (Bactec) was not used at the study hospital.\n\n【23】RFLP Analysis\n-------------\n\n【24】RFLP analysis was performed according to previously described methods  . In brief, M. tuberculosis DNA was extracted, digested with PvuII, subjected to electrophoresis, and hybridized with Southern blot techniques with a fragment of the insertion element IS6110 measuring 245 base pairs and generated by the polymerase chain reaction. Images were generated by enhanced chemiluminescence (Amersham, Arlington Heights, Ill.). A subgroup of M. tuberculosis strains was subjected to secondary RFLP analysis. The DNA was digested with AluI and probed in a similar manner with a 36-base oligonucleotide homologous to the direct-repeat region of the M. tuberculosis genome  . Two investigators visually examined the RFLP fingerprints. A cluster was defined as a group of two or more isolates from different patients whose RFLP fingerprints were identical with respect to both the number and molecular size of all bands. Isolates that had unique fingerprints were deemed nonclustered. The isolates were divided into two groups: group 1 contained all nonclustered isolates, and group 2 contained all the clustered isolates.\n\n【25】Confirmatory Analysis\n---------------------\n\n【26】We have found that RFLP analysis based solely on hybridization with IS6110 may not distinguish between isolates with only two apparently identical bands (data not shown). In order to confirm strain identity among these isolates, we subjected most such strains to secondary RFLP analysis using a second enzyme and a different probe . On this basis, we were able to sort strains with only two initially identical bands into three smaller clusters. One such strain had a unique secondary fingerprint and was reclassified into group 1. Two other strains could not be recultured for secondary RFLP analysis. One was placed in an established cluster of two-banded strains on the basis of a shared unusual pattern of resistance, and the other strain was excluded from the study. Two strains with only one apparently identical band detected on primary RFLP fingerprinting were also excluded to avoid a bias toward clustering.\n\n【27】Statistical Analysis\n--------------------\n\n【28】Chi-square tests (or Fisher's exact tests, when expected cell sizes were less than five) were performed to test the association of clustering with categorical predictor variables. Wilcoxon nonparametric tests and t-tests were performed to determine whether the distributions of the continuous variables differed between subjects with clustered strains and those with nonclustered strains. Predictor variables that were significantly associated with clustering (P<0.05) were included in a logistic-regression model. Statistical interaction with HIV status was also investigated. Logistic-regression analysis was also performed on the data on subjects who were Bronx residents to determine the independent effects of household income on clustering.\n\n【29】Results\n-------\n\n【30】Study Population\n----------------\n\n【31】Of 130 patients who had culture-proved tuberculosis during the study period, 117 had M. tuberculosis cultures available for RFLP fingerprinting. The medical records were reviewed for 114 of the 117 patients (97 percent) with known RFLP fingerprints. Three patients with negative smears, a single positive culture, and no clinical or radiologic evidence of tuberculosis on chart review were excluded because their culture results were thought to represent possible laboratory contamination or mislabeling. We also excluded three patients whose isolates had two or fewer bands on primary RFLP analysis and for which secondary RFLP analysis was not done. For classification purposes, the remaining 108 patients were assigned to group 1 or group 2 on the basis of their DNA fingerprints. For all other analyses, the four children under 15 years of age were excluded, because many of the social variables evaluated in this study are not relevant to children. The final study population consisted of the 104 adult patients for whom complete data were available. The demographic and clinical characteristics of the study population were compared with those of two series of patients with tuberculosis in New York City who were studied in 1991  . The study population was similar to those in the two earlier studies in most respects, including sex, age, proportion of foreign-born subjects, and proportion seropositive for HIV, but there was a greater proportion of Hispanic subjects, reflecting the ethnic makeup of the Bronx, and somewhat fewer patients with AIDS and homeless patients.\n\n【32】Characterization of Patients According to Tuberculosis Strain\n-------------------------------------------------------------\n\n【33】Of the 104 study patients, 65 (62.5 percent) had strains of M. tuberculosis with unique RFLP fingerprints (group 1) and 39 (37.5 percent) had strains whose RFLP patterns were identical to those of 1 to 11 isolates from other patients, which we classified into 12 clusters (group 2). A total of 77 RFLP patterns were seen in 104 patients.\n\n【34】Table 1. Characteristics of the Two Groups of Patients with Tuberculosis.\n\n【35】The clinical and epidemiologic characteristics of the groups are shown in Table 1 . The patients in group 2 were significantly more likely than the patients in group 1 to be HIV-infected (67 percent vs. 31 percent, P<0.001) and to have drug-resistant isolates (49 percent vs. 16 percent, P<0.001), including resistance to multiple drugs (24 percent vs. 5 percent, P = 0.003). There was a significant association with Hispanic ethnicity (49 percent vs. 26 percent, P = 0.02); however, birth outside the United States and Puerto Rico was less common (8 percent vs. 28 percent, P = 0.01). The patients in group 2 were also significantly younger than those in group 1 (mean age, 36 vs. 46 years, P = 0.001). Other clinical and radiologic features of the two groups were similar. The patients in group 2 were not more likely than the patients in group 1 to have been hospitalized previously at the study hospital; before their disease developed, only three patients could possibly have been exposed to other patients infected with strains with identical RFLP fingerprints.\n\n【36】Analysis of the Bronx residents according to census blocks and block groups permitted us to assess the local economic and demographic environment in which each patient lived. Patients in group 2 resided in block groups with lower median household incomes than those in group 1 ($17,676 vs. $22,338, P = 0.02) and lived in more crowded conditions (percentage of households on the block with more than one person per room, 22 percent vs. 17.8 percent; P = 0.04). The difference in median incomes was significant only among non-Hispanic patients ($17,713 vs. $24,480, P = 0.01) and HIV-seronegative patients ($14,977 vs. $23,476, P = 0.003). Hispanic patients as a whole lived in block groups with lower median incomes than non-Hispanic patients (mean income, $17,577 vs. $22,224; P = 0.02), but there was no significant difference in this variable between group 1 and group 2.\n\n【37】Age Differences\n---------------\n\n【38】Figure 1. Age Distribution of Patients in Group 1 and Group 2 According to HIV Status and Place of Birth.\n\n【39】Group 1 comprises at least three populations: an older population of HIV-seronegative patients and two younger populations, one HIV-seropositive and the other HIV-seronegative and foreign-born. Patients in group 2 are more homogeneous, with similar age distributions of HIV-seropositive and HIV-seronegative patients.\n\n【40】Younger age and HIV seropositivity were strongly associated with having a clustered isolate (group 2). However, group 1 also had young foreign-born and HIV-seropositive patients . The higher mean age of the patients in group 1 was due to the large number of HIV-seronegative patients over the age of 50 (21 of 45), as compared with group 2 (2 of 13, P = 0.04). Therefore, younger patients were at risk for both clustered and nonclustered isolates, whereas older patients had predominantly nonclustered isolates.\n\n【41】Multivariate Analysis of Differences between Groups\n---------------------------------------------------\n\n【42】Table 2. Multivariate Analysis of the Association of Study Variables with Clustering of Tuberculosis.\n\n【43】In the multivariate model , resistance to one or more drugs was strongly associated with clustering (adjusted odds ratio, 4.52; P = 0.005), as was younger age (adjusted odds ratio for a difference of 10 years as a continuous variable, 1.59; P = 0.02). Conversely, subjects known to be foreign-born were less likely to be in group 2 (adjusted odds ratio, 0.27; P = 0.08). There was an interaction between HIV status and ethnicity. The odds of clustering were significantly increased among patients who were Hispanic, HIV-seropositive, or both as compared with patients who were HIV-seronegative and not Hispanic. However, HIV seropositivity did not increase the risk of having a clustered isolate among the Hispanic patients, and conversely, being Hispanic did not increase this risk among HIV-seropositive patients.\n\n【44】Figure 2. Map of the Bronx, Showing the Ethnicity, HIV Status, and Geographic Location of Patients in Group 1 and Group 2 in Relation to Lower-Income Block Groups.\n\n【45】Patients in group 2 lived more often in poor block groups (median household income in 1989, below $20,000). This association was independent of ethnicity or HIV status. The lower-income block groups represented identifiable geographic areas where cases of recently transmitted tuberculosis were common.\n\n【46】Multivariate analysis was performed on data on Bronx residents, including block-group data on median household income. Crowded living conditions (more than 20 percent of households in the census block with more than one person per room) was not included in the final model because it was correlated with median household income (R  \\= 0.71, P<0.001). Previously noted associations with clustering remained the same, although their statistical significance was decreased as a result of the smaller number of observations and increased number of variables in the model. A median household income of less than $20,000 per year was associated with clustering (adjusted odds ratio, 3.22; P = 0.04). The geographic concentration of patients in group 2 in predominantly lower-income block groups and the relation of this variable to HIV status and ethnic group are shown in Figure 2 .\n\n【47】Analysis of Clustered Cases According to Date of Presentation and Pattern of Drug Resistance\n--------------------------------------------------------------------------------------------\n\n【48】Figure 3. Temporal Distribution and Patterns of Drug Resistance of Strains of Tuberculosis Isolated from Patients in Group 2.\n\n【49】Panel A shows that most cases caused by clustered strains presented within a limited period. The date of the initial positive M. tuberculosis culture is shown for each patient (39 adults and 2 children, the latter indicated by asterisks). The initial isolates in clusters A through F, H, I, and L were all cultured within 11 months of each other. New isolates with identical RFLP fingerprints were not otherwise seen during the 37-month study period. Isolates from clusters G and K were identified 12 and 14 months apart, respectively. Cases caused by strains from cluster J, the largest cluster, occurred throughout the study but were found predominantly in 1990 and 1992; only one case was seen in 1991.\n\n【50】Panel B shows that the patterns of drug resistance were similar within each cluster. Each entry describes the resistance profile of one M. tuberculosis isolate. Resistance profiles are listed according to cluster and are shown in the order in which they presented (from left to right) during the study period. In four of eight clusters with drug resistance, all isolates with known susceptibility profiles were resistant to the same drugs. In two clusters, one isolate was drug-sensitive whereas the others were resistant to a single drug, and in one cluster all isolates were multidrug-resistant but one was resistant to an additional drug. In only one cluster did all three isolates have different susceptibility profiles. I denotes isoniazid, R rifampin, E ethambutol, S streptomycin, a dash no resistance, and a question mark an unknown pattern of resistance. There were a total of 12 drug-sensitive isolates in cluster J (marked with a dagger).\n\n【51】Cases of tuberculosis due to clustered strains tended to appear together . The first positive cultures from the patients in each cluster were obtained over a limited period, a circumstance consistent with an episodic pattern of transmission. Strains in cluster J, the largest cluster, were isolated throughout the study, but they were found predominantly in 1990 and 1992; only one case was seen in 1991. This same strain has been implicated in an outbreak of tuberculosis at a men's shelter in northern Manhattan  .\n\n【52】Nearly 50 percent of group 2 isolates were drug-resistant. The isolates within an individual cluster had similar patterns of resistance ; most clusters were identical, and in three of four with different susceptibilities, the susceptibilities differed by only one drug. Five patients with drug-resistant strains had a history of tuberculosis, but the susceptibility of their previous isolate was unknown.\n\n【53】Discussion\n----------\n\n【54】We used DNA fingerprinting in conjunction with traditional epidemiologic methods to investigate the pathogenesis of resurgent tuberculosis in a New York City community. Traditional teaching has held that the majority of the cases of tuberculosis in developed countries result from reactivation during adulthood of an infection contracted decades before  . It is estimated that reactivation is responsible for up to 90 percent of the incident cases in the United States  . Excess cases have largely been attributed to increases in tuberculosis contracted outside the United States  and to the high reactivation rate among persons infected with both tuberculosis and HIV  . Recently transmitted tuberculosis (usually defined as disease occurring within two years of infection) was generally thought to have a minor role  . However, investigations of institutional outbreaks caused by a single strain have clearly demonstrated that transmission and rapid progression to disease can occur, particularly in persons with AIDS  . Some experts suggest that recent transmission may have a substantial role in the current spread of tuberculosis  . However, few data are available on the relative contributions of recent transmission and reactivation to incident cases in the community or on risk factors associated with transmission.\n\n【55】DNA fingerprinting provides a new tool for distinguishing recently transmitted from reactivated tuberculosis. Investigations of numerous outbreaks have demonstrated that epidemiologically linked strains of M. tuberculosis have identical RFLP patterns, whereas unrelated strains have differing patterns  . In the Netherlands, where the incidence of tuberculosis is declining, all epidemiologically unrelated M. tuberculosis isolates have unique RFLP fingerprints  .\n\n【56】The substantial diversity of RFLP patterns among members of the study population suggests that the chance occurrence of identical RFLP fingerprints among unrelated cases would be unusual. We therefore infer that cases of tuberculosis caused by strains with identical RFLP fingerprints (group 2) are due to recently transmitted disease and that cases caused by strains with unique RFLP fingerprints (group 1) are primarily due to the reactivation of infection. Several findings support this conclusion. Cases within each cluster occurred over a limited period, as would be expected with new focal outbreaks of disease. Furthermore, almost half the clustered cases were drug-resistant, with similar patterns of resistance within each cluster. Until recently, drug resistance was uncommon in New York City  ; therefore, clustering of RFLP-identical, drug-resistant isolates must be due to recently transmitted organisms. Conversely, many patients with nonclustered strains had demographic characteristics consistent with a finding of reactivated tuberculosis. Eighteen of 21 foreign-born patients were in group 1, as were 22 of 25 patients who were 50 years of age or older.\n\n【57】Our analysis probably underestimates the true extent of recent transmission. Some patients in group 1 might have been recently infected by persons outside the study population. Conversely, if each cluster includes one reactivated index case, it could overestimate the incidence of recent transmission. The latter possibility seems unlikely, since RFLP fingerprinting of strains from other area hospitals has revealed isolates with patterns identical to several of the clusters in this study,  demonstrating that the chain of transmission extends beyond our study population. Although it is possible that a bias toward clustering exists in any study performed at a single institution, our population was comparable in most respects to that of all patients with tuberculosis in New York City in 1991.\n\n【58】Our study suggests that recently transmitted tuberculosis accounts for almost 40 percent of the incident cases in an inner-city community. The independent risk factors for recently transmitted disease include younger age, Hispanic ethnicity in HIV-seronegative patients, and infection with drug-resistant organisms. Living in a lower-income block group was an additional risk factor for recent transmission in some demographic groups. Forty-three percent of the cases of tuberculosis in the HIV-seropositive patients were in group 1 and can be attributed to the increased reactivation rate with HIV coinfection. We believe that the remaining 57 percent are due to recent transmission. There was no association between clustering and previous admissions, and only three cases of possible nosocomial transmission at the study hospital; hence, nosocomial transmission is an unlikely explanation for our findings. We also found that most foreign-born patients did not have clustered strains, which implies that imported cases are not a major cause of recent transmission of tuberculosis in this area. The association between recent transmission and HIV infection reflects both biologic and social factors. HIV-seropositive patients can have rapid disease progression after infection with tuberculosis  . HIV may modify or overwhelm other risk factors for transmission. For example, in this study, the impact of lower income was found to be greatest in the HIV-seronegative population.\n\n【59】This study has several implications for the future control of tuberculosis. In this urban population, Hispanic ethnicity, HIV infection, and residence in lower-income block groups were risk factors for recent transmission. The recently transmitted cases can be mapped to lower-income neighborhoods, thus identifying an environment where there are other potential risk factors for transmission, such as crowding. In populations in which most cases of tuberculosis are due to reactivation, screening with the tuberculin test and selective use of preventive therapy offer effective strategies for controlling disease. However, in communities with high rates of recent transmission, identification of groups at high risk for transmission, early identification of cases, reduction of institutional spread, and treatment until the disease is cured require more emphasis. In this study, almost half the isolates from patients with recently transmitted infections were drug-resistant, and one quarter were resistant to multiple drugs. Recently transmitted cases accounted for almost two thirds of drug-resistant M. tuberculosis. In 1991, many of the drug-resistant cases in New York City were due to primary drug resistance  . The difficulty of treating drug-resistant tuberculosis until it is cured may lead to a prolonged infectious state, increasing the risk of selection for and transmission of drug-resistant strains. Our study suggests that efforts to control the increase in drug-resistant tuberculosis must include a strategy to reduce the transmission of the disease. In addition to intensive inpatient and outpatient therapeutic programs, unconventional approaches, including active case finding in the areas of high transmission and the option of immunization with bacille Calmette-Guerin for persons at higher risk, merit serious consideration to protect the community, health care workers, and hospitalized patients from further transmission of drug-resistant disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e266e07a-7301-454f-be8e-304e031ee3db", "title": "A Comparison of Nebulized Budesonide, Intramuscular Dexamethasone, and Placebo for Moderately Severe Croup", "text": "【0】A Comparison of Nebulized Budesonide, Intramuscular Dexamethasone, and Placebo for Moderately Severe Croup\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】In children with croup, treatment with nebulized budesonide decreases symptoms, but it is uncertain how budesonide compares with dexamethasone, the conventional therapy for croup, and whether either reduces the rate of hospitalization.\n\n【3】Methods\n-------\n\n【4】We performed a double-blind, randomized trial involving 144 children with moderately severe croup. The children were treated with racepinephrine and a single dose of 4 mg of nebulized budesonide (48 children), 0.6 mg of intramuscular dexamethasone per kilogram of body weight (47 children), or placebo (49 children). The children were assessed before treatment and then hourly for five hours after treatment. Physicians who were unaware of the treatment assignments determined the children's need for further treatment and hospitalization.\n\n【5】Results\n-------\n\n【6】The characteristics of the groups were similar at base line, including the types of viruses identified, the types of croup, and the clinical severity of the illness. The overall rates of hospitalization were 71 percent in the placebo group (35 of 49 children), 38 percent in the budesonide group (18 of 48 children), and 23 percent in the dexamethasone group (11 of 47 children) (unadjusted P=0.001 for the comparison of budesonide with placebo, P<0.001 for the comparison of dexamethasone with placebo, and P=0.18 for the comparison of budesonide with dexamethasone). Children treated with budesonide or dexamethasone had a greater improvement in croup scores than those given placebo (P=0.03 and P<0.001, respectively), and those treated with dexamethasone had a greater improvement than those treated with budesonide (P=0.003).\n\n【7】Conclusions\n-----------\n\n【8】In children with moderately severe croup, treatment with intramuscular dexamethasone or nebulized budesonide resulted in more rapid clinical improvement than did the administration of placebo, with dexamethasone offering the greatest improvement. Treatment with either glucocorticoid resulted in fewer hospitalizations.\n\n【9】Introduction\n------------\n\n【10】Each year croup is diagnosed in 3 of every 100 children under six years of age,  and approximately 1 percent of children with croup are hospitalized.  As a result, it accounts for a substantial proportion of all pediatric hospitalizations (2 percent at Alberta Children's Hospital). A treatment that substantially reduced the hospitalization rate in children with croup would therefore be expected to decrease health care costs and the social burden of the disease.\n\n【11】Klassen et al. found that among children with croup who were treated in an emergency department, those who received nebulized budesonide were less likely to be hospitalized than those who received placebo.  Though their results suggest that glucocorticoids act rapidly enough to reduce the likelihood of hospitalization, they studied only 54 children, and the difference in hospitalization rates between the two groups was marginal (P=0.05). Another study of nebulized dexamethasone, which included 55 children and had a similar design, found no significant reduction in the rate of hospitalizations.  Thus, it remains uncertain whether glucocorticoid therapy can reduce the need for hospitalization in children with croup.\n\n【12】Numerous studies have demonstrated that inhaled budesonide decreases the symptoms of croup more rapidly than does placebo.  However, it is unclear how treatment with budesonide compares with conventional therapy with parenteral or oral dexamethasone.  In one study fewer patients were hospitalized at 24 hours in the oral-dexamethasone group than in the nebulized-budesonide group, but the differences between groups were not significant. \n\n【13】We designed a randomized, double-blind, placebo-controlled trial to determine whether the administration of glucocorticoids to children with moderately severe croup evaluated in an emergency department reduces the rate of hospitalization and to compare nebulized budesonide and intramuscular dexamethasone.\n\n【14】Methods\n-------\n\n【15】Study Subjects\n--------------\n\n【16】Table 1. The Croup Scoring System.\n\n【17】Trained study nurses were notified of children with acute onset of audible stridor who were seen between 3 p.m. and 6 a.m. at the emergency departments of the Hospital for Sick Children in Toronto from September 1993 through May 1996 and Alberta Children's Hospital in Calgary from October 1995 to May 1996. Children were enrolled in the study if they were three months to nine years of age, had been given a diagnosis of croup (defined as acute onset of inspiratory stridor associated with a “seal-like” barking cough), and had persistent, moderately severe respiratory distress, defined as a croup score of 3 to 6  after treatment with humidified oxygen for 30 minutes.  Exclusion criteria included symptoms or signs suggesting another cause of stridor, such as epiglottitis, bacterial tracheitis, or supraglottic foreign body  ; inability of the patients' parents to speak English well enough to understand and give informed consent; or a history of chronic pulmonary disease, severe systemic disease, immune dysfunction,  stridor or intubation for more than one month, or glucocorticoid therapy in the four weeks before study entry.\n\n【18】The study was approved by the institutional review boards at each hospital. Written informed consent was obtained from each patient's parents before enrollment.\n\n【19】Study Design\n------------\n\n【20】Eligible children were randomly assigned to receive a single dose of nebulized budesonide (4 mg; Pulmicort Nebuamp, Astra Pharma, Mississauga, Ont.), intramuscular dexamethasone (0.6 mg per kilogram of body weight; Decadron Injection, Merck Sharp & Dohme Canada, Kirkland, Que.), or placebo. All patients received 0.5 ml of 2.25 percent racepinephrine (Vaponefrin, Rhône-Poulenc Rorer Canada, Ville St. Laurent, Que.) and normal saline combined with either the budesonide or placebo suspension (total volume, 8 ml). The suspension was delivered by means of a nebulizer (model 1700 Up-draft Neb-U-Mist, Hudson, Temecula, Calif.) with oxygen from a wall outlet at a rate of 6 to 7 liters per minute through a face mask held tightly to the patient's face over a period of 20 minutes. Study nurses closely supervised nebulizer treatments to ensure compliance.\n\n【21】The placebo suspension, prepared by Astra, was slightly clearer than the budesonide suspension. To make the nebulized study drugs indistinguishable from each other, they were packaged in opaque containers and discharged directly into a colored nebulizer with the racepinephrine and normal saline. For ethical reasons, only children who were randomly assigned to dexamethasone received injections. To maintain masking, the study nurse temporarily took the parents away from their child while an emergency staff nurse not otherwise involved in the care of the child injected the dexamethasone into the child's thigh, placed a bandage over the injection site (all children received a bandage whether or not they received dexamethasone), and initiated nebulization. The nurse was instructed not to inform anyone, including other emergency staff members, about the treatment delivered. At the completion of the observation period for each child, the adequacy of masking was assessed.\n\n【22】Randomization\n-------------\n\n【23】A blocked randomization code was produced by random-number–generating software (Astra) and provided only to the pharmacy at each hospital. The pharmacies prepared sequential patient packets containing study drugs that were sealed and were identical in appearance and weight. The code was not broken until after the study ended and all decisions regarding data analysis were finalized.\n\n【24】Viral Cultures and Assessment of the Type of Croup\n--------------------------------------------------\n\n【25】Cultures of nasopharyngeal secretions were obtained at each hospital with the use of standard procedures to test for parainfluenza virus, influenzavirus, and respiratory syncytial virus. The children were classified as having spasmodic croup, acute laryngotracheitis, or a mixed presentation. Patients with sudden onset of stridor (<2 hours before presentation), with no fever (temperature, <38.0°C) or upper respiratory tract infection, were considered to have spasmodic croup. Patients with a prodromal upper respiratory tract infection (>12 hours before the onset of stridor) and fever or a history of fever were considered to have acute laryngotracheitis. Patients with symptoms that did not meet either of the previous definitions were considered to have a mixed presentation. \n\n【26】Other Treatments and Hospitalization\n------------------------------------\n\n【27】Emergency physicians made all decisions regarding the need for further treatment with racepinephrine and hospitalization on the basis of their clinical judgment. No treatment or admission criteria were imposed, except that physicians were asked not to administer glucocorticoids unless they decided to admit the child to the hospital after the five-hour observation period. All children received mist therapy throughout the observation period. At the Hospital for Sick Children, mist was administered through a plastic hose held by the parents to the child's face, and at Alberta Children's Hospital, mist was administered by a bedside humidifier before the study treatment and with a mist tent thereafter.\n\n【28】Follow-up\n---------\n\n【29】Study nurses telephoned the parents of children who were not hospitalized 72 hours after they had been discharged from the emergency department to ascertain whether the children had been reevaluated. Children who remained symptomatic were followed up every 72 hours until they were free of symptoms.\n\n【30】Outcome Measures\n----------------\n\n【31】The hospitalization rate was the primary outcome measure, and the change in croup score (from base line to five hours after treatment) and the number of additional racepinephrine treatments administered (two to five hours after treatment with the study drug) were chosen as secondary outcome measures. A single nurse determined the croup score in each child when the child was in a quiet, alert state, before treatment and then hourly for five hours after treatment or until the child was sent home from the emergency department . The nurse also measured oxygen saturation while the child was breathing room air, respiratory rate, heart rate, and blood pressure at these times. The nurses were trained by the principal investigator to use the croup scoring system, and the results were validated throughout the enrollment period by having study physicians independently determine the children's scores at the same time that the nurses made their assessments. Physicians and nurses were unaware of each other's assessments. The respiratory rate was counted for one minute; the heart rate and oxygen saturation were measured by pulse oximetry while the child was breathing room air.\n\n【32】Statistical Analysis\n--------------------\n\n【33】On the basis of previous experience,  we estimated that the hospitalization rate in the control group would be approximately 65 percent. In order to allow the detection of a 40 percent reduction in the proportion of children who were hospitalized, we planned to enroll 210 children, with an interim analysis after the enrollment of approximately 150 children. A biostatistician who was not otherwise involved in the study performed the interim analysis using the primary outcome variable, hospitalization rate. The criterion for stopping the study was the finding in an overall test for differences between the three treatment groups of a P value of less than 0.005. To maintain an overall type I error of 0.05, we adjusted the alpha level for the final analysis to 0.048. \n\n【34】The data were analyzed with SAS software  and VMS/VAX software . The normality of the distribution of variables was assessed by the Wilk–Shapiro test.  Proportional variables were analyzed without adjustment by Fisher's exact test and with adjustment for covariates by multiple logistic-regression analysis. The latter analysis allowed us to explore the effect of several base-line prognostic factors (base-line croup score, study center, study nurse, type of croup, and viral-culture results) on the primary and secondary outcome measures.  Continuous variables were analyzed without adjustment by one-way analysis of variance and with adjustment by an analysis of covariance with the same covariates that were used in assessing proportional variables.  All statistical tests were two-tailed.\n\n【35】An intention-to-treat analysis was performed, in that all children who were randomly assigned to a treatment group were included in the full analysis. In addition, an analysis was performed of all children who completed the study except those with an unintentional deviation from the study protocol.\n\n【36】Results\n-------\n\n【37】Characteristics of the Patients\n-------------------------------\n\n【38】Table 2. Follow-up of Enrolled Children.\n\n【39】Among 4075 children with a discharge diagnosis of croup during the enrollment period, 3857 were not approached for enrollment in the study. The reasons for exclusion were as follows: disease that was too mild (3206 children); disease that was too severe, stridor that was not due to croup, or the presence of a disqualifying disease (229); previous treatment with glucocorticoids (121); wrong age (19); parents could not speak English well enough to give consent (7); evaluation outside of the study hours of 3 p.m. to 6 a.m. (176); refusal of physician to allow enrollment of child (7); and miscellaneous reasons — for example, the study nurse was not contacted (92). The parents of the remaining 218 eligible children were approached for consent, and 145 consented. All 145 enrolled children were randomly assigned to treatment and evaluated, except for 1 child who had a febrile seizure immediately after informed consent was obtained. A total of 49 children were randomly assigned to the placebo group, 48 to the budesonide group, and 47 to the dexamethasone group. The numbers of children who were evaluated at base line, three and five hours after treatment, and after discharge from the emergency department (among those who were not hospitalized) are shown in Table 2 . As planned, an interim analysis was performed after approximately 150 children had been enrolled. The study was terminated because the results of an overall test for differences in the hospitalization rate among the three treatment groups exceeded the pre-established stopping criteria (P<0.001).\n\n【40】Table 3. Reasons for Deviations from the Protocol in the Case of 25 Children.\n\n【41】Of the 144 randomized children, 25 had an unintended deviation from the protocol . The results of a subanalysis of children with each type of protocol deviation and those without it were qualitatively similar to the results of the intention-to-treat analysis. Therefore, only the results of the latter analysis are reported.\n\n【42】Table 4. Base-Line Characteristics of the Children.\n\n【43】There were no significant differences between treatment groups with regard to demographic or clinical variables, including the types of viruses identified and the types of croup . There were 100 boys and 44 girls; the mean (±SD) age was 24±18 months; and the mean weight was 12.9±4.5 kg.\n\n【44】The viral cause, study center, and type of croup did not qualitatively alter the differences between treatment groups for any of the primary or secondary outcomes. Consequently, none of these variables were included in the adjusted analyses as covariates.\n\n【45】Hospitalization\n---------------\n\n【46】The rates of hospitalization after treatment were highest in the placebo group (33 of 49 children, 67 percent), intermediate in the budesonide group (17 of 48 children, 35 percent), and lowest in the dexamethasone group (8 of 47 children, 17 percent). The need for hospitalization was determined by the time of the five-hour assessment or shortly thereafter in the case of most children (88 percent). Four children, all of whom were in the placebo group, were admitted to the intensive care unit. No child was intubated. Six children who were initially sent home were subsequently hospitalized (two in the placebo group, one in the budesonide group, and three in the dexamethasone group).\n\n【47】Table 5. Final Hospitalization Rates and Change in Croup Score.\n\n【48】The final rates of hospitalization were significantly lower in both glucocorticoid groups than in the placebo group . The unadjusted rates of hospitalization in the dexamethasone and budesonide groups were not significantly different from one another (P=0.18). Logistic-regression analyses that included adjustment for base-line croup score and study nurse yielded P values ranging from 0.03 to 0.12.\n\n【49】Clinical Assessments\n--------------------\n\n【50】Study nurses and physicians concurrently determined the croup score in 33 children. The agreement between raters was high (weighted kappa value, 0.9; 95 percent confidence interval, 0.8 to 1.0). The mean changes in the croup score were significantly greater in both glucocorticoid groups than in the placebo group, and the change in the dexamethasone group was significantly greater than that in the budesonide group before adjustment for covariates . Adjustment for covariates did not qualitatively alter any of these findings.\n\n【51】The changes in heart rate and respiratory rate from base line to the last assessment in each of the treatment groups mirrored the changes in the croup scores. The placebo group had the smallest mean change in heart rate (–8±25 beats per minute) and respiratory rate (–5±10 breaths per minute), with intermediate changes in the budesonide group (–16±23 beats per minute and –7±7 breaths per minute, respectively), and the largest changes in the dexamethasone group (–26±23 beats per minute and –9±8 breaths per minute, respectively). However, the only significant differences between treatments on pairwise comparisons were the change in heart rate for the comparison of dexamethasone with placebo (P<0.001) and for the comparison of dexamethasone with budesonide (P=0.005) and the change in respiratory rate for the comparison of dexamethasone with placebo (P=0.03). The changes in oxygen saturation and blood pressure were similar in all groups.\n\n【52】Other Treatments\n----------------\n\n【53】In addition to receiving mist therapy, eight children received an additional racepinephrine treatment within two hours after the administration of the study drug (four in the placebo group, three in the budesonide group, and one in the dexamethasone group). Two to five hours after treatment, 29 children received one or more additional racepinephrine treatments: 16 in the placebo group, 9 in the budesonide group, and 4 in the dexamethasone group. The unadjusted differences between groups were as follows: P=0.16 for the comparison of budesonide with placebo, P=0.005 for the comparison of dexamethasone with placebo, and P=0.23 for the comparison of budesonide with dexamethasone. Logistic-regression analyses that included adjustment for the base-line croup score and study nurse yielded P values ranging from 0.03 to 0.09 for the comparison of budesonide with placebo and from 0.05 to 0.13 for the comparison of budesonide with dexamethasone.\n\n【54】Adverse Effects\n---------------\n\n【55】No child had gastrointestinal bleeding or bacterial tracheitis. \n\n【56】Discussion\n----------\n\n【57】In our randomized, double-blind, placebo-controlled trial involving children with moderately severe croup, the hospitalization rate among those treated with glucocorticoids was less than half that among those given placebo. Although the differences in the rates could be due to factors other than the drug therapy, those factors should have been distributed evenly among the treatment groups. Furthermore, the consistency in the differences between groups for all clinical outcomes — croup score, heart and respiratory rates, doses of epinephrine, and admissions to the intensive care unit — suggests that the differences in the rates of hospitalization were due to glucocorticoid therapy.\n\n【58】The rate of hospitalization in the placebo group in our study was substantially higher than that reported by Klassen et al. (71 percent vs. 22 percent),  even though we used a similar croup scoring system.  The most likely explanation for the difference is that our clinical assessments were carried out when the children were quiet and alert, which resulted in the enrollment of sicker children than those in their study.\n\n【59】Treatment with intramuscular dexamethasone led to a significantly greater clinical improvement, as measured by the croup score, than did treatment with nebulized budesonide. It was surprising that dexamethasone was apparently more effective, because the concentration of budesonide in respiratory tissue increases much more rapidly after nebulized administration than does the concentration of dexamethasone after intramuscular administration.  However, this finding is unlikely to be due to inadequate delivery of budesonide. The administration of budesonide with racepinephrine should not have significantly affected delivery.  Also, the children were closely observed to ensure compliance, the dose was twice that administered in previous studies,  and the low flow rate and type of nebulizer used should have provided reasonably adequate delivery to the upper airway. \n\n【60】There were some deviations from the study protocol. A majority, however, involved the administration of dexamethasone, which, if anything, should have biased the results toward the null hypothesis — i.e., that there was no difference between the treatment groups. In contrast, the unmasking of the treatment assignment might have biased the results toward finding a difference. However, when we excluded these children from the analysis, the results were qualitatively similar to those obtained with the intention-to-treat analysis.\n\n【61】Another noteworthy aspect of our study design was the identification of viral causes and the type of croup. Several reviews have suggested that the failure to identify these factors was a major methodologic flaw in previous studies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9a051a5e-1d6a-4a82-8edf-bc5dfcabe37e", "title": "Bile Composition at and after Surgery in Normal Persons and Patients with Gallstones — Influence of Cholecystectomy", "text": "【0】Bile Composition at and after Surgery in Normal Persons and Patients with Gallstones — Influence of Cholecystectomy\nAbstract\n--------\n\n【1】The biliary lipid composition of hepatic and gallbladder bile obtained at surgery was compared in 11 patients with cholesterol gallstones and in control subjects: 10 with pigment stones and 10 without biliary disease. Indwelling T-tubes allowed daily postoperative collections of bile for two to 20 weeks in seven patients with cholesterol gallstones and 11 controls. Patients with cholesterol gallstones (a) secreted hepatic bile that was supersaturated and clearly different from that in controls at surgery; and (b) after cholecystectomy, produced bile with a composition identical to that of controls without gallstones. Thus, the gallbladder is essential for the continuous production of lithogenic hepatic bile. To define the hepatic defect further, bile-salt synthesis was determined from dependent T-tube drainage in 10 patients with cholesterol stones. Under the stimulus of an interrupted enterohepatic circulation, hepatic synthesis increased fourfold. Since synthesis can increase in response to loss, the hepatic defect appears to be in feedback inhibition and requires the presence of the gallbladder.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "de87dc1b-d9e9-4e8a-93fe-7bf250291f82", "title": "Ibrutinib Resistance in Chronic Lymphocytic Leukemia", "text": "【0】Ibrutinib Resistance in Chronic Lymphocytic Leukemia\nTo the Editor:\n--------------\n\n【1】Ibrutinib, an inhibitor that binds covalently to C481 of Bruton's tyrosine kinase (BTK), has produced remarkable responses in patients with relapsed and refractory chronic lymphocytic leukemia (CLL).  However, 5.3% of patients have disease progression, and the mechanism of resistance is largely unknown. Herein we describe the mechanism of resistance in such a case.\n\n【2】A 49-year-old woman had a diagnosis of CLL established in 2000. After the failure of multiple treatments, she began receiving ibrutinib at a dose of 560 mg daily in 2010 as part of a phase 1, dose-escalation study of ibrutinib in B-cell cancers.  By month 11, a partial response was achieved with an absolute lymphocyte count of 4530 cells per cubic millimeter. Computed tomography at month 18 showed a marked but incomplete reduction of lymphadenopathy. At month 21, a rapidly rising lymphocyte count and progressive lymphadenopathy were noted. Despite a dose escalation to 840 mg daily, CLL progressed during the next 4 weeks . Peripheral-blood samples were collected before ibrutinib administration (day −52), while the patient was having a response to the drug (day 472), when progressive disease was first noted (day 589), and before dose escalation (day 616). Fig. S1 in the Supplementary Appendix shows the dates of sample collection in relation to the patient's absolute lymphocyte count over the treatment course.\n\n【3】RNA sequencing revealed a thymidine-to-adenine mutation at nucleotide 1634 of the BTK complementary DNA (cDNA) (GenBank accession number, NM\\_000061.2), leading to a substitution of serine for cysteine at residue 481 (C481S). The mutation was detected in the samples collected when progressive disease was first noted (88% of reads) and before dose escalation (92% of reads) but not in those collected before ibrutinib administration or while the patient was having a response . No other genetic changes were identified that correlated with the patient's clinical course in the same manner as the BTK mutation. Sanger sequencing of cDNA verified that the mutation was detected only in the samples collected during relapse . A more sensitive, allele-specific polymerase-chain-reaction assay (1% analytic sensitivity) detected the mutation in the genomic DNA of samples collected during relapse but not in those collected before ibrutinib administration or while the patient was having a response .\n\n【4】Figure 1. Effect of C481S Mutation of Bruton's Tyrosine Kinase (BTK) on Ibrutinib Binding and the Ability of Ibrutinib to Inhibit BTK Phosphorylation.\n\n【5】Panel A shows structural modeling of nonmutant and mutant BTK with ibrutinib. The red arrows indicate the covalent bond between ibrutinib (purple and blue) and BTK (green and yellow) before and after the mutation. Panel B shows the inhibition of nonmutant BTK or C481S BTK phosphorylation by ibrutinib in HEK 293 cells. The half-maximal inhibitory concentration (IC <sub>50 </sub> ) of ibrutinib for inhibition of BTK phosphorylation was analyzed and plotted with GraphPad Prism. GFP denotes green fluorescent protein.\n\n【6】Ibrutinib binds covalently to the sulfhydryl group of C481 of BTK in the active site, resulting in irreversible inhibition of its kinase activity.  Structural modeling suggested that the C481S mutation would disrupt this covalent binding, changing irreversible binding to reversible binding . Fluorescently tagged ibrutinib labeled the nonmutant BTK, and the covalent binding that was formed withstood electrophoresis, whereas reversible binding to the C481S or C481A mutant of BTK did not. This showed biochemically the critical role of cysteine in covalent-bond formation .\n\n【7】Phosphorylation of BTK (pY223) reflects BTK kinase activity. Introduction of the recombinant nonmutant and C481S BTK constructs into HEK 293 cells showed that phosphorylation of C481S BTK at Y223 became significantly less sensitive to ibrutinib inhibition than the nonmutant BTK did (half-maximal inhibitory concentration, 1006 nM vs. 2.2 nM) .\n\n【8】Taken together, our data indicate that the C481S mutation disrupts the covalent binding between BTK and ibrutinib. The impaired binding leads to a loss of inhibition of BTK enzymatic activity that ultimately results in ibrutinib resistance in the patient. Consistent with the findings reported in the _Journal_ by Woyach et al.,  our studies confirm that BTK is a relevant pharmacologic target of ibrutinib from a genetic perspective.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "facafef9-44ca-4063-b976-4598d42ec7c6", "title": "In Vivo Activated T Lymphocytes in the Peripheral Blood and Cerebrospinal Fluid of Patients with Multiple Sclerosis", "text": "【0】In Vivo Activated T Lymphocytes in the Peripheral Blood and Cerebrospinal Fluid of Patients with Multiple Sclerosis\nAbstract\n--------\n\n【1】We found an increase in peripheral-blood lymphocytes bearing the T-cell–specific activation antigen Ta1 in 20 of 35 patients with progressive multiple sclerosis, 4 of 18 patients with stable or improving multiple sclerosis, 1 of 17 patients with other neurologic diseases, and 1 of 14 normal controls (P<0.0002, Fisher's exact test). No increases in two other markers of T-cell activation, T11 <sub>3 </sub> and the interleukin-2 receptor, were found.\n\n【2】In the cerebrospinal fluid, patients with progressive multiple sclerosis (pleocytosis, 3.9±1.6 cells per cubic millimeter) had 42±3.0 per cent Ta1 <sup>+ </sup> cells. In contrast, patients with other inflammatory central nervous system diseases (36±13 cells per cubic millimeter) had 9.6±1.8 per cent Ta1 <sup>+ </sup> cells (P<0.01). In patients with other neurologic diseases without inflammation (0.7±0.16 cells per cubic millimeter), the percentage of Ta1 <sup>+ </sup> cells was equivalent to that in patients with multiple sclerosis (39±5.4 per cent), although the absolute number was lower. There was a positive correlation between the presence of Ta1 <sup>+ </sup> cells in the spinal fluid and blood of patients with other neurologic diseases, but not patients with multiple sclerosis. Less than 1 per cent of lymphocytes from the spinal fluid of patients with multiple sclerosis expressed interleukin-2 receptors, as compared with 9.8 per cent of cells from subjects with other inflammatory neurologic diseases (P<0.01).\n\n【3】These results suggest that the T cells in the spinal fluid of patients with multiple sclerosis may be activated by a different mechanism or in a different temporal sequence from that in patients with other nervous system diseases. Furthermore, the increase in Ta1 <sup>+ </sup> cells in the peripheral blood of patients with multiple sclerosis demonstrates systemic immune activation in the disease; monitoring such cells may provide an objective measure of abnormal immunologic activity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a442be4d-e35b-41cb-b705-596cb2239138", "title": "Multiple Cerebral Hemorrhages in a Patient Receiving Lecanemab and Treated with t-PA for Stroke", "text": "【0】Multiple Cerebral Hemorrhages in a Patient Receiving Lecanemab and Treated with t-PA for Stroke\nTo the Editor:\n--------------\n\n【1】The results of the phase 3 trial of the anti–amyloid-β drug lecanemab for the treatment of early Alzheimer’s disease, reported in the _Journal_ by van Dyck et al.,  have suggested a beneficial effect on cognition scores and daily activities over a period of 18 months. An extension phase of the trial is ongoing. We report a case of numerous acute intracerebral hemorrhages that developed after treatment with intravenous tissue plasminogen activator (t-PA) for acute ischemic stroke syndrome in a patient who received three doses of intravenous lecanemab.\n\n【2】A 65-year-old patient who was homozygous for the _APOE_ ε4 allele and was in the early stages of cognitive decline presented to an emergency department 30 minutes after the acute onset of aphasia and left gaze preference due to an ischemic stroke. The patient had participated in the randomized phase of the trial of lecanemab, during which the treatment assignment is not known, followed by participation in the open-label phase, in which three intravenous lecanemab infusions were received (one infusion every 2 weeks), with the latest infusion administered 4 days before the stroke. Magnetic resonance imaging (MRI) of the head that had been performed 81 days before the stroke showed mild small-vessel disease, with no microhemorrhages, edema, or amyloid-related imaging abnormalities , and computed tomography (CT) performed just before t-PA administration showed hypodensities in the left temporal–parietal regions and a distal left middle cerebral artery branch occlusion but no hemorrhage. (Further clinical information is provided in the Supplementary Appendix .)\n\n【3】Figure 1. MRI and Neuropathological Findings.\n\n【4】Panel A shows a magnetic resonance imaging (MRI) susceptibility-weighted sequence in which extensive multifocal cortical intraparenchymal hemorrhages are visible. Panel B shows an MRI T2 fluid-attenuated inversion recovery (FLAIR) sequence in which extensive cerebral cortical and subcortical edema is seen in association with multifocal hemorrhages, as well as a right thalamocapsular acute ischemic infarct. Panel C shows a coronal section of the formalin-fixed cerebral hemispheres in which numerous cortical intracerebral hemorrhages are present. Panel D shows a representative hematoxylin and eosin–stained section of the left parietal cortex, in which a blood vessel with probable amyloid angiopathy and histiocytic infiltration of the blood-vessel wall is visible. Multinucleated histiocytes (arrowhead) and focal fibrinoid degeneration (arrow) are present. Panel E shows amyloid-β immunohistochemical staining of a cortical blood vessel affected by cerebral amyloid angiopathy. The vascular amyloid is fragmented, and the blood-vessel wall shows infiltration by lymphocytes and histiocytes.\n\n【5】The patient had no contraindications to thrombolysis (blood pressure, 163/84 mm Hg; platelet count, 256×10  per microliter; international normalized ratio, 1.0; fibrinogen level, 304 milligrams per deciliter) and was within the conventional time window for thrombolysis. After intravenous administration of an 8-mg t-PA bolus and 50 minutes into the t-PA infusion (when 65.7 mg of the total dose of 76 mg had been administered), hypertension suddenly developed (blood pressure, 250/111 mm Hg) and the t-PA infusion was stopped. A CT scan showed extensive, multifocal intraparenchymal hemorrhages. There was no systemic bleeding. Cryoprecipitate and tranexamic acid were administered. The patient had global aphasia and severe agitation; frequent, nonconvulsive seizures seen on electroencephalography were treated successfully with multiple antiseizure medications. Three days after presentation for the stroke, the patient underwent endotracheal intubation. MRI of the head showed acute right thalamocapsular infarction and innumerable multifocal cortical and subcortical hemorrhages with surrounding edema . The patient was treated with comfort measures at the request of the family and subsequently died. The autopsy showed extensive multifocal intraparenchymal hemorrhages, cerebral amyloid angiopathy, “high” Alzheimer’s disease neuropathologic changes,  and diffuse histiocytic vasculitis with necrotizing vasculopathy involving amyloid deposition within (but not outside) the blood-vessel walls .\n\n【6】The extensive number and variation in sizes of the cerebral hemorrhages in this patient would be unusual as a complication of t-PA solely related to cerebrovascular amyloid. The findings raise the possibility of cerebral hemorrhages and necrotizing vasculopathy associated with t-PA infusion in a patient with cerebrovascular amyloid who had received lecanemab.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "52899464-e954-4c1d-a9ae-5ea60e3d4253", "title": "Migraine", "text": "【0】Migraine\nThe trigeminal nerve and its projections to the intracranial vasculature — the trigeminovascular system — are at the nexus of migraine. Identification of the mechanisms that trigger signals in this system have led to targeted treatments and preventive therapies for migraine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "82ae1727-2a33-4f7c-a0cb-bd772344356e", "title": "Reduction of Mortality in Chloramphenicol-Treated Severe Typhoid Fever by High-Dose Dexamethasone", "text": "【0】Reduction of Mortality in Chloramphenicol-Treated Severe Typhoid Fever by High-Dose Dexamethasone\nAbstract\n--------\n\n【1】We compared high-dose dexamethasone (initial dose, 3 mg per kilogram of body weight) with placebo in a randomized, double-blind trial involving 38 patients with culture-positive, specifically defined severe typhoid fever. The patients in the two treatment groups ranged in age from 5 to 54 and were comparable at the outset. All patients received chloramphenicol. The case-fatality rate of 10 per cent (2 of 20 patients) in the dexamethasone group was significantly lower than the fatality rate of 55.6 per cent (10 of 18) in the placebo group (P = 0.003). There was no significant difference in the incidence of complications among the survivors in either group. Delirium, obtundation, and stupor were grave prognostic signs that were useful for predicting which patients were at high risk of dying before they became comatose or went into shock. Dexamethasone is unnecessary for most patients with typhoid but is recommended for all patients with suspected typhoid fever who are delirious, obtunded, stuporous, comatose, or in shock.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e986fcda-ea2b-49c6-9461-8c389bd2a8cc", "title": "Unfashionably Late", "text": "【0】Unfashionably Late\nAn 18-year-old man presented with shortness of breath, a cough that was productive of clear sputum, and a two-week history of pleuritic chest pain. He also reported night sweats, fever, and fatigue, but no hemoptysis, weight loss, recent travel, or new environmental exposures.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "644923f6-873f-415d-a7b6-752b7bb892dd", "title": "Are We Making Progress in Maternal Mortality?", "text": "【0】Are We Making Progress in Maternal Mortality?\nThe number of women who die during pregnancy or childbirth has decreased by more than a third globally since 1990. Yet progress has been uneven: while some countries have seen significant improvements, others have seen marked increases in maternal mortality.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6e2b42ff-7731-4171-87f0-6b60ab556ca9", "title": "Effect of Blood Glucose Control on Increased Glomerular Filtration Rate and Kidney Size in Insulin-Dependent Diabetes", "text": "【0】Effect of Blood Glucose Control on Increased Glomerular Filtration Rate and Kidney Size in Insulin-Dependent Diabetes\nAbstract\n--------\n\n【1】To investigate the relation between blood glucose control on the one hand and an increased glomerular filtration rate and enlarged kidneys on the other, we studied 12 patients with insulin-dependent diabetes and an increased glomerular filtration rate for a year after they were randomly assigned either to continuous subcutaneous insulin infusion or to unchanged conventional therapy.\n\n【2】Glycemie control, measured by mean plasma concentrations of glucose and glycosylated hemoglobin, was rapidly and significantly improved (P<0.001) in the pump group but did not change in the conventional-treatment group. In the pump group, the glomerular filtration rate fell significantly in the study period (P<0.001) and became normal in four of the six patients. It did not change in the conventional-treatment group. There was no change in kidney volume in either group. At the end of a year, a return to conventional insulin treatment in the pump group resulted in both metabolic deterioration and a significant rise in the mean glomerular filtration rate towaro  base-line values.\n\n【3】We conclude that in patients with established insulindependent diabetes, strict glycemie control normalizes the glomerular filtration rate, although the kidneys may remain enlarged.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "64a82e07-53af-47c3-86b3-cda9047cf64e", "title": "Maternal Alcohol Use during Breast-Feeding and Infant Mental and Motor Development at One Year", "text": "【0】Maternal Alcohol Use during Breast-Feeding and Infant Mental and Motor Development at One Year\nAbstract\n--------\n\n【1】The detrimental effects of maternal drinking during pregnancy on fetal health have been documented. The consequences for infants of maternal drinking during breast-feeding are unknown, but research in animals suggests that the infant could be affected by exposure to alcohol through the mother's milk. In a study of 400 infants born to members of a health maintenance organization, we investigated the relation of the mother's use of alcohol during breast-feeding to the infant's development at one year of age.\n\n【2】Mental development, as measured by the Bayley Mental Development Index (MDI), was unrelated to maternal drinking during breast-feeding. However, motor development, as measured by the Psychomotor Development Index (PDI), was significantly lower in infants exposed regularly to alcohol in breast milk (after alcohol exposure during gestation was controlled for), with a dose–response relation (P for linear trend, 0.006). The infants of breast-feeding mothers who had at least one drink daily had a mean PDI score of 98, whereas the infants exposed to less alcohol in breast milk had a mean PDI score of 103 (95 percent confidence interval for the difference of the two means, 1.2 to 9.8). The effect was more pronounced when mothers who supplemented breast-feeding with formula were excluded from the analysis. The association persisted even after we controlled for more than 100 potentially confounding variables, including smoking and other drug use during pregnancy and in the postpartum period.\n\n【3】We conclude that ethanol ingested through breast milk has a slight but significant detrimental effect on motor development, but not mental development, in breast-fed infants.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "400b05b8-7461-4754-ab76-95fc3e89c966", "title": "Use of Cocaine during Pregnancy", "text": "【0】Use of Cocaine during Pregnancy\nTo the Editor:\n--------------\n\n【1】Exposure of the fetus to cocaine may lead to serious destructive lesions in the brain  . Results of brain imaging studies in newborns have revealed that ischemic cerebral lesions may occur, leading to clinical manifestations when large. Fetal death in women using cocaine during pregnancy has been most frequently related to abruptio placentae. We report a case of fetal death with generalized cerebral ischemia and necrosis that was probably related to maternal abuse of cocaine in late pregnancy.\n\n【2】A 28-year-old woman with a history of cocaine abuse became pregnant in February 1992 and was followed at our antenatal clinic from the 13th week of gestation. At that time, she stated that she had stopped using cocaine two months earlier. Urine toxicologic screening for cocaine was negative at 13 and 30 weeks of gestation. The course of the pregnancy was initially uneventful, with normal laboratory tests and fetal ultrasonography at 13, 20, and 32 weeks. At 39 weeks of gestation, fetal death was diagnosed by auscultation during a routine visit and confirmed by ultrasonography. The next day, a dead male infant weighing 2700 g was delivered. The patient stated that she had been taking cocaine by nasal insufflation during the previous three weeks. Blood and urine tests for cocaine were positive. On fetal autopsy, signs of generalized cerebral ischemia and edema were found, with extensive infarction in the distribution of major cerebral vessels.\n\n【3】In rare instances, cerebral infarction may lead to death in fetuses exposed to cocaine. Ischemia caused by transient vasoconstriction has been proposed to explain common findings associated with cocaine use during pregnancy. As in adults, vasospasm may be crucial in the genesis of ischemic cerebrovascular phenomena in the fetus.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1e4d1ff7-5b8f-44fa-83c0-6d86c160492a", "title": "Postpartum Depression", "text": "【0】Postpartum Depression\nPostpartum depression is common and affects the woman, infant, and family. Treatment depends on the severity of symptoms and the level of functional impairment and can include social support, psychological therapy, and pharmacotherapy (generally an SSRI as first-line treatment).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0a729af9-754e-45b5-909e-52d03968b804", "title": "Fomepizole for Ethylene Glycol and Methanol Poisoning", "text": "【0】Fomepizole for Ethylene Glycol and Methanol Poisoning\nA 35-year-old man is brought to the emergency department after ingesting automobile antifreeze. His pH is 7.30, and his urinalysis reveals calcium oxalate crystals, findings consistent with ethylene glycol poisoning. Treatment with fomepizole is recommended. Fomepizole is a competitive inhibitor of alcohol dehydrogenase that prevents the formation of the toxic metabolites of ethylene glycol and methanol.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "679de98f-cf76-48a9-8e62-d000d23e61ad", "title": "The Genetic Archaeology of Influenza", "text": "【0】The Genetic Archaeology of Influenza\nA recent study suggests that the hemagglutinin protein of the 1918–1919 influenza (Spanish influenza) pandemic led to the hemorrhagic pneumonia and hence a high level of mortality associated with Spanish influenza. This finding, together with the epidemiology of Spanish flu, has implications for response to future pandemics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2a6cd5d7-bf59-4336-a59c-4af6203c48af", "title": "Prostaglandins as Mediators of Hypercalcemia Associated with Certain Types of Cancer", "text": "【0】Prostaglandins as Mediators of Hypercalcemia Associated with Certain Types of Cancer\nAbstract\n--------\n\n【1】We investigated the role of prostaglandins in the hypercalcemia associated with neoplasia. In patients with hypercalcemia and solid tumors the excretion of the major urinary metabolite of the E prostaglandins, 7 α-hydroxy-5,11-diketotetranorprostane-1,16-dioic acid (PGE-M), was significantly greater than normal, P < 0.01 (median of 58.4 and 7.1 ng per milligram of creatinine respectively). Slightly elevated values were seen in normocalcemic patients with solid tumors (14.3 ng per milligram). The levels of the metabolite were normal in hypercalcemic patients with either hematologic neoplasia or primary hyperparathyroidism. Immunoreactive parathyroid hormone was undetectable in the plasma of all hypercalcemic patients with solid tumors. Inhibition of prostaglandin synthesis by aspirin or indomethacin reduced excretion of both the urinary metabolite and serum calcium in six hypercalcemic patients with solid tumors and elevated excretion of the metabolite. These findings support the concept that prostaglandins are mediators of the hypercalcemia caused by certain solid tumors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f87d1640-7dd6-4a25-adb2-382e810f733b", "title": "Early versus Standard Antiretroviral Therapy for HIV-Infected Adults in Haiti", "text": "【0】Early versus Standard Antiretroviral Therapy for HIV-Infected Adults in Haiti\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】For adults with human immunodeficiency virus (HIV) infection who have CD4+ T-cell counts that are greater than 200 and less than 350 per cubic millimeter and who live in areas with limited resources, the optimal time to initiate antiretroviral therapy remains uncertain.\n\n【3】Methods\n-------\n\n【4】We conducted a randomized, open-label trial of early initiation of antiretroviral therapy, as compared with the standard timing for initiation of therapy, among HIV-infected adults in Haiti who had a confirmed CD4+ T-cell count that was greater than 200 and less than 350 per cubic millimeter at baseline and no history of an acquired immunodeficiency syndrome (AIDS) illness. The primary study end point was survival. The early-treatment group began taking zidovudine, lamivudine, and efavirenz therapy within 2 weeks after enrollment. The standard-treatment group started the same regimen of antiretroviral therapy when their CD4+ T-cell count fell to 200 per cubic millimeter or less or when clinical AIDS developed. Participants in both groups underwent monthly follow-up assessments and received isoniazid and trimethoprim–sulfamethoxazole prophylaxis with nutritional support.\n\n【5】Results\n-------\n\n【6】Between 2005 and 2008, a total of 816 participants — 408 per group — were enrolled and were followed for a median of 21 months. The CD4+ T-cell count at enrollment was approximately 280 per cubic millimeter in both groups. There were 23 deaths in the standard-treatment group, as compared with 6 in the early-treatment group (hazard ratio with standard treatment, 4.0; 95% confidence interval \\[CI\\], 1.6 to 9.8; P=0.001). There were 36 incident cases of tuberculosis in the standard-treatment group, as compared with 18 in the early-treatment group (hazard ratio, 2.0; 95% CI, 1.2 to 3.6; P=0.01).\n\n【7】Conclusions\n-----------\n\n【8】Early initiation of antiretroviral therapy decreased the rates of death and incident tuberculosis. Access to antiretroviral therapy should be expanded to include all HIV-infected adults who have CD4+ T-cell counts of less than 350 per cubic millimeter, including those who live in areas with limited resources. \n\n【9】Introduction\n------------\n\n【10】The optimal time to initiate antiretroviral therapy in adults who are infected with human immunodeficiency virus (HIV) remains uncertain. There have been no randomized trials to determine the optimal time to start antiretroviral therapy in adults who have CD4+ T-cell counts that are greater than 200 and less than 350 per cubic millimeter. Furthermore, there are few data on the optimal time to start antiretroviral therapy in persons who live in locations with limited resources, where high rates of tuberculosis, malnutrition, and coinfection with tropical diseases may alter the natural history of HIV disease and the optimal time to initiate therapy. Therefore, international guidelines differ on when to start antiretroviral therapy. \n\n【11】In Haiti, following World Health Organization (WHO) guidelines, the first-line regimen of antiretroviral therapy, which consists of zidovudine, lamivudine, and efavirenz, is initiated when the CD4+ T-cell count in a patient with HIV type 1 (HIV-1) infection is 200 per cubic millimeter or less or when clinical acquired immunodeficiency syndrome (AIDS) develops.  Among patients who are treated according to this standard strategy for the initiation of antiretroviral therapy, approximately 80% are alive at 5 years.  We conducted a randomized clinical trial in Haiti to determine whether early initiation of antiretroviral therapy, as compared with the standard timing for the initiation of therapy, improves survival.\n\n【12】Methods\n-------\n\n【13】Study Design and Setting\n------------------------\n\n【14】We conducted a randomized, open-label, controlled trial of early initiation of antiretroviral therapy, as compared with the standard timing for initiation of therapy, among HIV-infected adults with a CD4+ T-cell count that was greater than 200 and less than 350 per cubic millimeter and no history of an AIDS illness. The primary study end point was survival. The study was conducted at the center of the Haitian Group for the Study of Kaposi's Sarcoma and Opportunistic Infections (GHESKIO) in Port au Prince, Haiti.  The study was approved by the institutional review boards at Weill Cornell Medical College and GHESKIO. Two of the antiretroviral medications, zidovudine and lamivudine, were donated by GlaxoSmithKline, and one, lopinavir boosted by ritonavir, was donated by Abbott. (The other antiretroviral medications were donated by the Global Fund to Fight AIDS, Tuberculosis, and Malaria.) Neither GlaxoSmithKline nor Abbott had a role in the design of the study, the accrual or analysis of the data, or the preparation of the manuscript.\n\n【15】Inclusion and Exclusion Criteria\n--------------------------------\n\n【16】Subjects could be included in the study if they were infected with HIV, were at least 18 years of age, and had a confirmed CD4+ T-cell count that was greater than 200 and less than 350 per cubic millimeter within 45 days before enrollment. Subjects were excluded if they had a history of an AIDS-defining illness (stage 4 in the WHO staging system)  or had received antiretroviral therapy previously.\n\n【17】Recruitment and Randomization\n-----------------------------\n\n【18】Subjects were recruited at GHESKIO from August 2005 through July 2008. After the subjects had provided written informed consent,  the study team performed a screening evaluation, and eligible subjects were enrolled. Participants were randomly assigned with the use of a computer-generated random-numbers list, in a  ratio, to either early initiation of treatment (early-treatment group) or the standard timing for initiation of treatment (standard-treatment group).\n\n【19】Study Intervention\n------------------\n\n【20】Subjects in both groups were seen monthly by a clinician and received the package of services provided to all HIV-1–infected patients at GHESKIO. Prophylactic treatment with trimethoprim–sulfamethoxazole was administered in all participants,  and isoniazid was given to those who had a positive purified protein derivative (PPD) skin test.  Participants received nutritional support that consisted of daily multivitamins and a monthly food basket containing rice, beans, oil, and meat.  To encourage participants to continue medical follow-up and remain in the study, field workers visited their residences at the time of enrollment and in the case of a missed visit. Participants were counseled about adherence to therapy and about the importance of returning to the clinic whenever they had symptoms.\n\n【21】If a participant had a cough or symptoms that were suggestive of tuberculosis at any time during the study, a chest radiograph was obtained; in addition, three sputum smears were examined for acid-fast bacilli with the use of Ziehl–Neelsen staining and were cultured for _Mycobacterium tuberculosis_ on Löwenstein–Jensen medium.  Subjects with active tuberculosis received directly observed therapy consisting of daily administration of four drugs (isoniazid, rifampin, ethambutol, and pyrazinamide) for a 2-month initiation phase, followed by daily administration of two drugs (isoniazid and rifampin) for a 4-month continuation phase.  Subjects with drug-resistant tuberculosis were treated according to WHO guidelines. \n\n【22】The early-treatment group began treatment within 2 weeks after enrollment. Treatment consisted of lamivudine (150 mg every 12 hours) and zidovudine (300 mg every 12 hours), in a fixed-dose combination, and efavirenz (600 mg every 24 hours, at bedtime). For the first 2 months, antiretroviral therapy was provided under modified direct observation: receipt of the morning dose was observed at the study participant's home by a GHESKIO field worker; the evening dose was left with the participant and the participant was not observed while taking the medication.\n\n【23】If a drug caused toxic effects in a subject, the following drug substitution could be made: stavudine as a substitute for zidovudine and nevirapine or lopinavir, boosted by ritonavir, as a substitute for efavirenz. Among participants who were receiving rifampin for the treatment of active tuberculosis, the dose of efavirenz was increased from 600 mg every 24 hours to 800 mg every 24 hours. The failure of antiretroviral therapy was defined according to the WHO criteria: a confirmed decrease in the CD4+ T-cell count to a level that was 50% below the peak count or to a level below the baseline count, or a new AIDS illness during receipt of antiretroviral therapy.  Participants in whom first-line therapy failed were switched to the second-line regimen of abacavir, didanosine, and lopinavir boosted by ritonavir.\n\n【24】Participants in the standard-treatment group started therapy when they had a single CD4+ T-cell count of 200 per cubic millimeter or less or when an AIDS-defining illness developed. The treatment consisted of the same first-line regimen of antiretroviral therapy as that used for the early-treatment group (lamivudine, zidovudine, and efavirenz). In addition, for women in the standard-treatment group who became pregnant, antiretroviral therapy was initiated to prevent transmission of HIV to the fetus and was continued throughout the study, but nevirapine was substituted for efavirenz. Other drug substitutions and the second-line regimen were the same as those for the early-treatment group.\n\n【25】Study End Points\n----------------\n\n【26】The primary study end point was survival. Death was documented in one of the following ways: an obituary notice, an autopsy report, a hospital death certificate, or a report from a contact documenting oral communication with the subject's health care provider or family member. Incident tuberculosis was a secondary study end point. We used the case definition of the American Thoracic Society, as described previously. \n\n【27】Clinical and Laboratory Measurements\n------------------------------------\n\n【28】Adherence to antiretroviral medications was measured with the use of a questionnaire about medication adherence, described previously, that was translated into Haitian Creole. The questionnaire was administered every 6 months. \n\n【29】Serious adverse events and their relationship to antiretroviral medications were assessed, were graded according to the grading system in the National Institutes of Health, Division of AIDS, manual for the grading of adverse events, and were reported at the standard level of reporting.  We report all severe (grade 3) and life-threatening (grade 4) suspected drug reactions.\n\n【30】Laboratory tests that were performed included a baseline CD4+ T-cell count, a complete blood count, and measurements of aspartate aminotransferase, alanine aminotransferase, and creatinine. A complete blood count, liver-function tests, and serum chemical tests were repeated every 3 months for participants who were taking antiretroviral drugs. The CD4+ T-cell count was repeated for all participants every 6 months or more frequently, if requested by the primary care clinician.\n\n【31】Statistical Analysis\n--------------------\n\n【32】Information from clinical and laboratory case-report forms was entered electronically in Haiti through an Internet interface, and the data were managed by Frontier Science and Technology Research Foundation in New York. Data were exported into SAS software (SAS Institute) for analysis.\n\n【33】The study was designed to accumulate a fixed number of end points. We estimated that with a sample of 794 participants (397 per group), the study would have 80% power to show a hazard ratio for survival with early treatment of 2.0 after 65 deaths had occurred, with a two-sided type I error rate of 5%. Three interim analyses were scheduled, after 16, 32, and 48 deaths had occurred. The interim analyses, which were performed by investigators who were unaware of the treatment assignments, were reviewed by members of the data and safety monitoring board of the National Institutes of Health, Division of AIDS. Prespecified stopping rules were based on the O'Brien–Fleming boundary for significance, with Lan–DeMets flexible spending functions. \n\n【34】We hypothesized that early initiation of antiretroviral therapy, as compared with the standard timing for initiation of therapy, would improve survival. All analyses were based on the intention-to-treat principle. The primary study end point, survival, was analyzed with the use of standard Kaplan–Meier methods, and differences between the two survival curves were evaluated with the use of the log-rank test, as specified in the protocol.  Cox proportional-hazards regression analysis was used to estimate the hazard ratio with 95% confidence intervals. We used the same methods in the analysis of the secondary outcome of incident tuberculosis. For comparison of other proportions, we used Fisher's exact test. Two-sided hypotheses and tests were adopted for all statistical inferences.\n\n【35】Results\n-------\n\n【36】Recruitment and Baseline Characteristics\n----------------------------------------\n\n【37】Figure 1. Screening, Randomization, and Follow-up.\n\n【38】ART denotes antiretroviral therapy.Table 1.  Table 1. Baseline Characteristics of the Study Participants.\n\n【39】A total of 1066 subjects were screened between August 2005 and July 2008, and 816 were enrolled in the study . The median age of enrolled subjects was 40 years, and 470 (58%) were women. The median CD4+ T-cell count was 281 per cubic millimeter. The baseline characteristics were similar between the two groups .\n\n【40】Status at the Time of Analysis\n------------------------------\n\n【41】The data and safety monitoring board reviewed the second interim analysis, which included data accumulated up to May 1, 2009; there were 29 deaths at that point. The trial crossed the prespecified stopping boundary for a difference in survival between the groups, and the data and safety monitoring board recommended that the trial be stopped and that all participants in the standard-treatment group be given antiretroviral therapy.\n\n【42】The median length of follow-up was 21 months (range, 1 to 44). Of the 408 participants in the early-treatment group, 383 (94%) continued in follow-up to the end of the study, 6 (1%) died, and 19 (5%) were lost to follow-up. Of the 408 participants in the standard-treatment group, 367 (90%) were included in the follow-up assessments, 23 (6%) died, and 18 (4%) were lost to follow-up.\n\n【43】Of the 408 participants in the standard-treatment group, 160 (39%) met the criteria for initiation of antiretroviral therapy and began receiving antiretroviral drugs during the course of the study. Of the 408 participants in the standard-treatment group, 118 (29%) received isoniazid prophylaxis because of a positive tuberculin skin test and 400 (98%) received trimethoprim–sulfamethoxazole prophylaxis. Of the 408 participants in the early-treatment group, 99 (24%) received isoniazid prophylaxis and 388 (95%) received trimethoprim–sulfamethoxazole prophylaxis. Of the 327 participants in the early-treatment group who had at least 12 months of follow-up, 294 (90%) were adherent to antiretroviral therapy (i.e., received more than 95% of antiretroviral medications in the first year of antiretroviral therapy); of the 60 participants in the standard-treatment group who received antiretroviral therapy for at least 12 months, 57 (95%) were adherent to the therapy.\n\n【44】Survival\n--------\n\n【45】Figure 2. Kaplan–Meier Estimates of Survival in the Early-Treatment and Standard-Treatment Groups.\n\n【46】There were 29 deaths during the course of the study — 23 in the standard-treatment group and 6 in the early-treatment group (P=0.001 by the log-rank test). In the Kaplan–Meier analysis, 98% of the participants in the early-treatment group and 93% in the standard-treatment group were alive at 36 months . The unadjusted hazard ratio for the risk of death with standard treatment as compared with early treatment was 4.0 (95% confidence interval \\[CI\\], 1.6 to 9.8).\n\n【47】The causes of death among the 23 participants in the standard-treatment group who died were gastroenteritis (7 participants), tuberculosis (5), pneumonia (4), homicide (2), cancer (1), cardiomyopathy (1), cholangitis with sepsis (1), stroke (1), and suicide (1). The causes of death among the 6 participants in the early-treatment group who died were burn injury (1), gastroenteritis (1), myocardial infarction (1), pulmonary embolism after gynecologic surgery (1), stroke (1), and gastrointestinal bleeding (1). There was only 1 death from an infectious disease in the early-treatment group, as compared with 17 deaths in the standard-treatment group.\n\n【48】Antiretroviral therapy had been initiated in 7 of the 23 participants in the standard-treatment group who died. These seven participants died a median of 2 months after starting therapy.\n\n【49】Incident Tuberculosis\n---------------------\n\n【50】Figure 3. Kaplan–Meier Estimates of the Probability of Remaining Free from Active Tuberculosis in the Early-Treatment and Standard-Treatment Groups.\n\n【51】Among the 773 participants who did not have tuberculosis at enrollment, 54 received a diagnosis of tuberculosis during the follow-up period. There were 36 cases of incident tuberculosis in the standard-treatment group and 18 in the early-treatment group (P=0.01 by the log-rank test). In the Kaplan–Meier analysis of survival, tuberculosis developed by 36 months in 6% of the participants in the early-treatment group, as compared with 14% in the standard-treatment group . The hazard ratio for the risk of incident tuberculosis with standard treatment as compared with early treatment was 2.0 (95% CI, 1.2 to 3.6). Tuberculosis developed in 31 participants in the standard-treatment group before antiretroviral therapy was initiated and in 5 participants in the standard-treatment group a median of 6 months after the initiation of antiretroviral therapy. None of the participants in the early-treatment group, as compared with five of the participants in the standard-treatment group, died as a result of incident tuberculosis.\n\n【52】CD4+ T-Cell Count\n-----------------\n\n【53】The median CD4+ T-cell count in the early-treatment group increased from 280 per cubic millimeter at enrollment to 520 per cubic millimeter at month 36. The median CD4+ T-cell count in the standard-treatment group declined from 282 per cubic millimeter at baseline to 270 per cubic millimeter at month 36.\n\n【54】Initiation of Antiretroviral Therapy in the Standard-Treatment Group\n--------------------------------------------------------------------\n\n【55】Antiretroviral therapy was initiated in 160 of the 408 participants in the standard-treatment group (39%). In 147 of these participants, the CD4+ T-cell count fell to 200 per cubic millimeter or less; in 7 participants, an AIDS-defining illness developed; and in 3 participants, both an AIDS-defining illness developed and the CD4+ T-cell count fell to 200 per cubic millimeter or less. In addition, antiretroviral therapy was initiated in three pregnant women to prevent transmission of HIV infection to their offspring. Among the 160 participants in the standard-treatment group in whom antiretroviral therapy was initiated, the median CD4+ T-cell count at the start of antiretroviral therapy was 166 per cubic millimeter (interquartile range, 130 to 190).\n\n【56】Figure 4. Kaplan–Meier Estimates of Survival without Antiretroviral Therapy (ART) among the 408 Participants in the Standard-Treatment Group.\n\n【57】An additional 16 participants in the standard-treatment group died before antiretroviral therapy could be initiated. In the Kaplan–Meier analysis, the estimated median survival without antiretroviral therapy in the standard-treatment group was 24 months .\n\n【58】Drug Reactions\n--------------\n\n【59】Of the 408 participants in the early-treatment group, 32 (8%) had a severe or life-threatening drug reaction. Of the 160 participants in the standard-treatment group who received antiretroviral therapy, 18 (11%) had a severe or life-threatening drug reaction. Details of these drug reactions are provided in Table 2 in the Supplementary Appendix . Anemia associated with zidovudine therapy was the most common adverse drug reaction, occurring in 13 (8%) of the 160 participants in the standard-treatment group who received antiretroviral therapy and in 14 (3%) of the 408 participants in the early-treatment group.\n\n【60】Discussion\n----------\n\n【61】The results of this randomized, controlled trial show that among HIV-1–infected adults who live in resource-poor areas, antiretroviral therapy that is initiated when the CD4+ T-cell count is greater than 200 and less than 350 per cubic millimeter, as compared with antiretroviral therapy that is deferred until the CD4+ T-cell count falls to 200 per cubic millimeter or less or an AIDS-defining illness develops, results in a 75% reduction in the rate of death and a 50% decrease in the incidence of tuberculosis\n\n【62】Our finding that early antiretroviral therapy improves the rate of survival is consistent with data from observational studies.  The When To Start Consortium examined outcomes for more than 24,000 HIV-infected patients in 15 cohorts in Europe and North America. This study showed that with initiation of antiretroviral therapy when the CD4+ T-cell count was lower than 350 per cubic millimeter, as compared with initiation when the CD4+ T-cell count was 350 or higher, the hazard ratio for death was 1.4 to 2.0.  Our prospective study validates these observational findings in a randomized, controlled trial. Furthermore, the effect size in our trial, with a hazard ratio of 4.0, was larger than the effect size seen in the observational cohorts. One possible reason for the large effect size in our study is that it was conducted in a resource-poor setting, where high rates of tuberculosis, malnutrition, and coinfections with tropical diseases may exacerbate the effect of deferred therapy.\n\n【63】Early antiretroviral therapy also decreased the incidence of tuberculosis by 50% in our study. This finding is consistent with observational studies from Africa showing a decrease in the incidence of tuberculosis after antiretroviral therapy is started.  Tuberculosis is a leading cause of death among HIV-1–infected patients in developing countries,  and the effect of early antiretroviral therapy on the incidence of tuberculosis explains in part the decreased rate of death seen in our trial. Furthermore, the HIV epidemic has dramatically increased the incidence of active tuberculosis in countries with limited resources and is overwhelming tuberculosis-control programs.  Provision of early antiretroviral therapy on a large scale in areas with limited resources has the potential to decrease the incidence of active tuberculosis in the general population.\n\n【64】The WHO has promoted a public health approach in its guidelines to antiretroviral therapy, emphasizing feasibility, cost-effectiveness, and large-scale implementation.  Earlier initiation of antiretroviral therapy — when the CD4+ T-cell count is less than 350 per cubic millimeter — is likely to be consistent with this approach.  The median time to the initiation of antiretroviral therapy in our standard-treatment group was 2 years. At current pricing, 2 years of antiretroviral drugs will cost approximately $400 per person. Thus, for a cost of approximately $400 per person, the rate of death can be decreased by 75%, and the incidence of active tuberculosis by 50%. Furthermore, the standard-treatment group had higher rates of infectious diseases and of treatment-limiting drug reactions than did the early-treatment group and required frequent monitoring of CD4+ T-cell counts. This complex medical care consumed resources and the time of highly trained health care workers, factors that may in part offset the cost of starting antiretroviral therapy earlier.\n\n【65】Our study was not blinded. This would not affect the primary study end point of survival, but we cannot exclude the possibility that detection bias influenced the secondary end points. However, the rates at which participants remained in the study for follow-up assessments were high, and the fact that the rates were similar in the two groups suggests that the intensity of follow-up was similar.\n\n【66】In conclusion, early antiretroviral therapy decreased the rate of death by 75% in HIV-infected adults who had a CD4+ T-cell count that was greater than 200 and less than 350 per cubic millimeter. Access to antiretroviral therapy should be expanded to all HIV-infected adults who have a CD4+ T-cell count of less than 350 per cubic millimeter, including those who live in locations with limited resources.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9623567e-ff0a-458f-a689-5773a87c8783", "title": "Determinants of Atrial (S", "text": "【0】Determinants of Atrial (S\nAbstract\n--------\n\n【1】Clinical, hemodynamic and angiographic correlates in 23 patients with primary myocardial disease and either atrial (S <sub>4 </sub> ) or ventricular (S <sub>3 </sub> ) gallop sounds differentiated the two groups. Those with S4 gallop had normal mean left atrial pressures, low amplitude \"v\" waves, small drop from \"v\" to \"y\", usually reduced v/y slope, prominent \"a\" kick, abnormal echocardiogram of mitral valve, nearly normal cardiac output and marked left ventricular hypertrophy. In contrast the S <sub>3 </sub> group had elevated mean left atrial pressures, tall \"v\" waves and large drop from \"v\" to \"y\", with increased v/y slope, no prominent \"a\" kick, rapid filling on the echocardiogram, reduced cardiac output and left ventricular cavitary dilatation. Atrial gallop sound can thus be explained by ventricular hypertrophy, causing reduced compliance and increased resistance to filling, with augmented atrial boost and gallop, whereas considerable ventricular dilatation, with reduced cardiac output, elevated filling pressure and augmented early filling may produce ventricular gallop.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1fec557d-0ea8-46d3-98d2-003c007944bd", "title": "Case 26-2016 — A 28-Year-Old Woman with Back Pain and a Lesion in the Lumbar Spine", "text": "【0】Case 26-2016 — A 28-Year-Old Woman with Back Pain and a Lesion in the Lumbar Spine\nA 28-year-old woman presented with intractable back pain with radiation to the leg. Examination of biopsy specimens revealed a giant-cell tumor of bone. Management decisions were made.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "310df57e-3fcc-4912-a2b5-b9fbbf862cb9", "title": "Four Artemisinin-Based Treatments in African Pregnant Women with Malaria", "text": "【0】Four Artemisinin-Based Treatments in African Pregnant Women with Malaria\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Information regarding the safety and efficacy of artemisinin combination treatments for malaria in pregnant women is limited, particularly among women who live in sub-Saharan Africa.\n\n【3】Methods\n-------\n\n【4】We conducted a multicenter, randomized, open-label trial of treatments for malaria in pregnant women in four African countries. A total of 3428 pregnant women in the second or third trimester who had falciparum malaria (at any parasite density and regardless of symptoms) were treated with artemether–lumefantrine, amodiaquine–artesunate, mefloquine–artesunate, or dihydroartemisinin–piperaquine. The primary end points were the polymerase-chain-reaction (PCR)–adjusted cure rates (i.e., cure of the original infection; new infections during follow-up were not considered to be treatment failures) at day 63 and safety outcomes.\n\n【5】Results\n-------\n\n【6】The PCR-adjusted cure rates in the per-protocol analysis were 94.8% in the artemether–lumefantrine group, 98.5% in the amodiaquine–artesunate group, 99.2% in the dihydroartemisinin–piperaquine group, and 96.8% in the mefloquine–artesunate group; the PCR-adjusted cure rates in the intention-to-treat analysis were 94.2%, 96.9%, 98.0%, and 95.5%, respectively. There was no significant difference among the amodiaquine–artesunate group, dihydroartemisinin–piperaquine group, and the mefloquine–artesunate group. The cure rate in the artemether–lumefantrine group was significantly lower than that in the other three groups, although the absolute difference was within the 5-percentage-point margin for equivalence. The unadjusted cure rates, used as a measure of the post-treatment prophylactic effect, were significantly lower in the artemether–lumefantrine group (52.5%) than in groups that received amodiaquine–artesunate (82.3%), dihydroartemisinin–piperaquine (86.9%), or mefloquine–artesunate (73.8%). No significant difference in the rate of serious adverse events and in birth outcomes was found among the treatment groups. Drug-related adverse events such as asthenia, poor appetite, dizziness, nausea, and vomiting occurred significantly more frequently in the mefloquine–artesunate group (50.6%) and the amodiaquine–artesunate group (48.5%) than in the dihydroartemisinin–piperaquine group (20.6%) and the artemether–lumefantrine group (11.5%) (P<0.001 for comparison among the four groups).\n\n【7】Conclusions\n-----------\n\n【8】Artemether–lumefantrine was associated with the fewest adverse effects and with acceptable cure rates but provided the shortest post-treatment prophylaxis, whereas dihydroartemisinin–piperaquine had the best efficacy and an acceptable safety profile. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nThe PREGACT Study  \n\n【11】Malaria during pregnancy is a major public health problem in countries where the disease is endemic.  In areas in which the intensity of transmission is moderate to high and does not vary substantially from year to year, most malaria infections during pregnancy remain asymptomatic but increase the risk of maternal anemia and low birth weight, the latter of which is associated with increased infant mortality.  In areas where the intensity of transmission is low and varies substantially between years, symptomatic malaria and severe disease can develop in pregnant women, with an associated increased risk of fetal loss and maternal death.  Considering the harmful effects of malaria during pregnancy, it is extremely important to treat the disease adequately with efficacious medicines. However, little information is available regarding the pharmacokinetics, safety, and efficacy of new antimalarial agents in pregnant women  because pregnant women are systematically excluded from regulatory trials.\n\n【12】For women in the second or third trimester of pregnancy, World Health Organization (WHO) guidelines recommend a 3-day course with either an artemisinin-based combination therapy that is known to be effective in the country or region or clindamycin plus a 7-day course of either artesunate or quinine.  Although the experience regarding the use of artemisinin-based combination therapies in pregnancy is increasing,  this information is still limited, particularly in sub-Saharan Africa.\n\n【13】Methods\n-------\n\n【14】Trial Design\n------------\n\n【15】We conducted this randomized, open-label trial from June 2010 through August 2013 at seven sites in four sub-Saharan African countries: Burkina Faso (two sites), Ghana (three), Malawi (one), and Zambia (one). The trial protocol , which is available with the full text of this article at NEJM.org, has been described in detail elsewhere.  In brief, pregnant women in the second or third trimester who had a _Plasmodium falciparum_ monoinfection of any density, regardless of symptoms, a hemoglobin level of 7 g per deciliter or more, and no other serious illness were recruited into the trial and randomly assigned to one of the following four treatments: artemether–lumefantrine, amodiaquine–artesunate, mefloquine–artesunate, or dihydroartemisinin–piperaquine.\n\n【16】Sigma-Tau Industrie Farmaceutiche Riunite donated dihydroartemisinin–piperaquine, Novartis donated artemether–lumefantrine, and Sanofi-Aventis donated artesunate–amodiaquine. The Drugs for Neglected Diseases Initiative facilitated the negotiation for the procurement of artesunate–mefloquine from Farmanguinhos, which donated the treatment. The donors did not have any role in reviewing the protocol or the manuscript, although the protocol synopsis was provided to the manufacturers.\n\n【17】The trial was set up in a pragmatic approach with three treatment groups per country with the use of a balanced, incomplete block design, which allowed for the maximized use of resources and took into account the policies regarding antimalarial treatment in the respective countries . All doses of the study drugs were given under direct observation on days 0, 1, and 2 and according to the recommendations of the manufacturer . At recruitment, the gestational age was estimated with the use of the symphysiofundal height, and the fetal viability was assessed by means of ultrasonography with a portable multipurpose machine.\n\n【18】Follow-up\n---------\n\n【19】After completion of the 3-day treatment, patients were asked to return to the clinic (in Ghana, patients were visited at home) for follow-up visits on days 3 and 7 and then once every week until day 63. At each visit, a medical history was obtained, and information was collected regarding current signs and symptoms, including the start and end date, the severity (mild, moderate, severe, or life-threatening), and the perceived relationship to the study treatment (definitely unrelated, unlikely to be related, possibly related, probably related, or definitely related), as well as the outcome of any adverse events. A blood sample was obtained for malaria smears and dried blood spots for later genotyping, for full blood counts (on days 7, 14, 28, and 63 only), and for measurement of the total bilirubin, alanine aminotransferase, and creatinine levels (on days 7 and 14 only). Rescue treatment for recurrent infection was given according to national guidelines.\n\n【20】At the end of the active follow-up period, women were asked to attend the antenatal clinic monthly or when they felt unhealthy, until delivery. After delivery, the newborn was examined for congenital malformations and weighed, and the gestational age was estimated with the use of the total Ballard score (range, −10 \\[20 weeks of gestation\\] to 50 \\[44 weeks of gestation\\]).  A placental-biopsy specimen was obtained as soon as possible after delivery and was preserved in 10% neutral buffered formalin. The biopsy specimens were processed and embedded in paraffin wax by means of standard techniques and were kept at 4°C. Paraffin sections that were 4 mm thick were stained with hematoxylin and eosin and read at the Barcelona Center for International Health Research. \n\n【21】Laboratory Procedures\n---------------------\n\n【22】Giemsa-stained thick and thin blood films were read independently by two readers. Blood smears with discordant results (differences between the two microscopists with regard to the diagnosis of the species, positivity, or parasite density of >50%) were reexamined by a third, independent microscopist, and parasite density was calculated by averaging the two closest counts. We estimated parasite density by counting the number of asexual parasites per 200 white cells, assuming a white-cell count of 8000 per cubic millimeter. The total bilirubin, alanine aminotransferase, and creatinine levels were measured with the use of the Flexor Junior biochemistry analyzer. The full blood count was obtained with the use of the Sysmex XT-2000i hematology analyzer. The hemoglobin level was measured with the use of the HemoCue system. For polymerase-chain-reaction (PCR) analysis, blood samples were collected on filter papers (Whatman 3MM) that were subsequently transported to the Institute of Tropical Medicine, Antwerp, Belgium, where centralized genotyping (of the glutamate-rich protein \\[GLURP\\] and surface proteins of the _P. falciparum_ merozoite \\[MSP2 and MSP1\\]) was performed to distinguish reinfection from recrudescence.  Samples that did not produce a result were classified as indeterminate.\n\n【23】Trial End Points\n----------------\n\n【24】The primary end points of the trial were the PCR-adjusted cure rates at day 63  and the safety outcomes  . In the estimation of the PCR-adjusted cure rate, only recurrent infections that were shown by means of genotyping to be the same infections as those before treatment (i.e., recrudescent infections) were considered to be treatment failures; conversely, for the estimation of the PCR-unadjusted cure rate, all recurrent infections were considered to be treatment failures.\n\n【25】Treatment failures were classified as either early or late treatment failures, with the latter category comprising late clinical failures and late parasitologic failures. Early treatment failure was defined as one of the following: the development of danger signs or severe malaria or worsening of clinical conditions on day 0, 1, 2, or 3 in the presence of parasitemia; parasitemia on day 3 that was the same as or greater than the count on day 0; or parasitemia on day 3 and fever (axillary temperature, ≥37.5°C). Late clinical failure was defined as the either the development of danger signs or severe malaria or worsening of clinical conditions on any day after day 3 in the presence of parasitemia, without the patient having previously met any of the criteria for early treatment failure, or the presence of parasitemia and fever on any day after day 3, without the patient having previously met the criteria for early treatment failure. Late parasitologic failure was defined as the presence of parasitemia after day 3 and an axillary temperature of less than 37.5ºC, without the patient having previously met any of the criteria of early treatment failure or late parasitologic failure. An adequate clinical and parasitologic response was defined as the absence of parasitemia at the end of follow-up (day 63), regardless of the axillary temperature, without the patient having previously met any of the criteria of early treatment failure or late treatment failure.\n\n【26】Adverse events and serious adverse events were recorded and monitored throughout the trial by an independent data and safety monitoring board. The relationship between treatment and adverse events or serious adverse events was determined by the local investigator on the basis of clinical judgment, possible alternative causes (e.g., concomitant therapy), time of occurrence relative to the study treatment, and available information on the study treatment. The data and safety monitoring board reviewed listings of serious adverse events regularly. Secondary end points were PCR-unadjusted cure rates  at day 63, time to treatment failure (PCR-adjusted and PCR-unadjusted) , asexual parasite clearance,  gametocytemia (prevalence and density), and changes in the hemoglobin level.\n\n【27】Trial Oversight\n---------------\n\n【28】The contributions of the authors are listed in Table S5 in the Supplementary Appendix . The authors vouch for the accuracy and completeness of the data and for the fidelity of the trial to the protocol . The trial was approved by the ethics committee at the Antwerp University Hospital, the relevant national or local ethics committees, and the national drug regulatory authorities . All the study participants provided written informed consent. If the woman was illiterate, she would provide a fingerprint and a witness would write the name of the patient onto the form and sign and date it.\n\n【29】Statistical Analysis\n--------------------\n\n【30】The trial was designed to determine whether all four treatments had similar PCR-adjusted cure rates (difference, <5 percentage points), with 95% power for each of the six pairwise comparisons and 80% power for the combined hypothesis that all the treatments would be equivalent.  No multiplicity adjustment for the primary analysis was performed because the four treatments would be declared similar only if all six pairwise comparisons were shown to be within the 5-percentage-point margin. For this joint decision rule, no alpha-level correction was needed. \n\n【31】Data were captured in an electronic case-report form that was developed with the use of MACRO software (Infermed). A statistical analysis plan was developed before the database lock. For the primary end point, three analysis populations were used: a per-protocol population, an intention-to-treat population that excluded patients who were lost to follow-up or withdrew and those with missing or indeterminate results on PCR assay, and an intention-to-treat population that included multiple imputations of data from women who were lost to follow-up or withdrew and from those who had missing or indeterminate results on PCR assay. The per-protocol analysis was considered to be the primary analysis approach. Persons with major protocol violations, defined as a violation of the inclusion or exclusion criteria, receipt of a treatment different from the randomly assigned one, missing at least a full day of treatment, intake of other drugs with antimalarial activity, and missing day 63 blood smears, were excluded from the per-protocol analysis. All the secondary end points were analyzed with the use of an available-data approach.\n\n【32】We tested the primary hypothesis by calculating the 95% confidence interval for the difference in cure rates. If the difference in true (PCR-adjusted) cure rates was less than 5 percentage points, the treatments were considered to be therapeutically equivalent. This margin was chosen on the basis of the WHO recommendation that a new recommended antimalarial treatment that is adopted as policy should have an average cure rate of 95% or more as assessed in clinical trials. The confidence interval was calculated from a generalized linear model with adjustment for differences among the four countries. A number of sensitivity analyses were performed, including an analysis with multiple imputation of missing outcomes, a pairwise comparison that was limited to trial sites where a head-to-head comparison of treatments was performed, an analysis with adjustment for parasite density, gestational age, and gravidity at trial entry, and an analysis of the time to treatment failure with the use of Cox regression models. For the safety analysis, all the women who received at least one dose of the study treatment were included. Details of the subgroup analyses are provided in the Supplementary Appendix .\n\n【33】Results\n-------\n\n【34】Participants\n------------\n\n【35】Figure 1. Randomization of Patients and the Analysis Populations.\n\n【36】A total of five women were excluded after the receipt of the first dose of study medication because of entry-criteria violations. The exclusions from the intention-to-treat population (blue) and the per-protocol population (yellow-green) are based on the efficacy population. The intention-to-treat population excluded women who were lost to follow-up, died, withdrew, or had missing or indeterminate results on polymerase-chain-reaction assay; however, these women were included in the intention-to-treat analyses that used multiple imputations. The per-protocol population also excluded persons with major protocol violations, defined as a violation of the inclusion or exclusion criteria, receipt of a treatment different from the randomly assigned one, missing at least a full day of treatment, intake of other drugs with antimalarial activity, and missing day 63 blood smears.\n\n【37】A total of 3428 pregnant women who had _P. falciparum_ infection were randomly assigned to receive one of four treatment: artemether–lumefantrine (881 women), amodiaquine–artesunate (843), dihydroartemisinin–piperaquine (855), or mefloquine–artesunate (849) . The trial sites in Burkina Faso recruited 870 women, those in Ghana 788, the one in Malawi 870, and the one in Zambia 900. Five women were withdrawn by the investigators immediately after starting the treatment because of protocol deviations (3 did not have malaria, 1 was not pregnant, and 1 had been enrolled previously), although they were included in the safety analysis.\n\n【38】The intention-to-treat analysis included 3150 women (830 women in the artemether–lumefantrine group, 791 in the amodiaquine–artesunate group, 759 in the dihydroartemisinin–piperaquine group, and 770 in the mefloquine–artesunate group). The per-protocol analysis included 3000 women (810 women in the artemether–lumefantrine group, 742 in the amodiaquine–artesunate group, 720 in the dihydroartemisinin–piperaquine group, and 728 in the mefloquine–artesunate group) . Approximately half the exclusions from the per-protocol analysis (227 of 423 women \\[53.7%\\]) were due to loss to follow-up, withdrawal, or death.\n\n【39】Table 1. Characteristics of the Participants at Baseline.\n\n【40】The characteristics of the participants at baseline were similar among the treatment groups . Most women were included during the second trimester of pregnancy, and primigravidae represented approximately one third of the trial population. Parasite density was more than 2000 per cubic milliliter in approximately one third of the women, and few women (6.1%) had fever at the time of recruitment.\n\n【41】Treatment Efficacy\n------------------\n\n【42】Table 2. Efficacy Outcomes and Treatment Success Rates.\n\n【43】The large majority of treatment failures were late parasitologic failures, with fewer women having a late clinical failure . Most cases of late treatment failures were classified as new infections.\n\n【44】Figure 2. Differences in PCR-Adjusted and PCR-Unadjusted Treatment Success Rates at Day 63, According to Pairwise Analysis and Country.\n\n【45】In the estimation of the polymerase-chain-reaction (PCR)–adjusted cure rate, only recurrent infections that were shown by means of genotyping to be the same infections as those before treatment (recrudescences) were considered to be treatment failures. Conversely, for the estimation of the PCR-unadjusted cure rate, all recurrent infections were considered to be treatment failures. A positive value in the difference reflects a higher cure rate in the treatment listed first. If the difference in the cure rates was less than 5 percentage points (red lines), the treatments were considered to be therapeutically equivalent. Dashed horizontal lines indicate the 95% confidence intervals for the comparisons of treatment groups within a single country; solid horizontal lines are used for comparisons of total data from two countries.\n\n【46】According to the per-protocol analysis, the overall PCR-adjusted cure rate at day 63 was 94.8% (95% confidence interval \\[CI\\], 93.0 to 96.1; 748 of 789 women) in the artemether–lumefantrine group, 98.5% (95% CI, 97.3 to 99.2; 718 of 729 women) in the amodiaquine–artesunate group, 99.2% (95% CI, 98.2 to 99.6; 701 of 707 women) in the dihydroartemisinin–piperaquine group, and 96.8% (95% CI, 95.2 to 97.9; 693 of 716 women) in the mefloquine–artesunate group . There was no significant difference among the amodiaquine–artesunate group, the dihydroartemisinin–piperaquine group, and the mefloquine–artesunate group. The cure rate in the artemether–lumefantrine group was significantly lower than the rate in the other three treatment groups (P<0.001), although the difference was within the prespecified margin of 5 percentage points .\n\n【47】The unadjusted cure rates  were significantly lower in the artemether–lumefantrine group (52.5%; 425 of 810 women) than in the amodiaquine–artesunate group (82.3%; 611 of 742), the dihydroartemisinin–piperaquine group (86.9%; 626 of 720), and the mefloquine–artesunate group (73.8%; 537 of 728) (P<0.001) . Country-specific results are provided in Table S7 in the Supplementary Appendix . The intention-to-treat analyses and analyses with multiple imputations of unavailable outcomes further supported the efficacy results . Results from the sensitivity analyses were generally consistent with those from the primary analyses.\n\n【48】At day 2 after the initiation of treatment, nearly all the women (>99.5%) had a negative blood smear. However, parasite clearance was slower among the women treated with artemether–lumefantrine than among women treated with the other therapies; at day 1 after the start of treatment, 24.8% of the women (217 of 875 women) in the artemether–lumefantrine group still had detectable parasitemia, as compared with 6.9% (57 of 828) in the amodiaquine–artesunate group, 8.0% (67 of 837) in the dihydroartemisinin–piperaquine group, and 13.5% (113 of 837) in the mefloquine–artesunate group (P<0.001).\n\n【49】Gametocyte prevalence at enrollment was low , with a median density between 11 and 40 gametocytes per cubic millimeter. Gametocyte carriage remained low throughout follow-up, with no significant difference among the treatment groups. Similarly, changes in the hemoglobin level did not differ significantly among the treatment groups throughout follow-up .\n\n【50】The prevalence of placental malaria infection at delivery was similar among the treatment groups (P=0.47). The mean birth weight of the babies, after adjustment according to country, was similar among the treatment groups. The mean (±SD) birth weight was 2854±449 g in the artemether–lumefantrine group, 2880±452 g in the amodiaquine–artesunate group, 2901±454 g in the dihydroartemisinin–piperaquine group, and 2875±433 g in the mefloquine–artesunate group (P=0.40). Similarly, the percentage of babies with low birth weight did not vary significantly among the treatment groups (17.2% in the artemether–lumefantrine group, 15.5% in the amodiaquine–artesunate group, 14.1% in the dihydroartemisinin–piperaquine group, and 15.2% in the mefloquine–artesunate group; P=0.32).\n\n【51】Safety\n------\n\n【52】A total of 72 women had serious adverse events during the 63-day follow-up, including 1 woman in the mefloquine–artesunate group who died approximately 1 month after treatment, probably from meningitis . There were 10 serious adverse events that were assessed by the site investigator as being probably related to the study medication, including 5 in the amodiaquine–artesunate group (anemia in 2 women, upper abdominal pain in 1, and malaise in 2), 4 in the mefloquine–artesunate group (abdominal pain in 1, vomiting in 2, and malaise in 1), and 1 in the dihydroartemisinin–piperaquine group (a possible adverse drug reaction with headache and general weakness 2 days after the completion of treatment; the woman recovered completely). No significant difference in the occurrence of serious adverse events was found among the treatment groups.\n\n【53】Table 3. Safety Outcomes.\n\n【54】Women treated with mefloquine–artesunate and those treated with amodiaquine–artesunate had a significantly higher incidence of any adverse event (84.9% \\[722 of 850 women\\] and 79.0% \\[665 of 842\\], respectively) than did those in the artemether–lumefantrine group (72.8%; 641 of 881) and those in the dihydroartemisinin–piperaquine group (70.4%; 602 of 855) (P<0.001 for the comparison among the four groups) . Adverse events that were related to the treatment, as determined by the investigators, occurred significantly more frequently in the mefloquine–artesunate group (in 50.6% of women; 430 of 850 women) and the amodiaquine–artesunate group (in 48.5%; 408 of 842) than in the dihydroartemisinin–piperaquine group (in 20.6%; 176 of 855) and the artemether–lumefantrine group (in 11.5%; 101 of 881) (P<0.001 for the comparison among the four groups). This result was due mainly to the higher occurrence of asthenia, poor appetite, dizziness, nausea, and vomiting among women treated with mefloquine–artesunate or amodiaquine–artesunate than among those treated with dihydroartemisinin–piperaquine or artemether–lumefantrine .\n\n【55】Behavioral changes were observed in 4 women, of whom 2 were in the amodiaquine–artesunate group (changes noted on day 2 and day 3 after treatment), 1 was in the mefloquine–artesunate group (changes noted on day 2 after treatment), and 1 was in the artemether–lumefantrine group (changes noted on day 60 after treatment); the two behavioral changes in the amodiaquine–artesunate group were considered by the site investigator to be possibly related to treatment. All the women recovered completely. A woman treated with amodiaquine–artesunate reported hallucinations on day 3 after treatment; these were considered by the investigator to be possibly related to treatment. The woman recovered completely. Significantly more women in the amodiaquine–artesunate group than in the other three groups reported insomnia: 4.0% (34 of 842 women) in the amodiaquine–artesunate group versus 2.5% (21 of 850) in the mefloquine–artesunate group, 1.6% (14 of 855) in the dihydroartemisinin–piperaquine group, and 0.3% (3 of 881) in the artemether–lumefantrine group (P=0.04).\n\n【56】The pulse rate and blood pressure tended to be lower among women treated with amodiaquine–artesunate than among those in the other three groups . The percentage of women with a diastolic blood pressure of less than 50 mm Hg and a systolic blood pressure of less than 90 mm Hg was higher in the amodiaquine–artesunate group than in the other groups (P<0.001). Similarly, the percentage of women with a pulse rate of less than 60 beats per minute appeared to be higher in the amodiaquine–artesunate group than in the other groups, but this difference was not significant (P=0.40). Hypotension or a low diastolic blood pressure as an adverse event (i.e., considered by the local investigator to be clinically significant) occurred more frequently in the amodiaquine–artesunate group (1.5%) than in the other treatment groups (range, 0.6 to 0.8%). There were no significant differences in the laboratory safety values among the treatment groups.\n\n【57】Outcome of Pregnancy\n--------------------\n\n【58】There were 13 miscarriages (1 miscarriage in the artemether–lumefantrine group and 4 in each of the other three groups). There were 78 stillbirths overall, with 16 stillbirths occurring in 856 births (1.9%) in the artemether–lumefantrine group, 17 in 815 (2.1%) in the amodiaquine–artesunate group, 22 in 818 (2.7%) in the dihydroartemisinin–piperaquine group, and 23 in 821 (2.8%) in the mefloquine–artesunate group. The proportion of live births did not differ significantly among the treatment groups (P=0.85). The percentage of preterm babies, as determined by the total Ballard score, was 10.2% in the artemether–lumefantrine group, 3.4% in the amodiaquine–artesunate group, 9.5% in the dihydroartemisinin–piperaquine group, and 7.7% in the mefloquine–artesunate group (P=0.64). A total of 44 congenital malformations were observed, with 17 occurring in 832 newborns (2.0%) in the artemether–lumefantrine group, 8 in 776 (1%) in the amodiaquine–artesunate group, 6 in 767 (0.8%) in the dihydroartemisinin–piperaquine group, and 13 in 780 (1.7%) in the mefloquine–artesunate group.\n\n【59】Discussion\n----------\n\n【60】The PCR-adjusted cure rates were in the range of 94.8 to 99.2% for all four artemisinin-based combination therapies, and the differences among them were within the prespecified equivalence margin of 5 percentage points. The high success rates are remarkable given the long follow-up period, which was 3 weeks longer than the 6 weeks recommended by the WHO. Nevertheless, the cure rates in the artemether–lumefantrine group were significantly lower than those in the groups that received the other artemisinin-based combination therapies, which had similar high efficacy. In a previous trial in Uganda, the efficacy of artemether–lumefantrine (until day 42) during pregnancy was 99.3%.  The longer follow-up until day 63 in our trial cannot explain the lower cure rates in our trial than in the Uganda trial, because most treatment failures occurred between day 28 and day 42 . The efficacy of artemether–lumefantrine was low (82%) among pregnant women at the Thai–Burmese border,  a finding that was attributed to low drug concentrations and low antimalarial immunity.  In Uganda, the plasma concentration of lumefantrine was 27% lower in pregnant women than in nonpregnant women,  which suggests that the high efficacy of artemether–lumefantrine was probably due to the higher background immunity in Uganda than in Thailand. In our trial, artemether–lumefantrine was tested in three countries with a high risk of malaria infection (owing to the high intensity of malaria transmission), so the background immunity among recruited pregnant women was probably high.\n\n【61】Patients treated with artemether–lumefantrine had the highest rate of reinfection and the shortest time to reinfection. The duration of post-treatment prophylaxis is an important factor in the choice of antimalarial drugs, especially in areas with a high risk of infection. Lumefantrine has the shortest elimination half-life,  followed by mefloquine,  amodiaquine,  and then piperaquine. \n\n【62】The efficacy of artemether–lumefantrine was relatively low in Burkina Faso (93.2%). This was the same trial site at which the efficacy of artemether–lumefantrine among children with malaria was the lowest (90.2%) among trial sites in sub-Saharan Africa.  The sites in Burkina Faso also had the highest intensity of transmission, as suggested by the high rates of reinfection observed in all the treatment groups . This high intensity of transmission may influence the interpretation of the genotyping results, with the risk that new infections may be misclassified as recrudescences.  In our trial, capillary electrophoresis, a technique that can minimize misclassification,  was used for MSP2 genotyping. In addition, transmission intensity may influence the individual treatment cure rates but not the risk difference between treatments. \n\n【63】There was no significant difference in birth outcomes among the treatment groups. The mean birth weight as well as the proportion of miscarriages, stillbirths, preterm deliveries, and congenital malformations were similar among the groups.\n\n【64】Fewer adverse events were seen in the artemether–lumefantrine group and the dihydroartemisinin–piperaquine group than in the other two groups.  Approximately half the adverse events in the amodiaquine–artesunate group and mefloquine–artesunate group were considered by the investigator to be related to treatment. Asthenia was the most common event in the amodiaquine–artesunate group, followed by dizziness; both these events may be related to low blood pressure and pulse rate. Asthenia was more common in the amodiaquine–artesunate group than in the other three groups. Nausea or vomiting was also relatively common. General weakness, vomiting, dizziness, and nausea were the most commonly reported adverse events among pregnant women in Ghana who were treated with amodiaquine.  Dizziness was the most frequent treatment-related adverse event in the mefloquine–artesunate group, followed by vomiting, nausea, and asthenia. The association between mefloquine and dizziness has already been reported; more than 30% of pregnant women who were treated with mefloquine monotherapy at a dose of 15 mg per kilogram of body weight reported dizziness, but the occurrence decreased after subsequent doses.  Because this trial was an open-label trial, determination of the cause of adverse events may have been influenced by the knowledge of the treatment given.  Three women had behavior changes soon after the onset of treatment, one in the mefloquine–artesunate group and two in the amodiaquine–artesunate group. Mefloquine use has been associated with neuropsychiatric adverse events,  a phenomenon that has also been described for 4-aminoquinolines. \n\n【65】In conclusion, artemether–lumefantrine was associated with the fewest adverse effects and with acceptable cure rates but provided the shortest post-treatment prophylaxis. Dihydroartemisinin–piperaquine had the best efficacy and an acceptable safety profile.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "59e65069-93e9-46ad-a27c-8414e84270b6", "title": "The Evolving Challenge of Infections in Cirrhosis", "text": "【0】The Evolving Challenge of Infections in Cirrhosis\nPatients with cirrhosis are at increased risk for serious infections, including spontaneous bacterial peritonitis, multidrug-resistant infection, and infection with gut microorganisms. In-hospital mortality exceeds 50%. Empirical antibiotic coverage is used until a precise diagnosis can be made.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5094feba-3222-4066-8bec-c05d092ac226", "title": "Unexplained Chest Pain in Patients with Normal Coronary Arteriograms — A Follow-up Study of Functional Status", "text": "【0】Unexplained Chest Pain in Patients with Normal Coronary Arteriograms — A Follow-up Study of Functional Status\nAbstract\n--------\n\n【1】Approximately 10 per cent of patients referred for coronary arteriography because of chest pain have angiographically normal coronary arteries and no other heart disease. We examined the functional status of 57 patients who had undergone catheterization (23 men and 34 women), all of whom were told that their hearts were normal, that their pain was noncardiac, and that no limitation on activity was necessary. At a mean follow-up time of 16±7.7 months, 27 of the 57 patients (47 per cent) still described their activity as limited by chest pain (before catheterization, 42 of 57 or 74 per cent); 29 of 57 (51 per cent) were unable to work (before catheterization, 36 of 57 or 63 per cent); and 25 of 57 (44 per cent) still believed that they had heart disease (before catheterization, 45 of 57 or 79 per cent). Use of medical facilities was significantly reduced after catheterization (P<0.001). At follow-up the physician was more likely than the patient to believe that the symptoms had improved. We conclude that many of these patients remain limited in activity and may benefit from further efforts at communication and rehabilitation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a7fc8919-a884-4564-999f-85c80d905a1b", "title": "Bevacizumab in Renal-Cell Cancer", "text": "【0】Bevacizumab in Renal-Cell Cancer\nTo the Editor:\n--------------\n\n【1】In the report on their randomized, placebo-controlled, phase 2 trial evaluating bevacizumab, Yang et al. (July 31 issue)  conclude that bevacizumab significantly prolongs the time to progression of metastatic renal-cell cancer. I believe this conclusion is inappropriate. Although the patients are randomly assigned to the treatment groups in a randomized phase 2 study, it is not equivalent to a phase 3 study. The purpose of a randomized phase 2 trial is to select one of several novel regimens for the next phase of testing. One can only be reassured that the selected regimen is probably not significantly worse than the other regimens being evaluated.  Therefore, the proper conclusion is that bevacizumab at a dose of 10 mg per kilogram of body weight appears to be promising in the treatment of metastatic renal cancer and should be studied further.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4635341c-0726-490c-8538-5d253f9c97cc", "title": "Effect of Control of Blood Glucose on Urinary Excretion of Albumin and β", "text": "【0】Effect of Control of Blood Glucose on Urinary Excretion of Albumin and β\nAbstract\n--------\n\n【1】To study the effects of improved control of blood glucose on markers of renal glomerular and tubular function, we initially determined, by radio-immunoassay technics, urinary excretion rates of albumin and β <sub>2 </sub> microglobulin in 17 nondiabetic subjects and in 43 insulin-dependent, clinically nonproteinuric diabetic patients. Duration of diabetes ranged from six months to 39 years, and the patients were studied while receiving conventional therapy. Mean urinary albumin excretion was significantly elevated in the diabetics, but β <sub>2 </sub> \\-microglobulin excretion rates were not different from those of the controls, suggesting that the increased albumin excretion was due to increased transglomerular loss of albumin. Seven patients with long-term diabetes (duration of six to 33 years), selected because of elevated albumin excretion, were studied before and during a continuous, subcutaneous insulin infusion for a period of one to three days. Urinary albumin excretion was significantly reduced during the insulin infusion, but mean β <sub>2 </sub> \\-microglobulin excretion did not change. Strict control of blood glucose, even in the short term, may reverse a functional renal abnormality in long-duration, insulin-dependent diabetes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "698c6c42-90e6-4cc4-8a52-d8ab4c068202", "title": "Another Decade, Another Coronavirus", "text": "【0】Another Decade, Another Coronavirus\nArticle\n-------\n\n【1】For the third time in as many decades, a zoonotic coronavirus has crossed species to infect human populations. This virus, provisionally called 2019-nCoV, was first identified in Wuhan, China, in persons exposed to a seafood or wet market. The rapid response of the Chinese public health, clinical, and scientific communities facilitated recognition of the clinical disease and initial understanding of the epidemiology of the infection. First reports indicated that human-to-human transmission was limited or nonexistent, but we now know that such transmission occurs, although to what extent remains unknown. Like outbreaks caused by two other pathogenic human respiratory coronaviruses (severe acute respiratory syndrome coronavirus \\[SARS-CoV\\] and Middle East respiratory syndrome coronavirus \\[MERS-CoV\\]), 2019-nCoV causes respiratory disease that is often severe.  As of January 24, 2020, there were more than 800 reported cases, with a mortality rate of 3% .\n\n【2】As now reported in the _Journal_ , Zhu et al.  have identified and characterized 2019-nCoV. The viral genome has been sequenced, and these results in conjunction with other reports show that it is 75 to 80% identical to the SARS-CoV and even more closely related to several bat coronaviruses.  It can be propagated in the same cells that are useful for growing SARS-CoV and MERS-CoV, but notably, 2019-nCoV grows better in primary human airway epithelial cells than in standard tissue-culture cells, unlike SARS-CoV or MERS-CoV. Identification of the virus will allow the development of reagents to address key unknowns about this new coronavirus infection and guide the development of antiviral therapies. First, knowing the sequence of the genome facilitates the development of sensitive quantitative reverse-transcriptase–polymerase-chain-reaction assays to rapidly detect the virus. Second, the development of serologic assays will allow assessment of the prevalence of the infection in humans and in potential zoonotic sources of the virus in wet markets and other settings. These reagents will also be useful for assessing whether the human infection is more widespread than originally thought, since wet markets are present throughout China. Third, having the virus in hand will spur efforts to develop antiviral therapies and vaccines, as well as experimental animal models.\n\n【3】Much still needs to be learned about this infection. Most important, the extent of interhuman transmission and the spectrum of clinical disease need to be determined. Transmission of SARS-CoV and MERS-CoV occurred to a large extent by means of superspreading events.  Superspreading events have been implicated in 2019-nCoV transmission, but their relative importance is unknown. Both SARS-CoV and MERS-CoV infect intrapulmonary epithelial cells more than cells of the upper airways.  Consequently, transmission occurs primarily from patients with recognized illness and not from patients with mild, nonspecific signs. It appears that 2019-nCoV uses the same cellular receptor as SARS-CoV (human angiotensin-converting enzyme 2 \\[hACE2\\]),  so transmission is expected only after signs of lower respiratory tract disease develop. SARS-CoV mutated over the 2002–2004 epidemic to better bind to its cellular receptor and to optimize replication in human cells, enhancing virulence.  Adaptation readily occurs because coronaviruses have error-prone RNA-dependent RNA polymerases, making mutations and recombination events frequent. By contrast, MERS-CoV has not mutated substantially to enhance human infectivity since it was detected in 2012. \n\n【4】It is likely that 2019-nCoV will behave more like SARS-CoV and further adapt to the human host, with enhanced binding to hACE2. Consequently, it will be important to obtain as many temporally and geographically unrelated clinical isolates as possible to assess the degree to which the virus is mutating and to assess whether these mutations indicate adaptation to the human host. Furthermore, if 2019-nCoV is similar to SARS-CoV, the virus will spread systemically.  Obtaining patient samples at autopsy will help elucidate the pathogenesis of the infection and modify therapeutic interventions rationally. It will also help validate results obtained from experimental infections of laboratory animals.\n\n【5】A second key question is identification of the zoonotic origin of the virus. Given its close similarity to bat coronaviruses, it is likely that bats are the primary reservoir for the virus. SARS-CoV was transmitted to humans from exotic animals in wet markets, whereas MERS-CoV is transmitted from camels to humans.  In both cases, the ancestral hosts were probably bats. Whether 2019-nCoV is transmitted directly from bats or by means of intermediate hosts is important to understand and will help define zoonotic transmission patterns.\n\n【6】A striking feature of the SARS epidemic was that fear played a major role in the economic and social consequences. Although specific anticoronaviral therapies are still in development, we now know much more about how to control such infections in the community and hospitals, which should alleviate some of this fear. Transmission of 2019-nCoV probably occurs by means of large droplets and contact and less so by means of aerosols and fomites, on the basis of our experience with SARS-CoV and MERS-CoV.  Public health measures, including quarantining in the community as well as timely diagnosis and strict adherence to universal precautions in health care settings, were critical in controlling SARS and MERS. Institution of similar measures will be important and, it is hoped, successful in reducing the transmission of 2019-nCoV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "c722f272-34fd-4fdf-a7e5-5d748afda82b", "title": "An Efficacy Trial of Doxycycline Chemoprophylaxis against Leptospirosis", "text": "【0】An Efficacy Trial of Doxycycline Chemoprophylaxis against Leptospirosis\nAbstract\n--------\n\n【1】Because leptospirosis has been an important cause of morbidity in U.S. soldiers training in the Republic of Panama, we conducted a randomized, double-blind, placebo-controlled field trial during the fall of 1982 to determine whether doxycycline was an effective chemoprophylactic agent against this infection. Doxycycline (200 mg) or placebo was administered orally on a weekly basis and at the completion of training to 940 volunteers from two U.S. Army units deployed in Panama for approximately three weeks of jungle training. Twenty cases of leptospirosis occurred in the placebo group (an attack rate of 4.2 per cent), as compared with only one case in the doxycycline group (attack rate, 0.2 per cent, P<0.001), yielding an efficacy of 95.0 per cent. This study demonstrated the value of doxycycline as a prophylactic drug against leptospirosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "29ecb7e1-fbcd-4413-b08b-426255aaa806", "title": "Natalizumab Induction and Maintenance Therapy for Crohn's Disease", "text": "【0】Natalizumab Induction and Maintenance Therapy for Crohn's Disease\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Natalizumab, a humanized monoclonal antibody against α <sub>4 </sub> integrin, inhibits leukocyte adhesion and migration into inflamed tissue.\n\n【3】Methods\n-------\n\n【4】We conducted two controlled trials to evaluate natalizumab as induction and maintenance therapy in patients with active Crohn's disease. In the first trial, 905 patients were randomly assigned to receive 300 mg of natalizumab or placebo at weeks 0, 4, and 8. The primary outcome was response, defined by a decrease in the Crohn's Disease Activity Index (CDAI) score of at least 70 points, at week 10. In the second trial, 339 patients who had a response to natalizumab in the first trial were randomly reassigned to receive 300 mg of natalizumab or placebo every four weeks through week 56. The primary outcome was a sustained response through week 36. A secondary outcome in both trials was disease remission (a CDAI score of less than 150).\n\n【5】Results\n-------\n\n【6】In the first trial, the natalizumab and placebo groups had similar rates of response (56 percent and 49 percent, respectively; P=0.05) and remission (37 percent and 30 percent, respectively; P=0.12) at 10 weeks. Continuing natalizumab in the second trial resulted in higher rates of sustained response (61 percent vs. 28 percent, P<0.001) and remission (44 percent vs. 26 percent, P=0.003) through week 36 than did switching to placebo. Serious adverse events occurred in 7 percent of each group in the first trial and in 10 percent of the placebo group and 8 percent of the natalizumab group in the second trial. In an open-label extension study, a patient treated with natalizumab died from progressive multifocal leukoencephalopathy, associated with the JC virus, a human polyomavirus.\n\n【7】Conclusions\n-----------\n\n【8】Induction therapy with natalizumab for Crohn's disease resulted in small, nonsignificant improvements in response and remission rates. Patients who had a response had significantly increased rates of sustained response and remission if natalizumab was continued every four weeks. The benefit of natalizumab will need to be weighed against the risk of serious adverse events, including progressive multifocal leukoencephalopathy. \n\n【9】Introduction\n------------\n\n【10】The pathogenesis of Crohn's disease involves persistent recruitment of leukocytes into gut tissue, with resultant inflammation. Natalizumab (Tysabri, Elan Pharmaceuticals and Biogen Idec), a humanized IgG4 monoclonal antibody that blocks the adhesion and subsequent migration of leukocytes into the gut by binding α <sub>4 </sub> integrin, is a member of a new class of molecules referred to as selective adhesion-molecule inhibitors.  Previous studies have suggested that natalizumab may be effective for the treatment of active Crohn's disease.  Furthermore, natalizumab has been shown to be effective in the treatment of multiple sclerosis, another chronic inflammatory disease.  Therefore, we conducted a 12-week induction trial of natalizumab in patients with moderate-to-severe Crohn's disease. Patients who had a response to natalizumab were eligible to enroll in a 48-week maintenance trial.\n\n【11】Methods\n-------\n\n【12】Members of the steering committees of the Efficacy of Natalizumab as Active Crohn's Therapy (ENACT-1) induction trial and the Evaluation of Natalizumab as Continuous Therapy (ENACT-2) maintenance trial and the sponsors designed the studies. The authors had access to all data, participated in the analysis and interpretation of the data, and were members of the publication committee. The academic authors vouch for the completeness and veracity of the data and data analyses.\n\n【13】Patients\n--------\n\n【14】The ENACT-1 and ENACT-2 trials were randomized, double-blind, placebo-controlled studies conducted at 142 centers between December 2001 and March 2004. The protocols were approved by the institutional review board at each center. All patients gave written informed consent.\n\n【15】The ENACT-1 trial included patients 18 years of age or older who for at least six months had had Crohn's disease that was moderately to severely active as defined by a baseline Crohn's Disease Activity Index (CDAI)  score of 220 to 450 points (scores range from 0 to 600, with higher scores indicating more severe disease activity). Radiologic or endoscopic evidence was required within 36 months before enrollment and after surgical resection to confirm the presence of active disease. Concurrent therapies, including stable doses of 5-aminosalicylates, prednisolone (25 mg per day or less) or equivalent, budesonide (6 mg per day or less), azathioprine, mercaptopurine, methotrexate, and antibiotics, were permitted. Patients with the short-bowel syndrome, a stricture with obstructive symptoms, a draining fistula, or an abdominal abscess; those who had undergone ostomy; and those who had received therapy against tumor necrosis factor α (TNF-α) within the preceding three months were excluded. Patients who had a response, as defined below, in the ENACT-1 trial were eligible for the ENACT-2 trial.\n\n【16】Study Design\n------------\n\n【17】Patients who were eligible for the ENACT-1 trial were randomly assigned in a  ratio to receive an intravenous infusion of either 300 mg of natalizumab or placebo at weeks 0, 4, and 8 and were then followed through week 12. Treatment allocation was prospectively stratified according to disease activity (a CDAI score of less than 330 points vs. a score of at least 330 points) and the use of corticosteroids. The primary efficacy end point of the ENACT-1 trial was the proportion of patients with a response at the prespecified time of week 10. A response was defined as a reduction in the CDAI score of at least 70 points from week 0, whereas remission was defined as a CDAI score of less than 150 points. \n\n【18】Patients with a CDAI score of 0 to 220 at week 12 who had had a response at both weeks 10 and 12 without the need for intervention were eligible for the ENACT-2 trial. Eligible patients were randomly reassigned in a  ratio to receive an infusion of either placebo or 300 mg of natalizumab every four weeks from weeks 12 through 56 and were followed until week 60. Treatment allocation was prospectively stratified according to disease activity (remission vs. mildly active disease) and the use of corticosteroid therapy and immunosuppressive therapy. The primary efficacy end point of the ENACT-2 study was the proportion of patients treated with natalizumab who had a clinical response in the ENACT-1 study, who were randomly reassigned in the ENACT-2 trial, and who had a sustained response through the prespecified time of week 36. Loss of response was defined by an increase in the CDAI score of at least 70 points after week 12 and by an absolute score of at least 220 or the need for intervention after week 12. Loss of remission was defined as an increase in the CDAI score to at least 150 points or the need for intervention among patients in remission at week 12. Patients receiving placebo in the ENACT-1 study who were enrolled in the ENACT-2 study were also assessed in exploratory analyses.\n\n【19】Both trials were centrally randomized. Patients and investigators were unaware of treatment assignments.\n\n【20】The doses of all concurrent medications, except corticosteroids, remained constant. Patients receiving concomitant corticosteroids were required to attempt discontinuation according to a fixed tapering regimen. After week 10, the daily dose of prednisolone or equivalent was reduced by 5 mg weekly until a dose of 10 mg was reached. Thereafter, the dose was reduced by 2.5 mg each week until the drug was discontinued. After week 10, the daily dose of budesonide was decreased by 3 mg every three weeks until the drug was discontinued.\n\n【21】Efficacy and Safety Evaluations\n-------------------------------\n\n【22】Patients were assessed two weeks before randomized treatment was begun, on day 0, and every two weeks thereafter through week 12 in the ENACT-1 trial. In the ENACT-2 study, patients were assessed every four weeks through week 60. The CDAI score was determined at each visit, adverse events and concomitant medications were recorded, and samples were collected for laboratory evaluations. Safety evaluations included assessment of vital signs, physical examination, hematologic analysis, serum biochemical analysis, and urinalysis.\n\n【23】Statistical Analysis\n--------------------\n\n【24】We estimated that a minimum of 845 patients would be needed for the ENACT-1 study to have a statistical power of 90 percent to detect an absolute difference in response rates of 15 percent between the natalizumab and placebo groups, assuming a 55 percent rate of response to natalizumab, a 40 percent rate of response to placebo, and a 10 percent rate of responses that could not be evaluated. We anticipated that 380 patients who were treated with natalizumab in the ENACT-1 study would have a response, of whom at least 285 would elect to enroll in the ENACT-2 study. We estimated that 285 patients would need to undergo rerandomization for the ENACT-2 study to have a statistical power of 90 percent to detect an absolute difference of 21 percent in the rates of a sustained response, given a 65 percent rate of sustained response to natalizumab, a 44 percent rate for placebo, and a 10 percent rate of responses that could not be evaluated.\n\n【25】Logistic regression was used to analyze differences in the primary outcome measures, after adjustment for disease activity and corticosteroid use at week 0 in the ENACT-1 study and for the use of corticosteroids and immunosuppressants at week 0 as well as the rates of remission at week 12 in the ENACT-2 trial. Fisher's exact tests were used for analyses of adverse events. The primary analyses had an α level of 0.05 and used two-sided tests. Data from the ENACT-1 trial were analyzed according to the intention-to-treat principle. Efficacy analyses in the ENACT-2 trial excluded patients who underwent randomization despite the lack of a clinical response or remission in the ENACT-1 study. For continuous outcomes, efficacy data collected after intervention or early discontinuation were replaced by the last available value. For the calculation of induction and maintenance of clinical response or remission and corticosteroid-withdrawal end points, patients who required rescue therapy or those with missing data were classified as having no response to treatment.\n\n【26】Logistic regression and log-rank tests were used as appropriate to provide nominal P values for secondary end points. The time to clinical response and time to remission were evaluated by the Kaplan–Meier method and a Cox proportional-hazards model. For the ENACT-1 study, post hoc analyses were also performed to explore the effect of natalizumab in subpopulations with objectively determined active inflammation and chronic disease despite the use of conventional therapies (patients with an elevated concentration of C-reactive protein, those receiving immunosuppressants, and those who had previously received therapy against TNF-α). No adjustments were made for multiple comparisons.\n\n【27】Results\n-------\n\n【28】Characteristics of the Patients\n-------------------------------\n\n【29】Figure 1. Enrollment and Treatment.\n\n【30】Not all patients who were classified by investigators as having a response to induction therapy actually met the prespecified definition of a response (i.e., they were not eligible for the ENACT-2 trial owing to missing assessments or the use of rescue medications during the ENACT-1 trial that were not documented by the investigator before the patient entered the maintenance study). One of the nine patients assigned to placebo maintenance in the ENACT-2 trial who could not be evaluated entered remission but did not meet the criteria for a response.Table 1.  Table 1. Baseline Characteristics of the Patients in the ENACT-1 Trial and the Patients Who Had a Response to Natalizumab or Remission at the Time of Randomization in the ENACT-2 Trial.\n\n【31】Figure 1 summarizes the disposition of patients. In both trials, the baseline characteristics of the patients were generally similar in the two groups . In the ENACT-2 trial, there were significant but clinically unimportant differences between groups in the percentages of men enrolled, CDAI scores, and smoking status.\n\n【32】Efficacy in the ENACT-1 Trial\n-----------------------------\n\n【33】Figure 2. Percentage of Patients in the ENACT-1 Trial with a Clinical Response at Each Visit , in Remission at Each Visit , with a Response at Week 10 , and in Remission at Week 10 .\n\n【34】A response was defined as a decrease from baseline in the CDAI score of at least 70 points. Remission was defined as a decrease in the CDAI score to less than 150 points. Significant differences between the treatment groups are shown.\n\n【35】At week 10, 49 percent of patients in the placebo group had had a response (88 of 181), as compared with 56 percent of patients in the natalizumab group (408 of 724, P=0.05) . Similarly, 30 percent of patients in the placebo group were in remission at week 10 (55 of 181), as compared with 37 percent of patients in the natalizumab group (267 of 724, P=0.12) . The differences between the natalizumab and placebo groups were significant at week 10 with the use of the more stringent definition of response as a 100-point decrease in the CDAI score. The rates of response and remission were higher in the natalizumab group at all times, but the majority of these differences were not significant . The results among patients receiving corticosteroids at baseline were similar to those overall.\n\n【36】In contrast, the efficacy of natalizumab was demonstrated in the subgroups of 660 patients with an elevated concentration of C-reactive protein at baseline (upper limit of normal, 2.87 mg per liter), 300 patients with active disease despite the use of immunosuppressants, and 358 patients who had previously received anti–TNF-α therapy. Each subgroup was significantly more likely to have either a response or a remission after treatment with natalizumab than after receiving placebo . The rates of response and remission were also higher, but not significantly so, among natalizumab-treated patients in the subgroup who had not previously received anti–TNF-α therapy and the subgroup with active disease and no concomitant use of immunosuppressants but not in the subgroup without an elevated concentration of C-reactive protein at baseline . The dose of natalizumab (in milligrams per kilogram of body weight) had no significant effect on the rates of response or remission.\n\n【37】The ENACT-2 Trial\n-----------------\n\n【38】### _Efficacy among Patients with a Response to Natalizumab in the ENACT-1 Trial_\n\n【39】Figure 3. Percentage of Patients in the ENACT-2 Trial with a Sustained Response  and Sustained Remission  at Each Visit, Percentage of Patients with a Response or Remission at Weeks 36 and 60 , and Percentage of Patients Taking Oral Corticosteroids at Randomization in the ENACT-1 Trial Who Were No Longer Taking Oral Corticosteroids While Remaining in Remission during the ENACT-2 Trial .\n\n【40】In Panel A, the analysis was limited to patients with a response to natalizumab in the ENACT-1 trial who met the eligibility criteria for the ENACT-2 trial at randomization. In Panel B, the analysis was limited to patients with a response to natalizumab in the ENACT-1 trial who met the eligibility criteria for remission at the time of randomization for the ENACT-2 trial. Loss of response was defined as an increase in the CDAI score after week 12 of at least 70 points to an absolute score of at least 220 or the use of rescue intervention. Loss of remission was defined as in increase in the CDAI score to at least 150 points or the need for intervention in patients in remission at week 12. Significant differences between the treatment groups are shown.\n\n【41】In the ENACT-2 trial, 28 percent of patients randomly reassigned to placebo had a sustained response through week 36 (48 of 170), as compared with 61 percent of patients randomly reassigned to natalizumab (103 of 168, P<0.001) . Regression analyses demonstrated that adjustment for sex, baseline CDAI scores, smoking status, C-reactive protein concentration (at week 0 or 12), the use of prior anti–TNF-α therapy, and the dose of natalizumab did not influence the results. Overall, 26 percent of patients in the placebo group had a sustained remission through week 36 (31 of 120), as compared with 44 percent of patients in the natalizumab group (57 of 130, P=0.003) . The rates of sustained response and remission were significantly higher in the natalizumab group at every point beginning at week 20 and continuing through week 60 . The natalizumab group also had significantly higher rates of response and remission rates when the results were assessed at prespecified points (weeks 36 and 60) .\n\n【42】The median time to the loss of response was 86 days in the placebo group and more than 336 days in the natalizumab group (P<0.001). Among patients who entered remission after induction therapy with natalizumab, the median time to the loss of remission was 59 days in the placebo group and 137 days in the natalizumab group (P<0.001).\n\n【43】Forty-two percent of patients who had a response to induction therapy with natalizumab (143 of 339) had received oral corticosteroids or budesonide at baseline. Among patients who completed week 36 of the ENACT-2 study, 28 percent of those randomly reassigned to placebo were no longer taking corticosteroids, as compared with 58 percent of patients randomly reassigned to natalizumab (P<0.001); the results were similar at week 60 (P<0.001). At week 36, 22 percent of patients in the placebo group who received corticosteroid therapy at baseline were in remission and had discontinued corticosteroids, as compared with 45 percent of patients in the natalizumab group (P=0.01); the results were similar at week 60 (P=0.001) .\n\n【44】### _Efficacy among Patients with a Response to Placebo in the ENACT-1 Trial_\n\n【45】Among the patients who had a response to induction therapy with placebo at the time of randomization, 55 percent of patients who were randomly reassigned to placebo maintained a response through week 36 (18 of 33), as compared with 54 percent of patients who were randomly reassigned to natalizumab (19 of 35, P=0.58). Similarly, 36 percent of patients who were randomly reassigned to placebo maintained a response through week 60 (12 of 33), as compared with 49 percent of patients who were randomly reassigned to natalizumab (17 of 35, P=0.16).\n\n【46】Safety\n------\n\n【47】Table 2. Summary of Safety and Immunogenicity Analyses of All Randomized Patients to Week 12 in the ENACT-1 Trial and to Week 60 in the ENACT-2 Trial.\n\n【48】Table 2 shows the overall incidence of adverse events. Two patients treated with natalizumab died during or after participation in the ENACT-1 study. The first patient died of asphyxiation resulting from an occupational accident, as determined by a subsequent government investigation. The second patient died from complications of surgery for a severe exacerbation of Crohn's disease 28 weeks after completing the study. No deaths occurred during the ENACT-2 study. No hematologic cancers occurred during either study. Basal-cell carcinoma of the skin developed in one patient in each group during the ENACT-2 study. There were no clinically significant changes in laboratory values in either treatment group in either trial. Natalizumab-treated patients had moderate and sustained elevations in the median absolute circulating lymphocyte count (not above the upper limit of normal; 2900 and 2800 cells per cubic millimeter at weeks 10 and 36, respectively), as compared with baseline (1600 cells per cubic millimeter), a predicted pharmacologic effect, reversible on cessation of treatment with natalizumab.\n\n【49】Table 2 shows the overall incidence of infections. Influenza and influenza-like illness occurred more frequently in the natalizumab group than in the placebo group in the ENACT-2 trial. Specific types of serious infections observed in each group are shown in Table 2 . During the ENACT-2 trial, there was one report each of varicella pneumonia (after exposure to a child who had varicella–zoster) and cytomegalovirus hepatitis judged by the investigator not to be serious, both in natalizumab-treated patients. Both resolved with appropriate medical therapy, and there were no sequelae. In addition, one patient who received three doses of natalizumab in combination with azathioprine during the ENACT-1 study, nine doses of placebo in combination with azathioprine during the ENACT-2 study, and five doses of natalizumab without azathioprine during an open-label extension study after early discontinuation from the ENACT-2 study died from progressive multifocal leukoencephalopathy associated with the JC virus, a human polyomavirus.\n\n【50】During the ENACT-1 study, acute infusion reactions (any adverse event occurring within 120 minutes after the initiation of an infusion) occurred in 8 percent of patients in the placebo group and 11 percent of those who received natalizumab; the corresponding rates for the ENACT-2 study were 8 percent and 7 percent . Hypersensitivity-like reactions occurred in 2 percent of patients in the placebo group and 5 percent of those in the natalizumab group in the ENACT-1 study; the corresponding rates in the ENACT-2 study were 1 percent and 3 percent .\n\n【51】Immunogenicity\n--------------\n\n【52】Of the patients exposed to natalizumab during either study, 8 percent in the ENACT-1 study (53 of 650) and 9 percent in the ENACT-2 study (36 of 390) tested positive for antibodies against natalizumab (a positive test was defined as a concentration of at least 0.5 μg per milliliter at any time on at least one occasion). During the ENACT-2 study, patients were further classified as having persistent antibodies against natalizumab (defined as two positive antibody tests occurring at least six weeks apart or a single positive test at the last measurement) or transient antibodies (defined as antibody-positive but not meeting the definition of persistent antibodies). Patients in the ENACT-1 study could not be further classified because testing for antibodies was not performed until week 12. Six percent of patients in the ENACT-2 study were persistently positive for antibodies against natalizumab (23 of 390), and 3 percent were transiently positive (13 of 390). Testing was performed every 12 weeks during the ENACT-2 study. Concomitant immunosuppressive and corticosteroid therapy appeared to be moderately protective against the formation of antibodies against natalizumab in both trials, but the low overall rates of immunogenicity precluded statistical comparisons .\n\n【53】Acute infusion reactions occurred in 45 percent of patients who tested positive for antibodies against natalizumab in the ENACT-1 trial (24 of 53), as compared with 9 percent of patients who tested negative for antibodies (54 of 597, P<0.001). Of 650 patients in the ENACT-1 study who were tested, 5 had a serious hypersensitivity-like reaction, 3 of whom were positive for antibodies against natalizumab. In the ENACT-2 safety population, 19 percent of patients who tested positive for antibodies against natalizumab (7 of 36, all of whom had persistent antibodies) had either an acute infusion reaction (3 patients) or a hypersensitivity-like reaction (4 patients), as compared with 7 percent of patients who tested negative for antibodies (26 of 354, P=0.02). Of 390 patients (2 percent) in the ENACT-2 study who were tested, 8 had a hypersensitivity-like reaction, 4 of whom were persistently positive for antibodies against natalizumab. Although the incidence of acute infusion and hypersensitivity-like reactions was low, these events occurred more frequently among patients with persistent, but not transient, antibodies against natalizumab.\n\n【54】In the ENACT-1 study, of the 8 percent of patients who were treated with natalizumab and who were positive for antibodies against natalizumab, 53 percent (28 of 53) had a response at week 12, as compared with 62 percent of patients who tested negative for antibodies (371 of 597, P=0.18). In the ENACT-2 study, 7 percent of patients who had a response to induction therapy with natalizumab and who were randomly reassigned to natalizumab (11 of 168) tested positive for antibodies against natalizumab (7 of these 11 patients had persistent antibodies). None of these patients with persistent antibodies maintained a response through week 60, as compared with 3 of 4 patients with transient antibodies (75 percent) and 87 of 156 patients who tested negative for antibodies (56 percent). In addition, median lymphocyte counts over time in the seven patients with persistent antibodies were similar to those in patients receiving placebo, whereas patients with transient antibodies continued to have the expected elevations in circulating lymphocyte counts.\n\n【55】Discussion\n----------\n\n【56】In patients with Crohn's disease, induction and sustained maintenance of response and remission (particularly in patients who have had no response to currently available agents), corticosteroid-sparing therapies, and improved drug safety constitute important unmet medical needs. Inhibition of leukocyte adhesion and migration to sites of inflamed tissue represents a unique mechanism of action for the treatment of Crohn's disease. Although the results of the ENACT-1 trial failed to demonstrate that induction treatment with natalizumab was superior to placebo in patients with moderate-to-severe Crohn's disease, the ENACT-2 trial showed that among the group of patients who had a response to natalizumab, response and remission were more likely to be sustained through 60 weeks if natalizumab treatment was continued rather than stopped after three treatments.\n\n【57】Data from two previous phase 2 induction trials suggested that natalizumab may be effective for the induction of a response and remission in patients with moderate-to-severe Crohn's disease. The first double-blind, placebo-controlled study examined the efficacy of a single dose of natalizumab (3 mg per kilogram of body weight) or placebo. The CDAI score decreased by a mean of 45 points among the 18 natalizumab-treated patients, although this reduction was not significantly different from the mean reduction among the 12 patients receiving placebo (11 points).  The second double-blind, placebo-controlled, dose-finding study examined the efficacy of two doses of natalizumab (3 mg or 6 mg per kilogram) in 66 and 51 patients, one dose of natalizumab (3 mg per kilogram, followed by placebo) in 68 patients, or two doses of placebo in 63 patients. The findings of the prespecified primary analysis of remission at week 6 were not significant, but significant differences between natalizumab and placebo were observed at multiple times. \n\n【58】The ENACT-1 trial failed to demonstrate a significant benefit of natalizumab for induction therapy. A similar phenomenon has been reported in other placebo-controlled trials of a variety of other biologic agents in patients with Crohn's disease.  Subgroup analyses of these other studies have demonstrated that a lower concentration of C-reactive protein was associated with a higher likelihood of a response to placebo. Furthermore, a meta-analysis of factors contributing to higher rates of a response to placebo in induction trials involving patients with active Crohn's disease demonstrated that the duration of the study (greater than eight weeks), the frequency of study visits (less than four weeks apart), and the CDAI score at entry (less than 200 points) were important predictors of the rate of remission in the placebo group, with the duration of the study as the most important independent predictor. \n\n【59】To better evaluate the efficacy of natalizumab as an induction agent, we analyzed subpopulations of patients from the ENACT-1 trial who had objectively confirmed active inflammation or chronically active disease despite conventional therapies. These analyses demonstrated significantly greater response and remission rates for natalizumab than for placebo in patients with elevated C-reactive protein concentrations, active disease despite the use of immunosuppressants, or prior receipt of anti–TNF-α therapy. The analyses of patients who did not have these markers of active inflammation showed numerically greater (but not significant) rates of response and remission for natalizumab than for placebo in all the subgroups except patients without an elevated concentration of C-reactive protein at baseline (these patients had numerically greater \\[but not significant\\] response and remission rates for placebo than for natalizumab).\n\n【60】In the ENACT-2 maintenance trial, patients who received natalizumab were more than twice as likely as those who received placebo to have a sustained clinical response and almost twice as likely to have a sustained remission. Furthermore, natalizumab treatment had significant corticosteroid-sparing effects; nearly half the natalizumab-treated patients who were receiving corticosteroids at baseline in the ENACT-1 trial were not taking corticosteroids by week 60 of the ENACT-2 study, and more than 40 percent were in remission and corticosteroid-free. An unexpectedly high percentage of patients with a response to placebo in the ENACT-1 trial maintained that clinical benefit in the ENACT-2 trial, whether randomly reassigned to natalizumab or placebo, suggesting that some of these patients may not have had active Crohn's disease during the studies.\n\n【61】The incidence of serious adverse events is an important consideration in assessing the risk–benefit ratio of induction and maintenance therapies. The safety profile observed in both studies was similar to those in previous natalizumab studies.  All these trials lacked adequate statistical power to detect rare serious adverse events. Recently, progressive multifocal leukoencephalopathy associated with the JC virus developed in two patients with multiple sclerosis treated with natalizumab in combination with interferon beta-1a  and in one patient with Crohn's disease (described above)  ; two of these patients died. Further evaluation of this serious adverse event in patients who have been treated with natalizumab for any indication is under way.\n\n【62】The incidences of antibodies against natalizumab after induction therapy and of persistent antibodies against natalizumab during maintenance therapy were low. Concomitant use of immunosuppressive agents and corticosteroids, although not required for efficacy, may further reduce the formation of antibodies against natalizumab. When antibodies are persistently present, they appear to be associated with infusion reactions, hypersensitivity-like reactions, and loss of efficacy.\n\n【63】In conclusion, the response to induction therapy was nonsignificantly higher among patients treated with natalizumab than among patients who received placebo. Although the clinical benefit of therapy with natalizumab will ultimately need to be weighed against the potential risk of rare but serious adverse events, maintenance treatment with natalizumab in patients who had a response to natalizumab resulted in significantly higher rates of sustained response and remission and corticosteroid-sparing effects than withdrawal of treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "38a1846e-c4d3-41fc-8a37-71ea4276e0be", "title": "Placebo Effects in Medicine", "text": "【0】Placebo Effects in Medicine\n### Audio Interview\n\n【1】 Interview with Prof. Ted Kaptchuk on the outlook for harnessing the benefits of placebo effects in medicine. \n\n【2】Research has revealed placebo effects to be genuine biopsychosocial phenomena representing more than simply spontaneous remission or normal symptom fluctuations. How can this understanding be used to benefit patients?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2c226f82-84c6-4638-bf7a-e693f0033ac0", "title": "Obesity and Pharmacologic Control of the Body Clock", "text": "【0】Obesity and Pharmacologic Control of the Body Clock\nKnowledge of the mechanisms that govern mammalian circadian rhythm permits the molecular manipulation of these mechanisms. A recent study showed that pharmacologic intervention results in an increase in metabolism and subsequent weight loss in overfed, obese mice.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d7113466-4510-4902-af17-4353905fffde", "title": "Chlorambucil Treatment of Frequently Relapsing Nephrotic Syndrome", "text": "【0】Chlorambucil Treatment of Frequently Relapsing Nephrotic Syndrome\nAbstract\n--------\n\n【1】Chlorambucil, in combination with prednisone, was compared with prednisone alone in a randomized controlled trial in 21 children with either steroid-dependent or frequently relapsing nephrotic syndrome to assess its effect on the duration of remission and the rate of relapse. All control patients treated with prednisone alone continued to relapse at the same rate, with all patients experiencing a return of proteinuria by seven months. Conversely, those who received the same prednisone therapy along with chlorambucil for six to 12 weeks remained in complete remission, without further medication, during 12 to 34 months of follow-up observation. Complications were minimal. Immediate side effects commonly reported with cyclophosphamide were not seen with chlorambucil. Comparison with published reports also suggests that remission induced by chlorambucil is more stable than that after cyclophosphamide. Chlorambucil appears to be of value in the frequently relapsing nephrotic patient, adding an effect that is unattainable with prednisone alone.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "fcf9d89e-38d4-4301-a9b1-8ef6599a03d9", "title": "Contraception in Primary Care — Embracing the Institute of Medicine Challenge", "text": "【0】Contraception in Primary Care — Embracing the Institute of Medicine Challenge\nTo the Editor:\n--------------\n\n【1】On July 19, the Institute of Medicine released a historic report outlining key preventive health services for women to be covered by insurers under the Affordable Care Act (ACA) without consumer cost sharing. Particularly notable was the report's inclusion of contraception, with the implication that insurers should fully cover the costs for all contraceptive methods approved by the Food and Drug Administration (FDA), as well as for education and counseling.  These recommendations were accepted by the secretary of health and human services and will be incorporated into the ACA's minimum package of essential health benefits by August 2012. As Cleland et al. have argued,  adoption of these recommendations would represent a significant step forward for women's health, since half of U.S. pregnancies are unintended, and unintended pregnancies carry higher risks for women and children.\n\n【2】Also needed, however, are reforms in medical education and primary care delivery to ensure that women receive safe, effective, and appropriate contraceptive care. Surveys of medical schools, residents, and physicians show that training in contraceptive counseling and provision is insufficient.  A particularly problematic issue is the shortage of providers trained in highly effective, reversible contraceptives, such as intrauterine devices (IUDs) and implants. These methods have been recommended by the American College of Obstetricians and Gynecologists as first-line contraceptives for most women, and experts believe that increased use would dramatically reduce the rate of unintended pregnancies. Despite data supporting their use, however, IUDs or implants are used by only 5.6% of U.S. women who use contraception.  This paradox is largely attributable to the limited number of clinicians who are knowledgeable about these methods and trained in their placement. Since these procedures pose a low risk and are easily learned, efforts are needed to facilitate the training of interested primary care providers, including internists, pediatricians, and midlevel providers.\n\n【3】In addition, primary care practices and patient-centered medical homes must prioritize contraceptive care. To facilitate the provision of effective family planning, payment reforms must ensure that primary care providers are adequately compensated for time spent in counseling patients regarding contraception and preconception. Electronic decision support may also be useful in identifying women at risk for unintended pregnancy and ensuring that contraceptive counseling is provided in a timely manner. When referral for contraceptive care is necessary, facilities must develop efficient referral processes that ensure patients' timely access to their chosen method.\n\n【4】Since family planning protects the health of women, children, and communities, it is a quintessential primary care service. U.S. medical schools, residencies, and the health care system must rise to meet the challenge of translating the Institute of Medicine's guidelines into clinical practice.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5bca1e1c-5b42-4fa2-b719-fdd14e5c7304", "title": "Nivolumab plus Cabozantinib versus Sunitinib for Advanced Renal-Cell Carcinoma", "text": "【0】Nivolumab plus Cabozantinib versus Sunitinib for Advanced Renal-Cell Carcinoma\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The efficacy and safety of nivolumab plus cabozantinib as compared with those of sunitinib in the treatment of previously untreated advanced renal-cell carcinoma are not known.\n\n【3】Methods\n-------\n\n【4】In this phase 3, randomized, open-label trial, we randomly assigned adults with previously untreated clear-cell, advanced renal-cell carcinoma to receive either nivolumab (240 mg every 2 weeks) plus cabozantinib (40 mg once daily) or sunitinib (50 mg once daily for 4 weeks of each 6-week cycle). The primary end point was progression-free survival, as determined by blinded independent central review. Secondary end points included overall survival, objective response as determined by independent review, and safety. Health-related quality of life was an exploratory end point.\n\n【5】Results\n-------\n\n【6】Overall, 651 patients were assigned to receive nivolumab plus cabozantinib (323 patients) or sunitinib (328 patients). At a median follow-up of 18.1 months for overall survival, the median progression-free survival was 16.6 months (95% confidence interval \\[CI\\], 12.5 to 24.9) with nivolumab plus cabozantinib and 8.3 months (95% CI, 7.0 to 9.7) with sunitinib (hazard ratio for disease progression or death, 0.51; 95% CI, 0.41 to 0.64; P<0.001). The probability of overall survival at 12 months was 85.7% (95% CI, 81.3 to 89.1) with nivolumab plus cabozantinib and 75.6% (95% CI, 70.5 to 80.0) with sunitinib (hazard ratio for death, 0.60; 98.89% CI, 0.40 to 0.89; P=0.001). An objective response occurred in 55.7% of the patients receiving nivolumab plus cabozantinib and in 27.1% of those receiving sunitinib (P<0.001). Efficacy benefits with nivolumab plus cabozantinib were consistent across subgroups. Adverse events of any cause of grade 3 or higher occurred in 75.3% of the 320 patients receiving nivolumab plus cabozantinib and in 70.6% of the 320 patients receiving sunitinib. Overall, 19.7% of the patients in the combination group discontinued at least one of the trial drugs owing to adverse events, and 5.6% discontinued both. Patients reported better health-related quality of life with nivolumab plus cabozantinib than with sunitinib.\n\n【7】Conclusions\n-----------\n\n【8】Nivolumab plus cabozantinib had significant benefits over sunitinib with respect to progression-free survival, overall survival, and likelihood of response in patients with previously untreated advanced renal-cell carcinoma. \n\n【9】Introduction\n------------\n\n【10】Renal-cell carcinoma is a tumor characterized by loss of the _VHL_ gene, and this loss leads to increased angiogenesis.  Immunotherapies and antiangiogenic therapies have improved outcomes, and the treatment landscape has expanded rapidly.  Clinical benefits in patients with advanced renal-cell carcinoma have been observed with regimens that include different combinations of immune, antiangiogenic, and signal transduction–blocking agents,  and refining the individual components may further improve outcomes.\n\n【11】Both cabozantinib (a small-molecule inhibitor of tyrosine kinases) and nivolumab (a programmed death 1 \\[PD-1\\] immune checkpoint inhibitor antibody) are approved therapies for the treatment of advanced renal-cell carcinoma and have been shown to improve overall survival as single agents in phase 3 trials.  Cabozantinib inhibits tyrosine kinases involved in tumor-cell proliferation, neovascularization, and immune-cell regulation, including MET, vascular endothelial growth factor receptor 1 (VEGF-R1) through VEGF-R3, and the TAM family of kinases (TYRO3, AXL, and MER), and has immunomodulatory properties that counteract tumor-induced immunosuppression, which may enhance response to immune-checkpoint inhibition.  In a phase 1 dose-finding study of nivolumab plus cabozantinib involving patients with advanced genitourinary cancers, a cabozantinib dose of 40 mg per day had similar efficacy to that of 60 mg per day but had fewer toxic effects.  We conducted a phase 3 trial (CheckMate 9ER) to compare the efficacy and safety of the combination of nivolumab plus cabozantinib with sunitinib in the first-line treatment of patients with advanced renal-cell carcinoma with clear-cell histologic features.\n\n【12】Methods\n-------\n\n【13】Patients\n--------\n\n【14】Eligible patients were adults with previously untreated advanced renal-cell carcinoma with a clear-cell component. Patients had any International Metastatic Renal-Cell Carcinoma Database Consortium (IMDC) prognostic risk score  and a Karnofsky performance-status score of at least 70 (on a scale from 0 to 100, with lower scores indicating greater disability).  Patients had measurable disease according to Response Evaluation Criteria in Solid Tumors, version 1.1, as assessed by the investigator and either advanced renal-cell carcinoma (not amenable to curative surgery or radiation therapy) or metastatic renal-cell carcinoma (American Joint Committee on Cancer stage IV). Additional enrollment criteria included no previous systemic therapy for renal-cell carcinoma (one previous adjuvant or neoadjuvant therapy for completely resectable renal-cell carcinoma was permitted) and available tumor tissue for analysis. Patients were excluded if they had active central nervous system metastases or active autoimmune disease or had received systemic treatment with either glucocorticoids (>10 mg of prednisone equivalent per day) or other immunosuppressive medications within 14 days before randomization.\n\n【15】Trial Design and Treatments\n---------------------------\n\n【16】CheckMate 9ER is a phase 3, randomized, open-label trial of nivolumab combined with cabozantinib as compared with sunitinib monotherapy. Patients underwent randomization in a  ratio and were stratified according to IMDC prognostic risk score (0 \\[favorable\\] vs. 1 or 2 \\[intermediate\\] vs. 3 to 6 \\[poor\\]),  geographic region (United States and Europe vs. the rest of the world), and tumor expression of the PD-1 ligand PD-L1 (≥1% vs. <1% or indeterminate). Nivolumab was administered intravenously at a dose of 240 mg every 2 weeks, and cabozantinib was administered orally at a dose of 40 mg once daily. Sunitinib was administered orally at a dose of 50 mg once daily for 4 weeks, followed by 2 weeks off (6-week cycle). All trial treatment continued until disease progression or unacceptable toxic effects, with a maximum 2-year duration of nivolumab treatment. Crossover between groups was not permitted. Dose reductions were not allowed for nivolumab but were permitted for cabozantinib and sunitinib, according to the protocol. Dose delays for adverse events were permitted for all trial drugs. Discontinuation assessments for nivolumab and cabozantinib were made separately for each drug; if discontinuation criteria were met for only one drug, treatment could continue with the other drug that was not related to the observed toxic effect, according to the protocol. Dose-reduction specifications and discontinuation criteria for both groups are detailed in the trial protocol.\n\n【17】End Points and Assessments\n--------------------------\n\n【18】The primary end point was progression-free survival among all the patients who underwent randomization (intention-to-treat population). The secondary end points were overall survival and objective response (including time to and duration of response) in the intention-to-treat population and safety in patients who received at least one dose of trial treatment. Progression-free survival and objective response were assessed by blinded independent central review. Efficacy outcomes according to key disease and demographic characteristics at baseline were evaluated by means of prespecified supportive subgroup analyses. An exploratory analysis of secondary progression-free survival outcomes, including subsequent therapy (progression-free survival 2), was performed. Health-related quality of life was assessed as an exploratory end point with the use of the National Comprehensive Cancer Network 19-item Functional Assessment of Cancer Therapy–Kidney Symptom Index (FKSI-19; scores range from 0 to 76, with higher scores indicating fewer symptoms) and the 9-item subset of disease-related symptoms (FKSI-DRS; scores range from 0 to 36, with higher scores indicating fewer symptoms).  Threshold values for the change in scores that was considered important to patients for the FKSI-19 instrument and subscales have been estimated (total score, 3 points; FKSI-DRS, 1 point). \n\n【19】Adverse events were graded according to the National Cancer Institute Common Terminology Criteria for Adverse Events, version 4.0.  The incidences of adverse events (both of any cause and treatment-related) and of events leading to discontinuation of trial treatment or death are summarized. Immune-mediated adverse events and the use of glucocorticoids (≥40 mg prednisone daily or equivalent) to manage these events are also reported. In addition, PD-L1 expression was defined as the percent of positive tumor cell membrane staining in a minimum of 100 tumor cells that could be evaluated by means of the validated Dako PD-L1 IHC 28-8 pharmDx assay. \n\n【20】Trial Oversight\n---------------\n\n【21】This trial was approved by the institutional review board or an ethics committee at each site and was conducted in accordance with Good Clinical Practice guidelines defined by the International Council for Harmonisation. Enrolled patients provided written informed consent according to the principles of the Declaration of Helsinki. Efficacy and safety data were reviewed by an independent data monitoring committee. The trial was designed by the authors in collaboration with the sponsor (Bristol Myers Squibb) and partner (Exelixis). The authors vouch for the completeness and accuracy of the data and for the fidelity of the trial to the protocol. All the authors contributed to drafting and provided final approval of the manuscript. As part of the site agreement, investigators agreed to keep all aspects and outcomes of the trial confidential. A medical writer employed by the sponsor assisted with the preparation of the manuscript.\n\n【22】Statistical Analysis\n--------------------\n\n【23】It was estimated that 638 patients would undergo randomization. The overall alpha for this trial was 0.05 (two-sided) for the primary end point (progression-free survival) and secondary end points (overall survival, followed by objective response), and a hierarchical testing procedure was used.  Progression-free survival was to be evaluated at an alpha level of 0.05 (single final analysis). If the between-group difference in progression-free survival was significant, analysis of overall survival would be performed at an overall alpha level of 0.05, with the use of a hierarchical testing procedure. If the difference in progression-free survival (primary end point) was significant, it was specified that the trial would continue until the between-group difference in overall survival (secondary end point) was significant (0.011 at the first interim, 0.025 at the second interim, and 0.041 at the final analysis with an O’Brien and Fleming alpha spending function).  On rejection of the null hypothesis for overall survival, analysis of objective response would be performed at an alpha level of 0.05 (single final analysis), according to a hierarchical testing procedure. Confidence intervals were defined on the basis of the respective alpha level assigned to a given end point. All P values reported are two-sided. Further details of the analysis are included in the Methods section of the Supplementary Appendix , and the full statistical analysis plan is available with the protocol.\n\n【24】Progression-free and overall survival were compared between the treatment groups with the use of a stratified log-rank test, and the estimate of the hazard ratio between treatment groups was calculated by means of a stratified Cox proportional-hazards model that used IMDC prognostic risk score (0 vs. 1 or 2 vs. 3 to 6), tumor PD-L1 expression (≥1% vs. <1% or indeterminate), and region (United States or Europe vs. the rest of the world) as stratification factors. Progression-free and overall survival and response duration were estimated with the use of Kaplan–Meier methods. Estimates of the percentage of patients with an objective response, along with the exact two-sided 95% confidence interval, were computed according to the Clopper–Pearson method.  The forest plots of the unstratified hazard ratios for progression-free and overall survival and a forest plot of unweighted differences in the percentage of patients with an objective response were produced for each prespecified subgroup, with no adjustment for multiplicity. Change from baseline in health-related quality of life was assessed with the use of descriptive statistics, and nominal P values based on a linear-regression model for repeated measures that controlled for treatment group, time point, baseline patient-reported outcomes score, and the stratification factors (IMDC prognostic risk score, tumor PD-L1 expression, and geographic region) are reported. All data reported are based on the final analysis of progression-free survival, the first interim analysis of overall survival, and the final analysis of objective response from a database lock of March 30, 2020.\n\n【25】Results\n-------\n\n【26】Patients and Treatments\n-----------------------\n\n【27】Table 1. Demographic and Clinical Characteristics of the Patients at Baseline (Intention-to-Treat Population).\n\n【28】Between September 2017 and May 2019, a total of 651 patients underwent randomization at 125 sites in 18 countries; 323 patients made up the intention-to-treat population in the nivolumab-plus-cabozantinib group, and 328 patients made up the intention-to-treat population in the sunitinib group. Among patients in the intention-to-treat population, 22.4% had IMDC favorable-risk, 57.8% had intermediate-risk, and 19.8% had poor-risk prognostic features; 25.5% had at least 1% and 74.5% had less than 1% (or indeterminate) tumor PD-L1 expression at the time of stratification. Patient characteristics at baseline were representative of a population with previously untreated advanced renal-cell carcinoma and were balanced in the two treatment groups . The primary reason for discontinuation of trial treatment was disease progression; 55.6% of treated patients in the nivolumab-plus-cabozantinib group and 28.8% of those in the sunitinib group continued to receive treatment at the time of this analysis . Details of subsequent anticancer therapy (started on or after the date of the first trial dose) are summarized in Table S1.\n\n【29】Efficacy\n--------\n\n【30】Figure 1. Progression-free and Overall Survival in the Intention-to-Treat Population.\n\n【31】The intention-to-treat population included all the patients who underwent randomization. Shown are Kaplan–Meier estimates of progression-free survival  and overall survival . Progression-free survival was assessed according to Response Evaluation Criteria in Solid Tumors, version 1.1, by blinded independent central review of radiologic imaging. NE denotes could not be estimated, and NR not reached.\n\n【32】At a median follow-up for overall survival of 18.1 months (range, 10.6 to 30.6), the median progression-free survival was 16.6 months (95% confidence interval \\[CI\\], 12.5 to 24.9) with nivolumab plus cabozantinib and 8.3 months (95% CI, 7.0 to 9.7) with sunitinib, and the probability of progression-free survival at 12 months was 57.6% (95% CI, 51.7 to 63.1) and 36.9% (95% CI, 31.1 to 42.8), respectively. Nivolumab plus cabozantinib had a superior progression-free survival benefit over sunitinib , with a hazard ratio for disease progression or death of 0.51 (95% CI, 0.41 to 0.64; P<0.001). Nivolumab plus cabozantinib also had a significant overall survival benefit over sunitinib. The probability of overall survival at 12 months was 85.7% (95% CI, 81.3 to 89.1) with nivolumab plus cabozantinib and 75.6% (95% CI, 70.5 to 80.0) with sunitinib (hazard ratio for death, 0.60; 98.89% CI, 0.40 to 0.89; P=0.001). The median overall survival was not reached in either group .\n\n【33】Table 2. Objective Response (Intention-to-Treat Population).\n\n【34】The percentage of patients who had an objective response according to independent review was 55.7% (95% CI, 50.1 to 61.2) with nivolumab plus cabozantinib and 27.1% (95% CI, 22.4 to 32.3) with sunitinib (P<0.001); a complete response occurred in 8.0% of the patients in the nivolumab-plus-cabozantinib group and in 4.6% of those in the sunitinib group . The median time to response was 2.8 months with nivolumab plus cabozantinib and 4.2 months with sunitinib, and the median duration of response was 20.2 months and 11.5 months, respectively . The probability of ongoing response at 12 months was 71.1% with nivolumab plus cabozantinib and 40.9% with sunitinib . Of 284 patients with data that could be evaluated in the nivolumab-plus-cabozantinib group, 94.7% had any reduction in the sum of target-lesion diameters, and 70.4% had a reduction of at least 30%; of 259 patients with data that could be evaluated in the sunitinib group, 84.9% had any reduction and 42.5% had a reduction of at least 30% .\n\n【35】Figure 2. Progression-free and Overall Survival According to Subgroup.\n\n【36】Shown is the analysis of progression-free survival  and overall survival , according to subgroup. The International Metastatic Renal-Cell Carcinoma Database Consortium (IMDC) prognostic risk, programmed death ligand 1 (PD-L1) status, and geographic region (stratification factors) were recorded at screening by means of interactive response technology among all the patients who underwent randomization. Karnofsky performance-status scores range from 0 to 100, with lower scores indicating greater disability. Median progression-free survival and 95% confidence intervals according to subgroup are provided in Table S2 in the Supplementary Appendix .\n\n【37】The benefits of nivolumab plus cabozantinib over sunitinib with respect to progression-free survival, overall survival, and objective response were generally consistent across subgroups, including IMDC risk status, tumor PD-L1 expression, and the presence or absence of bone metastases. \n\n【38】Exposure and Safety\n-------------------\n\n【39】A total of 320 patients in each group received at least one dose of trial treatment. The median duration of treatment was 14.3 months (range, 0.2 to 27.3) in the nivolumab-plus-cabozantinib group and 9.2 months (range, 0.8 to 27.6) in the sunitinib group. In the nivolumab-plus-cabozantinib group, the median duration of treatment was 13.3 months (range, 0 to 24.0) with nivolumab and 13.8 months (range, 0.2 to 27.3) with cabozantinib. Among all treated patients, 71.9% had at least one nivolumab dose delay, 68.1% had at least one cabozantinib dose delay, and 51.9% had at least one sunitinib dose delay; 56.3% of the patients had a reduction in the dose of cabozantinib, and 51.6% had a reduction in the dose of sunitinib.\n\n【40】Table 3. Adverse Events (As-Treated Population).\n\n【41】Adverse events of any cause during treatment occurred in 99.7% of the patients who received nivolumab plus cabozantinib and in 99.1% of those who received sunitinib; adverse events of any cause of grade 3 or higher occurred in 75.3% of the patients in the nivolumab-plus-cabozantinib group and in 70.6% of those in the sunitinib group . Treatment-related adverse events occurred in 96.6% of the patients with nivolumab plus cabozantinib and in 93.1% with sunitinib; 60.6% of the patients in the nivolumab-plus-cabozantinib group and 50.9% in the sunitinib group had treatment-related adverse events of grade 3 or higher . Among patients treated with nivolumab plus cabozantinib, 9.8% had grade 3 or 4 laboratory abnormalities in alanine aminotransferase (ALT) levels, and 7.9% had grade 3 or 4 abnormalities in aspartate aminotransferase (AST) levels; overall, resolution to grade 0 or 1 occurred in 82.9%. In the sunitinib group, 3.5% had grade 3 or 4 laboratory abnormalities in ALT levels, and 2.6% had grade 3 or 4 abnormalities in AST levels; overall, resolution to grade 0 or 1 occurred in 66.7%. Immune-mediated adverse events are summarized in Table S4. Overall, 19.1% of the patients treated with nivolumab plus cabozantinib received glucocorticoids (≥40 mg of prednisone daily or equivalent) to manage immune-mediated adverse events for any duration of time; 10.3% and 3.8% of patients received glucocorticoids continuously for at least 14 days and at least 30 days, respectively.\n\n【42】Adverse events of any cause led to discontinuation of a trial drug in 19.7% of the patients treated with nivolumab plus cabozantinib (6.6% discontinued nivolumab only, 7.5% discontinued cabozantinib only, and 5.6% discontinued both nivolumab and cabozantinib) and in 16.9% of the patients treated with sunitinib. Overall, one death was considered by investigators to be treatment-related with nivolumab plus cabozantinib (small-intestine perforation), and two deaths were considered to be treatment-related with sunitinib (pneumonia and respiratory distress in one patient each).\n\n【43】Quality of Life\n---------------\n\n【44】The mean (±SD) FKSI-19 total scores at baseline were similar in the two groups (58.7±10.6 with nivolumab plus cabozantinib and 58.4±9.9 with sunitinib); the percentage of patients who completed the FKSI-19 questionnaire was more than 90% in both groups at baseline, and the percentage was at least 80% at all subsequent assessments during treatment with sufficient data (≥10 patients) through at least week 91 in both groups. Quality of life was maintained over time with nivolumab plus cabozantinib, whereas a consistent deterioration from baseline was reported with sunitinib. When we controlled for baseline score and other relevant covariates, patients treated with nivolumab plus cabozantinib had better quality of life than those treated with sunitinib at all time points through week 91 . In addition, disease-related symptoms as measured by the FKSI-DRS subscale improved from baseline in patients in the nivolumab-plus-cabozantinib group, whereas patients in the sunitinib group had a decline from baseline after week 7 through week 91 . The between-group differences were significant (P<0.05) at all time points except week 7 for the FKSI-19 total score and week 79 for the FKSI-DRS score.\n\n【45】Discussion\n----------\n\n【46】Progression-free survival (primary end point) was significantly longer with nivolumab plus cabozantinib than with sunitinib among patients with previously untreated advanced renal-cell carcinoma with a clear-cell component. The risk of disease progression or death was 49% lower with nivolumab plus cabozantinib than with sunitinib, and the median progression-free survival was twice as long in the combination group (16.6 months, vs. 8.3 months in the sunitinib group). Overall survival and the likelihood of objective response (secondary end points) were also better with the combination. The risk of death was 40% lower with nivolumab plus cabozantinib than with sunitinib. The percentage of patients with an objective response was twice as high with nivolumab plus cabozantinib than with sunitinib (55.7% vs. 27.1%), and complete responses were also more frequent with nivolumab plus cabozantinib (8.0%, vs. 4.6% with sunitinib). In a supportive subgroup analysis, nivolumab plus cabozantinib had consistent benefits over sunitinib with respect to progression-free survival, overall survival, and the likelihood of response, regardless of key baseline characteristics, including IMDC risk status, tumor PD-L1 expression, and the presence or absence of bone metastases. These results are consistent with previous data suggesting that cabozantinib may enhance immune-checkpoint inhibition. \n\n【47】The adverse-event profile of nivolumab plus cabozantinib was not trivial but was consistent with previous studies of each agent as monotherapy, and no new safety signals were identified.  One death was considered by the investigators to be related to treatment with the combination. The incidence of the most common treatment-related adverse events of any grade or of grade 3 or higher that were observed with nivolumab plus cabozantinib was similar to those seen with sunitinib monotherapy, including palmar–plantar erythrodysesthesia syndrome, hypertension, hypothyroidism, and fatigue. Most immune-mediated adverse events that were reported in the nivolumab-plus-cabozantinib group were of low grade, and 19.1% of the patients receiving the combination received glucocorticoids (≥40 mg of prednisone daily or equivalent) for any duration of time. Nivolumab or cabozantinib or both were discontinued before progression in 19.7% of patients owing to adverse events, including 5.6% who discontinued both. Yet, the patient-reported outcome measures suggested that the toxic effects did not have a major adverse effect on quality of life.\n\n【48】A limitation of this analysis is the relatively short duration of follow-up. As of the data cutoff date, the median overall survival was not reached in either group; follow-up is ongoing. In particular, few deaths have occurred in the IMDC favorable-risk group, and additional follow-up may better characterize survival with nivolumab plus cabozantinib as compared with sunitinib in these patients. Assessment of tumor response is also ongoing to determine longer-term outcomes, including depth and durability of response, especially complete responses. Another potential limitation of this trial is the lack of blinding, which could not be implemented in this trial.\n\n【49】First-line immunotherapy-based regimens have transformed the treatment landscape for advanced renal-cell carcinoma, providing significant improvements in clinical outcomes, including overall survival.  Dual checkpoint inhibition with nivolumab plus ipilimumab was the first to show a significant long-term survival advantage over sunitinib with a high incidence of durable and complete responses and better quality of life in the phase 3 CheckMate 214 trial; consistent outcomes were observed in intermediate- and poor-risk patients and the intention-to-treat population, which have been maintained after extended follow-up.  Regimens that combine an anti–PD-1 or anti–PD-L1 antibody with a tyrosine kinase inhibitor have also shown clinical benefits over sunitinib in phase 3 trials,  although the magnitude of benefit with respect to progression-free survival in the current trial with nivolumab plus cabozantinib as compared with sunitinib is notable in this context. Data on health-related quality of life for the new treatment combinations are limited; however, available patient-reported outcomes suggest no advantage with pembrolizumab–axitinib as compared with sunitinib through 30 weeks.  Patients had significantly better quality of life with nivolumab plus cabozantinib than with sunitinib at most time points through 91 weeks as measured by the FKSI-19 total scale and FKSI-DRS subscale. With improved treatment options, more patients are surviving substantially longer, and many receive treatment for an extended period of time. Therefore, overall efficacy, safety, and quality-of-life benefits as well as individual patient characteristics are important considerations when selecting appropriate therapy. \n\n【50】In this trial involving patients with previously untreated advanced renal-cell carcinoma, nivolumab plus cabozantinib had significant benefits over sunitinib with respect to progression-free survival, overall survival, and the likelihood of objective response. The combination was associated with substantial toxic effects; 19.7% of the patients in the combination group discontinued at least one of the trial drugs prematurely, and 5.6% discontinued both; nevertheless, quality of life was maintained at a high level. In addition, efficacy benefits with nivolumab plus cabozantinib were consistent across prespecified subgroups.\n\n【51】Table 1. Demographic and Clinical Characteristics of the Patients at Baseline (Intention-to-Treat Population). \n\n| Characteristic | Nivolumab plus Cabozantinib(N=323) | Sunitinib(N=328) |\n| --- | --- | --- |\n| Age |  |  |\n| Median (range) — yr | 62 (29–90) | 61 (28–86) |\n| <65 yr — no. (%) | 191 (59.1) | 210 (64.0) |\n| ≥65 yr — no. (%) | 132 (40.9) | 118 (36.0) |\n| Sex — no. (%) |  |  |\n| Male | 249 (77.1) | 232 (70.7) |\n| Female | 74 (22.9) | 96 (29.3) |\n| Geographic region — no. (%) |  |  |\n| United States or Europe | 158 (48.9) | 161 (49.1) |\n| Rest of the world | 165 (51.1) | 167 (50.9) |\n| Karnofsky performance-status score — no. (%)  |  |  |\n| 90 or 100 | 257 (79.6) | 241 (73.5) |\n| 70 or 80 | 66 (20.4) | 85 (25.9) |\n| Not reported | 0 | 2 (0.6) |\n| IMDC prognostic risk score — no. (%) |  |  |\n| Favorable: 0 | 74 (22.9) | 72 (22.0) |\n| Intermediate: 1 or 2 | 188 (58.2) | 188 (57.3) |\n| Poor: 3–6 | 61 (18.9) | 68 (20.7) |\n| Tumor PD-L1 expression — no. (%) |  |  |\n| ≥1% | 83 (25.7) | 83 (25.3) |\n| <1% or indeterminate | 240 (74.3) | 245 (74.7) |\n| Sarcomatoid features — no./total no. (%)  |  |  |\n| Yes | 34/313 (10.9) | 41/319 (12.9) |\n| No | 279/313 (89.1) | 278/319 (87.1) |\n| Previous radiotherapy — no. (%) | 46 (14.2) | 45 (13.7) |\n| Previous nephrectomy — no. (%) | 222 (68.7) | 233 (71.0) |\n| No. of sites with target or nontarget lesions — no. (%)  |  |  |\n| 1 | 63 (19.5) | 69 (21.0) |\n| ≥2 | 259 (80.2) | 256 (78.0) |\n| Most common sites of metastasis — no. (%) |  |  |\n| Lung | 238 (73.7) | 249 (75.9) |\n| Lymph node | 130 (40.2) | 131 (39.9) |\n| Bone | 78 (24.1) | 72 (22.0) |\n| Liver | 73 (22.6) | 53 (16.2) |\n| Adrenal gland | 36 (11.1) | 36 (11.0) |\n\n【53】 The intention-to-treat population includes all the patients who underwent randomization. The International Metastatic Renal-Cell Carcinoma Database Consortium (IMDC) prognostic risk score, programmed death ligand 1 (PD-L1) status, and geographic region (stratification factors) were recorded at screening by means of interactive response technology.\n\n【54】 Karnofsky performance-status scores range from 0 to 100, with lower scores indicating greater disability.\n\n【55】 Sarcomatoid status was not reported in 10 patients in the nivolumab-plus-cabozantinib group and in 9 patients in the sunitinib group.\n\n【56】 Data are for tumor sites defined at baseline by the investigators according to Response Evaluation Criteria in Solid Tumors (RECIST), version 1.1. The number of target or nontarget lesions at baseline was not reported for one patient in the nivolumab-plus-cabozantinib group and for three patients in the sunitinib group.\n\n【57】Table 2. Objective Response (Intention-to-Treat Population). \n\n| Variable | Nivolumab plus Cabozantinib(N=323) | Sunitinib(N=328) |\n| --- | --- | --- |\n| Confirmed objective response — % (95% CI)  | 55.7 (50.1–61.2) | 27.1 (22.4–32.3) |\n| Confirmed best overall response — no. (%) |  |  |\n| Complete response | 26 (8.0) | 15 (4.6) |\n| Partial response | 154 (47.7) | 74 (22.6) |\n| Stable disease | 104 (32.2) | 138 (42.1) |\n| Progressive disease | 18 (5.6) | 45 (13.7) |\n| Unable to determine or not reported | 21 (6.5) | 56 (17.1) |\n| Median time to response (interquartile range) — mo   | 2.8 (2.8–4.2) | 4.2 (2.8–6.9) |\n| Median duration of response (95% CI) — mo   | 20.2 (17.3–NE) | 11.5 (8.3–18.4) |\n\n【59】 Response was assessed according to RECIST, version 1.1, by blinded independent central review of radiologic imaging. Percentages may not total 100 because of rounding. NE denotes could not be estimated.\n\n【60】 The estimated difference in objective response between the nivolumab-plus-cabozantinib group and the sunitinib group was 28.6 percentage points (95% CI, 21.7 to 35.6), and the P value was less than 0.001.\n\n【61】 The median time to response and median duration of response were calculated only for patients who had a complete or partial response (180 patients in the nivolumab-plus-cabozantinib group and 89 patients in the sunitinib group).\n\n【62】 The median time to response was 2.8 months (range, 1.0 to 19.4) with nivolumab plus cabozantinib and 4.2 months (range, 1.7 to 12.3) with sunitinib.\n\n【63】 The median duration of response was 20.2 months (range, 1.4+ to 22.2+) with nivolumab plus cabozantinib and 11.5 months (range, 1.3+ to 18.4) with sunitinib. The plus sign indicates a censored value.\n\n【64】Table 3. Adverse Events (As-Treated Population). \n\n| Event | Nivolumab plus Cabozantinib(N=320) | Nivolumab plus Cabozantinib(N=320) | Sunitinib(N=320) | Sunitinib(N=320) |\n| --- | --- | --- | --- | --- |\n|  | Any Grade | Grade ≥3 | Any Grade | Grade ≥3 |\n|  | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) |\n| Any event | 319 (99.7) | 241 (75.3) | 317 (99.1) | 226 (70.6) |\n| Diarrhea | 204 (63.8) | 22 (6.9) | 151 (47.2) | 14 (4.4) |\n| Palmar–plantar erythrodysesthesia | 128 (40.0) | 24 (7.5) | 130 (40.6) | 24 (7.5) |\n| Hypertension | 111 (34.7) | 40 (12.5) | 119 (37.2) | 42 (13.1) |\n| Hypothyroidism | 109 (34.1) | 1 (0.3) | 94 (29.4) | 1 (0.3) |\n| Fatigue | 103 (32.2) | 11 (3.4) | 111 (34.7) | 15 (4.7) |\n| Increased ALT level | 90 (28.1) | 17 (5.3) | 27 (8.4) | 7 (2.2) |\n| Decreased appetite | 90 (28.1) | 6 (1.9) | 65 (20.3) | 4 (1.2) |\n| Nausea | 85 (26.6) | 2 (0.6) | 98 (30.6) | 1 (0.3) |\n| Increased AST level | 81 (25.3) | 11 (3.4) | 35 (10.9) | 4 (1.2) |\n| Dysgeusia | 76 (23.8) | 0 | 69 (21.6) | 0 |\n| Asthenia | 71 (22.2) | 14 (4.4) | 59 (18.4) | 10 (3.1) |\n| Rash | 69 (21.6) | 6 (1.9) | 26 (8.1) | 0 |\n| Mucosal inflammation | 66 (20.6) | 3 (0.9) | 81 (25.3) | 8 (2.5) |\n| Pruritus | 60 (18.8) | 1 (0.3) | 14 (4.4) | 0 |\n| Arthralgia | 59 (18.4) | 1 (0.3) | 29 (9.1) | 1 (0.3) |\n| Back pain | 58 (18.1) | 5 (1.6) | 40 (12.5) | 6 (1.9) |\n| Vomiting | 55 (17.2) | 6 (1.9) | 66 (20.6) | 1 (0.3) |\n| Cough | 55 (17.2) | 0 | 51 (15.9) | 0 |\n| Dysphonia | 55 (17.2) | 1 (0.3) | 11 (3.4) | 0 |\n| Stomatitis | 54 (16.9) | 8 (2.5) | 79 (24.7) | 7 (2.2) |\n| Increased lipase level | 53 (16.6) | 20 (6.2) | 38 (11.9) | 15 (4.7) |\n| Hyponatremia | 51 (15.9) | 30 (9.4) | 28 (8.8) | 19 (5.9) |\n| Abdominal pain | 50 (15.6) | 5 (1.6) | 27 (8.4) | 1 (0.3) |\n| Headache | 50 (15.6) | 0 | 37 (11.6) | 2 (0.6) |\n| Anemia | 48 (15.0) | 6 (1.9) | 81 (25.3) | 12 (3.8) |\n| Increased amylase level | 47 (14.7) | 10 (3.1) | 29 (9.1) | 8 (2.5) |\n| Hypophosphatemia | 46 (14.4) | 19 (5.9) | 18 (5.6) | 4 (1.2) |\n| Hypomagnesemia | 44 (13.8) | 2 (0.6) | 15 (4.7) | 2 (0.6) |\n| Increased blood creatinine level | 42 (13.1) | 4 (1.2) | 43 (13.4) | 1 (0.3) |\n| Constipation | 39 (12.2) | 3 (0.9) | 40 (12.5) | 1 (0.3) |\n| Pyrexia | 39 (12.2) | 2 (0.6) | 27 (8.4) | 1 (0.3) |\n| Muscle spasms | 38 (11.9) | 0 | 5 (1.6) | 0 |\n| Increased blood alkaline phosphatase level | 37 (11.6) | 3 (0.9) | 26 (8.1) | 2 (0.6) |\n| Upper respiratory tract infection | 36 (11.2) | 1 (0.3) | 12 (3.8) | 1 (0.3) |\n| Decreased weight | 35 (10.9) | 2 (0.6) | 10 (3.1) | 0 |\n| Peripheral edema | 34 (10.6) | 1 (0.3) | 28 (8.8) | 0 |\n| Proteinuria | 33 (10.3) | 9 (2.8) | 25 (7.8) | 7 (2.2) |\n| Dizziness | 33 (10.3) | 1 (0.3) | 19 (5.9) | 0 |\n| Hyperthyroidism | 32 (10.0) | 2 (0.6) | 9 (2.8) | 0 |\n| Dyspepsia | 26 (8.1) | 0 | 39 (12.2) | 1 (0.3) |\n| Thrombocytopenia | 25 (7.8) | 2 (0.6) | 62 (19.4) | 15 (4.7) |\n| Gastroesophageal reflux disease | 25 (7.8) | 0 | 36 (11.2) | 0 |\n| Epistaxis | 22 (6.9) | 0 | 32 (10.0) | 0 |\n| Decreased platelet count | 18 (5.6) | 0 | 61 (19.1) | 15 (4.7) |\n| Neutropenia | 15 (4.7) | 2 (0.6) | 50 (15.6) | 12 (3.8) |\n\n【66】 Shown are adverse events of any cause that occurred in at least 10% of patients in either group while patients were receiving the assigned treatment or within 30 days after the end of the trial treatment period. The as-treated population included all the patients who underwent randomization and received at least one dose of trial treatment. Events are listed in descending order of frequency in the nivolumab-plus-cabozantinib group. Adverse events are classified according to the _Medical Dictionary for Regulatory Activities_ , version 22.1. ALT denotes alanine aminotransferase, and AST aspartate aminotransferase.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5d645e52-213a-480b-989f-55c446c86f5b", "title": "Reduction in Myocardial Ischemia with Nitroglycerin or Nitroglycerin plus Phenylephrine Administered during Acute Myocardial Infarction", "text": "【0】Reduction in Myocardial Ischemia with Nitroglycerin or Nitroglycerin plus Phenylephrine Administered during Acute Myocardial Infarction\nAbstract\n--------\n\n【1】Nitroglycerin reduces ischemic injury during acute myocardial infarction (AMI) in dogs — an effect that is potentiated when drug-induced hypotension and tachycardia are prevented with phenylephrine. To determine the effectiveness of nitroglycerin, alone or with phenylephrine, during AMI in man, 12 patients (five of whom had left heart failure) were evaluated by summing ST-segment abnormalities (σST) from 35 precordial electrodes. The seven patients without heart failure did not benefit consistently from nitroglycerin alone; however, addition of phenylephrine to abolish nitroglycerin-induced arterial pressure reduction uniformly diminished σST(4.9 to 3.2 mv; P < 0.05). In patients with heart failure, nitroglycerin alone consistently reduced ischemia (5.8 to 4.4mv, P < 0.05); addition of phenylephrine often partially reversed this effect. Thus, administration of nitroglycerin, alone or with phenylephrine, can reduce myocardial ischemic injury during AMI in man; however, the response to phenylephrine depends on the presence or absence of left ventricular failure before treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5676c041-6f34-435c-b8f1-5b0b121eef55", "title": "High-Dose Chemotherapy with Hematopoietic Stem-Cell Rescue for Multiple Myeloma", "text": "【0】High-Dose Chemotherapy with Hematopoietic Stem-Cell Rescue for Multiple Myeloma\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】High-dose therapy with supporting autologous stem-cell transplantation remains a controversial treatment for cancer. In multiple myeloma, first-line regimens incorporating high-dose therapy yield higher remission rates than do conventional-dose treatments, but evidence that this translates into improved survival is limited.\n\n【3】Methods\n-------\n\n【4】In this multicenter study, the Medical Research Council Myeloma VII Trial, we randomly assigned 407 patients with previously untreated multiple myeloma who were younger than 65 years of age to receive either standard conventional-dose combination chemotherapy or high-dose therapy and an autologous stem-cell transplant.\n\n【5】Results\n-------\n\n【6】Among the 401 patients who could be evaluated, the rates of complete response were higher in the intensive-therapy group than in the standard-therapy group (44 percent vs. 8 percent, P<0.001). The rates of partial response were similar (42 percent and 40 percent, respectively; P=0.72), and the rates of minimal response were lower in the intensive-therapy group than in the standard-therapy group (3 percent vs. 18 percent, P<0.001). Intention-to-treat analysis showed a higher rate of overall survival (P=0.04 by the log-rank test) and progression-free survival (P<0.001) in the intensive-therapy group than in the standard-therapy group. As compared with standard therapy, intensive treatment increased median survival by almost 1 year (54.1 months \\[95 percent confidence interval, 44.9 to 65.2\\] vs. 42.3 months \\[95 percent confidence interval, 33.1 to 51.6\\]). There was a trend toward a greater survival benefit in the group of patients with a poor prognosis, as defined by a high beta <sub>2 </sub> \\-microglobulin level (more than 8 mg per liter).\n\n【7】Conclusions\n-----------\n\n【8】High-dose therapy with autologous stem-cell rescue is an effective first-line treatment for patients with multiple myeloma who are younger than 65 years of age.\n\n【9】Introduction\n------------\n\n【10】In the controversial field of high-dose chemotherapy, multiple myeloma is one disease in which this approach may provide tangible benefits, but data from rigorous studies are limited. In randomized trials carried out by the Medical Research Council of the United Kingdom between 1964 and 1990, the most effective standard regimen of conventional-dose chemotherapy consisted of doxorubicin, carmustine, cyclophosphamide, and melphalan and resulted in a median survival of 32 months.  Other conventional-dose regimens resulted in improved response rates, but not enduring remissions.  Escalating the doses of melphalan to a level requiring autologous stem-cell rescue  resulted in even higher rates of remission, with a complete response in approximately 50 percent of patients. An approach involving conventional-dose chemotherapy followed by high-dose therapy offered the prospect of a better outcome, but the evidence of a survival benefit has been inconclusive in nonrandomized  and randomized  studies. To investigate this strategy further, we initiated a phase 3 trial in which patients received either a standard regimen of doxorubicin, carmustine, cyclophosphamide, and melphalan or a regimen consisting of infusional combination chemotherapy followed by high-dose melphalan with autologous stem-cell transplantation. Both regimens included interferon alfa as maintenance therapy.\n\n【11】Methods\n-------\n\n【12】Patients\n--------\n\n【13】The Medical Research Council Myeloma VII Trial (ISRCTN66518389) was conducted from October 1993 to October 2000. All patients were previously untreated, fulfilled the Medical Research Council criteria for myeloma requiring treatment,  were less than 65 years of age, and were suitable candidates for high-dose therapy. Written informed consent was obtained from all patients. Randomization was by telephone and used a minimization algorithm based on age (<55 years vs. ≥55 years), serum creatinine level (<1.7 mg per deciliter \\[150 μmol per liter\\] vs. ≥1.7 mg per deciliter), hemoglobin level (<9 vs. ≥9 g per deciliter) and, in the latter part of the trial, whether total-body irradiation was intended as part of the conditioning regimen for transplantation. The trial was approved by a multicenter research ethics committee and by local ethics committees.\n\n【14】Treatment\n---------\n\n【15】### _Standard Therapy_\n\n【16】Standard therapy consisted of a short infusion of 30 mg of doxorubicin per square meter of body-surface area intravenously and 30 mg of carmustine per square meter intravenously on day 1 followed by 100 mg of cyclophosphamide per square meter per day orally and 6 mg of melphalan per square meter per day orally on days 22, 23, 24, and 25. The cycle was repeated every six weeks until the maximal response was attained. A minimum of 4 cycles was given, and the maximum was 12 cycles. There were per-protocol dose reductions in the case of renal dysfunction, but patients who had treatment delays owing to myelosuppression received 300 mg of cyclophosphamide per square meter intravenously each week plus 40 mg of prednisolone per square meter orally every other day for the first six weeks. The planned maintenance therapy was 3 million U of interferon alfa-2a (Roferon-A) subcutaneously three times per week.\n\n【17】### _Intensive Therapy_\n\n【18】Intensive therapy consisted of a continuous infusion of 9 mg of doxorubicin per square meter per day and 0.4 mg of vincristine per day on days 1 through 4, 1 g of methylprednisolone per square meter per day intravenously or orally (maximum, 1.5 g) on days 1 through 5, and 500 mg of cyclophosphamide per day intravenously on days 1, 8, and 15. The cycle was repeated every 21 days until a maximal response was attained. A minimum of three cycles was given before stem cells were harvested. Patients with a serum creatinine level of more than 3.4 mg per deciliter (300 μmol per liter) did not receive cyclophosphamide; cyclophosphamide was omitted on day 8 or 15 (or both) in the event of undue myelosuppression. Peripheral-blood stem cells were typically mobilized by the administration of 2 to 4 g of cyclophosphamide per square meter intravenously with hydration and granulocyte colony-stimulating factor on days 5 through 12. High-dose melphalan was given at a dose of 200 mg per square meter followed by the reinfusion of peripheral-blood stem cells 24 hours later. A bone marrow autograft and total-body irradiation plus melphalan (140 mg per square meter) were permissible options. Methylprednisolone (1.5 g per day) was given intravenously for four days after the administration of high-dose melphalan. The dose of melphalan was reduced according to the creatinine clearance. The planned maintenance therapy was 3 million U of interferon alfa-2a administered subcutaneously three times per week.\n\n【19】Assessment of Response\n----------------------\n\n【20】The response to treatment was monitored by means of serum and urine protein studies carried out centrally at the University of Birmingham. In the intensive-therapy group, the studies were conducted every three weeks during the chemotherapy regimen and every three months thereafter. In the standard-therapy group, the studies were conducted every three months. Bone marrow aspirates and trephine specimens were obtained as needed to determine the response to induction therapy, and also at three months and yearly after the completion of high-dose therapy and at relapse in the intensive-therapy group, and at the time of the maximal response and at progression in the standard-therapy group. The response criteria of the European Group for Blood and Marrow Transplantation – International Bone Marrow Transplant Registry  were used. A complete response was defined by the absence of monoclonal immunoglobulin in serum (or light chains in urine) on immunofixation. Causes of death were recorded by the participating centers as attributable to myeloma, infection, a variety of other secondary causes, unrelated causes, or combinations of these if the cause of death was considered to be multifactorial.\n\n【21】Statistical Analysis\n--------------------\n\n【22】Assuming the survival rate at four years to be 60 percent in the standard-therapy group, the study required 710 patients to have 80 percent power to detect an absolute improvement in survival of 10 percent in the intensive-therapy group. The recruitment target was 750 patients. The steering committee agreed to stop the trial in October 2000 when there was a total of 407 patients, in view of declining enrollment.\n\n【23】The primary end points were overall survival and progression-free survival. Overall survival was calculated from the date of randomization to the date of death from any cause. Data on patients who were lost to follow-up or who were alive at the time of analysis were censored in the survival analysis on the last date they were known to be alive. Progression-free survival was calculated from the date of randomization to the date of progression or death. Patients recorded as having died from multiple myeloma and for whom no prior date for progression was available were considered to have had progressive disease on the day of death. Data on patients who had not had progression were censored on the last date they were known to be alive and progression-free. Survival curves were constructed with the use of Kaplan–Meier estimates, and treatment groups were compared with the use of the log-rank test at a significance level of 5 percent. Cox proportional-hazards models were used to adjust survival analyses for minimization factors (age, serum creatinine level, and hemoglobin level) and to investigate the correlation of the beta <sub>2 </sub> \\-microglobulin level with survival (which was specified in the statistical-analysis plan before any data were analyzed). A cutoff date of October 20, 2001, was used for survival analysis. The maximal response was compared in the treatment groups with the use of chi-square tests. The proportions of deaths recorded as solely or partly attributable to myeloma and solely or partly attributable to infection are reported, with 95 percent confidence intervals. All analyses were two-sided and carried out on an intention-to-treat basis with the use of SAS software (SAS Institute).\n\n【24】We used published data to conduct a meta-analysis of trials comparing conventional therapy with high-dose therapy in patients with myeloma. The resulting Forrest plot yielded an estimate of the odds ratio of the combined treatment effect, with 95 percent confidence intervals, with use of a fixed-effects approach. Analyses were performed with Review Manager software . \n\n【25】Results\n-------\n\n【26】Characteristics of the Patients and Treatments\n----------------------------------------------\n\n【27】Figure 1. Summary of Treatment Received. Table 1.  Table 1. Base-Line Characteristics of the Randomized Patients.\n\n【28】A total of 407 patients were enrolled from 83 centers in the United Kingdom and New Zealand over a seven-year period from 1993 to 2000. Six patients could not be included in any data summaries or analyses: five underwent randomization in error, and one patient withdrew consent . A total of 200 patients were randomly assigned to receive standard therapy and 201 to receive intensive therapy. The characteristics of the patients are summarized in Table 1 . The myeloma subtypes were as follows: IgG in 56 percent of patients, IgA in 22 percent, IgD in 2 percent, light chain in 13 percent, and nonsecretory in 4 percent; data on subtype were missing in 3 percent of cases. Figure 1 summarizes the treatment received. In the intensive-therapy group, 197 patients received a median of five cycles (range, one to nine) of cyclophosphamide, vincristine, doxorubicin, and methylprednisolone. In the standard-therapy group, 146 patients received a median of 6 cycles (range, 1 to 13) of doxorubicin, carmustine, cyclophosphamide, and melphalan; 47 received doxorubicin, carmustine, cyclophosphamide, and melphalan as well as cyclophosphamide weekly for a median of 4 cycles (range, 1 to 12); and 3 received cyclophosphamide weekly alone.\n\n【29】In the intensive-therapy group, 50 of 201 patients (25 percent) did not receive high-dose melphalan, as a result of death, early disease progression (i.e., during induction chemotherapy), poor performance status, or low CD34 counts or by choice, and thus did not receive a stem-cell transplant. Therapy with high-dose melphalan was usually supported by the reinfusion of peripheral-blood stem cells (138 patients \\[92 percent\\]); only 8 patients received bone marrow (5 percent), and 3 received both bone marrow and stem cells (2 percent; this information was unavailable for 1 patient). The dose of melphalan was reduced in 17 patients (11 percent), as a result of poor stem-cell harvests, renal failure, or poor performance status. Eight patients received total-body irradiation plus melphalan (140 mg per square meter). Two patients received a second autograft at relapse. Only 30 patients (15 percent) in the standard-therapy group went on to receive an autograft, and 4 (2 percent) an allograft, as part of off-protocol therapy. In the standard-therapy group, 84 patients (42 percent) received interferon alfa-2a for at least a month, as compared with 118 patients (59 percent) in the intensive-therapy group. Among these patients, the drug was stopped because of intolerance or adverse events in 14 (17 percent) and 39 (33 percent), respectively, and because of disease progression in 43 (51 percent) and 33 (28 percent), respectively.\n\n【30】Overall Survival\n----------------\n\n【31】Figure 2. Kaplan–Meier Estimates of Overall Survival in the Intention-to-Treat Population.\n\n【32】Overall, there was an improvement in median survival of 11.8 months in the intensive-therapy group (median survival, 54.1 months; 95 percent confidence interval, 44.9 to 65.2) as compared with the standard-therapy group (42.3 months; 95 percent confidence interval, 33.1 to 51.6; P=0.04 by the log-rank test and P=0.03 by the Wilcoxon test).\n\n【33】As of October 20, 2001, 206 of the 401 patients (51 percent) had died: 112 patients in the standard-therapy group and 94 in the intensive-therapy group. The median duration of follow-up among survivors was 42 months (range, 9 to 96), with an overall median survival of 48.5 months (95 percent confidence interval, 42.2 to 56.3). The median survival was 54.1 months (95 percent confidence interval, 44.9 to 65.2) in the intensive-therapy group and 42.3 months (95 percent confidence interval, 33.1 to 51.6) in the standard-therapy group (P=0.04 by the log-rank test and P=0.03 by the Wilcoxon test) . A Cox model that adjusted for minimization factors showed that survival rates were higher among patients with a creatinine level of less than 1.7 mg per deciliter than among those with a level of 1.7 mg per deciliter or higher and among patients with a hemoglobin level of 9 g per deciliter or higher than among those with a level of less than 9 g per deciliter. A significant interaction between treatment group and the beta <sub>2 </sub> \\-microglobulin level was seen (P=0.003 in the Cox model), indicating that the treatment effect varied depending on the level of beta <sub>2 </sub> \\-microglobulin. Stratified log-rank analysis according to the serum levels of beta <sub>2 </sub> \\-microglobulin — low (less than 4 mg per liter), intermediate (4 to 8 mg per liter), or high (more than 8 mg per liter) — defined in previous Medical Research Council studies  showed that within each stratum, the intensive-therapy group had a longer median survival than the standard-therapy group. This difference was greatest among those with base-line beta <sub>2 </sub> \\-microglobulin levels of more than 8 mg per liter. In these patients median survival was 41.9 months (95 percent confidence interval, 31.3 to 65.2) in the intensive-therapy group, as compared with 13.1 months (95 percent confidence interval, 9.2 to 23.9) in the standard-therapy group.\n\n【34】Progression-Free Survival\n-------------------------\n\n【35】Figure 3. Kaplan–Meier Estimates of Progression-free Survival.\n\n【36】A total of 395 patients could be evaluated. The median duration of progression-free survival was longer in the intensive-therapy group than in the standard-therapy group (31.6 months \\[95 percent confidence interval, 27.4 to 38.0\\] vs. 19.6 months \\[95 percent confidence interval, 16.2 to 21.8\\], P<0.001 by the log-rank or Wilcoxon test).\n\n【37】As of October 20, 2001, 288 of the 395 patients who could be evaluated for disease progression (73 percent) had evidence of progression; 36 in the standard-therapy group and 71 in the intensive-therapy group remained progression-free. Overall, the median duration of progression-free survival was 25.1 months (95 percent confidence interval, 21.4 to 27.8). After a median follow-up of 31.5 months in the standard-therapy group and 40.0 months in the intensive-therapy group, 160 patients had evidence of progression in the standard-therapy group and 128 in the intensive-therapy group. The median duration of progression-free survival was 31.6 months (95 percent confidence interval, 27.4 to 38.0) in the intensive-therapy group, as compared with 19.6 months (95 percent confidence interval, 16.2 to 21.8) in the standard-therapy group (P<0.001 by the log-rank or Wilcoxon test) . Cox models showed that the serum creatinine level (P=0.001), hemoglobin level (P=0.07), and beta <sub>2 </sub> \\-microglobulin level (P=0.08) were significant prognostic factors for progression-free survival.\n\n【38】Response\n--------\n\n【39】Table 2. Maximal Response to Treatment.\n\n【40】The maximal response to randomized treatment is summarized in Table 2 : the intensive-therapy group had a higher overall rate of response and a higher rate of complete remission than the standard-therapy group. Although no formal statistical tests were carried out, there was a trend toward improved survival in the intensive-therapy group as the extent of the response increased from minimal (25.6 months; 95 percent confidence interval, 7.0 to 31.3) to partial (39.8 months; 95 percent confidence interval, 33.8 to 61.4) to complete (88.6 months; lower 95 percent confidence limit, 61.4).\n\n【41】Causes of Death\n---------------\n\n【42】Of the 206 deaths, 183 (89 percent) were recorded as due to myeloma or related factors, including treatment. Multiple myeloma was cited as a causal factor in more patients in the standard-therapy group than in the intensive-therapy group (69 of 112 patients \\[62 percent; 95 percent confidence interval, 53 to 71\\] vs. 46 of 94 patients \\[49 percent; 95 percent confidence interval, 39 to 59\\]). Infection, as at least a contributory factor, was reported in 68 patients (33 percent) who died and was more frequent in the intensive-therapy group than in the standard-therapy group (35 patients \\[37 percent; 95 percent confidence interval, 27 to 47\\] vs. 33 patients \\[29 percent; 95 percent confidence interval, 21 to 38\\]). Six deaths occurred within 100 days after transplantation, five of which were due to sepsis. The rate of early death was not higher than expected in either group.\n\n【43】Discussion\n----------\n\n【44】An approach involving high-dose chemotherapy with stem-cell rescue attempts to take advantage of the dose–response curve and often induces relatively high rates of tumor regression across a range of tumors, as compared with those achieved with conventional therapy. This has led to considerable enthusiasm for its use. It has increasingly been adopted as first-line treatment for multiple myeloma, despite the paucity of data from randomized trials providing convincing evidence of a survival benefit to support the routine use of this approach. Such studies are difficult and time-consuming to conduct, because of the technical complexity and, often, the strong beliefs about the effectiveness of one type of therapy or the other among clinicians and patients.\n\n【45】A systematic review of reports of trials comparing conventional with high-dose therapy  identified a number of randomized studies that clearly cannot be compared with our trial because of the timing of randomization (after response rather than at diagnosis),  the timing of transplantation (early vs. late),  use of higher-dose conventional therapy (so-called intermediate-dose melphalan),  and the number of transplantations (single vs. double).  Only two similar randomized studies were identified.  The Intergroupe Français du Myélome randomly assigned 200 patients to receive either conventional-dose combination chemotherapy or combination chemotherapy followed by melphalan (140 mg per square meter) with total-body irradiation.  Seventy-four patients in the intensive-therapy group underwent transplantation. The investigators reported a significant survival benefit with intensive therapy. In a recent update, the overall median duration of survival was 44 months in the standard-therapy group and 56 months in the intensive-therapy group (Attal M: personal communication). A second study, by the Groupe Myélome Autogreffe, compared conventional-dose combination chemotherapy with infusional chemotherapy (with vincristine, doxorubicin, and dexamethasone) followed by melphalan at a dose of 200 mg per square meter or melphalan at a dose of 140 mg per square meter plus busulfan at a dose of 16 mg per square meter.  Of the 190 patients between the ages of 55 and 65 years who underwent randomization, 94 were assigned to the intensive-therapy group (25 percent of whom did not ultimately undergo transplantation). No significant survival benefit was seen with intensive treatment; the median overall duration of survival was 50.4 months in the conventional-chemotherapy group and 55.3 months in the intensive-therapy group. At the time of disease progression, patients in the conventional-chemotherapy group could cross over to receive high-dose therapy at their physicians' discretion.\n\n【46】Figure 4. Forrest Plot Showing the Odds Ratios and 99 Percent Confidence Intervals (CIs) for the Current Study and Two Other Published Studies Comparing Conventional Treatment with High-Dose Treatment in Patients with Myeloma.\n\n【47】Data are from the current study, Attal et al. (the Intergroupe Français du Myélome trial),  and Fermand et al. (the Groupe Myélome Autogreffe trial).  The size of each square, representing the estimated treatment effect, is proportional to the size of the study (i.e., larger squares provide more information and hence have narrower 99% confidence intervals). The odds ratio for all the studies combined suggests a beneficial effect of high-dose therapy (chi-square test for heterogeneity, 1.95 with 2 df; P=0.38; and test for overall effect, z=–2.45; P=0.01).\n\n【48】We calculated the odds ratios and 99 percent confidence intervals for our study as well as for the Intergroupe Français du Myélome trial  and the Groupe Myélome Autogreffe trial  . The two earlier studies together did not show a survival benefit. However, when our results were taken into account and the results of all three studies were combined, the estimated treatment effect was consistent with a significant survival benefit with intensive therapy, as compared with standard therapy (odds ratio, 0.70; 95 percent confidence interval, 0.53 to 0.93; P=0.01).\n\n【49】The most important finding in our trial was the increase in median survival of approximately one year among patients in the intensive-therapy group, as compared with those in the standard-therapy group. Per-protocol analyses showed that this difference may be a conservative estimate of benefit, because 17 percent of the patients in the standard-therapy group crossed over to high-dose therapy, usually after disease progression. Although myeloma and infection were commonly recorded as causes of death, any differences in the frequency of these causes between the two groups in a multicenter study of this nature should be treated with caution, since death was often multifactorial. There were only six deaths within 100 days after transplantation.\n\n【50】In the Medical Research Council trials, three subgroups corresponding to a good, an intermediate, and a poor prognosis have been delineated, on the basis of serum beta <sub>2 </sub> \\-microglobulin levels (less than 4 mg per liter, 4 to 8 mg per liter, and more than 8 mg per liter, respectively).  In our study the proportions of patients in these groups were 38 percent, 28 percent, and 24 percent, respectively. A trend for patients in the group with a poor prognosis to benefit most from intensive therapy was identified. Cytogenetic data were not generally available, but more recently, prognostic groups have been redefined by combining the serum beta <sub>2 </sub> \\-microglobulin level with 13q– status. \n\n【51】We and others have previously shown a trend toward improved progression-free survival and overall survival among patients with undetectable myeloma protein in serum or urine (a complete response), as compared with those with a partial response.  In this study, we also identified a trend in the intensive-therapy group toward improved overall survival among patients who had a complete response. Further intensification of treatment through the use of multiple high-dose procedures to increase the rates of complete response remains under study.  Emerging biologic therapies may also offer a means of maintaining and enhancing such responses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "db4e2007-0f52-4a93-8351-ff4c83597e7c", "title": "Policy Lessons from Our Covid Experience", "text": "【0】Policy Lessons from Our Covid Experience\nArticle\n-------\n\n【1】As of August 24, 2020, nearly 5.7 million cases of Covid-19 had been reported in the United States, with more than 176,000 deaths. Although there is debate about the accuracy of these specific numbers — many people with mild symptoms are never tested for Covid, for example, and especially early in the epidemic, the difference between dying from Covid and dying with Covid may not have been accurately captured — the increase in excess mortality rates reported by the Centers for Disease Control and Prevention is consistent with a significant loss of life associated with the disease.\n\n【2】At the same time, the Covid-19 pandemic has led to staggering economic losses in the United States. Closing down the economy has had a devastating impact on the American people, even though the closure was imposed to save lives. The longest economic expansion on record abruptly ended in February, and the country officially entered a recession late that month.\n\n【3】The U.S. unemployment rate in February was 3.5% — a half-century low. By March, it was 4.4%, and by April, 14.7%, with 20.5 million people losing their jobs and more than 20% of the labor force filing for unemployment benefits. Experts predicted that the unemployment rate would approach 20% in May; instead, it was reported as 13.3%, although there’s debate about whether this figure reflected some workers’ self-classification as only temporarily laid off. The June unemployment rate was even lower — 11.1% — but the economy was still operating with 15 million fewer jobs than it had in February, and there was new concern that the economic impact of the pandemic may linger, given the recent resurgence of new cases.\n\n【4】As the country reopens, it’s important to assess how we can be better prepared to stave off such enormous economic losses during the next wave or the next epidemic. In my view, a few key policy changes will be critical.\n\n【5】First, expertise on pandemic-related policy and strategy should be located closer to the center of power. I believe that the type of pandemic-preparedness office (the Office of Pandemics and Emerging Threats) that now resides only in the Department of Health and Human Services (HHS) also needs to be reestablished as part of the National Security Council (NSC). Incorporating the office into the NSC doesn’t guarantee that the White House will pay attention to its recommendations, but it helps in commanding the attention of the most senior members of the White House staff. The HHS assistant secretary would continue to serve as the execution arm of the pandemic office.\n\n【6】Since the early 1990s, such an office has repeatedly been established after a national health scare — and then disbanded by the successor administration. The Biodefense and Health Security Office established during the Clinton administration was closed by President George W. Bush, reopened after the anthrax scare, closed by President Barack Obama, and then reopened after the Ebola and Zika scares, at which point the Directorate for Global Health Security and Biodefense was created. The plan prepared in the wake of the Ebola outbreak might have been helpful in preparing a response for the current Covid pandemic, but like his predecessors, former National Security Advisor John Bolton dissolved the office in 2018. Once again, some of the office’s personnel were merged into other NSC units, but the pandemic office itself no longer existed.\n\n【7】Whether as cause or effect of the office’s repeated dissolution or sidelining, neither the defense establishment nor the public seems to appreciate that disease threats are as serious to the country’s security as are wars with our traditional enemies. For example, another airborne disease, smallpox, caused 300 million to 500 million deaths worldwide — more than all 20th-century wars combined.\n\n【8】Second, in planning for unknown future epidemics, federal pandemic-preparedness officials must decide what constitutes a prudent level of supply stockpiling, with an understanding of the inevitable trade-offs between perceived readiness and the cost of equipment and supplies that we hope never to use. They should develop strategies for deploying supplies on an as-needed basis. And they should plan ways to provide surge capacity for beds, operating rooms, and trained personnel, which may involve calling on the National Guard, among other resources. Designation of particular people who will be accountable for these activities will be key to their success.\n\n【9】Third, when an outbreak occurs, determinations about policy and financial responses should be based on accurate epidemiologic knowledge. It is important to establish as quickly as possible who is most vulnerable to a new disease and to respond selectively, with targeted measures directed toward the most vulnerable populations. Older people, particularly those with underlying conditions and compromised immune systems, are especially vulnerable to Covid-19. Some 50,000 deaths from Covid had occurred in nursing homes and other senior care facilities by mid-June. But older people are not always the ones at greatest risk. The “Spanish flu” in 1918 was especially dangerous to infants and 20-to-40-year-olds. We need to understand who is contracting the disease and experiencing serious consequences in order to craft an appropriate and differentiated response.\n\n【10】In a recent study, John Birge and others claim that the economic cost of the U.S. Covid shutdown could have been reduced by 33 to 40% if neighborhoods had been selected for closure more strategically, in line with the infection risk for local residents and workers.  The authors’ rationale is that living areas are frequently different from the places with the highest concentration of jobs and that it may be possible to keep jobs open as long as many of the employees live in unaffected areas. To calculate the number of people at risk in a given area, however, we need to know who comes into contact with an infected person and be able to track their travel histories. Such work can be done with the type of contact tracing being done in Hong Kong, Germany, and elsewhere, but it may require that people provide detailed information to the authorities, including turning over their cell phones or providing details about their contacts in phone interviews. Many Americans are unlikely to be willing to do so.\n\n【11】Fourth, pandemic-control policies must have logically consistent components. The decision to shut down the U.S. borders to limit the entry of people carrying Covid-19, for example, required thousands of U.S. citizens to quickly reenter the United States, but no plans appeared to have been made for the safe reentry of large numbers of people through a limited number of entry ports. As a result, hundreds of people were crowded into inadequate spaces trying to get through customs and immigration. If any of them were carrying the virus, the crowding would have exacerbated the spread, with serious consequences for public health.\n\n【12】Finally, beyond public policy, individual citizens clearly need to accept responsibility for acting in ways appropriate to fighting pandemics. Though much remains unknown about Covid-19, there is increasing evidence about the efficacy of mask wearing. The refusal of some people to wear face coverings when indoors or to maintain reasonable social distances outdoors increases the burden on all of us. When a safe and effective vaccine becomes available, it will also be incumbent on all who are of an appropriate age and have no contraindications to get vaccinated.\n\n【13】Only time will tell whether the country will be better able to respond to the next health crisis. A key focus now needs to be on reopening the economy in a smart and sensible way. While the economy was shut down, the government provided critical short-term assistance, but I believe we need to ensure that future support is designed to encourage people to return to work as soon as they can do so with relative safety. A stimulus bill was under discussion in Congress but stalled when Congress went into their August recess on July 31. The president signed an executive order on August 10 to provide $400 weekly to unemployed Americans, but it is judged to be on shaky legal grounds. The additional federal spending related to the pandemic has added a huge long-term debt burden to the country. Replacing many Americans’ usual work income with federal funding is not a viable long-term strategy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5074c412-003f-43c5-ab9f-73b62f65c409", "title": "Dexamethasone Therapy for Bacterial Meningitis", "text": "【0】Dexamethasone Therapy for Bacterial Meningitis\nAbstract\n--------\n\n【1】We enrolled 200 infants and older children with bacterial meningitis in two prospective double-blind, placebo-controlled trials to evaluate the efficacy of dexamethasone therapy in addition to either cefuroxime (Study 1) or ceftriaxone (Study 2). Altogether, 98 patients received placebo and 102 received dexamethasone (0.15 mg per kilogram of body weight every six hours for four days). At the beginning of therapy, the clinical and demographic characteristics of the patients in the treatment groups were comparable.\n\n【2】The mean increase in the cerebrospinal fluid concentration of glucose and the decreases in lactate and protein levels after 24 hours of therapy were significantly greater in those who received dexamethasone than in those who received placebo (glucose, 2.0 vs. 0.4 mmol per liter \\[36.0 vs. 6.9 mg per deciliter\\], P<0.001; lactate, 4.0 vs. 2.1 mmol per liter \\[38.3 vs. 19.8 mg per deciliter\\], P<0.001; and protein, 0.64 vs. 0.25 g per liter \\[64.0 vs. 25.3 mg per deciliter\\], P<0.05). One patient in the placebo group in Study 1 died. As compared with those who received placebo, the patients who received dexamethasone became afebrile earlier (1.6 vs. 5.0 days; P<0.001) and were less likely to acquire moderate or more severe bilateral sensorineural hearing loss (15.5 vs. 3.3 percent; P<0.01). Twelve patients in the two placebo groups (14 percent) had severe or profound bilateral hearing loss requiring the use of a hearing aid, as compared with 1 (1 percent) in the two dexamethasone groups (P<0.001).\n\n【3】We conclude that dexamethasone is beneficial in the treatment of infants and children with bacterial meningitis, particularly in preventing deafness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "212f8ccb-07a8-4ee6-849f-86e82003f262", "title": "Reticuloendothelial Clearance in Cystic Fibrosis and Other Inflammatory Lung Diseases", "text": "【0】Reticuloendothelial Clearance in Cystic Fibrosis and Other Inflammatory Lung Diseases\nAbstract\n--------\n\n【1】To examine the possible pathophysiologic role of circulating immune complexes in patients with cystic fibrosis and other inflammatory lung diseases, we studied the reticuloendothelial clearance of IgG-sensitized autologous erythrocytes in 15 patients with cystic fibrosis, 6 with chronic obstructive lung disease not related to cystic fibrosis, 7 with immunodeficiencies, 5 with systemic lupus erythematosus, 4 who had previously undergone a splenectomy, and 10 normal subjects.\n\n【2】Patients with chronic inflammation and recurrent infections (i.e., those with cystic fibrosis, chronic obstructive lung disease, and immunodeficiencies) had significantly faster clearance rates (P<0.05, <0.01, and <0.005, respectively) than normal subjects. In contrast, patients with systemic lupus erythematosus (a classic immune complex–mediated disease) and those who had undergone a splenectomy had delayed clearance.\n\n【3】The accelerated reticuloendothelial clearance in patients with chronic inflammatory pulmonary disease associated with cystic fibrosis was similar to that observed in stimulated laboratory animals. The rapid clearance rate may account for the rareness of septicemia in such patients despite chronic, persistent local bacterial infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1bb8f41d-0417-4d01-b91e-50db78470a87", "title": "Newly Diagnosed Atrial Fibrillation", "text": "【0】Newly Diagnosed Atrial Fibrillation\nA 77-year-old woman with a history of hypertension treated with metoprolol presents for her annual examination. She reports no new symptoms. The examination is remarkable only for an irregular heart rate. Electrocardiographic testing reveals atrial fibrillation at an average rate of 75 beats per minute. She has no history of arrhythmia, coronary disease, valvular disease, diabetes, alcohol abuse, or transient ischemic attack or stroke. What should her physician advise?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "aa5096ef-1331-43c6-946c-9fbe366114db", "title": "A New Label for Mifepristone", "text": "【0】A New Label for Mifepristone\nArticle\n-------\n\n【1】> The vicissitudes of life produce pregnancies which may be unwanted, or which may impair “health” … which, in the full setting of the case, may create such suffering, dislocations, misery, or tragedy as to make an early abortion the only civilized step to take.\n> \n> — Justice William O. Douglas, January 22, 1973, writing a concurring opinion in _Roe v. Wade_\n\n【2】Whenever a sponsor conducts phase 3 trials of a new molecular entity to support Food and Drug Administration (FDA) approval for marketing, there are strong incentives to conduct the trials under conditions that will maximize efficacy and minimize the number of adverse events. Given the sociopolitical sensitivity surrounding the drug and its indication, these incentives were never stronger than they were for the sponsor (the Population Council) seeking approval of RU-486 (mifepristone) for the indication of early pregnancy termination. For that reason, the study conditions in the two U.S. trials conducted in 1994–1995 for submission to the FDA were very conservative. The maximum gestational age for pregnancies among women allowed into the studies was relatively early, less than 63 days of amenorrhea, to both maximize the probability of success and minimize the potential risk of excessive bleeding. The relatively high dose of mifepristone used (600 mg) maximized the probability of efficacy, and the dose of the second medication (misoprostol, 400 μg) needed to complete the procedure was administered under direct supervision and required a stay of a minimum of 4 hours in the prescribing medical care facility to minimize the risk of adverse events.  The sponsor’s request for the labeled indication for mifepristone was even more conservative (use at ≤49 days of amenorrhea) to further optimize conditions for success and to minimize failures requiring surgical completion and safety concerns regarding excessive bleeding.\n\n【3】Even as the New Drug Application was making its way through the FDA approval process, evidence was accumulating in large studies done by the World Health Organization demonstrating that a 200-mg dose of mifepristone was just as effective as a 600-mg dose, and at one third the cost.  Despite the growing evidence that a less expensive mifepristone regimen would be just as effective, no one wanted to restart the process, which ultimately took 54 months, by repeating the expensive and time-consuming large licensing trials with the lower dose. Further experience, accumulated over more than a decade of use, demonstrated the efficacy of the medical regimen through 70 days of amenorrhea  and the safety of home self-administration of misoprostol after mifepristone.  Not surprisingly, given the large body of evidence in the medical literature, many care providers adopted practices including the lower dose of mifepristone, use in more advanced gestations, and home self-administration of misoprostol relatively soon after the drug was licensed.\n\n【4】On March 30, 2016, the FDA released the new label for mifepristone , which included three important revisions: recommending the 200-mg dose (1 pill) rather than the originally approved 600-mg dose (3 pills), extending the indication for use to pregnancies of less than 70 days’ duration, and formally eliminating the requirement that the dose of misoprostol be given in a medical facility, including a protracted stay for observation. The pills are quite expensive because of the manufacturing cost, the overhead associated with administering the care providers’ application-for-use program, and the cost of meeting secure shipping and storage requirements. The changes included in the new label should substantially improve access to medical terminations of pregnancy, not only by reducing the cost of the medication, but also by formally eliminating the requirement for a clinic visit for misoprostol administration and expanding the gestational age range for which the treatment is indicated. Although these label changes may be dismissed as trivial — as the FDA merely catching up with common practice — doing so would underestimate their importance. Several states have passed laws or written regulations to mandate compliance with the more expensive, burdensome, and restrictive requirements of the older labeling. Passage of those targeted regulations has been justified as necessary to protect the health and safety of women seeking these services. The new mifepristone label will strip away that fig leaf, and failure to adopt the new, less restrictive label in laws and regulations will expose the more restrictive regulations for their true intent.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "42346029-4907-447b-ae90-725ffff97f8e", "title": "The Role of Critical Care Nurses in Euthanasia and Assisted Suicide", "text": "【0】The Role of Critical Care Nurses in Euthanasia and Assisted Suicide\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Euthanasia and assisted suicide have received considerable attention recently in medical literature, public discussion, and proposed state legislation. Almost all the discussion in this area has focused on the role of physicians. However, nurses — especially critical care nurses — may be in a special position to understand the wishes of patients and to act on this understanding.\n\n【3】Methods\n-------\n\n【4】I mailed a survey to 1600 critical care nurses in the United States, asking them to describe anonymously any requests from patients, family members or others acting for patients (surrogates), or physicians to perform euthanasia or assisted suicide, as well as their own practices.\n\n【5】Results\n-------\n\n【6】Of the 1139 nurses who responded (71 percent), 852 said they practiced exclusively in intensive care units for adults in the United States. Of these 852 nurses, 141 (17 percent) reported that they had received requests from patients or family members to perform euthanasia or assist in suicide; 129 (16 percent of those for whom data were available) reported that they had engaged in such practices; and an additional 35 (4 percent) reported that they had hastened a patient's death by only pretending to provide life-sustaining treatment ordered by a physician. Some nurses reported engaging in these practices without the request or advance knowledge of physicians or others. The method of euthanasia most commonly described was the administration of a high dose of an opiate to a terminally ill patient.\n\n【7】Conclusions\n-----------\n\n【8】As public debate continues about euthanasia and assisted suicide, some critical care nurses in the United States are engaging in such practices.\n\n【9】Introduction\n------------\n\n【10】Euthanasia is defined as administering medication or performing other interventions with the intention of causing a patient's death.  Unlike the practice of withholding or withdrawing life-sustaining treatments, which is generally well accepted in the United States, euthanasia remains illegal here and in nearly every other country in the world  — whether or not it is performed with the patient's permission. Still, the many theoretical discussions of euthanasia in the medical literature  and several surveys of physicians' attitudes in the United States do not indicate a consensus: many accept the idea of euthanasia, in appropriate circumstances and with procedural safeguards, but many others do not.  In surveys of British  and Australian  physicians, 7 to 29 percent admit having performed euthanasia. In a recent survey, 218 of 828 physicians in Washington State reported receiving requests for assisted suicide or euthanasia. These physicians satisfied 38 of 156 requests for assisted suicide and 14 of 58 requests for euthanasia. \n\n【11】Professional nursing codes also prohibit euthanasia.  However, in one study of 1210 oncology nurses in the United States, 47 percent indicated that they would vote to legalize physician-assisted death, and 16 percent indicated that they would, under a physician's order, administer a lethal injection to a competent, terminally ill patient who requested such assistance.  In another study,  15 of 36 U.S. nurses reported agreeing with the decision of a resident physician to administer a deliberate lethal overdose of morphine to a 20-year-old patient dying of ovarian cancer. \n\n【12】Little is known about the actual experiences and practices of nurses. In a survey of 943 Australian nurses, 218 reported being asked by a physician to engage in euthanasia; of these, 85 percent reported complying with the request. Moreover, 16 nurses reported complying with a patient's request for euthanasia without having been asked by a physician to do so.  In a different survey of 278 Australian nurses, 52 (19 percent) reported taking active steps to bring about the death of a patient, often without being asked to do so by the patient or the patient's family. \n\n【13】Given this evidence that nurses outside the United States may be as willing as physicians to engage in euthanasia, I conducted a study to examine the role of U.S. critical care nurses in euthanasia and assisted suicide. Critical care nurses frequently care for patients who wish to die, and these nurses are often in a position to hasten their deaths. And, like physicians, they may also be in a position to engage in such activities outside the practice setting, on behalf of friends or relatives who wish to die.\n\n【14】Methods\n-------\n\n【15】Study Subjects\n--------------\n\n【16】The subjects of this study were nurses practicing in intensive care units for adults in the United States. A random sample of 1600 subscribers to _Nursing_ magazine who practiced in critical care settings was selected in order to represent a broad spectrum of attitudes and experiences. _Nursing_ is the largest nursing journal in the world, with a circulation of nearly 500,000.\n\n【17】Survey Instrument\n-----------------\n\n【18】The survey consisted of eight pages and required about 10 minutes to complete. Because the terms “euthanasia” and “assisted suicide” can be unclear, subjects were provided with the following definition to use in responding to questions:\n\n【19】> Active euthanasia and assisted suicide may mean different things to different people. For the purposes of this questionnaire, we use both these terms to reflect circumstances in which someone performs an act with the specific intent of causing or hastening a patient's death, but we exclude those acts that reflect the withholding or withdrawing of life-sustaining treatment. By this definition, we would include such acts as providing an intentional overdose of narcotics or potassium chloride or providing explicit advice to patients about how to commit suicide but exclude such acts as withdrawing a mechanical ventilator, even though all these acts might result in the patient's death.\n\n【20】To address the subjects' experience with requests that they perform euthanasia or assist in a suicide, they were asked:\n\n【21】> Have you ever been asked by a patient, family member, or other surrogate to administer a medicine to a patient or perform some other act with the intent of causing that patient's death — other than the withholding or withdrawing of life-sustaining treatment? (Examples of this kind of behavior include administering a lethal injection of potassium chloride, or a deliberate overdose of opiates. Examples of this kind of behavior would not include withdrawing a patient from a mechanical ventilator.)\n\n【22】To address the subjects' actual practice, they were asked:\n\n【23】> While a critical care nurse, have you ever administered a medicine to a patient or performed some other act with the intent of causing or hastening that patient's death — other than the withdrawal of life-sustaining treatment? (Examples of this kind of behavior include administering a lethal injection of potassium chloride, only pretending to mix dopamine into the intravenous solutions, or increasing the dose or frequency of a morphine intravenous drip in a patient already unconscious. Examples of this kind of behavior would not include withdrawing a patient from a mechanical ventilator.)\n\n【24】The questions themselves did not contain the terms “euthanasia” and “assisted suicide.” The examples, however, were of euthanasia. No example of assisted suicide was offered. Nevertheless, it is possible that the responses involved cases of assisted suicide as well as euthanasia.\n\n【25】In addition to being asked about requests they had received to engage in euthanasia or assisted suicide and about their own behavior, the subjects were asked about occasions when they had wanted to engage in the practice but did not do so.\n\n【26】Most of the questions asked subjects to quantify their experiences during their careers and during the previous 12 months. They were asked whether their actions were undertaken at the request of patients, patients' families, other nurses, or physicians or with the advance knowledge of these persons. The subjects were invited to provide any explanatory comments they wished.\n\n【27】Mailing\n-------\n\n【28】The initial surveys were mailed during January and February 1995, and two additional mailings of the same instrument to the same group of potential respondents followed at one-month intervals. As part of a related study of survey techniques, potential subjects were randomly assigned to one of two groups. One group received a coded postcard with the survey instrument, to be returned separately from the questionnaire. The use of this postcard minimized the mailing of new copies of the instrument to nurses who had already responded, while keeping specific responses anonymous. The second group of nurses did not receive postcards, so each subject in this group received three complete, identical packets. The characteristics of the respondents and their responses did not differ between the two groups. All mailings included stamped, addressed envelopes for the return of the instrument and instructions not to return more than one completed questionnaire. Surveys received more than six weeks after the final mailing were not included.\n\n【29】The completed surveys were anonymous. No identifying information was encoded on the instruments or return envelopes. The protocol was approved by the human subjects committees of the University of Pennsylvania and the Philadelphia Veterans Affairs Medical Center. In addition, the study was reviewed both by an independent board selected by a major nursing professional society and by two experts in the areas of nursing and bioethics.\n\n【30】Results\n-------\n\n【31】Surveys were mailed to a total of 1600 nurses; 40 surveys were returned by the post office as undeliverable, and 1139 completed surveys were returned by the cutoff date, a response rate of 71 percent. Subjects were excluded if they were not critical care nurses or did not practice in critical care settings (165 subjects), if they were not clinically active (67 subjects), or if they sometimes practiced in pediatric intensive care units (46 subjects) or emergency departments (9 subjects). After these exclusions, a total study sample remained of 852 nurses who practiced exclusively in critical care units for adults. Because of occasional missing data, not all results reported here are based on the entire sample of 852 subjects.\n\n【32】Table 1. Characteristics of the Respondents.\n\n【33】Selected characteristics of the final 852 subjects are shown in Table 1 . All 50 states, Puerto Rico, and the District of Columbia were represented. On a scale of 1 to 5 on which they rated the importance of religion in their lives, the mean response of the nurses was 4.0, indicating a high level of religious feeling.\n\n【34】Requests for Euthanasia or Assisted Suicide from Patients and Surrogates\n------------------------------------------------------------------------\n\n【35】A total of 141 nurses (17 percent) reported receiving requests to engage in euthanasia or assisted suicide: 108 (13 percent) received requests from patients, and 101 (12 percent) received requests from family members or surrogates. Most nurses reported receiving three or fewer requests. Nineteen nurses (2 percent) reported receiving these requests outside a hospital or medical setting — for example, at a patient's home. On average, each nurse complied with 8 percent of the requests from patients and 12 percent of the requests from surrogates (this difference was not significant \\[P = 0.50\\]).\n\n【36】Participation in Euthanasia or Assisted Suicide\n-----------------------------------------------\n\n【37】Although 135 nurses answered yes to the question about participating in euthanasia or assisted suicide, 6 nurses were excluded from further analysis — 4 because their responses suggested a misunderstanding of the question and 2 because their answers were inconsistent. This left a group of 129 nurses (16 percent) reporting that they had participated in active euthanasia or assisted suicide at least once in their careers. Sixty-four (8 percent) reported having done so in the year before the study. Of the nurses who had performed such an act, 65 percent reported doing so 3 or fewer times and 5 percent reported doing so more than 20 times.\n\n【38】Table 2. Instances of Euthanasia or Assisted Suicide Reported by 827 Nurses.\n\n【39】Some of these actions were taken at the request or with the advance knowledge of patients, family members or surrogates, other nurses, or physicians. Table 2 shows the number of nurses who performed euthanasia or assisted suicide under these circumstances, as well as the total number of instances reported by all the nurses. Sixty-two nurses reported a total of 124 instances of euthanasia or assisted suicide in the preceding year, for a mean of 2 instances per nurse per year.\n\n【40】Not all reported instances of euthanasia or assisted suicide were in response to requests or performed with the knowledge of patients, family members, or surrogates. The questionnaire asked nurses to report the number of instances of these acts performed at the request of patients and the number performed at the request of family members or surrogates. Subtracting these responses from the total number of instances reported provides an estimate of the number of acts carried out without such requests. Although this design precludes a precise assessment of the number of nurses who reported engaging in euthanasia or assisted suicide without a request from either the patient or a surrogate, it could be calculated that at least 58 nurses (7 percent) had done so at least once. Some of these instances may have occurred with the advance knowledge of patients or surrogates and so may be cases of tacit consent. For example, one nurse described her participation in the following way:\n\n【41】> Usually, the patient has either verbalized or written several requests to have his/her suffering ended. It's like we never planned it, but, having developed a relationship with the patient, we both knew when it was time. In some instances, the patient was unconscious, on an opiate drip, which I increased or failed to decrease when vital signs dropped. The only physician I've ever had an agreement with is an oncologist I work with, but it's mostly unspoken.\n\n【42】By a similar calculation, 62 nurses (8 percent) reported at least one instance of engaging in euthanasia or assisted suicide without a request from the attending physician. Again, some, but not all, of these instances occurred with the advance knowledge of physicians and so may have involved tacit consent. Some of these 62 nurses reported also engaging in euthanasia or assisted suicide at other times at the explicit request of the attending physician.\n\n【43】Five nurses reported engaging in euthanasia or assisted suicide outside a hospital or other medical setting. One nurse wrote:\n\n【44】> My 71-year-old father was dying of \\. cancer\\. He did not wish to prolong his life. Morphine in pill form “controlled” his pain. He was at home. As he lay struggling for breath, I got liquid morphine from his physician and gave him as much as he could swallow. He was frothing at the mouth. I told him if he could hear me to try to swallow and that this would stop the struggle for a life he no longer wanted.\n\n【45】Other Actions to Hasten Death\n-----------------------------\n\n【46】In response to a separate question, 59 nurses (7 percent) reported having only pretended to carry out a physician's order or having engaged in some other clandestine practice in order to hasten a patient's death. For example, some nurses reported pretending to put vasopressors into the intravenous solutions given to patients with hypotension and instead administering only saline. Among these 59 nurses were 35 who did not answer yes to the earlier, more general question about their participation in euthanasia or assisted suicide. Although these actions may not all meet a narrow definition of euthanasia, altogether 164 nurses (19 percent) reported engaging in some act to hasten a patient's death.\n\n【47】Handwritten Comments and Explanations\n-------------------------------------\n\n【48】Of these 164 nurses, 74 described their activities in handwritten comments. These descriptions varied in the level of detail provided. For example, only 24 of the 129 nurses answering yes to the question about participation in euthanasia or assisted suicide provided descriptions of actions or intent sufficient to meet the definition used in the question. Others portrayed a range of other activities, perhaps with different moral implications.\n\n【49】For example, nine nurses reported administering large doses of sedatives or opiates to patients while withdrawing them from mechanical ventilation. Although these activities may have been performed in part to hasten death, the central purpose was to relieve suffering for patients or their families when death was already an explicitly accepted goal. Such practices are standard in many critical care settings.\n\n【50】The most common method of euthanasia reported in the handwritten comments was the administration of a lethal dose of an opiate. Often, patients were described as close to death, and often there were explicit decisions not to resuscitate them. Some nurses interpreted dosing guidelines liberally or moved beyond them. One nurse wrote: “I have given morphine doses much higher than prescribed, and falsified narcotic \\`waste' to disguise it.”\n\n【51】Decisions Not to Perform Euthanasia or Assisted Suicide\n-------------------------------------------------------\n\n【52】A total of 342 nurses reported that they had at times wanted to engage in euthanasia but did not. They were offered a number of possible explanations for the decision not to perform these acts. The most common explanations were a fear of getting caught, a concern that the practice might be illegal, and a concern that the patient's preferences were not fully understood. Many of the respondents expressed concern about losing their nursing licenses. Of the 164 nurses who reported engaging in some act to hasten a patient's death, 120 also reported forgoing such acts at some time. These nurses reported that their most important reasons for forgoing euthanasia or assisted suicide were a fear of getting caught (cited by 91 percent of the nurses), a concern that the practice might be illegal (83 percent), and a concern that the patient's preferences were not fully understood (80 percent).\n\n【53】Discussion\n----------\n\n【54】As public debate continues over the social, moral, and professional issues surrounding euthanasia and assisted suicide, 19 percent of the nurses in this study reported engaging in these practices. Some reported doing so on several occasions, and some reported doing so without the knowledge of physicians, patients, or surrogates and without their request.\n\n【55】If these practices had been sporadic, they might be attributed to a few lone practitioners, operating beyond the margins of their profession. Although the moral appropriateness of an action should not be measured by its pervasiveness, some will find it hard to accept the conclusion that so many nurses in this sample were acting inappropriately. We need to find another explanation.\n\n【56】One possible explanation is that although these activities may have been undertaken with the intent to hasten death, they may nevertheless reflect a continuum of moral acceptability and professional practice. At one end of the continuum, perhaps, are nurses who report hastening death in hidden or deceptive ways — for example, by deliberately giving lethal overdoses of medications. Others practiced at the limits of their authority — for example, by titrating intravenous drips within prescribed ranges but beyond required doses. Some nurses appealed, either fully or in part, to the doctrine of double effect, arguing either that their intent was only to relieve suffering or that their intent was both to relieve suffering and to hasten death. Finally, some nurses reported hastening death by administering especially high doses of opiates while withdrawing patients from mechanical ventilation. In these cases, death was imminent and was an accepted goal.\n\n【57】Because so many different kinds of activities are reflected in the responses of the nurses, and because these activities may have different moral implications, it is difficult to ascribe a single meaning to the results or to take a single moral stance toward them. Furthermore, a weakness of the study was the failure to distinguish between euthanasia and assisted suicide in the questionnaire. In either case, however, the intent to cause death was explicit. Those who see the worst in these findings should recognize that in some cases nurses reported only hastening death after life support was withdrawn. But those who see only innocent motivations should recall that the majority of the nurses who reported engaging in these practices also reported not engaging in them on some occasions for fear of getting caught. In the end, the range of activities described in this study may reveal the inadequacy of the term “euthanasia” and of the many professional and legal policies built on it.\n\n【58】Why were so many nurses in this study willing to perform euthanasia or assisted suicide? The nurses' handwritten comments help provide an explanation. A full discussion of these comments is not possible here, but recurring themes reported by the nurses include concern about the overuse of life-sustaining technology, a profound sense of responsibility for the patient's welfare, a desire to relieve suffering, and a desire to overcome the perceived unresponsiveness of physicians toward that suffering. Whether or not such goals justify euthanasia, they may reflect deeply held professional values. These values, when expressed through different practices, may be the source of the high professional regard nurses enjoy.\n\n【59】A related view is that these practices may represent an indictment of the behavior of some physicians. Although physicians increasingly accept the decisions of patients and surrogates to forgo life-sustaining treatment, in many cases they do not.  Furthermore, some physicians may not be sufficiently responsive to patients' pain and suffering; they may, for example, underuse analgesia, referrals to hospices, and other means of comforting dying patients. Many nurses raised these issues in their comments:\n\n【60】> Doctors need more bedside training — especially with patients and their families in the critical care setting. They should step into our shoes for about one month to get a much better idea of how much patients and their families are allowed to suffer.\n\n【61】> At my institution, physicians “beat” the people God has already called.\n\n【62】> I've often felt that the sign over a critical care unit should read: “Within Are Often Examples of Man's Inhumanity to His Fellow Man.”\n\n【63】These comments are consistent with some of the results from the Study to Understand Prognoses and Preferences for Outcomes and Risks of Treatment, in which nurses' reports of the preferences of terminally ill patients did not change either the behavior of physicians or the outcome of treatment.  Similarly, a comparison of the results reported here with those of a survey of physicians in Washington State  suggests that critical care nurses in the United States may be as willing to be involved in the practice of euthanasia and assisted suicide as are some physicians. This attitude may be explained by the close relationships nurses develop with severely ill patients.  Several nurses described the dilemmas they feel:\n\n【64】> I have experienced tremendous frustration and anger with physicians who either stress the possibility of a good prognosis, giving false hope — or place their belief system above that of their patients. The physician spends 5 to 10 minutes each day with the patient and then leaves me to carry out his orders and deal with the patient and his/her family for 8 to 12 hours. I'm left with the dilemma of carrying out orders that I believe — and sometimes know — are not in the patient's best interest or what the patient or family has expressed as their desires.\n\n【65】Comments such as these suggest latent disagreement between nurses and physicians. It would be a tragedy if, as a result of such disagreement, patients or nurses concluded that euthanasia or assisted suicide represented the best choice because potentially preferable options were undiscovered or seemed out of reach.\n\n【66】At the same time, many of the nurses who reported engaging in euthanasia said they did so at the request of attending physicians or other doctors. Such a finding raises many important questions: Were these physicians' requests based on requests from patients or surrogates? In asking nurses to participate in these activities, did physicians create difficult situations for the nurses? Unfortunately, this study cannot address these questions.\n\n【67】The reactions of the respondents to the survey were mixed. Many volunteered praise for the study and its goals. Some were grateful for a study they hoped would give voice to their concern. Others, however, were disturbed by the implication that nurses might be engaging in euthanasia. Although most of the respondents were supportive, it is likely that many of the nurses who were concerned or upset about the study simply chose not to respond to the mailings.\n\n【68】For this reason, the results of this survey are probably subject to nonrespondent bias. Nevertheless, the likely direction of such bias is unclear. The nurses most disturbed by the study might be those least likely to engage in these practices and also least likely to respond. Alternatively, as compared with the nurses who responded to the survey, the nonresponders might have been more likely to have engaged in euthanasia or assisted suicide but worried about disclosing that fact. Regardless of the direction of any nonrespondent bias, the high response rate suggests that such a bias could not have altered the findings much. Similarly, although the sample was designed to represent a broad range of critical care nurses, not all such nurses subscribe to _Nursing._ It is difficult to determine how representative of critical care nurses in the United States the final sample was. Finally, the results of this study are based on self-reports. Some respondents may have underreported or overreported activities, and some may have misunderstood questions. These limitations call for caution in interpreting the point estimates provided by this study; nevertheless, the broader implications of the study are likely to remain.\n\n【69】The issues surrounding euthanasia and assisted suicide are complex. On the one hand, in some cases the practice can appear to be a legitimate response to genuine human suffering. Permitting health professionals to carry out these activities may seem appropriate when the decision clearly fosters the patient's autonomy. From this perspective, the distinctions made between euthanasia and the withholding or withdrawing of life-sustaining treatment appear artificial and hard to sustain. In most cases, the aims and consequences of these actions are the same. On the other hand, should euthanasia be sanctioned, it might become too easy an option. Subtle pressures, real or perceived, might influence patients to choose this path when better options remain. Maintaining legal and professional prohibitions against euthanasia or assisted suicide may limit such tragedies better than any procedural safeguards.\n\n【70】An argument often made against physicians' performing euthanasia is that patients may come to fear physicians or distrust their motivations. I do not believe the results of this study suggest that patients or the public should fear or distrust critical care nurses. On the contrary, I think a central finding of this study is that these nurses struggle to uphold important personal values under extremely challenging circumstances — often with little support from physicians. National opinion surveys reveal that the majority of the public supports policies that would allow euthanasia under certain circumstances.  The results of this study should prompt nurses, physicians, and other health care professionals to examine their practices more openly and collaboratively, with the aim of understanding and reducing disagreement over goals and plans.\n\n【71】Although the increased discussion of euthanasia in the United States itself signals some movement of professional and popular thought, only a few state initiatives suggest any organized movement at the level of public policy. Two recent Federal Appeals Court decisions challenging Washington and New York statutes prohibiting assisted suicide may herald broader acceptance of these practices.  Nevertheless, practice often leads policy. Some may take the results presented here as a reason to permit euthanasia in specific circumstances — not just because the demand seems sufficient to result in the practice in any case, but also because by making procedures explicit, one can provide the oversight essential for protecting both patients and health care professionals.\n\n【72】Regardless of the policy implications of these findings, it is clear that the nurses in the study practice, often with little support, in extraordinarily difficult situations. In these complex environments, professional, moral, religious, and personal values frequently collide.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "813636d3-a39e-4209-85c2-f5d95954e695", "title": "Primary Care and the Opioid-Overdose Crisis — Buprenorphine Myths and Realities", "text": "【0】Primary Care and the Opioid-Overdose Crisis — Buprenorphine Myths and Realities\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with Dr. Brendan Saloner on increasing access to medication for treatment of opioid use disorder. \n\n【3】Despite widespread awareness of the opioid-overdose crisis, the epidemic continues to worsen. In 2016, there were 42,249 opioid-overdose deaths in the United States, a 28% increase from the previous year. According to the National Center for Health Statistics, life expectancy in the United States dropped in 2016 for the second consecutive year, partly because of an increase in deaths from unintentional injuries, including overdoses. It was the first 2-year decline since the 1960s. How can we be making so little progress?\n\n【4】Sign up for Alerts & Updates\n\n【5】Stay up to date on relevant content from the _New England Journal of Medicine_ with free email alerts.\n\n【6】SIGN UP\n\n【7】 Annual Change in Buprenorphine and Methadone Volume Dispensed in the United States, 2006–2016.\n\n【8】Each point represents the percentage change in absolute volume of buprenorphine or methadone dispensed from all sources in the United States (including retail pharmacies, hospitals, and outpatient methadone treatment facilities) as compared with the prior year, measured in grams dispensed. The earliest year with data available for buprenorphine was 2005, so 2006 is the first year we could estimate growth. Because of data irregularities for methadone in 2005, we began tracking methadone growth starting with 2006–2007. Data are from the Automation of Reports and Consolidated Orders System (ARCOS) of the Drug Enforcement Administration.\n\n【9】In part, the overdose crisis is an epidemic of poor access to care. One of the tragic ironies is that with well-established medical treatment, opioid use disorder can have an excellent prognosis. Decades of research have demonstrated the efficacy of medications such as methadone and buprenorphine in improving remission rates and reducing both medical complications and the likelihood of overdose death.  Unfortunately, treatment capacity is lacking: nearly 80% of Americans with opioid use disorder don’t receive treatment.  Although access to office-based addiction treatment has increased since federal approval of buprenorphine, data from the Drug Enforcement Administration (DEA) reveal that annual growth in buprenorphine distribution has been slowing, rather than accelerating to meet demand . To have any hope of stemming the overdose tide, we have to make it easier to obtain buprenorphine than to get heroin and fentanyl.\n\n【10】We believe there’s a realistic, scalable solution for reaching the millions of Americans with opioid use disorder: mobilizing the primary care physician (PCP) workforce to offer office-based addiction treatment with buprenorphine, as other countries have done. As of 2017, according to the Kaiser Family Foundation, there were more than 320,000 PCPs, plus a broad workforce of nurse practitioners and physician assistants, treating U.S. adults. In contrast, there are just over 3000 diplomates of the American Board of Addiction Medicine, and only 16% of 52,000 active psychiatrists had a waiver to prescribe buprenorphine in 2015 (moreover, 60% of U.S. counties have no psychiatrists).  Training enough addiction medicine or psychiatric specialists would take years, and most methadone treatment programs are already operating at 80% of capacity or greater. \n\n【11】However, PCPs and other generalists, including pediatricians, obstetrician–gynecologists, and physicians who treat human immunodeficiency virus (HIV) infection, are well situated to provide buprenorphine treatment. Many have risen to this challenge: PCPs are responsible for most ambulatory care visits for buprenorphine treatment. The importance of mobilizing the PCP workforce while ensuring the availability of sufficient specialists is not unique to the opioid-overdose crisis. During the height of the HIV/AIDS epidemic, for example, access to antiretroviral therapy was urgently needed. Although initially specialists were more likely to prescribe antiretrovirals, by 1990 equal percentages of patients were receiving antiretroviral therapy from PCPs and from specialists.\n\n【12】 Myths and Realities of Opioid Use Disorder Treatment.\n\n【13】How can we promote adoption of buprenorphine treatment by PCPs? The relevant federal and state regulatory barriers could be addressed, but they reflect a deeper problem: stigma and myths about buprenorphine treatment inhibit its acceptance .\n\n【14】The first myth is that buprenorphine is more dangerous than other interventions physicians master during training. In fact, PCPs regularly prescribe more complicated and risky treatments. Titrating insulin, starting anticoagulants, and prescribing full-agonist opioids for pain are often more challenging and potentially harmful than prescribing buprenorphine. Yet this perception has been cemented by federal policy. The Drug Addiction Treatment Act of 2000 requires that physicians complete 8 hours of training (sacrificing a full day of work) and apply for a DEA waiver to begin prescribing buprenorphine. After passing these hurdles, physicians are authorized to treat only a limited number of patients. These requirements make buprenorphine treatment intimidating.\n\n【15】The first step toward debunking this myth would be to scale back these federal regulations. Training in appropriate buprenorphine treatment optimizes outcomes and minimizes risks, but such training could be incorporated into existing medical education. All physicians could be trained during medical school and residency, so that both PCPs and other specialists would be equipped to offer this treatment — and, more generally, would be comfortable in caring for patients with opioid use disorder.\n\n【16】The second myth is that buprenorphine is simply a “replacement” and that patients become “addicted” to it — a belief still held by some physicians. But addiction is defined not by physiological dependence but by compulsive use of a drug despite harm. If relying on a daily medication to maintain health were addiction, then most patients with chronic health conditions such as diabetes or asthma would be considered addicted.\n\n【17】A closely related myth is that abstinence-based treatment, usually implying short-term detoxification and rehabilitation, is more effective than medication for addiction treatment. This belief underpins widespread advocacy for more substance use treatment “beds” as a key solution for the overdose crisis. But whereas there’s a strong evidence base for buprenorphine and methadone treatment, no study has shown that detoxification or 30-day rehabilitation programs are effective at treating opioid use disorder.  In fact, these interventions may increase the likelihood of overdose death by eliminating the tolerance that a patient had built up. To address myths about the effectiveness of buprenorphine and abstinence treatment, we can start with advocacy and education about the evidence to counter misleading depictions of addiction treatment in the media.\n\n【18】Another myth is that providing buprenorphine treatment is particularly onerous and time consuming. In our experience, it is no more burdensome than treating other chronic illnesses. A typical visit includes assessing medication adherence, examining disease control (e.g., cravings and use), titrating doses, and ordering laboratory tests. Moreover, buprenorphine treatment provides one of the rare opportunities in primary care to see dramatic clinical improvement: it’s hard to imagine a more satisfying clinical experience than helping a patient escape the cycle of active addiction. The fact that, for in-office inductions, patients must wait until withdrawal begins to take an initial buprenorphine dose under observation undoubtedly contributes to fears about the demand on physicians’ time. But this process has not been shown to be more effective than having patients start the medication outside the office. In fact, buprenorphine management provided by a PCP is effective with or without additional psychosocial interventions. This myth could be countered by developing and disseminating protocols emphasizing home induction and primary care models for treatment, including approaches consistent with efforts to transform practices into patient-centered medical homes.\n\n【19】Finally, some observers believe that physicians should simply stop prescribing so many opioids. The crisis began with increased opioid prescribing, yet as prescribing rates have fallen since 2011, overdose deaths have accelerated. If prescribing patterns were the sole driver of overdoses, then decreased prescribing should have had a measurable effect on opioid-related mortality over the past several years. In reality, research has demonstrated that interventions like the introduction of abuse-deterrent Oxycontin, which reduce access to frequently misused prescription opioids, have resulted in people shifting their opioid of choice predominantly to heroin. Rising overdose mortality despite decreasing opioid prescribing suggests that merely reducing the prescription-opioid supply will have little positive short-term impact. Reducing prescribing could even increase the death toll as people with opioid use disorder or untreated pain shift into the unstable, illicit drug market. Instead, we need safer, more thoughtful opioid prescribing and accessible support, such as electronic consultations with addiction specialists, to help physicians offer buprenorphine for people with opioid use disorder.\n\n【20】We are in the midst of a historic public health crisis that demands action from every physician. Without dramatic intervention, life expectancy in the United States will continue to decline. Mobilizing the PCP workforce to offer office-based buprenorphine treatment is a plausible, practical, and scalable intervention that could be implemented immediately. The opioid-overdose epidemic is complex and will require concerted efforts on multiple fronts, but few other evidence-based actions would have such an immediate lifesaving effect. It won’t be easy, but we are confident that U.S. PCPs have the clinical skill and grit to take on this challenge.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8ab429fb-c08d-4833-ada4-a88221950f71", "title": " and ", "text": "【0】Abstract\n--------\n\n【1】Background\n----------\n\n【2】A recent genomewide mutational analysis of glioblastomas (World Health Organization \\[WHO\\] grade IV glioma) revealed somatic mutations of the isocitrate dehydrogenase 1 gene ( _IDH1_ ) in a fraction of such tumors, most frequently in tumors that were known to have evolved from lower-grade gliomas (secondary glioblastomas).\n\n【3】Methods\n-------\n\n【4】We determined the sequence of the _IDH1_ gene and the related _IDH2_ gene in 445 central nervous system (CNS) tumors and 494 non-CNS tumors. The enzymatic activity of the proteins that were produced from normal and mutant _IDH1_ and _IDH2_ genes was determined in cultured glioma cells that were transfected with these genes.\n\n【5】Results\n-------\n\n【6】We identified mutations that affected amino acid 132 of _IDH1_ in more than 70% of WHO grade II and III astrocytomas and oligodendrogliomas and in glioblastomas that developed from these lower-grade lesions. Tumors without mutations in _IDH1_ often had mutations affecting the analogous amino acid (R172) of the _IDH2_ gene. Tumors with _IDH1_ or _IDH2_ mutations had distinctive genetic and clinical characteristics, and patients with such tumors had a better outcome than those with wild-type _IDH_ genes. Each of four tested _IDH1_ and _IDH2_ mutations reduced the enzymatic activity of the encoded protein.\n\n【7】Conclusions\n-----------\n\n【8】Mutations of NADP <sup>+ </sup> \\-dependent isocitrate dehydrogenases encoded by _IDH1_ and _IDH2_ occur in a majority of several types of malignant gliomas.\n\n【9】Introduction\n------------\n\n【10】Gliomas, the most common type of primary brain tumors, are classified as grade I to grade IV on the basis of histopathological and clinical criteria established by the World Health Organization (WHO).  This group of tumors includes specific histologic subtypes, the most common of which are astrocytomas, oligodendrogliomas, and ependymomas. WHO grade I gliomas, often considered to be benign, are generally curable with complete surgical resection and rarely, if ever, evolve into higher-grade lesions.  By contrast, gliomas of WHO grade II or III are invasive, progress to higher-grade lesions, and have a poor outcome. WHO grade IV tumors (glioblastomas), the most invasive form, have a dismal prognosis.  On the basis of histopathological criteria, it is impossible to distinguish a secondary glioblastoma, defined as a tumor that was previously diagnosed as a lower-grade glioma, from a primary tumor. \n\n【11】Several genes, including _TP53, PTEN, CDKN2A,_ and _EGFR,_ are altered in gliomas.  These alterations tend to occur in a defined order during the progression to a high-grade tumor. The _TP53_ mutation appears to be a relatively early event during the development of an astrocytoma, whereas the loss or mutation of _PTEN_ and amplification of _EGFR_ are characteristic of higher-grade tumors.  In oligodendrogliomas, allelic losses of 1p and 19q occur in many WHO grade II tumors, whereas losses of 9p21 are largely confined to WHO grade III tumors. \n\n【12】In a recent genomewide analysis, we identified somatic mutations at codon 132 of the isocitrate dehydrogenase 1 gene ( _IDH1_ ) in approximately 12% of glioblastomas.  These mutations were also found in five of six secondary glioblastomas. The results suggested that _IDH1_ mutations might occur after formation of a low-grade glioma and drive the progression of the tumor to a glioblastoma. To evaluate this possibility, we analyzed a large number of gliomas of various types.\n\n【13】Methods\n-------\n\n【14】DNA Samples\n-----------\n\n【15】Table 1. Summary of Genetic and Clinical Characteristics of Brain Tumors in the Study.\n\n【16】DNA was extracted from samples of primary brain tumor and xenografts and from patient-matched normal blood lymphocytes obtained from the Tissue Bank at the Preston Robert Tisch Brain Tumor Center at Duke University and collaborating centers, as described previously.  All analyzed brain tumors were subjected to consensus review by two neuropathologists. Table 1 lists the types of brain tumors we analyzed. The samples from glioblastomas included 138 primary tumors and 13 secondary tumors. Of the 138 primary tumors, 15 were from patients under the age of 21 years. Secondary glioblastomas were categorized as WHO grade IV on the basis of histologic criteria but had been categorized as WHO grade II or III at least 1 year earlier. Of the 151 tumors, 63 had been analyzed in our previous genomewide mutation analysis of glioblastomas. None of the lower-grade tumors were included in that analysis. \n\n【17】In addition to brain tumors, we analyzed 35 lung cancers, 57 gastric cancers, 27 ovarian cancers, 96 breast cancers, 114 colorectal cancers, 95 pancreatic cancers, and 7 prostate cancers, along with 4 samples from patients with chronic myelogenous leukemia, 7 from patients with chronic lymphocytic leukemia, 7 from patients with acute lymphoblastic leukemia, and 45 from patients with acute myelogenous leukemia. All samples were obtained in accordance with the Health Insurance Portability and Accountability Act. Acquisition of tissue specimens was approved by the institutional review board at the Duke University Health System and at each of the participating institutions.\n\n【18】Exon 4 of the _IDH1_ gene was amplified with the use of a polymerase-chain-reaction (PCR) assay and sequenced in DNA from the tumor and lymphocytes from each patient, as described previously.  In all gliomas and medulloblastomas without an R132 _IDH1_ mutation, exon 4 of the _IDH2_ gene (which contains the _IDH2_ residue equivalent to R132 of _IDH1_ ) was sequenced and analyzed for somatic mutations. In addition, we evaluated all astrocytomas and oligodendrogliomas of WHO grade I to grade III, all secondary glioblastomas, and 96 primary glioblastomas without R132 _IDH1_ mutations or R172 _IDH2_ mutations for alterations in the remaining coding exons of _IDH1_ and _IDH2_ . All coding exons of _TP53_ and _PTEN_ were also sequenced in the panel of diffuse astrocytomas, oligodendrogliomas, anaplastic oligodendrogliomas, anaplastic astrocytomas, and glioblastomas. _EGFR_ amplification and the _CDKN2A–CDKN2B_ deletion were analyzed with the use of quantitative real-time PCR in the same tumors.  We evaluated samples of oligodendrogliomas and anaplastic oligodendrogliomas for loss of heterozygosity at 1p and 19q, as described previously. \n\n【19】Enzymatic Activity\n------------------\n\n【20】To assess the enzymatic activity of wild-type and mutant IDH1 and IDH2 proteins, a human oligodendroglioma line without _IDH1_ or _IDH2_ mutations was transfected with a vector (pCMV6, Invitrogen) containing the coding sequences of the wild-type _IDH1,_ wild-type _IDH2,_ or mutant _IDH_ genes (corresponding to the most common _IDH1_ mutation, R132H, or the _IDH2_ mutations R172G, R172K, and R172M). Clones of the wild-type _IDH1_ and _IDH2_ genes were obtained from Origene, and mutations were introduced by standard methods.\n\n【21】Cells were collected 48 hours after transfection, subjected to centrifugation at 1000× _g_ for 10 minutes at 4°C, washed once with cold phosphate-buffered saline, and lysed in buffer containing 0.1% Triton X-100. They were then disrupted by ultrasonication and centrifuged at 12,000× _g_ for 30 minutes. The supernatants were used to measure IDH activity. Expression levels of wild-type and mutant IDH proteins were determined by Western blotting with the use of an antibody against FLAG, a polypeptide protein tag. For each enzymatic reaction, a volume of cell lysate containing the same amount of IDH protein was added to 1 ml of assay solution containing 33 mM of Tris buffer, 0.33 mM of EDTA, 0.1 mM of NADP <sup>+ </sup> , 1.33 mM of manganese chloride, and 1.3 mM of isocitrate. The activity of IDH was analyzed through the reduction of NADP <sup>+ </sup> to NADPH, which was measured at 25°C by spectrophotometry at 340 nm 5 times a second for 300 seconds. \n\n【22】Clinical Data and Survival\n--------------------------\n\n【23】Clinical information included the date of birth, the date the study sample was obtained, the date of pathological diagnosis, the date and pathology of any preceding diagnosis of a lower-grade glioma, the use or nonuse of radiation therapy or chemotherapy before the date that the study sample was obtained, the date of the last contact with the patient, and the patient's status at the time of the last contact. We calculated overall survival for patients with anaplastic astrocyomas, including 38 patients with mutations in _IDH1_ or _IDH2_ and 14 with wild-type genes, and for adult patients (≥21 years of age) with glioblastomas, including 14 patients with mutations in _IDH1_ or _IDH2_ and 115 with wild-type genes, using the date of histologic diagnosis and the date of the last contact with the patient or death. For patients with secondary glioblastomas, survival was calculated from the date of secondary diagnosis. Seven patients with glioblastomas were not included in the statistical analysis because of insufficient survival data.\n\n【24】Study Design\n------------\n\n【25】The authors designed the study, gathered and analyzed the data, wrote the manuscript, and made the decision to publish the findings. Gene sequencing was performed by Agencourt Bioscience, a subsidiary of Beckman Coulter. The lead academic authors vouch for the completeness and accuracy of the data and the analyses.\n\n【26】Statistical Analysis\n--------------------\n\n【27】We examined the association between the occurrence of mutations in _IDH1_ or _IDH2_ and other genetic alterations using Fisher's exact test. Kaplan–Meier survival curves were plotted and the survival distributions were compared with the use of the Mantel–Cox log-rank test and the Wilcoxon test. All reported P values are two-sided, and P values of less than 0.01 were considered to indicate statistical significance.\n\n【28】Results\n-------\n\n【29】Sequence Analysis\n-----------------\n\n【30】Figure 1. _IDH1_ and _IDH2_ Mutations in Human Gliomas.\n\n【31】Panel A shows mutations at codon R132 in _IDH1_ and R172 in _IDH2_ that were identified in human gliomas, along with the number of patients who carried each mutation. Codons 130 to 134 of _IDH1_ and 170 to 174 of _IDH2_ are shown. Panel B shows the number and frequency of _IDH1_ and _IDH2_ mutations in gliomas and other types of tumors. The roman numerals in parentheses are the tumor grades, according to histopathological and clinical criteria established by the World Health Organization. CNS denotes central nervous system.\n\n【32】Sequence analysis of _IDH1_ in 939 tumor samples revealed 161 somatic mutations at residue R132, including R132H (142 tumors), R132C (7 tumors), R132S (4 tumors), R132L (7 tumors), and R132G (1 tumor) . Table 1 and Figure 1B show the tumors with somatic R132 mutations. No other somatic mutations of _IDH1_ in the remaining _IDH1_ exons of R132-negative tumors were found in all WHO grade I to grade III astrocytomas and oligodendrogliomas, in all secondary glioblastomas, and in 96 primary glioblastomas. No R132 mutations were observed in 21 pilocytic astrocytomas (WHO grade I), 2 subependymal giant-cell astrocytomas (WHO grade I), 30 ependymomas (WHO grade II), 55 medulloblastomas, or any of the 494 non–central nervous system tumor samples.\n\n【33】We also sought alterations in other genes with functions similar to those of _IDH1_ in tumors without _IDH1_ mutations. For this purpose, we analyzed the _IDH2_ gene, which encodes the only human protein homologous to _IDH1_ that uses NADP <sup>+ </sup> as an electron acceptor. Sequence evaluation of all _IDH2_ exons in these glioma samples revealed nine somatic mutations of _IDH2,_ all at residue R172: R172G in two tumors, R172M in three tumors, and R172K in four tumors . The R172 residue in _IDH2_ is the exact analogue of the R132 residue in _IDH1,_ which is located in the active site of the enzyme and forms hydrogen bonds with the isocitrate substrate. \n\n【34】Figure 2. Enzymatic Activity of Wild-Type and Mutant IDH1 and IDH2 Proteins.\n\n【35】Cell lysates were extracted from a human oligodendroglioma cell line without _IDH1_ or _IDH2_ mutations that had been transfected with vectors encoding the indicated proteins. Panel A shows the expression of proteins encoded by wild-type and mutant _IDH1_ and _IDH2,_ as determined by Western blotting, with the use of an anti-FLAG antibody. Panel B shows the activity levels of these proteins, as analyzed by monitoring the production of NADPH. GAPDH denotes glyceraldehyde 3-phosphate dehydrogenase.\n\n【36】To determine whether the mutations in _IDH1_ and _IDH2_ disturb the function of the corresponding proteins, we measured the enzymatic activity (reduction of NADP <sup>+ </sup> to NADPH) of IDH1 and IDH2 proteins in an oligodendroglioma line that had been transfected with wild-type or mutant _IDH1_ or _IDH2_ genes. These mutants represented 88% of the _IDH1_ mutations and 100% of the _IDH2_ mutations found in patients. Figure 2 shows that exogenous expression of wild-type IDH1 or IDH2 significantly increased the production of NADPH, whereas only endogenous IDH activity was observed in cells that had been transfected with mutant _IDH1_ or _IDH2_ genes.\n\n【37】To further evaluate _IDH_ alterations during glioma progression, we assessed _IDH1_ mutations in seven progressive gliomas in which both low-grade and high-grade tumor samples were available. Sequence analysis identified _IDH1_ mutations in both the low-grade and high-grade tumors in all seven cases . These results demonstrate that _IDH1_ alterations in high-grade tumors are derived from the earlier lesions.\n\n【38】We also examined diffuse astrocytomas, oligodendrogliomas, anaplastic oligodendrogliomas, anaplastic astrocytomas, and a subgroup of glioblastomas for mutations in _TP53_ and _PTEN,_ amplification of _EGFR,_ deletion of _CDKN2A–CDKN2B,_ and allelic losses of 1p and 19q . _TP53_ mutations were more common in diffuse astrocytomas (74%), anaplastic astrocytomas (65%), and secondary glioblastomas (62%) than in oligodendrogliomas (16%) or anaplastic oligodendrogliomas (9%) (P<0.001 for all comparisons by Fisher's exact test). Conversely, deletions of 1p and 19q were found more often in oligodendrocytic than in astrocytic tumors, as expected. \n\n【39】Table 2. Frequency of Common Genetic Alterations in Gliomas with Mutated or Wild-Type _IDH1_ and _IDH2_ Genes.\n\n【40】Most (80%) of the anaplastic astrocytomas and glioblastomas with mutated _IDH1_ or _IDH2_ genes also had a mutation of _TP53,_ but only 3% had alterations in _PTEN,_ _EGFR,_ _CDKN2A,_ or _CDKN2B_ . Conversely, anaplastic astrocytomas and glioblastomas with wild-type _IDH1_ and _IDH2_ genes had few _TP53_ mutations (18%) and more frequent alterations of _PTEN,_ _EGFR,_ _CDKN2A,_ or _CDKN2B_ (74%) (P<0.001 for both comparisons by Fisher's exact test). Loss of 1p and 19q was observed in 45 of 53 (85%) of the oligodendrocytic tumors with mutated _IDH1_ or _IDH2_ but in none of the tumors with wild-type _IDH_ genes (P<0.001 by Fisher's exact test).\n\n【41】Patients with anaplastic astrocytomas or glioblastomas with _IDH1_ or _IDH2_ mutations were significantly younger than were patients with tumors carrying wild-type _IDH1_ and _IDH2_ genes (median age, 34 years vs. 56 years for patients with anaplastic astrocytomas and 32 years vs. 59 years for those with glioblastomas; P<0.001 for both comparisons by Student's t-test). Despite the lower median age of patients with _IDH1_ or _IDH2_ mutations, no mutations were identified in glioblastomas from the 15 patients who were under the age of 21 . In patients with oligodendrogliomas or anaplastic oligodendrogliomas, the median age of the patients with _IDH1_ or _IDH2_ mutation was 39 years; _IDH1_ mutations were identified in two teenagers (14 and 16 years) but not in four younger patients.\n\n【42】Figure 3. Survival of Adult Patients with Malignant Gliomas with or without _IDH_ Gene Mutations.\n\n【43】For patients with glioblastomas, the median survival was 31 months for the 14 patients with mutated _IDH1_ or _IDH2,_ as compared with 15 months for the 115 patients with wild-type _IDH1_ or _IDH2_ . For patients with anaplastic astrocytomas, the median survival was 65 months for the 38 patients with mutated _IDH1 or IDH2,_ as compared with 20 months for the 14 patients with wild-type _IDH1_ or _IDH2_ . Patients with both primary and secondary tumors were included in the analysis. For patients with secondary glioblastomas, survival was calculated from the date of the secondary diagnosis. Survival distributions were compared with the use of the log-rank test.\n\n【44】Our previous observation of improved outcome for patients whose glioblastomas carried the _IDH1_ mutation  was confirmed in this larger data set and extended to include such patients with mutations in _IDH2_ . Patients with a glioblastoma carrying an _IDH1_ or _IDH2_ mutation had a median overall survival of 31 months, which was significantly longer than the 15-month survival in patients with wild-type _IDH1_ (P=0.002 by the log-rank test) . Mutations of _IDH_ genes were also associated with improved outcome in patients with anaplastic astrocytomas; the median overall survival was 65 months for patients with mutations and 20 months for those without mutations (P<0.001 by the log-rank test) . Differential survival analyses could not be performed in patients with diffuse astrocytomas, oligodendrogliomas, or anaplastic oligodendrogliomas because there were too few tumors of these types without _IDH_ gene mutations.\n\n【45】Discussion\n----------\n\n【46】Our findings implicate mutations in the NADP <sup>+ </sup> \\-dependent isocitrate dehydrogenase genes, _IDH1_ and _IDH2,_ in the pathogenesis of malignant gliomas. Gliomas with _IDH_ mutations were clinically and genetically distinct from gliomas with wild-type _IDH_ genes. Notably, two subtypes of gliomas of WHO grade II or III (astrocytomas and oligodendrogliomas) often carried _IDH_ mutations but not other genetic alterations that are detectable relatively early during the progression of gliomas. This finding suggests that _IDH_ mutations occur early in the development of a glioma from a stem cell that can give rise to both astrocytes and oligodendrocytes. The identification of _IDH1_ mutations in 10 of 10 oligoastrocytomas and anaplastic oligoastrocytomas, tumors with morphologic features of both cell types, supports this conjecture.\n\n【47】Mutations in _IDH1_ or _IDH2_ were not identified in any pilocytic astrocytomas of WHO grade I, indicating that these tumors arise through a different mechanism. This conclusion is consistent with clinical observations that pilocytic astrocytomas rarely if ever undergo malignant transformation  and with recent data indicating that a duplication at 7q34 producing a _BRAF_ fusion gene occurs frequently in pilocytic astrocytomas but not higher-grade gliomas. \n\n【48】In each of the tested mutations, the enzymatic activity of the IDH proteins was eliminated. A previous study showed that in vitro substitution of glutamate for arginine at residue 132 of IDH1 (an alteration not observed in patients) resulted in a catalytically inactive enzyme.  Although our results demonstrate an effect of the mutations on the function of the IDH1 protein, they do not necessarily mean that the mutations are inactivating. For example, the mutant proteins that preclude the use of isocitrate as substrate could allow other, as-yet-unknown substrates to be used by the enzyme, thereby conferring a gain rather than a loss of activity. If future studies confirm this possibility, mutant IDH could become a target for therapeutic intervention.\n\n【49】Our results have important practical implications. Historically, glioblastomas have been divided into cancers that arise from low-grade gliomas (secondary tumors) and those without such an antecedent (primary tumors).  Secondary tumors account for only 5% of all glioblastomas. The finding that _IDH1_ or _IDH2_ is mutated in the vast majority of WHO grade II or III gliomas and in the secondary glioblastomas that develop from these precursors provides a biologic explanation for this clinical categorization: tumors with mutated NADP <sup>+ </sup> \\-dependent isocitrate dehydrogenases comprise a specific subgroup of glioblastomas.\n\n【50】The localization of _IDH1_ and _IDH2_ mutations to a single amino acid (R132 and R172, respectively) simplifies the use of this genetic alteration for diagnostic purposes. For example, _IDH_ mutation tests could help distinguish pilocytic astrocytomas (WHO grade I) from diffuse astrocytomas (WHO grade II), since these lesions can sometimes be difficult to categorize solely on the basis of histopathological criteria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "def0cee2-1b8f-418a-9e18-bc5970c7b90d", "title": "Springing a Leak", "text": "【0】Springing a Leak\nA 52-year-old man presented to the emergency department with general weakness and swelling in his legs. Symmetric swelling had begun 4 weeks earlier and had progressed to the point that it was difficult for him to wear shoes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "15a89cd4-647d-4652-ab9b-5d4c1dff6c33", "title": "Rapid Decay of Anti–SARS-CoV-2 Antibodies in Persons with Mild Covid-19", "text": "【0】Rapid Decay of Anti–SARS-CoV-2 Antibodies in Persons with Mild Covid-19\nTo the Editor:\n--------------\n\n【1】A recent article suggested the rapid decay of anti–SARS-CoV-2 IgG in early infection,  but the rate was not described in detail. We evaluated persons who had recovered from Covid-19 and referred themselves to our institution for observational research. Written informed consent was obtained from all the participants, with approval by the institutional review board. Blood samples were analyzed by enzyme-linked immunosorbent assay (ELISA) to detect anti–SARS-CoV-2 spike receptor-binding domain IgG.  The ELISA was further modified to precisely quantify serum anti–receptor-binding domain activity in terms of equivalence to the concentration of a control anti–receptor-binding domain monoclonal IgG (CR3022, Creative Biolabs).\n\n【2】Infection had been confirmed by polymerase-chain-reaction assay in 30 of the 34 participants. The other 4 participants had had symptoms compatible with Covid-19 and had cohabitated with persons who were known to have Covid-19 but were not tested because of mild illness and the limited availability of testing. Most of the participants had mild illness; 2 received low-flow supplemental oxygen and leronlimab (a CCR5 antagonist), but they did not receive remdesivir. There were 20 women and 14 men. The mean age was 43 years (range, 21 to 68) .\n\n【3】A total of 31 of the 34 participants had two serial measurements of IgG levels, and the remaining 3 participants had three serial measurements. The first measurement was obtained at a mean of 37 days after the onset of symptoms (range, 18 to 65), and the last measurement was obtained at a mean of 86 days after the onset of symptoms (range, 44 to 119).\n\n【4】Figure 1. Longitudinal Assessment of Anti–SARS-CoV-2 Receptor-Binding Domain IgG in Persons Who Recovered from Covid-19.\n\n【5】Approximately 80 persons who recovered from Covid-19 referred themselves to our institution to inquire about observational research. Of 68 persons who volunteered to provide initial blood samples, 41 returned to provide repeat samples. Of those persons, 3 were excluded from this analysis because of unclear timing of infection and 4 were excluded because of initial and repeat serum antibody measurements below the limit of reliable quantitative detection. For the 34 participants in our analysis, anti–SARS-CoV-2 receptor-binding domain (RBD) serum IgG concentrations were quantified by enzyme-linked immunosorbent assay as equivalent binding activity to a concentration of a control monoclonal IgG for at least two time points (31 of the 34 participants had two measurements, and the remaining 3 participants had three measurements). Panel A shows log-transformed IgG concentrations plotted against the time since the onset of symptoms in each participant. Panel B shows a linear regression model that was created to estimate the effects of the participants’ age and sex, the days from symptom onset to the first measurement, and the first measured log <sub>10 </sub> antibody level on the slope reflecting the change in anti-RBD antibody levels (in log <sub>10 </sub> ng per milliliters per day). The values for age and antibody level were centered at the mean. The time since symptom onset was centered at day 18 and adjusted per 100 days. Thus, the intercept of the model can be interpreted as the average slope adjusted for age, sex, and time and value of the first measurement. CI denotes confidence interval.\n\n【6】The initial mean IgG level was 3.48 log <sub>10 </sub> ng per milliliter (range, 2.52 to 4.41). On the basis of a linear regression model that included the participants’ age and sex, the days from symptom onset to the first measurement, and the first log <sub>10 </sub> antibody level, the estimated mean change (slope) was −0.0083 log <sub>10 </sub> ng per milliliter per day (range, −0.0352 to 0.0062), which corresponds to a half-life of approximately 36 days over the observation period . The 95% confidence interval for the slope was −0.0115 to −0.0050 log <sub>10 </sub> ng per milliliter per day (half-life, 26 to 60 days) .\n\n【7】The protective role of antibodies against SARS-CoV-2 is unknown, but these antibodies are usually a reasonable correlate of antiviral immunity, and anti–receptor-binding domain antibody levels correspond to plasma viral neutralizing activity. Given that early antibody decay after acute viral antigenic exposure is approximately exponential,  we found antibody loss that was quicker than that reported for SARS-CoV-1,  and our findings were more consistent with those of Long et al.  Our findings raise concern that humoral immunity against SARS-CoV-2 may not be long lasting in persons with mild illness, who compose the majority of persons with Covid-19. It is difficult to extrapolate beyond our observation period of approximately 90 days because it is likely that the decay will decelerate.  Still, the results call for caution regarding antibody-based “immunity passports,” herd immunity, and perhaps vaccine durability, especially in light of short-lived immunity against common human coronaviruses. Further studies will be needed to define a quantitative protection threshold and rate of decline of antiviral antibodies beyond 90 days.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e8198f97-3738-4879-bf08-a331a0fe3d1a", "title": "The Effect of Cyclosporine on the Use of Hospital Resources for Kidney Transplantation", "text": "【0】The Effect of Cyclosporine on the Use of Hospital Resources for Kidney Transplantation\nAbstract\n\n【1】Over the past decade the clinical results of kidney transplantation have improved substantially, with much of the benefit being attributed to the introduction in late 1983 of the immunosuppressive drug cyclosporine. To assess the effect of cyclosporine on the use of hospital services, we studied 702 patients who received kidney transplants at the University of California, San Francisco, between July 1982 and June 1986. All services were priced in constant 1985 dollars, and multiple regression analysis was used to adjust for changing patient and hospital characteristics.\n\n【2】The introduction of cyclosporine for patients receiving kidneys from cadavers was associated with a significantly shorter adjusted mean postoperative stay (26.4 days as compared with 37.0 for patients not taking the drug; P<0.0001) and lower adjusted mean hospital charges ($28,649 as compared with $37,895; P<0.0001), although cyclosporine was not associated with changes in the use of services by patients who received transplants from living related donors. Cyclosporine was also associated with a reduction in the use of certain ancillary services, such as laboratory tests and radiographic procedures. In patients without diabetes who received cadaver kidneys, a sequential cyclosporine regimen (in which a combination of antilymphoblast globulin, prednisone, and azathioprine was given before cyclosporine) reduced the use of hospital services even more than did a cyclosporine regimen in which the combination was not given.\n\n【3】The results suggest that new medications, such as cyclosporine, that reduce the frequency of complications and improve outcomes may also reduce the use of hospital resources.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "df6a19e6-9f8a-48e6-b0af-42645e2b4893", "title": "Metabolism and Congenital Malformations — NAD’s Effects on Development", "text": "【0】Metabolism and Congenital Malformations — NAD’s Effects on Development\nA new study shows that a deficiency of nicotinamide adenine dinucleotide causes congenital malformations. The mechanism is not known, but dietary supplementation with NAD precursors merits further study.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0feb65c0-29d2-43c7-9c1d-6eafda1ad76a", "title": "Multiple-System Atrophy", "text": "【0】Multiple-System Atrophy\nMultiple-system atrophy is a neurodegenerative disease characterized by progressive autonomic failure, parkinsonism, and cerebellar and pyramidal tract symptoms. Glial cytoplasmic inclusions of α-synuclein are a defining histologic feature. There is no curative treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f0c2981b-d1a3-40ab-9c8a-7a85363e6e1d", "title": "The Sequencing of Chemotherapy and Radiation Therapy after Conservative Surgery for Early-Stage Breast Cancer", "text": "【0】The Sequencing of Chemotherapy and Radiation Therapy after Conservative Surgery for Early-Stage Breast Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Patients with early-stage breast cancer who are at substantial risk for systemic metastases are increasingly treated with breast-conserving therapy and adjuvant chemotherapy. However, the optimal sequencing of chemotherapy and radiation therapy is not clear.\n\n【3】Methods\n-------\n\n【4】Two hundred forty-four patients with stage I or II breast cancer who were at substantial risk for distant metastases were randomly assigned to receive a 12-week course of chemotherapy either before or after radiation therapy. All had had breast-conserving surgery. The median length of follow-up in surviving patients was 58 months (range, 10 to 124).\n\n【5】Results\n-------\n\n【6】The five-year actuarial rates of cancer recurrence at any site and of distant metastases in the radiotherapy-first group and the chemotherapy-first group were 38 percent and 31 percent (P = 0.17) and 36 percent and 25 percent (P = 0.05), respectively. Overall survival was 73 percent and 81 percent (P = 0.11), respectively. The five-year crude rates of first recurrence according to site in the radiotherapy-first and chemotherapy-first groups, respectively, were 5 percent and 14 percent for local recurrence and 32 percent and 20 percent for distant or regional recurrence or both. This difference in the pattern of recurrence was of borderline statistical significance (P = 0.07).\n\n【7】Conclusions\n-----------\n\n【8】This study suggests that for patients at substantial risk for systemic metastases, it is preferable to give a 12-week course of chemotherapy followed by radiation therapy, rather than radiation therapy followed by chemotherapy.\n\n【9】Introduction\n------------\n\n【10】The temporal order in which patients with early-stage invasive breast cancer receive chemotherapy and radiation therapy after breast-conserving surgery may affect the clinical outcome.  To our knowledge, no previous randomized trial has addressed this issue directly, although two trials comparing different chemotherapy schedules may also yield information about the effect of the timing of radiation therapy after surgery. \n\n【11】We conducted a randomized trial to test whether the sequence of administration of chemotherapy and radiation therapy after breast-conserving surgery influences the outcome among patients at substantial risk for systemic metastases. We found that giving chemotherapy first had better overall results but was associated with an increased risk of local recurrence.\n\n【12】Methods\n-------\n\n【13】Study Design and Selection of Patients\n--------------------------------------\n\n【14】From June 1984 to December 1992, 244 patients with clinical stage I or II breast carcinoma  were randomly assigned after surgery to receive chemotherapy either before or after radiotherapy. Before June 1988, patients had to have pathologic involvement of the axillary nodes to be eligible; after that date, patients with uninvolved nodes were eligible if the primary tumor tested negative for the estrogen-receptor protein (less than 10 fmol per milligram) or if lymphatic vessels had been invaded. Patients were excluded if they had gross residual disease in the breast or axilla, previous cancer (excluding nonmelanoma skin cancer), previous radiotherapy or chemotherapy, or serious coexisting illnesses. Written informed consent was obtained in accordance with institutional and federal requirements.\n\n【15】Characteristics of Patients\n---------------------------\n\n【16】Table 1. Clinical and Pathological Features of the Two Groups of Patients.\n\n【17】Table 1 lists the relevant features of the patients and their breast tumors. Patients were considered premenopausal if at the time of diagnosis they had had at least one menstrual period in the previous 12 months. Histopathologic features of the primary tumor were recorded on the basis of the original reports; a central review was not performed. We defined a “negative” microscopical resection margin as the presence of more than 1 mm of uninvolved breast tissue between the tumor and the inked edge, a “close” margin as 1 mm or less, and a “positive” margin as the presence of tumor (either invasive or intraductal) at the inked resection margin.  Prognostic factors were distributed uniformly in the two treatment groups, except for a greater number of tumors containing extensive intraductal components  in the radiotherapy-first group.\n\n【18】Treatment\n---------\n\n【19】### _Surgery_\n\n【20】In most patients, the initial diagnostic and therapeutic procedure was excision of the tumor along with a small rim of normal breast tissue. Re-excision was commonly performed . A level I or II axillary dissection was recommended, although the actual operation performed was left to the discretion of the surgeon.\n\n【21】### _Radiation Therapy_\n\n【22】Radiotherapy technique has been described elsewhere.  The entire breast received a dose of 45 Gy in 25 fractions, with an allowable variation of 10 percent. This was followed by a boost to the primary tumor site of 16 to 18 Gy, with an allowable variation of 20 percent, delivered with either an electron beam or interstitial brachytherapy. Regional lymph nodes (supraclavicular, axillary, or internal mammary nodes, or a combination of these) were irradiated to a maximal dose of 50 Gy at the discretion of the radiation oncologist. Treatment procedures were similar in both groups . All treated patients received doses that were within protocol guidelines, except one who received a planned total dose of 50.4 Gy and one who received a dose of only 3.6 Gy because of the discovery of distant metastases during irradiation.\n\n【23】### _Chemotherapy_\n\n【24】Prescribed drug doses were based on ideal body weight. The drugs were administered according to the following doses and schedules: methotrexate — 200 mg per square meter of body-surface area administered intravenously on days 1 and 15; leucovorin — 10 mg per square meter orally every six hours for 12 doses, starting on day 2 and day 16; fluorouracil — 500 mg per square meter intravenously on day 1; cyclophosphamide — 500 mg per square meter intravenously on day 1; prednisone — 40 mg per square meter per day orally for five days, starting on day 1; and doxorubicin, 45 mg per square meter intravenously on day 3. Cycles were repeated every 21 days for a total of four cycles. Complete blood counts were performed on days 1 and 15 of each cycle. The doses of cyclophosphamide and doxorubicin were reduced by 25 percent for the next cycle if the granulocyte count on either day 15 or day 22 (i.e., day 1 of the next cycle) was 300 to 499 per cubic millimeter. If either of these granulocyte counts was 299 per cubic millimeter or less, or if the platelet count was less than 50,000 per cubic millimeter, the doses were reduced by 50 percent. At the beginning of each cycle, the doses were based on the lowest granulocyte and platelet counts of the previous cycle; if a treatment cycle with reduced doses did not again result in a need for dose modification, the doses were increased to the original starting level. If the granulocyte count on day 22 was less than on day 15, the next cycle was delayed one week. Methotrexate was not given on day 15 if the granulocyte count on that day was less than 500 per cubic millimeter, if the serum creatinine concentration had increased by 50 percent or more above the pretreatment value, or if active mucositis was present. The dose of doxorubicin was reduced by 50 percent if the bilirubin concentration on day 1 was greater than twice the upper limit of the institution's normal reference value. No patient failed to complete the chemotherapy protocol because of toxicity, but two patients (both in the radiotherapy-first group) in whom distant metastases were discovered during chemotherapy were switched to other regimens.\n\n【25】### _Tamoxifen_\n\n【26】After June 1988, 10 mg of tamoxifen was given orally twice daily for five years after completion of both radiotherapy and chemotherapy to 7 patients in the radiotherapy-first group and 10 patients in the chemotherapy-first group, all of whom were postmenopausal and had estrogen-receptor–positive tumors.\n\n【27】### _Intervals between Treatments_\n\n【28】Ninety-five percent of the patients in the chemotherapy-first group and 80 percent in the radiotherapy-first group began treatment within six weeks of the last surgical procedure, as required. Patients assigned to radiotherapy first were to start chemotherapy two weeks after completing radiotherapy, and vice versa. In one patient in the radiotherapy-first group and two patients in the chemotherapy-first group, these treatments overlapped by one or two days. The median interval between the last breast surgery and the start of radiotherapy was 36 days (range, 14 to 234) in the radiotherapy-first group and 126 days (range, 98 to 185) in the chemotherapy-first group. This interval between surgery and radiotherapy was more than 112 days (16 weeks) in 1 percent of the radiotherapy-first group and 84 percent of the chemotherapy-first group. The median interval between the first breast excision and the start of chemotherapy was 119 days (range, 48 to 179) in the radiotherapy-first group and 52 days (range, 15 to 119) in the chemotherapy-first group.\n\n【29】Compliance with Protocol Guidelines\n-----------------------------------\n\n【30】In the radiotherapy-first group, one patient had had a previous cancer, one received nonprotocol chemotherapy, two refused their assigned sequence (one of these also received nonprotocol chemotherapy), and three declined to begin or complete chemotherapy. In the chemotherapy-first group, three patients had had previous cancers, one received nonprotocol chemotherapy and also underwent mastectomy electively without radiotherapy, and one declined to complete chemotherapy.\n\n【31】Statistical Analysis\n--------------------\n\n【32】Patients were registered centrally, and treatment was randomly assigned after stratification for the combination of menopausal status, presence or absence of the estrogen-receptor protein, and number of involved nodes (0, 1 to 3, or 4 or more). The number of participants was originally set at 200 patients (to be enrolled over four years), which was thought to balance the competing interests of practicality and study power. With the use of Pocock boundaries, a sequential analysis was expected to be able to detect a ratio of 1.67 or more for the medians of disease-free survival in the two groups (e.g., 4.8 vs. 8 years) with a two-sided test of significance of 0.05 and a power of 80 percent. When patients with histologically negative nodes were made eligible, the number of participants was increased to 230 to maintain the same power.\n\n【33】Overall outcome and patterns of cancer recurrence were analyzed according to the intention-to-treat principle; all enrolled patients were included, including ineligible patients and those who did not follow the protocol. Actuarial curves of the length of time until the first recurrence, the length of time until distant recurrence (whether before, simultaneously with, or after local or regional nodal recurrences), and survival time (the length of time until death from any cause) were calculated by the Kaplan–Meier method. Neither the development of a contralateral breast cancer (4 patients in the radiotherapy-first group and 10 patients in the chemotherapy-first group) nor the development of a primary cancer not in the breast (1 patient in each group) was scored as a distant recurrence, nor were data on such patients censored in calculations of the time until the first recurrence or distant recurrence. All times were calculated from the day of randomization. The median follow-up time for surviving patients was 58 months (range, 10 to 124). Four patients lost to follow-up without recurrence at 10 to 39 months were scored as being alive without recurrence.\n\n【34】The site of first recurrence was categorized as “local” (within the parenchyma or skin of the treated breast, with or without simultaneous regional or distant recurrence), “regional” (in the ipsilateral axillary, infraclavicular, or internal mammary lymph nodes without evidence of local recurrence, with or without simultaneous distant recurrence), or “distant” (at other sites, occurring without simultaneous local or regional recurrence). Local recurrence after distant recurrence developed in one patient in the radiotherapy-first group and in two patients in the chemotherapy-first group; these events were excluded from the analysis. Because of the problems of competing risks,  patterns of recurrence were described with the use of the crude five-year incidence rates of recurrence rather than actuarial statistics.\n\n【35】Table 2. Crude Five-Year Incidence of First Recurrence in the Two Treatment Groups, According to Site.\n\n【36】Statistical tests used to analyze treatment differences were the exact multinomial test for the site of the first recurrence and toxic effects, the Wilcoxon rank-sum test for dose, and the log-rank test for censored time variables. Polychotomous logistic regression  (a form of multivariate analysis that does not suffer from the problem of competing risks) was used to create two models to examine the way different variables affected the risk of recurrence at different sites during the five years after treatment (that is, whether a patient would be free of recurrence, or whether a local recurrence or a regional or distant recurrence would be the first to develop). Statistical significance for these models was tested by the Wald test. The first model (the “main effect” model) included the treatment-sequence assignment as one of the variables, as well as such factors as the status of the margin and nodes. The corresponding P values for the effects of each of these variables in the model are shown in Table 2 . The second model (the “sequence interaction” model) examined whether other variables affected the relation that was found between the pattern of recurrence and the treatment sequence in the first model. (For example, would both premenopausal and postmenopausal patients have more local recurrences when treated with chemotherapy first and more distant recurrences when treated with radiotherapy first? Or would menopausal status affect the patterns of recurrence in these two groups?) The P values for each variable in this model are listed in Table 2 .\n\n【37】Results\n-------\n\n【38】Outcome of Therapy\n------------------\n\n【39】Figure 1. Actuarial Curves for Rates of Recurrence-free Survival , Survival without Distant Recurrence , and Overall Survival , According to the Assigned Treatment Group.\n\n【40】CT denotes the chemotherapy-first group, and RT the radiotherapy-first group. The percentages shown are those at five years.\n\n【41】Treatment failed in 45 patients in the radiotherapy-first group and 34 patients in the chemotherapy-first group over the entire period of observation, from 2 to 118 months after the start of therapy (P = 0.17). The numbers of patients in whom distant recurrences appeared at any time during follow-up in the radiotherapy-first and chemotherapy-first groups were 42 and 28, respectively (P = 0.07), and there were 35 and 24 deaths, respectively (including 1 patient in the chemotherapy-first group who died of ovarian cancer without evidence of recurrent breast cancer) (P = 0.13). The five-year actuarial incidences of any recurrence, distant recurrence, and overall survival in the radiotherapy-first and chemotherapy-first groups were 38 percent and 31 percent (P = 0.17), 36 percent and 25 percent (P = 0.05), and 73 percent and 81 percent (P = 0.11), respectively . The hazard ratios (and their 95 percent confidence intervals) were 1.37 (0.88 to 2.14) for the time until the first recurrence in the radiotherapy-first group as compared with the chemotherapy-first group, 1.62 (1.01 to 2.62) for the time until distant metastases were found, and 1.52 (0.90 to 2.56) for survival time.\n\n【42】Recurrences\n-----------\n\n【43】First sites of recurrence for all the patients who could be evaluated five years after randomization are shown in Table 2 . There was an increased risk of local recurrence in the chemotherapy-first group (14 percent vs. 5 percent) and of distant recurrence in the radiotherapy-first group (32 percent vs. 20 percent). This difference between the two groups in the pattern of where disease first recurred within five years after treatment was of borderline statistical significance (P = 0.07, by the exact multinomial test). It is noteworthy that one of the local recurrences in the radiotherapy-first group happened 41 months after treatment began in one of the two patients who refused their assigned sequence and received the reverse; she had an interval of 140 days between surgery and radiotherapy. To compare the two groups in another manner, the relative risk for local or distant recurrence (as compared with no recurrence at all, or in comparison with one another) was calculated for each group. Then, the ratio of these relative risks was calculated by dividing the relative risk in the radiotherapy-first group by that in the chemotherapy-first group. The ratios of relative risks (and their 95 percent confidence intervals) were 0.39 (0.12 to 1.29) for having a local recurrence as compared with no recurrence, 1.66 (0.79 to 3.47) for having a distant recurrence as compared with no recurrence, and 0.23 (0.06 to 0.86) for having a local recurrence as compared with a distant one.\n\n【44】The first site of recurrence within five years after treatment was examined with regard to treatment group and certain features of the patients and the tumors . In the first polychotomous logistic model, in which we examined the way variables (including treatment assignment) affected the risks of recurrence at various sites within five years after treatment, sequence assignment (P = 0.05), invasion of the lymphatic vessels (P<0.001), and re-excision (P = 0.06) had effects that were statistically significant or nearly significant . In the second model, we examined the way other variables affected the relation between the pattern of recurrence and the treatment sequence that was found in the first model. Only the status of the tumor margin and the number of axillary lymph nodes with metastases significantly affected the result (P<0.001) . For example, in patients with negative tumor margins there was little difference in the risks of recurrence, regardless of sequence assignment, and in patients with close, positive, or unknown tumor margins, the higher incidence of local recurrence in the chemotherapy-first group and the higher incidence of distant recurrence in the radiotherapy-first group persisted. For patients with one to three positive nodes, the sequence assignment made little difference in the risk of recurrence, but for patients with either negative nodes or four or more positive nodes there was a higher local-recurrence rate in the chemotherapy-first group and a higher distant-recurrence rate in the radiotherapy-first group.\n\n【45】Chemotherapy\n------------\n\n【46】The median dose of chemotherapy delivered (as a percentage of the cumulative planned dose) was lower in the radiotherapy-first group than in the chemotherapy-first group for cyclophosphamide (81 percent vs. 88 percent, P = 0.01), doxorubicin (81 percent vs. 88 percent, P = 0.01), and methotrexate administered on day 15 (50 percent vs. 75 percent, P<0.001). There were no differences in the median dose of methotrexate administered on day 1 or fluorouracil (100 percent of each planned drug dose was delivered in each group). The median time required to complete chemotherapy was the same in both groups (84 days).\n\n【47】Side Effects of Therapy\n-----------------------\n\n【48】Table 3. Short-Term and Long-Term Toxic Effects of Chemotherapy and Radiotherapy.\n\n【49】The short-term and long-term toxic effects of either chemotherapy or radiotherapy (within six months of the completion of treatment) are listed in Table 3 . There were no deaths. Nadir granulocyte counts of less than 500 per cubic millimeter after chemotherapy were more common in patients in the radiotherapy-first group during the first and third cycles but not the second and fourth cycles, probably because of the dose-reduction rules we used. Fever and neutropenia requiring hospitalization and radiation pneumonitis were more common in patients in the radiotherapy-first group. Complication rates did not vary with the irradiation technique except among patients in the radiotherapy-first group, among whom there was a slightly but not statistically significantly increased risk of radiation pneumonitis for those receiving radiation directed to regional nodes and the breast as compared with those receiving radiation directed to the breast alone. No cardiac events or other long-term complications have occurred as of this writing. Cosmetic results were assessed in patients without recurrence who were seen in follow-up by a radiation oncologist at the Joint Center for Radiation Therapy between 18 and 30 months after treatment. An “excellent” cosmetic result was defined as the virtual absence of changes due to treatment.  Results were excellent in 67 percent of the 39 patients who could be evaluated in the radiotherapy-first group and 60 percent of the 38 patients who could be evaluated in the chemotherapy-first group. Only 5 percent of the patients had fair cosmetic results, and only 3 percent had poor results.\n\n【50】Discussion\n----------\n\n【51】This randomized study was designed to test the effect of the order in which chemotherapy and radiotherapy were given after conservative surgery and axillary dissection in patients with breast cancer. Local recurrence of cancer was more common when radiation therapy was given after the completion of chemotherapy, whereas systemic recurrence was more frequent when chemotherapy followed radiation therapy.\n\n【52】The rates of distant recurrence may have been higher among the patients treated with radiation therapy before chemotherapy than among those treated with the reverse sequence because of the longer interval between surgery and the start of chemotherapy and because of the lower drug doses in the former group. Retrospective analyses have disagreed about whether the interval between surgery and chemotherapy is important.  Randomized studies of the effect of starting multidrug chemotherapy within a few hours or days to roughly four weeks after surgery have not found significant differences in outcome.  However, the interval before starting “delayed” chemotherapy in our study (a median of 17 weeks) was markedly longer than in those studies. We do not know whether the small differences in delivered doses of drugs between the two groups in this trial account for the difference in rates of distant recurrence. In one randomized study, however, patients who received only one half of the “standard” cumulative drug doses had an increased risk of recurrence as compared with patients given the full dose. \n\n【53】The risk of local recurrence is probably related to the number of tumor cells present when radiation therapy is initiated. This risk in turn depends on the tumor burden after surgery and the length of time the tumor cells have to proliferate before radiation therapy. The extent of breast resection, the details of radiotherapy, and the status of the resection margins may also influence the risk of recurrence. These complexities of breast cancer may account for the lack of agreement on the effect of the interval between surgery and radiotherapy on rates of local recurrence. \n\n【54】Our results suggest that it is preferable to give 12 weeks of chemotherapy before irradiation, rather than radiotherapy first, to patients at substantial risk for systemic recurrence of cancer. However, this study has several limitations. The results of the subgroup analyses must be viewed with special caution. Although our results suggest that the effect of a delay in initiating chemotherapy may be greatest for patients with the highest risk of subclinical systemic disease (i.e., those with four or more positive nodes) and that a delay in initiating radiotherapy may be most detrimental to patients with close or positive margins of the resected tumor, the statistical power of our subgroup analyses is low. In addition, extrapolating the results of this trial to other regimens, particularly those with more prolonged intervals between surgery and radiotherapy (e.g., six months or more), may be misleading. Also, too few patients received tamoxifen (and only late in the study) to allow us to evaluate its effect.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ebd87922-815c-461c-9d49-f61db3cd670a", "title": "The Wrong Frame of Mind", "text": "【0】The Wrong Frame of Mind\nA 62-year-old woman with hypertension and depression presented to a urology clinic with a 15-month history of episodic dysuria, hematuria, and polyuria. She reported suprapubic discomfort but no flank pain, nausea, or early satiety.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b006bec2-0e2c-4fda-bee7-6c5a7fde8c3a", "title": "Carcinoma of the Corneoscleral Limbus in a Patient Treated with Cyclosporine after Heart Transplantation", "text": "【0】Carcinoma of the Corneoscleral Limbus in a Patient Treated with Cyclosporine after Heart Transplantation\nTo the Editor:\n--------------\n\n【1】It is well established that immunosuppression after organ transplantation carries an increased risk of certain malignant tumors. Of these tumors, those of the skin and lip and lymphomas are the most common.  We report a carcinoma of the corneoscleral limbus in a patient who was treated with cyclosporine after undergoing heart transplantation.\n\n【2】A 60-year-old man was referred because of a tumor of the corneoscleral limbus in his left eye. He had undergone a heart transplantation five years earlier. The patient had not previously had cancer or ocular disease, and he had not worn contact lenses. After undergoing transplantation, he had been treated with prednisone and cyclosporine. The dose of prednisone was tapered over a period of two years and then stopped. The blood cyclosporine level was usually in the therapeutic range. Gum hypertrophy required surgical resection 24 and 32 months after transplantation.\n\n【3】Figure 1. Slit-Lamp Photograph Showing a Tumor of the Corneoscleral Limbus in a 60-Year-Old Man Treated with Cyclosporine after Undergoing Heart Transplantation.\n\n【4】Unilateral ocular symptoms (irritation and tearing) developed 50 months after transplantation. A tumor of the corneoscleral limbus subsequently appeared in the patient's left eye . It was excised, and pathological examination showed a 5-mm pedunculated tumor. Normal squamous epithelium lined the peduncle, and epithelial proliferation with elongated ridges and high-grade dysplasia (carcinoma in situ) was prominent in the head of the tumor.\n\n【5】The role of treatment with cyclosporine in the development of this carcinoma is unknown. In addition to its effects on the immune system, cyclosporine may affect the eye directly. After oral administration, cyclosporine has been found in tears.  In addition, pharmacokinetic studies have shown that cyclosporine has a strong affinity for the cornea. \n\n【6】It is likely that cyclosporine had a major role in the development of this exceptional tumor.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5a719890-3179-4bc0-9cd4-f0fb842827d8", "title": "The Role of Irradiation in Food Safety", "text": "【0】The Role of Irradiation in Food Safety\nFoodborne disease leads to about 325,000 hospitalizations and 5000 deaths each year in the United States. The irradiation of food could sharply reduce the incidence of foodborne disease, but currently it is rarely performed. This article examines the reasons for the slow growth in the use of food irradiation. Like pasteurization, irradiation is an effective strategy to improve food safety.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9bb18341-40f6-4d22-ba30-0494705cf262", "title": "Therapy of Lupus Nephritis", "text": "【0】Therapy of Lupus Nephritis\nAbstract\n--------\n\n【1】We evaluated renal function in 107 patients with active lupus nephritis who participated in long-term randomized therapeutic trials (median follow-up, seven years). For patients taking oral prednisone alone, the probability of renal failure began to increase substantially after five years of observation. Renal function was better preserved in patients who received various cytotoxic-drug therapies, but the difference was statistically significant only for intravenous cyclophosphamide plus low-dose prednisone as compared with high-dose prednisone alone (P = 0.027). The advantage of treatment with intravenous cyclophosphamide over oral prednisone alone was particularly apparent in the high-risk subgroup of patients who had chronic histologic changes on renal biopsy at study entry. Patients treated with intravenous cyclophosphamide have not experienced hemorrhagic cystitis, cancer, or a disproportionate number of major infections. We conclude that, as compared with high-dose oral prednisone alone, treatment of lupus glomerulonephritis with intravenous cyclophosphamide reduces the risk of end-stage renal failure with few serious complications.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1d2c8f0d-a79a-46c4-8a42-00d2a5752191", "title": "Technology and Health Care", "text": "【0】Technology and Health Care\nAbstract\n\n【1】People's needs, not availability of technology, should determine policies and priorities for its application in health services. Technology can improve efficiency and assist in solution of problems, but cannot \"drive the system\" or cure all ills. Fragmentation of markets, lack of organized health-care systems, and absence of national policies and standards are serious deterrents to technologic innovation. To avoid costly failures, we should concentrate initially on applications that reinforce services of demonstrated efficacy and affect large numbers of patients and providers. Before widespread adoption, we must rigorously test and evaluate the cost effectiveness and acceptability of these applications. Areas offering the greatest immediate potential for technologic applications include management information systems and \"do-it-yourself\" tests and devices for ambulatory medical care, hospital-discharge abstract systems, automation of commonly performed procedures in clinical laboratories, emergency-care systems for defined populations, restorative devices, and new communications media for health education.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9927affb-d19f-4b07-9614-3c8ec442ad44", "title": "Allocating Organs to Cognitively Impaired Patients", "text": "【0】Allocating Organs to Cognitively Impaired Patients\n### Audio Interview\n\n【1】 Interview with Dr. Scott Halpern on whether cognitive function should be considered in decisions about organ allocation. \n\n【2】Should patients’ cognitive function be weighed in the allocation of organs? This debate raises important questions, from the ethics of making judgments regarding others’ quality of life to the science of cognitive impairment’s effect on post-transplantation survival.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d43b74df-cbee-44de-ab7a-f824e5401091", "title": "Case 9-2021: A 16-Year-Old Boy with Headache, Abdominal Pain, and Hypertension", "text": "【0】Case 9-2021: A 16-Year-Old Boy with Headache, Abdominal Pain, and Hypertension\nA 16-year-old boy presented with headache, nausea, vomiting, and abdominal pain. He had been injured 17 months earlier in an ATV accident and again 2 months later while playing soccer. Since then, episodic headache had occurred. On evaluation, the pulse was 160 beats per minute, and the blood pressure 239/162 mm Hg. A diagnosis was made.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0e027036-e9ee-43e6-83e5-51233fd77cd6", "title": "Schizophrenia", "text": "【0】Schizophrenia involves multiple neurochemical pathways and brain circuits. Treatment is directed at ameliorating acute psychosis and reducing relapses. This review also discusses the use of psychosocial interventions and the management of medication effects of acute and tardive movement disorders and metabolic syndrome.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b696f1e8-8d47-494c-aa2b-be7c68ee3ad2", "title": "Concordance for Islet Autoimmunity among Monozygotic Twins", "text": "【0】Concordance for Islet Autoimmunity among Monozygotic Twins\nTo the Editor:\n--------------\n\n【1】The risk of type 1 diabetes among the monozygotic twins of patients with type 1 diabetes is reported to be as low as 30%; this percentage is usually based on the ascertainment of data at a single time point.  Less is known about the cumulative incidence of islet antibodies and diabetes among twins followed over time.  We analyzed the long-term risk of islet autoimmunity and type 1 diabetes in a cohort of twins of patients with type 1A diabetes as defined by the American Diabetes Association criteria (95% of the twins with a prediabetic condition who were initially discordant for diabetes were islet autoantibody–positive before diabetes developed). Eighty-three monozygotic twins without diabetes were prospectively followed for antibody expression (i.e., GAD65, ICA512, insulin, and cytoplasmic islet-cell antibodies),  progression to diabetes, or both for up to 43.8 years (mean, 12.8; median, 8.5; range, 0 to 43.8). Twins were identified at the Joslin Diabetes Center and the Barbara Davis Center for Childhood Diabetes and through the Diabetes Prevention Trial–Type 1 Diabetes.\n\n【2】Figure 1. Progressive Development of Anti-Islet Autoimmunity and Diabetes.\n\n【3】Panel A shows the correlation between the age at the onset of type 1 diabetes in the probands and in their monozygotic twins. The line of identity shows the values if diabetes developed in a proband and his or her twin at the same time. In the nine monozygotic twin pairs with a twin in whom diabetes developed before 10 years of age, there was a close correlation in the age at onset (r=0.78, P<0.001), whereas if diabetes developed in the twin after 10 years of age, there was greater variability in the ages at onset between the twin pairs, with the age at onset differing by more than 30 years in many twin pairs (r=0.55, P=0.05). Panel B shows the cumulative incidence of diabetes among the probands and the cumulative incidence of diabetes and autoantibody positivity according to life-table analysis in the twins who were initially discordant for diabetes. Kaplan–Meier estimates of progression to diabetes and anti-islet autoimmunity according to age are shown. Survival analyses of progression to diabetes in patients (probands) and in their monozygotic twins who initially did not have diabetes are indicated by the red lines (P<0.001). Progression to anti-islet autoimmunity (i.e., development of positive anti-islet autoantibodies, diabetes, or both) in the monozygotic twins of patients is indicated by the blue line. The numbers of patients still followed at each time point are shown. I bars denote 95% confidence intervals.\n\n【4】As shown in Figure 1 , by 60 years of age, the cumulative incidence of diabetes among monozygotic twins who were initially discordant for diabetes was 65% (95% confidence interval \\[CI\\], 39 to 91), and persistent autoantibody positivity, type 1 diabetes, or both had developed in 78% (95% CI, 61 to 95). Among 32 autoantibody-positive monozygotic twins, the risk of diabetes was 89% (95% CI, 72 to 100) within 16 years after the first positive antibody test. Because we studied only monozygotic twins who were initially discordant for diabetes, and a subgroup of twins before 1992 was not assessed with more recently available biochemical autoantibody assays (GAD65 and ICA512), the rates of progression to autoantibody positivity shown in the figure are probably underestimates. \n\n【5】We conclude that with prospective long-term follow-up, both autoantibody positivity and diabetes frequently develop in monozygotic twins of patients with type 1 diabetes, even if the twins were initially discordant for diabetes. We believe that, at least in studies of autoimmune disorders, estimates of the concordance rate for diabetes among monozygotic twins should include long-term follow-up.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f97fb5ab-7f44-4e23-8c99-b34ec0bc0d65", "title": "Age as a Factor in the Distribution of Lower-Airway Conductance and in the Pathologic Anatomy of Obstructive Lung Disease", "text": "【0】Age as a Factor in the Distribution of Lower-Airway Conductance and in the Pathologic Anatomy of Obstructive Lung Disease\nAbstract\n--------\n\n【1】Measurements with a retrograde catheter technic showed little change with age in the conductance of airways central to those of the twelfth to the fifteenth generation. Conductance of peripheral airways, on the other hand, rose sharply at about five years of age. Morphologic measurements suggested that changes in the linear dimension of peripheral airways were responsible for the increase observed.\n\n【2】Lower-airway disease appeared to be a common factor in bronchiolitis, fibrocystic disease and bronchiectasis. The age of the patient may determine to a large extent the type of resulting anatomic disorder.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f9f209bb-c3f9-4d2f-8185-dd6313f0e5b3", "title": "Quality of Life with Defibrillator Therapy or Amiodarone in Heart Failure", "text": "【0】Quality of Life with Defibrillator Therapy or Amiodarone in Heart Failure\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Implantable cardioverter–defibrillator (ICD) therapy significantly prolongs life in patients at increased risk for sudden death from depressed left ventricular function. However, whether this increased longevity is accompanied by deterioration in the quality of life is unclear.\n\n【3】Methods\n-------\n\n【4】In a randomized trial, we compared ICD therapy or amiodarone with state-of-the-art medical therapy alone in 2521 patients who had stable heart failure with depressed left ventricular function. We prospectively measured quality of life at baseline and at months 3, 12, and 30; data collection was 93 to 98% complete. The Duke Activity Status Index (which measures cardiac physical functioning) and the Medical Outcomes Study 36-Item Short-Form Mental Health Inventory 5 (which measures psychological well-being) were prespecified primary outcomes. Multiple additional quality-of-life outcomes were also examined.\n\n【5】Results\n-------\n\n【6】Psychological well-being in the ICD group, as compared with medical therapy alone, was significantly improved at 3 months (P=0.01) and at 12 months (P=0.003) but not at 30 months. No clinically or statistically significant differences in physical functioning among the study groups were observed. Additional quality-of-life measures were improved in the ICD group at 3 months, 12 months, or both, but there was no significant difference at 30 months. ICD shocks in the month preceding a scheduled assessment were associated with a decreased quality of life in multiple domains. The use of amiodarone had no significant effects on the primary quality-of-life outcomes.\n\n【7】Conclusions\n-----------\n\n【8】In a large primary-prevention population with moderately symptomatic heart failure, single-lead ICD therapy was not associated with any detectable adverse quality-of-life effects during 30 months of follow-up.\n\n【9】Introduction\n------------\n\n【10】Implantable cardioverter–defibrillators (ICDs) significantly extend survival among patients who are at high risk for sudden death because of the severity of their underlying heart disease.  However, concern has emerged about the effects of ICD therapy on the quality of life. One concern is that the use of ICD therapy could trade a quick, relatively painless (albeit premature) death for a more unpleasant death from progressive deterioration of the underlying heart disease or a coexisting illness.  Furthermore, in some previous studies, receipt of multiple ICD shocks has been associated with a reduced quality of life, although the causality of this relationship is unclear.\n\n【11】To date, two secondary-prevention trials (in which an ICD was implanted after a life-threatening arrhythmia to prevent future events) and one primary-prevention trial (in which an ICD was implanted in patients who were at increased risk for life-threatening arrhythmia but had not had such an event) have reported quality-of-life outcomes.  Although data from these trials have not shown any consistent evidence of a worse quality of life with ICD therapy, the conclusions that could be derived from these studies were limited by methodologic problems and relatively short follow-up.\n\n【12】The issue of long-term quality of life is particularly important in primary prevention, in which the willingness to accept a potentially unpleasant therapy for an uncertain future benefit may be low. We therefore examined the effects of primary-prevention ICD therapy on health-related quality of life in the Sudden Cardiac Death in Heart Failure Trial (SCD-HeFT) .\n\n【13】Although amiodarone did not improve survival in our trial, findings from earlier studies suggested that it might improve functional status.  Thus, we also report on the quality-of-life outcomes from the comparison between the use of amiodarone and placebo in this trial.\n\n【14】Methods\n-------\n\n【15】Patient Population and Study Overview\n-------------------------------------\n\n【16】Between September 16, 1997, and July 18, 2001, SCD-HeFT enrolled 2521 patients who were at least 18 years of age and who had New York Heart Association chronic, stable class II or III congestive heart failure and a left ventricular ejection fraction of 35% or less. The study design, methods, and primary results of SCD-HeFT have been reported previously.  The cause of heart failure was ischemic in 52% of the patients and nonischemic in 48%. Ninety percent of the study patients were enrolled in the United States, with the remainder in Canada and New Zealand.\n\n【17】Patients were randomly assigned to receive state-of-the-art medical therapy plus amiodarone (Cordarone, Wyeth–Ayerst Pharmaceuticals), an amiodarone placebo, or a conservatively programmed, single-chamber ICD (Medtronic, model 7223). Therapy was initiated on an outpatient basis according to the protocol. After a median follow-up of 45.5 months (range, 24 to 72), ICD therapy was associated with a 23% reduction in mortality, as compared with medical therapy, whereas amiodarone therapy was not associated with a significant effect on mortality.\n\n【18】Quality-of-Life Study\n---------------------\n\n【19】The quality-of-life portion of the trial was funded by the National Heart, Lung, and Blood Institute. The authors designed the study, collected and analyzed the data, and prepared the manuscript. The corporate sponsors who contributed to the support of the trial (Wyeth–Ayerst Laboratories, Knoll Pharmaceuticals, and Medtronic) had no role in the design, analysis, or interpretation of the quality-of-life study.\n\n【20】All patients provided written informed consent, and the study was conducted in cooperation with the National Heart, Lung, and Blood Institute. The institutional review board or ethics committee at each site approved the study protocol.\n\n【21】### _Data-Collection Methods_\n\n【22】Quality of life was measured by means of structured interviews at baseline, at months 3 and 12, and at 30 months or at the end of the study follow-up. At each site, the study coordinator conducted interviews at the time of a scheduled clinic visit or by telephone if a clinic visit was missed. Specific training was provided to each site coordinator to ensure standardization of data collection. According to the protocol, baseline quality-of-life assessments were to be conducted after informed consent had been obtained and before randomization. Follow-up quality-of-life assessments were to be performed within 1 month before or after the scheduled contact. For patients who were too ill to complete the full questionnaire, had a language barrier, or were otherwise unable to participate in the full interview, a short proxy form was collected.\n\n【23】### _Quality-of-Life Measures_\n\n【24】Two measures were prespecified as primary outcomes for the quality-of-life portion of the trial: the Duke Activity Status Index (DASI), reflecting cardiac-specific physical functioning, and the Medical Outcomes Study 36-Item Short-Form (SF-36) Mental Health Inventory 5 (MHI-5), reflecting psychological well-being. DASI was constructed to be a questionnaire-based analogue of the maximal exercise stress test used for patients with cardiac disease; it is scored from 0 to 58, with higher scores indicating better function and a difference of 4 points or more considered to be clinically significant.  The MHI-5, used to assess psychological well-being, is scored from 0 to 100, with higher scores indicating better function.  A clinically significant difference in the MHI-5 has not been formally defined but can be approximated as one quarter of 1 SD (5 points in this study).\n\n【25】Other scales from the SF-36 were used to assess role functioning (both physical and emotional limitations), general health perceptions, bodily pain, social functioning, and vitality. Like the MHI-5, these scales are scored from 0 to 100, with higher scores indicating better function and with one quarter of 1 SD representing a reasonable indication of a clinically significant difference.\n\n【26】The quality-of-life interviews also collected information on total numbers of “bed days,” which were defined as the number of days out of the last 42 days in which a patient was at home in bed for all or most of the day for health reasons, and “disability days,” which were defined as the number of days out of the last 42 days (not counting “bed days”) in which the patient had to cut down on usual activities for health reasons. Additional questions were asked to determine whether a patient could currently drive a car (yes or no) and manage money independently (yes or no). Employment details were obtained with the use of an abbreviated series of questions adapted from the Bypass Angioplasty Revascularization Investigation Substudy on Economics and Quality of Life. \n\n【27】We assessed the effect of heart failure on the quality of life, using the Minnesota Living with Heart Failure scale.  This scale is scored from 0 to 105, with higher scores indicating worse function, and a clinically significant difference considered to be approximately 5 points. \n\n【28】The relative desirability (utility) of each patient's health status, on a scale from 0 (a state of health equivalent to being dead) to 1 (excellent health), was assessed with the time–trade-off technique.  Patients were asked to assume that they would have a life expectancy of 5 years in their current state of health and then were asked in a series of questions to decide how many of those 5 years they would be willing to trade for excellent health in the remaining time. As a second, more intuitive global measure, patients were also asked to rate their health on a scale of 0 to 100, with 100 indicating excellent health and 0 a state of health equivalent to being dead. A 5-point difference in this scale (one quarter of 1 SD) approximated clinical significance.\n\n【29】Statistical Analysis\n--------------------\n\n【30】We used means and standard deviations, medians and 25th to 75th percentiles, or both to describe the distributions of continuous variables. Percentages were used to describe categorical variables. Univariate comparisons were performed with the use of Pearson's chi-square test for categorical variables and the Wilcoxon rank-sum test for continuous variables. Each active treatment was compared pairwise with the placebo group.\n\n【31】Patients with an ICD who received a shock from the device within the month preceding a scheduled quality-of-life assessment were compared with patients in the ICD group who did not receive a shock during the same period. These comparisons were based on the Wilcoxon rank-sum test for changes in the scores from the most recent quality-of-life measurements before the shock occurred. This analysis was repeated with the use of 2-month and 12-month time frames.\n\n【32】The primary outcome of the trial — death from any cause — differed significantly between the ICD group and the placebo group. This mortality difference resulted in a nonrandom subgroup of survivors whose quality of life could be compared according to the assigned treatments. To account for this potential bias, we applied an estimator for the survival average causal effect as a sensitivity analysis.  These estimates are based on weighted averages of the observed quality-of-life data multiplied by survival estimates specific to the study group, with P values and 95% confidence intervals for the estimated survival average causal effect based on a nonparametric bootstrap procedure. \n\n【33】All reported P values are two-sided. No adjustments were made for multiple testing.\n\n【34】Results\n-------\n\n【35】Patients and Baseline Characteristics\n-------------------------------------\n\n【36】Figure 1. Enrollment and Completeness of Quality-of-Life (QOL) Data Collection. Table 1.  Table 1. Baseline Characteristics of the Patients.\n\n【37】Of 2521 patients who underwent randomization, 2479 (98%) completed quality-of-life questionnaires at baseline . The initial demographic and clinical characteristics of the patients were well balanced among the study groups .  At each follow-up interval, questionnaires were collected from 93 to 95% of eligible patients. Overall, from a total of 9171 expected contacts with patients, 8747 quality-of-life questionnaires (95%) were collected. Only 1.2% of patients declined to complete the questionnaires, and only 1.4% of forms were judged to be incomplete. In 69 of 6268 follow-up interviews (1.1%), proxy forms were substituted for the full questionnaire.\n\n【38】Quality-of-Life Outcomes\n------------------------\n\n【39】Table 2. Primary Quality-of-Life Measures. Figure 2.  Figure 2. Scores on the DASI and MHI-5 in the Three Study Groups.\n\n【40】Panel A shows scores for cardiac physical function on the Duke Activity Status Index (DASI) on a scale ranging from 0 to 58, with higher scores indicating better function. Panel B shows scores for psychological well-being on the Medical Outcomes Study 36-Item Short-Form Mental Health Inventory 5 (MHI-5) on a scale ranging from 0 to 100, with higher scores indicating better function. The top lines of the box plots represent 75th percentiles, the bottom lines represent 25th percentiles, horizontal lines within the boxes represent medians, and dots represent means. ICD denotes implantable cardioverter–defibrillator.\n\n【41】In unadjusted comparisons, cardiac-specific physical functioning on DASI did not differ significantly between the ICD group and the placebo group at baseline (median score of 23 in both groups, P=0.76) and at months 3, 12, and 30 (median score of 24 in both groups for all three intervals, P>0.10) . There were also no significant differences at any point between the amiodarone group and the placebo group on cardiac-specific physical functioning.\n\n【42】Psychological well-being did not differ significantly between the ICD group and the placebo group at baseline on MHI-5 (median scores of 76 in both groups, P=0.17) but was better in the ICD group than in the placebo group at 3 months (median scores, 80 and 76, respectively; P=0.01) and at 12 months (median scores, 80 and 76, respectively; P=0.003). At 30 months, there was no significant difference between the two study groups (median score of 76 in both groups, P=0.79) . In the comparison between the amiodarone and placebo groups, we did not observe significant differences at any point during follow-up on the basis of MHI-5.\n\n【43】For each of the six other SF-36 scales, at least one interval comparison (i.e., at 3 months, 12 months, or both) showed significantly better scores in the ICD group. However, values were clinically similar and did not differ significantly at baseline or at 30 months on any of these scales . As compared with patients in the placebo group, those in the amiodarone group had significantly higher scores on the SF-36 pain index at all four time points.\n\n【44】At baseline, patients reported a mean of approximately 2 bed days and 8 to 9 disability days during the preceding 42 days. In addition, 86% of patients were able to drive a car, 92% could manage their finances independently, and 27% were employed outside the home. We were not able to detect an effect of ICD therapy as compared with placebo on the number of bed days or disability days or on the proportion of patients who were able to drive a car, manage their finances, or maintain employment during the follow-up period.\n\n【45】The Minnesota Living with Heart Failure scores were similar at baseline in the ICD group and the placebo group (median scores, 41 and 43, respectively; P=0.77). Scores were generally better (i.e., lower) in the ICD group than in the placebo group at 3 months (median scores, 30 and 36, respectively; P=0.006), at 12 months (median scores, 32 and 36, respectively; P=0.07), and at 30 months (median scores, 32 and 36, respectively; P=0.05).\n\n【46】The time–trade-off utility measure averaged 0.80 at baseline in all three study groups; there was a significant improvement in the ICD group over the placebo group at 3 months but not at any of the other time points. On a scale of 0 (worst) to 100 (best), the patients in the ICD group rated their overall health more highly than did those in the placebo group at 3 months (median scores, 75 and 70, respectively; P=0.002) and at 12 months (median scores, 75 and 70, respectively; P=0.05), but there was no significant difference at 30 months (median score of 70 in both groups, P=0.18).\n\n【47】Effect of ICD Shocks on Quality of Life\n---------------------------------------\n\n【48】Figure 3. Effect of ICD Shocks on Patients' Quality of Life.\n\n【49】Patients in the implantable cardioverter–defibrillator (ICD) group who had received an ICD shock within 1 month before a scheduled quality-of-life follow-up assessment were compared with patients who had not received a shock. Changes in scores on the Medical Outcomes Study 36-Item Short-Form (SF-36) scale for patients who had received an ICD shock were calculated as the value after the shock was delivered minus the most recent value before the shock was delivered. Changes in scores for the comparison groups were the quality-of-life values at 3 months minus the values at baseline. The results were similar when other follow-up time points (i.e., 12 months and 30 months) were used to calculate the changes in scores in the no-shock subgroup. A positive change indicates better function.\n\n【50】In the ICD group, 49 patients received a shock within 1 month before a scheduled quality-of-life assessment. As compared with patients in the ICD group who did not receive a shock, the quality of life of patients in the month after a shock was characterized by a significant decrease in perceived general health, physical and emotional functioning, social functioning, and self-rated health (all comparisons unadjusted) .\n\n【51】For the 66 patients who had received a shock within 2 months before a scheduled quality-of-life assessment, the pattern was the same but with smaller differences. When we compared the quality of life of 100 surviving patients who had received an ICD shock at any time during the first year of the study with that of 638 patients who did not receive a shock, no significant differences were evident. In addition, the number of ICD discharges above an arbitrary number, ranging from 2 to 5 or more, did not have a significant effect on the subsequent quality of life.\n\n【52】Survival-Adjusted Analyses\n--------------------------\n\n【53】Table 3. Differences between Study Groups in Quality-of-Life Measures.\n\n【54】To account for the improved survival of the patients in the ICD group, we estimated the survival average causal effect for each quality-of-life variable. Overall, the results were not materially different from the unadjusted comparisons described above .\n\n【55】Discussion\n----------\n\n【56】In our trial, single-lead ICD therapy enhanced survival and did not detectably diminish health-related quality of life for up to 30 months in patients with stable but moderately symptomatic heart failure. Although ICD therapy for primary prevention of sudden death was not expected to improve the quality of life, the possibility of harm from either psychological or physical complications of the therapy was of concern. In our overall comparisons, we found no statistically or clinically significant evidence of either psychological or physical harm. In a double-blind comparison of amiodarone with placebo, we did not detect an effect of amiodarone on either of the two primary quality-of-life measures.\n\n【57】Among patients in the ICD group who had received an ICD shock within the month preceding a scheduled follow-up visit, as compared with patients who had not received a shock, the quality of life was diminished in multiple domains. An analysis that included patients who had received shocks within 2 months before a quality-of-life assessment showed the same trends but with reductions in the magnitude and statistical significance of the differences. These trends were eliminated altogether when the window between the ICD shock and subsequent assessment was extended to 1 year. Although it may be plausible to assume that this association was causal, our analyses did not have sufficient statistical power to examine the relative contributions of the ICD shocks and concomitant deterioration in clinical status to these observations.\n\n【58】Among other primary-prevention trials of ICD therapy, only the Coronary Artery Bypass Graft (CABG)-Patch trial has reported on quality-of-life outcomes.  At 6 months, the ICD group had significantly lower levels of psychological well-being than the control group. Furthermore, patients who had received at least one ICD shock had a reduced quality of life on several measures. Important differences between the CABG-Patch quality-of-life study and our study include the ICD technology (a large, bulky ICD in the CABG-Patch study vs. a small, low-profile ICD in our study), the method of ICD implantation (open-chest implantation with an abdominal pocket vs. outpatient transvenous implantation with a pectoral pocket), and the target population (patients referred for CABG who had an ejection fraction of ≤35% vs. those with stable heart failure and an ejection fraction of ≤35%, about 50% of whom had a nonischemic cause of heart failure).\n\n【59】Another large study of quality of life in patients who had undergone ICD therapy was the Antiarrhythmics Versus Implantable Defibrillators (AVID) trial, a secondary-prevention study that was stopped prematurely by the data and safety monitoring board because of the improved survival in the ICD group.  In that study, the ICD group and the antiarrhythmic-therapy group had similar changes in scores on both the physical and mental components of the SF-36 questionnaire. Among patients in the ICD group with complete data on ICD shocks and follow-up quality-of-life data, the occurrence of one or more ICD shocks was significantly associated with subsequent reductions in both physical functioning and psychological well-being. Major differences between the AVID trial and our study include differences in the study populations (patients with life-threatening arrhythmias in the AVID trial vs. those with stable heart failure in our study) and a significantly lower rate of quality-of-life data collection in the AVID trial (83% at baseline and 61% at 1 year). Nonetheless, a reasonable conclusion from the AVID analysis is that in the absence of administered shock therapy, ICDs were well tolerated and did not diminish the quality of life.\n\n【60】The Canadian Implantable Defibrillator Study (CIDS), another secondary-prevention ICD trial, obtained quality-of-life data at 6 and 12 months of follow-up.  Scores for emotional and physical health improved in the ICD group, as compared with the amiodarone group. Patients in the ICD group who received five or more shocks did not have improvement in these quality-of-life scales, as compared with patients who received fewer than five shocks. However, the quality-of-life scores for patients in the ICD group who received one to four shocks during follow-up did not differ significantly from those for patients who received no shocks.\n\n【61】All studies of the effects of ICD therapy on quality of life, including our study, are limited by an inability to mask the therapy. Thus, the perceived effects of ICDs that we show may reflect attitudes of the study doctors and nurses that were transmitted to the patients, as well as the beliefs and expectations of the patients themselves. Patients may view the ICD either as an electronic security blanket or as an unpredictable and uncontrollable source of physical and emotional discomfort. In this unblinded comparison, we observed small improvements in some domains of quality of life during the first year of follow-up in the ICD group. We have no direct means of determining whether these improvements reflect the effects of such biases.\n\n【62】Caution should be exercised in interpreting significant differences in quality-of-life measures among the study groups, given the numerous statistical tests performed in this study. P values that are shown were uncorrected for multiple comparisons. Since the quality of life was a secondary outcome, the study was not constructed to test formally for the noninferiority of ICD therapy with respect to these outcomes.\n\n【63】Our evaluation of the effects of ICD shocks on subsequent quality of life was limited by the lack of quality-of-life data linked to the delivery of ICD shocks (collection of such data was judged to be logistically infeasible) and by the relatively small number of patients with a quality-of-life assessment shortly after a shock episode. We did not have enough patients with multiple shock episodes during a 24-hour period (“ICD storm”) to determine the effects of that phenomenon on subsequent quality of life.\n\n【64】In conclusion, we evaluated the quality of life of patients with moderately symptomatic, stable heart failure. Random assignment to the ICD group was not associated with adverse effects on health-related quality of life during the first 30 months of follow-up.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "df896fea-1b2e-4272-8c59-6ab6c26ca759", "title": "Emergence of Multiply Resistant Pneumococci", "text": "【0】Emergence of Multiply Resistant Pneumococci\nAbstract\n--------\n\n【1】Multiple antimicrobial resistance in pneumococci was detected in Johannesburg in July, 1977, and prompted an investigation of the prevalence of resistant strains in two hospitals. Carriers of Types 6A and 19A penicillin-resistant pneumococci, resistant to antibiotic concentrations ranging between 0.12 and 4 μg per milliliter were found in 29 per cent of 543 pediatric patients and 2 per cent of 434 hospital staff members. Multiply resistant Type 19A strains, resistant to β-lactam antibiotics, erythromycin, clindamycin, tetracycline and chloramphenicol, were isolated from 128 carriers, and were responsible for bacteremia in four patients. Isolates from 40 other carriers were resistant to penicillin alone or to penicillin and chloramphenicol or to penicillin, chloramphenicol and tetracycline. Pneumococci can be screened for penicillin resistance with a modified Kirby-Bauer technic; the strains with zones of <35 mm around 6-μg penicillin disks or <25 mm around 5-μg methicillin disks should be tested for sensitivity to penicillin by measurements of minimum inhibitory concentration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "35cb5ebe-8532-4b2e-a370-de7e9a8da3f0", "title": "Echocardiographic Measurement of the Left Ventricular Outflow Gradient in Idiopathic Hypertrophic Subaortic Stenosis", "text": "【0】Echocardiographic Measurement of the Left Ventricular Outflow Gradient in Idiopathic Hypertrophic Subaortic Stenosis\nAbstract\n--------\n\n【1】The characteristic hemodynamic abnormality in idiopathic hypertrophic subaortic stenosis is a highly variable subaortic obstruction to left ventricular ejection. Previous studies have shown that during systole, the left ventricular outflow tract is narrowed by motion of the anterior mitral leaflet toward the ventricular septum. If this were responsible for the obstruction, we believed it would be possible to predict quantitatively the gradient in the disease from echocardiograms. We computed an obstruction index by dividing the duration of outflow narrowing by the mean septal-anterior mitral-leaflet distance and compared the result with the simultaneously measured peak left ventricular outflow pressure gradient in 56 beats from 11 patients with this form of subaortic stenosis. The correlation between the obstruction index and gradient was highly significant (gradient = 1.8 obstruction index - 35; r = 0.95). Echocardiography thus confirms that the pressure gradient in the disorder is caused by narrowing of the left ventricular outflow tract between the anterior mitral leaflet and ventricular septum, and the technic may be used to quantitate the gradient.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "7c272a0f-ea39-4b0c-9fef-9b399723bdaf", "title": "An Outbreak of Primary Pneumonic Tularemia on Martha's Vineyard", "text": "【0】An Outbreak of Primary Pneumonic Tularemia on Martha's Vineyard\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】In the summer of 2000, an outbreak of primary pneumonic tularemia occurred on Martha's Vineyard, Massachusetts. The only previously reported outbreak of pneumonic tularemia in the United States also occurred on the island in 1978.\n\n【3】Methods\n-------\n\n【4】We conducted a case–control study of adults with pneumonic tularemia and investigated the environment to identify risk factors for primary pneumonic tularemia. Patients with confirmed cases were residents of or visitors to Martha's Vineyard who had symptoms suggestive of primary pneumonic tularemia, were ill between May 15 and October 31, 2000, and had a positive laboratory test for tularemia. Controls were adults who had spent at least 15 days on Martha's Vineyard between May 15 and September 28, 2000.\n\n【5】Results\n-------\n\n【6】We identified 15 patients with tularemia; 11 of these cases were primary pneumonic tularemia. _Francisella tularensis_ type A was isolated from blood and lung tissue of the one man who died. Patients were more likely than controls to have used a lawn mower or brush cutter in the two weeks before the illness (odds ratio, 9.2; 95 percent confidence interval, 1.6 to 68.0) and during the summer (odds ratio, undefined; 95 percent confidence interval, 1.8 to ∞). Lawn mowing and brush cutting remained significant risk factors after adjustment for other potentially confounding variables. Only one patient reported being exposed to a rabbit while cutting brush. Of 40 trapped animals, 1 striped skunk and 1 Norway rat were seropositive for antibodies against _F. tularensis_ .\n\n【7】Conclusions\n-----------\n\n【8】Study of this outbreak of primary pneumonic tularemia implicates lawn mowing and brush cutting as risk factors for this infection.\n\n【9】Introduction\n------------\n\n【10】Tularemia is a bacterial zoonosis caused by the small, gram-negative coccobacillus _Francisella tularensis_ . The organism may be found in contaminated water or soil, infected ticks, wild and domestic animals, and decaying animal carcasses. Mammals can acquire the infection through arthropod bites, direct contact with infected tissues, inhalation, or ingestion; person-to-person transmission has not been documented.\n\n【11】After an incubation period of 3 to 5 days (range, 1 to 21), infection with _F. tularensis_ can result in various clinical presentations, depending on the route of inoculation, the dose of the inoculum, and the virulence of the organism. Primary pneumonic tularemia results from the inhalation of _F. tularensis;_ although uncommon, it is the most severe clinical form of tularemia, with a mortality rate as high as 60 percent in the absence of treatment.  The more virulent type A _F. tularensis_ can be differentiated from the milder type B on the basis of biochemical reactions, virulence, and epidemiologic features.  Infection with type B rarely results in death, as evidenced by the mild clinical illness of patients who have pneumonic tularemia in Europe, where only type B causes disease in humans. \n\n【12】Each year 100 to 200 cases of tularemia are reported in the United States. The disease occurs throughout the United States, but Arkansas, Missouri, and Oklahoma generally account for over half the cases.  Cottontail rabbits from Arkansas and Missouri were introduced to Cape Cod and Martha's Vineyard, an island off the coast of Massachusetts, by game clubs in the late 1930s, and the first locally acquired cases of tularemia were reported shortly thereafter.  Since 1975, Massachusetts has reported as many as seven cases of tularemia annually, with two or fewer cases reported on Martha's Vineyard except in 1978 and 2000. In 1978, the only previously reported cluster of primary pneumonic tularemia in the United States occurred on Martha's Vineyard, with seven cases among the residents of a single cottage. A definitive source was never identified; however, the investigators speculated that the inhabitants became infected after a wet dog aerosolized _F. tularensis_ by shaking itself inside the cottage. \n\n【13】In July 2000, five cases of primary pneumonic tularemia were reported on Martha's Vineyard, with dates of onset from May 30 to June 22. We began active surveillance for tularemia on Martha's Vineyard, and by late August, six additional cases were identified, including four cases of primary pneumonic tularemia. One patient cut brush over a rabbit, and six other patients mowed lawns or cut brush within one week before becoming ill. Therefore, we conducted case–control and environmental studies to test the hypothesis that mowing lawns and cutting brush were risk factors for primary pneumonic tularemia.\n\n【14】Methods\n-------\n\n【15】We identified cases of tularemia through active surveillance of health care providers on Martha's Vineyard. We also reviewed medical records with a discharge diagnosis of unspecified pneumonia ( _International Classification of Diseases, 9th Revision,_ _Clinical Modification,_ code 486) from the Martha's Vineyard, Falmouth, Cape Cod, and Nantucket Cottage hospitals. The Massachusetts Department of Public Health enhanced its passive surveillance for tularemia statewide.\n\n【16】Epidemiologic Investigation\n---------------------------\n\n【17】For surveillance purposes, a patient was defined as any resident of or visitor to Martha's Vineyard between May 15 and October 31, 2000, who had symptoms suggestive of tularemia (e.g., acute onset of fever with lymphadenopathy, malaise, skin ulcer, or cough) and a serum titer of anti– _F. tularensis_ antibody of at least  on an agglutination assay,  a positive result for _F. tularensis_ antigen on direct fluorescence antibody testing, or a positive culture for _F. tularensis_ .  For the case–control study, a patient was defined as any person 18 years of age or older who met the case definition and who had clinical features of primary pneumonia at presentation. Through random-digit telephone dialing to Martha's Vineyard residents, we enrolled 100 control subjects who were at least 18 years old and who had spent at least 15 days on Martha's Vineyard between May 15 and the time they were interviewed in September 2000.\n\n【18】We conducted interviews from September 5 to 28, 2000, using a questionnaire to obtain the following information for each subject: occupation, landscaping activities and exposure to landscaping products, exposure to animals and arthropods, recreational activities, average amount of time spent outdoors each day, and history of tularemia, smoking, asthma, and pneumonia. We obtained information about exposure to possible risk factors between May 15 and the time of the interview, and during the two weeks before illness in the case of the patients and the two weeks before the interview in the case of the controls. One patient became ill in October and was interviewed shortly thereafter.\n\n【19】A human-subjects coordinator for the Centers for Disease Control and Prevention reviewed the study plan and determined that the study represented a public health response that did not require additional ethics review. The study plan received expedited approval from the Massachusetts Department of Public Health Human Subjects Committee.\n\n【20】Environmental Investigation\n---------------------------\n\n【21】We visited the suspected site of exposure of each patient and the 1978 outbreak site. At three sites we recreated possible activities that led to exposure, such as mowing the lawn and cutting weeds (“weed whacking”), and collected air samples using personal air samplers attached to the lawn mower and to the person who used the mowing equipment. Investigators wore protective gear while performing these activities. Samples of grass clippings, water, and soil were also collected. Small mammals were trapped at five properties, and we obtained serum samples from all dogs that lived at suspected sites of exposure. Samples of mammal tissue were examined by direct-fluorescence antibody staining, and mammal serum was tested for antibodies with use of an agglutination assay. Samples of air, grass, water, soil, and animal tissue were cultured for _F. tularensis_ .\n\n【22】Statistical Analysis\n--------------------\n\n【23】We used Epi Info version 6  to calculate crude odds ratios and Cornfield's 95 percent confidence intervals for categorical variables. When the odds ratio was undefined, the lower confidence limit was calculated with the use of StatXact software.  Continuous variables were tested with use of the Mann–Whitney U test. Using SAS software,  we constructed a multivariate model with variables referring to the two weeks before illness or interview that were significantly associated with illness or that approached statistical significance on univariate analysis and were biologically plausible risk factors.\n\n【24】Results\n-------\n\n【25】Characteristics of the Patients\n-------------------------------\n\n【26】Figure 1. Cases of Primary Pneumonic Tularemia, Tularemia with No Localizing Signs, and Ulceroglandular Tularemia on Martha's Vineyard, May 21 through October 28, 2000, According to the Week of Onset of Illness.\n\n【27】We identified 15 patients with confirmed cases of tularemia: 11 patients had primary pneumonic tularemia, 2 had ulceroglandular disease (fever with skin ulcers and lymphadenopathy), and 2 had fever and malaise but no localizing signs . All but one of the patients were male; the median age was 43 years (range, 13 to 59). Examination of paired serum samples in eight patients showed that the serum titer increased by a factor of four, with at least one measurement of at least . Analysis of single serum samples in six patients showed a titer of at least . In addition, _F. tularensis_ was cultured from blood and lung tissue of a 43-year-old man who died of primary pneumonic tularemia. He delayed seeking medical care for his illness, which began within one week after he mowed a lawn. The culture isolates were identified as type A _F. tularensis_ biovar tularensis and had growth, biochemical, and antigenic profiles typical of those of other naturally occurring isolates of _F. tularensis_ .\n\n【28】The residences of the patients were located throughout Martha's Vineyard; however, all but one of the properties where patients had mowed the lawn or cut brush before becoming ill were along the southern coast of the island, as was the site of the 1978 outbreak. The patient who mowed the lawn at the property that was not on the southern coast had also mowed lawns at two properties along the southern coast. No patients were identified from Cape Cod or Nantucket.\n\n【29】Case–Control Study\n------------------\n\n【30】Table 1. Frequency of Selected Risk Factors for Primary Pneumonic Tularemia.\n\n【31】Ten patients met the case definition for the case–control study (1 of the 11 patients with primary pneumonic tularemia was 13 years old and was excluded because controls had to be 18 years old or older). Of 100 controls, 1 reported having had tularemia in the past and was excluded from the analysis. Table 1 shows the frequency of selected possible risk factors for primary pneumonic tularemia among the patients and controls. In contrast to the control subjects, all patients were male and half were professional landscapers (odds ratio, 32.0; 95 percent confidence interval, 4.6 to 257). In the two weeks before becoming ill (or being interviewed, in the case of controls), 80 percent of patients (8 of 10) had used a lawn mower or brush cutter, as compared with 30 percent of controls (odds ratio, 9.2; 95 percent confidence interval, 1.6 to 68.0). Of these eight, four patients mowed four or five days before they became ill, two were professional landscapers who mowed daily, and two had mowed sometime within the week before their illness. All patients had used a lawn mower or brush cutter at some time during the summer, as compared with 48 percent of the controls (odds ratio, undefined; 95 percent confidence interval, 1.8 to ∞).\n\n【32】Only one patient remembered being exposed to a dead rabbit, and the exposure occurred while he was cutting brush. Patients were more likely than controls to have worked with bark chips in the two weeks preceding illness (odds ratio, 8.1; 95 percent confidence interval, 1.2 to 53.7), although only three patients had worked with bark chips. Patients were significantly more likely than controls to have worked with a weed whacker (odds ratio, 6.2; 95 percent confidence interval, 1.3 to 33.6) or with lumber (odds ratio, 5.6; 95 percent confidence interval, 1.2 to 30.3) during the summer, but neither of these activities performed within the two weeks before illness was a significant risk factor for primary pneumonic tularemia. Both dog ownership (odds ratio, 5.0; 95 percent confidence interval, 0.9 to 36.6) and smoking (odds ratio, 4.5; 95 percent confidence interval, 1.0 to 20.9) were more frequently reported by patients than controls. There was no significant difference in the proportions of patients and controls who were exposed to rabbits or ticks. Patients spent significantly more time outdoors than did controls (mean, 8.4 vs. 5.2 hours per day; P=0.01).\n\n【33】Table 2. Logistic-Regression Model of Risk Factors for Primary Pneumonic Tularemia.\n\n【34】As Table 2 shows, after adjustment for the presence or absence of a recent history of working with bark chips, average time spent outdoors, smoking status, and dog-ownership status, patients with primary pneumonic tularemia were more likely than controls to have used a lawn mower or brush cutter in the two weeks before illness (odds ratio, 6.7; 95 percent confidence interval, 1.1 to 39.9). We also used backward, forward, and stepwise selection procedures to obtain a parsimonious model that considered the effects of all variables. This parsimonious model retained only two variables: lawn mowing (odds ratio, 9.1; 95 percent confidence interval, 1.7 to 47.6) and working with bark chips (odds ratio, 7.8; 95 percent confidence interval, 1.3 to 48.2).\n\n【35】Environmental Investigation\n---------------------------\n\n【36】Cultures of three lawn-mower filters, 15 samples of cut grass, 11 air samples, 3 samples of raw water, and 9 samples of soil and mulch were all negative for _F. tularensis_ .\n\n【37】We set traps for a total of 442 trap-nights, with a success rate of 9.3 percent. Of 40 animals trapped, 2 were seropositive for antibodies against _F. tularensis:_ a striped skunk ( _Mephitis mephitis_ ) that was trapped where the patient who died had mowed before becoming ill and a Norway rat ( _Rattus norvegicus_ ) that was trapped where another patient had mowed. Other species trapped were a house mouse ( _Mus musculus_ ), white-footed mice ( _Peromyscus leucopus_ ), an eastern cottontail rabbit ( _Sylvilagus floridanus_ ), and meadow voles ( _Microtus pennsylvanicus_ ). Direct fluorescence antibody tests and cultures of mammal tissues were negative. All five dogs that were tested were seronegative. Of seven animals trapped at the site of the 1978 outbreak, none were seropositive.\n\n【38】Discussion\n----------\n\n【39】In this outbreak of primary pneumonic tularemia, lawn mowing and brush cutting were risk factors. Clinicians should be aware that primary pneumonic tularemia can occur after activities that aerosolize the organism from the environment. This is particularly important, since these are such common outdoor activities; 30 percent of the control subjects in the study reported mowing or cutting brush in the two weeks before being interviewed, and 48 percent reported engaging in these activities at some time during the summer.\n\n【40】Primary pneumonic tularemia has been reported in persons who disturbed the carcasses of infected rabbits,  in European farmers who worked with contaminated hay,  in two boys who mowed over a rabbit,  and in a man who used a brush cutter to clear a lot where many rabbits lived.  Lawn mowing has been epidemiologically implicated in an outbreak of psittacosis, in which patients were no more likely than controls to keep, handle, or feed birds but were more likely to have mowed lawns.  The authors hypothesized that lawn mowing aerosolized _Chlamydia psittaci_ shed by sick birds. Rodents can excrete viable _F. tularensis_ in both urine and feces,  and we propose that on Martha's Vineyard, _F. tularensis_ was shed in animal excreta, persisted in the environment, and infected people after being mechanically aerosolized and inhaled. Although one patient did cut brush over a rabbit, this outbreak indicates that people can acquire primary pneumonic tularemia from mowing in the absence of any obvious exposure to infected animal tissue.\n\n【41】Exposure to cigarette smoke increases the risk and severity of a variety of pulmonary infections.  In our study, smoking was associated with a risk of illness on univariate analysis; however, this finding was not statistically significant after adjustment for other variables. This finding may have been due to the low statistical power of our analysis, related to the small number of patients. Similarly, although dog ownership was reported by 80 percent of patients, dog ownership was not significantly associated with illness after adjustment for other variables.\n\n【42】Although _F. tularensis_ does not form spores, it can survive in water, soil, and decaying animal carcasses.  The organism persists in water and mud for as long as 14 weeks,  in straw for 6 months,  and in oats for 4 months.  We were unsuccessful in culturing _F. tularensis_ from environmental samples, but the optimal methods of collecting and isolating the organism from grass and air have not been determined. In addition, we may have collected samples under environmental conditions that differed from those existing when the patients were exposed. We did identify a seropositive skunk and a seropositive rat on properties where patients had mowed the lawn before becoming ill. Natural infection with _F. tularensis_ has been reported in skunks and rats,  but whether either of these species contributes to the maintenance and transmission of _F. tularensis_ on Martha's Vineyard could not be determined. _Dermacentor variabilis,_ the American dog tick, is a recognized reservoir and vector of tularemia,  is present throughout Martha's Vineyard, and feeds on small mammals.  The contribution of ticks to environmental contamination with _F. tularensis_ is unclear.\n\n【43】The probable exposure sites of patients in this outbreak and in the 1978 outbreak were concentrated along the island's southern coast. Such variables as proximity to brackish ponds and the level of wind, humidity, and precipitation arising from the sea might influence the presence and persistence of _F. tularensis_ in the environment.\n\n【44】One possible means of preventing primary pneumonic tularemia would be to minimize exposure to aerosolized bacteria during landscaping activities by surveying the area for carcasses or excreta before mowing. The use of protective skirting and collection bags on lawn mowers might help reduce the operators' exposure to aerosols. Little is known about the degree of protection against aerosolized _F. tularensis_ afforded by respirators. Respirators certified by the National Institute for Occupational Safety and Health that have a rating of 95 remove 95 percent of particulate matter with a diameter of 0.3 μm from the air. _F. tularensis_ measures 0.2 μm by 0.2 to 0.7 μm,  but it might be aerosolized in larger water droplets or on particles of grass or soil. Suggestions for respiratory protection could be extrapolated from recommendations for other infectious agents including _Mycobacterium tuberculosis,_  Sin Nombre virus,  and _Histoplasma capsulatum_ . \n\n【45】There are several possible limitations to our study. Patients and controls may have had difficulty recalling exposures that occurred one to three months before the interview. We tried to reduce recall bias by comparing exposures that had occurred in the two weeks before the interview in the case of controls with exposures that had occurred in the two weeks before the onset of illness in the case of patients. Using a lawn mower or brush cutter was the only type of exposure significantly associated with illness both in the two-week period and at any time during the summer. Controls were not tested serologically, and some may have had an inapparent infection. However, this factor would result in nondifferential misclassification and would bias our results toward the null.\n\n【46】People who mow lawns or cut brush in areas where tularemia is endemic may be at increased risk for primary pneumonic tularemia. Health care providers should consider the possibility of tularemia in patients in whom fever or pneumonia develops after such activities in areas where the disease is endemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "38862e78-1d8d-47c7-9ebb-6a49dad541d3", "title": "Medical Mystery — The Answer", "text": "【0】Medical Mystery — The Answer\nTo the Editor:\n--------------\n\n【1】Figure 1. Abdominal Radiograph Showing Two Ossified Renal Transplants and Adjacent Surgical Clips.\n\n【2】The medical mystery in the July 25 issue  involved a 34-year-old woman who was receiving dialysis and who had had multiple admissions for uremic pericarditis. An abdominal radiograph  was obtained during an evaluation for abdominal pain. The uremic pericarditis was the probable cause of referred abdominal pain. The patient had received two renal transplants, 18 and 20 years earlier, and had undergone partial parathyroidectomy 4 years earlier for hyperparathyroidism. The abdominal image shows two ossified renal transplants and adjacent surgical clips. There are marked changes indicative of osteomalacia. No cause of the abdominal pain is evident on the abdominal image.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e25247c1-8bbb-4dce-9fb8-5785e041dddc", "title": "Murder or Mercy? Hurricane Katrina and the Need for Disaster Training", "text": "【0】Murder or Mercy? Hurricane Katrina and the Need for Disaster Training\nArticle\n-------\n\n【1】In July 2006, a New Orleans physician, Anna Pou, and two nurses, Lori Budo and Cheri Landry, were arrested and accused of the second-degree murder of four patients at Memorial Medical Center in 2005, 4 days after Hurricane Katrina. According to Charles Foti, Jr., the Louisiana attorney general, the patients, ranging from 61 to 90 years of age, had been injected with a combination of morphine and midazolam that had killed them. Some observers have noted that these drugs are commonly given to reduce pain and anxiety, arguing that their administration probably represented an attempt to calm seriously ill patients during a crisis. Others, believing that the drugs were purposely given in overdose, have defended the acts as euthanasia, intended to prevent needless suffering in patients who had no realistic chances of surviving in a stranded, incapacitated hospital. Many in New Orleans have attacked Foti, as a representative of a government that was ineffectual during the crisis, for placing blame on those who cared for vulnerable citizens after Katrina.\n\n【2】The patients had been on the seventh floor of Memorial Medical Center in a long-term care unit operated by LifeCare Hospitals, which rented the space from Memorial's owner, Tenet Healthcare. Approximately 250 patients were stranded at Memorial after Katrina, and at least 34 of them died; the 4 patients at the center of the controversy were among 24 who died on the LifeCare unit. These patients had chronic medical conditions rendering them nonambulatory and in need of supportive care. Before Katrina, however, none had been thought to be in imminent danger of death.\n\n【3】Whatever roles Pou, Budo, and Landry eventually are proved to have played, a key question is whether Memorial's staff members were prepared to make life-and-death decisions during a disaster. If not, what could have prepared them?\n\n【4】In 2001, when I arrived at Tulane University in New Orleans, I had to prove my proficiency in my medical subspecialties. I also had to undergo yearly training in handling workplace sexual harassment, compliance with the Health Insurance Portability and Accountability Act, and insurance regulations. To run laboratories, I had to prove my competence in experimentation with animals and the safe handling of hazardous material. And there was the occasional fire drill.\n\n【5】I was taken by surprise the next fall, when, because of the proximity of Hurricane Lili, Tulane University Hospital and Clinic (where I was chief of hematology and medical oncology) announced a code gray, a procedure for determining which personnel would be assigned which duties when a hurricane struck. I had received no previous information on code gray, and I became educated later only by talking to colleagues who had practical experience but no formal training. As it turned out, there was no real system for code-gray assignments. Unlike the military, which doesn't send soldiers into combat until they have been trained for specific battle conditions and have undergone psychological testing to ensure that they can handle the horrors they'll face, hospitals generally enlisted whichever doctors happened to be on duty during a potential hurricane strike.\n\n【6】On August 27, 2005, as Katrina bore down, my wife, a physician at the Medical Center of Louisiana at New Orleans (Charity), was assigned code-gray duty there. I volunteered to staff Tulane Hospital across the street, and I visited Charity daily and worked there after Tulane was evacuated.\n\n【7】Katrina's floodwaters crippled emergency power generators, transforming hospitals into dark, fetid, dangerous shells. Extremely high indoor temperatures killed some people. We were under tremendous strain: in addition to the dire medical circumstances of many of our patients, we confronted uncertainty about our own evacuation, exacerbated by the tensions of threatened violence by snipers and frazzled soldiers and guards. I saw some competent professionals reduced to utter incoherence and uselessness as the crisis unfolded. I saw others perform heroic deeds that surprised me. Clearly, a better personnel selection process was needed.\n\n【8】Colleen Lambert, a nurse at Memorial's bone marrow transplantation unit, described conditions at Memorial that were similar to those at Charity and Tulane: no power, scarce food and water, nonfunctional toilets, uncertain prospects for evacuation, chaotic communication, gaps in leadership, poor security, and threats of violence. Nevertheless, staff and family members at Memorial evinced heroism and altruism as they worked together to mitigate suffering and save lives.\n\n【9】“Patients were complaining some,” reported one of Lambert's colleagues, “but we never considered euthanasia.” This description was echoed by Peter DeBlieux, the physician who ran Charity's intensive care unit during the crisis. “Patients complained of being too hot,” he said. “It was fairly unpleasant,” but no one ever asked for euthanasia, nor did such considerations arise among staff members.\n\n【10】DeBlieux did rate patients according to their potential for survival. “Red meant critical care. Black meant moribund — comfort care only. We had rated a number of patients red, and treated them accordingly. We finally got them evacuated to the airport. I found out later that the airport personnel re-rated them black, and gave comfort care only. We were devastated.” The same patients, he explained, received different levels of care depending on the availability of resources at specific locations. “The airport was just too swamped, and they didn't have anything to spare,” he noted. Acknowledging that conditions at Memorial, where the heat was extreme, were worse than those at Charity, DeBlieux added, “Now you have to imagine what decisions might have been made at Memorial, with its specific conditions.”\n\n【11】Lambert agreed that “different conditions require different responses and decisions.” She also acknowledged that she “did hear rumors” 3 days after Katrina “that \\`they're talking about euthanizing patients,'” but that discussion apparently centered only on patients in the LifeCare facility. She said she was not entirely surprised, given what she had heard about conditions there.\n\n【12】What might lead a health care professional to consider euthanasia in such a situation? If a terrorist bombs a building and we identify trapped people who are doomed to die before they can be rescued, should we offer to kill them or oblige them if they ask us to do so? If professionals who do undertake euthanasia in such circumstances have had no training in coping with disaster, does that change their accountability? When is such killing murder, and when, if ever, is it medically justified, humane, or legal?\n\n【13】Answering these questions is hardly an academic exercise. Tsunamis, earthquakes, fires, urban warfare, terrorist attacks, and other calamities are present and evolving threats that can rattle unprepared responders to the core and expose complex ethical, moral, legal, and medical conundrums.\n\n【14】At Tulane, we faced our share of these difficulties after Katrina. I cared for a woman with severe graft-versus-host disease caused by a bone marrow transplant. Given her dismal prognosis, I omitted her from the priority evacuation list — predicting a much greater chance of survival for other patients, including a man with a life-threatening low platelet count due to idiopathic thrombocytopenic purpura. Blood was oozing from his gums, bowels, and skin, and I couldn't treat him because all our blood products had been destroyed by the heat. Although he was at great risk for bleeding into his brain, which could precipitate paralysis or death, I never considered ending his life. Would someone else in the same position have done so?\n\n【15】Despite chaotic communications, we had some sense of central command and control at Tulane and Charity, as well as a tenuous group decision-making process. I find it unimaginable that our group discussions would have led to a decision to euthanize anyone. If the allegations of euthanasia at Memorial are borne out, such behavior might be attributable to less effective group decision making, lack of a sense of central control, or individual actions that were contrary to group decisions, in addition to environmental or medical conditions that were judged not to be survivable, requests of patients, or criminal intent.\n\n【16】How can health care professionals educate themselves about these factors and about how best to act and cope during a disaster? I am no expert, but my experience suggests that our sense of teamwork at Tulane and Charity was vital to our success in coping with Katrina. Thus, in addition to the obvious medical issues, I would argue that disaster training must include attention to the organization of an effective administrative operation in a chaotic setting. Communications failures must be prevented with the installation of fail-safe hardware that can also be used to communicate with police, fire, government, and military personnel. First responders must understand something about armed conflict and how to deal with violence. Ethical decision making, professionalism, and personal integrity must be emphasized. It would also be helpful to have an understanding of the legal ramifications of the actions one might take in such a situation. Finally, we must prioritize training according to the probabilities of events and find ways to identify the persons who would be best suited to responding and those who ought to be assigned other duties.\n\n【17】A careful analysis of what happened in New Orleans hospitals after Katrina — and why — should inform our evolving concepts of how best to prepare first responders. Whatever the outcome of the investigation in the Memorial case, I hope that the patients' deaths will catalyze the needed plans for training health care providers to deliver competent care — and survive — during the inevitable disasters of the 21st century.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "fe6cb2fb-59cf-4fcc-bcdc-fe7cbdf4e2e0", "title": "Immunization against Lyme Disease", "text": "【0】Immunization against Lyme Disease\nTo the Editor\n-------------\n\n【1】The studies by Steere et al.  and Sigal et al.  (July 23 issue) suggest that the vaccine consisting of recombinant _Borrelia burgdorferi_ outer-surface protein A is well tolerated and efficacious and “provides an important new public health approach to the prevention of Lyme disease.”  The role of this vaccine in public health efforts to prevent Lyme disease needs further consideration.\n\n【2】Many vaccines benefit public health by providing widespread immunity against potentially severe or fatal, and often untreatable, diseases. Vaccine-induced herd immunity against contagious diseases reduces the risk among unvaccinated persons. Lyme disease is a noncontagious, treatable, and nonfatal disease. Because humans are dead-end hosts of _B. burgdorferi,_ vaccinating some humans will not decrease the risk of acquiring Lyme disease for others. Although the incidence of Lyme disease is highest in children,  the safety and efficacy of the vaccine in children have not been established. The need for and timing of booster doses are also unknown. Furthermore, whether vaccination is more cost effective than early diagnosis and treatment of Lyme disease has not been established. Finally, should vaccination against Lyme disease result in a complacent decrease in personal efforts to avoid tick bites, then vaccinated persons in areas in which the disease is endemic may be at increased risk for other tick-borne diseases, such as ehrlichiosis, babesiosis, Rocky Mountain spotted fever, and tularemia.\n\n【3】In the United States, Lyme disease is primarily transmitted in certain counties in northeastern, north central, and Pacific states.  Vaccination of persons with frequent or prolonged exposure to tick habitats in areas where the disease is endemic is likely to be an important preventive strategy. For those who have only brief or intermittent exposure to tick habitats in such areas, the public health benefits of vaccination, as compared with those of early recognition and treatment of Lyme disease, are not clear. For persons who live in areas in which the disease is not endemic and for those who travel to areas in which the disease is endemic but who are not exposed to tick habitats, vaccination would not seem to be beneficial.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f2040ef2-f515-41f2-ae24-7f2345072939", "title": "Gamma-Beta Thalassemia: A Cause of Hemolytic Disease of the Newborn", "text": "【0】Gamma-Beta Thalassemia: A Cause of Hemolytic Disease of the Newborn\nAbstract\n--------\n\n【1】A full-term female infant had hemolytic hypochromic anemia at birth. Blood morphology was consistent with thalassemia. Neither hemoglobin H nor hemoglobin Barts was detected. Studies of globin-chain synthesis in peripheral blood revealed a deficiency of synthesis of gamma and beta chains in relation to alpha-chain synthesis. As the infant matured, her peripheral smear morphology improved and became indistinguishable from that of her father and six other relatives who had beta thalassemia proved by measurement of globin-chain synthesis. Gamma-beta thalassemia is therefore an evanescent disease that may be severe during the normal period of dependence on gamma-chain synthesis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ce92c609-ce91-46eb-9227-1a0c6c5fdcce", "title": "Epithelial-Cell Renewal in Cultured Duodenal Biopsies in Celiac Sprue", "text": "【0】Epithelial-Cell Renewal in Cultured Duodenal Biopsies in Celiac Sprue\nAbstract\n--------\n\n【1】Proliferation and migration of duodenal epithelium was studied in cultured mucosal biopsies from three normal volunteers and from three patients with celiac sprue before and after treatment with a gluten-free diet. The number of epithelial cells incorporating thymidine-  H into nuclei from the culture medium was over twice as great in pretreatment celiac-sprue biopsies as in normal biopsies. This indicates enhanced proliferation in celiac sprue. Labeled cells migrated to the surface epithelium in 24-hour cultures from all untreated patients whereas labeled cells did not migrate beyond the lower third of the villi in the normal volunteers. Surface epithelial cells, which appear damaged in uncultured celiac-sprue biopsies, reverted toward normal after only 24 hours of culture in a gluten-free environment. After six to 15 weeks of gluten withdrawal, cell proliferation and migration in cultured celiac-sprue biopsies were intermediate between pretreatment and normal biopsies. Thus, short-term treatment induces partial reversion of the abnormal epithelial-cell proliferation that characterizes untreated celiac sprue.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5dc6e743-2ae5-44a5-a288-ad27915d5fc7", "title": "Tetanus-Toxoid Emergency Boosters — A Reappraisal", "text": "【0】Tetanus-Toxoid Emergency Boosters — A Reappraisal\nAbstract\n--------\n\n【1】Measurement of tetanus antibody titers in 143 pediatric patients receiving routine and emergency immunization according to current practice showed that none who had received four or more injections failed to maintain the accepted protective level of 0.01 AU per milliliter for any measured interval. Analysis of the data indicated that the interval of protection after four or more injections is greater than 12 years from the last injection, with a confidence level of 99.9 per cent. When there is a valid history of the routine schedule of immunization outlined, special tetanus boosters on admission to camps, schools and colleges and emergency injections at times of injury should be abandoned, to minimize toxoid reactions. The risk of contracting tetanus if these recommendations are followed is calculated to be so remote as to approximate zero.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "bb0cac06-7c47-44d1-886b-20890e1d5f03", "title": "Obesity and Mortality", "text": "【0】Obesity and Mortality\nTo the Editor:\n--------------\n\n【1】It has been suggested that the magnitude of the association between obesity and total mortality is decreasing over time because of improvements in medical care for conditions related to obesity.  This hypothesis was generated to explain higher observed relative risks of death from any cause associated with obesity in the first National Health and Nutrition Examination Survey (NHANES I) as compared with two later surveys, NHANES II and NHANES III.  Whether similar trends are detectable in other cohorts that span recent decades is not known.\n\n【2】We examined secular trends in the association between body-mass index (BMI, defined as the weight in kilograms divided by the square of the height in meters) and mortality in the prospective cohort of the Cancer Prevention Study II, begun in 1982 by the American Cancer Society.  We followed 317,875 men and women who were healthy and were nonsmokers at baseline, in 1982, when their average age was 56 years. During 20 years of follow-up, 69,229 subjects died. We stratified deaths and person-years of follow-up into three periods according to calendar year (1982 to 1991, 1992 to 1997, and 1998 to 2002) and estimated the association between BMI in 1982 and subsequent mortality separately for each calendar period.\n\n【3】Table 1. Relative Risks of Death from All Causes According to Body-Mass Index, 1982 to 2002.\n\n【4】Relative-risk estimates increased across the range of overweight (BMI, 25.0 to 29.9) and were substantially higher among persons who were obese (i.e., BMI ≥30.0) . There was no indication that the relative risks associated with overweight and obesity had lessened in more recent calendar periods. In fact, in the earliest period, 1982 to 1991, the relative risks for overweight and obesity were slightly lower, and the relative risks for leanness were slightly higher, than in later calendar periods, possibly because weight loss related to an undiagnosed illness would be most likely to influence the measure of BMI for deaths occurring within 10 years of baseline.\n\n【5】These secular trends were seen in both men and women and within the following strata of attained age: 30 to 64, 65 to 74, and 75 or more years. Both smoking and preexisting illness cause a lower BMI over a period of decades of the adult lifespan, and both predict increased mortality. To the extent that the problem of reverse causality cannot be entirely eliminated in the database under study, the adverse prospective effect of adiposity on mortality will be underestimated. When the potential for confounding by smoking and preexisting disease is eliminated, overweight and obesity are significant predictors of death from any cause during 20 years of follow-up and as recently as calendar year 2002. In this large cohort, there is no evidence that the magnitude of the association between obesity and mortality is decreasing over time.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9d5df011-d4d1-4252-98f6-28eb756dc904", "title": "All in a Day’s Work — Equity vs. Equality at a Public ICU in Brazil", "text": "【0】All in a Day’s Work — Equity vs. Equality at a Public ICU in Brazil\nEvery day, Brazilian intensivists face cruel choices about which patients will be given the care they need. Should a 32-year-old woman with respiratory failure receive extracorporeal membrane oxygenation? Or should the money be spent instead on 1250 doses of antibiotics?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "412bfff4-21bd-4014-9280-69d63aad3a39", "title": "A New Antibiotic and the Evolution of Resistance", "text": "【0】A New Antibiotic and the Evolution of Resistance\nThe isolation and cultivation of antibiotic-producing bacteria can yield a rich supply of bacteria that produce new antibiotics to which pathogens are sensitive. However, predicting the evolution of pathogen resistance to these antibiotics is tricky.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "09a4505c-de03-4809-97fa-8c91555c8322", "title": "Inheritance Pattern and Clinical Response to Splenectomy as a Reflection of Erythrocyte Spectrin Deficiency in Hereditary Spherocytosis", "text": "【0】Inheritance Pattern and Clinical Response to Splenectomy as a Reflection of Erythrocyte Spectrin Deficiency in Hereditary Spherocytosis\nAbstract\n--------\n\n【1】To determine how various inheritance patterns and responses to splenectomy relate to erythrocyte spectrin deficiencies in hereditary spherocytosis, we measured the spectrin content of erythrocytes by radioimmunoassay in 33 patients with this disease.\n\n【2】Patients with the dominant form of hereditary spherocytosis generally had mild anemia, with spectrin at 63 to 81 percent of normal levels. Patients with the nondominant form of the disease had anemia ranging from severe to mild, with corresponding spectrin levels of 30 to 74 percent; their siblings were affected similarly. Distantly related homozygotes had different clinical severities with correspondingly different spectrin levels. The parents and offspring of patients with the nondominant form were clinically normal but consistently had subtle erythrocyte abnormalities. Spectrin levels in all patients were inversely related to osmotic fragility (P<0.0001), and they were also correlated with the clinical response to splenectomy: patients with spectrin levels above 70 percent achieved normal blood counts, those with levels of 40 to 70 percent had compensated hemolysis, and those with levels below 40 percent improved but remained anemic (P<0.0001).\n\n【3】We conclude that the inheritance pattern and response to splenectomy in hereditary spherocytosis reflect erythrocyte spectrin deficiencies as determined by radioimmunoassay.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d39e844e-448e-4751-950c-3436ef70a794", "title": "A Controlled Trial of Cyclophosphamide in Rheumatoid Arthritis — Cooperating Clinics Committee of the American Rheumatism Association", "text": "【0】A Controlled Trial of Cyclophosphamide in Rheumatoid Arthritis — Cooperating Clinics Committee of the American Rheumatism Association\nAbstract\n--------\n\n【1】Cyclophosphamide was given for 32 weeks to 48 patients with severe active rheumatoid arthritis. Twenty received a high dose (up to 150 mg daily), and 28 a low dose (up to 15 mg daily).\n\n【2】In comparison to the low-dose group, the high-dose patients showed greater reduction in disease activity in five of the six measures evaluated, in the assessments of both physicians and patients, and in the summing of changes in individual patients. More of the high-dose patients had reductions of rheumatoid-factor titer and of serum immunoglobulin G. A unique reduction in hand-joint erosions was found on x-ray study in the high-dose group. The clinical response was not related to leukopenia or to specific white-cell depressions.\n\n【3】Untoward effects were observed in 90 per cent of the high-dose and 40 per cent of the low-dose groups. Herpes zoster, cystitis and major hair loss were virtually confined to the former.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ae86bc52-3641-4850-8073-13248eaebe46", "title": "Empagliflozin in Heart Failure with a Preserved Ejection Fraction", "text": "【0】Empagliflozin in Heart Failure with a Preserved Ejection Fraction\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Sodium–glucose cotransporter 2 inhibitors reduce the risk of hospitalization for heart failure in patients with heart failure and a reduced ejection fraction, but their effects in patients with heart failure and a preserved ejection fraction are uncertain.\n\n【3】Methods\n-------\n\n【4】In this double-blind trial, we randomly assigned 5988 patients with class II–IV heart failure and an ejection fraction of more than 40% to receive empagliflozin (10 mg once daily) or placebo, in addition to usual therapy. The primary outcome was a composite of cardiovascular death or hospitalization for heart failure.\n\n【5】Results\n-------\n\n【6】Over a median of 26.2 months, a primary outcome event occurred in 415 of 2997 patients (13.8%) in the empagliflozin group and in 511 of 2991 patients (17.1%) in the placebo group (hazard ratio, 0.79; 95% confidence interval \\[CI\\], 0.69 to 0.90; P<0.001). This effect was mainly related to a lower risk of hospitalization for heart failure in the empagliflozin group. The effects of empagliflozin appeared consistent in patients with or without diabetes. The total number of hospitalizations for heart failure was lower in the empagliflozin group than in the placebo group (407 with empagliflozin and 541 with placebo; hazard ratio, 0.73; 95% CI, 0.61 to 0.88; P<0.001). Uncomplicated genital and urinary tract infections and hypotension were reported more frequently with empagliflozin.\n\n【7】Conclusions\n-----------\n\n【8】Empagliflozin reduced the combined risk of cardiovascular death or hospitalization for heart failure in patients with heart failure and a preserved ejection fraction, regardless of the presence or absence of diabetes\\.\n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nEmpagliflozin in HF with Preserved Ejection Fraction  \n\n【11】Patients with heart failure present with either a reduced or a preserved ejection fraction. Whereas heart failure with a reduced ejection fraction can be treated with drugs that act to attenuate the overactivation of endogenous neurohormonal systems,  therapeutic options for patients with heart failure and a preserved ejection fraction are limited. Although some benefits have been reported with mineralocorticoid-receptor antagonists and neprilysin inhibitors, the magnitude of the effects has been modest and the benefits have been apparent only in subgroups of patients. \n\n【12】Sodium–glucose cotransporter 2 (SGLT2) inhibitors have been shown to reduce the development and progression of heart failure in patients with type 2 diabetes and in those with heart failure and a reduced ejection fraction.  However, the effect of these drugs in patients with heart failure and a preserved ejection fraction has not been well studied. Post hoc analyses of a large-scale trial of dapagliflozin in type 2 diabetes indicated that SGLT2 inhibition might not reduce the incidence of serious adverse heart failure outcomes in patients with heart failure and a preserved ejection fraction.  In contrast, benefits in such patients were reported in a trial with sotagliflozin, but the number of events was too small to allow for a reliable estimate of a treatment effect. \n\n【13】The Empagliflozin Outcome Trial in Patients with Chronic Heart Failure with Preserved Ejection Fraction (EMPEROR-Preserved) was carried out to evaluate the effects of SGLT2 inhibition with empagliflozin on major heart failure outcomes in patients with heart failure and a preserved ejection fraction.\n\n【14】Methods\n-------\n\n【15】Trial Design and Oversight\n--------------------------\n\n【16】EMPEROR-Preserved was a randomized, double-blind, parallel-group, placebo-controlled, event-driven trial. The ethics committee at each center approved the trial, and all patients provided written informed consent. The sponsors were Boehringer Ingelheim and Eli Lilly.\n\n【17】The executive committee, which included representatives of Boehringer Ingelheim, developed the protocol and statistical analysis plan, oversaw the recruitment of patients, and supervised the analysis of the data. An independent data monitoring committee reviewed the safety data and the results of an interim analysis according to prespecified stopping boundaries. A clinical events committee adjudicated outcomes in a blinded manner according to prespecified definitions. Boehringer Ingelheim was responsible for data collection and storage. An independent statistician replicated and verified the analyses. The academic members of the executive committee provided an independent interpretation of the results. The authors made the decision to submit the manuscript for publication, assume full responsibility for the accuracy and completeness of the data, and attest to the fidelity of the trial to the protocol .\n\n【18】Patients\n--------\n\n【19】The trial design has been previously described in detail.  Participants were men or women, 18 years of age or older, who had New York Heart Association functional class II–IV chronic heart failure and a left ventricular ejection fraction of more than 40%. The protocol required patients to have an N-terminal pro–B-type natriuretic peptide (NT-proBNP) level of more than 300 pg per milliliter or, for patients with atrial fibrillation at baseline, an NT-proBNP level of more than 900 pg per milliliter.\n\n【20】Patients were excluded if they had a disorder that could change their clinical course, independent of heart failure, or if they had any condition that might jeopardize patient safety or limit their participation in the trial. The key inclusion and exclusion criteria are listed in the Supplementary Appendix .\n\n【21】Trial Visits and Follow-Up\n--------------------------\n\n【22】After a screening period of 4 to 28 days, eligible patients were randomly assigned in a  ratio and in double-blind fashion to receive either placebo or empagliflozin, 10 mg per day, in addition to usual therapy. Randomization was performed with a permuted block design and was stratified by geographic region, diabetes status, estimated glomerular filtration rate (eGFR) of less than 60 ml per minute per 1.73 m  of body-surface area or 60 ml or more per minute per 1.73 m  , and left ventricular ejection fraction of less than 50% or 50% or more, all measured at screening. We anticipated that half the patients would not have diabetes at enrollment. After randomization, all appropriate treatments for heart failure or other medical conditions could be initiated or altered at the discretion of the clinician.\n\n【23】Patients were evaluated periodically at trial visits for symptoms, health status (assessed with the Kansas City Cardiomyopathy Questionnaire), and adverse events. Vital signs, body weight, glycated hemoglobin level, NT-proBNP level, eGFR, and uric acid level were also assessed. All patients who had undergone randomization were to be followed for the occurrence of prespecified outcomes for the entire duration of the trial, whether or not they were taking the study medications or adhering to protocol-specified procedures.\n\n【24】Primary and Secondary Outcomes\n------------------------------\n\n【25】The primary outcome and the first two secondary outcomes were included in a hierarchical testing procedure, as described in Statistical Analysis, below. The primary outcome was a composite of adjudicated cardiovascular death or hospitalization for heart failure, analyzed as the time to the first event. The first secondary outcome was the occurrence of all adjudicated hospitalizations for heart failure, including first and recurrent events. The second secondary outcome was the rate of decline in the eGFR during double-blind treatment. Additional prespecified outcomes outside the testing hierarchy are described and adjudicated outcome event definitions provided in the Supplementary Appendix .\n\n【26】Statistical Analysis\n--------------------\n\n【27】The primary outcome, the combined risk of cardiovascular death or hospitalization for heart failure, was assessed in a time-to-event analysis. According to the intention-to-treat principle, the primary analysis for all patients who underwent randomization includes information through the end of the planned treatment period. Differences between the placebo and empagliflozin groups for the primary outcome were assessed for statistical significance at an alpha level of 0.0497, adjusted for one interim analysis, with the use of a Cox proportional-hazards model, with adjustment for prespecified baseline covariates of age, sex, geographic region, diabetes status, left ventricular ejection fraction, and eGFR. The effect of empagliflozin on the individual components of the primary outcome was also analyzed. Treatment effects were expressed as hazard ratios with 95% confidence intervals.\n\n【28】If the between-group difference in results for the primary outcome was significant, the two key secondary outcomes were to be analyzed in a prespecified stepwise, hierarchical manner, to preserve the overall type I error rate. The first secondary outcome — total (first and recurrent) hospitalizations for heart failure — was evaluated (alpha level, 0.0497) with the use of a joint frailty model that included cardiovascular death as the source of informative censoring. The second secondary outcome — the slope of the change in eGFR — was analyzed on the basis of on-treatment data with a random-coefficient model that included age, eGFR, and left ventricular ejection fraction at baseline as linear covariates and sex, geographic region, baseline diabetes status, and baseline-by-time and treatment-by-time interactions as fixed effects; the model allows for randomly varying slope and intercept between patients. The analysis of eGFR was assigned an alpha level of 0.001. The remaining alpha after hierarchical testing was applied to an analysis of pooled patient-level data from the current trial and a concurrent trial in patients with a reduced ejection fraction, which specified serious adverse renal events as the primary outcome variable.  Safety analyses were based on data from patients who had received at least one dose of the trial medication. For all hazard ratios or treatment differences not included in the testing hierarchy, no adjustment has been made for multiple comparisons, so the intervals should not be used to infer definitive treatment effects.\n\n【29】No imputation was used for missing data. For time-to-event analyses, data were censored at the end of treatment for patients who completed the planned treatment period or at the last available follow-up visit for patients who discontinued treatment early. For the analysis of eGFR, all available on-treatment data for the change from baseline were used. We explored the proportional hazards assumption for the primary outcome by plotting the log of the negative log of the estimated survival function against the log of time by treatment group and checking for parallelism. In addition, an interaction of treatment with log of time was included in the Cox regression model for an exploratory analysis.\n\n【30】For this event-driven study, we determined that a target number of 841 adjudicated primary outcome events would provide 90% power to detect a hazard ratio of 0.8 for the primary outcome at a two-sided alpha level of 0.05. Assuming an annual 10% event rate in the placebo group, a recruitment period of 18 months, and a follow-up period of 20 months, we established a planned enrollment of 4126 patients, with the option of enrolling up to 6000 patients if the accumulation of primary outcome events was slower than expected. Accordingly, on the basis of monitoring of the primary outcome event rate during the trial, the number of patients who underwent randomization was increased to at least 5750, without any change in the target number of events.  The increase in sample size was made without any knowledge of unblinded trial data. Subsequently, the data monitoring committee carried out one prespecified interim efficacy analysis after the occurrence of approximately 500 primary outcome events, with the possibility of recommending early termination of the trial if the one-sided alpha level of approximately 0.001 for a benefit of empagliflozin was achieved for both the primary outcome and cardiovascular death alone. The committee did not recommend early termination.\n\n【31】Results\n-------\n\n【32】Patient Characteristics and Randomization\n-----------------------------------------\n\n【33】Table 1. Characteristics of the Patients at Baseline.\n\n【34】Between March 27, 2017, and April 13, 2020, a total of 11,583 patients were screened for eligibility, and 5988 patients were randomly assigned to receive either empagliflozin (2997 patients) or placebo (2991 patients) at 622 centers in 23 countries . The reasons for screening failure are described in Table S1. The characteristics of the patients at baseline were similar in the two treatment groups . Nearly half the patients had diabetes and half had an eGFR of less than 60 ml per minute per 1.73 m  . Two thirds of the patients had a left ventricular ejection fraction of 50% or more; the median left ventricular ejection fraction was 54%.\n\n【35】The final date of follow-up for data collection was April 26, 2021. The trial medication was stopped for reasons other than death in 696 patients (23.2%) receiving empagliflozin and in 699 patients (23.4%) receiving placebo; 10.6% of the patients discontinued treatment owing to an adverse event. A total of 17 patients (0.6%) in the empagliflozin group and 19 patients (0.6%) in the placebo group had unknown vital status at the end of the trial . The median duration of follow-up for the primary outcome was 26.2 months (interquartile range, 18.1 to 33.1).\n\n【36】Primary Outcome\n---------------\n\n【37】Figure 1. Primary Outcome, a Composite of Cardiovascular Death or Hospitalization for Heart Failure.\n\n【38】The estimated cumulative incidence of the primary outcome in the two groups is shown. The inset shows the same data on an expanded y axis.Table 2.  Table 2. Primary and Secondary Cardiovascular Outcomes.\n\n【39】A primary composite outcome event (death from cardiovascular causes or hospitalization for heart failure) occurred in 415 patients (13.8%) in the empagliflozin group and in 511 patients (17.1%) in the placebo group (6.9 vs. 8.7 events per 100 patient-years; hazard ratio, 0.79; 95% CI, 0.69 to 0.90; P<0.001) . During a median trial period of 26 months, the number of patients treated with empagliflozin needed to prevent one primary outcome event was 31 (95% CI, 20 to 69). The results for the assessment of the proportional hazards assumption are shown in the Supplementary Appendix .\n\n【40】Hospitalization for heart failure occurred in 259 patients (8.6%) in the empagliflozin group and in 352 patients (11.8%) in the placebo group (hazard ratio, 0.71; 95% CI, 0.60 to 0.83) . Death from cardiovascular causes occurred in 219 patients (7.3%) in the empagliflozin group and in 244 patients (8.2%) in the placebo group (hazard ratio, 0.91; 95% CI, 0.76 to 1.09) . The causes of death among patients in the two treatment groups are summarized in Table S3.\n\n【41】Figure 2. Primary Composite Outcome in Prespecified Subgroups.\n\n【42】Results for the primary outcome of the trial — a composite of cardiovascular death or hospitalization for heart failure — are shown according to subgroups that were prespecified in the protocol. Race was reported by the patient. The body-mass index (BMI) is the weight in kilograms divided by the square of the height in meters. New York Heart Association (NYHA) class II includes 4 patients with NYHA class I. Baseline uric acid was calculated separately for male and female patients. ACE denotes angiotensin-converting enzyme, ARB angiotensin-receptor blocker, ARNI angiotensin receptor–neprilysin inhibitor, CKD-EPI Chronic Kidney Disease Epidemiology Collaboration, GFR glomerular filtration rate, LVEF left ventricular ejection fraction, MRA mineralocorticoid receptor antagonist, and NT-proBNP N-terminal pro–B-type natriuretic peptide.\n\n【43】The effect of empagliflozin on the incidence of primary outcome events was generally consistent across prespecified subgroups, including patients with or without diabetes at baseline .\n\n【44】Secondary Outcomes and Other Prespecified Analyses\n--------------------------------------------------\n\n【45】Figure 3. Hospitalizations for Heart Failure.\n\n【46】The mean number of events per patient for the first secondary outcome (total \\[first and recurrent\\] hospitalizations for heart failure) in the two groups is shown.\n\n【47】The total number of hospitalizations for heart failure was lower with empagliflozin than with placebo (hazard ratio, 0.73; 95% CI, 0.61 to 0.88; P<0.001; Figure 3 and Table S4). The rate of decline in the eGFR was slower in the empagliflozin group than in the placebo group (–1.25 vs. –2.62 ml per minute per 1.73 m  per year; P<0.001) . A total of 422 patients (14.1%) in the empagliflozin group and 427 patients (14.3%) in the placebo group died from any cause (hazard ratio, 1.00; 95% CI, 0.87 to 1.15) . Outcomes outside the hierarchical testing procedure are shown in Table 2 . Table S5 shows changes in glycated hemoglobin level, hematocrit level, NT-proBNP, systolic blood pressure, body weight, and uric acid level from baseline to 52 weeks.\n\n【48】Safety\n------\n\n【49】Three patients (one in the empagliflozin group and two in the placebo group) did not receive the study medication and were excluded from the safety analyses. Serious adverse events occurred in 1436 patients (47.9%) in the empagliflozin group and in 1543 patients (51.6%) in the placebo group. Adverse events leading to discontinuation of treatment occurred in 571 patients (19.1%) in the empagliflozin group and in 551 patients (18.4%) in the placebo group. Specific adverse events are listed in Table S6. Uncomplicated genital and urinary tract infections and hypotension were more common in patients treated with empagliflozin.\n\n【50】Discussion\n----------\n\n【51】In patients with heart failure and a preserved ejection fraction, SGLT2 inhibition with empagliflozin led to a 21% lower relative risk in the composite of cardiovascular death or hospitalization for heart failure, which was mainly related to a 29% lower risk of hospitalization for heart failure with empagliflozin. The effects on the incidence of primary outcome events were generally seen consistently across all prespecified subgroups, including patients with or without diabetes.\n\n【52】Empagliflozin also led to a lower total number of hospitalizations for heart failure and a longer time to first hospitalization for heart failure. The pattern of benefits shown in Table 2 is similar to that reported with empagliflozin in a similarly designed parallel trial of patients with heart failure and a reduced ejection fraction (EMPEROR-Reduced),  which suggests that the effects of SGLT2 inhibition on heart failure events do not vary meaningfully with the heart failure phenotype.\n\n【53】The effects of empagliflozin in patients with heart failure and a preserved ejection fraction are consistent with findings in previous reports that SGLT2 inhibitors reduce the risk of hospitalization for heart failure in patients with type 2 diabetes.  However, in these earlier trials, most patients did not have heart failure at the time of enrollment. Post hoc characterization of the heart failure phenotype, either at the time of randomization or at the onset of a post-randomization heart failure event, suggested that patients with heart failure and a preserved ejection fraction might have benefited from treatment,  but these analyses had a small number of events and substantial missing data. The current analysis — based on a large number of adjudicated events — shows a meaningful benefit of empagliflozin on major heart failure outcomes in patients with heart failure and a preserved ejection fraction. In addition, we show that the favorable effects in this heart failure phenotype were also seen in patients without diabetes.\n\n【54】Previous large-scale trials of drug interventions in patients with heart failure and a preserved ejection fraction have failed to demonstrate unequivocal benefits of treatment on the primary heart failure outcome. Trials of candesartan, spironolactone, and sacubitril–valsartan reported effects on cardiovascular death and hospitalizations for heart failure that were modest in size (i.e., a 10 to 15% reduction in risk) and of borderline statistical significance.  Subgroup analyses suggested that any benefit may have been preferentially seen in patients with an ejection fraction of 40 to 49%,  but patients with such mid-range ejection fractions have clinical features that are often more akin to those of patients with heart failure and a reduced ejection fraction than to patients with a preserved ejection fraction.  On the basis of these prior observations, we prespecified ejection fraction values of 50% and 60% as relevant thresholds for our subgroup analyses. In contrast with findings in earlier trials of candesartan, spironolactone, and sacubitril–valsartan, the results favored empagliflozin, with hazard ratios less than 1 for the primary outcome in each of the ejection fraction subgroups. \n\n【55】Treatment with empagliflozin led to a lower incidence of hospitalization for heart failure, but it did not appear to affect the number of deaths from cardiovascular or other causes in the current trial. It is noteworthy that the percentage of patients who discontinued treatment for reasons other than death was 23% overall and was similar in the two treatment groups; this high rate of discontinuation may have driven the effect size toward the null hypothesis. Nevertheless, a similar dissociation between treatment effects on hospitalizations for heart failure and cardiovascular mortality was seen in a previous trial with sacubitril–valsartan, which was conducted in a similar patient population that was followed for a similar period of time. \n\n【56】Our findings show that empagliflozin reduced the risk of cardiovascular death or hospitalization for heart failure in patients with heart failure and a preserved ejection fraction. This benefit was consistent across prespecified ejection fraction subgroups and was seen in patients with or without diabetes.\n\n【57】Table 1. Characteristics of the Patients at Baseline. \n\n| Characteristic | Empagliflozin(N=2997) | Placebo(N=2991) |\n| --- | --- | --- |\n| Age — yr | 71.8±9.3 | 71.9±9.6 |\n| Female sex — no. (%) | 1338 (44.6) | 1338 (44.7) |\n| Race — no. (%)  |  |  |\n| White | 2286 (76.3) | 2256 (75.4) |\n| Black | 133 (4.4) | 125 (4.2) |\n| Asian | 413 (13.8) | 411 (13.7) |\n| Other or missing | 165 (5.5) | 199 (6.7) |\n| Geographic region — no. (%) |  |  |\n| North America | 360 (12.0) | 359 (12.0) |\n| Latin America | 758 (25.3) | 757 (25.3) |\n| Europe | 1346 (44.9) | 1343 (44.9) |\n| Asia | 343 (11.4) | 343 (11.5) |\n| Other | 190 (6.3) | 189 (6.3) |\n| NYHA functional classification — no. (%) |  |  |\n| Class I | 3 (0.1) | 1 (<0.1) |\n| Class II | 2432 (81.1) | 2451 (81.9) |\n| Class III | 552 (18.4) | 531 (17.8) |\n| Class IV | 10 (0.3) | 8 (0.3) |\n| Body-mass index  | 29.77±5.8 | 29.90±5.9 |\n| Heart rate — beats per minute | 70.4±12.0 | 70.3±11.80 |\n| Systolic blood pressure — mm Hg | 131.8±15.6 | 131.9±15.7 |\n| Left ventricular ejection fraction |  |  |\n| Mean left ventricular ejection fraction — % | 54.3±8.8 | 54.3±8.8 |\n| Left ventricular ejection fraction >40% to <50% — no. (%)  | 995 (33.2) | 988 (33.0) |\n| Left ventricular ejection fraction ≥50% to <60% — no. (%) | 1028 (34.3) | 1030 (34.4) |\n| Left ventricular ejection fraction ≥60% — no. (%) | 974 (32.5) | 973 (32.5) |\n| Median NT-proBNP (interquartile range) — pg/ml | 994 (501–1740) | 946 (498–1725) |\n| Heart failure category — no. (%) |  |  |\n| Ischemic | 1079 (36.0) | 1038 (34.7) |\n| Nonischemic | 1917 (64.0) | 1953 (65.3) |\n| Cardiovascular history — no. (%) |  |  |\n| Hospitalization for heart failure during previous 12 mo | 699 (23.3) | 670 (22.4) |\n| Atrial fibrillation | 1543 (51.5) | 1514 (50.6) |\n| Diabetes mellitus | 1466 (48.9) | 1472 (49.2) |\n| Hypertension | 2721 (90.8) | 2703 (90.4) |\n| Mean eGFR — ml/min/1.73 m 2 | 60.6±19.8 | 60.6±19.9 |\n| eGFR <60 ml/min/1.73 m 2 — no./total no. (%) | 1504/2997 (50.2) | 1484/2989 (49.6) |\n\n【59】 Plus–minus values are means ±SD. The abbreviation eGFR denotes estimated glomerular filtration rate, NT-proBNP N-terminal pro–B-type natriuretic peptide, and NYHA New York Heart Association.\n\n【60】 Race was reported by the patient; patients who identified with more than one race or with no race were classified as other.\n\n【61】 The body-mass index is the weight in kilograms divided by the square of the height in meters.\n\n【62】 Two patients with an ejection fraction of exactly 40% underwent randomization and were included in the analysis.\n\n【63】Table 2. Primary and Secondary Cardiovascular Outcomes. \n\n| Variable | Empagliflozin(N=2997) | Empagliflozin(N=2997) | Placebo(N=2991) | Placebo(N=2991) | Hazard Ratio or Difference (95% CI) | P Value |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  | events per 100 patient-yr |  | events per 100 patient-yr |  |  |\n| Primary composite outcome — no. (%) | 415 (13.8) | 6.9 | 511 (17.1) | 8.7 | 0.79 (0.69–0.90) | <0.001 |\n| Hospitalization for heart failure | 259 (8.6) | 4.3 | 352 (11.8) | 6.0 | 0.71 (0.60–0.83) |  |\n| Cardiovascular death | 219 (7.3) | 3.4 | 244 (8.2) | 3.8 | 0.91 (0.76–1.09) |  |\n| Secondary outcomes specified in hierarchical testing procedure |  |  |  |  |  |  |\n| Total no. of hospitalizations for heart failure | 407 | — | 541 | — | 0.73 (0.61–0.88) | <0.001 |\n| eGFR (CKD-EPI) mean slope change per year — ml/min/1.73 m 2  | −1.25±0.11 | — | −2.62±0.11 | — | 1.36 (1.06–1.66) | <0.001 |\n| Other prespecified analyses |  |  |  |  |  |  |\n| Change in KCCQ clinical summary score at 52 wk  | 4.51±0.31 | — | 3.18±0.31 | — | 1.32 (0.45–2.19) |  |\n| Total no. of hospitalizations for any cause | 2566 | — | 2769 | — | 0.93 (0.85–1.01) |  |\n| Composite renal outcome — no. (%) | 108 (3.6) | 2.1 | 112 (3.7) | 2.2 | 0.95 (0.73–1.24) |  |\n| Onset of new diabetes in patients with prediabetes — no. (%) | 120 (12.0) | 6.1 | 137 (14.0) | 7.4 | 0.84 (0.65–1.07) |  |\n| Death from any cause — no. (%) | 422 (14.1) | 6.6 | 427 (14.3) | 6.7 | 1.00 (0.87–1.15) |  |\n\n【65】 All treatment effects are shown as hazard ratios, except for the slope of the change in the eGFR and the Kansas City Cardiomyopathy Questionnaire (KCCQ) clinical summary score. For all hazard ratios or treatment differences without P values, no adjustment has been made for multiple comparisons, so the intervals should not be used to infer definitive treatment effects.\n\n【66】 The eGFR (Chronic Kidney Disease Epidemiology Collaboration \\[CKD-EPI\\] formula) slope is analyzed on the basis of on-treatment data, using a random intercept–random slope model including age, baseline eGFR, and baseline left ventricular ejection fraction as linear covariates and sex, geographic region, baseline diabetes status, and baseline-by-time and treatment-by-time interactions as fixed effects; the model allows for randomly varying slope and intercept between patients.\n\n【67】 Change from baseline in KCCQ clinical summary score (scores range from 0 to 100, with higher scores indicating fewer or less severe symptoms or physical limitations) was analyzed with a mixed model for repeated measures, including age, baseline eGFR (CKD-EPI formula based on creatinine), and baseline left ventricular ejection fraction as linear covariates and baseline score-by-visit, visit-by-treatment, sex, geographic region, last projected visit based on dates of randomization and trial closure, and baseline diabetes status as fixed effects. The analysis is based on on-treatment data. The number of patients with available measurements for the KCCQ at week 52 in the empagliflozin and placebo groups are 2333 and 2335, respectively.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b668b28c-4ec5-49a0-a659-8d7c6d1e0979", "title": "Firearm Injuries and Violence Prevention — The Potential Power of a Surgeon General’s Report", "text": "【0】Firearm Injuries and Violence Prevention — The Potential Power of a Surgeon General’s Report\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with Dr. John Maa on his proposal for a Surgeon General’s report on firearm injuries and violence that could change the U.S. debate about guns. \n\n【3】In the aftermath of the mass shooting at a social services center in San Bernardino, California, in 2015, President Barack Obama suggested that the relationship between firearm ownership and gun injuries might be as strong as the connection between cigarette smoking and lung cancer. The full extent of the burden of firearm injuries is incompletely understood because of historical restrictions on federal funding for research on firearm violence by the Centers for Disease Control and Prevention (CDC). But recent increases in the frequency and lethality of mass shootings in the United States — and the approximately 90 gun deaths that occur each day — argue for efforts to reframe the national debate about firearms as a public health issue.\n\n【4】The 5-year anniversary of the Newtown, Connecticut, school shooting arrived in the shadow of mass shootings in Las Vegas and Sutherland Springs, Texas, in 2017, and shootings in Parkland, Florida, and Santa Fe, Texas, served as additional reminders of the risks for children attending school. As the United States came to understand a different set of health hazards — those associated with cigarette smoking — and the burden of smoking-related diseases on the health care system, a major step was the 1964 Surgeon General’s report on smoking and health.  This publication was the first of several Surgeon General’s reports on tobacco control, and similar reports on AIDS, mental health, and substance use disorders have influenced national discussions. A powerful step now would be a Surgeon General’s report to fully characterize the complex problem of firearm injuries and violence in the United States and to sharpen efforts to identify new solutions by revealing how the country got to its current state.\n\n【5】Objective data on the history, epidemiology, health effects, and financial costs of firearm violence, as well as the factors that contribute to it, could inform this discussion by conveying the full scope of the problem. The nearly 20,000 gun suicides and estimated 760 gun deaths related to domestic violence in the United States each year would be worthy areas of focus for such a report, given the programs already supported by the CDC and other federal agencies that are aimed at preventing suicide and intimate-partner violence. A definitive statement could also summarize the overwhelming scientific evidence that having a firearm in the home increases the risk of suicide. The report could serve as an urgent call to action for professional medical organizations and federal authorities. International leaders could help by describing the changes in both gun laws and social norms that have reduced firearm violence in their countries.\n\n【6】A study of World Health Organization (WHO) mortality data found that Americans are 25 times more likely to be victims of a gun-related murder and 8 times more likely to die by firearm suicide than people in other developed countries.  Japan, on the other hand, has among the lowest per capita rates of firearm ownership and gun murders and has the highest life expectancy in the world, as ranked by the WHO in 2015. The U.S. health care system is often blamed for the country’s ranking as 31st worldwide in life expectancy. But the complex and incompletely understood problem of firearm violence cuts across legal, political, educational, and financial systems. The new Surgeon General’s report could begin to tease apart entangled issues in these systems. The consequence of such system failures is enormous: beyond the deaths caused by gun violence, survivors often have lifelong physical and psychological problems, including disability, depression, and substance abuse.\n\n【7】The increasing burden from mass-casualty incidents on the country’s emergency departments, health care system, and law-enforcement agencies has highlighted the urgent need for action. After the mass shooting in Las Vegas, patient needs rapidly overwhelmed the capacity of emergency responders and paramedics. Miscommunications led to patients being taken to the closest hospitals, rather than to the trauma hospitals that were best equipped to treat them. But a surge of nearly 600 gunshot victims — many transported by private vehicles — is a nightmare that is nearly impossible for any institution or city to prepare for without the assistance of state or federal agencies. The coordinated and effective medical response to the November 2015 mass-casualty event in Paris can be partially attributed to a master plan developed 20 years earlier but never activated until that day.  In the United States, a joint federal and state collaboration spearheaded by the Department of Health and Human Services could coordinate countywide emergency responses to mass shootings. Strengthening the Bureau of Alcohol, Tobacco, Firearms, and Explosives and potentially dividing it into two separate agencies could also help address gun-related threats.\n\n【8】As we further elucidate the problem of firearm violence, new solutions may present themselves, including ones that involve the legal system. The National Rifle Association and gun-rights advocates have used litigation (such as _District of Columbia v. Heller_ ) to strike down existing gun-control laws and protect what they see as their Second Amendment rights. The challenge for firearm-safety advocates is to develop an equally effective legal strategy to protect public health. Wide variability in state laws related to firearm ownership complicates this mission. California implemented an assault-weapons ban in 1989, and Governor Jerry Brown signed the first “gun violence restraining order” in 2014 to allow family members and law-enforcement officers to petition the court to disarm a person who makes threats of firearm violence.  The Surgeon General’s report could catalogue these and other legislative efforts and help standardize firearm laws throughout the country.\n\n【9】The report might also stimulate new ways of thinking, shifts in societal norms, and development of new social programs related to firearm safety. The person behind the Sutherland Springs shooting had served time in prison for domestic violence and escaped from a mental health facility but was still able to acquire firearms. The Parkland shooting occurred despite repeated notifications to the Federal Bureau of Investigation and law enforcement about the threat posed by a student who had stated his violent intent on social media. Breakdowns in communication, straw purchases (buying a gun for another person who may be prohibited from purchasing one), the portrayal of gun violence in movies, limitations of background checks, lost and stolen firearms, and fragmented accountability in the chain of reporting of dangerous persons reflect larger societal challenges. A deeper understanding of the legal and administrative errors that result in firearms falling into the wrong hands could help move this discussion forward.\n\n【10】The United States has a long history of prioritizing the rights of gun owners over public safety. In 1992, after leaving the Office of the Surgeon General, C. Everett Koop wrote an editorial addressing violence as a public health issue.  He focused on firearm injuries and proposed that anyone owning or operating a firearm be required to meet specific criteria, such as being monitored in the firearm’s use. The recommendations were never implemented. During his Senate confirmation hearings to become Surgeon General in 2014, Vivek Murthy characterized the problems surrounding firearm violence in the United States as public health concerns. His support for gun control led to his appointment’s being delayed for several months. After Murthy’s confirmation, the political climate limited his office’s ability to champion firearm safety. A 2011 Florida law sought to prevent physicians from discussing firearm ownership with their patients; the ban was struck down by the 11th Circuit Court of Appeals in 2017.\n\n【11】In the aftermath of the Las Vegas and Parkland shootings, however, the tone of the conversation has changed. Perhaps the time has arrived to commission the first Surgeon General’s report on firearm injuries and violence prevention to stress the importance of collecting and disseminating data on the true nature of the public health problem we are facing. The United States could then begin using a public health approach to incorporate the principles of responsible and safe firearm ownership into the legal interpretation of the Second Amendment to ensure a safer future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9ca2beac-e2e8-4502-accd-59497475cd0e", "title": "The FDA and the Case of Ketek", "text": "【0】The FDA and the Case of Ketek\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with David Ross on the FDA review process for the antibiotic Ketek. \n\n【3】Three years ago, the Food and Drug Administration (FDA) approved the drug Ketek (telithromycin), lauding it as the first of a new class of antimicrobial agents that circumvent antibiotic resistance. Since then, Ketek has been linked to dozens of cases of severe liver injury, been the subject of a series of increasingly urgent safety warnings, and sparked two Congressional investigations of the FDA's acceptance of fraudulent safety data and inappropriate trial methods when it reviewed the drug for approval. As a former FDA physician who was involved in the Ketek review, I believe there are lessons to be learned from an examination of the events surrounding the approval of this product.\n\n【4】 Ketek Timeline.\n\n【5】Ketek is a ketolide antibiotic manufactured by Sanofi-Aventis and proposed for use in community-acquired respiratory tract infections. It was reviewed by the FDA three times . During the first round, reviewers identified substantial safety concerns, including multiple potential drug interactions, unique effects on visual acuity, and an apparent association with hepatocellular hepatitis, with pathological characteristics resembling those caused by drugs that have been withdrawn from the market because of hepatotoxicity. A federal advisory committee asked Sanofi-Aventis to obtain additional safety data by conducting a study involving patients who were likely to receive Ketek if the drug were approved.\n\n【6】In the second review, the FDA examined the results of such a study. Known as study 3014, it was an unblinded, randomized, controlled trial comparing the incidence rates of hepatic, cardiac, and visual adverse events in patients receiving Ketek and those receiving amoxicillin–clavulanate. Sanofi-Aventis recruited more than 1800 physicians to conduct the study, many of them new to clinical investigation, and paid them as much as $400 per patient enrolled, primarily to cover the costs of recruiting and gathering research data; more than 24,000 subjects were enrolled. The study was completed in 5 months and purported to show that Ketek was as safe as the other treatment.\n\n【7】A routine FDA inspection of the practices of the physician who enrolled the most patients — more than 400 — uncovered fraud, including complete fabrication of patient enrollment. The inspector notified FDA criminal investigators, and the physician is currently serving a 57-month sentence in federal prison for her actions. Inspections of nine other sites enrolling high numbers of patients revealed serious violations of trial conduct, raising substantial concerns about the overall integrity of the study. In the end, 4 of the 10 inspected sites were referred for criminal investigation.\n\n【8】Despite these discoveries, FDA managers presented study 3014 to the advisory committee in January 2003 without mentioning the issues of data integrity.  The managers have stated that they were legally barred from disclosing the problems to the committee because there was an open criminal investigation, but they have not explained why the data were presented at all, in view of the evidence of the study's lack of integrity. Unaware of the integrity problems, the committee voted 11 to 1 to recommend approval of Ketek.\n\n【9】The undisclosed problems with study 3014 led to a third review, during which FDA managers proposed using foreign postmarketing reports on Ketek as evidence of the product's safety, despite the unreliability of such data.  Although drug sponsors are required to submit such reports as part of an application, it is extremely unusual to use these data to address critical preapproval safety issues in place of a controlled study. The postmarketing data submitted by Sanofi-Aventis were reviewed by the FDA without any verification of their accuracy or completeness, even though 3 months before the third review, FDA criminal investigators recommended examining whether Sanofi-Aventis had been involved in systematic fraud in connection with Ketek. The FDA never conducted the recommended investigation or reviewed study 3014–related records showing that Sanofi-Aventis was aware of potential fraud in the study when it submitted the results to the FDA. The failure to look into or respond to concerns about integrity represented a marked deviation from FDA policies.\n\n【10】Against this backdrop of concerns about both safety and fraud, critical questions also arose about the efficacy of Ketek, which had been examined only in noninferiority trials. Such trials are not designed to demonstrate directly a new intervention's superiority to an active control or a placebo but instead involve the selection of a maximum margin by which the new intervention may be less effective than older interventions but still be considered better than placebo.  Throughout the 1990s, noninferiority trials had been standard procedure in the development of antimicrobial agents for the outpatient treatment of self-resolving respiratory tract infections. But by 2004, FDA workshops and advisory committee meetings on this topic had concluded that the use of noninferiority trials in this setting was not justifiable, since there is no evidence of a substantial treatment effect of antimicrobial drugs in self-resolving respiratory tract infections such as acute bacterial sinusitis and acute exacerbation of chronic bronchitis — the diseases for which clinicians most frequently prescribe antimicrobials, for which the market is largest, and for which treatment with Ketek was proposed.\n\n【11】Nevertheless, the FDA approved Ketek entirely on the basis of noninferiority trials. The reason given for the agency's continued acceptance of such trials in the study of antibiotics for self-resolving respiratory tract infections was the need to stand by prior agreements with industry sponsors regarding adequate trial designs — the Ketek trials, after all, had been designed and largely conducted before the adequacy of noninferiority trials had been called into question. Once it had been established that such trials could not demonstrate efficacy, however, it might reasonably have been argued that the welfare of prospective patients ought to outweigh any promise to manufacturers. Yet the FDA accepted the trials without discussion of either the patients who might be exposed to a drug that had serious toxic effects — and for which there was no evidence of effectiveness — or the failure of the trials to meet the FDA's own standards at the time of approval.\n\n【12】The review of Ketek was thus marked by pronounced departures from accepted review practices. In addition to the use of fraudulent data, the substitution of uncontrolled postmarket safety reports for controlled clinical trial data, and the acceptance of trials that could not show efficacy, there was also overt internal pressure brought to bear on FDA reviewers to alter their conclusions.\n\n【13】When the FDA approved Ketek on April 1, 2004, the approving officials stated in a memorandum that it was “difficult” to rely on study 3014 for approval  but revealed neither the fact that they had known for more than a year about serious problems that compromised the study nor the conclusion by FDA investigators that fraud and a failure of monitoring by Sanofi-Aventis made the study unusable. In this memo, the foreign postmarketing data were put forward as an acceptable substitute for an adequate and well-controlled trial, without any discussion of the lack of precedent for this approach or the unreliability of such data. Nor did the officials discuss the problems involved with relying on noninferiority trials for treatments of self-resolving infections, the conclusions of previous FDA meetings on this issue, or the applicable FDA standards that had been violated.\n\n【14】Sanofi-Aventis declared in advertisements that Ketek had the most successful launch of any antibiotic in history. In February 2005, 7 months after the drug was introduced to the U.S. market, the first death from Ketek-associated liver failure — in a patient treated for a mild respiratory tract infection — was reported to the FDA. The only formal response was an internal safety review written months later that devoted a few paragraphs to the event.\n\n【15】In January 2006, FDA management learned of the impending electronic report of a cluster of three cases of Ketek-associated acute liver failure at a single medical center, one of them the fatal case that had been reported almost a year earlier.  An emergency meeting of FDA senior managers resulted in a public announcement that the FDA regarded Ketek as safe; this announcement cited study 3014 as part of the evidence the FDA had relied on in approving the drug. References to this fraudulent study soon started to creep into the biomedical literature.\n\n【16】In February 2006, I and other reviewers alerted FDA senior management to the irregularities in the Ketek case. FDA management took no substantive actions. In an internal e-mail, one senior manager, though aware of the fraud in study 3014, defended the agency's citation of it, stating that the review division responsible for Ketek had used it. (Three days after a Congressional hearing on Ketek, in February 2007, the FDA finally removed any mention of study 3014 from its Web site.)\n\n【17】In the face of Congressional subpoenas and unfavorable publicity, reviewers at the FDA were warned at a June 2006 meeting by Andrew von Eschenbach, then the acting FDA commissioner, not to discuss Ketek outside the agency. By this time, 23 cases of acute severe liver injury and 12 cases of acute liver failure, 4 of them fatal, had been linked to Ketek. By the end of 2006, Ketek had been implicated in 53 cases of hepatotoxic effects. The FDA did not relabel Ketek to indicate its possible severe hepatotoxicity until 16 months after the first liver-failure cases became public. The withdrawal of approval for two indications, acute bacterial sinusitis and acute exacerbation of chronic bronchitis, for which Ketek's efficacy had never been demonstrated, did not occur until February 12, 2007 — only a day before the Congressional hearing on Ketek.\n\n【18】To date, the agency has not addressed the actions taken by FDA senior managers in dealing with Ketek, but the hearings recently convened by Congress suggest that it is ready to do so, as part of its efforts to resolve broader problems at the agency. If the case of Ketek leads to important reforms, then the drug may have done some good after all.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "50134a91-8896-4bfb-bf4e-fa61883942cc", "title": "Expression of Blood-Group Antigen A — A Favorable Prognostic Factor in Non-Small-Cell Lung Cancer", "text": "【0】Expression of Blood-Group Antigen A — A Favorable Prognostic Factor in Non-Small-Cell Lung Cancer\nAbstract\n--------\n\n【1】Background.\n-----------\n\n【2】New prognostic factors are needed to guide the treatment of patients with non-small-cell lung cancer. We evaluated the prognostic value of altered expression of ABH blood-group antigens, which has been implicated in the multistep process of carcinogenesis and tumor progression.\n\n【3】Methods.\n--------\n\n【4】The presence of blood-group antigens was assessed immunohistochemically in paraffin-embedded tumor samples from 164 patients who underwent curative surgery for non-small-cell lung cancer from 1980 through 1982. Monoclonal antibodies were used to detect the A and B antigens, and _Ulex europaeus_ agglutinin I to detect H antigen.\n\n【5】Results.\n--------\n\n【6】Survival of the 28 patients with blood type A or AB who had primary tumors negative for blood-group antigen A was significantly shorter than that of the 43 patients with antigen A—positive tumors (P<0.001) and of the 93 patients with blood type B or O (P = 0.002). The respective median survival times were 15, 71, and 39 months. Disease progressed significantly earlier in the 28 patients with tumors negative for blood-group antigen A than in the antigen A—positive patients (P<0.001). Expression of blood-group antigen B or H in tumor cells did not correlate with survival. Cox proportional-hazards regression analysis showed that expression of blood-group antigen A in tumor cells added significantly to the prediction of overall survival provided by other known prognostic factors among the patients with blood type A or AB (P = 0.004).\n\n【7】Conclusions.\n------------\n\n【8】Expression of blood-group antigen A in tumor cells is an important favorable prognostic factor in patients with non-small-cell lung cancer. This variable needs to be considered in the design of future trials of therapy. \n\n【9】Introduction\n------------\n\n【10】IN patients with non-small-cell lung cancer, the most important prognostic factor is tumor stage, and this factor largely determines treatment.    Surgery is the standard mode of treatment for patients with stage I and II tumors and for some patients with stage III tumors, with postoperative radiation therapy or chemotherapy (or both) added if the tumor invades the mediastinal lymph nodes. Even after apparently complete resection, however, five-year survival is still no more than 40 percent,  and the factors that account for disparate outcomes within the same stage are mostly unknown.\n\n【11】Recently, in examining the expression of epidermal growth factor receptor (EGFR) in paraffin-embedded samples of tissue with non-small-cell lung cancer, we found that immunostaining with the anti-EGFR monoclonal antibody  29.1 was associated with a favorable clinical outcome after surgical resection.  Because this antibody has been found to cross-react with an epitope for blood-group antigen A,  we expanded our study to determine whether the expression of blood-group antigens in tumor cells has the same prognostic importance in this disease.\n\n【12】ABH blood-group antigens are found in a variety of epithelial cells, as well as in erythrocytes; the antigenic determinants are carbohydrate side chains of glycoproteins and glycolipids. The expression of ABH blood-group antigens in relation to cancer development and cellular differentiation and maturation has been a subject of renewed interest.    Since the expression of blood-group antigens in tissue depends on blood type, we studied the survival of patients within the context of their ABO blood type. Our results indicate that expression of blood-group antigen A in tumor cells, but not of blood-group antigens B or H, is an important prognostic factor in patients with non-small-cell lung cancer who undergo surgery.\n\n【13】Methods\n-------\n\n【14】Patients\n--------\n\n【15】Our initial review included 253 patients who underwent surgery for lung cancer at the University of Texas M.D. Anderson Cancer Center from January 1980 through December 1982. This closing date provided a minimal follow-up period of 60 months for patients not lost to follow-up. We reviewed the medical records, original pathology reports, and histologic slides to identify patients with pathologically confirmed non-small-cell lung cancer of postsurgical pathological stages I to IIIb.  Clinical data, including ABO blood type as determined by the blood bank before surgery, were obtained from the medical records. The criteria for exclusion from the study were preoperative chemotherapy or radiotherapy, death within one month after surgery, and any intercurrent cancer. For 164 of the 175 patients who met the criteria for inclusion, tumor samples were available for immunostaining, and these samples were the focus of this study.\n\n【16】Histopathological Examination\n-----------------------------\n\n【17】All the histologic sections were reviewed by two of us. The histologic type of the tumor was determined in accordance with the criteria of the World Health Organization.  The other histologic measures assessed were the nuclear grade,  number of mitoses per 10 high-power fields, presence or absence of lymph-node metastasis, presence or absence of vascular invasion, and presence or absence of tumor in the margins of the resected tissue.\n\n【18】Immunohistochemical Staining\n----------------------------\n\n【19】For the immunohistochemical studies, paraffin-embedded tumor samples were selected to correspond with representative samples of the primary tumor. Samples of regional lymph nodes containing metastases were also retrieved, when available, for comparative study. Five-micrometer sections were deparaffinized with xylene and ethanol. Intrinsic peroxidase activity in tissue was blocked by treatment for five minutes with 3 percent hydrogen peroxide in methanol. Immunolocalization was carried out with the avidin–biotin–peroxidase enzyme complex Elite kit (Vector Laboratories, Burlingame, Calif.), according to the manufacturer's directions.\n\n【20】Staining in a section of tumor was considered positive if unequivocal staining of membranes or cytoplasm was seen in more than 5 percent of the tumor cells. When staining was confined to the nucleus, the section was considered negative. Erythrocytes and vascular endothelium in the same section were used as internal positive controls, and lymphocytes and fibroblasts as internal negative controls.\n\n【21】Monoclonal Antibodies\n---------------------\n\n【22】Mouse monoclonal antibodies to blood-group antigens A and B (Ortho Diagnostic Systems, Raritan, N.J.) were used in  dilution. Biotinylated _Ulex europaeus_ agglutinin I (UEA-I; Vector Laboratories) was used in  dilution to detect the blood-group H antigen.\n\n【23】Statistical Analysis\n--------------------\n\n【24】The Pearson chi-square test was used for contingency-table analyses.  Life-table probabilities of overall survival (i.e., survival without regard to cause of death) were calculated by the method of Kaplan and Meier,  and differences in survival between subgroups of patients were compared with the log-rank test.  Overall length of survival was measured from the day of surgery. In the analysis of time to tumor progression, patients who died of unknown causes were considered to have had tumor progression. A Cox proportional-hazards regression model  was used to determine the joint effect of several variables on survival.\n\n【25】Results\n-------\n\n【26】Characteristics of Patients\n---------------------------\n\n【27】Table 1. Characteristics of the Patients According to Blood Type.\n\n【28】No significant differences in clinical or pathological characteristics at the time of surgery were found when the patients were grouped according to blood type . A higher percentage of patients with type AB blood had lymph-node involvement. The distribution of blood types was not different from that in the general population. \n\n【29】Staining of Primary Tumor with Anti-A Antibody\n----------------------------------------------\n\n【30】Figure 1. Adenocarcinoma of the Lung from a Patient with Type AB Blood after Staining with Hematoxylin and Eosin , Anti-EGFR Antibody 29.1 (Positive, Panel B), Anti-A Antibody (Positive, Panel C), and Anti-B Antibody (Negative, Panel D) (Original Magnification, ×l00).\n\n【31】The anti-A antibody stained more tumor cells than the anti-EGFR antibody 29.1, and the anti-B antibody stained erythrocytes and endothelial cells but not tumor cells.\n\n【32】Forty-three of the 164 primary tumors stained positive with the anti-A antibody, all from patients with type A or AB blood. The staining pattern indicated that the antigen was found in both the cytoplasm and membranes . There was no aberrant expression of blood-group antigen A in the tumor cells from the patients with type B or type O blood.\n\n【33】Expression of Blood-Group Antigen A in Metastatic Tumors\n--------------------------------------------------------\n\n【34】Expression of blood-group antigen A was also assessed in metastatic tumors in regional lymph nodes from 27 patients with blood of type A or AB. Among 16 patients whose primary tumor stained positive with anti-A antibody, 4 patients had metastases that did not stain with the anti-A antibody. Among 11 patients whose primary tumor was negative for antigen A expression, none had metastases that stained with anti-A antibody.\n\n【35】Staining of Primary Tumor for Blood-Group Antigens B and H\n----------------------------------------------------------\n\n【36】Staining with the anti-B antibody was positive in the tumors from 8 of the 30 patients with blood type B or AB, and the tumors from 42 of the 73 patients with blood type O stained positive with UEA-I (anti-H). Among the patients with blood type AB, expression of antigen A was unrelated to that of antigen B, as shown in Figure 1 .\n\n【37】Survival According to Expression of Blood-Group Antigens\n--------------------------------------------------------\n\n【38】At the most recent follow-up, 61 patients were alive (median follow-up, 76 months; range, 2 to 105), and 103 were dead (median time to death, 20 months; range, 2 to 85). Since the expression of blood-group antigen A in tumor cells was confined to patients with type A or AB blood, the survival analysis according to expression of blood-group antigen A was stratified into blood groups A and AB, on the one hand, and groups B and O, on the other.\n\n【39】Figure 2. Kaplan–Meier Estimation of Overall Survival for All Patients, According to Antigen A Status in Tumor Cells.\n\n【40】The survival of the 28 patients with type A or AB blood who had a primary tumor negative for blood-group antigen A (dashed line) was significantly shorter than that of the 43 patients in the same blood group with a tumor positive for blood-group antigen A (solid line) (P<0.001) or the 93 patients with type B or O blood (all of whom were antigen A—negative) (dotted line) (P = 0.002). The horizontal dashed line represents the median probability of survival.\n\n【41】For the 28 patients with type A or AB blood who had primary tumors negative for blood-group antigen A, the median survival was 15 months. In contrast, the median survival was 71 months in the 43 patients with blood type A or AB who had expression of antigen A in their tumor cells (P<0.001). The estimated three- and five-year survival rates were 22 and 15 percent, respectively, for the patients negative for blood-group antigen A and 74 and 59 percent for the antigen A—positive group. In comparison, the median survival of the 93 patients with type B or O blood was 39 months, with three- and five-year survival rates of 51 and 38 percent, respectively . The difference in survival between the patients with type A or AB blood whose tumors were negative for blood-group antigen A and those with type B or O blood was significant (P = 0.002), as was the difference in survival between the patients with type A or AB blood whose tumors were positive for antigen A and those with type B or O blood (P = 0.033).\n\n【42】In contrast, the loss of blood-group antigen B or H in tumor cells was not associated with a disadvantage for survival either in the patients with type B or AB blood (P = 0.826) or in those with type O blood (P = 0.912).\n\n【43】Independent Predictive Value of Expression of Blood-Group Antigen A\n-------------------------------------------------------------------\n\n【44】Table 2. Outcome in Patients with Blood Type A or AB, According to Positivity or Negativity for Blood-Group Antigen A in Tumor Cells and Clinicopathological Characteristics.\n\n【45】Tumors negative for blood-group antigen A were significantly more likely than those positive for antigen A to have a high mitotic rate (P = 0.001), to be large (P = 0.036) and less differentiated (P = 0.026), and to be composed of large-cell carcinomas (P = 0.055) . There was no correlation between antigen A status and age, sex (data not shown), tumor stage, nodal status, presence or absence of vascular invasion (data not shown), or nuclear grade (data not shown).\n\n【46】When overall survival rates were compared by a log-rank test, the lack of expression of blood-group antigen A in tumor cells was associated with a worse prognosis among the patients with adenocarcinoma (P<0.001) and those with squamous-cell carcinoma (P<0.001), and in tumors of either size category . In the eight patients with large-cell carcinoma, the length of survival was indistinguishable from that in the patients with adenocarcinoma or squamous-cell carcinoma that was negative for blood-group antigen A.\n\n【47】Figure 3. Kaplan–Meier Estimation of Overall Survival According to Positive or Negative Antigen A Status in Tumor Cells and Disease Stage in Patients with Type A or AB Blood.\n\n【48】Panel A shows patients with Stage I disease, Panel B those with Stage II disease, and Panel C those with Stage III disease.\n\n【49】Of particular interest was the examination of whether status with respect to the expression of blood-group antigen A added predictive value to tumor stage, the most widely used prognostic factor in non-small-cell lung cancer. Among the patients with type A or AB blood who had stage I disease, survival in those with tumors negative for blood-group antigen A was far shorter ; the median survival was only 19 months. For the patients with tumors positive for antigen A, however, the median survival was not reached at the end of the study; their five-year survival rate was 76.5 percent (P<0.001). Even among the patients with stage II disease, survival in those with tumors negative for blood-group antigen A was shorter than in the patients with tumors positive for antigen A (P = 0.048), with a median survival of 11 months as compared with 53 months . In the patients with stage III disease, there was a trend toward the same survival advantage with tumors that expressed blood-group antigen A, but statistical significance was not reached (P = 0.158), perhaps because of the small sample .\n\n【50】Joint Effects of Variables\n--------------------------\n\n【51】Whether tumor expression of blood-group antigen A added significantly to the information provided by the other variables was explored with use of parsimonious models obtained by a backward-elimination procedure. Among the patients with type A or AB blood, univariate analysis demonstrated that age (P = 0.023), nodal status (P = 0.008), and mitotic rate (P = 0.072) were of significant prognostic value and that tumor-cell type, stage, tumor size, vascular invasion, degree of differentiation, and resection-margin status were not (P>0.22; maximal relative hazard for these variables, 1.82).\n\n【52】Removing age from the model that included age, nodal status, and mitotic rate did not render the model significantly worse according to the log-likelihood criterion (P = 0.144). After age was removed, however, neither nodal status nor mitotic rate could be eliminated at a 0.05 level of significance. This model was significantly improved by the addition of blood-group antigen A status in tumor cells (P = 0.004); that is, antigenic status added predictive value to that achieved with the other prognostic factors. However, although removing mitotic rate from the model including mitotic rate, nodal status, and antigenic status did not render the model significantly worse (P = 0.238), neither of the remaining two prognostic factors could be dropped.\n\n【53】When the proportional-hazards model for survival was fitted to the six groups obtained from the model (two groups for antigen A status times three groups for nodal status), we found three homogeneous risk groups. The low-risk group consisted of the 23 patients (32 percent) who had no nodal disease but had expression of blood-group antigen A in their tumor; we arbitrarily assigned this group a relative hazard of 1.0. The intermediate-risk group included the 10 patients (14 percent) with Nl staging and tumors positive for blood-group antigen A; their relative hazard was 3.47. The remaining 38 patients (54 percent) were at high risk of death, with relative hazards ranging from 7.24 to 8.59; that is, all the patients with tumors negative for blood-group antigen A were at increased risk, regardless of nodal status, as were all the patients with N2 disease, regardless of antigen A status.\n\n【54】Patterns of Disease Progression\n-------------------------------\n\n【55】Table 3. Patterns of Initial Progression of Disease, According to Antigen A Status in Tumor Cells in Patients with Type A or AB Blood.\n\n【56】To understand better the pathobiologic implications of the expression of blood-group antigen A in non-small-cell lung cancer, we analyzed the pattern of disease progression. Progressive disease, taken to include second primary tumors, occurred in 17 of the 43 patients with tumors positive for blood-group antigen A in tumor cells  after a median of 18 months (range, 2 to 83); in 14 of the 28 patients negative for blood-group antigen A who had type A or AB blood, progression occurred after a median of 6 months (range, 2 to 16). The difference between the times to progression of disease was significant (P<0.001) according to life-table analysis. The 43 patients with tumors positive for blood-group antigen A also had a significantly longer time to progression of disease than the 93 patients with type B or O blood (P = 0.026).\n\n【57】Distant metastasis was the most frequent type of tumor spread; its rate was higher in the patients negative for blood-group antigen A than in the antigen A—positive patients (12 of 28 vs. 10 of 43; P = 0.082). The brain was a common site of disease spread, but there was no difference with respect to blood-group antigen A status. However, the patients negative for antigen A had bone metastasis more often than those positive for antigen A (5 of 28 vs. 1 of 43; P = 0.033).\n\n【58】Discussion\n----------\n\n【59】We found that expression of blood-group antigen A in tumor cells is an important favorable prognostic factor for overall survival and the time to progression of disease in patients with non-small-cell lung cancer. Most important, perhaps, are the findings in the patients with stage I disease, in whom the loss of expression of blood-group antigen A in tumor cells was associated with shorter survival, of a length not significantly different from that in the patients with stage II or III disease . In addition, the retention of expression of blood-group antigen A in tumor cells conferred an improved prognosis, and the loss of such expression conferred a worsened prognosis for survival than that of the patients with type B or O blood. For the patients with type B or O blood, however, the expression of blood-group antigen B or H in tumor cells did not correlate with survival, suggesting that the important variable is something other than the simple retention or loss of blood-group antigen.\n\n【60】The data reported here suggest that loss of expression of blood-group antigen A is a possible marker for the multistage process of tumor progression in patients with type A or AB blood. First, expression of this antigen was lost more often in larger tumors (>3 cm) than in smaller ones. Second, expression of blood-group antigen A was lost more frequently in metastatic lesions in lymph nodes than in primary tumors. Third, loss of expression of the antigen was associated with a shorter time to the progression of disease. Along with this evidence, some other clinicopathological features (i.e., large-cell histology and bony metastasis) that were associated with the loss of expression of blood-group antigen A in our study have been previously found to predict decreased survival in patients with recurrent or metastatic non-small-cell lung cancer. \n\n【61】Altered expression of ABH blood-group antigens during malignant transformation and tumor progression has been previously reported.     In addition, loss of expression of blood-group A and B antigens occurs during wound healing.    For example, within hours after a wound, expression of blood-group antigen is lost from the epithelial cells adjacent to the wound margin, as well as from the epithelial cells migrating into the wound. Antigenic expression returns to normal after the restoration of epithelial continuity.  These findings are intriguing in the light of our finding that a loss of expression of blood-group antigen A appears to be closely related to the risk of metastasis. Indeed, it has been suggested that the loss of expression of blood-group ABH or Le <sup>a </sup> antigen is associated with more aggressive tumor behavior in cancers of the bladder,      uterine cervix,    colon,  and head and neck. \n\n【62】In lung cancers, using a specific red-cell-adherence technique, Davidsohn and Ni  found expression of ABH antigens in only 5 of 18 adenocarcinomas, in none of 86 other primary bronchogenic carcinomas, and in none of 145 metastases. When Hirohashi et al.  used an immunohistochemical technique (which is more sensitive than the specific red-cell-adherence technique), they detected appropriate expression of blood-group antigens in 34 of 42 adenocarcinomas of the lung. Most of these clinical studies, however, failed to show a specific association of expression of blood-group antigen A with improved clinical outcome, a result perhaps attributable to the lack of stratification according to blood group or, in some series, to an insufficient number of patients studied.   \n\n【63】These results suggest that expression of ABH blood-group antigens might be used to stratify patients in future clinical trials designed to evaluate the efficacy of postoperative adjuvant chemotherapy, particularly for patients with type A or AB blood. Given their relatively poor prognosis after surgery, such patients who have primary non-small-cell lung cancers that are negative for blood-group antigen A seem to merit aggressive and innovative treatment approaches, even those with early-stage tumors. If the primary tumor is found to be negative for blood-group antigen A before surgery, for example, primary induction chemotherapy and radiation therapy might be considered as an alternative to immediate surgery —an approach similar to that used in patients with advanced laryngeal cancer to preserve the natural voice.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "79f34d48-4c53-4205-9477-e1e9fddfd152", "title": "Gastroesophageal Reflux Disease", "text": "【0】Gastroesophageal Reflux Disease\nGERD may be diagnosed on the basis of symptoms and response to proton-pump inhibitors. Endoscopy is indicated in patients who have alarm symptoms or incomplete response to PPIs. Management includes lifestyle changes and medication.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5e9c4998-de19-46ea-8804-bef1817ce2cc", "title": "Antiretroviral Therapy during Pregnancy and the Risk of an Adverse Outcome", "text": "【0】Antiretroviral Therapy during Pregnancy and the Risk of an Adverse Outcome\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Some studies suggest that combination antiretroviral therapy in pregnant women with human immunodeficiency virus type 1 (HIV-1) infection increases the risk of premature birth and other adverse outcomes of pregnancy.\n\n【3】Methods\n-------\n\n【4】We studied pregnant women with HIV-1 infection who were enrolled in seven clinical studies and delivered their infants from 1990 through 1998. The cohort comprised 2123 women who received antiretroviral therapy during pregnancy (monotherapy in 1590, combination therapy without protease inhibitors in 396, and combination therapy with protease inhibitors in 137) and 1143 women who did not receive antiretroviral therapy.\n\n【5】Results\n-------\n\n【6】After standardization for the CD4+ cell count and use or nonuse of tobacco, alcohol, and illicit drugs, the rate of premature delivery (<37 weeks of gestation) was similar among the women who received antiretroviral therapy and those who did not (16 percent and 17 percent, respectively); the rate of low birth weight (<2500 g) was 16 percent among the infants born to both groups; and the rate of very low birth weight (<1500 g) was 2 percent for the group that received antiretroviral therapy and 1 percent for the group that did not. The rates of low Apgar scores (<7) and stillbirth were also similar or the same in the two groups. After adjustment for multiple risk factors, combination antiretroviral therapy was not associated with an increased risk of premature delivery as compared with monotherapy (odds ratio, 1.08; 95 percent confidence interval, 0.71 to 1.62) or delivery of an infant with low birth weight (odds ratio, 1.03; 95 percent confidence interval, 0.64 to 1.63). Seven of the women who received combination therapy with protease inhibitors (5 percent) had infants with very low birth weight, as compared with nine women who received combination therapy without protease inhibitors (2 percent) (adjusted odds ratio, 3.56; 95 percent confidence interval, 1.04 to 12.19).\n\n【7】Conclusions\n-----------\n\n【8】As compared with no antiretroviral therapy or monotherapy, combination therapy for HIV-1 infection in pregnant women is not associated with increased rates of premature delivery or with low birth weight, low Apgar scores, or stillbirth in their infants. The association between combination therapy with protease inhibitors and an increased risk of very low birth weight requires confirmation.\n\n【9】Introduction\n------------\n\n【10】Antiretroviral therapy is recommended during pregnancy to reduce the risk of perinatal transmission of human immunodeficiency virus type 1 (HIV-1) infection  and to improve maternal health.  Data on complications of pregnancy associated with monotherapy or combination therapy with antiretroviral agents are limited.\n\n【11】In 1998, a retrospective Swiss study of 30 women with HIV-1 who had received combination antiretroviral therapy during pregnancy (with protease inhibitors in 13 women and without protease inhibitors in 17) showed that such treatment was associated with a 33 percent risk of premature delivery.  Contemporaneously, the Pediatric AIDS Clinical Trials Group (PACTG) observed that in phase 1 trials of nelfinavir, ritonavir, or indinavir given in combination with zidovudine and lamivudine during pregnancy, 4 of 11 deliveries (36 percent) occurred before 37 weeks of gestation. We analyzed data from seven large studies involving pregnant women with HIV-1 infection, in order to assess the risk of premature delivery and other adverse outcomes of pregnancy associated with the use of antiretroviral agents.\n\n【12】Methods\n-------\n\n【13】Study Population\n----------------\n\n【14】We included women enrolled in two completed clinical trials (PACTG studies 076  and 185  ) and five ongoing, prospective, observational studies. The observational studies were two multisite studies (the Perinatal AIDS Collaborative Transmission Studies \\[PACTS\\]  and the Women and Infants Transmission Study \\[WITS\\]  ) and three single-site studies (at the University of Miami, the University of Southern California, and the University of California at Los Angeles \\[UCLA\\]). The enrollment criteria differed among the studies. PACTG studies 076 and 185 restricted enrollment according to the week of gestation (study 076, 14 to 34 weeks; study 185, 20 to 30 weeks) and the CD4 count (study 076, >200 per cubic millimeter; study 185, <500 per cubic millimeter). In PACTG study 076, previous antiretroviral therapy was a criterion for exclusion; in PACTG study 185, zidovudine use during the pregnancy was a criterion for inclusion. The single-site observational studies included all HIV-1–infected women who delivered at the site; the multisite studies included a subgroup of women who delivered at the sites.  Written informed consent was obtained from all women except for those at the University of Miami, where data were abstracted from clinical charts and informed consent at the time of the study was not required. The observational studies excluded women who were also enrolled in PACTG clinical trials of antiretroviral therapy.\n\n【15】The inclusion criteria for our study were documentation of the infant's gestational age at birth, delivery at 20 or more weeks of gestation, singleton gestation, documentation of at least one antenatal CD4+ cell count, and documentation of the use or nonuse of antiretroviral agents during pregnancy. We divided the women into two cohorts according to the date of delivery and the use or nonuse of antiretroviral therapy. Women who delivered between January 1, 1990, and February 28, 1994 (before the general use of zidovudine to decrease the risk of vertical HIV-1 transmission),  were in the cohort of women who did not receive antiretroviral therapy (referred to as the untreated cohort). The other cohort included women who received antepartum antiretroviral therapy and delivered from March 1, 1994, through 1998 (referred to as the treated cohort). Women who received antiretroviral therapy only during the intrapartum period were excluded. We excluded from the untreated cohort women who had conceived less than 44 weeks before March 1, 1994, and we excluded from the treated cohort women who had conceived more than 20 weeks before this date in order to reduce the chance of overrepresentation or underrepresentation of premature deliveries, respectively. Women enrolled in PACTS, the UCLA study, or WITS were included in both cohorts (untreated, 469, 20, and 446 women, respectively; treated, 487, 87, and 673 women, respectively). Women enrolled in PACTG study 185, the University of Miami study, or the University of Southern California study were included only in the treated cohort (463, 339, and 74 women, respectively), and women enrolled in PACTG study 076 were included only in the untreated cohort (208 women).\n\n【16】Study Variables\n---------------\n\n【17】Antiretroviral therapy was categorized as monotherapy, combination therapy without protease inhibitors, or combination therapy with protease inhibitors. If more than one regimen was used before delivery, therapy was categorized hierarchically as follows: combination therapy with protease inhibitors took precedence over combination therapy without protease inhibitors, which took precedence over monotherapy. The infant's gestational age at birth was determined on the basis of the last menstrual period, ultrasound data, or both; if these data were not available, it was based on the pediatric assessment at birth. Premature delivery was defined as delivery at less than 37 weeks of gestation, and very premature delivery as delivery at less than 32 weeks of gestation. Low birth weight was defined as less than 2500 g and very low birth weight as less than 1500 g. Apgar scores at one and five minutes were categorized as normal (≥7), possibly abnormal (<7), or definitely abnormal (<4). We used the lowest recorded antenatal CD4+ cell count (categorized as <200, 200 to 499, or ≥500 cells per cubic millimeter) for analysis. Because the exact date that antiretroviral therapy was started was not known for the majority of women, we used the week of gestation at which therapy was initiated, categorized as less than 27 weeks, 27 weeks or more, or unknown. For the group of women who did not receive antiretroviral therapy, the time of enrollment was categorized according to the week of gestation (<27 weeks or ≥27 weeks). Data for individual patients were submitted to the PACTG Statistical and Data Management Center for analysis.\n\n【18】Statistical Analysis\n--------------------\n\n【19】We performed pairwise comparisons to assess the outcome of pregnancy according to the type of antiretroviral therapy: monotherapy versus any combination therapy, monotherapy versus combination therapy with protease inhibitors, monotherapy versus combination therapy without protease inhibitors, and combination therapy without protease inhibitors versus combination therapy with protease inhibitors. We compared characteristics of the patients and the outcomes of pregnancy according to the use or nonuse of antiretroviral agents (controlling for the study), using stratified Cochran–Mantel–Haenszel tests  for categorical variables and analysis of variance for continuous variables (with logarithmic transformation). Data on outcomes in the untreated group were adjusted for differences in characteristics by direct standardization, with the treated group used as the standard population. \n\n【20】For each study, we calculated odds ratios and exact 95 percent confidence intervals  for premature delivery and low birth weight in the infant according to the use or nonuse of antiretroviral therapy. Overall odds ratios for the entire cohort were calculated with the use of the Mantel–Haenszel estimator (stratified according to the study), with 95 percent confidence intervals based on the Robins–Breslow–Greenland variance estimator.  The Breslow–Day test was used to determine whether odds ratios differed among studies.  Overall odds ratios, confidence intervals, and P values were also calculated by exact methods  (the conditional-maximum-likelihood method and Zelen test); these results did not differ substantially from the Mantel–Haenszel estimates and the Breslow–Day P values and are therefore not reported. We performed multivariate logistic-regression analyses to adjust odds ratios and calculate 95 percent confidence intervals, using the profile-likelihood method, for the following covariates: the study, the CD4+ cell count, age, race or ethnic group, presence or absence of a history of premature delivery, year of delivery, and use or nonuse of tobacco, alcohol, and illicit drugs. The Hosmer–Lemeshow test was used to assess the adequacy of the model's fit to the data. \n\n【21】Sensitivity analyses were performed to determine the effect of two potential sources of bias. Since data on prior premature delivery, a risk factor for subsequent premature delivery, were not available from one of the largest studies (PACTS), separate logistic-regression analyses were conducted for the women for whom data on all risk factors, including the presence or absence of prior premature delivery, were available and those for whom data on all risk factors except prior premature delivery were available. Since some women may have started using antiretroviral agents late in pregnancy (after 37 weeks of gestation), multivariate logistic-regression analyses were repeated with adjustment for the time at which antiretroviral therapy was initiated (<27 weeks, ≥27 weeks, or unknown).\n\n【22】Statistical analyses were performed with the use of SAS, version 8 (SAS Institute), Proc-StatXact, version 4 (Cytel Software), and Stata, version 6.0 (Stata) software. All reported P values are based on two-sided tests. A P value of less than 0.05 was considered to indicate statistical significance; no adjustment was made for multiple comparisons.\n\n【23】Results\n-------\n\n【24】Table 1. Characteristics of the 3266 Study Participants According to the Use or Nonuse of Antiretroviral Therapy during Pregnancy.\n\n【25】A total of 2123 women received antiretroviral therapy during pregnancy (1590 received monotherapy, 396 combination therapy without protease inhibitors, and 137 combination therapy with protease inhibitors), and 1143 women did not receive antiretroviral therapy. Women who received combination therapy with protease inhibitors had delivered more recently, had lower CD4+ cell counts, and had a higher median age than women who received antiretroviral regimens without protease inhibitors . The rate of illicit-drug use was higher among women who received monotherapy than among those who received combination therapy. Women who received antiretroviral therapy had lower CD4+ cell counts and lower rates of tobacco, alcohol, and illicit-drug use than women who did not receive antiretroviral therapy .\n\n【26】Table 2. Outcomes of Pregnancy According to the Use or Nonuse of Antiretroviral Therapy.\n\n【27】Unadjusted rates of very premature delivery and, among infants, low birth weight, very low birth weight, abnormal Apgar scores, and stillbirth did not differ significantly between the treated and untreated groups; however, unadjusted rates of premature delivery were significantly lower among treated women . The rates of each of these outcomes were also similar among the women who received monotherapy and those who received combination therapy.\n\n【28】Overall, the rates of adverse pregnancy outcomes remained similar in the treated and untreated groups after direct standardization to adjust for differences in the CD4+ count and rates of tobacco, alcohol, and illicit-drug use .\n\n【29】Rates of premature and very premature delivery did not differ significantly according to whether the antiretroviral regimen included protease inhibitors . Odds ratios for premature and very premature delivery in the group of women who received combination therapy without protease inhibitors, as compared with the group of women who received combination therapy with protease inhibitors, did not vary significantly among the studies .\n\n【30】The risk of low birth weight was lower among infants born to women who received combination therapy without protease inhibitors than among infants born to women who received monotherapy (unadjusted odds ratio, 0.58; 95 percent confidence interval, 0.41 to 0.84) and was higher among infants born to women who received combination therapy with protease inhibitors than among those born to women who received combination therapy without protease inhibitors (unadjusted odds ratio, 2.03; 95 percent confidence interval, 1.16 to 3.54). The risk of very low birth weight was also higher among infants born to women who received combination therapy with protease inhibitors than among infants born to women who received monotherapy or combination therapy without protease inhibitors, but these results were not statistically significant. Odds ratios for low and very low birth weight did not vary significantly among studies in the comparison of combination therapy that did not include protease inhibitors with combination therapy that did .\n\n【31】Table 3. Risks of Adverse Pregnancy Outcomes According to the Antiretroviral Regimen.\n\n【32】Data on all assessed risk factors for adverse outcomes were available for 1598 treated women, and data on all risk factors except prior premature delivery were available for 1936 treated women. After adjustment for covariates other than, or including, prior premature delivery, the risk of premature and very premature delivery among women who received combination therapy with protease inhibitors, as compared with those who received combination therapy without protease inhibitors, was not elevated . After adjustment for covariates other than prior premature delivery, combination therapy that included protease inhibitors was associated with higher risks of low birth weight and very low birth weight than was combination therapy without protease inhibitors . However, when the analysis was also adjusted for prior premature delivery, only the risk of very low birth weight remained significantly elevated , with a wide 95 percent confidence interval. These results did not change when the time of the initiation of antiretroviral therapy was added to the logistic-regression analyses (data not shown).\n\n【33】Table 4. Risks of Adverse Pregnancy Outcomes among Women Who Received Antiretroviral Therapy as Compared with Untreated Women.\n\n【34】We also compared the rates of adverse outcomes between women who received antiretroviral therapy and those who did not in multivariate analyses that both excluded and included a history of premature delivery . None of the antiretroviral regimens were associated with a significantly increased risk of premature or very premature delivery, as compared with no antiretroviral therapy. As compared with the infants of women who received no antiretroviral therapy, there was a lower risk of low birth weight among infants born to women who received combination therapy without protease inhibitors and a higher risk of very low birth weight among infants born to women who received combination therapy with protease inhibitors ; however, the latter result was not significant after adjustment for prior premature delivery.\n\n【35】Discussion\n----------\n\n【36】In a large cohort of HIV-1–infected women who received antiretroviral therapy during pregnancy, we found that the risk of an adverse outcome of pregnancy was not associated with the use of combination antiretroviral regimens overall. Our analysis was adjusted for several recognized risk factors for adverse outcomes of pregnancy.\n\n【37】Unlike the initial Swiss study,  our study showed that the risk of premature delivery was not significantly higher with combination antiretroviral therapy than with monotherapy or no therapy. The upper limit of the 95 percent confidence interval for the adjusted odds ratio for premature delivery with combination therapy, as compared with monotherapy, was 1.62, indicating that a substantial effect was unlikely.\n\n【38】Before March 1994, limited numbers of women received antiretroviral therapy for advanced HIV disease. Since then, the majority of pregnant women have received antiretroviral therapy as prophylaxis against vertical transmission.  In our study, the treated and untreated women gave birth in different years, which makes it difficult to compare these groups directly; however, the similar rates of premature delivery in the two groups argue against a profound effect of antiretroviral therapy on prematurity.\n\n【39】An analysis of combined data from the European Collaborative Study and the Swiss Cohort Study showed a significant association between the risk of premature delivery and combination therapy, without and with protease inhibitors (odds ratio, 1.8 and 2.6, respectively), after adjustment for the maternal CD4+ cell count and the presence or absence of a history of parenteral drug use.  The rates of premature delivery in the European study were similar to those in our study  for women who received no therapy or monotherapy but were higher among women who received combination therapy without protease inhibitors and among those who received combination therapy with protease inhibitors (22 and 29 percent, respectively, as compared with 14 and 18 percent, respectively, in our study).\n\n【40】The reason for the differences between our results and those of the European study is unclear. The risk factors for adverse outcomes of pregnancy are similar in HIV-1–infected women and women without HIV-1 infection.  We were able to adjust for more of these factors than were the European investigators, including prior premature delivery and use of alcohol or tobacco. Neither we nor the European investigators could control directly for the stage of maternal disease or the HIV-1 viral load — factors that in some studies were associated with premature delivery.  We found that the risk of low birth weight was lower among infants born to women who received combination therapy without protease inhibitors than among infants born to women who received no therapy, despite lower CD4+ cell counts in the women who received therapy.\n\n【41】The rate of very low birth weight was higher among infants born to women who received combination therapy with protease inhibitors than among infants born to women who received combination therapy without protease inhibitors, although the overall number of infants with very low birth weight was small, the confidence interval was wide, and we did not adjust for multiple comparisons. The difference may reflect an effect of the stage of maternal HIV disease on birth weight. It is likely that women who received combination therapy with protease inhibitors had more advanced disease than those who received combination therapy without protease inhibitors.\n\n【42】Forty-three percent of the women who received protease inhibitors had a CD4+ cell count of less than 200 per cubic millimeter, as compared with 12 percent of those who received no therapy, 19 percent of those who received monotherapy, and 28 percent of those who received combination therapy without protease inhibitors. We did not have data available to adjust our analyses for the viral load and the stage of disease. Any small increase in the risk of low birth weight is likely to be outweighed by the substantial benefits of treatment with protease inhibitors for both the mother and the infant.\n\n【43】Our study has some limitations. We did not have data on the precise time of the initiation of antiretroviral therapy, because most of the trials lacked these data. We also did not have information on early pregnancy loss, congenital anomalies, or long-term outcomes for the infants. We were unable to assess the effect of the duration of antiretroviral therapy on the outcomes of pregnancy or the effect of the stage of HIV-1 infection on the association between antiretroviral therapy and outcomes. The observed rates of premature delivery among women who received antiretroviral therapy may be lower than the actual rates because of the inclusion of some women who started antiretroviral therapy too late to be at risk for premature delivery. However, the odds ratios for premature and very premature delivery in association with antiretroviral therapy did not change when the analyses were adjusted for the time of the initiation of antiretroviral therapy.\n\n【44】Both a low maternal viral load and the use of combination antiretroviral therapy during pregnancy are associated with rates of vertical transmission of HIV-1 of 2 percent or less.  Guidelines issued by the Public Health Service support the use of combination antiretroviral therapy during pregnancy both to safeguard maternal health and to reduce the risk of vertical transmission of HIV-1 infection.  Our data provide reassurance that the risks of adverse outcomes of pregnancy that are attributable to antiretroviral therapy are low and are likely to be outweighed by the recognized benefits of such therapy during pregnancy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "7dc207ad-5153-4244-9158-74990ca8cb8b", "title": "Single-Incision Mini-Slings for Stress Urinary Incontinence in Women", "text": "【0】Single-Incision Mini-Slings for Stress Urinary Incontinence in Women\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Until recently, synthetic midurethral slings (made of mesh or tape) were the standard surgical treatment worldwide for female stress urinary incontinence, if conservative management failed. Data comparing the effectiveness and safety of newer single-incision mini-slings with those of standard midurethral slings are limited.\n\n【3】Methods\n-------\n\n【4】We performed a pragmatic, noninferiority, randomized trial comparing mini-slings with midurethral slings among women at 21 U.K. hospitals during 36 months of follow-up. The primary outcome was patient-reported success (defined as a response of very much or much improved on the Patient Global Impression of Improvement questionnaire) at 15 months after randomization (approximately 1 year after surgery). The noninferiority margin was 10 percentage points.\n\n【5】Results\n-------\n\n【6】A total of 298 women were assigned to receive mini-slings and 298 were assigned to receive midurethral slings. At 15 months, success was reported by 212 of 268 patients (79.1%) in the mini-sling group and by 189 of 250 patients (75.6%) in the midurethral-sling group (adjusted risk difference, 4.6 percentage points; 95% confidence interval \\[CI\\], −2.7 to 11.8; P<0.001 for noninferiority). At the 36-month follow-up, success was reported by 177 of 246 patients (72.0%) and by 157 of 235 patients (66.8%) in the respective groups (adjusted risk difference, 5.7 percentage points; 95% CI, −1.3 to 12.8). At 36 months, the percentage of patients with groin or thigh pain was 14.1% with mini-slings and 14.9% with midurethral slings. Over the 36-month follow-up period, the percentage of patients with tape or mesh exposure was 3.3% with mini-slings and 1.9% with midurethral slings, and the percentage who underwent further surgery for stress urinary incontinence was 2.5% and 1.1%, respectively. Outcomes with respect to quality of life and sexual function were similar in the two groups, with the exception of dyspareunia; among 290 women responding to a validated questionnaire, dyspareunia was reported by 11.7% in the mini-sling group and 4.8% in the midurethral-sling group.\n\n【7】Conclusions\n-----------\n\n【8】Single-incision mini-slings were noninferior to standard midurethral slings with respect to patient-reported success at 15 months, and the percentage of patients reporting success remained similar in the two groups at the 36-month follow-up. \n\n【9】Introduction\n------------\n\n【10】 VISUAL ABSTRACT  \nSingle-Incision Mini-Slings for Urinary Incontinence in Women\n\n【11】The lifetime risk of primary surgery for stress urinary incontinence in women is 3.6% in the United Kingdom and 13% in the United States.  Synthetic mesh midurethral slings have until recently been the most common surgical treatment for stress urinary incontinence worldwide. In England, 100,516 midurethral-sling procedures were performed between 2008 and 2017, as compared with 1195 for all other continence procedures. \n\n【12】The first midurethral sling was retropubic tension-free vaginal tape (retropubic tape), a minimally invasive day-surgery procedure with excellent short-term outcomes.  Retropubic tapes use a polypropylene mesh strip to create a suburethral hammock. Concerns about the blind insertion of the trocar into the retropubic trajectory and potential bladder or bowel injury led to the development of second-generation standard-length midurethral slings placed through the transobturator route (outside-in and inside-out obturator tapes). \n\n【13】Studies have shown similar percentages of patients reporting success with retropubic tapes and obturator tapes.  Retropubic tapes were associated with higher incidences of bladder injury and postoperative voiding dysfunction, whereas obturator tapes were associated with a higher incidence of postoperative groin or thigh pain.  More recently, single-incision mini-slings (mini-slings) were introduced. These used shorter polypropylene mesh and avoided both retropubic trajectory and adductor muscle perforation; hence, they were proposed to reduce perioperative morbidity. \n\n【14】In a systematic review published in 2014, we found that the percentages of patients with patient-reported and objective success with mini-slings (excluding TVT-Secur devices, which were discontinued by the manufacturer for commercial reasons) were similar to those with midurethral slings during a median follow-up at 18 months  ; mini-slings were associated with less postoperative pain and a shorter recovery time. However, trials included in this review were small and heterogeneous and had a high risk of bias. A Cochrane review recommended an adequately powered randomized, controlled trial with long-term follow-up to evaluate the clinical benefits and risks and the cost-effectiveness of mini-slings.  We performed a pragmatic, multicenter, noninferiority, randomized, controlled trial (the SIMS trial) comparing outcomes of adjustable anchored mini-slings with those of tension-free midurethral slings in women with stress urinary incontinence.\n\n【15】Methods\n-------\n\n【16】Trial Design and Patients\n-------------------------\n\n【17】We conducted a randomized noninferiority trial at 21 hospitals in the United Kingdom. We report here the clinical outcomes through 36 months of follow-up.\n\n【18】We included women 18 years of age or older who had predominant symptoms of stress urinary incontinence and in whom conservative treatment had failed or who had declined such treatment; all the women planned to undergo a midurethral-sling procedure. Exclusion criteria were anterior or apical prolapse of stage 2 or higher, previous surgery for stress urinary incontinence, predominant symptoms of overactive bladder, planned concomitant surgery, previous pelvic irradiation, pregnancy or planning pregnancy, and an inability to understand consent in English.\n\n【19】Eligible women received a trial information leaflet, and trial patients provided written informed consent.  Patients were randomly assigned in a  ratio to receive mini-slings or midurethral slings. Randomization was performed with the use of a remote Web-based system and was stratified with the use of minimization according to center and previous supervised pelvic-floor muscle training within 2 years.\n\n【20】Trial Oversight\n---------------\n\n【21】The trial was funded by the U.K. National Institute for Health Research. The University of Aberdeen and NHS Grampian were the scientific sponsors of the trial and had overall responsibility for the trial governance. The trial did not receive any financial support from industry. Collaborating sites purchased the devices through their standard NHS procurement procedures.\n\n【22】 The second author and the last two authors confirm the accuracy and completeness of the data reported.\n\n【23】Interventions\n-------------\n\n【24】The interventions are described in detail in the protocol. Adjustable anchored mini-slings met prespecified criteria that were identified in previous research as being necessary for their success.  The two main types of mini-slings used were Ajust (C.R. Bard) and Altis (Coloplast). Midurethral slings were either retropubic or obturator (inside-out or outside-in) tapes. The choice of device in both trial groups was made according to the surgeons’ standard practice. Mini-sling procedures were performed with the patient under local anesthesia unless the patient requested general anesthesia. Cystoscopy (rigid or flexible) was performed regardless of trial group. Standardized guidance was provided for participating surgeons regarding the administration of local anesthesia and the assessment of postoperative voiding .\n\n【25】Participating surgeons were experienced in performing at least one type of mini-sling procedure and at least one type of midurethral-sling procedure according to the protocol.  Clinical experts in the trial team visited 90% of the collaborating hospitals before starting local recruitment to observe participating surgeons perform two mini-sling procedures with the patient under local anesthesia, in order to confirm surgeons’ competence and to discuss standardization of surgical techniques and protocols. \n\n【26】Making surgeons and patients unaware of the trial-group assignments was not possible given the different anesthesia typically used for the two different types of procedures. All follow-up was patient-reported through postal questionnaires.\n\n【27】Data Collection\n---------------\n\n【28】We collected patient-reported data at baseline; 4 weeks and 3 months after surgery; and 15, 24, and 36 months after randomization (approximately 12, 21, and 33 months after surgery) . The data collected at 4 weeks and 3 months addressed perioperative outcomes in relation to surgery date.\n\n【29】Outcomes\n--------\n\n【30】The primary outcome was patient-reported success (defined as a response of very much improved or much improved on the Patient Global Impression of Improvement \\[PGI-I\\] questionnaire  ) at 15 months. All other responses (improved, same, worse, much worse, and very much worse) were considered to indicate treatment failure.\n\n【31】Safety data included all expected adverse events during follow-up through 36 months: pain, tape or mesh exposure (exposure of tape or mesh through the vaginal wall), operative complications (lower urinary tract injuries, severe bleeding, and bowel injuries), new onset or worsening of urinary urgency, dyspareunia, and intermittent catheterization by the patient. All serious unexpected adverse events were reported by the collaborating hospitals and reviewed by the trial sponsor and the independent data monitoring committee according to the trial protocol. \n\n【32】Secondary outcomes included postoperative pain (to 14 days), recovery (i.e., return to normal activities), objective success (defined as a 24-hour pad-test weight of <8 g), and the effect of the procedures on patients’ symptom severity, quality of life, and sexual function according to validated tools or questionnaires . Health economic outcomes are being analyzed separately.\n\n【33】Statistical Analysis\n--------------------\n\n【34】A noninferiority margin of 10 percentage points was determined by clinical experts as the maximum acceptable margin should mini-slings be shown to be superior to midurethral slings in other outcomes such as postoperative pain and earlier recovery. Assuming 85% success for both procedures,  we estimated that a sample size of 550 would provide the trial with 90% power at a one-sided significance level of 2.5%; assuming a 15% dropout rate, we planned for a sample size of 650. This number was reduced (without examination of any outcome data) to 600 in November 2016, given difficulties in completing recruitment on time and within budget, thereby reducing the power from 90% to 88%. Effect sizes are presented as risk differences and 95% confidence intervals. Odds ratios are also presented. A P value for noninferiority is reported for the primary outcome.\n\n【35】All primary and secondary outcomes were analyzed according to the intention-to-treat principle with the use of multiple imputation with chained equations to handle missing outcomes. More information on the multiple imputation is provided in the Supplementary Appendix . All models were adjusted for minimization covariates. The primary outcome was analyzed with the use of logistic regression with robust variances for clustering according to center. A prespecified per-protocol sensitivity analysis assessed the primary outcome for patients who underwent their assigned surgery. Secondary outcomes were analyzed with the use of linear mixed models that were adjusted for baseline versions of outcome. The 14-day postoperative pain scores were compared with the use of an area-under-the-curve approach. The number of days of analgesia use was compared (post hoc) between the two groups with the use of negative binomial regression. Estimates are presented with 95% confidence intervals, with no adjustments for multiple testing. All analyses were performed with the use of Stata 15 software (details are provided in the Supplementary Appendix ). \n\n【36】Prespecified subgroup analyses were performed according to urodynamic diagnosis (urodynamic stress incontinence vs. mixed urinary incontinence) and age (younger than the median age vs. the median age or older). We also performed post hoc subgroup analyses according to an age of younger than 65 years as compared with an age of 65 years or older; supervised pelvic-floor muscle training within the preceding 2 years as compared with no training during that time; and devices withdrawn from clinical practice (Ajust mini-sling and Monarc, I-Stop, and Aris midurethral slings) as compared with devices not withdrawn (Altis mini-sling and TVT, TVT-O, Advantage, and Obtryx midurethral slings).\n\n【37】Results\n-------\n\n【38】Trial Population\n----------------\n\n【39】Figure 1. Screening, Randomization, and Follow-up.\n\n【40】Seven patients were ineligible for more than one reason. “Responded” indicates that the patient returned a fully or partially completed trial questionnaire. PGI-I denotes Patient Global Impression of Improvement, SIMS single-incision mini-sling, and SMUS standard midurethral sling.Table 1.  Table 1. Characteristics of the Patients at Baseline.\n\n【41】A total of 600 women underwent randomization (February 2014 through July 2017) at 21 hospitals. Four women were excluded after randomization, leaving 298 assigned to receive mini-slings and 298 assigned to receive midurethral slings; 257 women (86.2%) in each group underwent their assigned surgery . At 15 and 36 months after randomization, the percentage of patients who returned a fully or partially completed trial questionnaire was 87.1% and 81.4%, respectively. The characteristics of the two groups were well balanced at baseline , except that more women in the mini-sling group used anticholinergic treatment.\n\n【42】Operative Data\n--------------\n\n【43】Table 2. Perioperative Outcomes.\n\n【44】Crossover between groups occurred in 16 women (5.8%) assigned to receive mini-slings and 4 (1.5%) assigned to receive midurethral slings; in addition, 3 women assigned to receive mini-slings underwent other procedures. Crossover was due to patient preference or the unavailability of a specific device or an experienced surgeon on the day of surgery. Women in the midurethral-sling group were more likely than those in the mini-sling group to have their procedure performed by a subconsultant-grade surgeon and less likely to have their procedures under local anesthesia or the sling adjusted with the use of a cough stress test. Most patients received intraoperative local anesthesia infiltration regardless of procedure type. Pain scores over the immediate postoperative period of 14 days appeared lower with mini-slings than with midurethral slings .\n\n【45】Patient-Reported and Objective Success\n--------------------------------------\n\n【46】Table 3. Patient-Reported and Objective Outcomes.\n\n【47】At 15 months after randomization, success was reported by 212 of 268 patients (79.1%) in the mini-sling group and by 189 of 250 patients (75.6%) in the midurethral-sling group (adjusted risk difference, 4.6 percentage points; 95% confidence interval \\[CI\\], −2.7 to 11.8; P<0.001 for noninferiority). At the 36-month follow-up, success was reported by 177 of 246 patients (72.0%) and by 157 of 235 patients (66.8%) in the respective groups (adjusted risk difference, 5.7 percentage points; 95% CI, −1.3 to 12.8) . The per-protocol estimates at 15, 24, and 36 months were similar to those in the intention-to-treat analysis . Figure S1 shows the results of the subgroup analyses; all were consistent with the results of the primary analysis.\n\n【48】Success objectively measured with a 24-hour pad test occurred in 102 of 119 patients (85.7%) in the mini-sling group and in 83 of 110 patients (75.5%) in the midurethral-sling group at 15 months (adjusted risk difference, 5.2 percentage points; 95% CI, −5.9 to 16.2) . In a post hoc analysis that used results on the International Consultation on Incontinence Questionnaire–Urinary Incontinence Short Form to identify “cure” (defined as no leaking, in response to both “how often do you leak urine?” and “how much urine do you usually leak?”), cure occurred in 93 of 241 patients (38.6%) in the mini-sling group and in 72 of 217 patients (33.2%) in the midurethral-sling group at 15 months (adjusted risk difference, 6.4; 95% CI, −1.2 to 13.9) . Table S3B shows PGI-I seven-point responses at each time point. The results of prespecified and post hoc subgroup analyses were consistent with the overall results .\n\n【49】Secondary Outcomes\n------------------\n\n【50】Table 4. Adverse Events.\n\n【51】There were no material between-group differences in scores on several scales assessing lower urinary tract symptoms, quality of life, and sexual function . Dyspareunia and coital incontinence were more common with mini-slings than with midurethral slings at almost all time points .\n\n【52】Safety\n------\n\n【53】The percentage of patients with serious adverse events was similar in the two groups . The percentage of patients with groin or thigh pain was higher in the mini-sling group than in the midurethral-sling group at 15 months but was similar in the two groups at 36 months (14.1% and 14.9%, respectively; risk difference, −0.8 percentage points; 95% CI, −4.1 to 2.5).\n\n【54】Over the 36-month follow-up period, tape or mesh exposure occurred in 3.3% of the patients in the mini-sling group and in 1.9% of those in the midurethral-sling group (risk difference, 1.3 percentage points; 95% CI, −1.7 to 4.4). One patient in the mini-sling group had persistent tape or mesh exposure after a procedure to bury the exposed mesh and subsequently underwent local excision of the exposed portion.\n\n【55】A total of 24 patients (8.7%) who received mini-slings and 12 (4.6%) who received midurethral slings received further surgical treatment over a period of 36 months, including further surgery for stress urinary incontinence (2.5% and 1.1%, respectively), for complete or partial removal of tape or mesh for any reason (2.9% and 1.9%), for pain (1.5% and 0.8%), or for mesh exposure (1.4% and 1.1%) . All 4 patients (2 in each group) who received a colposuspension or autologous sling and one patient in the mini-sling group who received urethral bulking had previously undergone complete or partial tape removal.\n\n【56】Discussion\n----------\n\n【57】This multicenter trial showed that mini-slings were noninferior to midurethral slings with respect to patient-reported success at 15 months and through the 36-month follow-up. Findings were similar in the per-protocol analysis and in prespecified subgroups.\n\n【58】Mini-sling procedures were more likely to be performed with the patient under local anesthesia, and mini-slings were associated with less postoperative pain up to 2 weeks after surgery. At 36 months, the percentage of patients with groin or thigh pain was similar in the two groups. However, more women in the mini-sling group reported dyspareunia, mesh exposure, or further surgery for urinary incontinence or treatment of adverse events. The percentage of patients who underwent mesh removal (partial or complete) for any indication was low and was similar in the two groups.\n\n【59】Our findings of similar likelihoods of success with mini-slings and with midurethral slings are consistent with previous evidence. Two randomized, controlled trials — one comparing mini-slings (MiniArc) with obturator tapes (193 patients) and the other comparing mini-slings (MiniArc) with retropubic tapes (185 patients) — with 36 months of follow-up showed no significant between-group differences in the percentage of patients with patient-reported success (according to the response on the PGI-I questionnaire) or objective success.  A third randomized, controlled trial comparing mini-slings (Ajust) with obturator tapes (368 patients) and using assessment tools similar to those that we used also showed no significant differences in the percentages of patients with subjective and objective success at 1 year, but mini-slings resulted in less immediate postoperative pain, a shorter operative time, and shorter recovery.  Our 2014 systematic review (26 randomized, controlled trials involving 3308 patients) similarly showed no significant differences between mini-slings and midurethral slings in the percentages of patients with patient-reported and objective success at 18 months of follow-up.  A more recent review of longer-term outcome results (≥3 years) showed that midurethral slings were associated with a significantly higher likelihood of objective success than mini-slings, although the likelihood of patient-reported success was similar with the two types of slings.  The present trial was larger than previous similar studies and had consistent methods. Moreover, follow-up through postal questionnaires probably helped to minimize the rate of loss to follow-up and minimize bias.\n\n【60】The safety of mesh devices has been the subject of substantial scrutiny over the past decade, owing to patient reports of adverse events during extended follow-up, including tape or mesh exposure, groin or thigh pain, and dyspareunia. Lawsuits have been filed against mesh manufacturers in various countries,  and some manufacturers have withdrawn their products from clinical practice.  The SIMS trial was performed during heightened public debate about mesh devices; hence, patients and clinicians were unlikely to have underreported adverse events. Clinical guidelines in the United States, Europe, and the United Kingdom continue to recommend tension-free midurethral slings as surgical treatment for female stress urinary incontinence, albeit being suspended in the United Kingdom since 2018.\n\n【61】The incidence of tape or mesh exposure over the 36-month follow-up period was 3.3% in the mini-sling group and 1.9% in the midurethral-sling group; the incidence did not differ significantly between the two groups, but the trial was not powered for this or other uncommon outcomes. Our results are consistent with those of our 2014 systematic review showing a slightly higher, although not significantly higher, incidence of mesh exposure with mini-slings than with midurethral slings (2.3% \\[15 of 659\\] and 1.4% \\[8 of 564\\], respectively; risk ratio, 1.43; 95% CI, 0.61 to 3.35). \n\n【62】The incidence of groin or thigh pain was higher in the mini-sling group than in the midurethral-sling group at 15 months but not at 36 months. In a previous trial (60 patients) comparing mini-slings (Ajust) with obturator tapes, 3 patients who received mini-slings reported persistent thigh pain 1 year after surgery, as compared with none who received obturator tapes.  In the Trial of Mid-Urethral Slings (597 patients), pain was more commonly reported with obturator tapes than with retropubic tapes (9.4% vs. 4.0%).  In the present trial, all 4 patients receiving tape removal for pain at up to 15 months were in the mini-sling group. However, by 36 months, 2 women in the midurethral-sling group had undergone partial removal for pain, with no further removals for pain in the mini-sling group. It is possible that surgeons, patients, or both perceived mini-slings to be easier to remove, given that they are shorter. However, the authors are aware of one report of difficulties in removing the mini-sling anchors with vaginal dissection only (Bhal K: personal communication).\n\n【63】In the present report, there were no cases of major visceral injuries, and intraoperative lower urinary tract injuries occurred exclusively in the midurethral-sling group. However, our 2014 systematic review showed no significant difference in lower urinary tract injuries between mini-slings and midurethral slings in 13 randomized, controlled trials (risk ratio, 0.99; 95% CI, 0.38 to 2.56). \n\n【64】More women in the mini-sling group received further surgery for urinary incontinence or mesh-related adverse events, findings consistent with previous evidence that more women receiving mini-slings underwent further continence surgery than those receiving midurethral slings (risk ratio, 2.00; 95% CI, 0.93 to 4.31).  Dyspareunia was significantly more common with mini-slings in the present trial. A previous randomized trial (205 patients) comparing mini-slings (Ajust) with midurethral slings showed no significant between-group difference in the incidence of dyspareunia at the 3-year follow-up.  In another randomized trial comparing mini-slings (Ajust) with obturator tapes, 2 patients in the mini-sling group, as compared with none in the obturator-tape group, reported new dyspareunia  ; these cases were thought to be possibly due to a painful anchor in the obturator membrane, and 1 patient underwent surgical removal of the anchor. Dyspareunia may also be caused by mesh-related infection, mesh exposure, or abnormal healing leading to scarring. \n\n【65】The main limitations of our trial were the availability of follow-up to only 3 years, a lack of blinding (for feasibility reasons), and inadequate power to detect important differences in adverse events. Late-onset adverse events, a decline in effectiveness over time, or both have been reported with both mini-slings and midurethral slings, and longer-term effectiveness and safety data are needed  ; 10-year follow-up of trial patients is planned. More midurethral-sling procedures than mini-sling procedures were performed by subconsultant-grade surgeons, but we consider this unlikely to have affected the results. Unlike with the relatively new mini-slings, placing midurethral slings is part of the structured training program of surgeons in the United Kingdom; thus, senior trainee surgeons and associate specialists who have completed their training are likely to be skilled at performing these procedures.\n\n【66】We had limited data on objective success; however, patient-reported outcomes better reflect patients’ experience than objective measures, which can overestimate the success of surgery for stress urinary incontinence.  Several mesh devices were withdrawn from the market during the heightened mesh debate. However, our trial compared two types of slings (tension-free midurethral slings and adjustable anchored mini-slings) and not specific devices; moreover, most patients received devices still on the market, and results were similar in this subgroup. Generalizability may be limited by our exclusion of women with anterior or apical prolapse beyond stage 2 and those undergoing concomitant prolapse and by the relatively young age (mean, 50 years) and nonobese body-mass index (the weight in kilograms divided by the square of the height in meters) (mean, 29) of the patients. Additional limitations include the absence of information on the race and ethnic group of the patients  and on preoperative pain level.\n\n【67】In the SIMS trial, adjustable anchored mini-slings were noninferior to tension-free midurethral slings with respect to patient-reported success at 15 months, and the between-group difference remained similar at 36 months.\n\n【68】Table 1. Characteristics of the Patients at Baseline. \n\n| Characteristic | Single-Incision Mini-Sling(N=298) | Standard Midurethral Sling(N=298) |\n| --- | --- | --- |\n| Age |  |  |\n| No. of patients with available data | 298 | 298 |\n| Mean — yr | 50.4±11.0 | 50.7±10.9 |\n| Body-mass index |  |  |\n| No. of patients with available data | 297 | 292 |\n| Mean | 28.9±5.5 | 28.7±5.6 |\n| Parity: no. of pregnancies |  |  |\n| No. of patients with available data | 296 | 294 |\n| Mean | 2.4±1.1 | 2.4±1.1 |\n| Manual job involving heavy lifting — no. (%) | 84 (28.2) | 84 (28.2) |\n| Smoker — no. (%) | 52 (17.4) | 43 (14.4) |\n| Pelvic-floor muscle training in past 2 yr — no. (%) | 254 (85.2) | 254 (85.2) |\n| 24-Hr pad-test weight |  |  |\n| No. of patients with available data | 234 | 204 |\n| Median (interquartile range) — g | 39 (24–60) | 40 (24–67) |\n| EQ-5D-3L score  |  |  |\n| No. of patients with available data | 286 | 284 |\n| Mean | 0.86±0.20 | 0.83±0.25 |\n| ICIQ-UI Short Form score  |  |  |\n| No. of patients with available data | 284 | 285 |\n| Mean | 14.4±3.3 | 14.4±3.6 |\n| Dyspareunia — no./total no. (%)  | 25/145 (17.2) | 21/145 (14.5) |\n| Coital incontinence — no./total no. (%)  | 60/145 (41.4) | 52/145 (35.9) |\n| Use of anticholinergic drug — no. (%) | 60 (20.1) | 35 (11.7) |\n| Diagnosis based on urodynamic testing — no. (%) |  |  |\n| Stress incontinence | 235 (78.9) | 231 (77.5) |\n| Mixed incontinence | 36 (12.1) | 33 (11.1) |\n| Clinical diagnosis of stress urinary incontinence — no. (%)  | 14 (4.7) | 11 (3.7) |\n| Grade of surgeon — no./total no. (%)  |  |  |\n| Subspecialist urogynecologist | 65/276 (23.6) | 46/261 (17.6) |\n| Consultant gynecologist | 183/276 (66.3) | 160/261 (61.3) |\n| Consultant urologist | 23/276 (8.3) | 9/261 (3.4) |\n| Associate specialist or staff grade | 1/276 (0.4) | 3/261 (1.1) |\n| Senior trainee | 4/276 (1.4) | 43/261 (16.5) |\n| Type of procedure — no./total no. (%)  |  |  |\n| Midurethral sling |  |  |\n| Retropubic tape | 7/276 (2.5) | 119/261 (45.6) |\n| Obturator tape | 9/276 (3.3) | 138/261 (52.9) |\n| Mini-sling |  |  |\n| Ajust | 62/276 (22.5) | 0/261 |\n| Altis | 195/276 (70.7) | 4/261 (1.5) |\n\n【70】 Plus–minus values are means ±SD. Percentages may not total 100 because of rounding. Further details on baseline characteristics are provided in Table S2 in the Supplementary Appendix.\n\n【71】 Scores on the EuroQol Group 5-Dimension 3-Level (EQ-5D-3L) questionnaire range from −0.594 to 1, with higher scores indicating better quality of life.\n\n【72】 Scores on the International Consultation on Incontinence Questionnaire–Urinary Incontinence (ICIQ-UI) Short Form range from 0 to 21, with higher scores indicating a poorer outcome.\n\n【73】 A total of 145 patients in each group were randomly assigned to answer direct questions on dyspareunia and coital incontinence, and the rest of the patients received the formal Pelvic Organ Prolapse–Urinary Incontinence Sexual Questionnaire–IUGA (International Urogynecological Association) Revised (PISQ-IR).\n\n【74】 No urodynamic testing was performed.\n\n【75】 Consultants and associate specialists are fully trained independent practitioners equivalent to attending surgeons in the United States. Seniors trainees are in the last 2 years of their training scheme and have completed their training in placing standard midurethral slings; they are equivalent to senior residents or fellows in the United States. Placing single-incision mini-slings is not part of the training schemes in the United Kingdom.\n\n【76】 In the mini-sling group, three women received other procedures: two received the MiniArc mini-sling, and one received an autologous fascial sling.\n\n【77】Table 2. Perioperative Outcomes. \n\n| Outcome | Single-Incision Mini-Sling(N=276) | Standard Midurethral Sling(N=261) | Adjusted Risk Difference (95% CI)  |\n| --- | --- | --- | --- |\n| Procedure time |  |  |  |\n| No. of patients with available data | 273 | 258 |  |\n| Mean — min | 39.2±16.8 | 41.3±11.6 | −2.2 (−5.9 to 1.6) |\n| Type of anesthesia — no. (%) |  |  |  |\n| General | 70 (25.4) | 238 (91.2) |  |\n| Spinal | 5 (1.8) | 7 (2.7) |  |\n| Local  | 201 (72.8) | 16 (6.1) |  |\n| Local anesthesia infiltration to facilitate dissection — no. (%) | 270 (97.8) | 235 (90.0) |  |\n| Tape was adjusted according to cough stress test — no. (%) | 180 (65.2) | 15 (5.7) |  |\n| Blood loss — no. (%) |  |  | 0.72 (0.48 to 1.08)  |\n| <50 ml | 134 (48.6) | 107 (41.0) |  |\n| 50–100 ml | 126 (45.7) | 129 (49.4) |  |\n| ≥100 ml | 15 (5.4) | 23 (8.8) |  |\n| Missing data | 1 (0.4) | 2 (0.8) |  |\n| Unplanned operative events — no. (%) |  |  |  |\n| More than one kit used | 7 (2.5) | 0 |  |\n| Anesthesia changed | 7 (2.5) | 1 (0.4) |  |\n| Trocar inserted more than once | 1 (0.4) | 0 |  |\n| Satisfactory voiding without any intervention — no. (%) | 230 (83.3) | 206 (78.9) | 4.1 (−6.1 to 14.2)  |\n| Postoperative hospital stay |  |  |  |\n| No. of patients with available data | 276 | 261 |  |\n| Mean — hr | 7.2±8.7 | 9.7±10.7 | −2.5 (−4.7 to −0.3) |\n| Return to normal activities within 28 days — no./total no. (%) | 185/246 (75.2) | 160/226 (70.8) | 4.2 (−3.4 to 11.9)  |\n| Pain score up to 14 days after surgery  |  |  |  |\n| No. of patients with available data | 238 | 213 |  |\n| Mean | 19.8±19.6 | 28.1±22.2 | −8.3 (−12.8 to −3.8) |\n| Days of analgesia up to 14 days after surgery |  |  |  |\n| No. of patients with available data | 238 | 213 |  |\n| Mean | 2.7±3.6 | 3.5±3.8 | 0.79 (0.64 to 0.98) |\n\n【79】 Plus–minus values are means ±SD.\n\n【80】 Because the 95% confidence intervals (CIs) were not adjusted for multiplicity, they should not be used to infer definitive treatment effects. Detailed information on the models used is provided in the Supplementary Appendix.\n\n【81】 Of the 201 patients in the mini-sling group who received local anesthesia, 47 had intravenous sedation, 26 had oral sedation, and 128 had local anesthesia only. Of the 16 patients in the midurethral-sling group, 14 had intravenous sedation, 1 had oral sedation, and 1 had local anesthesia only.\n\n【82】 Blood loss was analyzed with the use of using an ordered logistic regression, so the effect size is an odds ratio. The odds ratio of 0.72 suggests lower levels of blood loss in the mini-sling group than in the midurethral-sling group.\n\n【83】 The difference shown is in percentage points.\n\n【84】 Scores range from 0 to 130, with higher scores indicating greater pain.\n\n【85】Table 3. Patient-Reported and Objective Outcomes. \n\n| Outcome | Single-Incision Mini-Sling | Standard Midurethral Sling | Adjusted Difference(95% CI)  |\n| --- | --- | --- | --- |\n| Patient-reported success according to PGI-I response — no./total no. (%)  |  |  |  |\n| At 15 mo: primary outcome | 212/268 (79.1) | 189/250 (75.6) | 4.6 (−2.7 to 11.8)  |\n| At 36 mo | 177/246 (72.0) | 157/235 (66.8) | 5.7 (−1.3 to 12.8) |\n| Patient-reported cure on ICIQ-UI Short Form — no./total no. (%)  |  |  |  |\n| At 15 mo | 93/241 (38.6) | 72/217 (33.2) | 6.4 (−1.2 to 13.9) |\n| At 36 mo | 68/210 (32.4) | 62/202 (30.7) | 4.1 (−4.0 to 12.2) |\n| Objective cure on 24-hr pad test — no./total no. (%)  |  |  |  |\n| At 15 mo | 102/119 (85.7) | 83/110 (75.5) | 5.2 (−5.9 to 16.2) |\n| At 36 mo | 75/87 (86.2) | 64/79 (81.0) | 3.7 (−5.0 to 12.4) |\n| ICIQ-UI Short Form score |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 219 | 200 |  |\n| Mean | 4.4±5.0 | 4.7±5.0 | −0.4 (−1.2 to 0.5) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 195 | 187 |  |\n| Mean | 4.9±4.8 | 5.3±5.2 | −0.5 (−1.4 to 0.4) |\n| ICIQ-FLUTS filling score  |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 247 | 220 |  |\n| Mean | 3.4±2.4 | 3.5±2.5 | 0.1 (−0.3 to 0.5) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 214 | 199 |  |\n| Mean | 3.6±2.4 | 3.6±2.4 | 0.0 (−0.4 to 0.4) |\n| ICIQ-FLUTS voiding score  |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 248 | 217 |  |\n| Mean | 2.1±2.3 | 2.1±2.1 | 0.0 (−0.4 to 0.3) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 215 | 199 |  |\n| Mean | 1.9±2.1 | 2.0±2.1 | −0.1 (−0.5 to 0.2) |\n| ICIQ-FLUTS incontinence score  |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 241 | 215 |  |\n| Mean | 3.9±4.1 | 4.4±4.3 | −0.2 (−0.9 to 0.4) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 211 | 197 |  |\n| Mean | 4.4±4.2 | 4.5±4.3 | −0.2 (−1.0 to 0.5) |\n| EQ-5D-3L score |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 249 | 219 |  |\n| Mean | 0.848±0.243 | 0.825±0.300 | 0.022 (−0.018 to 0.062) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 217 | 205 |  |\n| Mean | 0.836±0.261 | 0.821±0.294 | 0.013 (−0.030 to 0.056) |\n| ICIQ-FLUTS quality-of-life score  |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 230 | 202 |  |\n| Mean | 26.6±10.2 | 27.6±10.5 | −0.7 (−2.5 to 1.1) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 203 | 181 |  |\n| Mean | 27.4±10.7 | 28.3±11.4 | −1.1 (−3.1 to 0.8) |\n| PISQ-IR score  |  |  |  |\n| At 15 mo |  |  |  |\n| No. of patients with available data | 75 | 55 |  |\n| Mean | 3.7±0.5 | 3.7±0.5 | 0.0 (−0.2 to 0.1) |\n| At 36 mo |  |  |  |\n| No. of patients with available data | 62 | 54 |  |\n| Mean | 3.6±0.6 | 3.5±0.6 | 0.0 (−0.1 to 0.1) |\n\n【87】 Plus–minus values are means ±SD. The 15-month and 36-month follow-ups are timed from randomization and can be considered to be 12 and 33 months after surgery, respectively. When an outcome was missing, a value has been inserted from multiple imputation using chained equations. ICIQ-FLUTS denotes International Consultation on Incontinence Questionnaire–Female Lower Urinary Tract Symptoms.\n\n【88】 For the two patient-reported outcomes, adjusted risk differences are shown. For the remaining (objective) outcomes, adjusted mean differences are shown. Because the 95% confidence intervals were not adjusted for multiplicity, they should not be used to infer definitive treatment effects.\n\n【89】 Patient-reported success was defined as a response of very much improved or much improved on the Patient Global Impression of Improvement (PGI-I) questionnaire. All other responses (improved, same, worse, much worse, and very much worse) were considered to indicate treatment failure.\n\n【90】 P<0.001 for noninferiority.\n\n【91】 In this post hoc analysis, a patient-reported cure on the ICIQ-UI Short Form was defined as a negative response to both “how often do you leak urine?” and “how much urine do you usually leak?”\n\n【92】 An objective cure on the 24-hour pad test was defined as a weight of less than 8 g.\n\n【93】 ICIQ-FLUTS filling scores range from 0 to 16, with higher scores indicating a poorer outcome.\n\n【94】 ICIQ-FLUTS voiding scores range from 0 to 12, with higher scores indicating a poorer outcome.\n\n【95】 ICIQ-FLUTS incontinence scores range from 0 to 20, with higher scores indicating a poorer outcome.\n\n【96】 ICIQ-FLUTS quality-of-life scores range from 19 to 76, with higher scores indicating a poorer outcome.\n\n【97】 PISQ-IR scores range from 1 to 5, with higher scores indicating better sexual function.\n\n【98】Table 4. Adverse Events. \n\n| Event | Single-Incision Mini-Sling | Standard Midurethral Sling | Risk Difference(95% CI)  | P Value |\n| --- | --- | --- | --- | --- |\n|  | no./total no. (%) | no./total no. (%) |  |  |\n| Operative |  |  |  |  |\n| Bladder injury | 0/276 | 9/261 (3.4) | −3.5 (−8.7 to 1.8) | 0.18 |\n| Urethral injury | 0/276 | 1/261 (0.4) | −0.4 (−1.2 to 0.4) | 0.32 |\n| Blood loss >200 ml | 5/276 (1.8) | 5/261 (1.9) | −0.1 (−2.6 to 2.4) | 0.94 |\n| Complications of general anesthesia | 1/276 (0.4) | 0/261 |  |  |\n| Vaginal button hole  | 6/276 (2.2) | 3/261 (1.1) |  |  |\n| Anaphylactic reaction to antibiotics  | 1/276 (0.4) | 0/261 |  |  |\n| Skin reaction in the area of surgery  | 0/276 | 1/261 (0.4) |  |  |\n| Intraoperative tonic–clonic seizure  | 1/276 (0.4) | 0/261 |  |  |\n| Postoperative serious adverse events |  |  |  |  |\n| Death | 0/298 | 1/298 (0.3)  |  |  |\n| Transient ischemic attack | 0/298 | 1/298 (0.3) |  |  |\n| Overdose of acetaminophen | 0/298 | 1/298 (0.3)  |  |  |\n| Lung cancer | 0/298 | 1/298 (0.3) |  |  |\n| Other postoperative adverse events |  |  |  |  |\n| Any degree of groin or thigh pain |  |  |  |  |\n| At 15 mo | 41/276 (14.9) | 31/261 (11.9) | 3.0 (−1.1 to 7.1) | 0.14 |\n| At 36 mo | 39/276 (14.1) | 39/261 (14.9) | −0.8 (−4.1 to 2.5) | 0.61 |\n| Use of any type of painkiller |  |  |  |  |\n| At 15 mo | 24/276 (8.7) | 13/261 (5.0) | 3.7 (0.0 to 7.4) | 0.047 |\n| At 36 mo | 21/276 (7.6) | 12/261 (4.6) | 3.0 (−0.4 to 6.4) | 0.08 |\n| Tape or mesh exposure  |  |  |  |  |\n| At 15 mo | 2/276 (0.7) | 2/261 (0.8) | 0.0 (−1.6 to 1.5) | 0.96 |\n| At 36 mo | 1/276 (0.4) | 0/261 | 0.4 (−0.4 to 1.1) | 0.33 |\n| Dyspareunia  |  |  |  |  |\n| At 15 mo | 25/145 (17.2) | 8/145 (5.5) | 11.8 (3.5 to 20.1) | 0.008 |\n| At 36 mo | 17/145 (11.7) | 7/145 (4.8) | 7.0 (1.9 to 12.1) | 0.01 |\n| Additional surgical treatments  | 24/276 (8.7) | 12/261 (4.6) | 4.1 (−1.1 to 9.4) | 0.12 |\n| For urinary incontinence | 12/276 (4.3) | 6/261 (2.3) |  |  |\n| For voiding dysfunction | 1/276 (0.4) | 2/261 (0.8) |  |  |\n| For pain | 7/276 (2.5) | 2/261 (0.8) |  |  |\n| For mesh exposure  | 7/276 (2.5) | 3/261 (1.1) |  |  |\n\n【100】 Details of additional surgical treatments and other adverse events are provided in Table S4.\n\n【101】 Effect sizes are presented for clinically important outcomes. Because the 95% confidence intervals were not adjusted for multiplicity, they should not be used to infer definitive treatment effects.\n\n【102】 Vaginal button hole is a surgical injury to the lateral vaginal sulcus.\n\n【103】 These events were intraoperative serious adverse events.\n\n【104】 Death at home was attributed to drug overdose 3 years after surgery; details and death certificate were not available.\n\n【105】 The event occurred 10 days after the surgical procedure.\n\n【106】 Tape or mesh exposure indicates exposure of tape or mesh through the vaginal wall.\n\n【107】 A total of 145 patients in each group were randomly assigned to answer direct questions on dyspareunia and coital incontinence,, and the rest of the patients received the formal PISQ-IR.\n\n【108】 Some patients underwent more than one additional surgery.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d2f07ab3-4bf6-4aeb-a7fa-05ced5183985", "title": "Influence of Positive End-Expiratory Pressure on Left Ventricular Performance", "text": "【0】Influence of Positive End-Expiratory Pressure on Left Ventricular Performance\nAbstract\n--------\n\n【1】Although left ventricular dysfunction is common during ventilatory support with positive end-expiratory pressure (PEEP), the mechanism of this disorder remains unclear. In 10 patients with the adult respiratory-distress syndrome we studied the effects of a stepwise increase in PEEP from 0 to 30 cm H <sub>2 </sub> O on left ventricular output, intracardiac transmural pressures, and two-dimensional echocardiographic measurements of left ventricular cross-sectional area at end-systole and at end-diastole. Increasing PEEP was associated with progressive declines in cardiac output, mean blood pressure, and left ventricular dimensions and with equalization of right and left ventricular filling pressures. The radius of septal curvature decreased at both end-diastole and end-systole, implying a leftward shift of the interventricular septum. At the highest PEEP, blood-volume expansion did not restore cardiac output, although left ventricular transmural filling pressures had returned to base-line values. We conclude that decreased cardiac output during PEEP is mediated by a leftward displacement of the interventricular septum, which restricts left ventricular filling.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "73e4a5ea-097f-4e7b-a698-90febe05dbad", "title": "The Use of Levonorgestrel Implants (Norplant) for Contraception in Adolescent Mothers", "text": "【0】The Use of Levonorgestrel Implants (Norplant) for Contraception in Adolescent Mothers\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Although levonorgestrel implants (Norplant) would appear to be a good contraceptive option for adolescent mothers, there is little information about the use of Norplant in this population.\n\n【3】Methods\n-------\n\n【4】We studied 100 postpartum adolescents who chose a contraceptive method at an urban teaching hospital between September 1991 and July 1992. Structured interviews were conducted and medical records were reviewed soon after delivery and at a mean (±SD) of 15.5 ±2.9 months post partum.\n\n【5】Results\n-------\n\n【6】Forty-eight of the adolescent mothers chose Norplant, 50 chose oral contraceptives, and 2 (not further studied) chose barrier methods of contraception. The factors significantly associated with the choice of Norplant were older age (16.7 years among those who chose Norplant, vs. 16.2 years among those who chose oral contraceptives), multiparity (24 subjects vs. 6 subjects), and previous use of oral contraceptives (34 subjects vs. 21 subjects). During follow-up, there were no differences between the Norplant group and the oral-contraceptive group in the frequency of clinic visits (an average of 2.3 visits per subject in each group), failure to return after the postpartum visit (9 subjects vs. 11 subjects), or the incidence of sexually transmitted diseases (42 percent vs. 36 percent). At follow-up, 95 percent of the subjects in the Norplant group and 33 percent of those in the oral-contraceptive group were still using the method they had chosen (P<0.001). During the first postpartum year 1 subject in the Norplant group and 19 in the oral-contraceptive group became pregnant (P<0.001). Norplant users did not differ from all other adolescents studied with regard to sexual activity or condom use.\n\n【7】Conclusions\n-----------\n\n【8】The selection of Norplant by adolescent mothers as a method of contraception is associated with higher rates of continued use and lower rates of new pregnancy than the selection of oral contraceptives and does not affect the use of health care services, sexual activity, condom use, or the rate of sexually transmitted diseases.\n\n【9】Introduction\n------------\n\n【10】Despite efforts to decrease sexual activity and increase the use of effective contraceptive practices, 10 percent of adolescent girls in the United States become pregnant each year  . Among those who give birth, 16 to 30 percent are pregnant again within a year  . Whereas adolescents who have one child may eventually attain levels of education and income similar to those of their peers who delay childbearing, adolescents who have two or more children within five years are more likely to curtail their education, rely on welfare, and bear additional children in a short time  .\n\n【11】Although to date oral contraceptives have been the most popular and effective contraceptive method used by adolescents,  pregnancy rates among adolescents who use oral contraceptives range from 8 to 18 percent, as compared with 2 percent in adults  . From 45 to 66 percent of adolescents discontinue the use of oral contraceptives within 12 months  . Compounding the risk of pregnancy among adolescent oral-contraceptive users is the tendency to stop and start use, depending on sexual activity  .\n\n【12】Levonorgestrel implants (Norplant, Wyeth-Ayerst Laboratories, Philadelphia) would seem to provide a good contraceptive option for adolescent mothers. Norplant is safe, easy to use, effective, and does not require the cooperation of a partner or planning before intercourse  . However, the rate of acceptance of Norplant by adolescents is unknown. Irregular bleeding, fear of insertion, and visibility of the implant may deter them from using it.\n\n【13】It is also possible that some of the appealing characteristics of Norplant may lead to adverse health consequences for adolescents. Unlike oral-contraceptive users, Norplant users need not return for prescription refills and may miss opportunities to be screened for sexually transmitted diseases and cervical dysplasia. Furthermore, concern that the effectiveness of Norplant in preventing pregnancy may lead to decreased condom use among its users has led to considerable controversy over the suitability of the method for adolescents  . However, there are no data on actual condom use by Norplant users.\n\n【14】This study is an examination of the choice of Norplant and its use by inner-city adolescent mothers. Participants were followed prospectively from the time they chose a contraceptive post partum. The study objectives were to determine the factors associated with the choice of Norplant as compared with other contraceptive methods and to assess the effect of Norplant use on the use of health care services, sexual activity, additional pregnancy, and condom use.\n\n【15】Methods\n-------\n\n【16】Study Subjects\n--------------\n\n【17】The study protocol was approved by the institutional review boards of the Hospital of the University of Pennsylvania, the Family Planning Council of Southeastern Pennsylvania, and New York Hospital.\n\n【18】The study population consisted of all adolescents 17 years of age or younger who gave birth at the Hospital of the University of Pennsylvania from September 1991 through July 1992 and who were scheduled to return post partum to its family-planning clinic. Adolescents receiving postpartum care at outside sites were not included in the study. All the participants gave written informed consent.\n\n【19】Study Design\n------------\n\n【20】A family-planning counselor visited eligible adolescents before discharge from the hospital and presented Norplant as a contraceptive option during an individualized, nondirective counseling session. A structured interview and medical-record review were performed to determine sociodemographic variables; to obtain a history of contraception, sexually transmitted diseases, and pregnancy; to assess the degree of support available from the family; and to evaluate prenatal care. The interview included measures of beliefs about the acquired immunodeficiency syndrome (AIDS) (20 true-false items derived from the AIDS Beliefs Survey of the Children's Hospital of Philadelphia  ); knowledge about contraception (5 true-false and 5 two-item choice questions); attitude toward contraception (8 agree-disagree statements derived from the Perinatal HIV Reduction and Education Demonstration Project Questionnaire of the Family Planning Council of Southeastern Pennsylvania); and knowledge about sexually transmitted diseases (10 true-false-unsure items derived from the Initial Client Survey of the Family Planning Council).\n\n【21】The study participants were scheduled for a visit to the family-planning clinic two weeks post partum. At this visit, the chosen type of contraception was dispensed free of charge, and the participants were interviewed about sexual activity in the interval, concern about future pregnancy and contraception, and expectations about sexual activity and condom use. The participants completed the Rosenberg Self-Acceptance subscale  and the Coopersmith Family and Self-Esteem subscales,  which have been previously validated in studies of inner-city adolescents  .\n\n【22】Follow-up visits were scheduled for six weeks post partum and every six months thereafter. Participants using oral contraceptives for the first time were scheduled for an additional visit at three months. Follow-up gynecologic care was provided routinely. Screening for Chlamydia trachomatis (with Microtrak \\[Syva, Palo Alto, Calif.\\]), Neisseria gonorrhea (by culture), trichomonas (by wet smear), and syphilis (by the rapid-plasma-reagin test) was performed at the discretion of the providers at the family-planning clinic. Free condoms were dispensed, and condom use was encouraged regardless of the contraceptive method chosen.\n\n【23】In April 1993, the study participants were telephoned and interviewed about the continuation of contraceptive use, satisfaction with the method, any additional pregnancy, sexual activity, and condom use. The records of the family-planning clinic and the hospital were reviewed for visits during the interval, pregnancies, sexually transmitted diseases, and pelvic inflammatory disease, as diagnosed by hospital physicians using standard criteria.\n\n【24】Statistical Analysis\n--------------------\n\n【25】Data entry and analysis were performed with the Statistical Package for the Social Services on an IBM-compatible 486 computer. Statistical analyses included Student's t-test  for continuous variables and the chi-square test  for categorical variables. Ordinal values were ranked and compared with use of the Kruskal-Wallis test  ; their means are reported for descriptive purposes only. All P values reported are two-tailed, and a P value below 0.05 was considered to indicate statistical significance. Logistic regression  was used to identify the variables most predictive of a specific contraceptive choice. Variables significantly associated with the choice of contraceptive method on bivariate analysis were made eligible for logistic regression with the use of both forward and backward stepwise selection. Adjusted odds ratios and 95 percent confidence intervals were calculated from the coefficients of the variables in the model. Life-table analysis  was used to compare the outcomes of continued use of contraception and additional pregnancy. Data were aggregated monthly from the date of delivery, and outcomes were compared by the log-rank method for censored data according to the algorithm of Lee and Desu  .\n\n【26】Outcomes related to follow-up visits, additional pregnancies, and sexually transmitted diseases were determined for all participants. Outcomes related to continuation of and satisfaction with contraception, menstrual patterns, sexual activity, and condom use were determined only for participants who completed the telephone interviews.\n\n【27】Results\n-------\n\n【28】During the 10-month enrollment period, 166 adolescent girls gave birth; 130 (78 percent) received in-hospital contraceptive counseling and scheduled visits to the family-planning clinic post partum. Twelve of the 130 (9 percent) declined to participate in the study. Of the 118 subjects in the initial study sample, 100 returned for contraception -- 74 (63 percent) as scheduled, and 26 (22 percent) after telephone contact.\n\n【29】Forty-eight subjects chose Norplant, and 50 chose oral contraceptives. The two who chose foam and condoms were not studied further. Of the 98 subjects in the final study sample, 42 in the Norplant group and 42 in the oral-contraceptive group (86 percent of the sample) completed the follow-up telephone interview.\n\n【30】Base-Line Findings\n------------------\n\n【31】Table 1. Base-Line Characteristics of the Study Groups.\n\n【32】The study subjects were predominantly single, black, and poor and lived in single-parent households . The Norplant group was significantly older than the oral-contraceptive group. The two groups did not differ significantly with respect to grade in school, performance in school as they described it, rate of dropping out of school before 28 weeks' gestation, or plans to attend college.\n\n【33】As compared with the subjects in the oral-contraceptive group, the subjects in the Norplant group were more likely to have conceived and given birth before the index pregnancy and to have used some form of contraception, particularly oral contraceptives. The groups did not differ in their knowledge about, attitudes toward, or use of contraception at the time the index pregnancy was conceived.\n\n【34】The subjects in the Norplant group and those in the oral-contraceptive group did not differ significantly with respect to their own or their partners' desire for another pregnancy within the year; 37 and 38 percent, respectively, planned abortions if they conceived. There were no significant differences between the groups in the degree of perceived family support during pregnancy. The mean scores related to family (Norplant group, 4.6; oral-contraceptive group, 4.7), assertiveness (3.8 vs. 3.9), and self-acceptance (8.4 vs. 8.6) did not differ significantly between the groups and were similar to the scores reported for inner-city adolescents  .\n\n【35】The Norplant and oral-contraceptive groups did not differ with respect to rates of participation in the comprehensive prenatal program for adolescents or in the proportion who received additional antepartum contraceptive counseling in that program. Although the groups did not differ in regard to gestational age at their first prenatal visit, the subjects in the Norplant group had poorer compliance with prenatal visits and prenatal vitamin therapy. These associations between choice of method and prenatal compliance were not significant when the analyses were controlled for parity. Despite their higher parity and presumably greater need for child care, the subjects in the Norplant group were more likely than those in the oral-contraceptive group to return post partum for contraception as scheduled.\n\n【36】Table 2. Base-Line Sexual Activity, Condom Use, and Sexually Transmitted Diseases in the Study Groups.\n\n【37】There were no significant differences between the Norplant group and the oral-contraceptive group on any measure of sexual activity. There were also no differences in AIDS-belief scores, knowledge of sexually transmitted diseases, or rates of such disease during the index pregnancy . The diagnoses made in the two groups included infections with chlamydia (20 subjects) and trichomonas (15), gonorrhea (5), syphilis (3), condyloma infection (5), and herpes infection (3).\n\n【38】There were no significant differences in reported condom use at base line between the Norplant group and the oral-contraceptive group. Although condom use had previously been poor, all the subjects predicted that they would “definitely” use condoms in the future or that such use was “highly likely.”\n\n【39】Five variables significantly associated with the contraceptive method chosen (age, gravidity, parity, previous oral-contraceptive use, and compliance with postpartum visits) were made eligible for a logistic-regression model predicting the choice of Norplant. Of these variables, parity and previous oral-contraceptive use were entered into the model with adjusted odds ratios of 5.8 percent (95 percent confidence interval, 3.4 to 10.0) and 2.2 (95 percent confidence interval, 1.4 to 3.6), respectively.\n\n【40】When subjects in the Norplant group were asked why they chose Norplant, they reported difficulty remembering to take pills (71 percent), side effects of oral contraceptives (38 percent), fear of pregnancy (57 percent), ease of use of Norplant (48 percent), and encouragement from others (34 percent). When subjects in the oral-contraceptive group were asked why they did not choose Norplant, they reported a fear of insertion, needles, or both (55 percent); concern about irregular bleeding (24 percent); concern about other side effects (15 percent); and fear that the implant would be visible (9 percent).\n\n【41】Follow-up\n---------\n\n【42】Table 3. Follow-up of the Study Groups.\n\n【43】Subjects who chose Norplant did not differ significantly from those who chose oral contraceptives with respect to the duration of follow-up, the number of follow-up clinic visits, or the rate of return after the postpartum visit .\n\n【44】Figure 1. Life-Table Analysis of Continued Use of the Chosen Contraceptive Method during Follow-up.\n\n【45】Values shown are for 84 subjects who completed follow-up interviews. The percentages at time zero reflect the proportion of subjects choosing the method in question. Subsequent percentages reflect both subjects who discontinued the method and subjects who never used the method as prescribed.\n\n【46】Subjects in the Norplant group as compared with those in the oral-contraceptive group were more likely to be “very satisfied” with their choice of contraception (P<0.001). At follow-up, 95 percent of those who chose Norplant were still using that method, as compared with 33 percent of those who chose oral contraceptives; 24 percent of the latter group never started their pills. Other methods used at follow-up by subjects who received oral contraceptives post partum were no method (26 percent), Norplant (21 percent), condoms (17 percent), and medroxyprogesterone acetate (2 percent). In a life-table analysis , the probability that a specific method would be discontinued within six months post partum was 0 percent for the Norplant group as compared with 45 percent for the oral-contraceptive group (P<0.001).\n\n【47】The reasons given for discontinuing oral contraceptives were missing pills or refills (32 percent), side effects (24 percent), and sexual inactivity (7 percent). Among subjects actually using oral contraceptives, 83 percent reported regular periods. Reasons given for discontinuing Norplant were abnormal bleeding and hepatitis B infection (one subject each). Among Norplant users, 38 percent reported regular menses, 31 percent irregular infrequent bleeding (less than monthly), 24 percent irregular frequent bleeding, and 7 percent amenorrhea.\n\n【48】Figure 2. Life-Table Analysis of Pregnancy Status during Follow-up.\n\n【49】Percentages were determined for all 98 subjects in the final study sample. The time from the date of the index delivery to a new pregnancy was based on the best determination of the last menstrual period before the new pregnancy.\n\n【50】Nineteen subjects in the oral-contraceptive group (38 percent) and one subject in the Norplant group (2 percent) became pregnant during follow-up. The latter pregnancy was conceived shortly after the removal of the Norplant implant. These pregnancies resulted in six miscarriages, four elective abortions, four births, and six pregnancies ongoing at the end of the study. Five subjects in the oral-contraceptive group who became pregnant subsequently chose Norplant. In the life-table analysis , the risk of pregnancy within the first postpartum year was 2 percent in the Norplant group and 38 percent in the oral-contraceptive group (P<0.001).\n\n【51】Logistic regression was performed to determine whether any variables associated with a specific contraceptive choice independently predicted the outcome of pregnancy and the continuation of contraception. Age, gravidity, parity, previous oral-contraceptive use, and method chosen at base line were eligible for the analysis. Only the choice of Norplant at base line predicted the continuation of contraception, with an adjusted odds ratio of 6.9 (95 percent confidence interval, 4.6 to 10.3). Only the choice of oral contraceptives at base line predicted pregnancy, with an adjusted odds ratio of 5.2 (95 percent confidence interval, 3.1 to 8.8).\n\n【52】Sexually transmitted diseases were documented during follow-up in 42 percent of the Norplant group and 36 percent of the oral-contraceptive group (P not significant). The diagnoses included gonorrhea (15 subjects), chlamydia infection (18), herpes infection (3), trichomonas infection (10), syphilis (3), and hepatitis B (1). Two cases each of gonorrhea and chlamydia infection were second infections. Four subjects in the oral-contraceptive group and two in the Norplant group were treated for pelvic inflammatory disease.\n\n【53】Table 4. Sexual Activity Reported at Follow-up by the Norplant Users as Compared with Those Using Other Methods of Contraception. Table 5.  Table 5. Condom Use and Sexually Transmitted Diseases in Norplant Users as Compared with All Others at Follow-up.\n\n【54】Because the contraceptive method chosen at base line was not necessarily the method used at follow-up, the follow-up (currently used) method was used in the analysis of recent sexual activity  and condom use . There was no difference in recent sexual activity between current Norplant users and users of all other methods at follow-up. However, only 36 percent of the subjects using no method of contraception reported having intercourse in the past month, as compared with 80 percent of the Norplant users and 88 percent of those using other methods (P = 0.003). Condom use reported at follow-up did not differ significantly between subjects using Norplant and all other subjects. Of the nine subjects using condoms for contraception at follow-up, only two reported condom use “all the time.”\n\n【55】Discussion\n----------\n\n【56】Our study explores the factors and outcomes associated with postpartum use of Norplant by inner-city adolescent mothers, a group of particular importance because of their high rates of both pregnancy and sexually transmitted disease. The demographic homogeneity of our sample facilitated control for many factors that have confounded previous research on contraceptive use by adolescents.\n\n【57】Our findings suggest that Norplant will be a popular contraceptive option among adolescents who have had contraceptive failure and pregnancy. However, it appears that many adolescents will not consider Norplant until oral contraceptives have been tried and have failed. Until that point, fears about insertion and possible side effects are deterrents, suggesting that counseling about contraceptives for adolescents should include a discussion of the procedure for inserting Norplant.\n\n【58】The rate of continued use of Norplant in our population was similar to previously reported rates in adults  . However, when the removal of a contraceptive implant requires a provider's visit, rates of continued use may not necessarily reflect the user's satisfaction. Therefore, it was important that adolescent users of Norplant also reported high rates of satisfaction with the method and willingness to recommend it to others. The patterns of bleeding reported by users in our study were similar to those reported by adult users,  suggesting that adolescents tolerate the abnormalities of bleeding that are associated with Norplant use as well as adults.\n\n【59】In contrast, the adolescents who chose oral contraceptives had low rates of continued use and satisfaction with that method. An important consequence was a 38 percent pregnancy rate in the first postpartum year. The need for daily compliance with oral contraceptives, cited by 70 percent of Norplant users and one quarter of those who discontinued oral contraceptives as a reason for choosing Norplant, continues to be problematic for adolescents. Furthermore, oral-contraceptive users, who can easily discontinue their method, may be less likely to wait out the minor side effects of hormonal contraception than Norplant users, for whom discontinuation requires surgical removal of the implant.\n\n【60】We found that users of Norplant and oral contraceptives have similar rates of gynecologic follow-up. This suggests that interventions to improve adolescents' compliance with health care need not focus specifically on Norplant users. Moreover, the need to return for prescription refills does not appear to impart any particular advantage to oral-contraceptive use.\n\n【61】In our study, sexual activity did not differ between adolescents using Norplant and those using other methods. This finding is consistent with previous reports on the sexual behavior of teenagers  and provides important and reassuring information. As would be expected, teenagers reporting no use of contraception at follow-up tended to be those who reported no recent intercourse. Research has shown that adolescents who report regular intercourse use contraception more consistently than those who report sporadic activity  \\-- a finding that is supported in this study by a 30 percent pregnancy rate among the adolescents with less frequent sexual activity.\n\n【62】The good news is that Norplant use does not appear to decrease condom use by adolescent mothers. The bad news is that this population in general uses condoms poorly and has a high rate of sexually transmitted disease, regardless of the contraceptive method. Although Norplant cannot be singled out as a risk factor for poor condom use or for sexually transmitted diseases in adolescents, merely recommending condoms for contraception cannot be heralded as the solution to the problem, since even adolescents who reported using condoms for contraception used them inconsistently. Attention needs to be shifted from contraceptive methods as antecedents of high-risk sexual behavior and to be focused instead on interventions that will decrease such behavior and increase condom use among all teenagers, regardless of the contraceptive method they choose.\n\n【63】Although outcomes associated with pregnancy and continued use of the contraceptive method clearly differed between adolescent mothers who chose Norplant and those who chose oral contraceptives, the size of our sample may have limited our ability to detect smaller differences in sexual activity and condom use. In addition, attitudes specific to the program or the provider may have influenced the choice of contraceptive. Our study patients all had access to Norplant free of charge, but the effect of health insurance and prescription-drug coverage on the choice of this method is likely to be important in more diverse socioeconomic groups.\n\n【64】Our results provide information on the factors that influence the choice of Norplant and on the ramifications of its use by inner-city adolescent mothers. Although our findings may not be generalizable to adolescents who have never been pregnant or to adolescents from other social groups, they should allow more objective discourse by family-planning providers, community leaders, policy makers, and the public about contraceptive use by sexually active adolescents.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6737c714-ffba-44c1-9a5f-ea256a585474", "title": "Medical Progress: Recent Advances in Head and Neck Cancer", "text": "【0】Medical Progress: Recent Advances in Head and Neck Cancer\nThere are more than half a million incident cases of squamous-cell carcinoma of the head and neck worldwide each year, primarily affecting the oropharynx, oral cavity, hypopharynx, and larynx. This review considers the biologic features of these tumors, including the role of human papillomavirus as a risk factor for cancer of the oropharynx. New therapies are considered, along with management approaches.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f448d59f-ce62-432b-81e9-7d8587bbc115", "title": "Hemolytic Anemia after Eculizumab in Paroxysmal Nocturnal Hemoglobinuria", "text": "【0】Hemolytic Anemia after Eculizumab in Paroxysmal Nocturnal Hemoglobinuria\nTo the Editor:\n--------------\n\n【1】Table 1. Clinical Data from the Patient, According to the Timing of Therapy.\n\n【2】A 68-year-old man was referred to our hospital in 2008 because of a progressive onset of profound asthenia. Macrocytic anemia was evident with signs of hemolysis and recurrent hyperchromic urine. Inherited and immune-mediated mechanisms were ruled out, and paroxysmal nocturnal hemoglobinuria (PNH) was diagnosed on the basis of the absence of CD55 and CD59 expression on leukocytes. The patient was initially treated with supportive therapy, including blood transfusions, low-dose glucocorticoids (initial dose, 50 mg per day of prednisone, gradually tapered to a maintenance dose of 12.5 mg per day), erythropoietin (30,000 IU per week), iron, and folic acid. Nonetheless, after 18 months of this treatment, the hemoglobin level progressively decreased, accompanied by headache and abdominal pain. Treatment with eculizumab, a monoclonal antibody that binds to the C5 component of complement and inhibits terminal complement activation, was started, and prednisone was gradually discontinued. There was a rapid and clear improvement in hematologic values and quality of life. Two months later, signs of hemolysis reappeared, with a new finding of positive results on a direct antiglobulin test (DAT) (C3d fraction). Glucocorticoid therapy was reintroduced at an initial dose of 75 mg per day, in association with eculizumab, with satisfactory results. Clinical data for the patient are shown in Table 1 .\n\n【3】Figure 1. Flow Cytometric Analysis of C3d Fraction Deposition on a Proportion of CD55-Negative Red Cells from a Patient with Paroxysmal Nocturnal Hemoglobinuria (PNH).\n\n【4】Shown are the absence of C3d fraction on normal red cells (i.e., CD55-positive) and its presence on a substantial proportion of PNH red cells (i.e., CD55-negative). FITC denotes fluorescein isothiocyanate, and PE phycoerythrin.\n\n【5】This report provides further evidence that extravascular hemolysis can develop in patients with PNH who are receiving eculizumab, despite initial good control of intravascular hemolysis.  This relapse mimics the clinical and laboratory features of de novo autoimmune hemolytic anemia. However, the pathogenetic mechanism may instead depend on the accumulation of C3 fragments on red cells under the influence of eculizumab. To confirm this hypothesis, we documented by flow cytometry that C3 binding was confined to the PNH (i.e., CD55-negative) clone . PNH red cells, which lack CD55 because of a somatic mutation in the X-linked gene encoding phosphatidylinositol glycan anchor biosynthesis class A ( _PIGA_ ), are not protected from complement activation and subsequent intravascular hemolysis . Eculizumab inhibits C5 cleavage, allowing the cells to survive long enough to accumulate C3. This determines DAT positivity and extravascular hemolysis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "aaf7f7d3-6155-4a38-bea6-de0c482f2a72", "title": "Case 7-2013 — A 77-Year-Old Woman with Long-Standing Unilateral Thoracic Pain and Incontinence", "text": "【0】Case 7-2013 — A 77-Year-Old Woman with Long-Standing Unilateral Thoracic Pain and Incontinence\nA 77-year-old woman sought neurosurgical and neurologic consultation because of a long history of left thoracic pain, for which no cause or effective treatment had been found. Review of imaging studies revealed multiple perineurial cysts in the thoracic spine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "98f9488f-a5f9-4bbb-8ba9-efaee3b49079", "title": "Bone-Density Testing Interval and Transition to Osteoporosis in Older Women", "text": "【0】Bone-Density Testing Interval and Transition to Osteoporosis in Older Women\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Although bone mineral density (BMD) testing to screen for osteoporosis (BMD T score, −2.50 or lower) is recommended for women 65 years of age or older, there are few data to guide decisions about the interval between BMD tests.\n\n【3】Methods\n-------\n\n【4】We studied 4957 women, 67 years of age or older, with normal BMD (T score at the femoral neck and total hip, −1.00 or higher) or osteopenia (T score, −1.01 to −2.49) and with no history of hip or clinical vertebral fracture or of treatment for osteoporosis, followed prospectively for up to 15 years. The BMD testing interval was defined as the estimated time for 10% of women to make the transition to osteoporosis before having a hip or clinical vertebral fracture, with adjustment for estrogen use and clinical risk factors. Transitions from normal BMD and from three subgroups of osteopenia (mild, moderate, and advanced) were analyzed with the use of parametric cumulative incidence models. Incident hip and clinical vertebral fractures and initiation of treatment with bisphosphonates, calcitonin, or raloxifene were treated as competing risks.\n\n【5】Results\n-------\n\n【6】The estimated BMD testing interval was 16.8 years (95% confidence interval \\[CI\\], 11.5 to 24.6) for women with normal BMD, 17.3 years (95% CI, 13.9 to 21.5) for women with mild osteopenia, 4.7 years (95% CI, 4.2 to 5.2) for women with moderate osteopenia, and 1.1 years (95% CI, 1.0 to 1.3) for women with advanced osteopenia.\n\n【7】Conclusions\n-----------\n\n【8】Our data indicate that osteoporosis would develop in less than 10% of older, postmenopausal women during rescreening intervals of approximately 15 years for women with normal bone density or mild osteopenia, 5 years for women with moderate osteopenia, and 1 year for women with advanced osteopenia. \n\n【9】Introduction\n------------\n\n【10】Current osteoporosis management guidelines  recommend routine bone mineral density (BMD) screening with the use of dual-energy x-ray absorptiometry (DXA) scans for women 65 years of age or older, but no guidelines specify an osteoporosis screening interval that is based on data from longitudinal cohort studies. The U.S. Preventive Services Task Force stated in 2011, “Because of limitations in the precision of testing, a minimum of 2 years may be needed to reliably measure a change in BMD; however, longer intervals may be necessary to improve fracture risk prediction.”  To our knowledge, no U.S. study has addressed this clinical uncertainty.\n\n【11】A previous prospective analysis  of data from the Study of Osteoporotic Fractures (SOF) suggested that repeating a BMD measurement up to 8 years after the initial screening provided little additional value beyond the initial BMD screening results for predicting new fractures in elderly women. A 2009 longitudinal analysis  involving 1008 women in Australia who were 60 years of age or older suggested that age and baseline T score were important factors to consider in determining a BMD testing interval, with the goal of detecting low BMD before the onset of a fragility fracture. However, neither of these studies estimated BMD testing intervals to identify osteoporosis (as defined by BMD criteria) before a major fracture occurred; instead, they used fracture or fracture combined with osteoporosis as the outcome.\n\n【12】To determine how the BMD testing interval relates to the timing of the transition from normal BMD or osteopenia to the development of osteoporosis before a hip or clinical vertebral fracture occurs, we conducted competing-risk analyses of data from 4957 women, 67 years of age or older, who did not have osteoporosis at baseline and who were followed longitudinally for up to 15 years in the SOF. The BMD testing interval was defined as the estimated time during which osteoporosis developed in 10% of women before they had a hip or clinical vertebral fracture and before they received treatment for osteoporosis. We expected women with osteopenia at baseline to have a more rapid transition to osteoporosis than women with normal T scores at baseline.\n\n【13】Methods\n-------\n\n【14】Study Participants\n------------------\n\n【15】Figure 1. Study Populations for Analyses of BMD Transitions.\n\n【16】Of the 8514 women who underwent dual-energy x-ray absorptiometry (DXA) for measurement of bone mineral density (BMD), 3557 were excluded from the study, including those who had osteoporosis (T score at the femoral neck or total hip, −2.50 or lower), those who had a past hip or clinical vertebral fracture or who received treatment for osteoporosis by the time of their first BMD measurement, and those who did not undergo BMD measurements at the femoral neck and total hip at two or more examinations. In the analytic cohort of 4957 women with adequate BMD measurements of the femoral neck and total hip before data censoring, two transitions were studied: normal BMD to osteoporosis (1255 women) and osteopenia to osteoporosis (4215 women). A total of 513 women made the transition from normal BMD to osteopenia and had at least one subsequent examination with BMD recorded; data from these women were included in both analyses.\n\n【17】The SOF cohort included 9704 ambulatory women (more than 99% of whom were white; race was self-reported), 65 years of age or older, recruited between 1986 and 1988 from population-based listings at four U.S. sites: Baltimore, Minneapolis, the Monongahela Valley near Pittsburgh, and Portland, Oregon.  Women with bilateral hip replacements were excluded from the study. All participants provided written informed consent. The follow-up period included study examinations at year 2 (1989–1990), year 6 (1992–1994), year 8 (1995–1996), year 10 (1997–1999), and year 16 (2002–2004). Details of the study examinations and the selection of the analytic cohort (4957 women) are described in the Supplementary Appendix  and in Figure 1 . The analysis protocol was approved by the institutional review board of the University of North Carolina. The SOF study protocol was approved by the institutional review boards at all participating sites .\n\n【18】Outcomes\n--------\n\n【19】The primary outcomes were the estimated intervals for 10% of participants to make the transition from normal BMD or osteopenia at baseline to osteoporosis before a hip or clinical vertebral fracture occurred and before treatment for osteoporosis was initiated.\n\n【20】Risk Factors for Fracture\n-------------------------\n\n【21】Several clinical risk factors for fracture, including components of the FRAX fracture risk assessment tool,  were covariates in the time-to-event analyses, including age, body-mass index (BMI, the weight in kilograms divided by the square of the height in meters), estrogen use at baseline, any fracture after 50 years of age, current smoking, current or past use of oral glucocorticoids, and self-reported rheumatoid arthritis (with a missing value classified as absence of disease).\n\n【22】Statistical Analysis\n--------------------\n\n【23】Competing risk analyses were conducted to estimate the cumulative incidence functions for the time to the development of osteoporosis before a hip or clinical vertebral fracture and before initiation of treatment for osteoporosis and the corresponding intervals for 10% of participants to make the transition from normal BMD or osteopenia to osteoporosis — that is, the cumulative incidence quantile as defined by Peng and Fine.  The participants were stratified into four groups according to the T-score range (lowest T score at femoral neck or total hip): normal BMD (T score, −1.00 or higher), mild osteopenia (T score, −1.01 to −1.49), moderate osteopenia (T score, −1.50 to −1.99), and advanced osteopenia (T score, −2.00 to −2.49). Parametric cumulative incidence curves for the time to osteoporosis were estimated from log–logistic-regression models for the cumulative incidence function based on interval-censored data.  The baseline for each participant (and her time origin, which was the first study examination at which a BMD measurement was recorded, in each primary analysis) was the first study examination that showed normal BMD or osteopenia, with follow-up continuing until the study examination that preceded death or withdrawal from the study. Incident hip or clinical vertebral fractures and the first reported use, before the development of osteoporosis, of a Food and Drug Administration–approved agent for the treatment of osteoporosis (i.e., bisphosphonate, calcitonin, or raloxifene) were treated as competing risks, whether or not there was a subsequent transition to osteoporosis, with the data coded as in the study by Hudgens et al.  for naive likelihood analyses of parametric cumulative incidence regression with interval censoring, in which each cumulative incidence function is analyzed separately.\n\n【24】Two primary analyses were conducted: one for the transition from normal BMD to osteoporosis (1255 women) and the second for the transition from osteopenia to osteoporosis (4215 women); 513 women in whom the transition from normal BMD to osteopenia occurred before their last BMD test were included in both analyses. For statistically significant clinical risk factors in the models, we conducted stratified analyses of the estimated time for 10% of women to make the transition to osteoporosis.\n\n【25】Two sensitivity analyses were conducted. First, the testing interval was redefined as the estimated time for 20% of women to make the transition from osteopenia to osteoporosis, or for 1%, 2%, or 5% of women to make the transition from normal BMD to osteoporosis. (Sensitivity analyses could not be conducted for thresholds between 1% and 5% among women with advanced osteopenia or for a 20% threshold among women with normal BMD, because the resulting time extrapolations were respectively shorter than the minimum and longer than the maximum follow-up times.) Second, we repeated the primary analyses using the secondary definition of osteoporosis, based only on the BMD at the femoral neck.\n\n【26】To better study women who had a fracture without first making the transition to osteoporosis, as defined by diagnostic criteria from the World Health Organization (WHO), and without first receiving treatment for osteoporosis, we also calculated the time for 2% of women to have a hip or clinical vertebral fracture in competing risk analyses of data from the same study population stratified according to the four T-score ranges. In the fracture analyses, the first reported use of bisphosphonate, calcitonin, or raloxifene and the first documentation of osteoporosis before fracture were treated as competing risks,  and data were censored for death or withdrawal from the study with the use of the approach of Hudgens et al. \n\n【27】All analyses were performed with the use of SAS software, version 9.2 (SAS Institute).  Further details of the statistical analysis are provided in the Supplementary Appendix .\n\n【28】Results\n-------\n\n【29】Characteristics of the Study Cohort\n-----------------------------------\n\n【30】Table 1. Characteristics of the 4957 Study Participants.\n\n【31】Baseline characteristics of the 4957 women who participated in the analysis are shown in Table 1 . Within each T-score range, the numbers of women in whom osteoporosis developed during the follow-up period were as follows: normal BMD, 10 of 1255 women (0.8%); mild osteopenia, 64 of 1386 (4.6%); moderate osteopenia, 309 of 1478 (20.9%); and advanced osteopenia, 841 of 1351 (62.3%).\n\n【32】Estimated BMD Testing Intervals\n-------------------------------\n\n【33】Figure 2. Unadjusted Cumulative Incidence of Osteoporosis According to Baseline T-Score Range.\n\n【34】The proportion of women who had a transition to osteoporosis is shown as a function of time. The cumulative incidence curves were estimated by means of parametric cumulative incidence models for interval-censored data. The dashed horizontal line marks the 10% threshold for the transition to osteoporosis; where this line intersects each cumulative incidence curve, a vertical dashed line to the x axis marks the estimated testing interval. The analysis of women with osteopenia at baseline is based on three T-score groups and included the 513 women who made the transition from normal BMD to osteopenia and had at least one subsequent examination with BMD recorded.Table 2.  Table 2. Interval between Baseline Testing and the Development of Osteoporosis in 10% of Study Participants, According to the Result of Baseline Testing.\n\n【35】Unadjusted estimates  and covariate-adjusted estimates of the cumulative incidence of osteoporosis as a function of testing interval were similar. The times for 10% of women without osteoporosis to make the transition to osteoporosis increased with higher baseline T scores at the hip. The adjusted estimates for women with normal BMD and for those with mild osteopenia at baseline were very similar (16.8 years \\[time conservatively estimated for the lowest BMD in the normal range\\] and 17.3 years, respectively)  . The adjusted estimates were 4.7 years for women with moderate osteopenia and 1.1 years for those with advanced osteopenia. For women with osteopenia at baseline, T-score group, age, BMI, current estrogen use, and the interaction of T-score group by BMI were significant predictors in the final model (P<0.02). The other covariates — any fracture after 50 years of age, current smoking, previous or current use of oral glucocorticoids, and self-reported rheumatoid arthritis — were not significant predictors (all P>0.20).\n\n【36】Table 3. Interval between Baseline Osteopenia and the Development of Osteoporosis in 10% of Study Participants, According to Age, BMI, and Estrogen-Use Status.\n\n【37】Within a given T-score range, the estimated time for the transition from osteopenia to osteoporosis was longer with younger age  . For example, among women with moderate osteopenia, the estimated BMD testing interval was approximately 5 years for women who were 70 years old and approximately 3 years for those who were 85 years old. The estimated transition time was also longer for women who were taking estrogen at baseline, as compared with those who had either taken estrogen in the past or never taken it. Among women with mild osteopenia, testing intervals were longer than 14 years for all BMI values evaluated. A higher BMI was associated with a slightly longer testing interval among women with advanced osteopenia (P<0.001 for trend), but all estimated intervals were close to 1 year (range, 0.8 to 1.3). For all BMI values evaluated, women with moderate osteopenia had an estimated testing interval of approximately 4.5 years. There was no significant association between BMI and the time to the development of osteoporosis for women with moderate osteopenia at baseline (P=0.51 for trend).\n\n【38】When the testing interval was redefined as the estimated times for 20% of women to make the transition from osteopenia to osteoporosis, the time estimates were approximately 80% longer (8.5 years and 2.0 years for women with moderate and advanced osteopenia, respectively), as compared with corresponding estimates based on a 10% transition threshold. In a sensitivity analysis in which we used the secondary definition of osteoporosis, based on the BMD at the femoral neck alone, the covariate-adjusted times for 10% of women to make the transition to osteoporosis were 1.0 years for women with advanced osteopenia, 4.7 years for those with moderate osteopenia, and more than 15 years for those with mild osteopenia or normal BMD. Although these estimates were similar to the estimates in the primary analysis (which was based on the BMD at the total hip or femoral neck) for women with osteopenia, the time estimate in this sensitivity analysis for women with normal BMD was more than twice as long as that in the primary analysis, and it was much longer than the maximum follow-up time of 15 years.\n\n【39】A total of 121 women (2.4%) had a hip or clinical vertebral fracture before the transition to osteoporosis, as defined by WHO diagnostic criteria, or before the receipt of treatment for osteoporosis. The adjusted estimated time for 2% of women to have a hip or clinical vertebral fracture was more than 15 years for women with normal BMD or mild osteopenia, and approximately 5 years for those with moderate or advanced osteopenia. Complete results of the sensitivity analyses are presented in the Supplementary Appendix .\n\n【40】Discussion\n----------\n\n【41】We conducted a study of rates of transition to osteoporosis in order to help clinicians decide on BMD testing intervals for older women with normal BMD or osteopenia at the initial assessment. Our results suggest that the baseline T score is the most important determinant of a BMD testing interval. During the 15-year study period, less than 1% of women with T scores indicating normal BMD and 5% of women with T scores indicating mild osteopenia at their first assessment made the transition to osteoporosis, with an estimated testing interval of about 15 years for 10% of women in each of these groups to make the transition. This finding suggests that if BMD testing is deferred for 15 years among women with T scores greater than −1.50, there is a low likelihood of a transition to osteoporosis during that period. We found that 10% of women with moderate osteopenia and 10% of women with advanced osteopenia made the transition to osteoporosis in 5 years and 1 year, respectively. Although clinical risk factors had a minimal effect on the time estimates as a whole, a significant trend for age supported shorter testing intervals as women age. The estimated time for only 2% of women to make the transition to hip or clinical vertebral fracture before the development of osteoporosis was 5 years for women with moderate or advanced osteopenia and at least 15 years for women with mild osteopenia or normal BMD. Thus, with the use of the stated criteria for the study, consideration of the time to a hip or clinical vertebral fracture would not substantially alter recommendations for osteoporosis screening intervals based on the time to osteoporosis alone.\n\n【42】Recent controversy over the harms of excessive screening for other chronic diseases  reinforces the importance of developing a rational screening program for osteoporosis that is based on the best available evidence rather than on health care marketing, advocacy, and public beliefs that have encouraged overtesting and overtreatment in the United States.  Our findings provide evidence-based estimates for an osteoporosis screening interval before new hip or clinical vertebral fractures and before initiation of treatment for osteoporosis. Our results are consistent with those of Hillier et al.,  suggesting that frequent BMD testing is unlikely to improve fracture prediction, and with those of Frost et al.,  suggesting that age and T score are key factors in determining a reasonable interval for BMD testing. Our study extends their findings by estimating the transition time to osteoporosis before a hip or clinical vertebral fracture, with the goal of treating osteoporosis to reduce the risk of such fractures, which account for the majority of fracture-related complications among older adults.\n\n【43】Several features of our analysis will assist clinicians in making decisions regarding osteoporosis screening intervals. Clinicians might feel impelled to shorten the BMD screening interval for patients with osteopenia who have clinical risk factors for fracture. Our estimates for BMD testing intervals proved to be robust after adjustment for major clinical risk factors. However, clinicians may choose to reevaluate patients before our estimated screening intervals if there is evidence of decreased activity or mobility, weight loss, or other risk factors not considered in our analyses. As expected, the estimated time to osteoporosis decreased with increasing age, so that an interval of 3 years, instead of 5 years, might be considered for women 85 years of age or older who have moderate osteopenia. Although the trends for BMI and estrogen use were also significant, they were less clinically relevant. If 10 years were to be considered the maximum testing interval for any woman, the BMI would not alter the recommendations for the testing intervals for each T-score range (based on a comparison of the time estimates in Table 3 vs. those in Table 2 ). Current estrogen use, as compared with use of estrogen in the past or no history of estrogen use, was significantly associated with higher BMD and a longer testing interval. These results were consistent with the finding of BMD loss after discontinuation of hormone therapy in the Postmenopausal Estrogen/Progestin Interventions (PEPI) trial   and a SOF analysis suggesting that previous hormone therapy does not provide protection against hip fracture.  Because of the transient effect of estrogen on BMD, we do not recommend modifying the screening interval on the basis of estrogen use.\n\n【44】Our study had several limitations. First, our testing interval was based only on BMD transitions, with adjustment for risk factors for fracture; the potential benefits and risks of screening and its cost-effectiveness were not considered. Second, owing to limitations imposed by the data set, precise time estimates were not possible for the following analyses: 5% threshold for women with advanced osteopenia at baseline, 20% threshold for women with normal BMD or mild osteopenia at baseline , and outcome for BMD at the femoral neck among women with normal BMD at baseline . Third, 49% of the original SOF participants (4747 of 9704 women) were excluded from our analysis. About half the excluded women were not eligible for screening because they had osteoporosis at baseline, had a history of a hip or clinical vertebral fracture, or had received treatment for osteoporosis at baseline; the remaining women had too few DXA examinations to be followed longitudinally. However, the mean age and mean baseline T scores in our analytic cohort were similar to those for all SOF participants who had any DXA scans of the hip. Fourth, our analysis was limited to women 67 years of age or older; different results might have been obtained from analyses that included younger postmenopausal women or men. Finally, white women accounted for more than 99% of our sample. However, because the prevalence of osteoporosis of the hip among white women is equal to or slightly higher than the prevalence among nonwhite women by estimates from the National Health and Nutrition Examination Survey,  the testing intervals we calculated are likely to be reasonable estimates for women of all races.\n\n【45】The strengths of our analysis include the large size of the cohort and the long follow-up period. Repeated BMD testing during the long follow-up period allowed precise estimation of testing intervals from event times (i.e., time to the development of osteoporosis) that were unknown, up to a time interval, for every woman in the study sample.\n\n【46】In conclusion, our results suggest that osteoporosis would develop in less than 10% of older, postmenopausal women during screening intervals that are set at approximately 15 years for women with normal bone density or mild osteopenia (T score, greater than −1.50) at the initial assessment, 5 years for women with moderate osteopenia (T score, −1.50 to −1.99), and 1 year for women with advanced osteopenia (T score, −2.00 to −2.49).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "243db83c-8f19-402e-8317-cfe04dd1ff50", "title": "Effects of Leukocyte Interferon on Clinical Symptoms and Hormone Levels in Patients with Mid-Gut Carcinoid Tumors and Carcinoid Syndrome", "text": "【0】Effects of Leukocyte Interferon on Clinical Symptoms and Hormone Levels in Patients with Mid-Gut Carcinoid Tumors and Carcinoid Syndrome\nAbstract\n--------\n\n【1】We treated nine patients who had carcinoid tumors of the small intestine, six of whom had the carcinoid syndrome, with daily intramuscular doses of leukocyte inferieron — 3X10  U per day for one month and 6X10  U per day for another two months. Seven patients had previously been treated with streptozocin and fluorouracil, without benefit.\n\n【2】Treatment with inferieron ameliorated the manifestations of the carcinoid syndrome and led to prompt and continuing decreases in urinary levels of 5-hydroxyindoleacetic acid and serum levels of human chorionic gonadotropin subunits and pancreatic polypeptide in all six patients with liver metastases, but it had no clear effect in two of three patients with only lymph-node involvement. After the treatment period, five of the six responders had relapses in clinical manifestations and increases in hormone levels.\n\n【3】We conclude that interferon is of benefit in treating metastatic small intestinal carcinoid tumors in patients with the carcinoid syndrome.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9de0f4e1-70a9-4875-9eed-7968849816c1", "title": "Acquired Resistance to Bedaquiline and Delamanid in Therapy for Tuberculosis", "text": "【0】Acquired Resistance to Bedaquiline and Delamanid in Therapy for Tuberculosis\nTo the Editor:\n--------------\n\n【1】Multidrug-resistant tuberculosis (MDR-TB) and extensively drug-resistant tuberculosis (XDR-TB) are an increasing public health threat.  Bedaquiline and delamanid are two drugs that were recently approved by the Food and Drug Administration for treatment of MDR-TB and XDR-TB.  Here we describe the stepwise amplification of drug resistance in a patient who had emigrated from Tibet to Switzerland in December 2010 and who presented to a Swiss hospital with preextensively drug-resistant tuberculosis at that time.\n\n【2】Figure 1. Clinical Features, Treatment History, Amplification of Drug Resistance, and Phenotypic Drug-Susceptibility Testing in the Patient.\n\n【3】Mutations that confer drug resistance are shown. Resistance to bedaquiline, intravenous capreomycin, and delamanid developed. Percentages on the y axis are based on the number of genome-sequencing reads supporting the corresponding drug resistance–conferring mutation (details are provided in Table S4 in the Supplementary Appendix ). Large colored circles indicate measurements, and dashed lines linking these circles indicate the changes in mutation frequency. The loss of the start codon is indicated with an asterisk, and fs indicates frame shift. The mutations listed on the left side of the graph were detected in the first and all subsequent isolates obtained from the patient. All these mutations were seen in 100% of sequencing reads in each isolate. X indicates the stop codon, and del indicates deletion. On the x axis, isolate numbers are listed under the month when the isolate was obtained. TMP-SMX denotes trimethoprim–sulfamethoxazole.\n\n【4】Genome sequencing revealed that the initial _Mycobacterium tuberculosis_ isolate harbored nine mutations in genes associated with resistance to seven drugs . The isolate also showed a compensatory mutation in _rpoC,_  indicating a “mature” preextensively drug-resistant strain that had evolved under drug pressure for some time. Given that the patient reported no previous treatment for tuberculosis, he was probably infected with a strain that was already resistant to these drugs.\n\n【5】In September 2011, bedaquiline was added to the regimen, which had consisted of four drugs (ethambutol, aminosalicylic acid, intravenous capreomycin, and cycloserine). The patient was considered to be clinically cured in March 2013, but he had a relapse in August 2013. Genome sequencing of five follow-up isolates revealed a mutation in _mmpR_ that was associated with bedaquiline resistance. This mutation persisted even though bedaquiline was discontinued in February 2012; this suggests that it did not cause any clinically significant reduction in the virulence of the infecting bacteria. \n\n【6】Additional resistance to second-line injectable agents (such as capreomycin) also developed. This was reflected in the emergence of mutations in _tlyA_ and _rrs_ . The latter mutation remained at low frequency and was detected only in drug-containing bacterial cultures . All the molecular findings were supported by phenotypic drug-susceptibility testing.\n\n【7】Following this amplification in resistance, delamanid was added to the regimen in March 2014. However, two mutations in _fbiA_ and _fgd1_ increased in frequency by June 2014, which coincided with the emergence of phenotypic resistance to delamanid. These genes have previously been associated with delamanid resistance. The _fgd1_ mutation decreased in frequency thereafter, indicating the presence of multiple delamanid-resistant clones in the patient. In August and September 2014, the patient underwent a lobectomy. After surgery, the sputum specimens obtained from the patient were culture-negative, and the patient received treatment on an ambulatory basis.\n\n【8】This case highlights the development of resistance in the context of inadequate MDR-TB and XDR-TB treatment regimens, despite personalized patient care in a well-resourced health care setting. It serves as a warning for the future rollout of new antituberculosis drugs and emphasizes the need for the use of appropriate companion drugs when bedaquiline and delamanid are administered. Our results add to previous findings showing that the development of drug resistance is a dynamic process involving multiple heterogeneous populations of bacteria within individual patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e14ce0e2-fa62-48e7-a5b4-7839ff9a9e4b", "title": "The Efficacy of Azidothymidine (AZT) in the Treatment of Patients with AIDS and AIDS-Related Complex", "text": "【0】The Efficacy of Azidothymidine (AZT) in the Treatment of Patients with AIDS and AIDS-Related Complex\nAbstract\n--------\n\n【1】We conducted a double-blind, placebo-controlled trial of the efficacy of oral azidothymidine (AZT) in 282 patients with the acquired immunodeficiency syndrome (AIDS) manifested by _Pneumocystis carinii_ pneumonia alone, or with advanced AIDS-related complex. The subjects were stratified according to numbers of T cells with CD4 surface markers and were randomly assigned to receive either 250 mg of AZT or placebo by mouth every four hours for a total of 24 weeks. One hundred forty-five subjects received AZT, and 137 received placebo.\n\n【2】When the study was terminated, 27 subjects had completed 24 weeks of the study, 152 had completed 16 weeks, and the remainder had completed at least 8 weeks. Nineteen placebo recipients and 1 AZT recipient died during the study (P<0.001). Opportunistic infections developed in 45 subjects receiving placebo, as compared with 24 receiving AZT. The base-line Karnofsky performance score and weight increased significantly among AZT recipients (P<0.001). A statistically significant increase in the number of CD4 cells was noted in subjects receiving AZT (P<0.001). After 12 weeks, the number of CD4 cells declined to pretreatment values among AZT recipients with AIDS but not among AZT recipients with AIDS-related complex. Skin-test anergy was partially reversed in 29 percent of subjects receiving AZT, as compared with 9 percent of those receiving placebo (P<0.001).\n\n【3】These data demonstrate that AZT administration can decrease mortality and the frequency of opportunistic infections in a selected group of subjects with AIDS or AIDS–related complex, at least over the 8 to 24 weeks of observation in this study.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5c06e949-dc40-4862-baf0-fe8cd76be0d2", "title": "A Trial of Itraconazole or Amphotericin B for HIV-Associated Talaromycosis", "text": "【0】A Trial of Itraconazole or Amphotericin B for HIV-Associated Talaromycosis\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】_Talaromyces marneffei_ infection is a major cause of human immunodeficiency virus (HIV)–related death in South and Southeast Asia. Guidelines recommend initial treatment with amphotericin B deoxycholate, but this drug has substantial side effects, a high cost, and limited availability. Itraconazole is available in oral form, is associated with fewer unacceptable side effects than amphotericin, and is widely used in place of amphotericin; however, clinical trials comparing these two treatments are lacking.\n\n【3】Methods\n-------\n\n【4】In this open-label, noninferiority trial, we randomly assigned 440 HIV-infected adults who had talaromycosis, confirmed by either microscopy or culture, to receive either intravenous amphotericin B deoxycholate (amphotericin) (219 patients), at a dose of 0.7 to 1.0 mg per kilogram of body weight per day, or itraconazole capsules (221 patients), at a dose of 600 mg per day for 3 days, followed by 400 mg per day, for 11 days; thereafter, all the patients received maintenance therapy with itraconazole. The primary outcome was all-cause mortality at week 2. Secondary outcomes included all-cause mortality at week 24, the time to clinical resolution of talaromycosis, early fungicidal activity, relapse of talaromycosis, development of the immune reconstitution inflammatory syndrome (IRIS), and the side-effect profile.\n\n【5】Results\n-------\n\n【6】The risk of death at week 2 was 6.5% in the amphotericin group and 7.4% in the itraconazole group (absolute risk difference, 0.9 percentage points; 95% confidence interval \\[CI\\], −3.9 to 5.6; P<0.001 for noninferiority); however, the risk of death at week 24 was 11.3% in the amphotericin group and 21.0% in the itraconazole group (absolute risk difference, 9.7 percentage points; 95% CI, 2.8 to 16.6; P=0.006). Treatment with amphotericin was associated with significantly faster clinical resolution and fungal clearance and significantly lower rates of relapse and IRIS than itraconazole. The patients who received amphotericin had significantly higher rates of infusion-related reactions, renal failure, hypokalemia, hypomagnesemia, and anemia than patients in the itraconazole group.\n\n【7】Conclusions\n-----------\n\n【8】Amphotericin was superior to itraconazole as initial treatment for talaromycosis with respect to 6-month mortality, clinical response, and fungicidal activity. \n\n【9】Introduction\n------------\n\n【10】The dimorphic fungus _Talaromyces_ (previously _Penicillium_ ) _marneffei_ causes a life-threatening mycosis in immunocompromised persons living in or traveling to Southeast Asia, China, and India.  Talaromycosis (previously penicilliosis) is a major cause of human immunodeficiency virus (HIV)–related death; its prevalence is surpassed only by the prevalence of tuberculosis and cryptococcosis,  and it leads to 4 to 15% of HIV-related hospital admissions in regions in which the disease is endemic.  Talaromycosis is increasingly diagnosed among patients who are not infected with HIV but who have other immunodeficiency conditions  and is reported to be the second most common cause of all bloodstream infections in southern Vietnam.  Despite a mortality rate of up to 30% among affected patients who receive antifungal therapy,  randomized trials evaluating treatment strategies are lacking. International guidelines recommend initial (induction) therapy with amphotericin B deoxycholate (amphotericin), at a dose of 0.7 to 1 mg per kilogram of body weight per day for 2 weeks, followed by itraconazole at a dose of 400 mg per day for 10 weeks,  on the basis of a noncomparative trial in Thailand that showed clinical resolution in 72 of 74 patients (97.3%) who received treatment according to this regimen.  Amphotericin has substantial renal, hematologic, and infusion-related toxic effects for which patients typically receive in-hospital care and close monitoring. In addition, access to amphotericin is hampered by cost and by inadequate supply chains in Asia. In Vietnam, a 2-week course of amphotericin costs approximately $350 (in U.S. dollars), excluding the costs of hospitalization and laboratory monitoring. In contrast, itraconazole is associated with fewer unacceptable side effects, is widely available, is available in oral form, and costs one seventh the price of amphotericin. According to large, retrospective case series from China, India, and Vietnam, itraconazole is the most commonly used treatment for talaromycosis and is similar to amphotericin with respect to clinical responses and mortality. \n\n【11】We hypothesized that itraconazole would be noninferior to amphotericin as induction therapy for talaromycosis, with the advantages of fewer toxic effects, lower cost, availability as oral treatment that can be administered on an outpatient basis, and wider availability. To test this hypothesis, we conducted the Itraconazole versus Amphotericin B for Penicilliosis (IVAP) trial, a randomized, open-label, controlled trial that was powered to assess survival, in five trial centers in Vietnam.\n\n【12】Methods\n-------\n\n【13】Trial Design and Oversight\n--------------------------\n\n【14】From October 2012 through December 2015, we enrolled patients at five referral hospitals located in the Vietnam provinces that have the highest prevalence of HIV.  Eligible patients were HIV-infected adults (≥18 years of age) who had talaromycosis that was confirmed by either microscopy or culture. Exclusion criteria were pregnancy, central nervous system involvement assessed either clinically or by analyses of cerebrospinal fluid, an allergy to either itraconazole or amphotericin, or the concomitant use of certain medications that interact with either itraconazole or amphotericin, an alanine aminotransferase or aspartate aminotransferase level of more than 400 U per liter, an absolute neutrophil count of less than 500 per cubic millimeter, a creatinine clearance of less than 30 ml per minute (calculated by the method of Cockcroft and Gault), a concurrent diagnosis of cryptococcal meningitis, concurrent treatment with rifampicin, or previous treatment for talaromycosis for more than 48 hours. Written informed consent was obtained from all the patients or their representatives.\n\n【15】The protocol was approved by the independent ethics committee at each participating hospital, by the Vietnam Ministry of Health, and by the Oxford University Tropical Research Ethics Committee. The Vietnam Ministry of Health and an independent data monitoring and ethics committee oversaw the safety of the trial. The data monitoring and ethics committee performed a formal interim analysis after the enrollment of every 100 patients or after every 20 deaths. A trial steering committee supervised and monitored the progress of the trial. The investigators and associated research personnel collected and maintained the data. The trial statistician performed the final analysis. The trial was sponsored by the Medical Research Council, the Department for International Development, and the Wellcome Trust in the United Kingdom. The sponsors were not involved in the design or implementation of the protocol, the analysis of the data, or the preparation or review of the manuscript. All the authors made the decision to submit the manuscript for publication and vouch for the accuracy and completeness of the data and analyses and for the fidelity of the trial to the protocol. Additional details about enrollment and monitoring of the trial can be found in Sections 3 and 4, respectively, in the Supplementary Appendix .\n\n【16】Treatment\n---------\n\n【17】In this open-label trial, patients were randomly assigned in a  ratio to receive treatment for 14 days with either intravenous amphotericin B deoxycholate (amphotericin) (purchased from Cipla, India) at a dose of 0.7 to 1.0 mg per kilogram per day or itraconazole capsules (purchased from Stada, Vietnam) at a dose of 300 mg twice daily for the first 3 days, followed by 200 mg twice daily for the remaining 11 days. Thereafter, all the patients received itraconazole at a dose of 200 mg twice daily for 10 weeks, followed by itraconazole at a dose of 100 mg twice daily until their CD4+ cell counts were higher than 100 cells per cubic millimeter for at least 6 months while they were receiving antiretroviral therapy (ART). Patients were instructed to take itraconazole immediately after a full meal or acidic beverage. All the patients received prophylactic treatment with trimethoprim–sulfamethoxazole for _Pneumocystis jiroveci_ pneumonia. Patients who had not previously received ART were referred to HIV clinics for the initiation of first-line ART, which consisted of tenofovir, lamivudine, and efavirenz, administered according to the national guidelines. All the patients were hospitalized during the initial 2-week period of the trial, during which time administration of therapy was directly observed. Adherence to itraconazole was assessed during all the follow-up visits by means of patient-reported estimates of the number of missed doses during the preceding month.\n\n【18】Randomization\n-------------\n\n【19】Randomization was stratified according to trial center. A computer-generated randomization list with block sizes of 4 or 6 was prepared for each center by the trial pharmacist, who had no clinical involvement in the trial. The randomization lists, which were incorporated into a Web-based program that was accessible by authorized trial investigators, showed only the next assigned treatment. All the transactions on the Web server were logged, unchangeable, and auditable.\n\n【20】Assessments\n-----------\n\n【21】Patients were assessed daily during the first 2-week inpatient period and then monthly for 6 months in outpatient clinics. Side effects were monitored clinically and with the use of hematologic, chemical, and liver-enzyme testing performed at least twice weekly. All the patients were screened for tuberculosis by means of sputum microscopy. The number of fungal colony-forming units (CFUs) per milliliter of blood was measured on days 1 through 4 and every other day thereafter until the cultures became negative or until the patient was discharged from the hospital. The method for quantifying fungal CFUs was adapted from the method that has been used to quantify cryptococcal CFUs in cerebrospinal fluid  and is described in detail in Section 6 in the Supplementary Appendix .\n\n【22】Outcomes\n--------\n\n【23】The primary outcome measure was all-cause mortality, which was defined as the absolute risk of death from any cause during the first 2 weeks after randomization. The secondary outcome measures were mortality at week 24, the time to clinical resolution of talaromycosis, early fungicidal activity, relapse of talaromycosis, the development of the immune reconstitution inflammatory syndrome (IRIS), and the incidence of adverse events of grade 3 or higher. Clinical resolution of talaromycosis was defined as a temperature of less than 38°C (100°F) for 3 days, resolution of skin lesions, and sterile blood cultures. Early fungicidal activity was defined as the rate of decline in blood _T. marneffei_ CFUs from serial blood cultures obtained during the first 2 weeks. Relapse of talaromycosis was defined as the recurrence of symptoms and a positive fungal culture from any sterile site that led to reinduction of therapy in patients who had achieved clinical resolution. IRIS was defined as unexpected worsening of symptoms associated with inflammation in patients who started ART and had increasing CD4+ cell counts. Adverse events were graded according to the 2009 National Institutes of Health Division of AIDS Grading of Adverse Events, version 1.0.  An independent expert review committee whose members were unaware of the treatment assignments adjudicated the IRIS and relapse events, assessed whether there was a causal relationship between serious adverse events and trial drugs, and assessed whether a patient’s death was due to talaromycosis.\n\n【24】Statistical Analysis\n--------------------\n\n【25】Assuming a 15% risk of death during the first 2 weeks (on the basis of data from the Hospital for Tropical Disease in southern Vietnam between 2009 and 2011  ), we calculated that a sample size of 440 patients would provide the trial with 80% power to show the noninferiority of itraconazole to amphotericin at a one-sided alpha level of 0.025 with the use of a noninferiority margin of 10 percentage points for the absolute risk difference (detailed justification is provided in Section 3.7.5 of the protocol ). All the analyses were specified before the release of the randomization list, as detailed in the protocol and the statistical analysis plan. In brief, we estimated the risk of death at 2 weeks and at 24 weeks with the use of the Kaplan–Meier method. Confidence intervals and tests for the absolute risk difference were based on standard errors calculated with the use of Greenwood’s formula. Cumulative mortality by week 24 was analyzed with the use of a Cox proportional-hazards model. The time to clinical resolution of talaromycosis and the time to relapse of talaromycosis and to the development of IRIS were analyzed with the use of competing-risks methods (cumulative incidence functions and Fine–Gray regression models) to account for the competing risk of death without a previous event. Cause-specific death was analyzed similarly. Declines in log <sub>10 </sub> \\-transformed longitudinal measurements of fungal CFUs in blood were estimated for each patient with the use of a least-squares regression model. The comparison of declines in fungal CFUs between the two treatment groups was based on linear regression, with treatment as the main covariate and with adjustment for the baseline log <sub>10 </sub> fungal count. Statistical analyses were performed with the use of the R statistical package, version 3.3.1 . \n\n【26】Results\n-------\n\n【27】Trial Population\n----------------\n\n【28】Figure 1. Enrollment, Randomization, and Follow-up.\n\n【29】ALT denotes alanine aminotransferase, ANC absolute neutrophil count, AST aspartate aminotransferase, and CNS central nervous system.\n\n【30】Of the 573 patients screened, 440 patients underwent randomization; 219 patients were assigned to the amphotericin group and 221 to the itraconazole group . A total of 5 patients were excluded from the intention-to-treat analysis (as defined in the protocol) because the initial diagnosis of talaromycosis that was confirmed by microscopy was incorrect. An additional 8 patients were excluded from the modified intention-to-treat analysis because they did not receive the assigned intervention, and 8 other patients were excluded from the per-protocol analysis because of violations of the inclusion or exclusion criteria, withdrawal from the trial, or receipt of the trial drug for fewer than 7 days . In 998 of 1067 outpatient visits (93.5%) in the amphotericin group and 984 of 1038 outpatient visits (94.8%) in the itraconazole group, the number of missed doses of itraconazole as maintenance therapy was recorded as zero.\n\n【31】Patient Characteristics\n-----------------------\n\n【32】Table 1. Clinical and Laboratory Characteristics of the Patients at Baseline.\n\n【33】Baseline characteristics were similar in the two treatment groups, with the exception of the duration of ART (which was approximately 1 month longer in the amphotericin group than in the itraconazole group) . The median CD4+ cell count at enrollment was 10 cells per cubic millimeter. Skin lesions were present in 80.0% of the patients; the skin smear was positive for _T. marneffei_ in 90.1% of the patients, and the skin culture was positive in 87.6% of the patients. The blood culture was positive for _T. marneffei_ in 70.0% of the patients; the median number of fungal CFUs in blood was 2.35 log <sub>10 </sub> CFUs per milliliter.\n\n【34】Primary Outcome\n---------------\n\n【35】Table 2. Overview of Primary and Secondary Outcomes.\n\n【36】Key trial outcomes are summarized in Table 2 . In the intention-to-treat analysis, 14 of 217 patients (6.5%) in the amphotericin group and 16 of 218 patients (7.4%) in the itraconazole group died by week 2 (absolute risk difference, 0.9 percentage points; 95% confidence interval \\[CI\\], −3.9 to 5.6; P<0.001 for noninferiority). The findings in the modified intention-to-treat and per-protocol populations were similar to those in the intention-to-treat population. No evidence of heterogeneity of effect was observed in the prespecified subgroups of the intention-to-treat population, which were defined according to baseline characteristics, including injection-drug use, ART status, CD4+ cell counts, fungemia, the number of fungal CFUs, dyspnea for which the patient received oxygen, and oropharyngeal ulcers . A logistic-regression analysis  identified a higher number of fungal CFUs at baseline as an independent predictor of death in the first 2 weeks (odds ratio per each additional 1 log <sub>10 </sub> CFU per milliliter, 2.18; 95% CI, 1.43 to 3.45; P<0.001).\n\n【37】Secondary Outcomes\n------------------\n\n【38】### _Mortality by Week 24_\n\n【39】Figure 2. Cumulative Incidence of Death by Week 24 and Fungal Colony-Forming Units in Blood.\n\n【40】Panel A shows the cumulative incidence of death over the course of 24 weeks in the two groups, estimated with the use of the Kaplan–Meier method. The estimated risks of death at week 2 (the primary outcome) were 6.5% in the amphotericin group and 7.4% in the itraconazole group. At week 24, the estimated risks of death were 11.3% in the amphotericin group and 21.0% in the itraconazole group. The inset shows the same data on an expanded y axis. Panel B shows all recorded quantitative fungal colony-forming units (CFUs) in blood among patients who had detectable fungal CFUs at enrollment (143 patients in the amphotericin group and 148 patients in the itraconazole group) from the day of randomization (day 1) until day 15. The gray lines indicate data for individual patients. The blue lines represent a linear interpolation of observed mean log <sub>10 </sub> CFUs for each day after linear extrapolation of fungal CFU measurements for each patient through day 15 (to avoid overrepresentation of patients who did not yet have sterile cultures at late time points). The decrease in blood fungal CFUs during the first 14 days was significantly slower among patients in the itraconazole group than among patients in the amphotericin group: −0.95 log <sub>10 </sub> CFUs per milliliter per day in the amphotericin group versus −0.36 log <sub>10 </sub> CFUs per milliliter per day in the itraconazole group (estimated difference, 0.52; 95% CI, 0.41 to 0.63; P<0.001).\n\n【41】By week 24, a total of 24 of the 217 patients (11.3%) in the amphotericin group and 45 of the 218 patients (21.0%) in the itraconazole group had died (absolute risk difference, 9.7 percentage points; 95% CI, 2.8 to 16.6; P=0.006). The results were consistent in the modified intention-to-treat and per-protocol populations . The Kaplan–Meier curves showed a separation of the two treatment groups after 8 weeks, and the risk of death continued to increase in the itraconazole group until week 24 . No clear evidence of heterogeneity of effect was seen in the prespecified subgroups of the intention-to-treat population; however, there was evidence of a time-varying effect of the treatment assignment, whereby treatment with itraconazole increased the risk of death primarily during weeks 9 through 16 (hazard ratio, 11.20; 95% CI, 1.45 to 86.76; P=0.02) and during weeks 17 through 24 (hazard ratio, 8.55; 95% CI, 1.07 to 68.36; P=0.04) . A multivariable Cox regression model identified treatment with itraconazole (hazard ratio, 1.86; 95% CI, 1.09 to 3.19; P=0.02) and the number of baseline fungal CFUs (hazard ratio, 1.45; 95% CI, 1.13 to 1.86; P=0.004) as independent predictors of death over the course of 24 weeks . Among the patients in the trial who had not previously received ART, 101 of 124 patients (81.5%) in the amphotericin group and 96 of 123 patients (78.0%) in the itraconazole group initiated ART during the course of the trial after a median of 23 days (interquartile range, 16 to 36) in the amphotericin group and after a median of 22 days (interquartile range, 15 to 31) in the itraconazole group.\n\n【42】### _Time to Clinical Resolution_\n\n【43】The time to clinical resolution of talaromycosis was 8 days (interquartile range, 6 to 11) in the amphotericin group, as compared with 9 days (interquartile range, 6 to 12) in the itraconazole group (P=0.049). Poor response to treatment, which was defined as persistent or worsening symptoms and a positive culture after day 7, was observed in 1 patient who was receiving amphotericin and in 13 patients who were receiving itraconazole; 9 of these 13 patients were switched to amphotericin by the treating clinician.\n\n【44】### _Early Fungicidal Activity_\n\n【45】Figure 2B shows the results of a longitudinal assessment of fungal CFUs in patients with fungemia (143 patients in the amphotericin group and 148 patients in the itraconazole group). Early fungicidal activity up to day 14 was significantly greater in the amphotericin group than in the itraconazole group: a decrease of 0.95 log <sub>10 </sub> CFUs per milliliter per day versus a decrease of 0.36 log <sub>10 </sub> CFUs per milliliter per day (estimated difference, 0.52; 95% CI, 0.41 to 0.63; P<0.001). A post hoc analysis also showed that more patients in the amphotericin group than in the itraconazole group had sterile cultures by day 8 (118 of 119 patients \\[99.2%\\] vs. 78 of 115 patients \\[67.8%\\], P<0.001). Further exploratory joint modeling of longitudinal fungal CFUs and cumulative mortality showed that the number of fungal CFUs at baseline (hazard ratio per each additional 1.0 log <sub>10 </sub> CFUs per milliliter, 1.49; 95% CI, 1.09 to 2.00) and early fungicidal activity (hazard ratio per 0.1 log <sub>10 </sub> less decrease in CFUs per milliliter, 1.20; 95% CI, 1.07 to 1.36) were associated with mortality up to 24 weeks, independent of treatment assignment.\n\n【46】### _Relapse of Talaromycosis and Development of IRIS_\n\n【47】A relapse of talaromycosis and the development of IRIS were more common in the itraconazole group than in the amphotericin group. Talaromycosis relapse occurred in 15 of 218 patients (7.0%) in the itraconazole group as compared with 3 of 217 patients (1.5%) in the amphotericin group (absolute risk difference, 5.4 percentage points; 95% CI, 1.6 to 9.3; P=0.005). IRIS developed in 14 of 218 patients (6.6%) in the itraconazole group versus 0 of 217 patients in the amphotericin group (absolute risk difference, 6.6 percentage points; 95% CI, 3.2 to 9.9; P<0.001).\n\n【48】### _Adverse Events_\n\n【49】Table 3. Overview of Clinical and Laboratory Adverse Events (Grade 3 or Higher) by Week 24.\n\n【50】Clinical adverse events of grade 3 or higher (i.e., adverse events that were considered to be severe or life-threatening or that resulted in death) were reported in 229 patients (52.6%); the incidence was similar in the two groups . However, a higher number of patients in the itraconazole group than in the amphotericin group had complications from talaromycosis, including relapse (15 patients vs. 3 patients, P=0.006), poor response to treatment (13 patients vs. 1 patient, P=0.002), and IRIS (14 patients vs. no patients, P<0.001). In contrast, a higher number of patients in the amphotericin group than in the itraconazole group had infusion-related reactions (49 patients vs. 1 patient, P<0.001), blood and lymphatic disorders (16 patients vs. 3 patients, P=0.002), and renal failure (10 patients vs. 1 patient, P=0.006) . New acquired immunodeficiency syndrome (AIDS)–associated stage III or IV diseases were reported in 80 patients (18.4%), and the incidence was similar in the two groups, with tuberculosis being the most common of these events (67.5% of patients). Adverse events that led to a change in the patient’s randomized treatment were reported in 10 patients (4.6%) in the amphotericin group and in 9 patients (4.1%) in the itraconazole group (P=0.82). New abnormalities in laboratory variables were termed laboratory adverse events and were defined as any worsening of a laboratory value to grade 3 or 4 (including changes from grade 3 to grade 4) as compared with the patient’s previous laboratory value. Laboratory adverse events were significantly more common among patients who received amphotericin than among patients who received itraconazole, including anemia (hemoglobin level of <7.4 g per deciliter) (89 patients vs. 64 patients, P=0.01), hypokalemia (potassium level of <2.4 mmol per liter) (25 patients vs. 7 patients, P<0.001), and hypomagnesemia (magnesium level of <0.44 mmol per liter) (10 patients vs. 2 patients, P=0.02) .\n\n【51】Serious adverse events were reported in 158 patients (36.3%) and were more common among patients who received itraconazole than among those who received amphotericin: 100 of 218 patients (45.9%) versus 58 of 217 patients (26.7%) (P<0.001) . A total of 32 of the 218 patients (14.7%) in the itraconazole group and 14 of the 217 patients (6.5%) in the amphotericin group were judged by the expert review committee to have serious adverse events that were possibly, probably, or definitely related to the trial drugs (P=0.007).\n\n【52】Discussion\n----------\n\n【53】In this randomized, controlled trial of treatment for talaromycosis, we found that itraconazole induction therapy was noninferior to amphotericin with respect to the primary outcome of death at week 2; however, this effect was soon lost, and by week 24, the risk of death in the itraconazole group was almost twice that in the amphotericin group (21.0% vs. 11.3%). Consistent with this higher risk of death, the time to clinical resolution of talaromycosis was longer and more cases of relapse and IRIS were reported among patients who received itraconazole than among patients who received amphotericin. The differences in the risk of death were not evident during the first 8 weeks of follow-up but became evident during weeks 9 to 24. This finding suggests that the between-group difference in early fungicidal activity had an ongoing effect on disease progression and that a substantial portion of the deaths were directly due to talaromycosis. Concomitant opportunistic infections, such as tuberculosis, occurred with similar frequency in the two treatment groups and are unlikely to explain the differences between the two groups in risk of death and in complications of talaromycosis. The previous case series did not show a difference in outcomes between patients receiving amphotericin and those receiving itraconazole  ; these results may in part be a reflection of insufficient durations of follow-up. Our trial emphasizes the importance of prolonged follow-up in trials of antifungal treatment.\n\n【54】We assessed quantitative fungal cultures and rates of decline of fungal CFUs in blood, which showed that treatment with amphotericin was associated with early fungicidal activity that was almost four times as fast as that with itraconazole, with 99.2% fungemia clearance by day 8 in the amphotericin group as compared with 67.8% in the itraconazole group. The greater early fungicidal activity observed with amphotericin treatment provides a biologically plausible explanation for the mortality benefit of amphotericin. Similar relationships between cerebrospinal fluid fungal clearance and clinical outcomes have been observed among patients with HIV-associated cryptococcal meningitis.  Furthermore, we found baseline fungal CFUs in blood to be an independent predictor of mortality at weeks 2 and 24, and early fungicidal activity to be associated with mortality at 24 weeks, independent of the antifungal treatment regimen. These data suggest that early fungicidal activity may be an appropriate biomarker to screen antifungal activity of new treatment regimens for talaromycosis.\n\n【55】Unsurprisingly, infusion-related reactions, elevated creatinine levels, electrolyte disturbance, and anemia were more common in the amphotericin group than in the itraconazole group; however, few of these events were classified as serious adverse events, and the number of patients in the amphotericin group who had a change in therapy was similar to that in the itraconazole group. All the patients in our trial received prehydration and preemptive electrolyte supplementation in accordance with World Health Organization guidelines for amphotericin administration,  which may explain the small number of serious toxic effects associated with amphotericin. Shorter courses of 5 to 7 days of amphotericin have been shown to substantially reduce its toxic effects,  and data from studies in animals and in humans have shown that fungicidal responses with 3-to-5-day courses of amphotericin are similar to those with 14-day courses for the treatment of cryptococcal meningitis.  Our data also suggest that 8 days of amphotericin treatment resulted in sterile cultures in almost all the patients; thus, there is scope to shorten the duration of amphotericin treatment for talaromycosis. A shorter course of treatment, which would further reduce the number of toxic effects, the duration of hospitalization, and costs, warrants further studies.\n\n【56】A limitation of our trial was its open-label design, which may have introduced bias in the assessment of adverse events and clinical resolution. However, we reduced this risk by using the objective outcomes of mortality and fungal clearance. The assessments of relapse of talaromycosis, the development of IRIS, the relationship between serious adverse events and trial drugs, and the causes of death were reviewed by members of an independent expert committee whose members were unaware of the treatment assignments. This trial was pragmatic, in that the formulations of the treatments assessed were affordable and the trial had few exclusion criteria; these factors may help to enhance the potential generalizability of the trial across Asia.\n\n【57】In conclusion, this trial showed that the antifungal drug amphotericin B was superior to itraconazole as induction therapy for HIV-associated talaromycosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "aa5b4833-5df0-40c7-8b64-2935b56d860d", "title": "Tissue Matching before the Era of HLA Typing", "text": "【0】Tissue Matching before the Era of HLA Typing\nTo the Editor:\n--------------\n\n【1】Figure 1. Early Tissue Typing and Donor Selection — The “Third Man” Technique.\n\n【2】The patient's graft is on the left, the brother's in the center, and the mother's on the right.\n\n【3】In the summer of 1961, I “volunteered” for $50 to receive skin grafts in order to help determine the most suitable bone marrow donor for a teenage girl who was severely anemic as a result of treatment with chloramphenicol. I noted that the mother's graft was rejected in a manner similar to that of the patient's, whereas the brother's graft lasted several more days before rejection occurred and closely retained its larger size and shape . I was told that the girl did not live to receive a transplant. I have carried the scars as a medical curiosity for 40 years.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a032d511-1a16-454c-94b8-b3e44fca8837", "title": "Phase 2 Trial of Baxdrostat for Treatment-Resistant Hypertension", "text": "【0】Phase 2 Trial of Baxdrostat for Treatment-Resistant Hypertension\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Aldosterone synthase controls the synthesis of aldosterone and has been a pharmacologic target for the treatment of hypertension for several decades. Selective inhibition of aldosterone synthase is essential but difficult to achieve because cortisol synthesis is catalyzed by another enzyme that shares 93% sequence similarity with aldosterone synthase. In preclinical and phase 1 studies, baxdrostat had  selectivity for enzyme inhibition, and baxdrostat at several dose levels reduced plasma aldosterone levels but not cortisol levels.\n\n【3】Methods\n-------\n\n【4】In this multicenter, placebo-controlled trial, we randomly assigned patients who had treatment-resistant hypertension, with blood pressure of 130/80 mm Hg or higher, and who were receiving stable doses of at least three antihypertensive agents, including a diuretic, to receive baxdrostat (0.5 mg, 1 mg, or 2 mg) once daily for 12 weeks or placebo. The primary end point was the change in systolic blood pressure from baseline to week 12 in each baxdrostat group as compared with the placebo group.\n\n【5】Results\n-------\n\n【6】A total of 248 patients completed the trial. Dose-dependent changes in systolic blood pressure of −20.3 mm Hg, −17.5 mm Hg, −12.1 mm Hg, and −9.4 mm Hg were observed in the 2-mg, 1-mg, 0.5-mg, and placebo groups, respectively. The difference in the change in systolic blood pressure between the 2-mg group and the placebo group was −11.0 mm Hg (95% confidence interval \\[CI\\], −16.4 to −5.5; P<0.001), and the difference in this change between the 1-mg group and the placebo group was −8.1 mm Hg (95% CI, −13.5 to −2.8; P=0.003). No deaths occurred during the trial, no serious adverse events were attributed by the investigators to baxdrostat, and there were no instances of adrenocortical insufficiency. Baxdrostat-related increases in the potassium level to 6.0 mmol per liter or greater occurred in 2 patients, but these increases did not recur after withdrawal and reinitiation of the drug.\n\n【7】Conclusions\n-----------\n\n【8】Patients with treatment-resistant hypertension who received baxdrostat had dose-related reductions in blood pressure. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nBaxdrostat for Treatment-Resistant Hypertension  \n\n【11】Elevated blood pressure, the leading global risk factor for cardiovascular disease, stroke, disability, and death, affects an estimated 1.4 billion persons worldwide.  In the United States, approximately 10% of persons with hypertension (10 to 12 million persons) have treatment-resistant hypertension, which is defined as elevated blood pressure despite concurrent use of at least three antihypertensive drugs of different classes, including a diuretic.  Persons with treatment-resistant hypertension have a substantially increased risk of cardiovascular and renal adverse events. \n\n【12】Patients with treatment-resistant hypertension are often prescribed at least four antihypertensive agents,  with a goal (in the United States) of an in-office systolic blood pressure of less than 130 mm Hg and diastolic blood pressure of 80 mm Hg or less.  Current guidelines recommend the addition of spironolactone, a mineralocorticoid receptor antagonist, as a fourth-line agent despite common, dose-limiting adverse effects.  A total of 40 to 50% of patients with hypertension remain inadequately treated,  and yet no new class of antihypertensive medication has been approved since 2007. \n\n【13】Aldosterone synthase inhibitors target a likely cause of treatment resistance by suppressing hormone synthesis rather than by blocking the mineralocorticoid receptor. Preclinical and phase 1 studies have shown that baxdrostat has high selectivity (selectivity ratio,  for aldosterone synthase as compared with the enzyme required for cortisol synthesis, 11β-hydroxylase, which shares 93% sequence similarity with aldosterone synthase.  In the current trial, we examined the efficacy and safety of baxdrostat in patients with treatment-resistant hypertension.\n\n【14】Methods\n-------\n\n【15】Trial Design and Population\n---------------------------\n\n【16】BrigHTN, a multicenter, randomized, double-blind, placebo-controlled, parallel-group, dose-ranging trial, had an adaptive design. The trial enrolled men and women who were 18 years of age or older, were receiving stable doses of at least three antihypertensive medications (one of which was a diuretic), and had a mean blood pressure of at least 130/80 mm Hg while seated. Blood pressure was determined as the average of three measurements obtained with the use of an automated in-office blood-pressure monitor.\n\n【17】Key exclusion criteria were a mean seated systolic blood pressure of at least 180 mm Hg or a diastolic blood pressure of at least 110 mm Hg, an estimated glomerular filtration rate (GFR) of less than 45 ml per minute per 1.73 m  of body-surface area, and uncontrolled diabetes. To be eligible for the trial, patients who had been receiving a mineralocorticoid receptor antagonist or a potassium-sparing diuretic were required to discontinue these agents for a total of 4 weeks before randomization.\n\n【18】The trial registration was submitted to ClinicalTrials.gov on August 17, 2020. Although the first patient in our trial underwent screening before that date, no patients were enrolled before the trial registration was submitted or accepted. After a screening period of up to 8 weeks, eligible patients entered a 2-week, single-blind run-in period to determine whether medication adherence was a factor in not attaining their blood-pressure goal. Patients with at least 70% adherence (determined on the basis of pill counts) to each antihypertensive medication and placebo during the run-in period and a seated blood pressure of at least 130/80 mm Hg were randomly assigned to receive either 0.5 mg, 1 mg, or 2 mg of baxdrostat or matching placebo. The doses of baxdrostat were selected on the basis of decreases in aldosterone levels that had been measured in our previous multiple-ascending-dose study.  After randomization, clinical visits were conducted on days 3, 8, 15, 22, 43, 64, and 85 (the last day of the treatment period), and a follow-up telephone call occurred 1 week after the last dose . Additional details regarding the trial design are provided in the Supplementary Appendix .\n\n【19】BrigHTN was conducted primarily at community-based practices in the United States, as well as at a small number of academic hospital–based practices. The trial was designed by CinCor Pharma, CinRx Pharma, and external scientific advisers and funded by CinCor Pharma. The first draft of the manuscript was written by the first and last authors. A professional writer paid by the sponsor assisted the authors in the preparation of the manuscript. All the authors made the decision to submit the manuscript for publication.\n\n【20】The trial was conducted in accordance with the principles of the Declaration of Helsinki and the Good Clinical Practice guidelines of the International Council for Harmonisation. All the clinical sites that participated in the trial obtained approval from an institutional review board at the site that was authorized to approve studies involving humans, and all patients provided written informed consent before enrollment in the trial. An independent data monitoring committee performed a formal unblinded interim analysis when approximately 200 patients had completed the 12-week treatment period. Details regarding the interim analysis are provided in the Supplementary Appendix .\n\n【21】End Points\n----------\n\n【22】The primary efficacy end point was the change in the mean seated systolic blood pressure from baseline to the end of the 12-week treatment period in each baxdrostat group as compared with the placebo group. The secondary end points were the change from baseline in the mean seated diastolic blood pressure and the percentage of patients with a seated blood pressure of less than 130/80 mm Hg at the end of the 12-week treatment period. No adjustments were made for multiplicity in the analysis of the secondary outcomes; hence, the confidence intervals should not be used in place of hypothesis tests.\n\n【23】The safety end points included adverse events, vital signs, and the results of laboratory tests, electrocardiography, and physical examinations. Prespecified adverse events of special interest were hyperkalemia, hyponatremia, and hypotension warranting clinical intervention. Patients with potassium levels between 5.5 and 5.9 mmol per liter were contacted and asked to return for a repeat electrolyte measurement as soon as feasible, but they continued to receive baxdrostat or placebo. In contrast, patients with potassium levels between 6.0 and 6.4 mmol per liter were directed to discontinue baxdrostat or placebo and return for reevaluation as soon as feasible.\n\n【24】Exploratory pharmacokinetic and pharmacodynamic analyses were performed to assess the levels of plasma baxdrostat, serum and urinary aldosterone, and serum cortisol as well as plasma renin activity. Details are provided in the Supplementary Appendix .\n\n【25】Statistical Analysis\n--------------------\n\n【26】We estimated that a sample of 77 patients in each group would provide the trial with at least 80% power to detect a difference in the mean (±SD) seated systolic blood pressure of 5±11 mm Hg after the 12-week treatment period between each of the three baxdrostat groups and the placebo group at a two-sided significance level of 0.05. This estimate was based on the assumption of a 13% early withdrawal rate, and an initial enrollment of 348 patients (87 per trial group) was planned. To prevent imbalance between the trial groups, patients were stratified according to their baseline systolic blood pressure (<145 mm Hg or ≥145 mm Hg) and their baseline estimated GFR (<60 ml per minute per 1.73 m  or ≥60 ml per minute per 1.73 m  ).\n\n【27】The primary efficacy analysis compared the change in the mean seated systolic blood pressure from baseline to the end of the treatment period between each baxdrostat group and the placebo group in the modified intention-to-treat population (patients who received at least one dose of baxdrostat or placebo and had a baseline value for the systolic blood-pressure assessment). We used linear regression for a repeated-measures model and an unstructured covariance matrix, and we assumed that missing blood-pressure values were missing at random. The model used change from baseline as the dependent variable and included fixed effects for treatment, visit, and treatment-by-visit interaction, including covariates of the baseline systolic blood pressure and estimated GFR. Restricted maximum likelihood estimation was used with an unstructured covariance matrix and the Kenward–Roger approximation for degrees of freedom. To preserve the overall alpha level for the primary end point, a fixed-sequence testing procedure was performed with the highest-dose group comparison tested first.\n\n【28】The secondary efficacy end point of change from baseline in diastolic blood pressure was analyzed with the use of linear regression for a repeated-measures model like that used in the primary efficacy analysis. A sensitivity analysis was performed for missing data for the primary end point, with the use of a control-based pattern imputation model and the assumption that the data were not missing at random.\n\n【29】Results\n-------\n\n【30】Patient Characteristics\n-----------------------\n\n【31】Figure 1. Screening, Randomization, and Follow-up.\n\n【32】Patients who received at least one dose of baxdrostat or placebo and who had a baseline systolic blood-pressure measurement were included in the modified intention-to-treat population. The efficacy analyses were based on the intention-to-treat approach. The primary reasons for trial discontinuation were withdrawal of consent and loss to follow-up. A total of 248 patients completed the 12-week treatment period.Table 1.  Table 1. Demographic and Clinical Characteristics of the Patients at Baseline.\n\n【33】A total of 779 patients underwent screening, and 360 were included in the placebo run-in period. A total of 275 patients were randomly assigned to receive once-daily baxdrostat at a dose of 0.5 mg (69 patients), 1 mg (70 patients), or 2 mg (67 patients), or placebo (69 patients). One patient who was randomly assigned to receive baxdrostat in the 1-mg group never received the drug, so the modified intention-to-treat population included 274 patients . The trial groups were similar with respect to demographic and clinical characteristics at baseline. The trial population was predominantly White (70%). Black patients constituted a higher percentage of patients in the trial (28%) than their proportionate representation in the U.S. population . The trial included a sizable percentage of patients who identified as Hispanic (43%) and a small percentage who identified as Asian (2%) .\n\n【34】The first patient underwent screening on July 30, 2020, and the last patient visit occurred on June 14, 2022. A total of 248 patients (90%) completed the 12-week treatment period . The most common reasons for trial discontinuation were withdrawal of consent (7 patients) and loss to follow-up (8 patients). Although discontinuations were not shown to be related to adverse events, adverse events initially occurred at a higher frequency in the 1-mg and 2-mg dose groups than in the 0.5-mg and placebo groups . Fewer losses to follow-up occurred in the latter half of the trial, potentially because of improved coronavirus disease 2019 protocols and immunization, and these losses were evenly distributed among the trial groups. \n\n【35】The use of background medication was similar among the trial groups. All the patients received a diuretic, 91 to 96% received an angiotensin-converting–enzyme inhibitor or angiotensin-receptor blocker, and 64 to 70% received a calcium-channel blocker .\n\n【36】Primary End Point\n-----------------\n\n【37】Figure 2. Dose-Dependent Decreases in Blood Pressure in Patients with Treatment-Resistant Hypertension Who Received Baxdrostat.\n\n【38】Shown are the changes from baseline in the least-squares mean (LSM) seated systolic blood pressure  and diastolic blood pressure  according to the dose of baxdrostat. The changes in systolic blood pressure  and diastolic blood pressure  according to the trial day are also shown. The baseline measurement was the measurement at randomization. Restricted maximum likelihood estimation was used with an unstructured covariance matrix and the Kenward–Roger approximation for degrees of freedom . P values are shown for significant changes in blood pressure between the baxdrostat and placebo groups. 𝙸 bars indicate 95% confidence intervals.\n\n【39】After the prespecified interim analysis, the trial was stopped early because the independent data monitoring committee concluded that the trial had met the criteria for overwhelming efficacy. At week 12, baxdrostat was associated with dose-dependent changes in the least-squares mean (±SE) systolic blood pressure of −20.3±2.1 mm Hg, −17.5±2.0 mm Hg, and −12.1±1.9 mm Hg at the 2-mg, 1-mg, and 0.5-mg doses, respectively . As compared with the change in systolic blood pressure of −9.4 mm Hg in the placebo group, there were significantly greater decreases in systolic blood pressure in the 2-mg baxdrostat group (difference between the 2-mg group and the placebo group, −11.0 mm Hg; 95% confidence interval \\[CI\\], −16.4 to −5.5; P<0.001) and in the 1-mg baxdrostat group (difference between the 1-mg group and the placebo group, −8.1 mm Hg; 95% CI, −13.5 to −2.8; P=0.003), but these decreases were not significantly greater with the 0.5-mg dose. The results of the per-protocol analysis are presented in Table S5, and the results of the sensitivity analysis for missing data are shown in Table S6.\n\n【40】Secondary End Points\n--------------------\n\n【41】At the 2-mg dose, baxdrostat reduced diastolic blood pressure by 14.3±1.31 mm Hg. The difference in the change in diastolic blood pressure between the baxdrostat 2-mg group and the placebo group was −5.2 mm Hg (95% CI, −8.7 to −1.6) .\n\n【42】Exploratory End Points\n----------------------\n\n【43】### _Pharmacokinetic Measures_\n\n【44】Plasma levels of baxdrostat increased proportionately with increasing doses and reached a maximum plasma level in less than 4 hours. Details of this analysis are presented in Figure S2 and Table S7.\n\n【45】### _Pharmacodynamic Measures_\n\n【46】Figure 3. Effects of Baxdrostat on Pharmacodynamic Measures.\n\n【47】Shown are the changes from baseline in the LSM values for urinary aldosterone normalized for urinary creatinine excretion , serum aldosterone , plasma renin activity , and serum total cortisol . The baseline measurement was the measurement at randomization. Reported urinary aldosterone levels that were below the assay lower limit of quantification (<3 ng per deciliter) were imputed to be one half this lower limit (1.5 ng per deciliter). Reported serum aldosterone levels that were below the assay lower limit of quantification (<1 ng per deciliter) were imputed to be one half this lower limit (0.5 ng per deciliter). Values censored owing to dilution error and other missing values were excluded. To convert the values for cortisol to nanomoles per liter, multiply by 27.6. To convert the values for aldosterone to picomoles per liter, multiply by 27.74. 𝙸 bars indicate 95% confidence intervals.\n\n【48】The use of baxdrostat led to a sustained dose-dependent decrease in serum aldosterone levels . The least-squares mean differences in changes in aldosterone levels at the end of the trial between the baxdrostat groups and the placebo group ranged from −3.0 ng per deciliter (95% CI, −4.3 to −1.7) (−83.2 pmol per liter; 95% CI, −119.3 to −47.2) at the 0.5-mg dose to −4.9 ng per deciliter (95% CI, −6.3 to −3.5) (−135.9 pmol per liter; 95% CI, −174.8 to −97.1) at the 2-mg dose. The 24-hour urinary aldosterone levels decreased with all three dose levels of baxdrostat ; the changes in the urinary aldosterone level (normalized for urinary creatinine excretion) from baseline to the end of the trial were −187 ng of aldosterone (95% CI, −254 to −119) per gram of creatinine with the 0.5-mg dose, −180 ng of aldosterone (95% CI, −250 to −110) per gram of creatinine with the 1-mg dose, and −273 ng of aldosterone (95% CI, −342 to −204) per gram of creatinine with the 2-mg dose. In patients in the placebo group, the urinary aldosterone level increased by 6 ng of aldosterone (95% CI, −55 to 67) per gram of creatinine.\n\n【49】The least-squares mean change in plasma renin activity from baseline to the end of the trial was higher by 13.8 ng per milliliter per hour (95% CI, 9.6 to 17.9) in the 2-mg baxdrostat group than in the placebo group . Serum cortisol levels were not reduced in any of the baxdrostat groups throughout the trial, and there were no significant differences between the baxdrostat groups and the placebo group in the least-squares mean change in these levels at week 12. At the highest dose level of baxdrostat, the least-squares mean change in the serum cortisol level from baseline to the end of the trial was 1.91 μg per deciliter (95% CI, 0.70 to 3.12) (52.7 nmol per liter; 95% CI, 19.3 to 86.1) .\n\n【50】Safety\n------\n\n【51】Table 2. Adverse Events That Occurred during the Treatment Period.\n\n【52】No deaths occurred during the trial, and baxdrostat had an acceptable side-effect profile overall. A total of 232 adverse events occurred during the treatment period in 120 patients (44%) . A higher percentage of patients in the 1-mg group (52%) and 2-mg group (48%) had adverse events than those in the 0.5-mg group (35%) or the placebo group (41%). Adverse events that occurred in 5% or more patients in any of the trial groups were urinary tract infections, hyperkalemia, headache, and fatigue. Most adverse events (62%) were mild, and 89% were deemed by the investigators to be unrelated to baxdrostat or placebo. A total of 18 serious adverse events occurred in 10 patients; 6 occurred in a patient with urosepsis. None of the serious adverse events were deemed by the investigators to be related to baxdrostat or placebo. There were no instances of adrenocortical insufficiency.\n\n【53】A total of 10 adverse events of special interest occurred in eight patients. These events, which were prespecified as adverse events of special interest because they warranted clinical intervention, included one case of hypotension, three cases of hyponatremia, and six cases of hyperkalemia. Three of the cases of hyperkalemia, which occurred in three patients, led to potassium levels ranging from 6.0 to 6.3 mmol per liter. At a potassium level of 6.0 mmol per liter, the protocol mandated temporary discontinuation of baxdrostat or placebo. One of these patients was the patient with urosepsis who discontinued the trial because of multiple serious adverse events that were deemed by the investigators before unblinding of the trial-group assignments to be unrelated to baxdrostat or placebo. The other two patients resumed baxdrostat 2 days and 6 days after it was discontinued and completed the trial with normal potassium levels. The other three patients with hyperkalemia had potassium levels between 5.5 and 5.9 mmol per liter on at least two consecutive occasions, and baxdrostat was discontinued. Two of these three patients resumed baxdrostat and also completed the trial with normal potassium levels while receiving baxdrostat. The third patient in this group did not resume baxdrostat. Hyperkalemia was not correlated with the estimated GFR at screening. As shown in Table S8, which provides data on additional vital signs, no meaningful change in body weight occurred in any of the trial groups. Results of additional safety analyses are presented in Table S9.\n\n【54】Discussion\n----------\n\n【55】Our trial showed substantial decreases in blood pressure when patients with treatment-resistant hypertension who were receiving stable doses of at least three antihypertensive medications also received the selective aldosterone synthase inhibitor baxdrostat. The reduction in blood pressure was associated with a decrease in the plasma aldosterone level and a compensatory increase in plasma renin activity, without a reduction in the cortisol level. Baxdrostat generally had an acceptable side-effect profile, and none of the patients discontinued the trial because of hyperkalemia.\n\n【56】Treatment-resistant hypertension is associated with high cardiovascular risk,  but this classification provokes skepticism regarding the frequency with which poor medication adherence accounts for the condition.  Although nonadherence may be a contributor, there is mounting evidence that in adherent patients, treatment-resistant hypertension is a subtype of hypertension that has a poor response to conventional drugs because of its pathogenesis. The results of the Prevention and Treatment of Hypertension with Algorithm-based Therapy–2 (PATHWAY-2) trial, combined with inferences from its three mechanistic substudies, provide support for the hypothesis that treatment-resistant hypertension is associated with autonomous aldosterone production, which could account for the finding in that trial that a mineralocorticoid receptor antagonist (spironolactone) had superior efficacy in reducing blood pressure as compared with multiple other antihypertensive agents.  Our trial, which adds to the evidence that aldosterone appears to be a driver of treatment resistance, showed dose-related reductions in both blood pressure and indexes of aldosterone secretion. We did not compare baxdrostat with alternative drugs. A meta-analysis involving 11,000 participants from 42 trials showed that in patients who received currently licensed drugs (angiotensin-converting–enzyme inhibitors, calcium-channel blockers, or diuretics) in addition to previous therapy, systolic blood pressure was a mean of 7 to 8 mm Hg lower than that in those who received placebo. \n\n【57】The main limitations of spironolactone with respect to side effects — gynecomastia in men and menstrual irregularities and postmenopausal bleeding in women — are due to the off-target blockade of multiple steroid hormone receptors by spironolactone. In addition, the risk of inducing hyperkalemia with spironolactone, as shown in a large epidemiologic study involving patients with heart failure, has contributed to a decrease in its use for other medical conditions. \n\n【58】An alternative strategy to mineralocorticoid receptor blockade is to lower aldosterone levels through inhibition of aldosterone synthase. However, the development of such a drug has been thwarted by the 93% sequence similarity between aldosterone synthase and the final enzyme required for cortisol synthesis, 11β-hydroxylase.  The first aldosterone synthase inhibitor to enter clinical development, LCI-699 (osilodrostat), was associated with off-target inhibition of cortisol synthesis and was ultimately repurposed to treat excess cortisol states rather than hypertension.  The selective action of baxdrostat may avert the risk of inducing adrenal insufficiency and the loss of blood-pressure–lowering efficacy that can result from the accumulation of mineralocorticoid receptor–activating steroid precursors seen with first-generation aldosterone synthase inhibitors.  These advantages will need to be confirmed in phase 3 trials involving more patients over a longer period.\n\n【59】Similar caution applies to our findings regarding potassium. Mild hyperkalemia is common in patients taking approved renin–angiotensin–aldosterone system inhibitors.  In our trial, few patients who received baxdrostat had recurrent hyperkalemia, and the cases of elevated potassium levels resolved quickly, without dose modification or intervention other than routine dietary advice.\n\n【60】The limitations of the present phase 2 dose-ranging trial include the fact that it was not designed to test the benefits and risks of aldosterone synthase inhibition beyond 12 weeks, nor to compare aldosterone synthase inhibition with alternative antihypertensive agents. The selection of patients with an estimated GFR greater than 45 ml per minute reduced the likelihood of hyperkalemia, and planned longer-term studies may determine whether the incidences of hyperkalemia and other adverse events differ from those reported for currently licensed drugs. The inclusion of patients with at least 70% adherence (assessed on the basis of pill counts) was based on the PATHWAY-2 trial, in which high adherence was confirmed in a subgroup of patients by means of urinary drug analysis.  Excluded patients might have had a lower response to baxdrostat.\n\n【61】In our trial, aldosterone synthase inhibition with baxdrostat led to substantial reductions in systolic and diastolic blood pressure in patients with treatment-resistant hypertension.\n\n【62】Table 1. Demographic and Clinical Characteristics of the Patients at Baseline. \n\n| Characteristic | Placebo(N=69) | Baxdrostat,0.5 mg(N=69) | Baxdrostat,1 mg(N=70) | Baxdrostat,2 mg(N=67) |\n| --- | --- | --- | --- | --- |\n| Age |  |  |  |  |\n| Mean — yr | 63.8±10.8 | 61.5±10.3 | 62.7±10.1 | 61.2±10.8 |\n| <65 yr — no. (%) | 32 (46) | 39 (56) | 39 (56) | 41 (61) |\n| ≥65 yr — no. (%) | 37 (54) | 30 (43) | 31 (44) | 26 (39) |\n| Male sex — no. (%) | 42 (61) | 36 (52) | 37 (53) | 38 (57) |\n| Race or ethnic group — no. (%)  |  |  |  |  |\n| White | 51 (74) | 45 (65) | 48 (69) | 47 (70) |\n| Black | 16 (23) | 22 (32) | 20 (29) | 19 (28) |\n| Asian | 2 (3) | 1 (1) | 2 (3) | 1 (1) |\n| American Indian or Alaska Native | 0 | 1 (1) | 0 | 0 |\n| Hispanic or Latinx | 30 (43) | 33 (48) | 23 (33) | 32 (48) |\n| Body-mass index  | 32.1±5.3 | 33.2±5.3 | 31.9±5.2 | 33.3±5.1 |\n| Seated blood pressure — mm Hg |  |  |  |  |\n| Systolic | 148.9±12.4 | 147.6±12.5 | 147.7±13.1 | 147.3±11.8 |\n| Diastolic | 88.2±6.1 | 87.6±7.7 | 87.7±6.0 | 88.2±7.1 |\n| Estimated glomerular filtration rate |  |  |  |  |\n| Mean — ml/min/1.73 m 2 | 85.5±17.5 | 81.0±20.4 | 83.2±20.6 | 85.2±19.4 |\n| <60 ml/min/1.73 m 2 — no. (%) | 6 (9) | 14 (20) | 11 (16) | 8 (12) |\n| ≥60 ml/min/1.73 m 2 — no. (%) | 63 (91) | 55 (80) | 59 (84) | 59 (88) |\n| Diabetes — no. (%) |  |  |  |  |\n| Yes | 28 (41) | 26 (38) | 20 (29) | 31 (46) |\n| No | 41 (59) | 43 (62) | 50 (71) | 36 (54) |\n| Sodium level — mmol/liter | 139±3 | 139±2 | 138±3 | 140±2 |\n| Potassium level — mmol/liter | 4.2±0.5 | 4.3±0.4 | 4.0±0.4 | 4.1±0.4 |\n| Creatinine level — mg/dl | 0.9±0.2 | 1.0±0.3 | 0.9±0.3 | 0.9±0.3 |\n| Background antihypertensive drug — no. (%) |  |  |  |  |\n| Diuretic | 69 (100) | 69 (100) | 70 (100) | 67 (100) |\n| Beta-blocker | 47 (68) | 44 (64) | 41 (59) | 35 (52) |\n| Calcium-channel blocker | 47 (68) | 44 (64) | 49 (70) | 47 (70) |\n| ACE inhibitor or ARB | 63 (91) | 64 (93) | 65 (93) | 64 (96) |\n| General antihypertensive drug | 9 (13) | 8 (12) | 11 (16) | 8 (12) |\n\n【64】 Plus–minus values are means ±SD. Baseline characteristics are shown for the intention-to-treat population (all the patients who underwent randomization). Percentages may not total 100 because of rounding. To convert the values for sodium to milligrams, multiply by 23, to convert the values for potassium to milligrams, multiply by 39, and to convert the values for creatinine to micromoles per liter, multiply by 88.4. ACE denotes angiotensin-converting enzyme, and ARB angiotensin-receptor blocker.\n\n【65】 Race or ethnic group was reported by the patient.\n\n【66】 The body-mass index is the weight in kilograms divided by the square of the height in meters.\n\n【67】Table 2. Adverse Events That Occurred during the Treatment Period.\n\n| Event | Placebo(N=69) | Placebo(N=69) | Baxdrostat,0.5 mg(N=69) | Baxdrostat,0.5 mg(N=69) | Baxdrostat,1 mg(N=69) | Baxdrostat,1 mg(N=69) | Baxdrostat,2 mg(N=67) | Baxdrostat,2 mg(N=67) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | No. of Patients with Event (%) | No. of Events | No. of Patients with Event (%) | No. of Events | No. of Patients with Event (%) | No. of Events | No. of Patients with Event (%) | No. of Events |\n| Any serious adverse event  | 2 (3) | 3 | 0 | 0 | 2 (3) | 3 | 6 (9) | 12 |\n| Any adverse event | 28 (41) | 50 | 24 (35) | 38 | 36 (52) | 77 | 32 (48) | 67 |\n| Adverse event of special interest  | 0 | 0 | 1 (1) | 1 | 5 (7) | 6 | 2 (3) | 3 |\n| Hyponatremia | 0 | 0 | 0 | 0 | 2 (3) | 2 | 1 (2) | 1 |\n| Hypotension | 0 | 0 | 0 | 0 | 1 (1) | 1 | 0 | 0 |\n| Potassium level ≥6.0 mmol/liter | 0 | 0 | 0 | 0 | 2 (3) | 2 | 1 (2) | 1 |\n| Potassium level between 5.5 and 5.9 mmol/liter on at least two consecutive occasions  | 0 | 0 | 1 (1) | 1 | 2 (3) | 2 | 1 (2) | 1 |\n\n【69】 No serious adverse events were deemed by the investigators to be related to baxdrostat.\n\n【70】 Elevated potassium levels were adverse events of special interest if they warranted clinical intervention.\n\n【71】 One patient had a potassium level between 5.5 and 5.9 mmol per liter as well as a potassium level of 6.0 mmol per liter or higher, and these measurements were counted as the same event; thus, a total of six patients with hyperkalemia had an adverse event of special interest.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3f819ed2-5140-4d16-a625-d25a960ff3a4", "title": "Pulmonary Epithelial Permeability in Hyaline-Membrane Disease", "text": "【0】Pulmonary Epithelial Permeability in Hyaline-Membrane Disease\nAbstract\n--------\n\n【1】Neonatal hyaline-membrane disease is complicated by pulmonary edema, yet left atrial pressures are normal. Alveolar-capillary-membrane permeability may therefore be increased. To assess pulmonary epithelial permeability, we measured the pulmonary clearance and half-life of aerosolized <sup>99m </sup> Tc-diethylenetriamine pentacetate ( <sup>99m </sup> Tc-DTPA) on 31 occasions in 15 intubated premature infants with hyaline-membrane disease. Three infants with respiratory failure due to other diseases were studied on four occasions. All studies of infants with hyaline-membrane disease that were performed in the first 72 hours of life demonstrated a biphasic clearance curve with a rapid-phase half-life of 1.6±0.6 minutes (mean ±S.D.). As these infants recovered, the curve became monophasic with a half-life of 56.0±32.1 minutes. Two infants remained dependent on oxygen and ventilator support and had persistent biphasic curves with a rapid-phase half-life of 1.5±0.7 minutes. All infants without hyaline-membrane disease had monophasic curves with a half-life of 65.4±33.6 minutes. Using a similar technique, we observed that newborn lambs and piglets have a monophasic pulmonary clearance of <sup>99m </sup> Tc-DTPA (114±59 minutes in lambs and 52.5±16.3 minutes in piglets).\n\n【2】We conclude that the lungs of neonates with hyaline-membrane disease are abnormally permeable to small solutes and that this abnormality persists in infants with subsequent chronic lung disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "daceaf79-05b1-491c-953d-77ac2cf65b36", "title": "Structural Ableism — Essential Steps for Abolishing Disability Injustice", "text": "【0】Structural Ableism — Essential Steps for Abolishing Disability Injustice\n### Audio Interview\n\n【1】 Interview with Ari Ne’eman on opportunities to support access to home- and community-based services for people with disabilities. \n\n【2】People with disabilities face inequities in social determinants of health, health care access and quality, and the impact of global crises, all of which compound the harms to the disability community.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "40b6cb0e-aa00-4870-9020-092c1eae0b2c", "title": "An Outbreak of Amebiasis Spread by Colonic Irrigation at a Chiropractic Clinic", "text": "【0】An Outbreak of Amebiasis Spread by Colonic Irrigation at a Chiropractic Clinic\nAbstract\n--------\n\n【1】From June 1978 through December 1980, at least 36 cases of amebiasis occurred in persons who had had colonic-irrigation therapy at a chiropractic clinic in western Colorado. Of 10 persons who required colectomy, six died. Of 176 persons who had been to the clinic in the last four months of 1980,80 had received colonic-irrigation therapy and 96 had received other forms of treatment. Twenty-one per cent of the colonic-irrigation group had bloody diarrhea, as compared with 1 per cent of the non-irrigation group (P = 0.00013). Thirty-seven per cent of the colonic-irrigation group who submitted specimens had evidence of amebic infection on either stool examination or serum titer, as compared with 2.4 per cent in the non-irrigation group (P = 0.00012). Persons who were given colonic irrigation immediately after a person with bloody diarrhea received it were at the highest risk for the development of amebiasis. Tests of the colonic-irrigation machine after routine cleaning showed heavy contamination with fecal coliform bacteria. The severity of disease in this outbreak may have been related to the route of inoculation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d4a86a7d-aecc-44e0-b605-a20ab7ca3a20", "title": "Lumpectomy Compared with Lumpectomy and Radiation Therapy for the Treatment of Intraductal Breast Cancer", "text": "【0】Lumpectomy Compared with Lumpectomy and Radiation Therapy for the Treatment of Intraductal Breast Cancer\nAbstract\n--------\n\n【1】Background and Methods\n----------------------\n\n【2】Women with ductal carcinoma in situ have been treated both by lumpectomy and by lumpectomy followed by radiation therapy, but the benefit of combined therapy is uncertain. A group of 818 women with ductal carcinoma in situ were randomly assigned to undergo lumpectomy or lumpectomy followed by breast irradiation (50 Gy). Sufficient tissue was removed that the margins of the resected specimens were histologically tumor-free. The mean duration of follow-up was 43 months (range, 11 to 86). The principal end point of the study was event-free survival, as defined by the presence of no new ipsilateral or contralateral breast cancers, regional or distant metastases, or other cancers and by no deaths from causes other than cancer.\n\n【3】Results\n-------\n\n【4】Five-year event-free survival was better in the women who received breast irradiation (84.4 percent, vs. 73.8 percent for the women treated by lumpectomy alone; P = 0.001). The improvement was due to a reduction in the occurrence of second ipsilateral breast cancers; the incidence of each of the other events was similar in the two groups. Of 391 women treated by lumpectomy alone, ipsilateral breast cancer developed in 64 (16.4 percent); it was noninvasive in 32 and invasive in the remaining 32. Of 399 women treated with lumpectomy and breast irradiation, ipsilateral breast cancer developed in 28 (7.0 percent) (noninvasive in 20 and invasive in 8). The five-year cumulative incidence of second cancers in the ipsilateral breast was reduced by irradiation from 10.4 percent to 7.5 percent for noninvasive cancers and from 10.5 percent to 2.9 percent for invasive cancers (P = 0.055 and P<0.001, respectively).\n\n【5】Conclusions\n-----------\n\n【6】Breast irradiation after lumpectomy is more appropriate than lumpectomy alone for women with localized ductal carcinoma in situ\\.\n\n【7】Introduction\n------------\n\n【8】Women with intraductal breast cancer (ductal carcinoma in situ) have been treated in many different ways, ranging from local excision of the tumor with or without breast irradiation to unilateral or bilateral mastectomy. Because there is little justification for mastectomy in women with invasive breast cancer,  the concept that an operation for noninvasive breast cancer should be more radical than one for invasive disease seems paradoxical.\n\n【9】Before the advent of mammography, approximately 3 to 5 percent of new cases of breast cancer involved ductal carcinomas in situ,  most of which were large, palpable masses. Information about the incidence of recurrence after the removal of these tumors was based on studies of women with clinically detectable lesions  . As a consequence of mammography, ductal carcinoma in situ is diagnosed more frequently, and a much higher proportion of the tumors are not clinically detectable  .\n\n【10】Because of uncertainty about the most appropriate treatment for women with these tumors, the National Surgical Adjuvant Breast and Bowel Project began a randomized clinical trial in 1985 to test the hypothesis that in women with localized ductal carcinoma in situ that was thought to have been completely removed, lumpectomy (or more accurately, local excision, since most women did not have a palpable mass) plus breast irradiation is more effective than local excision alone in preventing a second cancer in the ipsilateral breast. The initial results from this trial are presented here.\n\n【11】Methods\n-------\n\n【12】Selection of Patients\n---------------------\n\n【13】Women with localized ductal carcinoma in situ detected by physical examination or mammography were eligible for the study. Those with a tumor consisting of both ductal carcinoma in situ and lobular carcinoma in situ were also eligible, as were women with multiple lesions in the same breast if one lesion contained ductal carcinoma in situ and the others were histologically benign. Women with tumor-positive axillary nodes on clinical examination were excluded. If axillary dissection was carried out, all the nodes had to be tumor-negative. Also, women with previous cancers were ineligible, except those who had had in situ carcinoma of the cervix or squamous- or basal-cell cancer of the skin.\n\n【14】Within eight weeks after initial biopsy, the women underwent a lumpectomy, with removal of the tumor and a sufficient amount of normal breast tissue that the margins of the resected specimen were histologically tumor-free. Women with a histologic diagnosis of ductal carcinoma in situ whose mammograms showed diffuse microcalcifications were eligible only if histologic examination of tissue containing the microcalcifications demonstrated no tumor. The tumor diagnoses were made by institutional pathologists.\n\n【15】Study Design\n------------\n\n【16】Between October 1, 1985, and December 31, 1990, after they had undergone lumpectomy and given informed consent, the women enrolled in the study were randomly assigned to receive either ipsilateral breast irradiation or no radiation therapy. Randomization was carried out at the study biostatistical center with a stratified scheme designed to avoid inequalities in treatment assignment  . The stratification variables were age ( ≤ 49 or >49 years), axillary dissection (performed or not performed), tumor type (ductal carcinoma in situ or ductal carcinoma in situ plus lobular carcinoma in situ), and method of detection (mammography, clinical examination, or both). Initially, all women underwent axillary-node dissection. After June 1987, such dissection became optional because of reports indicating the infrequency of positive axillary nodes in women with this type of tumor  .\n\n【17】Radiation Therapy\n-----------------\n\n【18】Breast irradiation was begun when wound healing permitted, but no later than eight weeks after the operation. The patients received a dose of 50 Gy, calculated at a depth equal to two thirds of the distance between the skin overlying the breast and the base of the tangential fields at midseparation. The dose was given at a rate of 10 Gy per week (2 Gy per day for five days). Nine percent of the women received additional radiation therapy to the tumor site. Interstitial radiation and regional node irradiation were not allowed. The radiation-therapy technique was similar to that used in previous studies  .\n\n【19】Follow-up and Identification of Study End Points\n------------------------------------------------\n\n【20】Women underwent follow-up examinations every six months; mammography was performed annually. A tumor detected at a local or regional site after the initial operation was considered an event when a biopsy of the lesion was positive. A tumor at a distant site was considered an event when the clinical, radiographic, or pathological findings indicated that tumor was present. The presence of ipsilateral or contralateral breast cancer, regional or distant metastasis, or a second primary tumor other than a breast tumor occurring as a first event after the operation or the patient's death in the absence of evidence of recurrent breast cancer was used to determine event-free survival.\n\n【21】The location of primary tumors and postoperative ipsilateral breast tumors was determined from mammographic, operative, and pathological reports submitted to the biostatistical center. Tumors were classified as being in a specific quadrant, on the border between two quadrants, central (in a subareolar position or at the border of a quadrant and the areola), or diffuse (in more than one of the preceding sites).\n\n【22】Statistical Analysis\n--------------------\n\n【23】The results in the two treatment groups were compared for all randomized patients for whom information was available that could be evaluated. The percentage of women in whom no event occurred was determined with a life-table estimate computed by the actuarial method  . The two treatment groups were compared in relation to the distribution of the time before the occurrence of a first event by means of a two-sided summary chi-square (log-rank) test  . P values and relative odds with 95 percent confidence intervals were determined annually and were designated as cumulative; the values summarized the outcome to the designated time  . Two-sided P values less than 0.05 were considered statistically significant.\n\n【24】Average annual incidence rates, relative risks, and 95 percent confidence intervals were calculated for each type of first event. The chi-square test was used to compare the average annual type-specific incidence rates in the treatment groups  . Relative risks and 95 percent confidence intervals were determined by univariate proportional-hazards analysis  . Cumulative incidence rates were plotted for events for which the comparison of treatments revealed a significant difference  . To test for interactions between the stratification variables and treatment in relation to event-free survival, multivariate proportional-hazards analysis was used  . The results presented here represent the first formal testing of the primary study hypothesis as originally planned on the basis of data obtained one year after the accrual of patients into the study.\n\n【25】Results\n-------\n\n【26】Distribution of Patients\n------------------------\n\n【27】Table 1. Distribution of Patients According to Treatment Group and Protocol Status. Table 2.  Table 2. Characteristics of the Study Patients, According to Treatment Group.\n\n【28】A total of 818 women were enrolled in the trial . None of the records obtained at one institution (representing 24 patients) could be evaluated, because of problems with the quality of the data. No follow-up results were available for 4 of the 794 women who could be evaluated (0.5 percent). Sixteen of the 790 women included in the analyses (2.0 percent) did not meet the criteria for study entry. Five years after the operation, follow-up results were up to date for 97.5 percent of the women who could be evaluated. All women who could be evaluated for whom there were follow-up data, including those who did not meet the criteria for entry into the study, were included in the analyses. The mean duration of follow-up was 43 months (range, 11 to 86). There were no differences between treatment groups in the distribution of selected characteristics of the patients and tumors .\n\n【29】Event-free Survival\n-------------------\n\n【30】Figure 1. Event-free Survival of Women Treated by Lumpectomy (open circle) or Lumpectomy and Radiation Therapy (solid circle).\n\n【31】Ipsilateral and contralateral breast cancers, regional and distant metastases, second primary tumors other than breast cancers, and deaths due to causes other than breast cancer were included in the analysis when they occurred as first events. There were 84 events in the lumpectomy group and 51 events in the group treated by lumpectomy and radiation. The P value for the comparison of the two curves is shown. Cumulative odds, 95 percent confidence intervals (CI), and P values are shown for each year of follow-up.Table 3.  Table 3. Average Annual Incidence of First Events in Each Treatment Group after Operation.\n\n【32】The women treated by lumpectomy followed by ipsilateral breast irradiation had significantly better event-free survival during five years of follow-up than the women treated by lumpectomy alone (84.4 percent vs. 73.8 percent, respectively; P = 0.001) . The cumulative odds ratio of 1.8 (95 percent confidence interval, 1.2 to 2.5) confirmed the beneficial effect of radiation therapy. There was a 47.3 percent reduction in the average annual incidence rate (per 100 women) for all first events (6.7 for lumpectomy alone vs. 3.8 for lumpectomy plus irradiation; P = 0.001) as a result of radiation therapy . No significant interactions between stratification variables and treatment were identified.\n\n【33】First Events in the Ipsilateral Breast\n--------------------------------------\n\n【34】Sixty-four of all 84 first events (76.2 percent) in the group treated by lumpectomy alone were ipsilateral breast tumors, as compared with 28 of 51 first events (54.9 percent) in the group undergoing lumpectomy and radiation therapy . The average annual incidence rate of ipsilateral breast cancer was reduced by 58.8 percent as a consequence of radiation therapy (lumpectomy, 5.1; lumpectomy plus radiation, 2.1; P<0.001). The rates of both noninvasive cancers (2.6 and 1.5 in the respective groups; P = 0.055) and invasive cancers (2.6 and 0.6; P<0.001) were reduced, but the percent reduction was greater in the latter (42.3 percent vs. 76.9 percent). There was no reduction in the average annual incidence rate of other types of first events (contralateral breast cancers, other second cancers, and deaths from causes other than cancer) as a result of radiation therapy.\n\n【35】Figure 2. Cumulative Incidence of Noninvasive and Invasive Ipsilateral Breast Cancers and of All Other First Events in Women Treated by Lumpectomy (open circle) or Lumpectomy and Radiation Therapy (solid circle).\n\n【36】P values are for the comparisons of average annual incidence rates between treatment groups.\n\n【37】At five years of follow-up, in the women treated by lumpectomy alone, the cumulative incidence of ipsilateral noninvasive and invasive cancers was 10.4 percent and 10.5 percent, respectively, whereas in the group treated by lumpectomy and radiation, it was 7.5 percent and 2.9 percent . There was no significant difference between the two groups in the cumulative incidence of first events other than ipsilateral breast tumors (6.7 percent and 7.1 percent, respectively). Three of the 92 women in whom second tumors developed in the ipsilateral breast subsequently had distant disease. Two of the three were treated by lumpectomy alone, and cancer subsequently developed in the contralateral breast. The other woman was treated by lumpectomy and radiation, and liver metastases subsequently developed.\n\n【38】The location of the primary and second tumors was evaluated in the 83 women in whom the sites of both tumors were known. In 63 women (76.9 percent), either the first and second tumors were within the same quadrant, or one tumor was inside a quadrant and the other was on the border of the same quadrant. In two of the women (2.4 percent), the second tumors were in the same quadrant and extended into other quadrants as well. In seven women (8.4 percent), one of the two tumors was retroareolar; the other was either on the border of a quadrant with the areola or within a quadrant. In 11 women (13.3 percent), the second tumors were located at sites different from those of the primary tumors. Nine of the 11 second tumors were in quadrants different from those of the primary tumors, and in two patients the primary tumor occurred on one border of the quadrant and the second tumor on another border of the same quadrant.\n\n【39】Among the 64 women whose primary ductal carcinoma in situ was treated by lumpectomy only and in whom a second breast tumor developed, 28 (43.8 percent) were treated by repeat lumpectomy and 36 (56.3 percent) by mastectomy. The percentage of women treated by a second local excision was similar regardless of whether the second tumors were noninvasive or invasive (43.8 percent). Twenty-eight women who received radiation therapy to treat their primary tumors had second tumors in the ipsilateral breast. Ten of the 20 who had noninvasive tumors (50 percent) were treated with lumpectomy, and 10 had mastectomies. All eight women whose second tumors were invasive had mastectomies. Six of the 54 women in both groups whose second tumors were treated by mastectomy had a prophylactic mastectomy of the contralateral breast.\n\n【40】Regional and Distant First Events\n---------------------------------\n\n【41】Few regional and distant events occurred as first events in either treatment group . Three women had regional metastases. In two women whose ductal carcinoma in situ was detected from microcalcifications on a mammogram and who were treated by lumpectomy and radiation therapy, ipsilateral node metastasis subsequently developed (one after 15 months and one after 7 months) without evidence of disease in the breast. The third woman, treated by lumpectomy alone, also had ductal carcinoma in situ that was detected from microcalcifications; she had nodal metastases 5 1/2 years later.\n\n【42】Two women had distant metastatic breast cancer. In one of them, treated by lumpectomy alone, the breast cancer was found to have metastasized to her femur, with no evidence of disease in the breast eight months later. The other had concomitant contralateral supraclavicular nodal metastases, liver metastases, and extensive invasive ipsilateral breast cancer 12 months after lumpectomy and radiation therapy. She died one month after the diagnosis of metastasis.\n\n【43】First Events in the Contralateral Breast\n----------------------------------------\n\n【44】Eighteen women, 8 in the lumpectomy group and 10 in the group treated by lumpectomy and radiation, had contralateral breast cancer as a first event. The initial cancer was detected by palpation in 1 woman, nipple discharge in 1, and mammography (revealing microcalcifications) in 16. Eleven of the contralateral breast cancers were invasive, and seven were ductal carcinoma in situ. Fourteen of the 18 contralateral tumors were treated by lumpectomy, 3 by mastectomy, and 1, an inflammatory cancer, by systemic therapy followed by mastectomy.\n\n【45】Second Primary Tumors as First Events\n-------------------------------------\n\n【46】In the group treated by lumpectomy alone, five second cancers exclusive of those in the ipsilateral breast occurred as first events: two in the colon and one each in the pancreas, lung, and skin of the arm. Eight second cancers occurred in women who received radiation therapy: three in the colon and one each in the tonsil, esophagus, uterus, lung, and the skin of the arm.\n\n【47】Death as a First Event\n----------------------\n\n【48】Death from causes other than cancer occurred as a first event in five women in the lumpectomy group and two in the group receiving radiation therapy. One of the 92 women in whom a second ipsilateral breast tumor developed as a first event died of breast cancer. There were only two other deaths from breast cancer. One occurred in the woman who had extensive distant disease and concomitant ipsilateral breast involvement as a first event after lumpectomy and radiation therapy. The other woman who died of breast cancer, also in the group receiving radiation, had supraclavicular nodal metastases as the first event.\n\n【49】Discussion\n----------\n\n【50】The results of this randomized trial of therapy in women with ductal carcinoma in situ indicate that radiation therapy after lumpectomy improved overall event-free survival as compared with lumpectomy alone. All first events occurring after the initial therapy were considered in the life-table analysis, ensuring that ipsilateral breast tumors were accounted for and that events that could have been attributed to undesirable sequelae of therapy were included. Moreover, the inclusion of all the first events addressed the problem of competing risks by taking into account women who had metastases to a regional or distant site, had second tumors other than in the ipsilateral breast, or died from other causes before a second ipsilateral breast tumor developed.\n\n【51】The benefit of radiation therapy was most evident when each type of first event was evaluated according to its contribution to the overall effect. As expected, there was no evidence that irradiation reduced the average annual incidence rates or the cumulative incidence of non-cancer-related deaths, second primary cancers (either in the contralateral breast or at other sites), and regional or distant metastatic disease. The benefit of radiation therapy was related to its ability to decrease the cumulative incidence of all second ipsilateral breast tumors as compared with that in the group treated only by lumpectomy. This reduction was evident for both noninvasive and invasive second cancers.\n\n【52】It might be considered that women with ductal carcinoma in situ should be treated by mastectomy, since women treated by lumpectomy alone had a 20.7 percent estimated risk of a second ipsilateral breast tumor. However, since radiation therapy significantly reduced the incidence of both noninvasive and invasive second tumors to 7.5 percent and 2.9 percent, respectively, mastectomy may be difficult to justify to treat localized ductal carcinoma in situ. Furthermore, since the women in whom another ductal carcinoma in situ developed after radiation therapy are likely to have the same prognosis that they had when the first lesion was identified, that contention is strengthened. The development of an invasive lesion after the removal of ductal carcinoma in situ and treatment by radiation therapy, as occurred in 2.9 percent of the women in our study, may be viewed differently. It may be argued that those invasive cancers, with their potential for dissemination, could have been prevented had the primary tumor been treated by mastectomy. If the incidence of such cancers remains low, however, and if they are detected when they are small, few of them should result in distant disease and death  .\n\n【53】Pathologists may have difficulty differentiating between benign breast lesions (e.g., highly atypical hyperplasia) and ductal carcinoma in situ, between ductal carcinoma in situ and lobular carcinoma in situ, and between ductal carcinoma in situ and a focus of invasive cancer, especially when the latter is associated with ductal carcinoma in situ  . In large randomized trials, women with tumors of uncertain type should be equally distributed among the treatment groups, thus eliminating that source of bias. Moreover, the findings from a large trial that includes women who have ductal carcinoma in situ with a focus of invasive tumor should be applicable to the general population of women with ductal carcinoma in situ, some of whom could have tumors with undiagnosed invasive components. A review of slides submitted to the pathology center demonstrated invasive cancers in 1.5 percent of the primary tumors surveyed and atypical hyperplasia in 4.7 percent. The discordance in the diagnosis could have been related at least in part to a sampling error -- i.e., the slides submitted may not have been representative of the material from which the original diagnosis of ductal carcinoma in situ was made. Because the discordant diagnoses were few and were distributed between the two groups of women, they did not affect the conclusions of the study.\n\n【54】After the local excision of ductal carcinoma in situ, second ipsilateral tumors are almost always at or close to the site of the primary tumor  . In this study, 13.3 percent of the second tumors were in a different location, a proportion somewhat higher than is usually reported. Because most information about concordance comes from the study of women with invasive cancer, there may be a difference between noninvasive and invasive tumors in that regard.\n\n【55】In summary, since the incidence of second ipsilateral tumors is significantly reduced by breast irradiation after lumpectomy, combined therapy is more appropriate than lumpectomy alone for women with localized intraductal carcinoma in situ.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2270a815-bbf6-4860-bb09-405b89b2b43d", "title": "Adjuvant Therapy in Stage I and Stage II Epithelial Ovarian Cancer — Results of Two Prospective Randomized Trials", "text": "【0】Adjuvant Therapy in Stage I and Stage II Epithelial Ovarian Cancer — Results of Two Prospective Randomized Trials\nAbstract\n--------\n\n【1】About a third of patients with ovarian cancer present with localized disease; despite surgical resection, up to half the tumors recur. Since it has not been established whether adjuvant treatment can benefit such patients, we conducted two prospective, randomized national cooperative trials of adjuvant therapy in patients with localized ovarian carcinoma (International Federation of Gynecology and Obstetrics Stages la to lie). All patients underwent surgical resection plus comprehensive staging and, 18 months later, surgical reexploration.\n\n【2】In the first trial, 81 patients with well-differentiated or moderately well differentiated cancers confined to the ovaries (Stages la <sub>i </sub> and lb <sub>i </sub> ) were assigned to receive either no chemotherapy or melphalan (0.2 mg per kilogram of body weight per day for five days, repeated every four to six weeks for up to 12 cycles). After a median follow-up of more than six years, there were no significant differences between the patients given no chemotherapy and those treated with melphalan with respect to either five-year disease-free survival (91 vs. 98 percent; P = 0.41) or overall survival (94 vs. 98 percent; P = 0.43).\n\n【3】In the second trial, 141 patients with poorly differentiated Stage I tumors or with cancer outside the ovaries but limited to the pelvis (Stage II) were randomly assigned to treatment with either melphalan (in the same regimen as above) or a single intraperitoneal dose of  P (15 mCi) at the time of surgery. In this trial (median follow-up, >6 years) the outcomes for the two treatment groups were similar with respect to five-year disease-free survival (80 percent in both groups) and overall survival (81 percent with melphalan vs. 78 percent with  P; P = 0.48).\n\n【4】We conclude that in patients with localized ovarian cancer, comprehensive staging at the time of surgical resection can serve to identify those patients (as defined by the first trial) who can be followed without adjuvant chemotherapy. The remaining patients with localized ovarian cancer should receive adjuvant therapy, and with adjuvant melphalan or intraperitoneal  P should have a five-year disease-free survival of about 80 percent. \n\n【5】Introduction\n------------\n\n【6】APPROXIMATELY 20,000 new epithelial ovarian cancers are reported annually in the United States, and about 30 percent of these are apparently localized (Stages I and II according to the classification of the International Federation of Gynecology and Obstetrics \\[FIGO\\]) at initial diagnosis.  The published five-year survival rates for such patients range from 50 to 70 percent for Stage I cancer to only 38 to 60 percent for Stage II.    Prognostic factors such as cell type and histologie grade provide only a partial explanation for the widely variable results. Earlier studies have demonstrated the need for thorough surgical staging to define the extent of disease and for a prospective comparison of treatments in groups of patients balanced for known prognostic factors. Few if any single institutions in the United States have enough patients to perform randomized clinical trials of patients with early disease. As a result, two collaborative clinical trials were begun by the Ovarian Cancer Study Group (composed of physicians at the Mayo Clinic, the M.D. Anderson Hospital and Tumor Institute, the National Cancer Institute, and the Roswell Park Memorial Institute) and the Gynecologic Oncology Group. Both trials required carefully predefined surgical staging before patients were entered in the study. The first trial included patients with Stage la and lb disease with well-differentiated or moderately well differentiated histologie grades, and the second trial included patients with poorly differentiated Stage I disease and all patients with Stage II disease. The median follow-up of the surviving patients from the two randomized trials now exceeds six years.\n\n【7】Methods\n-------\n\n【8】Table 1. Definitions of Stages I and II Ovarian Cancer, According to the International Federation of Gynecology and Obstetrics.\n\n【9】The two clinical trials were begun in 1976 by the Ovarian Cancer Study Group, which was joined in 1978 by the Gynecologic Oncology Group. Both used the staging criteria established by FIGO in 1976.  Only patients with Stage I disease (limited to the ovaries) or Stage II disease (limited to the pelvis) were included, and all such patients were assigned to subgroups as defined in Table 1 . No patients with Stage I or II disease were excluded from the study except those with bulky Stage II tumors that could not be resected. After complete surgical staging, the patients in the first trial were randomly assigned to receive no further therapy or therapy with melphalan (0.2 mg per kilogram of body weight orally, daily for five days), with repeat courses every four to six weeks for 12 cycles or 18 months, whichever came first. A total of 92 patients were randomly assigned, 48 to receive melphalan and 44 to receive no further therapy. Patient enrollment ended in October 1984.\n\n【10】The patients in the second trial were randomly assigned to receive either melphalan (in the same dose and according to the same schedule as in the first trial) or intraperitoneal  P (15 mCi of chromic phosphate \\[before 1979 the dose was 7.5 mCi\\]). One hundred forty-five patients were randomly assigned, 71 to receive melphalan and 74 to receive  P. The study was closed to new patients in November 1986.\n\n【11】Eligibility for either trial required a histopathological diagnosis of epithelial ovarian cancer. No patient had received any previous therapy except surgery, and all patients were required to have adequate bone marrow function as evidenced by a white-cell count above 4000 per cubic millimeter and a platelet count above 150,000 per cubic millimeter, as well as a blood urea nitrogen level below 7.1 mmol per liter (<20 mg per deciliter), a serum creatinine level below 106 μmol per liter (<1.2 mg per deciliter), and normal results of liver-function testing. Initial noninvasive staging procedures included chest radiography, intravenous pyelography, and before March 1983, lymphangiography. In addition, pelvic ultrasonography, barium enemas, pelvic and abdominal CT scanning, and proctosigmoidoscopy were performed when clinically indicated.\n\n【12】Surgical staging was performed through a vertical incision of sufficient length to allow the evaluation of the abdominal contents and the sites at high risk for surface metastases. For all patients the surgery included a total abdominal hysterectomy, a bilateral salpingo-oophorectomy, and partial infracolic omentectomy. In addition, the tumor capsule was examined for rupture, excrescences, or any adherence requiring sharp dissection; ascitic fluid was examined for malignant cells, and if no ascites was present, separate 250-ml saline washings were obtained from the pelvis and both abdominal gutters and examined cytologically. Biopsies of suspicious lesions were performed, along with random biopsies of the pelvic peritoneum, the cul-de-sac peritoneum, the right and left abdominal gutter peritoneum, and the undersurface of the right diaphragm. Biopsies of any suspicious omental nodule were performed. To assess the nodal spread of disease, the paraaortic and pelvic lymph nodes were palpated, sampled, and examined histologically.\n\n【13】A complete pathological review of all tissue specimens and cytologie findings was performed before randomization at the primary institution. Subsequently, all pathological findings were reviewed by the central pathology office in accordance with the established procedures of the Pathology Review Committee of the Gynecologic Oncology Group.\n\n【14】A stratified randomization plan was developed with computer-generated random numbers and implemented by telephone contact between the clinical site and the data-coordinating center. The stratifications were made on the basis of cell type and histologie grade (well, moderately, or poorly differentiated, according to the pattern-grading classification    ). In addition, the patients in the second trial were stratified according to stage into three groups —Group A (those with Stage Ia <sub>i </sub> or Ibi disease with poorly differentiated histologie grades and those with Stage Ia <sub>ii </sub> or Ib <sub>ii </sub> disease), Group B (those with Stage Ha or lib disease), and Group C (those with Stage Ic or lie disease and any patient with disease detectable microscopically). Randomization within each stratum was done in blocks of two, so that for each consecutive pair of patients within each stratum, one was assigned to each treatment. Because the clinical site was not a factor in the stratification, the assignment to treatment could not be predicted by individual physicians. At randomization, no patient included in the trials had macroscopic residual disease (any nodule more than 2 cm in transverse diameter). Informed consent was obtained from all patients before entry into either trial.\n\n【15】Both protocols required that patients who were free of symptoms suggestive of recurrent disease 18 months after entry into the study undergo routine reexploration. Symptomatic patients underwent reexploration earlier unless obvious recurrent disease was documented by noninvasive study. At second-look surgery, all patients had peritoneal washings, and multiple biopsies were performed of the right and left paracolic gutters, the cul-de-sac, the lateral pelvic wall, the small-bowel mesentery, and the omentum. In addition, biopsies were performed of adhesions and known sites of previous disease. The results of this second-look surgery have been published previously. \n\n【16】Life-table probabilities of survival and disease-free survival were calculated by the method of Kaplan and Meier.  Disease-free survival was defined as the time to a first event — recurrence or death. Comparisons of the survival distributions were made with the log-rank test.  The Cox proportional-hazards regression model was used to perform comparisons after adjustment for the base-line characteristics and to investigate the prognostic value of the baseline variables. Confidence limits for the measures of interest were calculated according to the methods described by Simon. \n\n【17】The target samples for these studies were 110 patients in the first trial and 142 in the second. The goal for the second trial was achieved. In the first trial, accrual was terminated at 74 percent of the accrual goal (after eight years during which patients were entered) because of an observed rate of relapse so low as to preclude with any reasonable probability the eventual detection of moderate differences between the two groups.\n\n【18】Results\n-------\n\n【19】First Trial\n-----------\n\n【20】The first trial included patients who had Stage la or lb disease with well-differentiated or moderately well differentiated epithelial ovarian cancer. After comprehensive staging, 92 patients were randomly assigned — 44 to receive no additional therapy and 48 to receive melphalan. However, 11 patients were deemed ineligible and were excluded from analysis (6 of the patients assigned to observation and 5 of those assigned to melphalan). Eight patients were found ineligible on the basis of stage: 2 were found in the review by the central pathology office to have benign tumors, and 1 had a poorly differentiated tumor. Thus, a total of 81 patients were available for analysis — 38 assigned to observation and 43 assigned to treatment with melphalan.\n\n【21】One of the 38 patients assigned to observation alone received melphalan. She was initially classified as ineligible, and her physician elected to treat her with melphalan. Subsequent central review established her eligibility, and she was therefore included in the analysis. Of the 43 patients assigned to chemotherapy with melphalan, 1 received no drug, 2 received only one course, and 2 received only three courses. Thirty-one (72 percent) received 10 or more courses. The survival analysis included all patients randomized, regardless of whether they received the assigned therapy or completed their full course. A secondary analysis omitting patients who received inappropriate, limited, or no therapy yielded similar results. Three patients randomly assigned to melphalan who were subsequently found to be ineligible for the first trial but eligible for the second were followed and included in the analysis of toxic effects and pathological findings for the second trial, but not in the comparisons of treatment efficacy. Five patients were lost to follow-up before five years: two assigned to observation were lost at 35 and 43 months, and three assigned to melphalan were lost at 15, 18, and 50 months. Our analysis treated these as random losses, and the patients were included in the analyses of survival and disease-free survival up to the date of their last contact with the study.\n\n【22】Table 2. Characteristics of Patients with Stage I or Stage II Ovarian Cancer.\n\n【23】The characteristics of the randomized patients are shown in Table 2 . The two groups were well matched with respect to histologie type, histologie grade, and stage, but the patients assigned to melphalan were somewhat younger. The histologie type and grade were based on the classifications made in the central pathology review. The histologie grades were well-differentiated, moderately well differentiated, and poorly differentiated tumors, as well as tumors of borderline malignancy. A central pathological review was made for 72 of 81 patients who could be evaluated (89 percent), and the results compared with those of the review by the original institution (data not shown). There were no major differences in histologie type noted. Although the design of the trial sought to exclude patients with tumors of borderline malignancy, the difficulty of making this exclusion is evident. As shown in Table 2 , 27 of the patients in the first trial (33 percent) were later declared on central pathological review to have tumors of borderline malignancy, even though the original pathologist considered that the criteria for a more malignant process had been met. The patients with tumors of borderline malignancy were reasonably evenly distributed between the two study groups.\n\n【24】### __Toxicity__\n\n【25】The patients assigned to observation did not receive placebo, so the frequency of placebo-related toxic effects was not assessed. For the patients receiving melphalan, the immediate toxic effects were mild and tolerable. Seventy-nine percent of the patients had some degree of myelosuppression. Seven patients (16 percent) had severe myelosuppression at some point in their course, five (12 percent) had platelet-count nadirs under 50,000 per cubic millimeter, and four (9 percent) had white-count nadirs under 2000 per cubic millimeter. There were no infectious complications related to leukopenia and no bleeding episodes related to thrombocytopenia induced by the chemotherapy. Eleven patients (26 percent) reported mild-to-moderate gastrointestinal side effects. No other adverse reactions were reported. One patient treated with melphalan died six years after completing treatment, with a diagnosis of aplastic anemia. No autopsy was performed. No other myeloproliferative disorders or second cancers have been seen in these patients treated with melphalan after more than 250 person-years of follow-up.\n\n【26】### __Survival__\n\n【27】Table 3. Tumor Recurrences and Deaths among Patients with Stage I or Stage II Ovarian Cancer. Figure 1.  Figure 1. Disease-free Survival of Patients with Localized Ovarian Cancer, According to Protocol and Treatment.\n\n【28】Of the 81 patients in the first trial, 4 of 38 in the observation group and 2 of 43 in the melphalan group had recurrences of disease. Of the 141 patients in the second trial, 16 of 73 in the  P group and 18 of 68 in the melphalan group had recurrences.Figure 2.  Figure 2. Overall Survival of Patients with Stage I or Stage II Epithelial Ovarian Cancer, According to Protocol and Treatment.\n\n【29】Of the 81 patients in the first trial, 4 of 38 in the observation group and 2 of 43 in the melphalan group died. Of the 141 patients in the second trial, 16 of 73 in the  P group and 15 of 68 in the melphalan group died.\n\n【30】With a median follow-up of more than six years in the surviving patients and 71 of 75 surviving patients who were followed for three or more years, only six deaths have been observed . Four of the deaths occurred in the observation group, and two in the melphalan group. One of the deaths in the observation group was that of the patient who received melphalan inadvertently. One of the six deaths was that of a patient who had been classified as having a tumor of borderline malignancy. Five deaths were attributed primarily or secondarily to ovarian cancer, and one to aplastic anemia; four of the six patients who died had documented recurrences of disease. The sites of recurrence were reviewed for the presence of distinctive patterns, but as shown in Table 3 , none were apparent. Life-table plots of disease-free and overall survival according to treatment group are shown in Figures 1 and 2 , respectively. The estimated five-year disease-free and overall survival rates for untreated patients were 91 and 94 percent, respectively. For the patients treated with melphalan, both the estimated disease-free survival and the overall five-year survival rates were 98 percent. Reliable confidence intervals for differences in rates are difficult to calculate when the rates are close to 1  ; however, the results of the log-rank test for the differences between groups in survival distributions were not significant or even suggestive (for survival, P = 0.43; for disease-free survival, P = 0.41). With the assumption that the use of melphalan has no effect on the long-term survival of this group of patients, we calculated the overall five-year survival rate for both groups at 96 percent, with a lower 90 percent confidence limit for five-year survival of 92 percent.\n\n【31】Second Trial\n------------\n\n【32】The second trial included all patients who after comprehensive surgical staging were found to have disease of Stage Ia <sub>ii </sub> , Ib <sub>ii </sub> , Ic, Ha, lib, or lie; or of Stage la or lb with poorly differentiated histologie features. The patients were randomly assigned to receive either melphalan (in a dose and according to a schedule similar to those in the first trial) or 15 mCi of intraperitoneal  P as chromic phosphate. A total of 145 patients were entered in the trial. Four patients (3 percent) — three randomly assigned to melphalan and one to  P — were ineligible (one because of Stage III disease, one with a benign tumor, one with idiopathic thrombocytopenic purpura, and one for whom the primarily ovarian origin of the tumor was uncertain). Therefore, a total of 141 patients were evaluated; 73 were randomly assigned to receive  P, and 68 to receive melphalan. The characteristics of the patients are shown in Table 2 .\n\n【33】Of the 73 patients assigned to receive  P, 5 (7 percent) did not receive it because of difficulties in catheter implantation; one additional patient refused this treatment. Five of these received melphalan according to the regimen prescribed for the patients in the other group. Sixty-eight patients were assigned to melphalan. Only one patient refused chemotherapy before beginning treatment; she was treated with intraperitoneal  P. All the other patients assigned to melphalan received at least 10 courses of the drug, except for six patients who had early progressive disease. The data analysis included all patients randomly assigned to the therapy regardless of whether they received the treatment assigned. A secondary analysis, comparing the patients as actually treated rather than as randomized, produced similar results. Three patients were lost to follow-up before the end of five years; one (lost at 28 months) was assigned to  P, and two (lost at 15 and 19 months) were assigned to melphalan. As in the first trial, our analysis treated these as random losses.\n\n【34】The two groups in the study were well balanced with respect to histologie type, histologie grade, disease stage, extent of residual tumor, and age. The histologie type and grade were based on the findings of the central pathological review, and the histologie grade was coded as in the first trial. Central pathological review was performed for 130 of the 141 patients (92 percent), and there were no major differences between the determinations of histologie type made by the original institution and the central pathology office (data not shown). As in the first trial, the patients with tumors of borderline malignancy were to be excluded from entry into this trial. However, after reanalysis in the central pathological review, 17 percent were reclassified as having tumors of borderline malignancy. These patients were reasonably well distributed between the two treatment groups .\n\n【35】### __Toxicity__\n\n【36】The immediate side effects were generally mild and manageable in both treatment groups. Of the 68 patients treated with  P, 14 (21 percent) reported mild-to-moderate abdominal pain, and 4 (6 percent) had severe pain. One patient had chemical peritonitis, and one had infectious peritonitis. Four patients (6 percent) underwent exploratory surgery for bowel obstruction, and no tumor was identified. Approximately two thirds of all patients treated with  P had no side effects. Of the patients treated with melphalan, 74 percent had some myelosuppression, and in 20 percent the myelosuppression was classified as severe. The median white-count nadir in the patients with myelosuppression was 2400 per cubic millimeter, and 12 patients (18 percent) had white-count nadirs below 2000 per cubic millimeter. The median platelet-count nadir was 81,000 per cubic millimeter, and 5 (7 percent) had platelet counts under 25,000 per cubic millimeter. No serious infectious complications or bleeding episodes were reported. Eleven patients who were treated with melphalan (16 percent) reported mild-to-moderate gastrointestinal toxicity. One patient received 12 courses of melphalan in 1977 and 1978 and died in March 1982, 15 months after being given a diagnosis of preleukemia. In January 1986 acute myelogenous leukemia developed in a second patient, who was treated with seven courses of melphalan in 1981 and 1982 and who died in July 1987.\n\n【37】### _Survival_\n\n【38】In 27 patients (19 percent), tumors have recurred — 14 (19 percent) in the group receiving  P and 13 (19 percent) in the group receiving melphalan . Twenty-two (81 percent) of the recurrences were seen in the first two years of follow-up. Five recurrences were observed after two years — at 30, 34, 61, 78, and 84 months. The patterns of recurrence are listed in Table 3 ; there were no major differences between groups. Thirty-one patients have died (22 percent) — 16 who received  P and 15 who received melphalan. Twenty-four of the deaths (77 percent) were attributed primarily or secondarily to ovarian cancer. The deaths not due to ovarian cancer were from various causes — cerebral vascular accidents (in two), myeloproliferative disorders (two), gram-negative pneumonia (one), lung cancer (one), and unknown cause (one).\n\n【39】The follow-up in the surviving patients now exceeds six years, and 86 percent of these patients have been observed for at least three years. The distributions of survival and disease-free survival in the two groups are virtually identical . The five-year disease-free survival rate is 80 percent in each arm. The five-year survival rates are 78 percent in the patients assigned to  P and 81 percent in those assigned to melphalan. The 90 percent confidence intervals for the differences in these rates are from −0.11 to 0.11 for disease-free survival and from −0.08 to 0.16 for survival, with a negative number indicating an advantage for  P and a positive number indicating an advantage for melphalan. Thus, even a moderate advantage of one treatment over the other is unlikely on the basis of the observed data.\n\n【40】Prognostic Factors\n------------------\n\n【41】A multivariate analysis was performed to identify the base-line characteristics that might influence prognosis. Age, previous hysterectomy, histologie type, histologie grade, and FIGO stage were considered. Residual tumors were present in six patients, four of whom had recurrences; however, this characteristic was not included in the multivariate analysis because of the small number of patients affected. Only age and histologie grade were treated as ordinal variables. The end point of the analysis was disease-free survival.\n\n【42】The only variables that appeared to be strongly and independently related to outcome were clear-cell tumor type and histologie grade. However, histologie grade appeared to be highly prognostic only when patients with tumors of borderline malignancy were included. Clear-cell tumors were associated with poorer outcomes.\n\n【43】Pathological Findings\n---------------------\n\n【44】Table 4. Recurrences of Ovarian Cancer According to Histologie Type.\n\n【45】The number of recurrences observed in both trials is shown in Table 4 according to pathological classification. Although the number of patients with clear-cell carcinomas in each trial was small, this was the only histologie type associated with a higher-than-expected frequency of relapse. If one analyzes only patients who had a central pathological review, a total of 3 of 8 patients who had clear-cell tumors in the first trial (38 percent) had relapses, as compared with 2 of 63 patients with other tumor types (3 percent). In the second trial, relapses occurred in 9 of 26 patients with clear-cell tumors (35 percent), as compared with 16 of 107 patients with tumors of other histologie types (15 percent).\n\n【46】Discussion\n----------\n\n【47】Comprehensive surgical staging allows the selection of the appropriate treatment for early ovarian carcinomas (FIGO Stages la to lib). It is clear that the entire abdomen is at risk for metastases. Adjuvant therapies, if required, must treat these sites of high risk. Unfortunately, the surgical evaluation of patients with ovarian cancer is commonly incomplete.    Such deficiencies of staging make it hard to interpret earlier clinical studies and select appropriate therapy for patients with FIGO Stage I or II disease.\n\n【48】These two trials were designed to include only patients in whom a careful, standardized, comprehensive surgical exploration confirmed the presence of localized disease, so that the true benefit of adjuvant therapy, if any, could be assessed. With a median follow-up in excess of six years and a follow-up of more than two years in all patients, several conclusions are clear. For patients who underwent surgical staging as defined by this study and were found to have Stage Iai or Ibi disease with well-differentiated or moderately well differentiated histologie features, the five-year survival was excellent (≥90 percent), and adjuvant therapy with melphalan did not significantly improve the results. In view of the toxicity, the expense, the inconvenience, and the risk of second cancers, the identification of a group of patients who do not require adjuvant therapy represents a substantial benefit. Three patients in these two trials who received melphalan therapy had myeloproliferative disorders or acute leukemia that were most likely a result of treatment. Although such complications are uncommon, they are a well-known catastrophic risk.   \n\n【49】For patients with localized ovarian cancer as defined by the entry criteria of the second trial, the five-year survival was 80 percent, with similar results in both the  P group and the melphalan group. Although this result is substantially better than the five-year survival rates of 40 to 60 percent published previously for patients with Stages Ic to lie disease, it is impossible to determine whether the surgical staging simply served to define a group of patients with a better prognosis. However, even with the staging as performed in this study, 20 percent of this group have had relapses and died of ovarian cancer, making it appear likely that some adjuvant treatment is required in such patients. In the absence of a therapeutic difference between the two groups in the second trial, we have concluded that the group receiving the single dose of intraperitoneal  P, with its limited toxicity (and no known risk of leukemia), is the preferred standard group for subsequent study. Since the evidence from trials in advanced ovarian cancer suggests that platinum-containing combination drugs administered in intensive doses are effective,    the Gynecologic Oncology Group has initiated a replacement trial comparing intraperitoneal  P therapy with treatment involving three cycles of cyclophosphamide and cisplatin.\n\n【50】Unfortunately, most trials of therapy for localized ovarian cancer have not been randomized,     have not required comprehensive surgical staging,    or have included patients with Stage II or III disease who have minimal residual disease.    Because these trials contained a mixture of patients with various stages of disease, extent of residual disease, and histologie grades, it is impossible to determine the benefit, if any, of adjuvant treatment. However, in the absence of thorough surgical staging, patients in all the early stages of disease combined have substantial relapse rates at 5 and 10 years, and no subgroup of these patients can safely be left without adjuvant therapy. Our trial confirms the risk of upper abdominal relapse seen in earlier trials.     The site of the majority of recurrences in this study (71 percent) was either distant (32 percent) or abdominal (39 percent).\n\n【51】Ovarian tumors of borderline malignancy have been difficult to define histologically, but there is virtually complete agreement that their usual course is more indolent than that of tumors in patients with invasive disease.    In both trials described here, the patients with tumors of borderline malignancy were evenly distributed between groups, and the comparisons of therapeutic efficacy were unaffected by the exclusion of these patients. The five-year survival of the patients in the first trial was unchanged when the patients with tumors of borderline malignancy were excluded. After a similar exclusion in the second trial, the five-year survival became 76 percent rather than 80 percent. The substantial number of reclassifications emphasizes how difficult it is to be certain whether stromal invasion is present unless a large number of slides from tumors sectioned at 1-cm intervals are available for review. Only 2 of the 51 patients in our trial who were considered to have tumors of borderline malignancy (4 percent) have died. In neither patient was the death clearly caused by the underlying cancer. Given the risks, costs, and inconvenience of adjuvant therapy, our results would suggest that patients with Stage I or II disease who have tumors of borderline malignancy should not receive adjuvant treatment.\n\n【52】The presence of clear-cell varieties of epithelial ovarian cancer, whether alone or with other epithelial-cell elements, may be associated with a higher risk of recurrent disease. Although only 35 of the 204 patients who had a central pathological review (17 percent) had clear-cell histologie features, 12 of 30 recurrences (40 percent) involved this histologie element. The clinical behavior of this type of tumor has been defined incompletely in the literature, probably because of the inclusion of widely divergent histologie features such as endodermal sinus tumors and endometrioid tumors with clear-cell features.   \n\n【53】Comprehensive surgical staging of patients with early ovarian cancer serves to define a group of patients who require no further therapy and allows the selection of suitable adjuvant therapy for the remaining patients. Both groups identified and treated in this manner have excellent long-term disease-free and overall survival.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9aace802-8bc2-4c12-85af-d234f6dae678", "title": "The Effect of Debulking Surgery after Induction Chemotherapy on the Prognosis in Advanced Epithelial Ovarian Cancer", "text": "【0】The Effect of Debulking Surgery after Induction Chemotherapy on the Prognosis in Advanced Epithelial Ovarian Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Although the value of primary cytoreductive surgery for epithelial ovarian cancer is beyond doubt, the value of debulking surgery after induction chemotherapy has not yet been defined. In this randomized study we investigated the effect on survival of debulking surgery.\n\n【3】Methods\n-------\n\n【4】Eligible patients had residual lesions measuring more than 1 cm in diameter after primary surgery. After three cycles of cyclophosphamide and cisplatin, these patients were randomly assigned to undergo either debulking surgery or no surgery, followed by further cycles of cyclophosphamide and cisplatin. The study end points were progression-free survival and overall survival. At surgery 65 percent of the patients had lesions measuring more than 1 cm. In 45 percent of this group, the lesions were reduced surgically to less than 1 cm.\n\n【5】Results\n-------\n\n【6】Of the 319 patients who underwent randomization, 278 could be evaluated (140 patients who underwent surgery and 138 patients who did not). Progression-free and overall survival were both significantly longer in the group that underwent surgery (P = 0.01). The difference in median survival was six months. The survival rate at two years was 56 percent for the group that underwent surgery and 46 percent for the group that did not. In the multivariate analysis, debulking surgery was an independent prognostic factor (P = 0.012). Overall, after adjustment for all other prognostic factors, surgery reduced the risk of death by 33 percent (95 percent confidence interval, 10 to 50 percent; P = 0.008). Surgery was not associated with death or severe morbidity.\n\n【7】Conclusions\n-----------\n\n【8】Debulking surgery significantly lengthened progression-free and overall survival. The risk of death was reduced by one third, after adjustment for a variety of prognostic factors.\n\n【9】Introduction\n------------\n\n【10】The value of cytoreductive surgery in the management of ovarian cancer has been debated for years. The reasons for cytoreductive surgery are manifold. Large tumors with relatively poor central blood supplies and the areas with the lowest growth rates are both rather insensitive to cytotoxic drugs.  In better-perfused small, residual tumors, the growth rate and the diffusion of chemotherapeutic agents are higher — factors that are apt to increase the efficacy of chemotherapy. The removal of large tumors also reduces the likelihood that drug-resistant clones will appear as a result of spontaneous mutations.  Moreover, small tumors require fewer cycles of chemotherapy, thus decreasing the probability of drug-induced resistance.\n\n【11】Several nonrandomized studies showed improved survival of patients with residual tumors less than 1 cm in diameter after primary surgery, as compared with patients with larger lesions.  In a case–control study of patients with minimal residual disease, Eisenkop et al. reported that patients whose small lesions were all resected survived significantly longer than patients in whom such lesions were not resected.  On the other hand, Hacker et al.  and Hoskins et al.  reported that despite optimal cytoreduction, the survival of patients with large intraabdominal metastases before resection was significantly worse than that of patients with small initial intraabdominal lesions. These results suggest that in addition to residual disease after cytoreduction, intrinsic tumor factors are of prognostic importance. They also raise the question of whether cytoreduction has a significant effect on survival among patients with the same size tumors and the same intrinsic prognostic factors.\n\n【12】The value of debulking surgery after induction chemotherapy is even more difficult to assess. Several studies indicated that patients in whom cytoreduction was optimal after induction chemotherapy had approximately the same survival rate as patients in whom cytoreduction was optimal at primary surgery.  Neijt et al., however, reported just the opposite.  Patients with optimal cytoreduction at intervention surgery had poorer rates of survival than patients with optimal cytoreduction at primary surgery. Moreover, the survival of patients with optimal cytoreduction at intervention surgery was the same as that of patients with suboptimal cytoreduction. All these studies, however, included only small numbers of patients with different prognoses.\n\n【13】In 1987 the Gynecological Cancer Cooperative Group of the European Organization for Research and Treatment of Cancer (EORTC) initiated a randomized phase 3 study to establish the effect on survival of debulking surgery after induction chemotherapy.\n\n【14】Methods\n-------\n\n【15】Selection of Patients, Study Design, and Evaluation Methods\n-----------------------------------------------------------\n\n【16】From March 1987 to May 1993, 425 patients were enrolled in the study. Eligible patients had to have biopsy-proved epithelial ovarian carcinoma with an International Federation of Gynecology and Obstetrics stage of IIb to IV,  residual lesions measuring more than 1 cm in diameter, a World Health Organization (WHO) performance status of 0 to 2,  an age of less than 75 years, and adequate bone marrow and renal function, and they had to have undergone primary surgery no more than six weeks before treatment began. Clinical response was assessed according to the standard WHO response criteria.  A complete response was defined pathologically as the absence of macroscopic and microscopic tumor at surgery. Optimal cytoreduction was defined as the reduction of all tumor lesions to less than 1 cm in diameter.\n\n【17】Figure 1. Design of the Study.\n\n【18】The study design is shown in Figure 1 . All eligible patients, after giving informed consent, were registered centrally at the EORTC Data Center before chemotherapy was begun. The patients received three cycles of cyclophosphamide at a dose of 750 mg per square meter of body-surface area, and cisplatin at a dose of 75 mg per square meter every three weeks. After the third and sixth cycles, a clinical evaluation was performed consisting of a gynecologic and general physical examination, computed tomography or sonography (or both), and measurement of serum CA-125. After the third cycle, patients with tumor progression or a contraindication to surgery were removed from the study. Patients with a clinical response or stable disease were randomly assigned to undergo debulking surgery or not to undergo surgery. Randomization was done centrally at the EORTC Data Center after stratification with a minimization technique to account for institution, performance status, and clinical response.\n\n【19】At surgery, scheduled within 28 days after the third cycle of chemotherapy, maximal cytoreduction was performed and, if not done previously, a bilateral salpingo-oophorectomy, hysterectomy, and infracolic omentectomy were carried out when possible. Chemotherapy was to be resumed within four weeks after surgery. All randomized patients were scheduled to receive at least six cycles of cyclophosphamide and cisplatin unless this therapy was clearly contraindicated. The decision to continue or discontinue treatment after the sixth cycle was based on the policy of the center. Patients with a complete clinical response preferably had a “second-look” operation regardless of whether they had undergone debulking surgery. After therapy patients were seen every two months during the first two years and indefinitely thereafter according to each center's policy. The end points of the study were overall survival and progression-free survival.\n\n【20】Statistical Analysis\n--------------------\n\n【21】An accrual target of 440 randomized patients was specified in the protocol in order for the study to have an 80 percent probability of detecting a 30 percent reduction in the risk of death, with the use of a two-tailed log-rank test at the conventional 5 percent level of significance.  No formal statistical rules were specified in the protocol for interim analyses, which were performed once a year according to EORTC policy.  A difference in survival first emerged in September 1991 (P = 0.006). This difference was confirmed by further interim analyses in October 1992 (P = 0.016), February 1993 (P = 0.028), and April 1993 (P = 0.012), at which time the group decided to stop enrolling patients in the study. Follow-up was continued on all randomized patients.\n\n【22】Survival was calculated from the day chemotherapy was begun until death, regardless of the cause of death. Progression-free survival was calculated from the day chemotherapy was begun to the time of progression or death. All randomized patients with some follow-up information were included in the analyses of survival and progression-free survival, which were performed strictly according to the intention-to-treat principle. Survival and progression-free survival curves were calculated for each treatment group with Kaplan–Meier estimates  and compared with the log-rank test.  Stratified analyses and Cox regression analyses were performed to adjust treatment comparisons for all known prognostic factors. \n\n【23】Results\n-------\n\n【24】Recruitment and Demographic Characteristics of the Patients\n-----------------------------------------------------------\n\n【25】Of the 425 patients enrolled in the study, 319 patients underwent randomization. One hundred six patients did not undergo randomization for the following reasons: 39 had disease progression, 22 had a contraindication to surgery, 17 were still receiving induction chemotherapy, 11 died, 10 declined to participate in the study, 4 were found to be ineligible, and 3 were lost to follow-up.\n\n【26】Table 1. Characteristics of the Patients.\n\n【27】At the time of the analysis follow-up data were available on 278 of the 319 randomized patients: 140 patients who were assigned to undergo debulking surgery and 138 who were assigned not to undergo surgery. All known prognostic characteristics were well balanced between the two treatment groups . The patients ranged in age from 32 to 74 years, with a median age of 59 years in both groups.\n\n【28】Treatment Received\n------------------\n\n【29】Treatment data were available for all 278 patients. Ninety-three percent of the patients (130 patients) assigned to undergo surgery had had surgery, and 100 percent of the patients (138 patients) assigned not to undergo surgery had had no surgery. In both groups 84 percent of the patients received at least six cycles of cyclophosphamide and cisplatin. A reduction in the dose or postponement of chemotherapy was necessary in 36 percent and 48 percent of the patients who had had surgery and in 37 percent and 49 percent of the patients who had not undergone surgery, respectively. The length of time from cycle 4 to cycle 6 was similar in both treatment groups (P = 0.42), lasting a median of seven weeks in each. The median overall duration of treatment from cycle 1 to cycle 6 was 21 weeks for the patients who had had surgery and 17.5 weeks for the patients who had not had surgery.\n\n【30】The reasons for stopping chemotherapy after randomization and before the sixth cycle in the group that had undergone surgery and in the group that had not were as follows: progressive disease in 5 and 3 percent, respectively; toxic reactions in 3 and 2 percent, respectively; refusal to continue chemotherapy in 4 and 5 percent, respectively; and unknown in 4 and 6 percent, respectively. Consolidation chemotherapy after the sixth cycle was given to 36 percent of the patients who had undergone surgery and 51 percent of those who had not had surgery. A second-look operation was performed in 52 patients who had undergone surgery and 51 patients who had not.\n\n【31】Results of Debulking Surgery\n----------------------------\n\n【32】Table 2. Results of Debulking Surgery after Three Cycles of Cyclophosphamide and Cisplatin.\n\n【33】Surgical data were available on 127 of the 130 patients who underwent debulking surgery. At laparotomy, 83 of the 127 patients (65 percent) had tumors that exceeded 1 cm in diameter and 44 patients had tumors that had been reduced to less than 1 cm by chemotherapy . In 37 of the 83 patients (45 percent) with lesions measuring more than 1 cm, the tumors were reduced by surgery to a diameter of less than 1 cm. In the other 46 patients an attempt was made to perform maximal cytoreduction, but lesions measuring more than 1 cm had to be left behind. The ovaries were resected in 32 of 41 patients (78 percent) whose ovaries were still in situ. Overall, 81 of the 127 patients (64 percent) had residual lesions of less than 1 cm after debulking surgery.\n\n【34】No lethal or serious complications were observed after debulking surgery. The intraoperative complications consisted of bowel injury in 3 percent of the patients and bladder injury in 2 percent. Twenty-two percent lost more than 500 ml of blood. The postoperative complications consisted of fever (temperature of more than 38.5°C on more than two days) in 4 percent, ileus for more than five days in 1 percent, urinary tract infection in 4 percent, wound infection in 2 percent, deep venous thrombosis in 1 percent, and lung embolism in 2 percent. These complications were similar in kind and severity to those observed at primary surgery.\n\n【35】Clinical Response\n-----------------\n\n【36】The overall rate of clinical response after the sixth cycle of chemotherapy was 84 percent for the group that underwent debulking surgery and 70 percent for the group that did not undergo surgery; the rate of complete response was 70 percent and 35 percent, respectively. The disease progressed in 8 percent of the patients who underwent debulking surgery and 12 percent of those who did not.\n\n【37】Table 3. Size of the Tumors before Cytoreduction and after Cytoreduction at Second-Look Surgery.\n\n【38】Before second-look surgery, 79 percent of the patients who had undergone debulking surgery had a complete clinical response, as compared with 59 percent of the patients who did not have debulking surgery. Second-look surgery in the patients who had undergone debulking surgery and in those who had not revealed similar rates of complete response (37 percent vs. 33 percent), partial response (28 percent vs. 37 percent), stable disease (2 percent vs. 6 percent), disease progression (8 percent vs. 6 percent) and microscopic disease (15 percent vs. 10 percent), and similar percentages of patients whose response could not be evaluated (10 percent vs. 8 percent). The size of the tumors before cytoreduction and after cytoreduction at second-look surgery is shown in Table 3 .\n\n【39】Survival and Progression-Free Survival\n--------------------------------------\n\n【40】At the time of the analysis, 75 percent of the patients had been followed for more than 2.2 years, 50 percent for more than 3.5 years, and 25 percent for more than 4.3 years. The maximal follow-up was 5.6 years. Both overall survival and progression-free survival were significantly longer (P = 0.01) for the patients who underwent debulking surgery. In addition, the three most recent interim analyses had shown a significant benefit of surgery. The difference in survival was more substantial when patients with stage IV disease were excluded (P = 0.003 for overall survival and P = 0.002 for progression-free survival).\n\n【41】Figure 2. Survival of Patients with Advanced Epithelial Ovarian Cancer According to Whether They Underwent Debulking Surgery.\n\n【42】P = 0.012 for the comparison between the groups by the log-rank test.Figure 3.  Figure 3. Survival of Patients with Advanced Epithelial Ovarian Cancer Who Did Not Have Debulking Surgery and Patients Who Had Such Surgery, According to Whether the Lesions Were Less Than 1 cm in Diameter before Cytoreduction, Less Than 1 cm after Cytoreduction (Optimal), or More Than 1 cm after Cytoreduction (Suboptimal).\n\n【43】The median survival was 26 months for the patients who underwent debulking surgery and 20 months for those who did not. The proportion of patients alive at two years was 56 percent in the former group and 46 percent in the latter group . Overall, surgery reduced the unadjusted risk of death by 31 percent (the hazard rate, or the risk of death at any given moment). The beneficial effect of surgery on survival was consistent throughout the observation period. Figure 3 shows the survival curves for the patients who did not undergo debulking surgery and for the patients who did undergo debulking surgery according to the size of residual lesions at the time of surgery. Patients whose lesions measured less than 1 cm before cytoreduction survived significantly longer (median, 41.6 months; P<0.001) than patients whose lesions measured more than 1 cm after debulking surgery (median survival, 19.4 months); survival in the latter group was similar to overall survival in the group that did not undergo debulking surgery (median survival, 20.0 months). Patients with optimal cytoreduction (whose remaining lesions measured less than 1 cm) also survived significantly longer (median survival, 26.6 months; P = 0.04) than patients with suboptimal cytoreduction.\n\n【44】Figure 4. Progression-free Survival of Patients with Advanced Epithelial Ovarian Cancer According to Whether They Underwent Debulking Surgery.\n\n【45】P = 0.013 for the comparison between the groups by the log-rank test.\n\n【46】The median progression-free survival was 18 months for the patients who had debulking surgery and 13 months for the patients who did not have debulking surgery. The percentage of patients alive and free of progressive disease at two years was 38 percent in the former group and 26 percent in the latter .\n\n【47】Figure 5. Progression-free Survival of Patients with Advanced Epithelial Ovarian Cancer Who Did Not Have Debulking Surgery and Patients Who Had Such Surgery, According to Whether the Lesions Were Less Than 1 cm in Diameter before Cytoreduction, Less Than 1 cm after Cytoreduction (Optimal), or More Than 1 cm after Cytoreduction (Suboptimal).\n\n【48】Figure 5 shows the progression-free survival curves for the patients who did not have debulking surgery and for the patients who did have debulking surgery, according to the size of the residual lesions at the time of surgery. The progression-free survival of patients whose lesions were less than 1 cm in diameter before cytoreduction was significantly longer (median, 24.1 months; P<0.001) than that of patients with suboptimal cytoreduction (median survival, 12.1 months); results for the latter were similar to the progression-free survival seen in the group that did not have debulking surgery (median survival, 12.9 months). Likewise, the progression-free survival of patients with optimal cytoreduction was significantly longer (median survival, 23.3 months; P = 0.003) than the progression-free survival of patients with suboptimal cytoreduction.\n\n【49】Adjusted and Multivariate Analyses\n----------------------------------\n\n【50】Table 4. Analysis of the Risk of Death after Debulking Surgery.\n\n【51】Table 4 shows the effect of adjusting the comparison of survival between the two treatment groups for all known prognostic factors. The 31 percent reduction in the risk of death derived from the unadjusted comparison (95 percent confidence interval, 8 to 49 percent) remained qualitatively unchanged by all adjustments for prognostic factors (risk reductions ranging from 29 to 36 percent). The relative benefit of surgery was remarkably consistent across all subgroups of patients (data not shown). When all prognostic factors were taken into account simultaneously in a Cox regression model, the reduction in the risk of death due to surgery was 33 percent (95 percent confidence interval, 10 to 50 percent; P = 0.008). When the population of patients was subdivided into two equal groups on the basis of the multivariate risk, there was no evidence that the relative benefit of debulking surgery was different for the high-risk and the low-risk patients.\n\n【52】Discussion\n----------\n\n【53】Our study establishes the effect of tumor reduction on progression-free and overall survival, which were both significantly prolonged in patients randomly assigned to undergo debulking surgery. The median progression-free survival and overall survival were increased by five and six months, respectively. Moreover, the beneficial effect of surgery on progression-free and overall survival was consistent throughout the observation period (with a median follow-up of 3.5 years). However, much longer follow-up is clearly needed before the curative value of debulking surgery can be assessed.\n\n【54】The majority of the studies in the literature compared the survival of patients after optimal cytoreductive surgery with the survival of patients after suboptimal cytoreduction.  All these studies are hampered by the unavoidable and serious bias inherent in the comparison of patients with different prognostic factors. Only one other prospective randomized study has addressed the question of the value of cytoreduction.  In the interim analysis there was no significant difference in survival between the group that underwent surgery (median survival, 23 months) and the group that did not (median survival, 16 months). Only 34 patients in each group were evaluated, however.\n\n【55】In our study patients with lesions of less than 1 cm at debulking surgery before or after cytoreduction survived longer than patients with larger residual lesions . Nonetheless, the survival of patients with residual lesions of more than 1 cm after surgery — the patients with the poorest prognosis — was not significantly different from the survival of the patients who did not undergo debulking surgery. The fact that survival was similar in these two groups means either that patients with suboptimal cytoreduction did benefit from cytoreduction or that there were patients in the group that did not undergo debulking surgery who might have benefited from cytoreductive surgery. Qualitatively, the same observations were made for progression-free survival .\n\n【56】We also attempted to identify a group of patients who did not benefit from debulking surgery. In the multivariate analysis, the benefit of surgery remained significant even after adjustment for all other known prognostic factors (33 percent reduction in the risk of death, P = 0.008). When the patients were subdivided into high-risk and low-risk groups on the basis of either individual prognostic factors or the multivariate risk, there was no difference in the relative benefit of surgery between the two groups. Thus, we could not identify a subgroup of patients who clearly did not benefit from debulking surgery or a subgroup of patients in whom the benefit was much larger than the overall benefit. This, of course, does not imply that all patients do benefit from intervention surgery, but it does underline the need for further investigations.\n\n【57】In an attempt to exclude other factors that might have influenced the prognosis of the patients who underwent surgery, we evaluated the overall dose of chemotherapy and the degree of tumor reduction at second-look surgery. The total number of cycles of cyclophosphamide and cisplatin and the number of patients who required reductions in the doses were identical in both groups. Also, the percentages of patients who underwent optimal cytoreduction at second-look surgery and of patients who underwent consolidation therapy were similar. Therefore, even if there had been a benefit in terms of survival from cytoreduction at second-look surgery or from consolidation chemotherapy, such a benefit would have affected both treatment groups about equally.\n\n【58】Debulking surgery was not associated with death or severe morbidity. Intraoperative complications were observed in 5 percent of the patients. Mild postoperative complications were observed in 14 percent of the patients, a figure that is comparable to the rates of postoperative complications observed in other studies.  In addition, surgery did not have a negative effect on postoperative chemotherapy. Thus, we believe that the six-month increase in median survival among the patients who underwent surgical debulking outweighs the morbidity associated with surgery.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "42568993-de78-4ab0-8ac5-c799feb300af", "title": "A Growing Problem", "text": "【0】A Growing Problem\nA 36-year-old pregnant woman at 21 weeks' gestation presented with a 4-week history of a dry, nonproductive cough. She said she had no fever, chills, dyspnea, chest pain, or weight loss. She had no new pets, environmental exposures, or sick contacts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a7144241-dde8-4dc9-9939-39ebbfaabb15", "title": "Ethical Considerations Related to Pregnancy in Transplant Recipients", "text": "【0】Ethical Considerations Related to Pregnancy in Transplant Recipients\nIn deciding whether to pursue pregnancy, transplant recipients need to know what they can expect for their own health and for that of their potential children. This article considers how physicians can ethically address fertility issues with female transplant recipients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2dbd2a8d-47a2-4b82-9df0-3b214b034f26", "title": "Case 28-2007 — A 68-Year-Old Man with Syncope", "text": "【0】Case 28-2007 — A 68-Year-Old Man with Syncope\nA 68-year-old man was admitted to the hospital because of a syncopal episode, followed by hemiparesis and altered mental status. An electrocardiogram revealed ST-segment elevations in the anterolateral leads, and MRI of the brain revealed infarcts involving all vascular distributions. Transthoracic echocardiography showed hypokinesis of the left ventricle and a mobile echodensity in the left atrium. Angiography revealed occlusion of the left anterior descending coronary artery, both renal arteries, and the superior mesenteric artery. A diagnostic procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "dda305b5-305f-4e23-b3da-2bcf373141c9", "title": "Brief Report: Sequential Stem Cell–Kidney Transplantation in Schimke Immuno-osseous Dysplasia", "text": "【0】Brief Report: Sequential Stem Cell–Kidney Transplantation in Schimke Immuno-osseous Dysplasia\nSummary\n-------\n\n【1】Lifelong immunosuppression is required for allograft survival after kidney transplantation but may not ultimately prevent allograft loss resulting from chronic rejection. We developed an approach that attempts to abrogate immune rejection and the need for post-transplantation immunosuppression in three patients with Schimke immuno-osseous dysplasia who had both T-cell immunodeficiency and renal failure. Each patient received sequential transplants of αβ T-cell–depleted and CD19 B-cell–depleted haploidentical hematopoietic stem cells and a kidney from the same donor. Full donor hematopoietic chimerism and functional ex vivo T-cell tolerance was achieved, and the patients continued to have normal renal function without immunosuppression at 22 to 34 months after kidney transplantation. \n\n【2】Introduction\n------------\n\n【3】On the basis of recent estimates, the median survival of transplanted kidney grafts is 11.7 years for organs from deceased donors and 19 years for organs from living donors; therefore, most children who receive a kidney transplant will receive more than one in their lifetime.  The lifelong immunosuppression received by kidney-transplant recipients exposes them to risks of infections, cancer, and other complications and, even with full adherence to immunosuppression, does not eliminate the risk of chronic rejection and eventual allograft loss. Initial attempts to achieve immune tolerance by establishing transient mixed chimerism through allogeneic hematopoietic stem-cell transplantation (HSCT) have been successful 60 to 70% of the time.  The establishment of stable mixed chimerism has been associated with immune tolerance in HLA-matched patients, whereas in HLA-mismatched kidney-transplant recipients, mixed chimerism has permitted a reduction in the number of immunosuppressive drugs needed but not their discontinuation.  Alternative approaches to establish 95 to 100% donor engraftment after HSCT have been complicated by the risk of fatal graft-versus-host disease (GVHD), which has occurred in some patients.  The use of HSCT to eliminate the need for immunosuppression after kidney transplantation in children has not been evaluated as it has been in adults. We developed an approach that abrogates immune-mediated rejection and the need for long-term post-transplantation immunosuppression and tested it in three patients with Schimke immuno-osseous dysplasia (SIOD) who had been determined to need a kidney transplant.\n\n【4】SIOD is a rare autosomal recessive multisystem disease that characteristically includes glucocorticoid-resistant nephrotic syndrome, spondyloepiphyseal dysplasia, short stature, and variable T-cell immunodeficiency.  The disease is due to biallelic mutations in the gene encoding SMARCAL1 (SWI/SNF-related matrix-associated actin-dependent regulator of chromatin, subfamily A-like 1),  which is implicated in replication-coupled DNA repair and telomere maintenance.  The nephrotic syndrome in SIOD is a podocytopathy that usually progresses to focal segmental glomerulosclerosis and end-stage kidney disease. \n\n【5】Previously, four of five patients with SIOD who underwent HSCT after myeloablative conditioning died from HSCT-related causes, including infections and GVHD  ; the one long-term survivor has had full immune and hematopoietic reconstitution.  The high post-HSCT mortality among the patients with SIOD may reflect, at least in part, the increased sensitivity of SMARCAL1-deficient cells to DNA-damaging agents,  which are used in myeloablative conditioning regimens.\n\n【6】Success with αβ T-cell–depleted and CD19 B-cell–depleted haploidentical (αβ–haploidentical) HSCT has been achieved in patients undergoing transplantation who have nonmalignant diseases, with overall survival of 90%.  The depletion of alloreactive T cells and B cells results in low rates of both acute and chronic GVHD and of transplant-related death.  No deaths due to GVHD have occurred in a cohort of 70 patients who were treated in this manner, including patients with disorders such as Fanconi’s anemia and dyskeratosis congenita, in which radiosensitivity is a symptom, which necessitated the use of reduced-intensity conditioning to avoid toxic effects.  We hypothesized that αβ–haploidentical HSCT after reduced-intensity conditioning could result in successful engraftment in patients with SIOD, whose cells are also radiosensitive. A predicted advantage of sequential HSCT and kidney transplantation has been suggested by outcomes among patients who first underwent HSCT for an unrelated disease and then subsequently received a kidney transplant from the same HSCT donor, which resulted in long-term kidney-graft survival without long-term immunosuppression. \n\n【7】We report on the treatment and clinical courses of three children with SIOD and T-cell immunodeficiency who underwent sequential αβ–haploidentical HSCT and kidney transplantation from the same parental donor in order to establish functional tolerance and abrogate the need for post-transplantation immunosuppression.\n\n【8】Methods\n-------\n\n【9】Patients\n--------\n\n【10】Three patients with SIOD who had documented biallelic mutations in _SMARCAL1_  underwent αβ–haploidentical HSCT after enrollment in studies that had been approved by the Stanford University institutional review board. Informed consent for the HSCT and kidney donation were obtained from the parents who served as donors. This research was conducted in accordance with the International Council for Harmonisation and the Declaration of Helsinki.\n\n【11】Treatment\n---------\n\n【12】Figure 1. Sequential αβ T-Cell–Depleted and CD19 B-Cell–Depleted Haploidentical HSCT and Kidney Transplantation in Three Patients with SIOD.\n\n【13】Three patients with Schimke immuno-osseous dysplasia (SIOD) received a reduced-intensity conditioning regimen consisting of antithymocyte globulin, fludarabine (the dose of which was adjusted because of the patients’ renal insufficiency), cyclophosphamide, total-body irradiation, and rituximab. The graft used for hematopoietic stem-cell transplantation (HSCT), which was obtained from one of the parents, was depleted of αβ T cells and CD19 B cells, and no immunosuppression was administered after the graft infusion. Each patient received a kidney transplant 5 to 10 months later from the parent who served as the donor for HSCT. A short course of immunosuppression was stopped by 30 days after kidney transplantation. All three patients have had normal kidney function without any immunosuppression for a period of follow-up ranging from 22 to 34 months. AUC denotes area under the curve.\n\n【14】The αβ T-cell and CD19 B-cell depletion was performed as previously reported.  All patients underwent peripheral blood lymphocyte immunophenotyping by flow cytometry, karyotype analysis, determination of circulating leukocyte telomere length , and chimerism studies conducted with the use of short-tandem-repeat polymerase chain reaction.  The pre-HSCT reduced-intensity conditioning consisted of antithymocyte globulin (7.5 mg per kilogram of body weight), fludarabine (starting dose of 1 mg per kilogram daily for 4 days), cyclophosphamide (1200 mg per square meter of body-surface area), total-body irradiation (200 cGy), and rituximab (200 mg per square meter) . Adjustment of fludarabine doses for all patients was based on levels in plasma and calculations of the area under the concentration–time curve .\n\n【15】In the first 30 days after HSCT, Patients 1 and 2 received 1×10  donor-derived αβ T cells per kilogram; the cells had been engineered ex vivo to express an inducible caspase 9 (iC9) suicide gene,  but the patients did not receive dimerization therapy to activate the iC9 transgene product. (At the time of the treatment of Patient 3, iC9 genetically modified T cells were no longer being manufactured.) When full (i.e., >95%) donor myeloid and lymphoid chimerism after HSCT had been confirmed, each patient received a living-donor kidney transplant from the parental HSCT donor. The patients received intraoperative methylprednisolone and postoperative low-dose oral prednisone (0.5 mg per kilogram per day with taper) and tacrolimus (target serum level of 3 to 5 ng per milliliter) to reduce potential reperfusion-related inflammation. All peritransplantation immunosuppressive drugs were tapered off by day 30 after transplantation, and no subsequent immunosuppressive therapy was administered. One-way mixed-lymphocyte culture was performed to evaluate the functional tolerance of recipient T cells to the alloantigens of the kidney-transplant donor. Peripheral blood mononuclear cells (PBMCs) were used as a source of responder T cells, and irradiated Epstein–Barr virus–transformed lymphoblastoid cell lines derived from both parents of the patients and an unrelated healthy third-party donor were used as stimulators .\n\n【16】Results\n-------\n\n【17】Outcomes\n--------\n\n【18】Table 1. Characteristics of the Patients and the Hematopoietic Stem-Cell Transplantations.\n\n【19】Because all three patients with SIOD had both moderate adaptive immunodeficiency (e.g., diminished antibody responses to vaccines and diminished T-cell responses to mitogens, vaccine antigens, or both)  and advanced renal insufficiency, we performed parental αβ–haploidentical HSCT to establish a competent donor-derived immune system that would be tolerant to a subsequent kidney allograft from the same donor. No post-HSCT pharmacologic immunosuppression was used . All three patients had successful engraftment, with full donor lymphoid and myeloid chimerism at the time of the neutrophil engraftment for Patients 1 and 3 and by day 150 after transplantation for Patient 2, and normal T-cell immune reconstitution, including normal mitogen responsiveness . When adequate nutritional status (i.e., a body-mass index \\[the weight in kilograms divided by the square of the height in meters\\] of >18.5) had been reached through the use of enteral feedings and an ambulatory level of activity had been attained, kidney transplantation from the HSCT parental donor was performed (at 5, 10, and 5.5 months after HSCT in Patients 1, 2, and 3, respectively) . Immunosuppression was withdrawn by day 30 after transplantation, and at the time of this report, 34, 23, and 22 months had passed since transplantation; renal function remained normal, without any clinical signs of rejection .\n\n【20】Figure 2. One-Way Mixed-Lymphocyte Cellular Assay of T-Cell Alloreactivity at 1 Year after Kidney Transplantation.\n\n【21】Peripheral blood mononuclear cells (PBMCs), which were used as responder cells, were isolated from the three patients at least 1 year after their kidney transplantation (KT). The PBMCs were stained with CellTrace Violet (Invitrogen) before coculture with stimulator Epstein–Barr virus (EBV)–transduced B lymphoblastoid cell lines (EBV-LCLs) previously generated from the parents of the patients or from an unrelated healthy third-party control. CellTrace Violet fluorescence intensity (on a logarithmic scale) for CD3+CD19− cells (x axis) is shown plotted against T-cell count (y axis); a loss of fluorescence indicates cell proliferation. T-cell proliferation was measured after 6 days of culture and obtained at a responder-to-stimulator ratio of . T cells of both of the two siblings (Patients 1 and 2) and of Patient 3 showed functional tolerance to EBV-LCLs derived from their respective donors, whereas they were immune competent and proliferated in the presence of EBV-LCLs derived from nondonor parents or a healthy, unrelated control. The plots in the first column show the absence of T-cell proliferation when PBMCs were cultured in medium alone. The results shown are representative of three replicates for each combination of stimulator and responder cells.\n\n【22】To evaluate the recipients for functional T-cell tolerance after kidney transplantation, PBMCs were isolated from the patients and evaluated in a one-way mixed-lymphocyte culture assay . After HSCT and kidney transplantation, the T cells of all three recipients did not proliferate in response to stimulation by the irradiated cells from their parental donors, whereas they did proliferate after stimulation by cells from the nondonor parent or third-party control cells . These results indicated that after sequential αβ–haploidentical HSCT and transplantation of a kidney from the same donor, the circulating donor-derived T cells were functionally tolerant to the alloantigens of the transplanted kidney and, therefore, potentially unable to mediate graft rejection even in the absence of immune suppression.\n\n【23】Case Reports\n------------\n\n【24】Patient 1 received a maternal αβ–haploidentical HSCT (8/10 HLA-matched) after pharmacokinetically guided pre-HSCT conditioning.  Dialysis was continued during HSCT to control fluid and electrolyte balance. Engraftment occurred on day 12 after HSCT, when the absolute neutrophil count was greater than 500 per microliter and showed full donor chimerism of both CD15+ myeloid and CD3+ T cells . On day 27, the patient received 1×10  donor-derived, iC9 genetically modified T cells per kilogram.  His post-HSCT course was complicated by asymptomatic reactivation of human herpesvirus 6 (HHV-6), which resolved with 2 weeks of ganciclovir therapy, and by grade II cutaneous acute GVHD (onset on day 52 after HSCT), which was treated with prednisone (1 mg per kilogram per day for 1 week followed by a rapid taper). CD4+ and CD8+ T cells, including their naive subsets, progressively increased after HSCT, with levels of 800 per microliter and 400 per microliter, respectively, present at 1 year after HSCT. Five months after HSCT, he received a kidney transplant from his mother followed by immunosuppression with prednisone and tacrolimus for 30 days. At the time of this report 34 months later, the patient had full donor hematopoietic chimerism, was free of clinical rejection without any immunosuppressive drugs, and had a serum creatinine level of 0.25 mg per deciliter (22 μmol per liter).\n\n【25】Patient 2, the sibling of Patient 1, received paternal αβ–haploidentical HSCT (5/10 HLA-matched) after pharmacokinetically guided pre-HSCT conditioning. Because of increasing creatinine levels and fluid overload, she began to undergo hemodialysis on day 21 after HSCT. Donor-derived iC9 genetically modified T cells (1×10  per kilogram) were given on day 24. The patient did not have any viral reactivation or signs of acute or chronic GVHD. Chimerism studies performed at the time of hematopoietic engraftment on day 14 showed mixed chimerism (71% donor cells in whole blood) that became fully donor by day 150. Because of the coronavirus disease 2019 (Covid-19) pandemic, paternal kidney transplantation was postponed until 10 months after HSCT, by which time the patient had normal T-cell function . At the time of this report, 23 months after kidney transplantation, the patient had full hematopoietic donor chimerism, was free of rejection without any immunosuppressive drugs, and had a serum creatinine level of 0.29 mg per deciliter (26 μmol per liter).\n\n【26】Patient 3 received αβ–haploidentical HSCT (7/10 HLA-matched) from her father after having received pharmacokinetically adjusted pre-HSCT conditioning while undergoing peritoneal dialysis. Her post-HSCT course was uneventful, and she reached full donor chimerism with hematopoietic engraftment at day 15. Five and a half months after HSCT, she received a kidney transplant from her father. As a result of hyperglycemia, her post–kidney transplantation treatment with tacrolimus and methylprednisolone was stopped on days 5 and 12, respectively. At the time of this report, 22 months after kidney transplantation, she had full donor hematopoietic chimerism, was free of rejection without any immunosuppressive drugs, and had a serum creatinine level of 0.32 mg per deciliter (28 μmol per liter).\n\n【27】Discussion\n----------\n\n【28】To achieve full donor chimerism while avoiding the risk of severe GVHD and to fully reconstitute the adaptive immune systems of three pediatric patients with SIOD, we successfully transplanted HLA-haploidentical hematopoietic stem cells depleted of αβ T cells and CD19 B cells after reduced-intensity conditioning. Each patient then received a kidney transplant from the same parent who served as the donor for haploidentical HSCT, and all immune suppression was withdrawn by 30 days after kidney transplantation. The αβ–haploidentical HSCT resulted in full donor hematopoietic engraftment, lymphocyte recovery, and immune reconstitution without any post-HSCT immunosuppression. One patient had transient grade II cutaneous acute GVHD, which responded to a short course of glucocorticoids. Only one patient had HHV-6 viremia. Cytomegalovirus reactivation after either HSCT or the kidney transplantation was not detected in any of the patients.\n\n【29】Previous attempts to induce tolerance by establishing transient mixed chimerism required revisions of the conditioning regimen to overcome engraftment syndrome and improve the consistency of the results.  Persistent mixed chimerism in the context of an HLA mismatch has allowed the reduction of immunosuppression after kidney transplantation to a single agent but has not allowed complete discontinuation of immunosuppression in those cases.  The establishment of 100% donor chimerism has been associated with HSCT-related toxic effects.  In contrast, our strategy resulted in rapid and persistent full donor lymphoid and myeloid engraftment and complete discontinuation of all immunosuppression by day 30 after kidney transplantation, without severe GVHD. SIOD includes both impaired T-cell immunity and impaired hematopoiesis, which might have favored the rapid appearance of functional tolerance to the kidney allograft. These results are also consistent with previous preclinical studies in mice showing functional tolerance to skin allografts after haploidentical HSCT  and the previously unplanned clinical use of kidney transplantation after HSCT.  Further studies will be needed to determine whether these outcomes can be achieved in allograft recipients with intact pretransplantation T-cell immunity and hematopoiesis.\n\n【30】We successfully transplanted kidneys into three patients with SIOD, a syndrome characterized by variable T-cell immunodeficiency; each patient underwent αβ T-cell–depleted and CD19 B-cell–depleted haploidentical HSCT followed by kidney transplantation from the same donor and continued to have normal renal function without immunosuppression at 22 to 34 months after kidney transplantation. This regimen has resulted in the correction of their primary immune deficiency.\n\n【31】Table 1. Characteristics of the Patients and the Hematopoietic Stem-Cell Transplantations. \n\n| Characteristic | Patient 1 | Patient 2 | Patient 3 |\n| --- | --- | --- | --- |\n| Age at HSCT (yr) | 6.0 | 4.8 | 8.6 |\n| HLA match of HSCT graft (no. of alleles matched/total no.) | 8/10 | 5/10 | 7/10 |\n| Donor-specific antibodies | Negative | Negative | Negative |\n| CKD stage at HSCT  | 5 | 3 | 5 |\n| Renal dialysis at HSCT | Yes | No | Yes |\n| CMV IgG serostatus |  |  |  |\n| Donor | Positive | Negative | Positive |\n| Recipient | Negative | Positive | Negative |\n| ABO and Rh type |  |  |  |\n| Donor | O+ | A+ | O+ |\n| Recipient | A+ | A+ | O+ |\n| Graft composition (cells/kg) |  |  |  |\n| CD34+ HSCs | 15×10 6 | 20×10 6 | 19.9×10 6 |\n| αβ+ T cells | 0.69×10 5 | 0.67×10 5 | 0.07×10 5 |\n| γδ+ T cells | 3.75×10 6 | 5.76×10 6 | 4.33×10 6 |\n| NK cells | 16.4×10 6 | 4.07×10 6 | 66.6×10 6 |\n| B cells | 3.75×10 5 | 50.2×10 5 | 0.9×10 5 |\n| Post-HSCT day of neutrophil engraftment  | 12 | 14 | 12 |\n| Post-HSCT day of platelet engraftment  | 16 | 14 | 15 |\n| Treatment-related toxic effects | Mucositis grade II | Mucositis grade II | Mucositis grade I |\n| CMV reactivation after HSCT or KT | None | None | None |\n| Opportunistic infections after HSCT or KT | None | None | None |\n| Chimerism at hematopoietic engraftment |  |  |  |\n| Whole blood (% donor) | 100 | 71  | 100 |\n| Day after HSCT | 12 | 14 | 12 |\n| Pre-KT chimerism |  |  |  |\n| Lymphoid (% donor CD3+) | 100 | 96 | 97 |\n| Day after HSCT | 148 | 150 | 61 |\n| Myeloid (% donor CD15+) | 100 | 99 | 99 |\n| Day after HSCT | 148 | 150 | 61 |\n| Post-KT chimerism |  |  |  |\n| Lymphoid (% donor CD3+) | 100 | 97 | 100 |\n| Day after KT | 219 | 313 | 201 |\n| Myeloid (% donor CD15+) | 100 | 96 | 100 |\n| Day after KT | 219 | 313 | 201 |\n| PHA response before KT (counts/min) | 135,082 | 161,795 | 104,191 |\n| Day of immune reconstitution of naive T cells after HSCT  |  |  |  |\n| CD4+ | 180 | 180 | 180 |\n| CD8+ | 365 | 365 | 270 |\n| Graft-versus-host disease |  |  |  |\n| Acute | Grade II skin only | None | None |\n| Chronic | None | None | None |\n| Virus reactivated after HSCT | HHV-6 | None | None |\n| iC9 genetically modified donor T cells | Yes | Yes | No |\n| Day after HSCT | 27 | 24 | — |\n| Creatinine level at last follow-up (mg/dl) | 0.25 | 0.29 | 0.32 |\n| Response to vaccination after HSCT and KT |  |  |  |\n| _Streptococcus pneumoniae_ IgG titers, 11 serotypes | Protective titers | Protective titers | Patient vaccinated but titers not yet available |\n| Tetanus antibody level (IU/ml) | \\>5.33 | \\>5.33 | Patient vaccinated but titers not yet available |\n\n【33】 The HSCT conditioning regimen received by all three patients consisted of fludarabine, cyclophosphamide, total-body irradiation (200 cGy), antithymocyte globulin, and rituximab. To convert values for creatinine to micromoles per liter, multiply by 88.4. CMV denotes cytomegalovirus, DSA donor-specific antibody, HHV-6 human herpesvirus 6, HLA human leukocyte antigen, HSC hematopoietic stem cell, HSCT hematopoietic stem-cell transplantation, KT kidney transplant, and PHA phytohemagglutinin.\n\n【34】 Chronic kidney disease (CKD) stages range from 1 to 5, with stage 5 indicating end-stage kidney disease. Stage 3 CKD is defined as a glomerular filtration rate (GFR) of 30 to 59 ml per minute per 1.73 m  , and stage 5 CKD as a GFR of less than 15 ml per minute per 1.73 m  .\n\n【35】 A definition of engraftment is provided by Kharfan-Dabaja et al. \n\n【36】 Full chimerism was present in Patient 2 by day 150 after HSCT.\n\n【37】 Immune reconstitution of naive CD4+ and CD8+ T cells has been defined as the presence of more than 50 cells per microliter. The gating strategy for naive CD4+ T cells was CD45+/singlets/CD3+/CD4+CD8−/CD27+CD45RO−, and the gating strategy for naive CD8+ T cells was CD45+/singlets/CD3+/CD4−CD8+/CD27+CD45RO−.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "10107538-eaf2-45a6-8e71-a07f3cee4785", "title": "Enrollment of Women in Cardiovascular Clinical Trials Funded by the National Heart, Lung, and Blood Institute", "text": "【0】Enrollment of Women in Cardiovascular Clinical Trials Funded by the National Heart, Lung, and Blood Institute\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】With the recognition that certain aspects of cardiovascular disease are specific to sex, the U.S. government has sought to ensure that federally funded clinical research yields adequate high-quality information about heart disease in women.\n\n【3】Methods\n-------\n\n【4】We tabulated the numbers of men and women in cardiovascular clinical trials funded by the National Heart, Lung, and Blood Institute (NHLBI) between 1965 and 1998, recording both total numbers and the numbers for each type of cardiovascular disease. We analyzed the data according to the sex-specific prevalence of disease and assessed changes in enrollment over time. We performed a similar analysis after excluding all single-sex trials.\n\n【5】Results\n-------\n\n【6】A total of 398,801 subjects (215,796 women and 183,005 men) were enrolled in NHLBI-funded studies of cardiovascular disease. The overall enrollment rate for women (54 percent) exceeded the prevalence of cardiovascular disease in women in the general population (49 percent) and increased over time (P=0.002). With single-sex trials excluded, the enrollment rate for women was 38 percent, which did not change significantly over time. In studies of coronary artery disease and hypertension the rates of enrollment of women were similar to or exceeded the prevalence of these disorders in women. The enrollment rate increased significantly over time in studies of coronary artery disease (P<0.001) but not in studies of hypertension or arrhythmia. Women were underenrolled in studies of heart failure, and the rate of enrollment did not change significantly over time. When single-sex trials were excluded from the analysis of enrollment rates according to the prevalence of disease, the results were similar. There was no change in enrollment rates over time for any category of disease.\n\n【7】Conclusions\n-----------\n\n【8】Federal efforts to increase the representation of women in clinical trials have been moderately successful primarily because of the institution of a small number of large, single-sex trials involving coronary artery disease. There has been no change in the sex composition of cohorts in the majority of studies of cardiovascular disease.\n\n【9】Introduction\n------------\n\n【10】Cardiovascular disease is the most common cause of death in both men and women in the United States and is increasingly recognized as having sex-specific features. Clinical and epidemiologic studies have shown that men and women with cardiovascular disease differ with respect to disease processes, clinical presentations, and outcomes.  The recognition of sex-based differences in cardiovascular disease is a compelling reason for ensuring that the enrollment of women in cardiovascular clinical trials is proportional to the prevalence of cardiovascular disease among women in the general population.\n\n【11】In 1965, the National Heart Institute, as the National Heart, Lung, and Blood Institute (NHLBI) was then known, funded its first cardiovascular clinical trial, the Coronary Drug Project. Since then, the institute has funded a total of 126 cardiovascular clinical trials. The enrollment of women in clinical trials came under scrutiny in the mid-1980s, with the realization that less information about treatment was available for women with cardiovascular disease than for men with the disease.  Various federal mandates and other initiatives have subsequently sought to increase the enrollment of women in clinical trials. We performed a study to determine how successful these efforts have been in increasing the enrollment of women in cardiovascular clinical trials conducted by the NHLBI. In addition, we determined whether the pattern of enrollment differed according to the type of cardiovascular disease under study.\n\n【12】Methods\n-------\n\n【13】Clinical Trials\n---------------\n\n【14】On October, 1, 1999, in response to a request filed under the Freedom of Information Act, the NHLBI provided one of us with a list of all cardiovascular clinical trials conducted between 1965 and 1998. The information included a brief description of each trial, the date of its inception, and the numbers of enrolled men and women (including boys and girls in studies of congenital heart disease). Actual numbers were provided for studies conducted before 1995 and predicted numbers for more recent studies. Of the 126 clinical trials, 5 were initiated before the NHLBI required that researchers document the sex of subjects enrolled in clinical trials; since the sex of the 1291 subjects enrolled in these 5 trials was not recorded, they were excluded from the analysis. The 121 remaining trials were categorized according to the type of cardiovascular disease being investigated (heart failure, coronary artery disease, hypertension, congenital heart disease, arrhythmias, or other). The numbers of men and women were tabulated according to the category of disease and according to whether the study was a mixed or single-sex trial (there were 54,449 men in 8 trials that enrolled only men and 138,542 women in 14 trials that enrolled only women).\n\n【15】Sex-Specific Prevalence\n-----------------------\n\n【16】Data on the sex-specific prevalence of cardiovascular disease in general and of coronary artery disease, hypertension, and heart failure were obtained from the American Heart Association  and compared with the enrollment rates for men and women in the 121 clinical trials. Given the heterogeneity of arrhythmias, data on their sex-specific prevalence may be misleading. For this reason and because the data were not easily obtained, arrhythmias and congenital heart disease were not included in the comparison. For cardiovascular disease in general, coronary artery disease, and heart failure, data on patients between the ages of 45 and 74 years were used. The data were tabulated on the assumption that the size of the population was the same for each decade of age. The sources of the data were the third National Health and Nutrition Examination Survey, the National Center for Health Statistics, and the American Heart Association. For hypertension, data on patients between the ages of 20 and 74 years were used, and the source of the data was the National Center for Health Statistics.\n\n【17】Statistical Analysis\n--------------------\n\n【18】The percentage of women enrolled in each trial was calculated. A linear regression analysis was performed to assess the relation between the starting date of the trial and the proportion of women enrolled. The results are expressed as correlation coefficients. All reported P values are two-tailed.\n\n【19】Results\n-------\n\n【20】Table 1. Enrollment of Women and Men in All NHLBI-Funded Clinical Trials, 1965–1998. Figure 1.  Figure 1. Percentage of Women among Enrollees in All Cardiovascular Clinical Trials Funded by the National Heart, Lung, and Blood Institute between 1965 and 1998, According to the Year the Trial Was Started.\n\n【21】Years are shown for trials for which data on sex were available.Table 2. Table 2. Enrollment of Women and Men in NHLBI-Funded, Mixed-Sex Clinical Trials, 1965–1998. Figure 2.  Figure 2. Percentage of Women among Enrollees in Cardiovascular Trials Involving Men and Women, According to the Year the Trial Was Started.\n\n【22】Years are shown for trials for which data on sex were available. Single-sex trials are not included.\n\n【23】A total of 398,801 subjects, including 215,796 women and 183,005 men, were enrolled in cardiovascular clinical trials funded by the NHLBI between 1965 and 1998 . The overall enrollment rate for women (54 percent) exceeded the prevalence of cardiovascular disease in women in the general population between the ages of 45 and 74 years (49 percent). In addition, the enrollment of women as a percentage of overall enrollment increased significantly over time (r=0.57, P=0.002) . However, half the women were enrolled in two large, single-sex trials investigating the primary prevention of coronary artery disease: the Women's Health Study (an investigation of the protective effects of aspirin and vitamin E in 39,876 women, started in 1991) and the Women's Health Initiative (an investigation of hormone-replacement therapy and diet in 68,135 women, started in 1992). With all single-sex trials excluded from the analysis, the total number of enrolled subjects was 205,810, of whom 38 percent were women . Furthermore, in trials that enrolled both men and women, the percentage of women did not increase significantly over time (r=0.08) .\n\n【24】In mixed and single-sex studies of coronary artery disease and hypertension, the enrollment rates for women were proportional to or greater than the prevalences of these disorders among women in the general population . The enrollment of women increased significantly over time in studies of coronary artery disease (r=0.74, P<0.001) but not in studies of hypertension (r=0.31, P=0.29). In contrast, in all trials that investigated heart failure, women were underenrolled , and the enrollment of women in trials investigating heart failure or arrhythmia did not change significantly over time.\n\n【25】Figure 3. Percentage of Women among Enrollees in Clinical Trials According to the Type of Cardiovascular Disorder and the Year the Trial Was Started.\n\n【26】With single-sex clinical trials excluded from the analysis, the enrollment rate for women was proportional to the sex-specific prevalence for studies of coronary artery disease and hypertension but not for studies of heart failure . For all three disorders, there was no significant change over time in the proportion of women enrolled in studies (r=0.07 to 0.11, P=0.64 to 0.86) . However, there was an abrupt increase in the proportion of women enrolled in studies of coronary artery disease in the mid-1980s, with minimal subsequent changes.\n\n【27】Discussion\n----------\n\n【28】Efforts to increase the enrollment of women in studies of cardiovascular disease funded by the NHLBI have been partially successful. In studies of coronary artery disease performed since the mid-1980s and in studies of hypertension performed since the mid-1960s, the rates of enrollment of women have been proportional to the sex-specific prevalence of disease.\n\n【29】Two shifts in thinking have contributed to efforts to increase the representation of women in cardiovascular clinical trials. The first was scientific and was based on a growing appreciation of the differences in the pathophysiology, presentation, treatment, and outcomes of cardiovascular disease in women and in men. Since comparatively little information was available to guide decisions about the treatment of cardiovascular disease in women, there was a call for clinical trials that would generate the data necessary for the practice of evidence-based medicine involving female patients. The second shift was societal — the movement away from a protectionist approach to the selection of research subjects, as women demanded greater autonomy in all matters of health, including the decision to participate in clinical trials. The combination of these two forces has led to a number of federal regulations designed to include more women in clinical trials in general, and in cardiovascular studies in particular.\n\n【30】Federal recognition of the need to include larger numbers of women in clinical trials began in 1985, with the recommendation of the Public Health Service's Task Force on Women's Health Issues that more attention be paid to research on women's health.  In response, the National Institutes of Health (NIH) Advisory Committee on Women's Health Issues stipulated that if women were to be excluded from a clinical trial, a satisfactory explanation would have to be provided by the researchers before their grant application would be approved.  In 1990, the General Accounting Office concluded that these recommendations had been unevenly applied.  The NIH and the Alcohol, Drug Abuse, and Mental Health Administration, in turn, instituted the requirement that all grant applications for clinical studies include a study design with a sex composition that was “appropriate to the known incidence/prevalence of the disease or condition studied.”  In addition, the Office of Research on Women's Health was established to ensure fair representation of the sexes in all studies funded by the NIH. In 1992, the Public Health Service Action Plan for Women's Health was initiated to “implement policy regarding inclusion of women in National Institutes of Health–supported clinical research.”  In 1993, the National Institutes of Health Revitalization Act (Public Law 103-43) required the inclusion of women in every clinical trial involving a disorder that affects women and directed the NIH to “ensure that the trial is designed and carried out in a manner sufficient to provide for a valid analysis of whether the variables being studied in the trial affect women \\[and men\\] differently.”  The costs of including women would not be a “permissible consideration” for waiving the requirement.\n\n【31】Since there have been no substantial increases in the proportion of women enrolled in mixed-sex trials over the past 30 years, our data indicate that, apart from single-sex trials, the temporal patterns of enrollment of women and men in NHLBI-funded cardiovascular clinical trials appear to be unrelated to the federal mandates with regard to the sex composition of study cohorts. The one observed increase in the enrollment of women — in studies of coronary artery disease in the mid-1980s — is abrupt and difficult to attribute to the federal mandates, since it predated most of them. The growing awareness of the importance of biologic differences between men and women and the enunciation of the right of women to participate in clinical trials may have played a part.\n\n【32】The federal regulations have altered the sex composition of clinical trials primarily through the institution of large trials restricted to women. Although these trials are an appropriate solution to the problem, they should not be allowed to obscure the underrepresentation of women in studies of particular types of cardiovascular disease. Most of the trials that have enrolled only women have investigated coronary artery disease; few of these trials have focused on hypertension, and none on heart failure or arrhythmias. Thus, the Women's Health Initiative and other clinical trials that have enrolled women exclusively do not cover the broad spectrum of cardiovascular diseases that affect women as well as men.\n\n【33】In a 1993 editorial, Angell cautioned against the development of a new medical specialty for women's health, stating, “It would marginalize the care of women and leave the mainstream to men, where the lack of attention to women's health would then be officially sanctioned.”  Care must be taken to ensure that with the current pattern of enrollment in NHLBI-funded cardiovascular clinical trials, including several large trials focused on a limited number of issues in coronary artery disease in women, broader issues in coronary artery disease and in cardiovascular disease in general are not neglected. However, to the extent that trials restricted to women can provide answers to specific questions, some of which have already been addressed in men, they may help correct the underenrollment of women in studies of coronary artery disease before the mid-1980s.\n\n【34】Although the enrollment of women in studies of coronary artery disease has exceeded the prevalence of the disease as a result of single-sex trials, the enrollment of women in mixed-sex trials has been commensurate with the prevalence of the disease in women. This is an important point, because trials such as the Women's Health Initiative address a limited set of questions, and broader questions must be examined in clinical trials involving both men and women. However, for such trials to address sex-related differences effectively, as required by federal regulations, subgroup analyses must be included in the study design, and the study population must be large enough to draw valid conclusions separately for women and for men. Recent data suggest that such analyses are performed infrequently. \n\n【35】Previous justifications for the underenrollment of women in clinical trials included concern about the safety of women of childbearing potential and of pregnant women and their fetuses; barriers to participation, such as the general unwillingness of women to volunteer for participation in studies; and the tendency for women to withdraw from studies before their completion. The adequate enrollment of women in studies of hypertension and coronary artery disease, however, strongly discounts these claims, indicating that women are both eligible for and willing to participate in cardiovascular clinical trials. Thus, the overall underrepresentation of women in studies of cardiovascular disease in general and in studies of heart failure specifically cannot be attributed to such factors. Other barriers may be present.\n\n【36】In view of the importance of sex-based differences in cardiovascular disease, we must obtain the data required to tailor treatment for women. The pattern of enrollment of women in studies of coronary artery disease and hypertension is encouraging, but improvement is needed in other areas. The positive findings also make it clear that the persistent underenrollment of women in certain areas is not due to barriers to their recruitment and retention. Policies designed to increase the enrollment of women in NHLBI-funded cardiovascular clinical trials have had mixed results. In fact, the proportion of women enrolled in trials involving both men and women has not changed significantly, despite a series of federal regulations intended to increase the enrollment of women and ensure the evaluation of sex-related differences in findings.\n\n【37】What steps can be taken to ensure success in all areas? The problem falls on the shoulders of researchers, funding agencies, and policy makers. More stringent NIH requirements or stricter enforcement of current regulations may seem punitive to investigators and may act as a disincentive to the performance of valuable research. The legal requirements that the levels of enrollment of men and women in NHLBI-funded clinical trials be statistically appropriate and that sex be considered as an independent variable are expensive as well as time-consuming; our data cannot be used to determine the success of this law. Single-sex clinical trials are potentially appropriate for addressing the persistent underenrollment of women and for resolving clinical issues that are specific to sex. However, they are not an adequate solution as long as the majority of cardiovascular clinical trials continue to underenroll women. The successful resolution of this difficult problem is critical for the prevention and management of cardiovascular disease in women and men.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6b9cb6d6-147e-47cc-9074-b7718fd6d11d", "title": "Current Causes of Death in Children and Adolescents in the United States", "text": "【0】Current Causes of Death in Children and Adolescents in the United States\nTo the Editor:\n--------------\n\n【1】The Centers for Disease Control and Prevention (CDC) recently released updated official mortality data that showed 45,222 firearm-related deaths in the United States in 2020 — a new peak.  Although previous analyses have shown increases in firearm-related mortality in recent years (2015 to 2019), as compared with the relatively stable rates from earlier years (1999 to 2014),  these new data show a sharp 13.5% increase in the crude rate of firearm-related death from 2019 to 2020.  This change was driven largely by firearm homicides, which saw a 33.4% increase in the crude rate from 2019 to 2020, whereas the crude rate of firearm suicides increased by 1.1%.  Given that firearm homicides disproportionately affect younger people in the United States,  these data call for an update to the findings of Cunningham et al. regarding the leading causes of death among U.S. children and adolescents. \n\n【2】Figure 1. Leading Causes of Death among Children and Adolescents in the United States, 1999 through 2020.\n\n【3】Children and adolescents are defined as persons 1 to 19 years of age.\n\n【4】The previous analysis, which examined data through 2016, showed that firearm-related injuries were second only to motor vehicle crashes (both traffic-related and nontraffic-related) as the leading cause of death among children and adolescents, defined as persons 1 to 19 years of age.  Since 2016, that gap has narrowed, and in 2020, firearm-related injuries became the leading cause of death in that age group . From 2019 to 2020, the relative increase in the rate of firearm-related deaths of all types (suicide, homicide, unintentional, and undetermined) among children and adolescents was 29.5% — more than twice as high as the relative increase in the general population. The increase was seen across most demographic characteristics and types of firearm-related death .\n\n【5】In addition, drug overdose and poisoning increased by 83.6% from 2019 to 2020 among children and adolescents, becoming the third leading cause of death in that age group. This change is largely explained by the 110.6% increase in unintentional poisonings from 2019 to 2020. The rates for other leading causes of death have remained relatively stable since the previous analysis, which suggests that changes in mortality trends among children and adolescents during the early Covid-19 pandemic were specific to firearm-related injuries and drug poisoning; Covid-19 itself resulted in 0.2 deaths per 100,000 children and adolescents in 2020. \n\n【6】Although the new data are consistent with other evidence that firearm violence has increased during the Covid-19 pandemic,  the reasons for the increase are unclear, and it cannot be assumed that firearm-related mortality will later revert to prepandemic levels. Regardless, the increasing firearm-related mortality reflects a longer-term trend and shows that we continue to fail to protect our youth from a preventable cause of death. Generational investments are being made in the prevention of firearm violence, including new funding opportunities from the CDC and the National Institutes of Health, and funding for the prevention of community violence has been proposed in federal infrastructure legislation. This funding momentum must be maintained.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "88dc5721-5819-40f7-a83c-77d9b7b2ae13", "title": "A Trial of Topical Acyclovir in Genital Herpes Simplex Virus Infections", "text": "【0】A Trial of Topical Acyclovir in Genital Herpes Simplex Virus Infections\nAbstract\n--------\n\n【1】Seventy-seven patients with first episodes of genital herpes and 111 with recurrent episodes were enrolled in a double-blind trial comparing topical acyclovir with a placebo (polyethylene glycol ointment). Among acyclovir-treated patients with first-episode primary genital herpes, the mean duration of viral shedding (4.1 days) and the time to complete crusting of lesions present at the initiation of therapy (7.1 days) were shorter than among placebo recipients (7.0 and 10.5 days, respectively) (P<0.05). Acyclovir-treated patients with recurrent herpes had a shorter duration of viral shedding than placebo recipients (0.95 vs. 1.90 days) (P = 0.03). Among the patients with recurrent herpes, acyclovir reduced the time to crusting of lesions in men but had no effect on the symptoms or healing times in women. Topical acyclovir shortens the duration of viral shedding and accelerates healing of some genital herpes simplex virus infections\\.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4460d507-cc95-4e2b-8c34-7ebe234f2e12", "title": "Reciprocal Relation between Renin Dependency and Sodium Dependency in Essential Hypertension", "text": "【0】Reciprocal Relation between Renin Dependency and Sodium Dependency in Essential Hypertension\nAbstract\n--------\n\n【1】To investigate the roles of angiotensin II and sodium in essential high-renin, normal-renin and low-renin hypertension, 14 patients received the competitive antagonist of angiotensin II, saralasin, during periods of sodium depletion and repletion. Blood-pressure response to saralasin was determined by the state of sodium balance. Patients from all three renin subgroups exhibited a fall in blood pressure when sufficiently sodium depleted, and an elevation in blood pressure when sodium replete or insufficiently depleted. However, those with low renin required loss of substantially more sodium (sufficient to elicit compensatory stimulation of renin) before depletion could be achieved.\n\n【2】In patients with essential hypertension of all three renin subgroups, sodium balance determines the degree of participation of the renin-angiotensin system in sustaining high blood pressure. Even the low-renin type can become renin dependent with sufficient sodium depletion.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a0bfad0a-cfc0-447a-b37d-82f063b443a5", "title": "Current Concepts: Autoimmune Pancreatitis", "text": "【0】Current Concepts: Autoimmune Pancreatitis\nAutoimmune pancreatitis is a form of chronic pancreatitis characterized by an autoimmune inflammatory process that may involve the biliary ducts, bowel, regional lymph nodes, and sometimes the lung and kidney. Patients with this uncommon form of pancreatitis may present with an obstructing pancreatic mass that mimics pancreatic cancer but responds to corticosteroid treatment. This review summarizes how the diagnosis of autoimmune pancreatitis can be established on the basis of imaging, histologic, and serologic criteria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f15ac56b-b331-411c-9bac-6560b9159f42", "title": "Linkage between the Gene (or Genes) Controlling Synthesis of the Fourth Component of Complement and the Major Histocompatibility Complex", "text": "【0】Linkage between the Gene (or Genes) Controlling Synthesis of the Fourth Component of Complement and the Major Histocompatibility Complex\nAbstract\n--------\n\n【1】In an attempt to map the gene (or genes) controlling the synthesis of the fourth component of complement (C4), we performed linkage studies in a family with hereditary C4 deficiency. The proband, a seven-year-old boy with lupus erythematosus, consistently lacked detectable serum C4 by both functional and protein measurements. The complement defect was transmitted as an autosomal recessive disorder. Eight of 15 family members were considered to be heterozygotes, seven because of low C4 levels and one because of genetic data (obligate heterozygote). The gene (or genes) coding for C4 deficiency appeared to be linked to the major histocompatibility complex (A2,B12, DW2 on the maternal side and A2,BW15,LD108 on the paternal side) and to other markers known to be in close proximity to the histocompatibility complex on chromosome 6 (phosphoglucomutase-3, glyoxalase-1 and properdin factor B).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5ac932b9-ddac-4095-8c5b-0eab9557a5ab", "title": "Physician–Public Defender Collaboration — A New Medical–Legal Partnership", "text": "【0】Physician–Public Defender Collaboration — A New Medical–Legal Partnership\nInvolvement in the criminal justice system is one of the most fundamental upstream determinants of health. Fortunately, a model for helping patients navigate legal issues that negatively affect their health does exist in medicine: the medical–legal partnership.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e94ad37c-cb29-4781-a2c7-63e22040150e", "title": "Physicians' Experiences with the Oregon Death with Dignity Act", "text": "【0】Physicians' Experiences with the Oregon Death with Dignity Act\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Physician-assisted suicide was legalized in Oregon in October 1997. There are data on patients who have received prescriptions for lethal medications and died after taking the medications. There is little information, however, on physicians' experiences with requests for assistance with suicide.\n\n【3】Methods\n-------\n\n【4】Between February and August 1999, we mailed a questionnaire to physicians who were eligible to prescribe lethal medications under the Oregon Death with Dignity Act.\n\n【5】Results\n-------\n\n【6】Of 4053 eligible physicians, 2649 (65 percent) returned the survey. Of the respondents, 144 (5 percent) had received a total of 221 requests for prescriptions for lethal medications since October 1997. We received information on the outcome in 165 patients (complete information for 143 patients and partial for an additional 22). The mean age of the patients was 68 years; 76 percent had an estimated life expectancy of less than six months. Thirty-five percent requested a prescription from another physician. Twenty-nine patients (18 percent) received prescriptions, and 17 (10 percent) died from taking the prescribed medication. Twenty percent of the patients had symptoms of depression; none of these patients received a prescription for a lethal medication. In the case of 68 patients, including 11 who received prescriptions and 8 who died by taking the prescribed medication, the physician implemented at least one substantive palliative intervention, such as control of pain or other symptoms, referral to a hospice program, a consultation, or a trial of antidepressant medication. Forty-six percent of the patients for whom substantive interventions were made changed their minds about assisted suicide, as compared with 15 percent of those for whom no substantive interventions were made (P<0.001).\n\n【7】Conclusions\n-----------\n\n【8】Our data indicate that in Oregon, physicians grant about 1 in 6 requests for a prescription for a lethal medication and that 1 in 10 requests actually results in suicide. Substantive palliative interventions lead some — but not all — patients to change their minds about assisted suicide.\n\n【9】Introduction\n------------\n\n【10】The Oregon Death with Dignity Act, enacted on October 27, 1997, legalized physician-assisted suicide in the state of Oregon.  This law allows the physician who has primary responsibility for managing a patient's terminal illness to prescribe a dose of lethal medication, which the patient may administer. The prognosis (death within 6 months) must be confirmed by a consultant, and the patient must make two oral requests and one written request over a period of 15 days. Referral to a mental health professional is required if either the attending physician or the consultant is concerned that the patient's judgment may be impaired by a mental disorder.\n\n【11】Physicians are required to report to the Oregon Health Division that they have prescribed the medication and complied with the act's safeguards. The Oregon Health Division has reported information on 57 patients who received prescriptions for lethal medications in 1998 and 1999, including 43 who died after administering the medications themselves.  These reports have been limited to patients who actually received prescriptions and do not provide information on physicians' experiences with requests for assistance with suicide.\n\n【12】We surveyed physicians in Oregon who were eligible to prescribe lethal medications under the new law. We sought to describe the characteristics of physicians who have received requests for assistance with suicide, the characteristics and outcomes of the patients who requested prescriptions, the reasons for the requests, and any interventions that were carried out or recommended other than the prescription of lethal medications.\n\n【13】Methods\n-------\n\n【14】We mailed a questionnaire to all licensed physicians practicing in Oregon in the fields of internal medicine and its subspecialties, family practice, general practice, gynecology, surgery and its subspecialties, therapeutic radiology, and neurology. The list of physicians was purchased from the Oregon Board of Medical Examiners. We excluded physicians in training.\n\n【15】The questionnaire was based on those used in previous studies of this issue  and on discussions with physicians in Oregon who had received requests for assistance with suicide and who had provided such assistance. Faculty members and scholars of the Project on Death in America, members of the Task Force to Improve the Care of Terminally Ill Oregonians, and physicians known to be strongly for or against the legalization of assisted suicide reviewed the questionnaire. It was refined after pretesting with a convenience sample of 20 physicians, including 6 who had prescribed medications under the provisions of the Oregon Death with Dignity Act.\n\n【16】Physicians were asked to provide information about patients who had requested a prescription for a lethal medication only if the patient was terminally ill, if the request was explicitly for a lethal prescription, and if the request was made after November 1997. The Oregon Death with Dignity Act requires that a consultant evaluate the patient to determine whether he or she meets the criteria outlined in the law for assisted suicide. We asked that only attending physicians, not consultants, complete the questionnaire in order to minimize the possibility of receiving duplicate information.\n\n【17】For most of the questions, there were forced-choice responses. We asked about the physicians' attitudes toward the law and their willingness to prescribe lethal medications according to its provisions. We also asked about demographic and clinical characteristics of the patients who had requested assistance with suicide, and the outcomes, as well as whether, on the basis of the physician's conversations with the patient, a particular value, condition, or symptom was an important factor in the decision to request the prescription. The physicians reported interventions other than a prescription for a lethal medication that they had recommended or implemented and described, in response to an open-ended question, interventions that had altered the patient's desire for assisted suicide.\n\n【18】To identify cases in which two or more physicians might be reporting information about the same patient, we matched patients for age within one year, sex, marital status, disease, and the size of the community in which the patient lived. When two or more physicians reported information that may have pertained to the same patient, we used the information from the physician who had seen the patient most recently, unless this physician did not complete the questionnaire.\n\n【19】We mailed the questionnaire in February 1999, with a reminder postcard sent two weeks later; a second copy of the questionnaire was sent to nonrespondents in March 1999, with a simultaneous fax or telephone call. In May 1999, after 47 percent of the sample had responded, we sent nonrespondents a third copy of the questionnaire with a check for $25, a letter of endorsement from the governor of Oregon, John Kitzhaber, M.D., and a simultaneous fax. Returned questionnaires were accepted through August 1999.\n\n【20】The survey was anonymous and therefore exempt from the requirement for informed consent by the institutional review board of Oregon Health Sciences University. To allow tracking of the questionnaires, each return envelope was coded with an identifying number. The questionnaire was separated from the identifying envelope on receipt and was then given a new identifying number to ensure anonymity. Completed questionnaires were scanned into an electronic data base.\n\n【21】Summary statistics included proportions for categorical variables and means and standard deviations for continuous variables. We used Pearson's chi-square test to analyze associations between variables. \n\n【22】Results\n-------\n\n【23】We identified 4544 physicians who were potentially eligible for inclusion in the survey from the list provided by the Oregon Board of Medical Examiners. On the basis of telephone calls, data bases of physicians in training, and returned questionnaires, we determined that 209 physicians were in training, 201 were retired or not in practice for another reason, 73 were no longer practicing in Oregon, and 8 had died. Of the remaining 4053 eligible physicians, 2649 (65 percent) returned the questionnaire.\n\n【24】One hundred forty-four respondents (5 percent) reported that they had received a total of 221 requests for lethal prescriptions after November 1997. Nine requests appeared to have been reported by more than one physician. Six other requests were excluded because we could not determine whether the data were duplicated. Of the remaining 206 requests, we received complete information on 143 and partial information on 22. Thus, the number of responses to specific questions varied. Twenty-seven physicians reported that they had received a total of 41 requests but gave no information about the patients. Physicians who supported the Oregon Death with Dignity Act were more likely to give partial or complete information than those who opposed the act or neither supported nor opposed it (P=0.007).\n\n【25】Physicians' Characteristics\n---------------------------\n\n【26】Table 1. Characteristics of 144 Physicians in Oregon Who Received Requests for Prescriptions for Lethal Medications.\n\n【27】Eighty-four percent of the respondents were internists, general practitioners, or family practitioners . Of the 69 internists who received requests for assistance with suicide, 24 had training in a subspecialty, including 11 in oncology and 6 in pulmonology. Forty-one physicians practiced in communities with populations of fewer than 25,000 residents. Seventy-one percent of the physicians had cared for six or more terminally ill patients, and 58 percent had referred six or more patients to a hospice program in the previous 12 months. Fifty-five percent supported the Oregon Death with Dignity Act, and 51 percent were willing to prescribe a lethal medication for a terminally ill patient. In the previous four years, 127 respondents (88 percent) had sought to improve their knowledge of the use of pain medications in the terminally ill “somewhat” or “a great deal,” 110 (76 percent) had sought to improve their ability to recognize psychiatric illnesses such as depression in the terminally ill “somewhat” or “a great deal,” and 124 (86 percent) reported that their confidence in the use of pain medications in the terminally ill had improved “somewhat” or “a great deal.”\n\n【28】Patients' Characteristics\n-------------------------\n\n【29】Table 2. Characteristics of 165 Patients Who Requested Prescriptions for Lethal Medications.\n\n【30】Seven requests for assistance with suicide were made in 1997, 112 in 1998, and 29 in 1999; in 17 cases, the year was not specified. The mean age of the 165 who requested assistance was 68 years, 97 percent were white, 52 percent were men, 46 percent were married, 5 percent (8 of 157) had not completed high school, and 2 percent had no medical insurance . Four patients had lived in Oregon for less than six months, but only one patient had moved to the state specifically because of the availability of physician-assisted suicide. Cancer was the most common diagnosis.\n\n【31】At the time of the request for assistance with suicide, 32 percent of the patients (45 of 141) were receiving hospice services, 59 percent (84 of 143) were confined to a bed or chair for more than half their waking hours, and 76 percent (108 of 142) had an estimated life expectancy of less than six months. In 41 percent of cases (58 of 140), the request was associated with an acute deterioration in the patient's medical condition. According to the physician's assessment, 20 percent of the patients had symptoms of depression, but 93 percent were competent to make medical decisions. For 80 percent of the patients (114 of 143), family members knew about the request, and the physician spoke to a family member about the request in the case of 73 percent of the patients (105 of 143). Thirteen patients kept their intentions from their family, seven patients had no family to inform, and for nine patients, the physician did not know whether the family was aware of the request.\n\n【32】Figure 1. Reasons for Requesting Prescriptions for Lethal Medications.\n\n【33】A total of 143 patients gave their physicians a specific reason for the request. Some patients gave more than one reason.\n\n【34】Symptoms that were an important consideration in the decision to request a prescription for a lethal medication (whether the patient had the symptom at the time of the request or anticipated it) were pain (for 43 percent of patients), fatigue (for 31 percent), and dyspnea (for 27 percent) . The most common conditions and values that played an important part in the patient's decision were loss of independence (for 57 percent of patients), poor quality of life (for 55 percent), readiness to die (for 54 percent), and a desire to control the circumstances of death (for 53 percent). Uncommon reasons for requested assistance with suicide were a perception of a financial burden to others (for 11 percent of patients) and lack of social support (for 6 percent).\n\n【35】Physicians' Interventions\n-------------------------\n\n【36】Physicians provided information about interventions they recommended or implemented in the case of 142 patients. The most commonly recommended interventions were pain control (for 30 percent), control of other physical symptoms (for 30 percent), seeking the advice of a colleague (for 28 percent), referral to a hospice program (for 27 percent), a mental health consultation (for 20 percent), a trial of antidepressant or antianxiety medication (for 18 percent), withdrawal of food and water as another means to hasten death (for 16 percent), a palliative-care consultation (for 13 percent), a social-work consultation (for 11 percent), a consultation with a chaplain (for 10 percent), and a transfer to another physician (for 9 percent). Interventions were implemented in approximately half the instances in which they were recommended. Physicians reported that in the cases of 42 of 140 patients, one or more interventions altered the patient's desire for a prescription for a lethal medication. These interventions included the control of pain and other symptoms (in the case of 11 patients); referral to a hospice program, general reassurance, and specific reassurance that the prescription would be made available (8 each); treatment of depression, a social-work consultation resulting in the provision of services to the family, and an alternative means of hastening death (3 each); and a palliative-care consultation (1).\n\n【37】In the case of 68 patients, including 11 of those who received prescriptions for lethal medications and 8 who died by taking a lethal medication, the physician implemented at least one substantive intervention (control of pain or other symptoms; referral to a hospice program; a mental health, social-work, chaplaincy, or palliative-care consultation; or a trial of antidepressant medication) or sought the advice of a colleague. Patients for whom a substantive intervention was made were more likely to change their minds about wanting a prescription for a lethal medication (31 of 67) than were those for whom no substantive intervention was made (11 of 73) (P<0.001). A total of 28 patients received medications for depression or anxiety or were evaluated by a mental health practitioner; 3 of the 28 changed their minds about obtaining a prescription for a lethal medication. Substantive interventions were made for 21 of the 42 patients (50 percent) enrolled in a health maintenance organization or other managed-care plan, as compared with 47 of the 101 patients (47 percent) who did not have this kind of insurance coverage (P= 0.70). Of the 18 patients who received lethal prescriptions in the absence of other substantive interventions, 11 were already receiving hospice care.\n\n【38】Thirty-five percent of the patients requested a prescription for a lethal medication from a physician other than the respondent . Twenty-seven patients were referred to 17 of the respondents specifically because of the patient's interest in receiving a prescription for a lethal medication. Fifteen of the 27 patients received prescriptions from the physicians to whom they were referred, and 7 died after taking the medication. Reflecting this referral process, 27 percent of the respondents (38 of 143) had known the patient for less than one month at the time of the request for assistance with suicide. In the group of 27 patients who had been referred to a physician in our survey specifically to receive a lethal prescription, substantive interventions were recommended for 20 patients and were implemented for 7. Despite the interventions, five of the seven patients died by assisted suicide.\n\n【39】Patients' Experiences\n---------------------\n\n【40】Table 3. Outcomes of 165 Requests for Assistance with Suicide.\n\n【41】Physicians reported the outcomes for 165 patients. Twenty-nine received prescriptions for lethal medications, and 17 died after administering them . Of the 136 patients who did not receive prescriptions, 20 percent died before all the provisions of the Oregon Death with Dignity Act had been met, 15 percent did not meet the legal criteria for receiving a prescription, and 15 percent changed their minds. Among the 44 patients who died before the physician completed the questionnaire, who were eligible to receive a prescription for a lethal medication under the act, who lived through the waiting period, and who requested a prescription from a physician willing to prescribe it, 17 (39 percent) died by taking a prescribed lethal medication.\n\n【42】Table 4. Characteristics of Physicians and Patients According to Whether the Patient Received a Prescription for a Lethal Medication.\n\n【43】Fifty-nine percent of the respondents who practiced in small towns supported the law, but physicians in small towns were unlikely to prescribe lethal medications . A request for assistance with suicide was less likely to be honored if the patient perceived himself or herself as a burden to others or was depressed and was more likely to be honored if the patient was enrolled in a hospice program or wanted to control the manner of his or her death or if cancer was the terminal disease. Patients who received prescriptions for lethal medications and those who did not receive them did not differ with respect to any other variables that we examined.\n\n【44】Respondents provided additional information about 28 patients who received prescriptions for lethal medications, including 16 who died after administering the medications. In all cases, the respondent obtained an opinion from another physician with respect to the patient's prognosis and treatment options. At the time the prescription was written, 13 patients were thought to have one to six months to live, and 15 were thought to have less than one month to live. Twenty-two patients were confined to bed or a chair during more than 50 percent of their waking hours. In the case of 18 patients, less than four weeks elapsed between the request for a prescription and its receipt.\n\n【45】Thirteen patients who died by assisted suicide were enrolled in a hospice program. In one case, a hospice refused to provide services because of the patient's interest in assisted suicide, and in another case, a patient refused hospice care. In nine cases, the physician was present when the patient took the medication. The time to death was noted in the case of 10 patients — 3 died more than five hours after taking the lethal medication. There were no reported adverse events, although one patient who was still conscious 30 minutes after taking the lethal medication was given more of the medication to take.\n\n【46】Problems Reported by Physicians\n-------------------------------\n\n【47】Some physicians who provided assistance with suicide under the Oregon Death with Dignity Act reported problems, including unwanted publicity (three physicians), difficulty obtaining the lethal medication or a second opinion (three), difficulty understanding the requirements of the law (three), difficulties with hospice providers (one), not knowing the patient (one), or the absence of someone to discuss the situation with (one). The law requires that the physician confidentially report the prescription for the lethal medication to the Oregon Health Division. Twenty-seven of the physicians had met this requirement by the time they completed the questionnaire. Some physicians were concerned about reporting because they feared that the patient's privacy (in 16 cases), their own privacy (in 18), or the privacy of the patient's family (in 15) would be violated or that retroactive sanctions would be imposed by the Drug Enforcement Agency (in 7). Four physicians expressed ambivalence about having provided assistance with suicide, though two of the four noted that they had become less ambivalent over time. One of these physicians decided not to provide such assistance again.\n\n【48】Discussion\n----------\n\n【49】We surveyed physicians in Oregon who were eligible to provide assistance with suicide under the Oregon Death with Dignity Act, in order to obtain information about their experiences with requests for prescriptions for lethal medications from terminally ill patients. One hundred forty-four physicians received a total of 221 requests and gave information on the outcomes for 165 patients, of whom 29 received prescriptions for lethal medications.\n\n【50】There is concern that with the legalization of assisted suicide, women, poor persons, and those who are members of ethnic or racial minority groups may request assistance with suicide because of inadequate social support or lack of access to health care.  The demographic characteristics of the patients who requested assistance with suicide in our survey were almost identical to those of members of the general population of Oregon who died. In 1998, 2 percent of all decedents in Oregon lacked health insurance for hospice care. In 1996, 97 percent of Oregon decedents were white, and 51 percent were men.  Moreover, concern about finances and lack of social support were rarely the reasons that patients gave for requesting assistance with suicide. The type of health care coverage was not associated with whether the patient received a prescription or whether another intervention was made. More than a third of the patients requested assistance with suicide because they perceived themselves as a burden to others, but only three of these patients received prescriptions for lethal medications, suggesting that the physicians were reluctant to accede to requests for assistance under these circumstances.\n\n【51】In the Netherlands, two thirds of requests for assistance with suicide or euthanasia are rescinded, often as the result of palliative interventions.  Similarly, we found that 39 percent of eligible patients who survived the 15-day waiting period and requested a prescription from a physician willing to provide it died by taking lethal medications that were prescribed for them. Substantive interventions by the physician led many patients to change their minds about assisted suicide. However, some patients who wanted to obtain a prescription were very determined to do so, despite palliative interventions.  Thirty-five percent of the patients had requested a prescription from at least one other physician. Eighty-one percent of those who died by assisted suicide were enrolled in a hospice program.\n\n【52】Twenty percent of the patients had symptoms of depression, a finding that is similar to the reported prevalence of depression in patients with terminal illnesses.  Depression has been reported in 59 to 100 percent of terminally ill persons interested in assisted suicide or another means of hastening death and in 80 percent of patients with cancer who committed suicide.  We could not determine whether depression was in fact less common in persons in Oregon who requested a prescription for a lethal medication or whether the physicians failed to detect depression in some instances. Nonetheless, most of the respondents reported that they had made efforts to improve their ability to recognize depression in terminally ill patients. Only 11 percent of the patients who either received a trial of medication for depression or anxiety or were evaluated by a mental health expert changed their minds about obtaining a prescription for a lethal medication.\n\n【53】Our study has several sources of bias and potential error. We do not know the experiences of the 35 percent of physicians who did not return the questionnaire. We may have underestimated duplicate patient information if physicians erred in reporting the demographic characteristics of patients. Physicians who were opposed to or uncertain about the Oregon Death with Dignity Act were significantly less likely to provide complete information about patients than were physicians who favored the act. Because of this response bias, it is difficult to make general statements about the perceptions and interventions recommended by physicians in our sample who were opposed to assisted suicide. Finally, although the physicians were instructed to base information about patients' reasons for requesting assistance with suicide only on conversations with the patients, this method of obtaining information is not as reliable as surveying patients directly.\n\n【54】In conclusion, after two years of legalized assisted suicide in Oregon, we found little evidence that vulnerable groups have been given prescriptions for lethal medication in lieu of palliative care. Physicians granted 1 in 6 requests for a prescription, and 1 in 10 requests actually resulted in suicide. As a result of palliative interventions, some patients, though not all, changed their minds about assisted suicide.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "04c5703f-197d-473f-94ce-1b276dad8828", "title": "Project NextGen — Defeating SARS-CoV-2 and Preparing for the Next Pandemic", "text": "【0】Project NextGen — Defeating SARS-CoV-2 and Preparing for the Next Pandemic\nArticle\n-------\n\n【1】The development of safe and effective Covid-19 vaccines and treatments within a year after SARS-CoV-2 was first identified represents one of the great successes of modern science.  Thanks to the ingenuity of scientists, along with cooperation between the U.S. government and the private sector, these medical countermeasures changed the trajectory of the pandemic, saving millions of lives in the United States and tens of millions globally. Today, nearly 70% of Americans have received their primary vaccination series, including nearly 95% of those over 65 years of age. \n\n【2】Although the numbers of deaths and hospitalizations due to Covid-19 have dropped by more than 90% since President Joe Biden took office, our fight against SARS-CoV-2 is not over. The virus continues to evolve rapidly  and still causes substantial numbers of infections, hospitalizations, and deaths each day. The emergence of new variants is hard to predict and continues to threaten the ability of countermeasures such as monoclonal antibodies to protect vulnerable patients. Immunity from both vaccines and infection wanes over time. The only way to stay ahead of the virus is to continue to update the composition of our vaccines and administer them in a regular cadence. Although this strategy is critical, with our current generation of vaccines, it also requires immense resources for mounting frequent vaccination campaigns — at a time when antivaccination sentiment continues to grow and the public’s appetite for regular vaccinations has waned.\n\n【3】Next-generation vaccines and treatments are needed if we are to break the cycle of responding to new variants as they appear: we need tools that can improve our bodies’ ability to stop infections, reduce transmission, build longer-lasting immunity, and target parts of the virus that are less likely to evolve.  Ideally, such vaccines and treatments would provide better protection, enabling us to avoid disruptions of our lives and continue to enjoy the activities we value.\n\n【4】Since it’s safe to assume that SARS-CoV-2 will continue to evolve, the goal for the next generation of vaccines and treatments is to be effective irrespective of that evolution — to protect against infection, transmission, and severe illness. This new approach is important for everyone, but particularly for the most vulnerable people — older adults and people who are immunocompromised, for whom infections can have more severe consequences.\n\n【5】The Biden administration has therefore announced Project NextGen, which will coordinate a whole-of-government effort to advance innovations from labs through clinical trials and safely deliver them to the public. It aims to bring new vaccines and treatments to market by investing in research and development, expanding manufacturing capability and innovation, and providing updated and streamlined regulatory guidance. This $5 billion investment will focus on three main areas: vaccines that provide broader immunity both against new SARS-CoV-2 variants and across the family of epidemic-prone sarbecoviruses, vaccines that generate effective mucosal immunity to block infection and transmission, and monoclonal antibodies that can weather viral evolution and serve as a basis for our arsenal against new threats from beta-coronaviruses.\n\n【6】Why is government investment needed at this time and for this effort? Although there is consensus that these tools are critical for our fight moving forward, current market forces have made development slow. Reduced interest in traditional vaccines has limited investments in this area. In addition, the science underpinning these efforts is difficult and requires work that is not guaranteed to pay off on the timelines that many private investors seek. There are also important scientific and regulatory challenges, such as determining how to best measure a new vaccine’s efficacy. Although companies may eventually bring the needed products to market, the current anticipated timelines could leave the public vulnerable, without additional tools, for many years. This prospect reflects a classic market failure: the costs of development have been left to private market forces that may not place adequate value on products’ broad benefits for the people of the United States and the rest of the world.\n\n【7】The U.S. government has committed to accelerating the science by streamlining development processes, using such strategies as standardizing assays, standardizing protocols, and providing timely regulatory guidance. This approach will build on years of success of the Biomedical Advanced Research and Development Authority (an agency within the Department of Health and Human Services), and it is designed to help ensure that new tools reach the American people in the shortest time possible.\n\n【8】Over the past 2 years, many experts have underscored the importance of such a new generation of tools.  And though this U.S. government investment is one of the largest to date, other organizations, including the Coalition for Epidemic Preparedness Innovation, Japan’s Strategic Center of Biomedical Advanced Vaccine Research and Development for Preparedness and Response, and the European Union’s Health Emergency Preparedness and Response Authority, have either already invested in similar initiatives or signaled their interest in doing so. This moment offers an important opportunity for us to coordinate with our international partners, create strong channels of communication so that the failure or success of one strategy can inform the path forward for another, and plan out strategies to provide rapid and wide access to products when they are successful.\n\n【9】Although our public health emergency (PHE) has ended, SARS-CoV-2 continues to evolve and pose challenges to human health. The important work of keeping Americans safe continues throughout the Biden–Harris administration. As the investment in Project NextGen makes clear, the end of the PHE did not end the government response, for the reality is that Covid-19 is unlikely to be the last pandemic we face. Technological innovations leading to new vaccines and treatments will have direct benefit in future pandemics caused by respiratory pathogens, enabling more rapid development of better vaccines against other high-priority pathogens, whether they are other coronaviruses or pandemic influenza. These innovations may also help us improve our approaches to current threats that still result in a significant burden of disease, such as seasonal influenza and respiratory syncytial virus.\n\n【10】By bringing together government agencies, scientists, and the private sector, the administration aims to catalyze a new approach to building vaccines and treatments that finally tames SARS-CoV-2 and prevents it from continuing to cause a high burden of disease. Equally important, we expect this effort to advance the science needed to better prepare our country to prevent the next pandemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "67768342-ac93-40fc-82f9-8fd605a366fe", "title": "Effect of Antibiotic Therapy in Acute Salmonellosis on the Fecal Excretion of Salmonellae", "text": "【0】Effect of Antibiotic Therapy in Acute Salmonellosis on the Fecal Excretion of Salmonellae\nAbstract\n--------\n\n【1】Treatment of patients with acute salmonella gastroenteritis with either chloramphenicol or ampicillin prolonged the period of postconvalescent excretion of salmonellae. Of 185 patients treated with antibiotics, 65.4 per cent were still positive for _Salmonella typhimurium_ 12 days after exposure, and 27.0 per cent were positive at 31 days. In contrast, of 87 who were not treated, only 42.5 per cent and 11.5 per cent were positive at 12 and 31 days, respectively. Therapy also favored the in vivo acquisition of antibiotic resistance by the infecting strain, which was initially susceptible to multiple antibiotics. Salmonella strains resistant to one or more antibiotics were isolated from 9.7 per cent of patients treated with antibiotics, whereas no resistant strains were obtained from untreated patients. Resistance transfer factor was demonstrated in nine of 12 strains with acquired multiple resistances that were studied. Thus, antibiotic therapy increases the opportunity not only for person-to-person spread of infection, but also for dissemination of resistant organisms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9d63e566-219f-456e-b6e5-f1a44dce60fb", "title": "20-Year Follow-up of Two Cases of Bilateral Hand Transplantation", "text": "【0】20-Year Follow-up of Two Cases of Bilateral Hand Transplantation\nTo the Editor:\n--------------\n\n【1】### Video\n\n【2】 Functional Ability after Bilateral Hand Transplantation \n\n【3】We report the 20-year follow-up of two men who underwent bilateral hand transplantation performed by teams led by Jean-Michel Dubernard of Lyon, France, and by Raimund Margreiter of Innsbruck, Austria.\n\n【4】Patient 1 (treated by the Lyon team) was 33 years old at the time of the transplantation, and Patient 2 (treated by the Innsbruck team) was 47 years old. In each case, the amputation of the distal forearms had occurred during an explosion.  The patients received antithymocyte globulins as induction immunosuppressive treatment and tacrolimus, mycophenolate mofetil, and glucocorticoids for maintenance therapy. After year 3, Patient 2 received mono- or dual-maintenance immunosuppression (sirolimus alone, sirolimus plus mycophenolate, or sirolimus plus glucocorticoids), as needed according to the clinical situation.  Each of the patients began an intensive rehabilitation program immediately after the transplantation procedure and adhered to the program with exceptional compliance and motivation.\n\n【5】Figure 1. Appearance of the Hands and Forearms and Histopathological Analysis in the Two Patients.\n\n【6】Shown are the appearances of bilateral forearm allografts in Patient 1  and Patient 2 . Histopathological examinations of skin samples obtained from the two patients show a mild perivascular lymphocytic infiltrate in the dermis (consistent with Banff grade 1 \\[mild\\] rejection) in Patient 1  and normal structures in Patient 2 .\n\n【7】Both recipients had a relatively uneventful follow-up course, although each patient had several acute rejection episodes (three episodes in Patient 1 and four in Patient 2); these episodes were successfully treated by transiently increasing their immunosuppressive therapy. Neither patient was found to have donor-specific alloantibodies during the entire follow-up period. No evidence of chronic rejection or graft vasculopathy was detected on histopathological examination of skin biopsy specimens , ultrasonography, angiography, or computed tomographic angiography.\n\n【8】A major concern regarding upper-limb transplantation is whether the risks associated with lifelong immunosuppression are justified for a non-lifesaving procedure in an otherwise healthy patient. Patient 1 was found to have serum sickness and transient hyperglycemia during the early post-transplantation period. Later, hypercholesterolemia, osteopenia, tinea versicolor, scabies, and transiently increased levels of serum creatinine occurred. Patient 2 had recurrent cytomegalovirus infection, along with hyperlipidemia, type 2 diabetes mellitus, two basal-cell carcinomas (nasolabial and frontal), a nasal keratoacanthoma, and a bullous pemphigoid that affected both his native and allograft skin.\n\n【9】Nerve regrowth in the transplanted hands and cortical reorganization resulted in good sensorimotor recovery in the two recipients, which allowed them to perform a majority of daily activities and to have a normal social life.  Patient 1 is able to perform most activities of daily living and started working 3 years after transplantation. Patient 2 is able to perform all daily activities with complete independence and no restrictions.\n\n【10】The two patients have recovered sensation and self-appropriation of the grafted hands . They also both report that their levels of physical and mental health and their social integration are good. Twenty years after transplantation, the two patients remain highly satisfied with their hand allografts and have long-term outcomes that show the feasibility, risks, and benefits of upper-limb transplantation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3d6e550d-057c-4553-ab96-31f19abcaabb", "title": "Drug Therapy: Chemotherapy-Induced Nausea and Vomiting", "text": "【0】Drug Therapy: Chemotherapy-Induced Nausea and Vomiting\nNew insights into the pathophysiology of chemotherapy-induced nausea and vomiting, a better understanding of patients at risk, and the availability of new antiemetic agents have all contributed to substantial improvements in emetic control. This review focuses on our current understanding of chemotherapy-induced nausea and vomiting and the status of pharmacologic interventions in its prevention and treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b869c092-00d3-4894-9f95-af6fdd95f62c", "title": "Hemodynamic and Humoral Effects of Caffeine in Autonomic Failure — Therapeutic Implications for Postprandial Hypotension", "text": "【0】Hemodynamic and Humoral Effects of Caffeine in Autonomic Failure — Therapeutic Implications for Postprandial Hypotension\nAbstract\n--------\n\n【1】We examined the effects of caffeine and meals on blood pressure and heart rate in 12 patients with autonomic failure. The influence of caffeine on plasma norepinephrine, epinephrine, and renin activity was also studied. Caffeine 250 mg, raised blood pressure by 12/6 mm Hg, from 129±25/78±12 (mean±S.D.) to a maximum of 141±30/84±16 mm Hg at 45 minutes (P<0.01), but did not change heart rate, levels of norepinephrine, or epinephrine, or plasma renin activity. Blood pressure fell by 28/18 mm Hg after a standardized meal, from 133±32/80±15 to a minimum of 105±21/62±12 mm Hg at 60 minutes (P<0.01 ). After pretreatment with 250 mg of caffeine, the standardized meal induced a fall of only 11/10 mm Hg, from 140±33/79±7to 129±31/69±13 mm Hg at 60 minutes (P<0.05 vs. values after the control meal). After long-term administration of caffeine (250 mg per day for seven days) in five patients, postprandial blood pressures remained higher after caffeine than after placebo (P<0.05).\n\n【2】We conclude that caffeine is a pressor agent and attenuates postprandial hypotension in autonomic failure, and that this effect is not primarily due to elevations in sympathoadrenal activity or activation of the renin—angiotensin system. Caffeine may be useful in the treatment of orthostatic hypotension due to autonomic failure, especially in the postprandial state.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "694f2837-e5d8-44ec-b2be-ebb69f8e6006", "title": " Infection", "text": "【0】 Infection\nTesting for _H. pylori_ , either directly on biopsy specimens or by means of stool antigen or urea breath testing, is recommended in persons at increased risk for this infection. Treatment choice depends on whether there is penicillin allergy, previous exposure to macrolides, and macrolide resistance. Testing for cure is recommended after treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "008790b8-47a0-4cca-a85c-09943ba860a6", "title": "Clinical Importance of Myeloid Antigen Expression in Adult Acute Lymphoblastic Leukemia", "text": "【0】Clinical Importance of Myeloid Antigen Expression in Adult Acute Lymphoblastic Leukemia\nAbstract\n--------\n\n【1】To determine the clinical importance of immunophenotypes in adult acute lymphoblastic leukemia (ALL), we prospectively studied 76 patients with this condition. Before treatment, lymphoblasts were tested for reactivity with monoclonal antibodies to B-cell, T-cell, and myeloid (My) antigens.\n\n【2】Unexpectedly, myeloid antigens (MCS-2 or MY9) were identified in 25 patients (33 percent), usually in conjunction with B-cell or T-cell antigens. Among My <sup>+ </sup> patients, 15 (60 percent) expressed B-cell antigens (B <sup>+ </sup> T  y <sup>+ </sup> ); all 6 tested had rearranged immunoglobulin genes. Five patients (20 percent) expressed T-cell antigens (B  T <sup>+ </sup> My <sup>+ </sup> ), and one My <sup>+ </sup> patient expressed both B-cell and T-cell antigens. Only myeloid antigens (B <sup>- </sup> T  My <sup>+ </sup> ) were expressed in four patients (16 percent); three who were tested had germ-line immunoglobulin and T-cell–receptor gene configurations.\n\n【3】Although no significant differences in presenting clinical features were found, My <sup>+ </sup> patients had fewer complete remissions than My  patients (35 vs. 76 percent, P<0.01). No differences in response or survival were observed between My <sup>+ </sup> and My  patients expressing T-cell antigens. However, among those expressing B-cell antigens, My <sup>+ </sup> patients had fewer complete remissions (29 vs. 71 percent, P = 0.02) and shorter survival (P = 0.03; median, 8.1 vs. >26 months).\n\n【4】These findings indicate that expression of myeloid antigen identifies a high-risk group of patients with adult ALL for whom alternative forms of treatment should be investigated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f3a7e53b-8de3-4d13-8473-2db18ad57365", "title": "The Web We Weave", "text": "【0】The Web We Weave\nThe traditional model of autonomy sees patients as atomistic, and the clinician’s duty as providing information and protecting the patient from undue influence. But people are interconnected and interdependent, shaped by communities and relationships with loved ones.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5878416f-5c82-4716-8a84-ff38db8041fc", "title": "Case 3-2000", "text": "【0】To the Editor:\n--------------\n\n【1】The January 27 Case Record (Case 3-2000)  concerned a patient who had systemic amyloidosis, AL type, with restrictive cardiomyopathy and multiple organ involvement. I wish to draw attention to amyloidosis of the adrenal glands, which Dr. DiSalvo mentioned briefly.\n\n【2】Secondary amyloidosis (AA type) is a rare cause of primary adrenal insufficiency (accounting for 1.73 percent of cases in one series),  since extensive accumulation of amyloid and destruction of the adrenal cortex are necessary to produce clinical symptoms. Secondary adrenal insufficiency and hypopituitarism are even more rare, occurring if there is destruction of the pituitary gland.  Primary amyloidosis (AL type) usually involves the adrenal vasculature (with angiocentric deposits) rather than the parenchyma, and adrenal insufficiency is thus thought to be less common in patients with AL amyloidosis than in those with AA amyloidosis. In one series, however, AL amyloidosis accounted for half the cases of amyloid-induced adrenal insufficiency. \n\n【3】The accumulation of amyloid, mostly in the zona fasciculata and the zona reticularis, can cause adrenal enlargement. If the amyloid is concentrated in the inner cortex, however, there may not be substantial enlargement. Certainly, some of the symptoms of adrenal insufficiency, such as orthostatic hypotension and diarrhea, may be thought by the treating physician to be due to amyloid-induced autonomic neuropathy. Also, other symptoms (such as weakness, anorexia, nausea, vomiting, and hyperkalemic acidosis) may be ascribed to uremia in patients with renal amyloid.\n\n【4】In two studies, 44 percent and 46 percent of patients with renal amyloid, manifested as proteinuria, the nephrotic syndrome, or renal failure, had abnormal results on corticotropin (250 μg) stimulation testing.  The mortality rate was high, with adrenal insufficiency causing or contributing to death in 27 percent of the patients with renal amyloid.  Isolated hypoaldosteronism has also been reported,  which may be associated with hyporeninemia, if there is involvement of the juxtaglomerular apparatus, or with hyperreninemia, if there is selective infiltration of the zona glomerulosa. Patients with hypoaldosteronism have hyponatremia, orthostatic hypotension, and hyperkalemia.\n\n【5】Some of the symptoms described in Case 3-2000 (orthostatic hypotension, diarrhea, and normocytic anemia) may be accounted for in part by adrenal insufficiency. It could be argued that all patients with renal amyloid should be evaluated for adrenal insufficiency, since treatment with glucocorticoid replacement (and mineralocorticoid replacement, if warranted) is simple and potentially effective.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f5dd308b-02a4-42b3-a3d2-c80af6f3147d", "title": "Rilzabrutinib, an Oral BTK Inhibitor, in Immune Thrombocytopenia", "text": "【0】Rilzabrutinib, an Oral BTK Inhibitor, in Immune Thrombocytopenia\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Rilzabrutinib, an oral, reversible covalent inhibitor of Bruton’s tyrosine kinase, may increase platelet counts in patients with immune thrombocytopenia by means of dual mechanisms of action: decreased macrophage (Fcγ receptor)–mediated platelet destruction and reduced production of pathogenic autoantibodies.\n\n【3】Methods\n-------\n\n【4】In an international, adaptive, open-label, dose-finding, phase 1–2 clinical trial, we evaluated rilzabrutinib therapy in previously treated patients with immune thrombocytopenia. We used intrapatient dose escalation of oral rilzabrutinib over a period of 24 weeks; the lowest starting dose was 200 mg once daily, with higher starting doses of 400 mg once daily, 300 mg twice daily, and 400 mg twice daily. The primary end points were safety and platelet response (defined as at least two consecutive platelet counts of ≥50×10  per cubic millimeter and an increase from baseline of ≥20×10  per cubic millimeter without the use of rescue medication).\n\n【5】Results\n-------\n\n【6】Sixty patients were enrolled. At baseline, the median platelet count was 15×10  per cubic millimeter, the median duration of disease was 6.3 years, and patients had received a median of four different immune thrombocytopenia therapies previously. All the treatment-related adverse events were of grade 1 or 2 and transient. There were no treatment-related bleeding or thrombotic events of grade 2 or higher. At a median of 167.5 days (range, 4 to 293) of treatment, 24 of 60 patients (40%) overall and 18 of the 45 patients (40%) who had started rilzabrutinib treatment at the highest dose met the primary end point of platelet response. The median time to the first platelet count of at least 50×10  per cubic millimeter was 11.5 days. Among patients with a primary platelet response, the mean percentage of weeks with a platelet count of at least 50×10  per cubic millimeter was 65%.\n\n【7】Conclusions\n-----------\n\n【8】Rilzabrutinib was active and associated with only low-level toxic effects at all dose levels. The dose of 400 mg twice daily was identified as the dose for further testing. Overall, rilzabrutinib showed a rapid and durable clinical activity that improved with length of treatment. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nRilzabrutinib in Immune Thrombocytopenia  \n\n【11】Immune thrombocytopenia is an acquired autoimmune disease that is characterized by immune-mediated platelet destruction and impairment of platelet production, leading to a decrease in the platelet count to less than 100×10  per cubic millimeter.  In most cases, immune thrombocytopenia is primary, with no underlying disease.  Regardless of cause, a transient or persistent decrease in platelet count can predispose patients to increased risks of bleeding, hospitalization, death, fatigue, and an impaired quality of life. \n\n【12】In adults with immune thrombocytopenia, first-line therapies (e.g., intravenous immune globulin, glucocorticoids, and anti-D, an antibody against the Rh factor on red cells) generally focus on minimizing bleeding by interfering with platelet destruction, thereby rapidly increasing the platelet count.  Although these therapies typically result in an initial response, long-term durable remission is not common, which leads to the use of continuous or alternative treatment methods.  In patients with relapsing immune thrombocytopenia, subsequent treatments with thrombopoietin-receptor agonists, rituximab, fostamatinib, and immunosuppressive therapies (e.g., vincristine, mycophenolate mofetil, and cyclosporine), as well as splenectomy in patients with refractory disease, may elicit adequate responses.  However, it is challenging to predict response to particular therapies, and neither durable responses nor long-term remissions are guaranteed. Despite the range of available therapies, a need remains for safer therapy that results in durable increases in platelet counts, prolongs remission, eliminates glucocorticoid use and related long-term toxic effects, and improves quality of life.\n\n【13】As our understanding of the underlying pathophysiology of immune thrombocytopenia has expanded, it has become clear that it is a disorder of increased immune platelet destruction and inefficient thrombopoiesis.  Although the binding of IgG autoantibodies to platelet glycoprotein surface antigens on megakaryocytes promotes their apoptosis, it also targets the platelets for phagocytosis by Fcγ receptors on macrophages in the spleen and liver and possibly the Ashwell–Morell receptor on hepatocytes.  Current therapies increase the platelet count by increasing megakaryocyte viability or by decreasing antibody-mediated platelet destruction.\n\n【14】Bruton’s tyrosine kinase (BTK) is widely expressed in many cells and plays a critical role in B-cell maturation, antibody production, and Fcγ receptor–mediated signaling pathways.  BTK inhibition has the potential to reduce Fcγ receptor–mediated macrophage function and reduce autoantibody production. Rilzabrutinib (PRN1008) is an oral, reversible, potent BTK inhibitor that was designed for the treatment of immune-mediated diseases.  The covalent binding of rilzabrutinib contributes to long BTK-target engagement and durable inhibition with limited drug exposure, a clinical advantage that is accompanied by rapid systemic clearance, which reduces the potential for off-target toxic effects.  A preclinical study identified simultaneous mechanisms of rapid antiinflammatory effects, neutralization of pathogenic autoantibody signaling, and prevention of new autoantibody production.  The high specificity of rilzabrutinib is thought to decrease the risk of off-target toxic effects (e.g., atrial fibrillation) by means of the phosphatidylinositol 3-kinase (PI3K)–AKT signaling pathway, which is associated with other BTK inhibitors. \n\n【15】An early clinical study involving healthy volunteers showed rapid, sustained, and high BTK occupancy (the level of drug binding to BTK) with no evidence of the clinically significant adverse events that have been reported with other BTK inhibitors.  In contrast to the known effects that have been observed with other BTK inhibitors, rilzabrutinib use did not alter platelet aggregation in healthy volunteers or in patients with immune thrombocytopenia.  We conducted a phase 1–2 trial to identify a safe and effective rilzabrutinib dose in patients with immune thrombocytopenia to be tested in phase 3 trials.\n\n【16】Methods\n-------\n\n【17】Trial Oversight\n---------------\n\n【18】We designed this trial and conducted it with adherence to the Good Clinical Practice guidelines of the International Council for Harmonisation E6 requirements and in accordance with the principles of the Declaration of Helsinki. The trial protocol , which is available with the full text of this article at NEJM.org, and informed-consent documents were reviewed and approved by the ethics committee at each participating institution. All the patients provided written informed consent.\n\n【19】The sponsor (Sanofi) and first author designed the trial in collaboration. The conduct of the trial was overseen by the sponsor. Data were collected by the investigators and analyzed by the sponsor. The first draft of the manuscript was written by the first author, and medical writing assistance was paid for by the sponsor. The authors reviewed the manuscript, provided feedback on drafts, and approved the final manuscript for submission for publication. All the authors vouch for the accuracy and completeness of the data, for the fidelity of the trial to the protocol, and for the complete reporting of adverse events.\n\n【20】Participants\n------------\n\n【21】Patients with immune thrombocytopenia were eligible for trial participation if they were 18 to 80 years of age (or 18 to 65 years of age in the Czech Republic and Norway) and had platelet counts of less than 30×10  per cubic millimeter on two occasions no less than 7 days apart within the 15 days before trial entry. Patients were required to have had a response to at least one previous therapy for immune thrombocytopenia (including splenectomy) but to not have had a response to the previous or concomitant therapy maintained at baseline.\n\n【22】Trial Design\n------------\n\n【23】This adaptive, open-label, dose-finding, phase 1–2 clinical trial of oral rilzabrutinib was conducted in eight countries. Rilzabrutinib was administered with intrapatient dose escalation with the use of a 3+3 design . The initial dose could be 200 mg once daily, 400 mg once daily, 300 mg twice daily (i.e., 600 mg per day), or 400 mg twice daily (i.e., 800 mg per day; the highest dose). Intrapatient dose escalation was allowed every 28 days, according to the investigator’s judgment, to improve response. A data and safety monitoring committee reviewed safety and efficacy data before doses were escalated in any patient. The initial trial protocol specified a treatment period of 12 weeks; this was subsequently extended to a 24-week treatment period followed by a 4-week safety follow-up period.\n\n【24】Only stable concomitant therapy with a glucocorticoid or thrombopoietin-receptor agonist with no more than a 10% change in the dose within the 2 weeks before the initiation of rilzabrutinib was allowed throughout the treatment period. Eligible patients (defined as those with a platelet count of ≥50×10  per cubic millimeter or with a platelet count of ≥30×10  per cubic millimeter plus a doubling of the baseline count for ≥50% of the patient’s final 8 weeks of rilzabrutinib therapy) could continue in a long-term extension study at a dose of 400 mg twice daily. Treatment was discontinued if a dose-limiting toxic effect occurred, rescue medication was used, or concomitant medication for immune thrombocytopenia was changed beyond the 10% level as defined above . Rescue medication was used for deterioration in the platelet count that, in the investigator’s opinion, put the patient at risk for a serious adverse event.\n\n【25】End Points\n----------\n\n【26】The primary end points were safety and platelet response. Safety was graded according to the Common Terminology Criteria for Adverse Events, version 4.0, of the National Cancer Institute. Platelet response was defined as at least two consecutive platelet counts (separated by ≥5 days) of at least 50×10  per cubic millimeter and an increase from baseline of at least 20×10  per cubic millimeter without the use of rescue medication for immune thrombocytopenia in the 4 weeks before the latest elevated platelet count.\n\n【27】The secondary efficacy end points were the percentage of weeks with a platelet count of at least 50×10  per cubic millimeter, the percentage of patients who had a platelet count of at least 50×10  per cubic millimeter at four or more of the final eight platelet counts, the change from baseline to the mean of the post–day 1 platelet counts among patients who had received more than 4 weeks of treatment, the number of weeks with a platelet count of at least 50×10  per cubic millimeter, the number of weeks with a platelet count of at least 30×10  per cubic millimeter, and the time to the first platelet count of at least 50×10  per cubic millimeter. Secondary safety end points included rescue medication use, the percentage of patients with a bleeding event of grade 2 or higher, and bleeding scale scores (according to an immune thrombocytopenia–specific bleeding assessment tool  ) at the end of the treatment period. Post hoc analyses of secondary efficacy end points were conducted in patients with a response. We also conducted post hoc subgroup analyses of the primary efficacy end point to evaluate the potential effect of patient and disease characteristics at baseline. Trends in the platelet counts over time are provided, with stratification according to response status on the basis of the primary efficacy outcome.\n\n【28】Statistical Analysis\n--------------------\n\n【29】The safety population included all the patients who received at least one dose of rilzabrutinib. The intention-to-treat population included all the patients who were enrolled in the trial. We calculated that an initial sample size of 24 and an expected efficacy of 40% (i.e., a platelet response in 15 patients with evaluable data) would provide the trial with 80% confidence that the true efficacy would be at least 24% (i.e., the 80% confidence interval for 40% would be 24 to 56%) using normal approximation methods. The sample size was later increased in order to better evaluate outcomes at the highest dose (400 mg twice daily). We estimated that the enrollment of 60 patients would result in approximately 15 patients completing 24 weeks of rilzabrutinib treatment and in at least 10 patients completing 24 weeks of treatment at the highest starting dose.\n\n【30】Descriptive data and frequency tabulations were summarized both overall and according to dose level. The primary efficacy end point and binomial confidence intervals were analyzed by means of the Clopper–Pearson method. Kaplan–Meier analysis was used for time-to-event estimates.\n\n【31】Results\n-------\n\n【32】Characteristics of the Patients\n-------------------------------\n\n【33】The trial was initiated on March 22, 2018, and the data-cutoff date was May 4, 2021. The reported results include all 60 patients who enrolled in the trial; eligible patients subsequently had the option to continue in the long-term extension study . Three of the 60 patients completed the original protocol for 12 weeks of treatment; their data are also included in this report.\n\n【34】Table 1. Characteristics of the Patients at Baseline (Safety Population).\n\n【35】The median age of the patients was 50 years (range, 19 to 74), and 57% of the patients were women . The median baseline platelet count was 15×10  (range, 2×10  to 33×10  ) per cubic millimeter. The median duration of immune thrombocytopenia was 6.3 years (range, 0.4 to 52.5), and patients had received a median of 4 different therapies (range, 1 to 17) for immune thrombocytopenia previously. The most common previous therapies were glucocorticoids (in 92% of the patients), thrombopoietin-receptor agonists (in 58%), intravenous immune globulin (in 43%), and rituximab (in 40%); 25% of the patients had undergone splenectomy. Patients who had been receiving glucocorticoids or thrombopoietin-receptor agonists at enrollment and had an inadequate platelet count (<30×10  per cubic millimeter) were allowed to continue stable doses of these medications.\n\n【36】Treatment\n---------\n\n【37】Patients received rilzabrutinib for a median of 167.5 days (range, 4 to 293). Nine patients started treatment at a dose of 200 mg once daily; over the treatment period, 2 continued receiving this dose, 1 had the dose escalated to a maximum of 400 mg once daily, 3 had the dose escalated to a maximum of 300 mg twice daily, and 3 had the dose escalated to a maximum of 400 mg twice daily . One patient started rilzabrutinib treatment at a dose of 400 mg once daily and had the dose escalated to a maximum of 400 mg twice daily. Of the 5 patients who started rilzabrutinib treatment at a dose of 300 mg twice daily, 2 continued receiving this dose, and 3 had the dose escalated to a maximum dose of 400 mg twice daily. All 45 patients who started rilzabrutinib treatment at the highest dose (400 mg twice daily) continued at that dose with no dose reductions or interruptions due to adverse events and no dose-limiting toxic effects.\n\n【38】Overall, 40 patients received concomitant medication (not including rescue medication). The most frequently used concomitant medications were thrombopoietin-receptor agonists (in 24 patients) and glucocorticoids (in 23). Of the 24 patients who met the primary efficacy end point , 4 patients received only concomitant thrombopoietin-receptor agonists, 8 only glucocorticoids, 3 both thrombopoietin-receptor agonists and glucocorticoids, and 9 neither. These patients were considered to have had an inadequate response to previous therapy at baseline.\n\n【39】Safety\n------\n\n【40】Table 2. Adverse Events According to Grade in All 60 Patients.\n\n【41】During the treatment period, 31 patients (52%) had at least one treatment-related adverse event; all these events were of grade 1 or 2 and were transient . The most common treatment-related adverse events of any grade were diarrhea (in 32% of the patients), nausea (in 30%), and fatigue (in 10%). One patient had a treatment-related grade 1 contusion (bleeding), and 1 had treatment-related grade 2 erysipelas (infection) that resolved with treatment. There were no treatment-related bleeding or thrombotic events of grade 2 or higher. No treatment-related adverse events of grade 3 or higher or serious adverse events were observed. There were no other signs or symptoms of adverse events that have been typically associated with BTK inhibitors (i.e., neutropenia, treatment-related infection, bleeding, thrombotic events, fungal infection, or atrial fibrillation).\n\n【42】One death occurred, which was considered by the investigator to be unrelated to treatment. This patient had discontinued rilzabrutinib at a dose of 400 mg twice daily on day 8 owing to exacerbation of preexisting Evans syndrome (a combination of immune thrombocytopenic purpura, autoimmune hemolytic anemia, and immune neutropenia) and died more than 4 months after discontinuing the trial.\n\n【43】Rescue medication was used in seven patients (12%; in one patient receiving 200 mg once daily, in one receiving 300 mg twice daily, and in five receiving 400 mg twice daily) . The use of rescue medication was the cause of trial discontinuation (as required by the protocol) in four patients. Two additional patients received rescue medication but discontinued treatment owing to adverse events unrelated to rilzabrutinib treatment. One patient received rescue medication after discontinuing the trial because of a lack of response.\n\n【44】The bleeding scale scores, as assessed with the immune thrombocytopenia–specific bleeding assessment tool, within each domain showed no increase in bleeding from baseline through the end of the treatment period . Scores improved at skin and oral sites.\n\n【45】Efficacy\n--------\n\n【46】Table 3. Efficacy End Points.\n\n【47】All 60 patients could be evaluated for efficacy in this 24-week trial. The primary end point of platelet response was met in 24 patients (40%; 95% confidence interval \\[CI\\], 28 to 53) . According to the dose level at any time during the trial (patients could have had a response at more than one dose level), 1 of 9 patients (11%) met the primary end point at a dose of 200 mg once daily, 2 of 8 (25%) at a dose of 400 mg once daily, 4 of 12 (33%) at a dose of 300 mg twice daily, and 20 of 52 (38%) at a dose of 400 mg twice daily (the highest dose). Of the 45 patients who had started rilzabrutinib at the highest dose, 18 (40%) met the primary end point of platelet response .\n\n【48】Among all the patients, the mean percentage of weeks with a platelet count of at least 50×10  per cubic millimeter was 29%; post hoc analyses involving patients who met the primary end point showed that the mean percentage of weeks with this platelet count was 65%. On the basis of a safety profile that was low-grade and with consistent efficacy observed at the highest evaluated dose, the minimum effective dose of rilzabrutinib was identified as 400 mg twice daily. No dose escalations beyond 400 mg twice daily were tested.\n\n【49】Overall, 17 of 60 patients (28%) had a platelet count of at least 50×10  per cubic millimeter at four or more of the final eight platelet counts; this end point was met in 14 of the 45 patients (31%) who had started rilzabrutinib treatment at the highest dose . A platelet count of more than 400×10  per cubic millimeter did not develop in any patient. The mean (±SD) changes from baseline to the mean of the post–day 1 platelet counts among patients who completed more than 4 weeks of treatment were 29×10  (±40×10  ) per cubic millimeter among all the patients and 31×10  (±43×10  ) per cubic millimeter among the patients who had started rilzabrutinib at the highest dose. In the overall trial population, a platelet count of at least 50×10  per cubic millimeter was maintained for a median of 1 week and a platelet count of at least 30×10  per cubic millimeter for a median of 5 weeks. Among patients with a response, 24 in the overall trial population had a platelet count of at least 50×10  per cubic millimeter maintained for a median of 16 weeks and 18 who had started rilzabrutinib treatment at the highest dose had this platelet count maintained for a median of 14 weeks; in each of these two groups, a platelet count of at least 30×10  per cubic millimeter was maintained for a median of 21 weeks .\n\n【50】Figure 1. Platelet Counts over Time in All Patients and in Patients with a Starting Rilzabrutinib Dose of 400 mg Twice Daily and a Swimmer’s Plot for All Patients.\n\n【51】The median platelet counts from the initiation of treatment through the 24-week treatment period are shown for all 60 patients  and for the 45 patients with a starting rilzabrutinib dose of 400 mg twice daily . 𝙸 bars indicate the interquartile range. The first platelet count was obtained on day 8. Horizontal lines at platelet counts of 30×10  per cubic millimeter and 50×10  per cubic millimeter represent clinically significant thresholds for platelet response. The primary end point of platelet response was defined as at least two consecutive platelet counts, separated by at least 5 days, of at least 50×10  per cubic millimeter and an increase from baseline of at least 20×10  per cubic millimeter without the use of rescue medication in the 4 weeks before the latest elevated platelet count. The swimmer’s plot  shows the durations of platelet response in the range of 30×10  to less than 50×10  per cubic millimeter (orange line) and of at least 50×10  per cubic millimeter (blue line). According to the protocol, concomitant stable doses of glucocorticoids (GC) or thrombopoietin-receptor agonists (TPO-RA), or both, with no more than a 10% change in the dose within the 2 weeks before the start of rilzabrutinib treatment, were allowed (left column). Doses of rilzabrutinib and intrapatient dose modifications are represented by the background shading color; time points for dose modifications are denoted by triangles. One patient had the dose escalated from 300 mg twice daily to 400 mg twice daily and then had the dose deescalated (as indicated by the single triangle pointing down) to 300 mg twice daily owing to grade 1 gastrointestinal events.Figure 2.  Figure 2. Subgroup Analysis of Primary Platelet Response.\n\n【52】Shown are the percentages of patients who met the primary end point of platelet response. Chronic immune thrombocytopenia (ITP) was defined as ITP that had been diagnosed more than 12 months previously.  In patients who could be evaluated for a response, concomitant ITP therapy included thrombopoietin-receptor agonists (in 17 patients), glucocorticoids (in 16), or both (in 7). Previous splenectomy was clinically relevant because refractory disease is historically defined as splenectomy failure with continued severe ITP characterized by bleeding symptoms leading to treatment. \n\n【53】The median time to a first platelet count of at least 50×10  per cubic millimeter was 11.5 days among all the patients and 12.5 days among the patients who had started rilzabrutinib treatment at the highest dose . Primary platelet responses were similar across subgroups that were defined according to immune thrombocytopenia type (chronic \\[duration of disease, >12 months\\]), receipt of at least four previous therapies, receipt of rilzabrutinib monotherapy or concomitant immune thrombocytopenia therapy, or splenectomy status .\n\n【54】Discussion\n----------\n\n【55】This clinical trial showed that BTK inhibition with rilzabrutinib was effective at suppressing immune-mediated platelet destruction in patients with immune thrombocytopenia, thus providing evidence of a new mechanism for targeting the underlying pathologic characteristics of immune thrombocytopenia. Patients had had immune thrombocytopenia for a median duration of 6.3 years and had received a median of four different therapies previously. Eligibility was based on an inability to maintain an adequate response to previous or concomitant therapies (including splenectomy) and on the basis of ongoing indication for treatment. Treatment with oral rilzabrutinib was associated with only low-grade toxic effects across all the dose levels tested, in alignment with an early clinical study involving healthy volunteers  and with projected doses from a preclinical study.  There was no evidence of infections, thrombotic events, cardiac arrhythmias, liver-related toxic effects, or increased bleeding typically associated with BTK inhibitors and thrombopoietin-receptor agonists, although the follow-up was limited.  In addition, a platelet count of more than 400×10  per cubic millimeter did not develop in any patient.\n\n【56】Rilzabrutinib therapy led to rapid platelet responses and to clinically significant platelet responses (≥50×10  per cubic millimeter) in 40% of the patients. Important treatment factors for patients with immune thrombocytopenia are the attainment of a rapid increase in the platelet count to a level preventing bleeding and the maintenance of this response over time. The median time to a first platelet count of at least 50×10  per cubic millimeter was 11.5 days; this level was maintained in patients with a response for a mean of 65% of the weeks during the 24-week treatment period. Platelet responses were consistent across subgroups defined according to baseline characteristics.\n\n【57】No standard treatment recommendations currently exist for patients with immune thrombocytopenia who have multiple relapses. Treatment and trial comparisons are further complicated by a range of time points and platelet-response end points. Recent American Society of Hematology treatment guidelines for immune thrombocytopenia lasting at least 3 months recommend thrombopoietin-receptor agonists, rituximab, or splenectomy.  Immunomodulators for the treatment of relapsed immune thrombocytopenia include the spleen tyrosine kinase inhibitor fostamatinib  and the anti-CD20 antibody rituximab.  Although the percentage of patients with durable remission is high with splenectomy (approximately 60 to 70%), this treatment is generally viewed less favorably than nonsurgical therapies owing to potential surgical complications, higher risks of infection or thromboembolic events, and a lifelong increased risk of sepsis with capsuled microorganisms.  Generally, the choice of treatment is individualized on the basis of the patient’s age, characteristics, coexisting conditions, disease duration, bleeding frequency, treatment availability, and various other preferences.  Despite considerable advances in therapeutic options for immune thrombocytopenia over the past 15 years, some patients have disease that remains refractory to existing therapies, and durable remission remains elusive in these patients. Because the majority of the patients in our trial had been heavily pretreated and many were viewed by the investigators as having limited treatment options, the inhibition of BTK by rilzabrutinib provided a new therapeutic approach by targeting immune-mediated platelet destruction and impaired platelet production.\n\n【58】This phase 1–2, dose-finding clinical trial established a role for BTK inhibition in the treatment of immune thrombocytopenia. Overall, treatment with oral rilzabrutinib led to rapid and durable clinical activity in 40% of these patients with immune thrombocytopenia, who had received multiple therapies previously. Rilzabrutinib also had a low-grade safety profile. The 400-mg twice-daily dose of rilzabrutinib was shown to be safe and effective. A randomized, double-blind, phase 3 trial comparing rilzabrutinib with placebo in adults and adolescents (≥12 years of age) with persistent or chronic immune thrombocytopenia  is ongoing to assess the magnitude and durability of clinical benefit with rilzabrutinib treatment.\n\n【59】Table 1. Characteristics of the Patients at Baseline (Safety Population). \n\n| Characteristic | All Patients(N=60) | Patients with StartingRilzabrutinib Dose of400 mg Twice Daily(N=45) |\n| --- | --- | --- |\n| Median age (range) — yr | 50 (19–74) | 49 (19–74) |\n| Sex — no. (%) |  |  |\n| Male | 26 (43) | 18 (40) |\n| Female | 34 (57) | 27 (60) |\n| Median baseline platelet count (range) — ×10 −3 /mm 3 | 15 (2–33) | 15 (2–33) |\n| Median duration of ITP (range) — yr  | 6.3 (0.4–52.5) | 6.1 (0.4–52.5) |\n| Median no. of different previous ITP therapies (range)  | 4 (1–17) | 4 (1–17) |\n| Previous splenectomy — no. (%)  | 15 (25) | 11 (24) |\n| Most common previous ITP therapies — no. (%)  |  |  |\n| Glucocorticoid | 55 (92) | 42 (93) |\n| Thrombopoietin-receptor agonist  | 35 (58) | 24 (53) |\n| Intravenous immune globulin | 26 (43) | 21 (47) |\n| Rituximab | 24 (40) | 22 (49) |\n| Fostamatinib | 8 (13) | 7 (16) |\n\n【61】 The safety population included all the patients who received at least one dose of rilzabrutinib. ITP denotes immune thrombocytopenia.\n\n【62】 The duration of ITP was defined as the difference between the date of the initial diagnosis and the date of the first dose of rilzabrutinib.\n\n【63】 The different ITP therapies were identified with the use of standardized administration codes; splenectomy was counted as one previous ITP therapy. Stable concomitant glucocorticoid or thrombopoietin-receptor agonist use was defined in the protocol as no more than a 10% change in the dose within the 2 weeks before the initiation of rilzabrutinib therapy.\n\n【64】 Thrombopoietin-receptor agonists included eltrombopag and romiplostim.\n\n【65】Table 2. Adverse Events According to Grade in All 60 Patients.\n\n| Event | Adverse Events Due to Any Cause | Adverse Events Due to Any Cause | Adverse Events Due to Any Cause | Adverse Events Due to Any Cause | Treatment-Related Adverse Events  | Treatment-Related Adverse Events  | Treatment-Related Adverse Events  | Treatment-Related Adverse Events  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | Any Grade | Grade 1 | Grade 2 | Grade 3 or 4 | Any Grade | Grade 1 | Grade 2 | Grade 3 or 4 |\n|  | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) | number of patients (percent) |\n| Any adverse event | 48 (80) | 43 (72) | 30 (50) | 8 (13)  | 31 (52) | 27 (45) | 15 (25) | 0 |\n| Diarrhea | 22 (37) | 19 (32) | 3 (5) | 0 | 19 (32) | 16 (27) | 3 (5) | 0 |\n| Nausea | 21 (35) | 18 (30) | 3 (5) | 0 | 18 (30) | 16 (27) | 2 (3) | 0 |\n| Fatigue | 12 (20) | 10 (17) | 2 (3) | 0 | 6 (10) | 5 (8) | 1 (2) | 0 |\n| Abdominal distention | 6 (10) | 6 (10) | 0 | 0 | 4 (7) | 4 (7) | 0 | 0 |\n| Vomiting | 4 (7) | 3 (5) | 1 (2) | 0 | 3 (5) | 2 (3) | 1 (2) | 0 |\n\n【67】 Adverse events were reported after the first dose of rilzabrutinib. Relatedness of the adverse event to treatment was determined by the investigators. The treatment-related adverse events listed here are those that occurred in at least 5% of the patients.\n\n【68】 Eight patients had an adverse event of grade 3 or 4 that was due to any cause and that was considered by the investigators to be unrelated to rilzabrutinib treatment. Multiple events may have occurred in a single patient. These events included grade 3 anemia (in two patients); grade 3 abnormal alanine aminotransferase level, contusion, gastrointestinal hemorrhage, hematoma, ITP, myelofibrosis, and thrombocytopenia (in one patient each); and grade 4 Evans syndrome and thrombocytopenia (in one patient each).\n\n【69】Table 3. Efficacy End Points. \n\n| End Point | All Patients(N=60) | Patients with StartingRilzabrutinib Dose of400 mg Twice Daily(N=45) |\n| --- | --- | --- |\n| Primary end point |  |  |\n| No. of patients with primary platelet response | 24  | 18 |\n| Percent of patients (95% CI) | 40 (28–53) | 40 (26–56) |\n| Secondary efficacy end points |  |  |\n| Percent of weeks with platelet count of ≥50×10 3 /mm 3 |  |  |\n| All patients | 29±35 | 28±36 |\n| Patients with primary platelet response  | 65±25 | 67±26 |\n| Platelet count of ≥50×10 3 /mm 3 at ≥4 of the final 8 platelet counts |  |  |\n| All patients |  |  |\n| No. of patients who met the end point | 17 | 14 |\n| Percent of patients (95% CI) | 28 (17–41) | 31 (18–47) |\n| Patients with primary platelet response  |  |  |\n| No. of patients who met the end point/total no. | 17/24 | 14/18 |\n| Percent of patients (95% CI) | 71 (49–87) | 78 (52–94) |\n| Change from baseline to the mean of post–day 1 platelet counts among patients who received >4 wk of treatment — ×10 −3 /mm 3  |  |  |\n| All patients | 29±40 | 31±43 |\n| Patients with primary platelet response  | 58±43 | 64±48 |\n| Median no. of weeks with platelet count of ≥50×10 3 /mm 3 (range) |  |  |\n| All patients | 1 (0–26) | 0 (0–24) |\n| Patients with primary platelet response  | 16 (2–26) | 14 (3–24) |\n| Median no. of weeks with platelet count of ≥30×10 3 /mm 3 (range) |  |  |\n| All patients | 5 (0–32) | 5 (0–24) |\n| Patients with primary platelet response  | 21 (3–32) | 21 (7–24) |\n| Median no. of days to first platelet count of ≥50×10 3 /mm 3 (range)  |  |  |\n| All patients | 11.5 (7–142) | 12.5 (8–142) |\n| Patients with primary platelet response  | 10.5 (7–71) | 11.5 (8–71) |\n\n【71】 Plus–minus values are means ±SD. The primary end point of platelet response was defined as at least two consecutive platelet counts, separated by at least 5 days, of at least 50×10  per cubic millimeter and an increase from baseline of at least 20×10  per cubic millimeter without the use of rescue medication during the 4 weeks before the latest elevated platelet count. The 95% confidence intervals were based on the Clopper–Pearson method. The widths of the confidence intervals have not been adjusted for multiplicity and should not be used to definitively infer efficacy. Efficacy end points according to the initial rilzabrutinib dose during the 24-week trial (and before the long-term extension study) are reported in Table S3.\n\n【72】 The primary end point was met at rilzabrutinib doses of 200 mg once daily (in 1 patient), 400 mg once daily (in 2), 300 mg twice daily (in 4), and 400 mg twice daily (in 20) at any time during treatment. The primary end point may have been met at different doses in the same patient.\n\n【73】 Analyses involving the 24 patients who met the primary end point and involving the 18 such patients who had started treatment at the dose of 400 mg twice daily were post hoc analyses.\n\n【74】 A total of 54 patients in the overall trial and 41 who had started treatment at the dose of 400 mg twice daily received more than 4 weeks of rilzabrutinib treatment. All 24 and 18 patients, respectively, who had a primary platelet response received more than 4 weeks of treatment.\n\n【75】 Data are shown for 30 patients in the overall trial and for 22 who started rilzabrutinib treatment at the dose of 400 mg twice daily.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "623fc1e9-0666-4dc4-8aa4-0ad9be28d227", "title": "Drug Therapy: New Drugs for Rheumatoid Arthritis", "text": "【0】Drug Therapy: New Drugs for Rheumatoid Arthritis\nRheumatoid arthritis affects about 1 percent of the U.S. population and can cause irreversible joint deformities and functional impairment. Although the cause of this autoimmune disease remains obscure, greater understanding of its underlying mechanisms has facilitated the development of new drugs and revolutionized treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f80e858a-c06b-4eeb-9d3a-efb8ac6c7d71", "title": "A Randomized, Controlled Trial of a Geriatric Assessment Unit in a Community Rehabilitation Hospital", "text": "【0】A Randomized, Controlled Trial of a Geriatric Assessment Unit in a Community Rehabilitation Hospital\nAbstract\n--------\n\n【1】We conducted a randomized trial in a community rehabilitation hospital to determine the effect of treatment in a geriatric assessment unit on the physical function, institutionalization rate, and mortality of elderly patients. Functionally impaired elderly patients (mean age, 78.8 years) who were recovering from acute medical or surgical illnesses and were considered at risk for nursing home placement were randomly assigned either to the geriatric assessment unit (n = 78) or to a control group that received usual care (n = 77). The two groups were similar at entry and were stratified according to the perceived risk of an immediate nursing home placement.\n\n【2】After six months, the patients treated in the geriatric assessment unit had significantly more functional improvement in three of eight basic self-care activities (P<0.05). Those in the lower-risk stratum had significantly more improvement in seven of eight self-care activities. Both six weeks and six months after randomization, significantly more patients treated in the geriatric assessment unit than controls (79 vs. 61 percent after six months) were residing in the community. During the year of follow-up, the control patients had more nursing home stays of six months or longer (10 vs. 3; P<0.05). However, there was no difference between the groups in the mean number of days spent in health care facilities (acute care hospital, nursing home, or rehabilitation hospital). Survival analysis showed a trend toward fewer deaths among the patients treated in the geriatric assessment unit, and mortality was significantly reduced in the patients considered to be at lower risk of immediate nursing home placement (P<0.05).\n\n【3】We conclude that the treatment of selected elderly patients in a specialized geriatric rehabilitation unit improves function, decreases the risk of nursing home placement, and may reduce mortality. The beneficial effects on mortality and function appear greatest for patients at a moderate rather than high risk of nursing home placement. \n\n【4】Introduction\n------------\n\n【5】WHEN impaired older persons become ill, they are at high risk of death or functional deterioration that leads to institutionalization.    Acute care hospitals have financial incentives to discourage the prolonged stays these patients may require to recover functional independence.  Unfortunately, older patients who remain functionally impaired after hospitalization may not qualify for skilled nursing care, and home health services may not meet all their medical, rehabilitative, and social needs. A relatively new clinical resource is the inpatient geriatric assessment unit,  a multidisciplinary unit that provides comprehensive medical and social assessment as well as treatment, often with an emphasis on rehabilitation.\n\n【6】The initial descriptive studies suggested that geriatric assessment units favorably affected health status, functional activities, and discharge to more independent living arrangements.      However, there have been few randomized trials to evaluate the effectiveness of such units. In 1984 Rubenstein and colleagues reported the results of a randomized clinical trial of treatment in a geriatric assessment unit based in a Veterans Administration (VA) hospital.  That study found that treatment in the unit reduced subsequent rates of mortality and institutionalization and lowered health care costs per year of life. Questions have remained about whether these results can be extrapolated to non-VA settings and whether the differences ascribed to the geriatric assessment unit may have been a result of specialized follow-up care in an outpatient geriatric clinic.\n\n【7】We conducted a randomized, controlled clinical trial to determine whether care for functionally impaired older patients in a geriatric assessment unit in a community-based rehabilitation hospital would affect their subsequent function, rate of institutionalization, and mortality. The patients in this study received care in the geriatric assessment unit as the only intervention and did not receive specialized follow-up care. To evaluate which types of patients might benefit most from such a unit, randomization to the geriatric assessment unit or a control group was stratified according to whether the patient was judged before randomization to be at a lower or higher risk of immediate nursing home placement.\n\n【8】Methods\n-------\n\n【9】Patients\n--------\n\n【10】The patients who participated in this study were selected from all patients referred to the geriatric assessment unit of the Baptist Memorial Hospital, Memphis, by physicians or social-work personnel. Referrals came primarily from discharge planners and an array of physicians in primary care and various specialties throughout the Baptist Memorial Hospital (a 1500-bed university-affiliated community hospital) system, although some referrals were received from each hospital in the city. In general, the patients referred were functionally impaired and were deemed to be at risk of a nursing home placement.\n\n【11】All patients referred to the geriatric assessment unit were first evaluated by one of two physicians on the staff of the unit, who judged whether or not the patients met the criteria for the study. To be eligible, a patient had to be considered at risk for nursing home placement, to have potentially reversible functional impairment, or both. In addition, the patient had to meet the following criteria for inclusion: age of 65 or older, loss of independence in more than one activity of daily living,  willingness to participate in a randomized study and give signed informed consent, and access to a primary physician willing to resume care of the patient at discharge. A few patients under the age of 65 were considered if they met all other criteria. Patients were excluded if they had medical problems that were unstable or required continued short-term monitoring, if their survival was estimated to be less than six months, if they had serious chronic mental impairment, or if a nursing home placement was considered inevitable.\n\n【12】During the enrollment period from July 1985 to June 1987, 278 referrals were received. Of the patients referred, 123 (44 percent) were considered ineligible for the following reasons: medical instability in 18 (6.5 percent of the total number of referrals), low estimate of survival in 12 (4.3 percent), excessively severe cognitive impairment in 34 (12.2 percent), insufficient medical or functional impairment in 12 (4.3 percent), refusal of consent in 21 (7.6 percent), inevitability of a nursing home placement in 8 (2.9 percent), and miscellaneous reasons in 18 (6.5 percent). The remaining 155 patients were randomly assigned, 77 to the control group and 78 to the geriatric assessment unit.\n\n【13】Randomization\n-------------\n\n【14】Randomization was conducted in a stratified-block design with a computer-generated table of random numbers. Stratification was performed before randomization according to whether the patient, the family, or the consulting physician thought that the patient had a higher or lower risk of immediately going to a nursing home. In the case of patients whose mental status was intact, their opinion of the likelihood of immediate nursing home placement was used in the stratification. If the patient was confused, the family's opinion was used or, if there was no family, the physician's opinion was used. At the initial consultation, the patient (or family) was asked the following question: \"If you were discharged from the hospital today, where would you go?\" If the answer was \"definitely (or possibly) to a nursing home,\" the patient was placed in the higher-risk stratum. If the patient or the family mentioned another option for placement at discharge, the patient was placed in the lower-risk stratum.\n\n【15】After the initial consulting physician had seen the patient and agreed that the patient was a candidate for the study, a trained interviewer obtained informed consent from the patient or family member and collected base-line data. If no new information was discovered that would serve to exclude the patient from the study, the interviewer obtained the randomization assignment. The time from referral to randomization averaged two days, and the time from randomization to admission to the geriatric assessment unit averaged three days.\n\n【16】The Geriatric Assessment Unit\n-----------------------------\n\n【17】The geriatric assessment unit studied was a 10-bed unit in a rehabilitation hospital that occupies a separate building within the Baptist Memorial Hospital complex. The unit's organization and functioning and the specific roles of the staff members have been described in detail elsewhere.    In the unit, emphasis was placed equally on an interdisciplinary assessment of the problems of the patients and on rehabilitation. The primary objective of the unit was to improve health and functional status sufficiently that patients at risk of institutionalization could avoid placement in a nursing home. The secondary purpose was to provide rehabilitation for acutely disabled older patients, regardless of the risk of nursing home placement.\n\n【18】In the geriatric assessment unit, an interdisciplinary assessment of medical, social, and psychological function was completed within 72 hours of admission by a team of physicians (university faculty and fellows), rehabilitation nurses, physical therapists, occupational therapists, psychologists, social workers, nutritionists, and specialists in speech therapy and audiology.    Particular attention was paid to problems common in frail, hospitalized older persons, such as incontinence, decubitus ulcers, contractures, confusion, sensory disorders, depression, toxic effects of medication, inadequate nutrition, weakness or lack of motivation, inability to perform activities of daily living, gait disorders, and falls. Also, the degree of family and other social support was assessed.\n\n【19】After the assessments were completed, the team determined at the first of a series of weekly meetings whether the patient was a candidate for a specific treatment, rehabilitation, or both. If medical treatment was required, the patient was either treated in the unit or returned to the care of the referring physician. Any patient with a defect in vision, hearing, or speech was referred to the appropriate therapist. If the patient needed rehabilitative care, a rehabilitation plan with specific goals was developed, and the patient's progress was reevaluated weekly. All patients receiving rehabilitative care were required to have a degree of impairment such that physical, occupational, or recreational therapy was needed in some combination three times a day in order to meet Medicare requirements. When patients reached their rehabilitation goals or attained a stable level of function, they were discharged without any subsequent services from the geriatric-assessment-unit team.\n\n【20】Control Group\n-------------\n\n【21】Neither the staff members of the geriatric assessment unit nor the investigators in the study were involved in the care of the patients in the control group after randomization. Instead, the controls received the usual care provided by their physicians. There were no differences between groups in the specialties of the primary physicians providing care for their patients; two thirds of the patients in each group received primary care from internists in the community. The patients in the control group received a wide range of services after discharge from the acute care hospital, including home health care in 36 patients (47 percent) and care in other rehabilitation units in 17 (22 percent).\n\n【22】The care provided to patients in the control group in the Baptist Memorial Hospital system would compare favorably with national norms. The Baptist Memorial Hospital offers a diverse range of services (including the free-standing rehabilitation hospital that houses the geriatric assessment unit). Because the patients were referred by physicians in many specialties and received no special follow-up care, it is unlikely that the control group received care different from that given to the geriatric-assessment-unit group, except for care received in the unit itself.\n\n【23】Measures\n--------\n\n【24】All data were collected by trained interviewers who did not participate in the clinical care of the patients. The evaluations of the patients were made before randomization and six weeks, six months, and one year after randomization. The interviews after discharge were conducted at each patient's place of residence. The data collected included demographic characteristics, information on place of residence, self-reported ability to perform physical activities ranging from basic self-care to moving heavy objects,  observed performance on timed physical tests (including timed manual-function tests),  scores on the Center for Epidemiologic Studies depression (CES-D) scale,  and results of the Folstein Mini—Mental State examination. \n\n【25】For the activities-of-daily-living or ADL scale, the data on eight indexes were graded on scales with five points, as follows: ( 1 ) unable to perform the activity, (2) requires assistance of another person and equipment, (3) requires assistance of another person, (4) requires assistance of equipment only, and (5) requires no assistance. Independence with respect to each activity was defined as the ability to perform the task without assistance from another person. Minimal ADL impairment was defined as independence on six or more of the indexes, moderate impairment as independence on two to five indexes, and severe impairment as independence on no more than one index. Data on nursing home use and hospital use were obtained from the patient and the family and were verified by the institutions involved.\n\n【26】At the end of the study, the patients' severity of illness during the hospital stay before randomization was assessed retrospectively from the medical records by a physician experienced in such assessments and not previously involved in the study. Severity was measured as an APACHE II score for each patient with the method of Knaus and colleagues.  In cases of missing or undocumented clinical data, normal values with no points for severity were assigned. The assignments to intervention groups were in the medical records, but the risk-stratification assignments were not.\n\n【27】Statistical Analysis\n--------------------\n\n【28】For continuous variables, the comparability of the study groups was tested with Student's t-test for differences between the means of two independent samples (pooled estimate of population variance); the chi-square test of statistical independence was used for categorical variables.  All analyses related to outcomes used the intention-to-treat approach, with all data for a patient assigned to the group to which the patient had been assigned at randomization. Survival distributions and distributions of the time to a first nursing home admission were analyzed with the log-rank-test statistic.    Hazard ratios and their 95 percent confidence intervals were estimated with Cox proportional-hazards regression models.    All statistical tests were nondirectional (two-sided), with P = 0.05 used as the criterion of statistical significance.\n\n【29】Before the start of the study, we estimated the sample sizes needed for an alpha level of 0.05 (two-tailed) and a power equal to 0.80. These calculations showed that if half the frail elderly patients who were hospitalized and had loss of function were placed in nursing homes within one year, a total sample of 180 would be needed for reliable detection of a reduction in the rate of nursing home placement to 30 percent in the intervention group. Although projections indicated that it would be possible to enroll more than 200 subjects in the study, time and fiscal constraints allowed the randomization of only 155 subjects.\n\n【30】Results\n-------\n\n【31】Table 1. Characteristics of 155 Patients at Randomization.\n\n【32】Table 1 shows the base-line demographic characteristics and functional status of the 155 patients enrolled. The mean age of the patients was 78.8 years (range, 61 to 100). There were no significant differences between the geriatric-assessment-unit group and the control group with respect to base-line demographic data, scores on the CES-D inventory and the Mini—Mental State examination, activities of daily living, or measures of timed physical performance. There were no significant differences in types of diagnoses or severity-of-illness scores between the two groups. The most common diagnoses at referral were as follows: hip fracture in 28 patients (18 percent), other fractures in 23 (15 percent), other conditions requiring orthopedic surgery in 9 (6 percent), conditions requiring nonorthopedic surgery in 14 (9 percent), circulatory disorders in 21 (14 percent), stroke in 15 (10 percent), musculoskeletal disorders in 9 (6 percent), psychiatric disorders in 7 (5 percent), endocrine disorders in 6 (4 percent), pulmonary disorders in 6 (4 percent), and miscellaneous medical disorders in 17 (11 percent). There was a trend toward longer initial hospital stays in the control group (P = 0.09).\n\n【33】Table 2. Characteristics of the Patients at Randomization According to Risk of Immediate Placement in a Nursing Home.\n\n【34】The base-line demographic characteristics, severity-of-illness scores, and functional status of the patients after stratification according to higher or lower risk of immediate nursing home placement are shown in Table 2 . The patients in the higher-risk stratum had significantly longer initial hospital stays, higher severity-of-illness scores, and higher levels of dependence as measured on the ADL scale at base line. A logistic regression analysis of the stratum assignments identified six variables that were related to the opinion expressed by the patient or family at randomization about the likelihood of an immediate nursing home placement if functional status did not improve. These variables were increased length of initial hospital stay (P<0.001), age (P<0.01), inability to bathe independently (P<0.005), inability to eat independently (P<0.05), previous nursing home stay (P<0.05), and scores indicating depression on the CES-D scale (P = 0.09). With this six-factor model the stratum placement was identified correctly for 79 percent of the patients.\n\n【35】The mean (±SD) length of stay in the geriatric assessment unit was 23.6±13.2 days. For the high-risk stratum, the average stay was 28.6± 14.4 days, and for the lower-risk stratum it was 21.1±11.9 days. The outcomes with respect to functional status, residence and use of health services, and mortality are described here. Data are presented for each of the study groups as a whole, then for the two risk strata.\n\n【36】Functional Status\n-----------------\n\n【37】Table 3. Mean Changes after Six Months in Eight Indexes of the ADL Scale, According to Treatment Group.\n\n【38】As Table 3 shows, the group assigned to the geriatric assessment unit had significantly more improvement (P<0.05) than the control group in regard to three basic self-care activities (bathing, dressing, and the ability to transfer) during the six months after randomization and tended to have less deterioration in one other activity (the ability to administer medications). However, these differences between groups disappeared after one year. When performance in these basic activities was analyzed within risk strata, the lower-risk patients in the geriatric assessment unit had significantly greater improvement (P<0.05) after six months in seven of eight activities than did the control patients at lower risk. In the higher-risk stratum, there were no differences between patients in the geriatric assessment unit and the control patients in terms of change in the ability to perform these activities.\n\n【39】We also collected data on cognition and depressive symptoms. Assignment to the geriatric assessment unit was not found to affect subsequent Mini—Mental State or CES-D scores.\n\n【40】Residence and Use of Health Services\n------------------------------------\n\n【41】At follow-up, smaller percentages of the patients assigned to the geriatric assessment unit than of the control patients were living in institutions (either a nursing home or a board-and-care home): after six weeks, 6 patients assigned to the geriatric assessment unit (8 percent) were living in institutions, as compared with 17 controls (24 percent; P<0.01 by chi-square test); after six months, there were 8 (11 percent) as compared with 14 (23 percent), respectively (P = 0.08); and after one year, 7 (11 percent) as compared with 15 (26 percent; P<0.05). Residence in an institution was less common in the geriatric-assessment-unit group at each follow-up interval for both risk strata, but the difference was statistically significant only in the higher-risk stratum. Survival analysis with a proportional-hazards regression model was used to compare the groups with regard to the risk of admission to a nursing home during the year after randomization. In the control patients, the risk of nursing home admission was 3.3 times higher (95 percent confidence interval, 2.6 to 3.8; P<0.001 by the log-rank test), although this calculation includes some short-term stays in nursing homes by the control patients, who did not go to the geriatric assessment unit.\n\n【42】An examination of the number of days spent in a nursing home revealed a trend for the patients in the geriatric assessment unit to spend fewer mean days than the controls in nursing homes during the study period (28.6 vs. 55.8; P = 0.07 by t-test) but more days in rehabilitation settings (24.3 vs. 4.9; P<0.0001). There were no differences in the mean number of days per patient spent in acute care hospitals (13 vs. 13) or in the total days per patient spent in any of these types of health care facilities (geriatric-assessment-unit group, 69 days; control group, 74).\n\n【43】To determine whether time spent in the geriatric assessment unit could prevent long-term nursing home placement as compared with short-term (convalescent) placement, we divided the nursing home stays into two groups: those of six months or more and those of less than six months. After one year, significantly fewer patients assigned to the geriatric assessment unit had nursing home stays of six months or more (3 vs. 10; P<0.05 by the chi-square test).\n\n【44】Table 4. Frequency with Which Patients Returned to Residence in the Community.\n\n【45】One of the most important goals of rehabilitation is that of returning patients to independent living in the community. As shown in Table 4 , both six weeks and six months after randomization, significantly more patients in the geriatric-assessment-unit group than in the control group were living in the community as opposed to residing in an institution or having died. A similar trend was observed after one year (P = 0.058).\n\n【46】Mortality\n---------\n\n【47】Figure 1. Cumulative Mortality of 78 Patients Randomly Assigned to the Geriatric Assessment Unit (Dashed Line) and 77 Controls (Solid Line).\n\n【48】P = 0.08 for the difference in survival risk between the two groups of patients after six months (log-rank test).\n\n【49】Figure 1 shows mortality curves for the two groups. A survival analysis of these data showed a trend toward earlier death in the control group throughout the year after randomization. The differences were greatest after six months (P = 0.08 by the log-rank test), but they diminished thereafter. By six months, 8 patients in the geriatric-assessment-unit group (10 percent) and 16 patients in the control group (21 percent) had died.\n\n【50】Figure 2. Mortality Curves within the Higher-Risk Stratum (Upper Panel) and the Lower-Risk Stratum (Lower Panel) of Patients Randomly Assigned to the Geriatric Assessment Unit (28 Patients at Higher Risk and 50 at Lower Risk) (Dashed Line) and the Control Group (27 at Higher Risk and 50 at Lower Risk) (Solid Line).\n\n【51】P = 0.92 for the difference in survival risk after six months between groups in the higher-risk stratum, and P = 0.025 for the difference in survival risk after six months between groups in the lower-risk stratum (log-rank test).\n\n【52】The results comparing mortality among the geriatric-assessment-unit patients and the controls are presented according to risk strata in Figure 2 . In the lower-risk stratum, the patients in the control group had a risk of death 4.3 times that of the patients in the geriatric-assessment-unit group six months after randomization (95 percent confidence interval, 1.2 to 15.2; P<0.05 by the log-rank test). In contrast, there was no survival advantage attributable to care in the geriatric assessment unit in the higher-risk stratum.\n\n【53】Discussion\n----------\n\n【54】Our randomized trial of a geriatric assessment unit in a community-based rehabilitation hospital found that the patients in the unit improved in physical function significantly more than the control patients and were less likely to be residing in nursing homes or board-and-care homes at each follow-up. The patients who went to the geriatric assessment unit spent more days in a rehabilitation setting but were less likely to be admitted to a nursing home and spent fewer days in nursing homes. When the total number of days spent in any health care facility during the year after randomization was analyzed, there were no significant differences between the patients in the geriatric assessment unit and the controls. However, significantly fewer patients assigned to the unit stayed in a nursing home longer than six months, and more such patients were living in the community after each follow-up interval. Care in the geriatric assessment unit did not result in fewer deaths over the one-year study period, although there was a trend toward increased survival in this group overall, particularly in the six months after randomization.\n\n【55】Before the start of the study, it was known that the patients referred to the geriatric assessment unit were prognostically heterogeneous. Thus, randomization was stratified according to the perceived risk of an immediate nursing home placement. Although the analyses of subgroups should be viewed with caution, prognostic stratification at randomization is recommended in situations in which the study cohort may be extremely diverse in terms of the base-line risk of an ultimate outcome.  Also, the a priori nature of a stratified randomization increases the credibility of the inferences drawn. Some important differences in outcome between the patients assigned to the geriatric assessment unit and the controls were observed between the risk strata. For the stratum at higher risk of a nursing home placement, care in the unit appeared to decrease the likelihood of residence in an institution but did not influence mortality or physical function. For the lower-risk group, care in the unit appeared to improve survival significantly during the first six months, with a trend still favoring the unit after one year. In addition, care in the geriatric assessment unit appeared to improve physical function more in this lower-risk stratum than in the higher-risk stratum.\n\n【56】The differences between strata in the effect of the geriatric assessment unit on outcomes are consistent with the clinical profile of the patients at base line. The patients in the higher-risk stratum had higher severity-of-illness scores, longer initial hospital stays, and more functional impairment. These patients may have been too ill and impaired to benefit fully from the intervention in terms of improved mortality and function. The finding that the group assigned to the geriatric assessment unit in the lower-risk stratum (which was somewhat less functionally dependent and less severely ill) had significant improvements in function and survival suggests that these units may have their maximal effect on the health of patients who are moderately but not severely impaired.\n\n【57】Because our study was based on referrals, it is impossible to estimate accurately the proportion of hospitalized patients over the age of 65 who might be eligible for care in a geriatric assessment unit. A previous study found that 4 percent of all persons over 65 who were admitted to a VA hospital (and 8.5 percent of those who stayed in the hospital longer than one week) would be eligible for such care.  It seems likely that similar percentages should apply in this community-hospital setting. As Table 2 shows, approximately 65 percent of the patients admitted to the geriatric assessment unit were in the lower-risk group. Although we do not report financial data, some estimates can be made. There were no differences between the patients assigned to the geriatric assessment unit and the controls with respect to subsequent hospital stays. The mean total charges for inpatient rehabilitation during the study period were approximately $400 per patient per day,  whereas the mean total nursing home charges in this community range from $40 to $90 per patient per day. Therefore, the charges during the one-year study for the mean excess of 19 days the patients in the geriatric assessment unit spent in rehabilitation settings as compared with the controls (24 vs. 5 days, respectively) would probably not have been offset by the savings from the mean reduction of 27 days (29 vs. 56 days) spent in nursing homes. Whether a longer study would have found more savings as a result of fewer nursing home days is unknown.\n\n【58】The limitations of our study include the fact that we did not enroll a sample of the size estimated to be necessary, thus increasing the probability of a Type II error for some of the comparisons. Because the study involved a single intervention site, the generalizability of the results to other sites cannot be guaranteed. In addition, the subjective nature of the stratifications, although highly predictive of patient outcome, does not allow definitive statements about the appropriate targeting of services.\n\n【59】The effect of our geriatric assessment unit on patients' subsequent mortality and function was less striking than that reported previously by Rubenstein et al.  In their study, the intervention group had a 50 percent reduction in mortality and a 48 percent improvement in overall function. However, the effect of the geriatric assessment units on nursing home residence was comparable in their study and ours. There are several crucial differences between the studies that may help explain the differences in the findings. First, the intervention group of Rubenstein et al. received both care in a geriatric assessment unit and specialized follow-up care in a geriatric clinic. Second, our sample was drawn from a community hospital instead of a VA hospital. Third, our sample consisted predominantly of women, whereas theirs was almost all men. On the other hand, both studies demonstrated reductions in nursing home residence and improvements in physical function in the intervention groups; both studies involved at-risk populations and combined assessment with intensive treatment. That the patients in our study, unlike those in the study of Rubenstein et al., received no special intervention after the geriatric assessment unit argues strongly that treatment in the geriatric assessment unit alone has a significant effect on clinical outcome.\n\n【60】Our findings are also supported by earlier descriptive studies    and one cohort study  involving geriatric assessment units. However, other studies combining geriatric assessment with treatment in different settings, including inpatient consultation services       and outpatient assessment services,     have had conflicting results, and the effectiveness of these services is still questioned. Recent reviews of the effect of geriatric assessment in various settings have been published.    According to the summary statement of a recent National Institute on Aging Consensus Conference, data from studies of the various types of geriatric assessment indicate that the forms of assessment that have shown the most impressive results to date are those that target at-risk patients, are based on studies of inpatients, and combine assessment with intensive treatment or rehabilitation.  Our study supports this conclusion, because our inpatient geriatric assessment unit identified patients with functional disabilities, and the outcome data indicated that the unit improved function and increased the probability of continued residence in the community. Furthermore, our analysis of the differences in outcome between risk strata indicated that there may be a threshold effect with regard to the degree of benefit patients will receive from the services of a geriatric assessment unit. Our data suggest that the beneficial effect of such a unit on mortality and physical function may be greatest for elderly hospitalized patients with moderate illness and moderate functional impairment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "007ae609-c113-48ad-b778-5ef29b72bb7e", "title": "The Occupation of Fishing as a Risk Factor in Cancer of the Lip", "text": "【0】The Occupation of Fishing as a Risk Factor in Cancer of the Lip\nAbstract\n--------\n\n【1】To study the role of commercial fishing and related factors in the development of lip cancer, a project that included a case–control study and a cohort analysis was undertaken in Newfoundland. Household survey data were linked with cancer-registry and census data. In comparison with other males, fishermen had a probability of development of lip cancer that was 1.5 times higher (by the case–control method, P < 0.05) or 4.4 times higher (by cohort analysis, P < 0.001 ). Despite the effect of pipe smoking, \"outdoorness\" and age on the development of lip cancer in general, the occupation of fishing was an additional, independent contribution to the risk. Unexpectedly, using the mouth as a third hand to handle tar-coated nets seemed to protect fishermen from the disease. It was not possible to attribute the higher risk to a particular work activity, nor was a specific responsible carcinogen identified.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "10e8a892-c8e1-4774-9db5-dd97baec90a4", "title": "The Confirmation and Maintenance of Smallpox Eradication", "text": "【0】The Confirmation and Maintenance of Smallpox Eradication\nAbstract\n\n【1】In December 1979, an independent scientific commission certified global eradication of smallpox. This conclusion was accepted at the 33d World Health Assembly of the World Health Organization (WHO) in May 1980. After WHO'S intensified eradication program began in 1967, special certification procedures were used in 35 countries where the disease had been endemic and in 44 others at special risk.\n\n【2】Six laboratories are known to retain variola virus; efforts have been made to ensure strict containment of these strains.\n\n【3】There is no evidence that smallpox will recur as an endemic disease. Nevertheless, WHO will promote surveillance of smallpox-like disease and selected laboratory research on certain orthopoxviruses. These efforts will maintain confidence that smallpox has been eradicated and confirm that there are no animal reservoirs of variola virus. A more complete understanding of the orthopoxviruses, including monkeypox virus, should also be obtained.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1279b569-38dd-44de-be12-9317c9bd1c50", "title": "Rooting Out Gun Violence", "text": "【0】Rooting Out Gun Violence\nArticle\n-------\n\n【1】Here we are again. Less than a year ago, an editorial in the _Journal_ by Kassirer reexamined the massive public health problem of gun violence in the United States,  and a Perspective article by Sacks, born of a personal tragedy, lamented the defunding of research on firearm-injury prevention.  Kassirer called for electing “lawmakers at all levels of government with the courage to defy gun lobbyists,” so that essential regulatory changes can finally be enacted — as physicians, public health experts, and others have been recommending for decades. But in early December, the day after a young couple turned up at a holiday party in San Bernardino, California, with semiautomatic weapons and went on a shooting rampage, killing 14 people and injuring 21, Congress voted down a measure that would have prevented people on the terrorist watch list from getting guns and stalled on a measure to enhance background checks.  An existing federal ban on military-style assault weapons, which ought to be an uncontroversial, if inadequate, first step, was allowed to lapse in 2004.\n\n【2】In 2013, the latest year for which the Centers for Disease Control and Prevention (CDC) has data, more than 33,000 people in the United States died from gunshot injuries and more than 84,000 survived with such injuries, many of them requiring ongoing care for both physical and mental health. After this year’s high-profile events in San Bernardino, Colorado Springs, Charleston, and Roseburg, Oregon, commentators once again marveled at the vast gap between U.S. rates of gun deaths and those in other developed countries, which either have never witnessed the epidemic of gun violence and the kind of mass shootings that are now routine here or have acted effectively to stop them.  If any other public health menace were consistently killing and maiming so many Americans, without research, recommendations, and action by the CDC, the public would be outraged.\n\n【3】Sign up for Alerts & Updates\n\n【4】Stay up to date on relevant content from the _New England Journal of Medicine_ with free email alerts.\n\n【5】SIGN UP\n\n【6】But in the United States, the National Rifle Association (NRA), the legislators it has funded, and a certain breed of gun owners have stood in the way even of research to determine what policies might help. The single or double gun deaths that go on around us every day rarely evoke any public response, and the predictable reflex response to the louder mass shootings, after exhortations to pray for the victims, takes two forms: calls for reviving research and calls for improving mental health care.\n\n【7】No doubt both of these avenues are important to pursue. Though the research pathway often feels like a delaying tactic doomed to encounter the same barriers that efforts to change laws face, House Democrats did on December 10 insert into an omnibus funding bill a provision lifting the ban on federal funding for gun-violence research. While the mental health care pathway, for its part, could have broader benefits, it would, at best, solve a tiny fraction of the problem — 3 to 5%, says Friedman, noting that “most people who are violent are not mentally ill, and most people who are mentally ill are not violent.”  It also seems to make mental health workers or primary care physicians responsible for the impossible task of diagnosing and treating anyone who might conceivably start shooting people.\n\n【8】Most important, neither of these responses gets at the root of the problem. Something in the psychological or sociological makeup of the United States has left us at this long-standing impasse. Gun-control advocates blame the NRA, but perhaps the NRA is less like a foreign pathogen that has invaded our body politic, to which we could mount an immune response, than like a cancer, growing from our own mutated cells. And that dangerous mutation seems to be a sad distortion of the American principle of individualism that prioritizes one’s right to live the way one wants, without any government interference, over other people’s right to live at all — a distortion that has found one of its key expressions in firearm-related freedoms. But gun violence is an assault on the health of the public. An equally fundamental American principle holds that ensuring the public health sometimes requires curbing the rights of individuals in order to benefit and protect the community as a whole.\n\n【9】Previous commentators including Hemenway and Miller have listed among the steps toward reducing gun violence “changing social norms.”  Given that it requires “deep cultural changes,”  however, that is far easier said than done — it is, as Wintemute has argued, “the work of generations.”  But it is work that we need to begin. If we never address the underlying beliefs that sustain this guns-everywhere extremism, we will not be able to diminish its power. Too many Americans will continue to get their hands on assault weapons, too many will kill or maim other Americans, and we will continue to bicker about whether the first step is more research or better mental health care — while we continue to do nothing to cure the disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e6e44ca6-4d70-4021-b2e1-81226119660c", "title": "Effects of Urea and Cyanate on Sickling in Vitro", "text": "【0】Effects of Urea and Cyanate on Sickling in Vitro\nAbstract\n--------\n\n【1】Erythrocytes from patients with SS disease were incubated under reduced oxygen tension to determine the comparative effects of urea and cyanate upon the sickling phenomenon. At a urea concentration of 0.2 M (BUN, 560 mg per 100 ml), no inhibition of sickling could be demonstrated. A more sensitive index of the rate of sickling used for further studies involved measurement of the rate of potassium loss from sickle cells undergoing sickling in a hypoxic environment. Whereas 20 mM CNO  markedly inhibited sickling and potassium loss under 3 per cent oxygen, potassium loss was only slightly decreased by 0.2 M urea and was not significantly reduced by 0.1 M urea (BUN, 280 mg per 100 ml). In other studies, oxygen affinity of hemoglobin S was increased by 0.4 M urea (BUN, 1120 mg per 100 ml) but not by 0.1 M urea. Hypoxia-induced hyperviscosity of sickle blood was only slightly decreased by 0.1 M urea. Thus, urea appears to have an inhibitory effect on sickling in vitro but only at concentrations exceeding those that can be practically achieved in vivo.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "56c494bb-b97b-47fb-9e30-baf826c5b3df", "title": "Prophylaxis of Gonococcal and Chlamydial Ophthalmia Neonatorum", "text": "【0】Prophylaxis of Gonococcal and Chlamydial Ophthalmia Neonatorum\nAbstract\n--------\n\n【1】We evaluated the use of silver nitrate drops and tetracycline ointment for the prophylaxis of ophthalmia neonatorum in a controlled trial involving 2732 newborns in Nairobi, Kenya. The overall rates of prevalence of intrapartum maternal gonococcal and chlamydial infection were 6.4 and 8.9 percent, respectively. After prophylaxis with silver nitrate, the incidence rates of gonococcal, chlamydial, and nongonococcal, nonchlamydial ophthalmia neonatorum were 0.4, 0.7, and 6.2 percent, respectively, whereas after prophylaxis with tetracycline, the rates were 0.1, 0.5, and 4.5 percent. The attack rates of gonococcal ophthalmia neonatorum in newborns exposed to _Neisseria gonorrhoeae_ at birth were 7.0 percent in those receiving silver nitrate and 3.0 percent in those receiving tetracycline (95 percent confidence interval for the difference in rates, -3.4 to 11.4 percent). As compared with historical controls, the incidence of gonococcal ophthalmia neonatorum decreased 83 percent among infants treated with silver nitrate and 93 percent among those treated with tetracycline. Failure of prophylaxis was associated with postpartum maternal endometritis (P = 0.05).\n\n【2】Among newborns exposed to maternal infection with _Chlamydia trachomatis_ , chlamydial conjunctivitis developed in 10.1 percent given silver nitrate and in 7.2 percent given tetracycline (95 percent confidence interval for the difference in rates, -4.7 to 10.5 percent), yielding reductions in the incidence of chlamydial ophthalmia of 68 and 77 percent, respectively, as compared with the historical controls.\n\n【3】We conclude that tetracycline is as effective as silver nitrate in preventing gonococcal ophthalmia neonatorum.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "36148752-5b1b-4af8-a027-7efed0062b6e", "title": "Carbohydrate and Fat Metabolism in Patients with Pheochromocytoma", "text": "【0】Carbohydrate and Fat Metabolism in Patients with Pheochromocytoma\nAbstract\n--------\n\n【1】Abnormal carbohydrate metabolism in pheochromocytoma has been ascribed to glycogenolysis provoked by catecholamines. Studies in two patients with this tumor revealed two additional factors: suppression of insulin release and resistance to the hypoglycemic action of endogenous or exogenous insulin. Immediately after removal of the tumor, the response of plasma insulin levels to glucose and tolbutamide was exaggerated, but resistance to the hypoglycemic effects of insulin continued. These findings are attributed to concomitantly elevated plasma cortisol levels. With time, carbohydrate metabolism reverted toward normal.\n\n【2】Before operation elevated fasting plasma free fatty acid levels (FFA) fell after oral glucose, whereas, unexpectedly, plasma glycerol levels did not. Both levels were reduced after exogenous insulin. In postoperative patients both plasma glycerol and FFA fell after glucose had been administered by mouth.\n\n【3】Direct inhibition of lipolysis by insulin appeared decreased owing to both suppression of and resistance to this hormone during hypercatecholaminemia. Hyperglycemia enhanced re-esterification and regulated FFA, presumably thereby averting ketoacidosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "fc3fb2d5-23df-482e-a90a-d73488588381", "title": "Preexposure Chemoprophylaxis for HIV Prevention in Men Who Have Sex with Men", "text": "【0】Preexposure Chemoprophylaxis for HIV Prevention in Men Who Have Sex with Men\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Antiretroviral chemoprophylaxis before exposure is a promising approach for the prevention of human immunodeficiency virus (HIV) acquisition.\n\n【3】Methods\n-------\n\n【4】We randomly assigned 2499 HIV-seronegative men or transgender women who have sex with men to receive a combination of two oral antiretroviral drugs, emtricitabine and tenofovir disoproxil fumarate (FTC–TDF), or placebo once daily. All subjects received HIV testing, risk-reduction counseling, condoms, and management of sexually transmitted infections.\n\n【5】Results\n-------\n\n【6】The study subjects were followed for 3324 person-years (median, 1.2 years; maximum, 2.8 years). Of these subjects, 10 were found to have been infected with HIV at enrollment, and 100 became infected during follow-up (36 in the FTC–TDF group and 64 in the placebo group), indicating a 44% reduction in the incidence of HIV (95% confidence interval, 15 to 63; P=0.005). In the FTC–TDF group, the study drug was detected in 22 of 43 of seronegative subjects (51%) and in 3 of 34 HIV-infected subjects (9%) (P<0.001). Nausea was reported more frequently during the first 4 weeks in the FTC–TDF group than in the placebo group (P<0.001). The two groups had similar rates of serious adverse events (P=0.57).\n\n【7】Conclusions\n-----------\n\n【8】Oral FTC–TDF provided protection against the acquisition of HIV infection among the subjects. Detectable blood levels strongly correlated with the prophylactic effect. \n\n【9】Introduction\n------------\n\n【10】A total of 2.7 million new infections with the human immunodeficiency virus (HIV) were diagnosed worldwide in 2008, according to the Joint United Nations Program on HIV/AIDS (UNAIDS). Combination antiretroviral therapy for patients with HIV infection restores health and may decrease the transmission of the virus to uninfected partners.  Therapy also decreases mother-to-child transmission. \n\n【11】Postexposure chemoprophylaxis is recommended after occupational or nonoccupational exposure to HIV-infected fluids.  The use of such chemoprophylaxis requires that people recognize when they might have been exposed to HIV and that they start therapy within 72 hours. Both challenges are substantial limitations to the use of postexposure chemoprophylaxis. \n\n【12】We selected emtricitabine (FTC) and tenofovir disoproxil fumarate (TDF) combination therapy in a single tablet (FTC–TDF) for evaluation of preexposure prophylaxis because of several favorable characteristics.   The protective activity of FTC and TDF has been shown in mice transplanted with human immune cells  and in nonhuman primates.  In these studies, there were increased levels of efficacy when both agents were used together, as compared with the use of either agent alone. The administration of the drug both before and after exposure was important for maximizing the protective benefit. \n\n【13】Daily preexposure prophylaxis with oral TDF had an acceptable side-effect profile in a trial involving West African women.  A tenofovir 1% vaginal gel reduced HIV infection rates by 39% among women.  Men and transgender women who have sex with men are disproportionately affected by the global epidemic.  Surveys of such persons in the United States indicate that the current use of preexposure prophylaxis is rare, although the majority would consider such use if evidence of safety and efficacy became available. \n\n【14】In this multinational study, called the Preexposure Prophylaxis Initiative (iPrEx) trial, we aimed to evaluate the safety and efficacy of once-daily oral FTC–TDF as compared with placebo for the prevention of HIV acquisition among men and transgender women who have sex with men.\n\n【15】Methods\n-------\n\n【16】Protocol Development\n--------------------\n\n【17】We developed the concept and protocol for this study using methods that came to be approved as “good participatory practices” by UNAIDS.  The development of the protocol was sponsored by the National Institute of Health's Division of Acquired Immunodeficiency Syndrome (DAIDS). The protocol was approved by national government public health authorities in Peru, Ecuador, South Africa, Brazil, Thailand, and the United States and by the ethics committee at each site. All subjects provided written informed consent. The study coordinator vouches for the fidelity of the report to the protocol. The study protocol is available at NEJM.org, and a detailed description of the methods is provided in the Supplementary Appendix .\n\n【18】Study Population and Randomization\n----------------------------------\n\n【19】Inclusion criteria were male sex at birth, an age of 18 years or older, HIV-seronegative status, and evidence of high risk for acquisition of HIV infection. Subject codes were randomly assigned in blocks of 10, stratified according to site. The subject codes were assigned consecutively at the study sites to eligible subjects at the time of the first dispensation of a study drug. Serologic testing for hepatitis B was performed at screening.\n\n【20】Study Visits\n------------\n\n【21】Study visits were scheduled every 4 weeks after enrollment. Each 4-week visit included drug dispensation, pill count, adherence counseling, rapid testing for HIV antibodies, and taking of a medical history. Chemical and hematologic analyses were performed at weeks 4, 8, 12, 16, and 24 and every 12 weeks thereafter. During screening, a computer-assisted structured interview collected information about education level, self-identified sex, and alcohol use, along with subjects' perceived study-group assignment at week 12. High-risk behavior was assessed by interview every 12 weeks, and physical examinations and evaluations for sexually transmitted infections were performed at least every 24 weeks. Visits through May 1, 2010, are included in this report of the primary analysis of safety and efficacy. The visit cutoff date was set by the study sponsor without any access to interim findings and was intended to ensure observation of the targeted number of seroconversion events (85). The use of study drugs was intensively monitored and promoted (for details, see Methods in the Supplementary Appendix ).\n\n【22】Standard Prevention Interventions\n---------------------------------\n\n【23】At every scheduled visit, subjects received a comprehensive package of prevention services, including HIV testing, risk-reduction counseling, condoms, and diagnosis and treatment of symptomatic sexually transmitted infections, including gonorrhea and chlamydia urethritis, syphilis, and herpes simplex virus type 2 (HSV-2). In addition, at 24-week intervals, subjects were screened for asymptomatic urethritis, syphilis, antibodies to HSV-2, and genital warts and ulcers; treatment was provided when indicated. Sexual partners were offered treatment of sexually transmitted infections that were diagnosed in the subject. Subjects were linked to local prevention and treatment services when required to receive standard-of-care services. All subjects were instructed to protect themselves from HIV with conventional methods, since they were unaware of their study-group assignment. Subjects who reported a recent unprotected exposure to an HIV-infected partner were referred for postexposure prophylaxis (at sites where such therapy was available), and the administration of a study drug was temporarily suspended. Vaccination against hepatitis B virus (HBV) was offered to all susceptible subjects.\n\n【24】Laboratory Testing\n------------------\n\n【25】Testing for HIV antibody was performed on whole blood with the use of two different rapid tests at every scheduled visit, and reactive rapid tests were tested with the use of Western blot analysis of serum . Subjects with failed rapid tests were retested during the visit. HIV plasma RNA testing with the use of an assay with a lower limit of quantitation of 40 copies per milliliter was performed if seroconversion was detected within 12 weeks after enrollment. RNA testing was also used to identify the first date of laboratory evidence of infection for the as-treated analysis. Testing for drug-resistance genotyping and phenotyping was performed with the use of clinically validated assays on the basis of the viral load at the seroconversion visit.\n\n【26】Subgroup Analysis of Drug Levels\n--------------------------------\n\n【27】A prespecified subgroup analysis was performed to investigate whether drug levels correlated with protective effect. Subjects with HIV infection were matched with two control subjects, one from each study group who were selected from among seronegative subjects, according to study site . Plasma was tested for the presence of FTC and tenofovir (TFV), and peripheral-blood mononuclear cells were tested for FTC triphosphate (FTC-TP) and TFV diphosphate (TFV-DP), which are the active intracellular metabolites of FTC and TFV, respectively, with the use of validated liquid chromatography and tandem mass spectrometry assays.\n\n【28】Study Oversight\n---------------\n\n【29】The study was designed by four of the investigators in collaboration with all the site investigators and communities. DAIDS reviewers approved the protocol, which was developed by the study investigators, and monitored the conduct of the trial at study sites. The Bill and Melinda Gates Foundation also provided funding but did not have a role in protocol development or site monitoring. Gilead Sciences donated both FTC–TDF and placebo tablets and provided travel-related support for meetings conducted by non-Gilead investigators. The role of Gilead Sciences in the development of the protocol was limited to sections regarding the handling of the study drugs. Neither Gilead Sciences nor any of its employees had a role in the accrual or analysis of the data or in the preparation of the manuscript. DAIDS agreed to give Gilead 30 days to comment on the manuscript, but there was no agreement to accept suggestions. The first author wrote the first draft of the manuscript (except for the drug-level sections, which were drafted by another investigator) and decided to submit the manuscript for publication. The protocol statistician and data manager vouch for the accuracy of the data, and the protocol chair and site investigators vouch for the completeness of the reported data.\n\n【30】Statistical Analysis\n--------------------\n\n【31】Data were collected on case-report forms and faxed to a DataFax server at DF/Net Research. It was determined that the observation of 85 incident HIV infections would yield a power of at least 80% with a one-sided alpha level of 0.05 to reject a null hypothesis of efficacy of 30% or less if the true efficacy were 60% or more. The modified intention-to-treat analysis included available data for all subjects except those with HIV RNA detected in their enrollment sample. The as-treated analysis used a time-dependent covariate indication as to whether the subject was known to fall below the prespecified level of study-drug compliance (50%) on any of the following: records of study-drug dispensation alone, pill-use calculation on the basis of study-drug dispensation and returns, and subjects' self-report. For the as-treated analysis, pills from unreturned bottles were assumed to have been taken, and late visits were included in the analysis if the last dispensation allowed pill use on 50% or more of days. Safety analyses included all subjects.\n\n【32】Results\n-------\n\n【33】Study Subjects\n--------------\n\n【34】Figure 1. Enrollment and Outcomes.\n\n【35】The most common laboratory abnormalities that led to exclusion were elevations in hepatic aminotransferase levels, hyperbilirubinemia, and renal insufficiency. A total of 18 enrollees (0.7%) did not meet all eligibility criteria, including 2 subjects with preexisting diabetes mellitus, who were instructed to stop taking a study drug when the history was discovered. All enrolled subjects, including those who were subsequently found to be ineligible, were followed for HIV infection and safety. Quarterly-visit attendance is shown. Visits were considered to have been completed if they occurred before the subsequent visit window, with completion rates of 75 to 94% for all visits. The completion rate was more than 86% for all visits before week 132. Visits occurred within the protocol-defined window of ±5 days in 62 to 86% of visits.Table 1.  Table 1. Baseline Characteristics of the Subjects.\n\n【36】Of 4905 subjects who were screened, 2499 were enrolled in the study from July 10, 2007, through December 17, 2009, at 11 sites in six countries . The baseline characteristics of the two study groups were similar . All subjects were born male, although 29 (1%) reported their current gender identity as female. The ages of the subjects ranged from 18 to 67 years; the FTC–TDF group was on average 9 months older than the placebo group (mean age, 27.5 vs. 26.8 years; P=0.04).\n\n【37】Among HBV-susceptible subjects at screening, 94% accepted HBV vaccination. We enrolled 13 subjects with chronic HBV infection that was detected at screening, and acute HBV infection was reported as an adverse event in 3 additional subjects (2 in the FTC–TDF group and 1 in the placebo group) after enrollment when elevated liver aminotransferase levels were observed. All the HBV infections resolved with detectable levels of immunity.\n\n【38】Follow-up and Adherence\n-----------------------\n\n【39】The cohort was followed for 3324 person-years with a variable duration of observation (median, 1.2 years; maximum, 2.8 years) . There were no significant trends in visit completion rates over time. Most subjects said they did not know their study-group assignment at week 12, and those who guessed their assignment were evenly distributed between the two groups . No subjects were told their study-group assignment during the course of the trial. A study drug was temporarily discontinued in 21 subjects (8 in the FTC–TDF group and 13 in the placebo group) so that they could receive postexposure prophylaxis for HIV (P=0.28).\n\n【40】The rate of self-reported pill use was lower in the FTC–TDF group than in the placebo group at week 4 (mean, 89% vs. 92%; P<0.001) and at week 8 (mean, 93% vs. 94%; P=0.006) but was similar thereafter (mean, 95% in the two groups). At each visit, a portion of subjects (approximately 6%) did not report the number of pills missed. The percentage of pill bottles returned was 66% by 30 days and 86% by 60 days. The rate of pill use that was estimated according to pill count also increased during the first 8 weeks and then remained stable at a median ranging from 89 to 95%, depending on whether pills from unreturned bottles were counted as having been taken or not taken. On the basis of pill-dispensation dates and quantities, the rate of pill use decreased during the first year, from 99% to 91%, a trend that contrasted with pill counts and self-report, which indicated an increased rate of use.\n\n【41】Sexual Practices\n----------------\n\n【42】Sexual practices were similar in the two groups at all time points (P=0.97) . The total numbers of sexual partners with whom the respondent had receptive anal intercourse decreased, and the percentage of those partners who used a condom increased after subjects enrolled in the study. There were no significant between-group differences in the numbers of subjects with syphilis (P=0.49), gonorrhea (P=0.74), chlamydia (P=0.43), genital warts (P=0.53), or genital ulcers (P=0.62) during follow-up .\n\n【43】Safety\n------\n\n【44】Table 2. Adverse Events.\n\n【45】In testing for elevations in serum creatinine levels, there were 41 instances of elevations that were at least 1.1 times the upper limit of the normal range or more than 1.5 times the baseline level. Of these elevations, 26 (2%) were in the FTC–TDF group and 15 (1%) were in the placebo group (P=0.08). Two of these elevations increased in grade, accounting for a total of 43 creatinine adverse events . Overall, 18 creatinine elevations (44%) remained in the normal range, and 36 (88%) were not confirmed on the next test. A total of 10 elevations led to discontinuation of a study drug (7 in the FTC–TDF group and 3 in the placebo group); study drugs were restarted in 9 subjects. Serum creatinine levels were elevated at more than one consecutive test in 5 subjects in the FTC–TDF group (<1%) and in none of the subjects in the placebo group. All elevations in the serum creatinine level resolved after the discontinuation of a study drug, within 4 weeks in 3 subjects, within 12 weeks in 1 subject, and within 20 weeks in 1 subject. Four of the subjects resumed taking FTC–TDF without recurrence of the elevation.\n\n【46】Moderate nausea (grade 2 and above) was reported more frequently in the FTC–TDF group than in the placebo group (22 vs. 10 events, P=0.04), as was unintentional weight loss of 5% or more (34 vs. 19 events, P=0.04) (for details, see Table S10 in the Supplementary Appendix ).\n\n【47】Effect of FTC–TDF on HIV Acquisition\n------------------------------------\n\n【48】Figure 2. Kaplan–Meier Estimates of Time to HIV Infection (Modified Intention-to-Treat Population).\n\n【49】The cumulative probability of HIV acquisition is shown for the two study groups. The efficacy of preexposure prophylaxis with emtricitabine and tenofovir disoproxil fumarate (FTC–TDF) was 44%, as compared with placebo (P=0.005). The inset graph shows a more detailed version of the overall graph up to a probability of 0.10.Figure 3.  Figure 3. HIV Incidence among Subjects Receiving FTC–TDF, According to Subgroup.\n\n【50】The efficacy of emtricitabine and tenofovir disoproxil fumarate (FTC–TDF) is 1 minus the hazard ratio. Hazard ratios of less than 1 indicate efficacy, and 95% confidence intervals (shown by horizontal lines) that do not cross 1 indicate significant evidence of efficacy. All subgroup analyses were prespecified except for testing for herpes simplex virus type 2 (HSV-2) at screening and pill use at the rate of 90%. P values for the intention-to-treat analysis and the modified intention-to-treat analysis apply to the hypothesis of any evidence of efficacy; P values for other comparisons refer to the hypothesis that efficacy differed between the two strata. NA denotes not applicable, and URAI unprotected receptive anal intercourse.\n\n【51】HIV rapid testing was performed at 39,613 visits, during which there were false reactive tests for 3 subjects at 7 visits; each subject had multiple negative tests afterward. HIV seroconversion was observed in 110 persons, of whom 10 had plasma HIV RNA subsequently detected in specimens obtained at the enrollment visit. A finding of fewer than 40 copies per milliliter of plasma HIV RNA was documented for the other 100 HIV-infected subjects before seroconversion. Among the 100 subjects with emergent HIV infection, 36 occurred in the FTC–TDF group, and 64 occurred in the placebo group, representing a relative reduction of 44% in incidence in the modified intention-to-treat population (95% confidence interval \\[CI\\], 15 to 63; P=0.005) . After adjustment for the difference in age between the two groups, the efficacy was 43% (95% CI, 14 to 62). The rate of pill use on 50% or more of days was recorded on the basis of pill counts, self-report, and dispensation records at 81% of visits on which efficacy was 50% (95% CI, 18 to 70; P=0.006). This rate did not differ significantly (P=0.48) from the efficacy at visits with less than 50% pill use of 32% (95% CI, −41 to 67%) . Efficacy of less than 30% could not be ruled out in the modified intention-to-treat analysis (P=0.15) or in the prespecified as-treated analysis at 50% pill use (P=0.09). There was no evidence of a change in HIV efficacy with longer follow-up (P=0.44).\n\n【52】In prespecified analyses of efficacy according to subgroup, efficacy was higher among subjects who reported at screening that they had previously had unprotected receptive anal intercourse than among those who did not (efficacy, 58%; 95% CI, 32 to 74) . There was no significant between-group difference in protection on the basis of region, race or ethnic group, male circumcision, level of education, alcohol use, or age. In post hoc analyses, pill use on 90% or more of days was recorded at 49% of visits on which efficacy was 73% (95% CI, 41 to 88; P<0.001). Among all subjects, without exclusion for HIV infection at enrollment or the degree of compliance to the drug regimen, the efficacy was 47% (95% CI, 22 to 64; P=0.001).\n\n【53】Among the 10 subjects in whom plasma HIV RNA was subsequently detected in specimens obtained at enrollment, 5 had symptoms of an acute viral syndrome at enrollment, 2 had symptoms 1 week later (prompting an interim study visit), 1 had an anal sore, and 2 had leukopenia at enrollment. In these subjects, the clinicians did not suspect acute HIV infection, because the symptoms were attributed to an upper respiratory tract infection, sinusitis, or other non-HIV cause.\n\n【54】Of the preexisting HIV infections at enrollment, two occurred in the FTC–TDF group and eight in the placebo group (P=0.06). Among subjects who were infected after enrollment, the numbers with detectable plasma HIV RNA before seroconversion were 5 of 36 (14%) in the FTC–TDF group and 7 of 64 (11%) in the placebo group (P=0.75). The time to seroconversion after RNA detection was similar in the two groups (P=0.55). After the discontinuation of a study drug, seroconversion rates were similar among 320 subjects (161 in the FTC–TDF group and 159 in the placebo group) (P=0.42). These subjects had a total of 1173 visits for HIV testing after the discontinuation of a study drug (642 in the FTC–TDF group and 531 in the placebo group). During these visits, 5 seroconversions were observed (2 in the FTC–TDF group and 3 in the placebo group).\n\n【55】Drug-Level Detection and Prophylactic Effect\n--------------------------------------------\n\n【56】Figure 4. Levels of Study-Drug Components in Blood of Subjects Receiving FTC–TDF, According to HIV Status.\n\n【57】Shown are intracellular levels  and plasma levels  of components of emtricitabine and tenofovir disoproxil fumarate (FTC–TDF), quantified in specimens obtained from subjects in the FTC–TDF group. FTC-TP denotes emtricitabine triphosphate, and TFV-DP tenofovir diphosphate. The horizontal lines in each panel indicate medians.\n\n【58】Among subjects who became infected with HIV, the median time between the tested specimen date and the last uninfected visit was 35 days (interquartile range, 28 to 56). No drug was detected in any plasma or cell specimens from subjects in the placebo group. Among subjects in the FTC–TDF group, at least one of the study-drug components was detected in 3 of 34 subjects with HIV infection (9%) and in 22 of 43 seronegative control subjects (51%) . Of the 3 HIV-infected subjects with a detectable level of a study drug, none had cell-associated drug levels higher than the median for the 22 seronegative control subjects in whom a study-drug component was detected. Only 8% of subjects with HIV infection and 54% of control subjects who were considered “on treatment” on more than 50% of days had a detectable level of a study drug in plasma or peripheral-blood mononuclear cells . Detection of the different drug components was more than 95% concordant .\n\n【59】In the FTC–TDF group, among subjects with a detectable study-drug level, as compared with those without a detectable level, the odds of HIV infection were lower by a factor of 12.9 (95% CI, 1.7 to 99.3; P<0.001), corresponding to a relative reduction in HIV risk of 92% (95% CI, 40 to 99; P<0.001). After adjustment for reported unprotected receptive anal intercourse, the relative risk reduction was 95% (95% CI, 70 to 99; P<0.001).\n\n【60】Effect of FTC–TDF on HIV Infection\n----------------------------------\n\n【61】Plasma HIV RNA levels and CD4+ T-cell counts were similar among subjects with seroconversion in the two groups . Among the 10 subjects who were infected at enrollment, 3 had FTC-resistant infections (2 of 2 in the FTC–TDF group and 1 of 8 in the placebo group) . No TDF-resistant infections were observed. Among 36 subjects in the FTC–TDF group and 64 subjects in the placebo group who became infected with HIV during the trial, no FTC or TDF resistance was detected.\n\n【62】Discussion\n----------\n\n【63】Once-daily oral FTC–TDF provided 44% additional protection from HIV among men or transgender women who have sex with men who also received a comprehensive package of prevention services. The protective effect of FTC–TDF was significant but not as high as originally hypothesized during the design of the study. Although reported pill use was high, drug exposure that was measured objectively was substantially lower. The intracellular assay that was used in this study is expected to detect TFV-DP for 14 days or more after the last dose of TDF is taken . Other evidence of low drug exposure included the lack of drug resistance observed among emergent infections and the absence of suppression of the HIV RNA level in plasma at the seroconversion visit. There was no evidence of delayed seroconversion among subjects who were infected in the FTC–TDF group. More information will be available after the entire cohort stops receiving the study drug.\n\n【64】The estimate of biologic activity of FTC–TDF persists after adjustment for high-risk sexual practice, suggesting that the correlation between drug detection and protection is primarily due to the drug and not to other characteristics of subjects that may link poor adherence with higher risk. The testing of a larger number of specimens, from more subjects at more times, is needed to better define the minimum protective drug concentration. Protective drug levels may differ according to the type of exposure (rectal vs. penile). Drug level may have a role in monitoring trials, programs, and individual users. Methods for inexpensively measuring long-term drug exposure, such as that afforded by analysis of hair,  would be helpful once such a method is fully validated.\n\n【65】Side effects may have contributed to low pill use among some subjects. As with treatment of HIV infection and the use of FTC–TDF in postexposure prophylaxis,  the initiation of FTC–TDF preexposure prophylaxis was associated with self-limited start-up symptoms in a few subjects. The trial design involving a placebo may also have contributed to lower-than-expected pill use. All subjects were counseled that the study pill might be a placebo or an active drug having no proven benefit. Open-label research and program development could provide users with clearer information about expected benefits and risks, which might increase the use and efficacy of preexposure prophylaxis. Engagement with communities and additional behavioral research are needed to develop methods of counseling that better support such use.\n\n【66】The initiation of chemoprophylaxis either before or after exposure should be deferred in patients with signs or symptoms of a viral syndrome, which are often present during acute HIV infection.  The initiation of postexposure prophylaxis in patients who are RNA-positive but antibody-negative has been linked with acquisition of resistance to FTC and lamivudine (3TC),  as occurred in subjects in the FTC–TDF group who were already infected at enrollment in our trial. Ways to increase recognition of acute HIV infection would include routine measurement of body temperature and testing for HIV antibodies to evaluate viral syndromes, regardless of whether the presentation suggests HIV infection or another cause. Testing for HIV RNA at the time of the initiation of preexposure prophylaxis should be considered where available.\n\n【67】TDF treatment is known to cause decreases in renal function,  and there were trends toward more creatinine elevations in the FTC–TDF group than in the placebo group. Most creatinine elevations were self-limited and were not confirmed on repeat testing of a new specimen, as might occur due to dehydration, creatine use, or exercise. The ability to detect safety outcomes, including drug resistance, may have been decreased by lower-than-expected drug exposure. In light of evidence of the efficacy of FTC–TDF, more information is needed about possible subclinical effects that may affect bone mineral density, low-level drug resistance, and proximal renal tubular function. Flares of hepatitis caused by HBV after stopping preexposure prophylaxis with TDF were not seen in West African women,  but more information is needed. These issues are being investigated in existing trials of preexposure prophylaxis.\n\n【68】Reported high-risk behavior decreased substantially after enrollment and remained lower than at baseline during the trial. Safer behavior was also observed in a trial of preexposure prophylaxis with TDF in West African women  and may reflect the services (e.g., counseling, testing, and dispensing of condoms) that are provided as part of such interventions. In addition, taking a pill a day may have served as a daily reminder of imminent risk and may have promoted planning for sex, which has been associated with lower HIV risk.  Behavioral changes during future open-label use of preexposure prophylaxis may differ because of an increased expectation of benefits, although such “risk compensation” was not observed during an open-label study of postexposure prophylaxis, during which benefits were expected. \n\n【69】The optimal regimen for preexposure prophylaxis has not been established, and data from the subjects in our study cannot be applied to other populations. Alternative regimens in different populations are being studied. \n\n【70】In our study, preexposure prophylaxis with oral FTC–TDF among men and transgender women who have sex with men addressed an important unmet need in public health. HIV prevalence is higher in this population than in other groups in almost all countries.  In the United States, rates of HIV infection among such men and transgender women have climbed since the early 1990s, affecting in particular black and Hispanic subpopulations.  Intensive counseling in behavioral risk reduction for such subjects has not been shown to be better than standard counseling.  Although male circumcision partially protects heterosexual men,  penile circumcision is not expected to protect those who are exposed on the rectal mucosa.  Heterosexual women were partially protected by tenofovir 1% vaginal gel,  but the safety and utility of tenofovir topical gels for rectal use is not yet known. In the FTC–TDF group, there was increased efficacy among subjects who reported having unprotected receptive anal intercourse, which is the main mode of HIV transmission among the subjects in our study and increases the risk of heterosexual women who engage in the practice.  We showed that such subjects with a high risk of exposure to HIV can be mobilized to participate in prevention initiatives and that preexposure prophylaxis is effective for slowing the spread of HIV in this population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5a5d884a-b6e9-4651-bc8b-e0d091817027", "title": "Extended Use of Dabigatran, Warfarin, or Placebo in Venous Thromboembolism", "text": "【0】Extended Use of Dabigatran, Warfarin, or Placebo in Venous Thromboembolism\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Dabigatran, which is administered in a fixed dose and does not require laboratory monitoring, may be suitable for extended treatment of venous thromboembolism.\n\n【3】Methods\n-------\n\n【4】In two double-blind, randomized trials, we compared dabigatran at a dose of 150 mg twice daily with warfarin (active-control study) or with placebo (placebo-control study) in patients with venous thromboembolism who had completed at least 3 initial months of therapy.\n\n【5】Results\n-------\n\n【6】In the active-control study, recurrent venous thromboembolism occurred in 26 of 1430 patients in the dabigatran group (1.8%) and 18 of 1426 patients in the warfarin group (1.3%) (hazard ratio with dabigatran, 1.44; 95% confidence interval \\[CI\\], 0.78 to 2.64; P=0.01 for noninferiority). Major bleeding occurred in 13 patients in the dabigatran group (0.9%) and 25 patients in the warfarin group (1.8%) (hazard ratio, 0.52; 95% CI, 0.27 to 1.02). Major or clinically relevant bleeding was less frequent with dabigatran (hazard ratio, 0.54; 95% CI, 0.41 to 0.71). Acute coronary syndromes occurred in 13 patients in the dabigatran group (0.9%) and 3 patients in the warfarin group (0.2%) (P=0.02). In the placebo-control study, recurrent venous thromboembolism occurred in 3 of 681 patients in the dabigatran group (0.4%) and 37 of 662 patients in the placebo group (5.6%) (hazard ratio, 0.08; 95% CI, 0.02 to 0.25; P<0.001). Major bleeding occurred in 2 patients in the dabigatran group (0.3%) and 0 patients in the placebo group. Major or clinically relevant bleeding occurred in 36 patients in the dabigatran group (5.3%) and 12 patients in the placebo group (1.8%) (hazard ratio, 2.92; 95% CI, 1.52 to 5.60). Acute coronary syndromes occurred in 1 patient each in the dabigatran and placebo groups.\n\n【7】Conclusions\n-----------\n\n【8】Dabigatran was effective in the extended treatment of venous thromboembolism and carried a lower risk of major or clinically relevant bleeding than warfarin but a higher risk than placebo. \n\n【9】Introduction\n------------\n\n【10】Anticoagulant treatment with vitamin K antagonists is recommended for patients with venous thromboembolism.  Most patients receive at least 3 months of treatment. Long-term treatment is recommended if there are risk factors for recurrence, such as multiple thrombotic episodes.  In the absence of clear contraindications to anticoagulant therapy, the risk of major bleeding is approximately 1% per year with extended vitamin K antagonist therapy after venous thromboembolism.  The risk of major bleeding, together with the need for frequent laboratory monitoring and dose adjustments, makes long-term treatment problematic.\n\n【11】Dabigatran, a direct thrombin inhibitor, does not require frequent monitoring and dose adjustments. At a dose of 150 mg twice daily, it was shown in one trial to be noninferior to warfarin for the initial 6-month treatment of venous thromboembolism and was associated with a lower rate of clinically relevant nonmajor bleeding.  These findings suggested that dabigatran might also be suitable for extended therapy beyond the initial period of treatment. We performed two studies to evaluate the efficacy and safety of dabigatran during long-term prophylaxis after venous thromboembolism. In one trial, dabigatran was compared with warfarin (the active-control study); in the other trial, dabigatran was compared with placebo (the placebo-control study).\n\n【12】Methods\n-------\n\n【13】Study Design\n------------\n\n【14】The active-control study was designated RE-MEDY, and the placebo-control study was designated RE-SONATE. Both studies had a randomized, double-blind design and enrolled patients who had completed at least 3 months of treatment for venous thromboembolism. Both studies were funded, designed, and conducted, and the data analyzed, by Boehringer Ingelheim in conjunction with the steering committee, which was a common steering committee for the two trials. The members of the steering committee (the seven academic authors), who had independent access to the data, wrote the manuscript and made the decision to submit it for publication.\n\n【15】Study Patients\n--------------\n\n【16】We recruited patients for the active-control study from 265 sites in 33 countries and for the placebo-control study from 147 sites in 21 countries. Patients were eligible if they were at least 18 years of age and had objectively confirmed, symptomatic, proximal deep-vein thrombosis or pulmonary embolism that had already been treated with an approved anticoagulant or if they had received dabigatran in one of two previous clinical trials of short-term treatment of venous thromboembolism: the RE-COVER  and RE-COVER II  studies.\n\n【17】The main difference between the two trials in the selection of patients was that participants in the active-control study were considered to be at increased risk for recurrent venous thromboembolism on the basis of the site investigator's assessment. The required duration of initial treatment before trial enrollment was 3 to 12 months for the active-control study and 6 to 18 months for the placebo-control study.\n\n【18】All patients provided written informed consent. The institutional review board at each participating clinical center approved the studies.\n\n【19】Randomization and Treatment\n---------------------------\n\n【20】In both studies, patients underwent randomization by means of an interactive voice-response system. Randomization was stratified according to the presence or absence of active cancer and according to the index diagnosis (deep-vein thrombosis or pulmonary embolism) in the active-control study and according to study center in the placebo-control study. Prior anticoagulant therapy was discontinued, and the study drug was started when the international normalized ratio (INR) was 2.3 or lower. If the patient was enrolled from the RE-COVER study or the RE-COVER II study, a point-of-care coagulometer with encrypted INR results was used to guide the transition so that the patients and investigators would remain unaware of the initial treatment.\n\n【21】In the active-control study, patients were assigned in a  ratio to receive active dabigatran (at a fixed dose of 150 mg twice daily) and a warfarin-like placebo or active warfarin and a dabigatran-like placebo. Bilateral compression ultrasonography of the leg veins was performed within 72 hours after randomization. We adjusted the dose of warfarin or placebo to maintain an INR of 2.0 to 3.0 with the use of a point-of-care instrument that provided an encrypted INR. The true or sham INR was then obtained by means of an interactive voice-response system with a central computer that had been programmed with the randomization schedule.\n\n【22】In the placebo-control study, patients were assigned in a  ratio to receive dabigatran (at a fixed dose of 150 mg twice daily) or a matching placebo. Laboratory monitoring of coagulation was not performed in the placebo-control study.\n\n【23】Follow-up and Outcome Measures\n------------------------------\n\n【24】Patients were assessed at 15 and 30 days after randomization, then monthly until day 180. After that, patients in the active-control study were assessed every 90 days until the end of treatment; INR or sham INR results were obtained at intervals of no longer than 4 weeks. An additional visit occurred 30 days after the end of treatment in both studies.\n\n【25】The active-control study was initially designed for 18 months of treatment. Because of a lower-than-projected event rate, the protocol was amended to increase the sample and extend the planned treatment period for patients already enrolled who consented to the extension, with the resulting planned study treatment period ranging from 6 to 36 months. The placebo-control study was amended, 6 months after recruitment of the first patient, to extend the follow-up to 12 months after completion of the study treatment to evaluate the long-term risk of recurrence.\n\n【26】In both studies, the primary efficacy outcome was recurrent symptomatic and objectively verified venous thromboembolism or death associated with venous thromboembolism (or unexplained death in the placebo-control study). Safety outcomes included major bleeding and clinically relevant nonmajor bleeding. Definitions of the efficacy and safety outcomes are provided in the Supplementary Appendix . Central committees, whose members were not aware of the treatment assignments, adjudicated suspected cases of recurrent venous thromboembolism, bleeding, death, acute coronary events (as well as cerebrovascular events in the placebo-control study), and liver-function abnormalities (according to clinical and routine laboratory data).\n\n【27】Statistical Analysis\n--------------------\n\n【28】The active-control study was designed to show that dabigatran is noninferior to warfarin in preventing recurrent venous thromboembolism (including death related to venous thromboembolism). The sample size was determined on the basis of an expected rate of the primary efficacy outcome of 2.0% in both groups,  with a power of 85% to exclude a hazard ratio of 2.85 (the noninferiority margin for the hazard ratio),  and an absolute increase in the risk of recurrent venous thromboembolism of 2.8 percentage points at 18 months (the noninferiority margin for the risk difference), at a one-sided alpha level of 0.025. To meet these specifications, we estimated that we would need to enroll 2000 patients. In the protocol amendment described above, the sample was increased to 2850 patients. Details of the sample-size calculation and the determination of the noninferiority margin are provided in the Supplementary Appendix .\n\n【29】The amended protocol for the active-control study resulted in a study population with a treatment duration ranging from 6 to 36 months. Consequently, the protocol was further revised to group the study participants into three cohorts according to the duration of treatment (<18 months, 18 months, or >18 months) and to perform a meta-analysis of these three cohorts for the primary efficacy outcome. Details of this analytic method are provided in the Supplementary Appendix . The prespecified analyses were the hazard ratio for the planned treatment period and the risk difference at 18 months. Both estimates had to satisfy the noninferiority criteria for dabigatran to be judged to be noninferior.\n\n【30】The placebo-control study was designed to determine whether dabigatran was superior to placebo for preventing recurrent venous thromboembolism. We assumed a 70% reduction in the relative risk of the primary outcome with dabigatran  and planned for a power of 95% to detect this difference, at a two-sided alpha of 0.05; on the basis of these specifications, 36 events were required. With an assumed frequency of the primary outcome of 3% at 6 months in the placebo group, a sample of approximately 1800 patients was needed. The primary efficacy outcome was analyzed with the use of a Cox proportional-hazards model.\n\n【31】In both trials, a modified intention-to-treat analysis was performed for efficacy, with exclusion of patients who did not receive any dose of the study drug. In the safety analyses, we included all events during the period from receipt of the first dose of the study drug until 3 days after receipt of the last dose. All safety analyses and secondary efficacy analyses were prespecified.\n\n【32】Results\n-------\n\n【33】Patients\n--------\n\n【34】Table 1. Characteristics of the Patients and Treatments According to Study and Assigned Study Drug.\n\n【35】From July 2006 through July 2010, a total of 2866 patients underwent randomization in the active-control study, and from November 2007 through September 2010, a total of 1353 patients underwent randomization in the placebo-control study . Demographic and clinical characteristics of the participants in both trials are shown in Table 1 .\n\n【36】Treatment and Follow-up\n-----------------------\n\n【37】In the active-control study, one patient in each group mistakenly received the study drug assigned to the other group throughout the study. In the warfarin group, the INR was in the therapeutic range (2.0 to 3.0) for a median of 65.3% of the time. The INR was below the therapeutic range 17.3% of the time and above the therapeutic range 12.2% of the time. Additional characteristics of the treatments are provided in Table 1 .\n\n【38】In the active-control study, the study drug was stopped early in 276 patients (19.3%) in the dabigatran group (in 147 because of an adverse event, 23 because of nonadherence, 2 because of loss to follow-up, 64 because of their decision to stop taking the study medication, and 40 for other reasons) and in 281 patients (19.7%) in the warfarin group (in 129 because of an adverse event, 34 because of nonadherence, 6 because of loss to follow-up, 58 because of their decision to stop taking the study medication, and 54 for other reasons).\n\n【39】In the placebo-control study, 3 patients assigned to placebo mistakenly received dabigatran throughout the study. The study drug was stopped early in 71 patients (10.4%) in the dabigatran group (in 50 because of an adverse event, 9 because of nonadherence, and 12 because of their decision to stop taking the study medication) and in 99 patients (15.0%) in the placebo group (in 81 because of an adverse event, 5 because of nonadherence, and 13 because of their decision to stop taking the study medication).\n\n【40】Efficacy and Safety in the Active-Control Study\n-----------------------------------------------\n\n【41】Table 2. Efficacy and Safety Outcomes in the Active-Control Study. Figure 1.  Figure 1. Cumulative Risk of Recurrent Venous Thromboembolism or Related Death (or Unexplained Death in the Placebo-Control Study).\n\n【42】Panel A shows data from the active-control study, and Panel B shows data from the placebo-control study.\n\n【43】The primary outcome for efficacy was confirmed by central adjudication in 26 patients (1.8%) in the dabigatran group and in 18 patients (1.3%) in the warfarin group . The overall hazard ratio with dabigatran for time to the first primary-outcome event was 1.44 (95% confidence interval \\[CI\\], 0.78 to 2.64), and the risk difference at 18 months was 0.38 percentage points (95% CI, −0.50 to 1.25) , calculated with the use of the meta-analytic approach. In a Cox regression analysis of pooled individual data, the hazard ratio was 1.47 (95% CI, 0.80 to 2.68). Dabigatran thus met the criteria for noninferiority to warfarin with regard to the prevention of recurrent or fatal venous thromboembolism (P<0.001 for the risk-difference criterion and P=0.01 for the hazard–ratio criterion). There were no significant differences in efficacy according to study treatment in predefined subgroups .\n\n【44】Figure 2. Cumulative Risk of Any Bleeding.\n\n【45】Panel A shows data from the active-control study, and Panel B shows data from the placebo-control study.\n\n【46】Thirteen patients (0.9%) in the dabigatran group had a major bleeding event, as compared with 25 patients (1.8%) in the warfarin group. The overall hazard ratio for time to first major bleeding event was 0.52 (95% CI, 0.27 to 1.02) . The sites of major bleeding are presented in Table S1 in the Supplementary Appendix . In both groups, 2 patients had bleeding at two sites. Major or clinically relevant bleeding occurred in 80 patients (5.6%) in the dabigatran group and 145 patients (10.2%) in the warfarin group (hazard ratio, 0.54; 95% CI, 0.41 to 0.71; P<0.001). The risk of any bleeding is shown in Figure 2A . There were no significant differences in the risk of bleeding according to study treatment in predefined subgroups.\n\n【47】We observed a higher incidence of acute coronary events in the dabigatran group (events in 13 patients \\[0.9%\\]: 10 with myocardial infarction and 3 with unstable angina) than in the warfarin group (events in 3 patients \\[0.2%\\]: 1 with myocardial infarction and 2 with unstable angina). All these events were adjudicated as definite except for myocardial infarction in 1 patient in the dabigatran group and unstable angina in 1 patient in the warfarin group. In each group, 1 patient had an event within 3 days after stopping the study treatment.\n\n【48】Efficacy and Safety in the Placebo-Control Study\n------------------------------------------------\n\n【49】Table 3. Efficacy and Safety Outcomes in the Placebo-Control Study.\n\n【50】The primary outcome for efficacy was confirmed by central adjudication in 3 patients (0.4%) in the dabigatran group and in 37 patients (5.6%) in the placebo group. The hazard ratio with dabigatran for time to the first primary-outcome event was 0.08 (95% CI, 0.02 to 0.25; P<0.001) . There were no significant differences in efficacy according to study treatment in predefined subgroups . The extended 12-month follow-up was completed for 1323 of the 1343 patients who underwent randomization and received the study drug (98.5%), at which point the cumulative incidence of the primary efficacy outcome was 6.9% in the dabigatran group as compared with 10.7% in the placebo group (hazard ratio, 0.61; 95% CI, 0.42 to 0.88).\n\n【51】There were 2 patients with major bleeding events, both in the dabigatran group; both events were gastrointestinal bleeding requiring at least 2 units of blood but without a fall in the hemoglobin level of 20 g per liter or more. Major or clinically relevant nonmajor bleeding occurred in 36 patients (5.3%) in the dabigatran group as compared with 12 patients (1.8%) in the placebo group (hazard ratio, 2.92; 95% CI, 1.52 to 5.60; P=0.001). The risk of any bleeding is shown in Figure 2B . Rectal bleeding occurred in 19 patients (2.8%) in the dabigatran group and in 5 patients (0.8%) in the placebo group, whereas the incidence of bleeding at other sites was similar in the two groups. In this study, there was 1 acute coronary event in each group.\n\n【52】Discussion\n----------\n\n【53】We evaluated the use of dabigatran for the long-term treatment of venous thromboembolism in two complementary randomized trials. In the active-control study, dabigatran met the predefined noninferiority criteria in comparison with warfarin for the prevention of recurrent venous thromboembolism, with fewer major bleeding events and significantly fewer clinically relevant nonmajor bleeding events. As compared with placebo, dabigatran significantly reduced the rate of recurrent venous thromboembolism, but with a significantly higher risk of major or clinically relevant nonmajor bleeding.\n\n【54】In the active-control study, there was a numerical increase in recurrent or fatal venous thromboembolic events during treatment with dabigatran (26 events \\[1.8%\\], vs. 18 events with warfarin \\[1.3%\\]). The upper limit of the 95% confidence interval for the hazard ratio (2.64) was close to the predefined noninferiority margin (2.85), and the confidence interval gives boundaries for the event rate with dabigatran as low as 1.0% and as high as 3.4%. This corresponds to an annual risk of 0.8% to 2.6%. The annualized point estimates of 1.0% for warfarin and 1.4% for dabigatran can be compared with the annualized point estimates in previous trials of extended treatment with warfarin (range, 0.7% to 5.0%)  or with experimental drugs (ximelagatran, 1.3%; low-dose warfarin, 1.9% to 2.6%; and rivaroxaban, 1.9%).  The prespecified noninferiority margin for the hazard ratio of 2.85 is large, since it allows an increase in risk by a factor of nearly 3 to be accepted as noninferior. This is a limitation of the trial design. However, large noninferiority margins have been prespecified in trials of short-term treatment of venous thromboembolism with dabigatran (2.75),  rivaroxaban (2.0),  and idraparinux (2.0). \n\n【55】The efficacy of dabigatran in comparison with placebo (hazard ratio, 0.08; 95% CI, 0.02 to 0.25) is of the same magnitude as the efficacy of rivaroxaban versus placebo (hazard ratio, 0.18; 95% CI, 0.09 to 0.39)  and of warfarin versus placebo or control (odds ratio, 0.05 and 0.11, respectively).  The benefit of extended treatment with dabigatran was maintained during the 12 months of extended follow-up after discontinuation of the study drug. The risk of clinically relevant bleeding with dabigatran as compared with placebo (hazard ratio, 2.9; 95% CI, 1.5 to 5.6) is similar to that of rivaroxaban as compared with placebo (hazard ratio, 5.2; 95% CI, 2.3 to 11.7). \n\n【56】There was a higher rate of acute coronary events with dabigatran than with warfarin, with no significant difference in these events between dabigatran and placebo. In the Randomized Evaluation of Long-Term Anticoagulation Therapy (RE-LY), which compared dabigatran and warfarin in patients with atrial fibrillation, there was a higher risk of acute coronary syndromes with dabigatran,  although after further analysis of the data this difference was no longer significant.  However, a recent meta-analysis of seven noninferiority trials showed a significantly higher risk of myocardial infarction or acute coronary syndromes with dabigatran than with the comparator.  Whether dabigatran increases the risk of myocardial infarction is therefore still unclear.\n\n【57】In summary, we evaluated the use of dabigatran for the long-term treatment of venous thromboembolism in two complementary randomized trials. In the active-control study, dabigatran was noninferior to warfarin for the prevention of recurrent venous thromboembolism, with a lower risk of bleeding. In the placebo-control study, dabigatran significantly reduced the rate of recurrent venous thromboembolism, but with a significantly higher risk of bleeding.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "047a2fe7-8e61-48de-9996-9e089e0d04a8", "title": "Audio Interview: Five Disease Outbreaks beyond Covid-19", "text": "【0】Audio Interview: Five Disease Outbreaks beyond Covid-19\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Five Disease Outbreaks beyond Covid-19 \n\n【3】The continuing spread of SARS-CoV-2 remains a Public Health Emergency of International Concern. What physicians need to know about transmission, diagnosis, and treatment of Covid-19 is the subject of ongoing updates from infectious disease experts at the _Journal_ .\n\n【4】In this audio interview conducted on November 29, 2022, the editors discuss ongoing outbreaks of influenza, RSV, mpox, Ebola, and cholera in the context of our response to Covid-19.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e2b239d1-1984-4747-b20d-04dbe9a1096e", "title": "Trial of Globus Pallidus Focused Ultrasound Ablation in Parkinson’s Disease", "text": "【0】Trial of Globus Pallidus Focused Ultrasound Ablation in Parkinson’s Disease\nAbstract\n--------\n\n【1】### Background\n\n【2】Unilateral focused ultrasound ablation of the internal segment of globus pallidus has reduced motor symptoms of Parkinson’s disease in open-label studies.\n\n【3】### Methods\n\n【4】We randomly assigned, in a  ratio, patients with Parkinson’s disease and dyskinesias or motor fluctuations and motor impairment in the off-medication state to undergo either focused ultrasound ablation opposite the most symptomatic side of the body or a sham procedure. The primary outcome was a response at 3 months, defined as a decrease of at least 3 points from baseline either in the score on the Movement Disorders Society–Unified Parkinson’s Disease Rating Scale, part III (MDS-UPDRS III), for the treated side in the off-medication state or in the score on the Unified Dyskinesia Rating Scale (UDysRS) in the on-medication state. Secondary outcomes included changes from baseline to month 3 in the scores on various parts of the MDS-UPDRS. After the 3-month blinded phase, an open-label phase lasted until 12 months.\n\n【5】### Results\n\n【6】Of 94 patients, 69 were assigned to undergo ultrasound ablation (active treatment) and 25 to undergo the sham procedure (control); 65 patients and 22 patients, respectively, completed the primary-outcome assessment. In the active-treatment group, 45 patients (69%) had a response, as compared with 7 (32%) in the control group (difference, 37 percentage points; 95% confidence interval, 15 to 60; P=0.003). Of the patients in the active-treatment group who had a response, 19 met the MDS-UPDRS III criterion only, 8 met the UDysRS criterion only, and 18 met both criteria. Results for secondary outcomes were generally in the same direction as those for the primary outcome. Of the 39 patients in the active-treatment group who had had a response at 3 months and who were assessed at 12 months, 30 continued to have a response. Pallidotomy-related adverse events in the active-treatment group included dysarthria, gait disturbance, loss of taste, visual disturbance, and facial weakness.\n\n【7】### Conclusions\n\n【8】Unilateral pallidal ultrasound ablation resulted in a higher percentage of patients who had improved motor function or reduced dyskinesia than a sham procedure over a period of 3 months but was associated with adverse events. Longer and larger trials are required to determine the effect and safety of this technique in persons with Parkinson’s disease. \n\n【9】 QUICK TAKE VIDEO SUMMARY  \nFocused Ultrasound Ablation for Parkinson’s Disease", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1e4a39de-a88a-4747-8b02-8ca21b5aa172", "title": "A Reappraisal of the Results of Stopping Therapy in Childhood Leukemia", "text": "【0】A Reappraisal of the Results of Stopping Therapy in Childhood Leukemia\nAbstract\n--------\n\n【1】We examined the results of stopping therapy in children with acute lymphocytic leukemia. Of 639 patients in eight consecutive \"total therapy\" studies, 278 (44 per cent) had all treatment stopped, usually after 2 1/2 years of complete remission. About one fifth (55 of 278) of this group have relapsed, mainly in the bone marrow. The relapse rate for the first year off therapy was higher than that for the next three years (0.16 vs. 0.04, P<0.01). The life-table estimates of the four-year relapse rates were 0.24 for all patients and 0.22 for patients receiving adequate Central-nervous-system prophylaxis. Boys had a higher relapse rate than girls (0.33 vs. 0.15, P<0.01). None of the 79 patients who remained in complete remission for at least four years off therapy have yet relapsed.\n\n【2】Acute lymphocytic leukemia appears curable in over one third of all newly diagnosed patients who receive treatment for approximately 2 1/2 years.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "bdab9b66-5c11-4c00-aa65-3b6a132a8e3b", "title": "Sideroblastic Anemia with Dermal Photosensitivity and Greatly Increased Erythrocyte Protoporphyrin", "text": "【0】Sideroblastic Anemia with Dermal Photosensitivity and Greatly Increased Erythrocyte Protoporphyrin\nAbstract\n--------\n\n【1】Sideroblastic anemia was found in an elderly man in association with dermal photosensitivity, a greatly increased erythrocyte free protoporphyrin content and reduced erythrocyte catalase activity. Markedly reduced heme synthetase activity was found in the patient's reticulocytes. Since the activity of mixtures of the patient's cells with cells from a patient with hemolytic anemia was greater than the sum of the components of the mixture, the patient may have lacked an essential cofactor. The clinical, hematologic and biochemical manifestations of disease in this case appear to be consequences of this defect.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b8278ace-34a6-4388-a702-d3e7f4714301", "title": "Host Immune Response to a Common Cell-Surface Antigen in Human Sarcomas — Detection by Cytotoxicity Tests", "text": "【0】Host Immune Response to a Common Cell-Surface Antigen in Human Sarcomas — Detection by Cytotoxicity Tests\nAbstract\n--------\n\n【1】Cytotoxic antibody was demonstrated in the serums of four patients with sarcoma against their own autologous sarcoma cells in tissue culture. The presence of a common cell-surface antigen in human sarcomas was suggested by the finding that 70 per cent (58 of 83) of serums from patients with different histologic types of sarcomas were cytotoxic to an osteosarcoma cell line, whereas in serums from patients with other types of neoplasms cytotoxic antibody was no more frequent than in normal blood-donor serums. Further cross-reactivity studies revealed the common sarcoma antigen on tissue-culture cells from four different human sarcomas. Immunization of patients with sarcoma with their own irradiated sarcoma cells induced an increased immune response to this common sarcoma antigen, with a rising titer of cytotoxic antibody.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3a11a8ae-05e9-42f6-b8e1-398eb997759d", "title": "Obstructive Sleep Apnea in Adults", "text": "【0】Obstructive Sleep Apnea in Adults\nObstructive sleep apnea is common and is associated with daytime sleepiness and increased risks of motor vehicle accidents and cardiovascular disease. CPAP is considered first-line therapy for symptomatic or moderate-to-severe obstructive sleep apnea; mandibular-advancement devices and various surgical options are other approaches.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "7d55f4c6-6138-4ac1-91a4-042648c1a09b", "title": "A 52-Week Trial Comparing Briakinumab with Methotrexate in Patients with Psoriasis", "text": "【0】A 52-Week Trial Comparing Briakinumab with Methotrexate in Patients with Psoriasis\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Briakinumab is a monoclonal antibody against the p40 molecule shared by interleukin-12 and interleukin-23, which is overexpressed in psoriatic skin lesions. We assessed the efficacy and safety of briakinumab as compared with methotrexate in patients with psoriasis.\n\n【3】Methods\n-------\n\n【4】In this 52-week trial, we randomly assigned 317 patients with moderate-to-severe psoriasis to briakinumab, at a dose of 200 mg at weeks 0 and 4 and 100 mg at week 8 and every 4 weeks thereafter (154 patients), or methotrexate, at a dose of 5 to 25 mg weekly (163 patients). The primary end points were the percentages of patients with at least 75% improvement in the score on the psoriasis area-and-severity index (PASI) at weeks 24 and 52 and a score on the physician's global assessment of 0 (clear; i.e., no apparent disease) or 1 (minimal disease) at weeks 24 and 52. A total of 248 patients were enrolled in an ongoing 160-week open-label continuation study.\n\n【5】Results\n-------\n\n【6】At week 24, a total of 81.8% of the patients in the briakinumab group versus 39.9% in the methotrexate group had at least 75% improvement in the PASI score, and 80.5% versus 34.4% had a score of 0 or 1 on the physician's global assessment. The corresponding percentages at week 52 were 66.2% versus 23.9% with at least a 75% improvement in the PASI score and 63.0% versus 20.2% with a score of 0 or 1 on the physician's global assessment (P<0.001 for all comparisons). During the 52-week study, serious adverse events occurred in 9.1% of the patients in the briakinumab group (12.9 events per 100 patient-years) and in 6.1% in the methotrexate group (10.6 events per 100 patient-years). Serious infections occurred in 2.6% of the patients in the briakinumab group (4.1 events per 100 patient-years) and in 1.8% in the methotrexate group (2.7 events per 100 patient-years); cancers occurred in 1.9% (2.0 events per 100 patient-years) versus 0%.\n\n【7】Conclusions\n-----------\n\n【8】Briakinumab showed higher efficacy than methotrexate in patients with moderate-to-severe psoriasis. Serious infections and cancers occurred more frequently with briakinumab, but the differences were not significant. \n\n【9】Introduction\n------------\n\n【10】Psoriasis is a chronic skin disease that is characterized by T-cell–mediated systemic inflammation  and is associated with considerable impairment in health-related quality of life and physical and mental functioning.  Interleukin-12 and interleukin-23 play important roles in the pathogenesis of psoriasis.  Interleukin-12 is associated with the type 1 helper T-cell (Th1) phenotype, leading to production of interferon-γ; interleukin-23 fosters the type 17 helper T-cell (Th17) phenotype and the production of interleukin-17 and interleukin-22. The shared p40 subunit of interleukin-12 and interleukin-23 is overexpressed in psoriatic skin lesions  and is linked to psoriasis in genetic studies.  Ustekinumab (CNTO 1275, Centocor)  and briakinumab (ABT-874, Abbott Laboratories)  — monoclonal antibodies targeting the p40 molecule shared by interleukin-12 and interleukin-23 — have shown clinical efficacy in patients with psoriasis.\n\n【11】Although there is strong evidence supporting the efficacy of biologic therapy for the treatment of psoriasis,  there is a need to compare the efficacy and safety of biologic agents with those of traditional systemic therapies such as methotrexate, which is the most commonly prescribed systemic therapy for psoriasis worldwide.  We evaluated the efficacy and safety of 1 year of treatment with briakinumab as compared with methotrexate in patients with moderate-to-severe plaque psoriasis.\n\n【12】Methods\n-------\n\n【13】Patients\n--------\n\n【14】We conducted this phase 3, multicenter, randomized, double-blind trial from May 2008 through November 2009 at 43 sites in Europe and Canada. Patients 18 years of age or older were eligible to participate if they had had psoriasis, diagnosed clinically, for at least 6 months and stable plaque psoriasis for at least 2 months; were candidates for systemic therapy or phototherapy; had at least 10% of their body-surface area affected by psoriasis; and had a score on the physician's global assessment of 3 or higher and a score on the psoriasis area-and-severity index (PASI) of 12 or higher at baseline. (Scores on the physician's global assessment range from 0 to 5, with scores of 2, 3, 4, and 5 indicating mild, moderate, severe, and very severe disease, respectively; PASI scores range from 0 to 72, with higher scores indicating more severe disease. Written informed consent was obtained from all patients.\n\n【15】Study Oversight\n---------------\n\n【16】Abbott Laboratories sponsored the study, which was designed jointly by academic advisers and Abbott personnel. Abbott conducted the data analyses, and all the authors had full access to the data; the first author vouches for the completeness and accuracy of the data presented and of the data analyses. The first draft of the manuscript was written by a medical writer employed by Abbott, with input from all the authors. All the authors reviewed and provided feedback on all subsequent versions of the manuscript and made the decision to submit it for publication. All the authors signed confidentiality disclosures with the sponsor regarding these data.\n\n【17】The study protocol was approved by an independent ethics committee or institutional review board at each participating site. All the authors vouch for the fidelity of the study to the protocol.\n\n【18】Procedures\n----------\n\n【19】Figure 1. Study Design and Dosing Regimens.\n\n【20】Patients in the methotrexate group who had at least 75% improvement in the psoriasis area-and-severity index (PASI) score and a score on the physician's global assessment (PGA) of 0 (clear; i.e., no apparent disease) or 1 (minimal disease) at week 24 maintained their current weekly dose of methotrexate through week 51. The dose was adjusted only if there were abnormalities on laboratory tests. Patients in either treatment group who did not have at least a 75% improvement in the PASI score and a score on the PGA of 0 or 1 at week 24 or in whom the response was lost (i.e., had less than a 50% improvement in the PASI score over the baseline value and a score on the PGA of 3, 4, or 5, indicating moderate, severe, or very severe disease, respectively) after week 24 discontinued the trial and were eligible to enroll in an open-label continuation study of briakinumab. To ensure that the treatment assignments were concealed, patients in the briakinumab group received placebo capsules that matched the folate and methotrexate capsules, and patients in the methotrexate group received placebo injections to match the briakinumab injections.\n\n【21】At baseline (week 0), patients were randomly assigned, in a  ratio, to receive briakinumab, administered subcutaneously at a dose of 200 mg at weeks 0 and 4 and 100 mg every 4 weeks from week 8 through week 48, or methotrexate, administered orally at a dose of 5 to 25 mg per week, plus folate, administered orally at a dose of 5 mg per week, from week 0 through week 51 . Patients in the methotrexate group received 5 mg of methotrexate at week 0, 10 mg at week 1, and 15 mg per week from week 2 through week 9. The dose of methotrexate was increased to 20 mg per week at week 10 and to 25 mg per week at week 16 in patients who did not meet the criterion of at least a 75% improvement in the PASI score or of a score on the physician's global assessment of 0 or 1. Neither the patients nor the investigators were aware of the treatment assignments. Placebo capsules that looked identical to the methotrexate capsules and folate or placebo injections to match briakinumab injections were administered to maintain the concealment of the treatment assignments.\n\n【22】Treatment success was defined as both an improvement of at least 75% from baseline in the PASI score and a score on the physician's global assessment of 0 (clear; i.e., no apparent disease) or 1 (minimal disease) at week 24. Patients in the methotrexate group who met the criteria for treatment success continued to take their current weekly dose of methotrexate for the remainder of the study. At any time during the study, the safety assessor could reduce or withhold the dose of methotrexate if warranted by the development of symptoms reported by the patient, adverse events, or laboratory abnormalities or by the results of a physical examination. Patients in either treatment group who did not meet the criteria for treatment success or in whom the response was lost (i.e., had <50% improvement in the PASI score over the baseline value and a score on the physician's global assessment of ≥3) after week 24 discontinued the trial and were eligible to enroll in an ongoing open-label continuation study in which all patients receive briakinumab.\n\n【23】Study End Points\n----------------\n\n【24】The primary efficacy end points were the percentages of patients with at least 75% improvement in the PASI score at week 24, a score on the physician's global assessment of 0 or 1  at week 24, at least 75% improvement in the PASI score at week 52, and a score on the physician's global assessment of 0 or 1 at week 52.\n\n【25】Secondary efficacy end points included the median time to reach the end point of at least 75% improvement in the PASI score; rates of improvement of at least 50%, 75%, 90%, and 100% in the PASI score as assessed at all study visits through week 52; mean percentage improvements in PASI scores from baseline; the change from baseline in the score on the Nail Psoriasis Severity Index (in which scores range from 0, indicating no nail psoriasis, to 8, indicating psoriasis in 100% of the nail  ) for the nail most affected by psoriasis at baseline; and assessments of patients on the basis of the Dermatology Life Quality Index (DLQI), which measures the effect of dermatologic diseases on the quality of life (on a scale from 0 to 30, with lower scores indicating a lesser effect on health-related quality of life).  The DLQI assessments included the percentage of patients with a DLQI score of 0 or 1 (indicating no effect of psoriasis),  the percentage of patients with a decrease of at least 5 points (a clinically meaningful reduction),  and mean reductions in DLQI scores from baseline.\n\n【26】Adverse events, laboratory data, and vital signs were assessed throughout the study. Treatment-emergent adverse events were defined as those occurring at the time the first dose of study drug was administered or any time thereafter, up to 45 days after the last dose was administered . A follow-up call was made 45 days after the last dose of the study drug was administered.\n\n【27】Statistical Analysis\n--------------------\n\n【28】According to the original study plan, we estimated that with approximately 250 patients undergoing randomization, the study would have 90% power to show a greater response with briakinumab than with methotrexate (i.e., at least 75% improvement in the PASI score at week 24 in 70% of the briakinumab group as compared with 50% of the methotrexate group), at a two-sided significance level of 5% with the use of a chi-square test. However, owing to difficulties in obtaining accurate reports of the enrollment numbers in a timely manner, more than 250 patients were enrolled. With an actual enrollment number of 317 patients, the power increased from 90% to 95%.\n\n【29】The efficacy analyses were performed on data from the intention-to-treat population (all patients who underwent randomization). All primary efficacy comparisons were performed with the use of a two-sided Cochran–Mantel–Haenszel test with adjustment for country, at an alpha level of 0.05. We maintained the type I error rate at 0.05 by adhering to the a priori defined order of statistical hypotheses. The primary end points were ordered a priori as follows: at least 75% improvement in the PASI score at week 24, a score on the physician's global assessment of 0 or 1 at week 24, at least 75% improvement in the PASI score at week 52, and a score on the physician's global assessment of 0 or 1 at week 52. Thus, the superiority of briakinumab over methotrexate could be claimed for an end point only if briakinumab had been shown to be superior for all previous end points. Any patient with a missing score on the PASI or physician's global assessment at a visit was considered not to have had a response at that visit (imputation of no response). The last-observation-carried-forward method was used for a sensitivity analysis.\n\n【30】For other categorical variables, the chi-square test or Fisher's exact test (if the expected cell size was <5) was used to evaluate differences between the treatment groups. Missing values for categorical secondary end points were handled in the same way as missing values for primary end points. The median times to reaching the end points of at least 75% improvement in the PASI score and a score on the physician's global assessment of 0 or 1 were calculated with the use of the Kaplan–Meier method. The between-group difference was tested with the use of the log-rank test. Data from patients who did not have treatment success on or before week 52 were included in the analysis as censored observations, with the observation time calculated as the interval from week 0 (baseline) to the date of the last assessment of the PASI score or the last physician's global assessment.\n\n【31】Differences between treatment groups in the change from baseline in scores on the PASI and DLQI and in other continuous variables were analyzed with the use of analysis of covariance, with baseline value in the model. For the percentage change in the score on the Nail Psoriasis Severity Index, only patients with nonzero baseline scores were included in the analysis. Wilcoxon two-sample tests were used to compare the between-group difference in the change. For these assessments, the last-observation-carried-forward method was used in the case of missing data, and analysis as observed (i.e., no imputation) was performed as a sensitivity analysis.\n\n【32】The safety analyses were conducted on data from the safety population, which included all patients who received at least one dose of a study drug. Adverse events that occurred during both the 52-week trial and the open-label continuation study were summarized as the number and percentage of patients with treatment-emergent adverse events and as the number of events per 100 person-years. During the 52-week trial, the rate of adverse events was compared between treatment groups with the use of Fisher's exact test.\n\n【33】Results\n-------\n\n【34】Patients\n--------\n\n【35】Figure 2. Screening, Randomization, and Follow-up. Table 1.  Table 1. Baseline Demographic and Clinical Characteristics.\n\n【36】A total of 317 patients met the inclusion criteria and were randomly assigned to briakinumab (154) or methotrexate (163) (intention-to-treat population) . Baseline demographic and clinical characteristics and disease severity were generally similar in the two groups ; no significant differences were seen, with the exception of the overall score on the physician's global assessment (P=0.04). Per protocol, 17 patients in the methotrexate group required an increase in the dose to 20 mg per week at week 10, and 100 patients in that group required an increase in the dose to 25 mg per week at week 16.\n\n【37】Efficacy\n--------\n\n【38】### _PASI_\n\n【39】Figure 3. Efficacy of Briakinumab as Compared with Methotrexate over the Course of 52 Weeks.\n\n【40】By week 8 (or earlier) and at all time points through week 52, there was a significantly greater percentage of patients in the briakinumab group than in the methotrexate group with at least a 75% improvement in the PASI score (PASI 75)  and with at least a 90% improvement (PASI 90) or a 100% improvement (PASI 100) in the PASI score . Significantly greater percentages of patients in the briakinumab group than in the methotrexate group had a PGA score of 0 or 1 at each time point from week 4 to week 52 . Mean reductions from baseline in scores on the Dermatology Life Quality Index (DLQI, in which scores range from 0 to 30, with lower scores indicating a lesser effect on health-related quality of life) were significantly greater in the briakinumab group than in the methotrexate group at all the time points assessed . P<0.001 for the comparison of briakinumab with methotrexate at all the time points shown after 2 weeks in Panels A, C, and D, and at all the time points shown after 4 weeks in Panel B. All the data shown are based on the intention-to-treat population with imputation of no response in the case of missing data on the PASI or PGA score.\n\n【41】Significantly more patients in the briakinumab group than in the methotrexate group met the criterion for the primary end point of at least 75% improvement in the PASI score, both at week 24 (81.8% vs. 39.9%) and at week 52 (66.2% vs. 23.9%) (P<0.001 for the comparison at both time points). At week 12, a total of 76.6% of the patients in the briakinumab group, as compared with 36.2% in the methotrexate group, had at least 75% improvement in the PASI score (P<0.001). The median time to an improvement of at least 75% in the PASI score was 56 days in the briakinumab group, as compared with 140 days in the methotrexate group (P<0.001). The percentage of patients who had at least 75% improvement in the PASI score was significantly greater in the briakinumab group than in the methotrexate group by week 4 and at all time points through week 52 . (The numbers of patients available for follow-up according to study visit are shown in Table 1 in the Supplementary Appendix .) Significant between-group differences in the percentage of patients with at least 50% improvement in the PASI score were apparent beginning at week 2 , and significant differences in the percentages of patients with at least 90% improvement and with 100% improvement in the PASI score were apparent at week 8 . The mean percentage improvement from baseline in PASI scores increased over time in both treatment groups but was significantly greater in the briakinumab group at all time points .\n\n【42】### _Physician's Global Assessment_\n\n【43】Significantly more patients in the briakinumab group than in the methotrexate group met the criterion of a score of 0 or 1 on the physician's global assessment, both at week 24 (80.5% vs. 34.4%) and at week 52 (63.0% vs. 20.2%) (P<0.001 for the comparison at both time points). At week 12, a total of 68.2% of the patients in the briakinumab group, as compared with 22.1% in the methotrexate group, had a score of 0 or 1 (P<0.001). Significantly more patients in the briakinumab group than in the methotrexate group had a score of 0 (clear) at week 24 (46.1% vs. 9.2%) and at week 52 (45.5% vs. 9.8%) (P<0.001 for the comparison at both time points). The median time to a score of 0 or 1 on the physician's global assessment was 69 days in the briakinumab group, as compared with 171 days in the methotrexate group (P<0.001). Significantly greater percentages of patients in the briakinumab group than in the methotrexate group had a score of 0 or 1 at each time point from week 4 to week 52 .\n\n【44】### _Other Assessments_\n\n【45】Baseline scores on the Nail Psoriasis Severity Index were similar in the two treatment groups . Mean scores for the target fingernail at baseline, week 24, and week 52 were 4.8, 2.1, and 1.2, respectively, in the briakinumab group, as compared with 4.8, 3.0, and 3.0, respectively, in the methotrexate group (P<0.001 for the change from baseline with briakinumab vs. methotrexate).\n\n【46】More patients in the briakinumab group than in the methotrexate group had a score of 0 or 1 on the DLQI, both at week 24 (70.8% vs. 34.4%) and at week 52 (61.7% vs. 17.8%) (P<0.001 for the comparison at both time points). Similarly, significantly more patients in the briakinumab group than in the methotrexate group had a clinically meaningful reduction in the DLQI score, both at week 24 (66.2% vs. 47.9%) and at week 52 (56.5% vs. 18.4%) (P<0.001 for the comparison at both time points). Mean reductions in DLQI scores from baseline were significantly greater in the briakinumab group than in the methotrexate group at all the time points assessed .\n\n【47】For all efficacy variables, sensitivity analyses performed to account for missing data yielded similar results (data not shown).\n\n【48】Adverse Events\n--------------\n\n【49】Table 2. Adverse Events According to Treatment Group.\n\n【50】During the 52-week trial, the most common treatment-emergent adverse events were nasopharyngitis, headache, diarrhea, arthralgia, and upper respiratory tract infection . Diarrhea and adverse events related to the injection site occurred more frequently among patients receiving briakinumab than among those receiving methotrexate.\n\n【51】A total of 12 patients in the briakinumab group (7.8%) discontinued the study owing to adverse events; in the case of 5 of these patients, the adverse events were serious (gastrointestinal hypomotility coincident with legionella infection, breast cancer, breast neoplasm \\[intraductal carcinoma\\], prostate cancer, and herpes zoster). A total of 10 patients in the methotrexate group (6.1%) discontinued the study owing to adverse events; in the case of 5 of these patients, the adverse events were serious (increased hepatic enzyme levels and hepatitis, sacroiliitis, diverticulitis, erythrodermic psoriasis, and angioedema and urticaria).\n\n【52】A total of 14 patients in the briakinumab group (9.1%) and 10 in the methotrexate group (6.1%) had treatment-emergent serious adverse events; 7 patients had serious infections: 4 in the briakinumab group (one case of legionella infection with candidemia and septic shock, one case of osteomyelitis, one case of herpes zoster, and one case of tonsillitis) and 3 in the methotrexate group (two cases of diverticulitis and one case of drug-induced hepatitis). The incidence rate of serious infectious adverse events was 4.1 per 100 patient-years in the briakinumab group and 2.7 per 100 patient-years in the methotrexate group.\n\n【53】Cancer was diagnosed in three patients in the briakinumab group (breast cancer in one patient, breast neoplasm \\[intraductal carcinoma\\] in one, and prostate cancer in one, at 276, 184, and 205 days, respectively, after the commencement of therapy). One death was reported: a patient in the methotrexate group died from an esophageal rupture. There were no reports of major cardiovascular events such as myocardial infarction, stroke, or death from cardiovascular causes.\n\n【54】Of the 317 patients who took part in the 52-week study, 131 in the methotrexate group and 117 in the briakinumab group enrolled in the open-label continuation study. Efficacy results from the open-label continuation study are shown in Table 2 in the Supplementary Appendix . To date, among patients receiving briakinumab continuously in both studies, the adverse-event profile in the open-label continuation study is similar to that in the 52-week trial . The adverse-event profile was similar in patients receiving methotrexate during the 52-week trial and patients receiving briakinumab continuously in both the 52-week trial and the open-label continuation study. A total of 154 patients have been treated with briakinumab in the two studies, representing a total of 247.6 patient-years of exposure to briakinumab. Among the patients receiving briakinumab in both trials combined, serious adverse events have been reported in 13.6% (11.3 events per 100 patient-years), serious infections in 3.2% (2.8 events per 100 person-years), and cancers in 3.2% (2.0 events per 100 person-years). Nine new serious adverse events have been reported among these patients during the open-label continuation study: one event each of ulcerative colitis, cyst, pyelonephritis, femur fracture, radius fracture, squamous-cell carcinoma, transitional-cell carcinoma, deep-vein thrombosis, and peripheral-artery occlusive disease.\n\n【55】Discussion\n----------\n\n【56】In this 52-week study, briakinumab showed significantly higher efficacy than methotrexate in reducing the signs and symptoms of moderate-to-severe plaque psoriasis. There was a nonsignificant imbalance in some adverse events; serious adverse events, adverse events leading to withdrawal from the study, serious infections, and cancers occurred more frequently among patients receiving briakinumab than among those receiving methotrexate .\n\n【57】This study provides insight into the maintenance of a response with methotrexate. In the methotrexate group, the percentage of patients with at least 75% improvement in the PASI score was the highest between weeks 16 and 24, supporting the selection of week 24 as an appropriate time point for the assessment of efficacy. After week 24, there was a relatively sharp drop in the percentage of patients in that group who had at least 75% improvement in the PASI score. It should be noted that at week 24, patients who did not have both an improvement of at least 75% in the PASI score and a score of 0 or 1 on the physician's global assessment discontinued the study per protocol; the largest decrease in the percentage of patients remaining in the study occurred between weeks 24 and 28, with more patients in the methotrexate group than in the briakinumab group discontinuing the study owing to a lack of efficacy. The rates of response with respect to both the PASI and the physician's global assessment were calculated in a conservative fashion by imputing nonresponses for patients who discontinued the study.\n\n【58】With respect to the long-term safety of methotrexate, in previous, 16-week trials, hepatic adverse events led to discontinuation in 12 of 43 patients (28%) who received 15 mg per week  and in 4 of 110 patients (4%) who received a starting dose of 7.5 mg per week, which was increased to 25 mg per week if required and if the side-effect profile warranted it.  In the present, 52-week study, in which methotrexate was administered at a dose of 5 to 25 mg per week, 16 patients (9.8%) in the methotrexate group had liver-related adverse events, but only 2 of these patients (1.2% of all the patients in the methotrexate group) discontinued the study drug. These results suggest that initiating methotrexate at a low dose and increasing it as indicated and as tolerated may be a safe strategy for long-term use of this drug.\n\n【59】The incidence of serious infections was 4.1 events per 100 patient-years with briakinumab and 2.7 events per 100 patient-years with methotrexate. Although more patients in the briakinumab group than in the methotrexate group had serious infections and cancers, too few patients were enrolled in the study to draw conclusions regarding these adverse events. No serious cardiovascular events (e.g., myocardial infarction and stroke) were reported during this study, in contrast with low frequencies of such events in another trial of briakinumab  and in trials of ustekinumab. \n\n【60】One of the limitations of this study was the lack of evaluation of manifestations of psoriasis, such as psoriatic arthritis. A recent study showed that ustekinumab was effective in reducing the signs and symptoms of psoriatic arthritis  ; future studies are needed to evaluate the effects of briakinumab in patients with psoriatic arthritis.\n\n【61】In conclusion, in patients with moderate-to-severe psoriasis, briakinumab showed higher efficacy than methotrexate through 52 weeks of treatment. Serious adverse events, serious infections, and cancers occurred more frequently among patients receiving briakinumab than among those receiving methotrexate, but the differences were not significant.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d5ecbc99-8bc4-4175-bb39-1bdfaf7f89bb", "title": "Maternal Exposure to Spermicides in Relation to Certain Birth Defects", "text": "【0】Maternal Exposure to Spermicides in Relation to Certain Birth Defects\nAbstract\n--------\n\n【1】Several studies have found no increase in the overall frequency of birth defects in association with the use of spermicides, but the possibility of an increase in specific defects remains. We evaluated this possibility in a large case-control study. Infants with certain malformations (265 with Down's syndrome, 396 with hypospadias, 146 with limb reduction defects, 116 with neoplasms, and 215 with neural-tube defects) were compared with 3442 control infants with a wide variety of other defects. Exposure to spermicides was assessed for three periods: use during the periconceptional period (one month before through one month after the last menstrual period), use during the first trimester (the first four lunar months of pregnancy), and any use during the lifetime. For the five groups of cases and for each interval, the odds ratios were close to 1.0 (range, 0.7 to 1.3); the upper 95 percent confidence bounds were 2.2 or lower. Risks did not increase with the duration of exposure. When each of the active ingredients in currently available spermicides was considered separately, no differences in odds ratios were apparent between the types of spermicides. With the possible exception of a subgroup of cases (limb reduction defects of unknown cause), these results suggest that risks for the five specific birth defects evaluated are not increased by exposure to spermicides.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1ff31adb-3155-412e-a013-0bc1eb4fb30d", "title": "Association between Polymorphism of the Glycogen Synthase Gene and Non-Insulin-Dependent Diabetes Mellitus", "text": "【0】Association between Polymorphism of the Glycogen Synthase Gene and Non-Insulin-Dependent Diabetes Mellitus\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The storage of glucose as glycogen in skeletal muscle is frequently impaired in patients with non-insulin-dependent diabetes mellitus (NIDDM) and their nondiabetic relatives. Despite an intensive search for candidate genes associated with NIDDM, no data have been available on the gene coding for the key enzyme of this pathway, glycogen synthase.\n\n【3】Methods and Results\n-------------------\n\n【4】Using a human complementary DNA probe, the restriction enzyme XbaI, and Southern blot analysis, we identified two polymorphic alleles, A <sub>1 </sub> and A <sub>2 </sub> , in the glycogen synthase gene. The gene was localized to chromosome 19. The A <sub>1 </sub> A <sub>2 </sub> or A <sub>2 </sub> A <sub>2 </sub> genotype was found in 30 percent of 107 patients with NIDDM but in only 8 percent of 164 nondiabetic subjects without a family history of NIDDM (P<0.001). The diabetic patients with the A <sub>2 </sub> allele had a stronger family history of NIDDM (P = 0.019), a higher prevalence of hypertension (P = 0.008), and a more severe defect in insulin-stimulated glucose storage (P = 0.001) than the diabetic patients with the A <sub>1 </sub> allele. The concentration of the glycogen synthase protein in biopsy specimens of skeletal muscle from the patients with the A <sub>2 </sub> allele was normal, however, suggesting that expression of the gene was unaltered. The XbaI polymorphism was due to a change of a single base in an intron.\n\n【5】Conclusions\n-----------\n\n【6】The XbaI polymorphism of the glycogen synthase gene identifies a subgroup of patients with NIDDM characterized by a strong family history of NIDDM, a high prevalence of hypertension, and marked insulin resistance.\n\n【7】Introduction\n------------\n\n【8】There is strong evidence that non-insulin-dependent diabetes mellitus (NIDDM) is a genetic disorder  . The concordance in identical monozygotic twins approaches 90 percent,  and the lifetime risk of NIDDM is about 40 percent in children with one diabetic parent  . Muscle and hepatic resistance to insulin is a characteristic feature of patients with NIDDM  . In muscle, glucose is oxidized to carbon dioxide and water or stored as glycogen. Defects in the nonoxidative pathway (i.e., glycogen synthesis) predominate in the early stages of diabetes and also occur in persons at increased risk for NIDDM  . Impaired activation of the key enzyme of glycogen synthesis, glycogen synthase, has been reported in patients with NIDDM  and in insulin-resistant first-degree relatives of such patients  . There is as yet no evidence, however, linking NIDDM and insulin resistance to the glycogen synthase gene.\n\n【9】The purpose of this study was to determine whether polymorphism of the human glycogen synthase gene is associated with NIDDM in general and with insulin resistance in particular. To accomplish this, we screened patients with NIDDM and subjects with no family history of NIDDM for polymorphisms in the glycogen synthase gene with a human glycogen synthase complementary DNA (cDNA) probe.\n\n【10】Methods\n-------\n\n【11】Subjects\n--------\n\n【12】We studied 107 unrelated patients with NIDDM and 164 unrelated nondiabetic subjects with no family history of NIDDM. The patients with NIDDM were consecutive patients seen in an outpatient clinic. The nondiabetic subjects were from the same ethnic group and were recruited from the spouses of the diabetic patients (67 subjects), members of the staff of Helsinki University (66 subjects), and nondiabetic patients in the outpatient clinic (31 subjects). In addition, 18 members of three families were examined to establish whether the possible polymorphism was inherited in a mendelian fashion. All the patients with NIDDM were treated with diet, oral hypoglycemic drugs, or both. A subgroup of 32 patients with diabetes underwent studies with the euglycemic-hyperinsulinemic clamp, and 15 diabetic patients underwent muscle biopsies for the measurement of glycogen synthase protein. None of these patients had clinical evidence of cardiac, hepatic, or renal disease or endocrine disease other than diabetes. Informed consent was obtained from all the subjects, and the study protocol was approved by the local ethics committee.\n\n【13】DNA Studies\n-----------\n\n【14】Genomic DNA was extracted from frozen peripheral-blood leukocytes by standard methods  . Aliquots of DNA (5 μg) were digested with 13 different restriction enzymes (ApaI, BamHI, BglII, EcoRI, EcoRV, HindIII, MspI, NcoI, PstI, PvuII, TaqI, XmnI, and XbaI) to search for polymorphisms. The resulting fragments were separated by agarose-gel electrophoresis, transferred to nitrocellulose filters, hybridized with a human glycogen synthase cDNA probe,  and labeled with phosphorus-32 (specific activity, 0.6 micro Ci per nanogram of DNA). The filters were prehybridized with a solution containing 4 × saline sodium citrate buffer (SSC) (1 × SSC is 0.15 M sodium chloride and 0.015 M sodium citrate), 10 × Denhardt's solution, 0.1 M phosphate buffer (pH 6.8), 1 mg of herring-sperm DNA per 20 ml of solution, and 5 percent dextran sulfate at 65 °C for 24 hours. The filters were then hybridized with the  P-labeled glycogen synthase probe for 24 hours with the same solution and were washed twice for 20 minutes each with 3 × SSC and 0.1 percent sodium dodecyl sulfate at 65 °C. The bands were visualized by autoradiography with Fuji Rx film within two days and quantitated by densitometry.\n\n【15】Localization of the Chromosome for Glycogen Synthase\n----------------------------------------------------\n\n【16】To determine the chromosomal location of the glycogen synthase gene, Bios chromosome panels I and II (containing human and hamster somatic-cell hybrid DNA digested with the restriction enzyme EcoRI) (Bios, New Haven, Conn.) were hybridized with the  P-labeled glycogen synthase cDNA probe. The signals obtained were compared with the fragment pattern in control lanes containing either human or hamster DNA. The fragments specific to humans were present only in cell lines that retained human chromosome 19.\n\n【17】Localization of the XbaI Polymorphism\n-------------------------------------\n\n【18】To determine the location of the restriction-fragment-length polymorphism, genomic glycogen synthase clones were isolated from the Lambda Dash genomic library (Stratagene, La Jolla, Calif.) with the glycogen synthase cDNA probe  . The putative area for a new XbaI cleavage site was sequenced directly from genomic DNA with the double-stranded DNA (dsDNA) Cycle Sequencing System (GIBCO BRL, Gaithersburg, Md.).\n\n【19】Glycogen Synthase Protein in Skeletal Muscle\n--------------------------------------------\n\n【20】Twenty to 40 mg of muscle was obtained by biopsy of the vastus lateralis muscle in each of eight diabetic patients with the A <sub>1 </sub> A <sub>1 </sub> genotype and each of seven patients with the A <sub>1 </sub> A <sub>2 </sub> genotype. The biopsy specimens were immediately placed in liquid nitrogen and were kept frozen at -70 °C until the concentration of glycogen synthase protein was measured by immunoblotting  with a polyclonal antibody. The antibody was raised in a rabbit with use of an oligopeptide composed of 12 amino acids and specific for the carboxy terminal of the enzyme (Ser-Pro-Thr-Ser-Ser-Leu-Gly-Glu-Arg-Asn-Cys). After the blot was washed, it revealed a single 86-kd band, compatible with the molecular size of glycogen synthase. The blots were quantitated by densitometry and by a count of the radioactivity of the excised bands with a gamma counter (Wallac, Turku, Finland). DNA was quantitated with a fluorometric assay. The results were expressed as a fraction of the mean value in a control sample -- i.e., as the relative number of optical-density units per nanogram of DNA.\n\n【21】Clinical and Metabolic Investigations\n-------------------------------------\n\n【22】Blood pressure was measured with the patients in the sitting position after 30 minutes of rest. Hypertension was defined as the presence of blood pressure above 160/95 mm Hg or the use of a known treatment for hypertension. Blood samples for determinations of fasting plasma glucose and serum insulin, C peptide, total and high-density lipoprotein (HDL) cholesterol, triglyceride, and hemoglobin A <sub>1c </sub> were collected after an overnight 12-hour fast. In the diabetic patients, β-cell function was also assessed by measuring serum C-peptide concentrations six minutes after the intravenous administration of 1 mg of glucagon. The body-mass index was calculated as the weight in kilograms divided by the square of the height in meters.\n\n【23】The action of insulin was measured by means of a two-hour study with the euglycemic-hyperinsulinemic clamp, combined with indirect calorimetry and the infusion of \\[3-  H\\]glucose in 32 patients with NIDDM  . Total glucose disposal was calculated by adding the mean value of residual hepatic glucose production during the final 60 minutes of the insulin-clamp study to the mean rate of glucose infusion during the same period. The rate of glycogen synthesis was calculated as the difference between the rate of glucose disposal and the rate of glucose oxidation, as measured by calorimetry.\n\n【24】Assays\n------\n\n【25】Plasma glucose was measured with a glucose oxidase method adapted for the Beckman Glucose Analyzer II (Beckman Instruments, Fullerton, Calif.). Serum insulin and C peptide were measured by double-antibody radioimmunoassays. The specific activity of \\[3-  H\\]glucose was measured in duplicate in supernatants of 1 M perchloric acid extracts of plasma samples, after the evaporation of radiolabeled water. Serum cholesterol, HDL cholesterol, and triglycerides were measured by specific enzymatic assays. Hemoglobin A <sub>1c </sub> was measured by high-performance liquid chromatography (reference level, 4 to 6 percent).\n\n【26】Statistical Analysis\n--------------------\n\n【27】The Mann-Whitney rank-sum test was used to test for differences in the distribution of continuous variables, and chi-square analysis with Yates' correction was used to test the significance of differences in frequency.\n\n【28】Results\n-------\n\n【29】XbaI Polymorphism of the Glycogen Synthase Gene\n-----------------------------------------------\n\n【30】Figure 1. Autoradiograph of DNA after Digestion with the Restriction Enzyme XbaI and Hybridization with a Human Glycogen Synthase cDNA Probe.\n\n【31】Two constant bands of 23 kb and 5.3 kb are shown. In addition, two polymorphic alleles are visible, A <sub>1 </sub> and A <sub>2 </sub> . A <sub>1 </sub> A <sub>1 </sub> refers to a genotype with a band of 6.8 kb, and A <sub>2 </sub> A <sub>2 </sub> to a genotype with bands of 5.1 kb and 1.7 kb. Subjects with the A <sub>1 </sub> A <sub>2 </sub> genotype are considered heterozygous for both alleles. Fragment sizes are shown in kilobases with lambda HindIII markers (Promega, Madison, Wis.).\n\n【32】Two constant 23-kb and 5.3-kb bands were found in all the study subjects when DNA was digested with XbaI and hybridized with the glycogen synthase cDNA probe . In addition, two polymorphic patterns were found. Many subjects had a single 6.8-kb band, referred to as allele A <sub>1 </sub> ; some subjects, however, did not have this band but instead had a 5.1-kb and a 1.7-kb band, together referred to as allele A <sub>2 </sub> . Subjects who had only the A <sub>1 </sub> or the A <sub>2 </sub> allele were considered homozygous for the corresponding genotypes (A <sub>1 </sub> A <sub>1 </sub> and A <sub>2 </sub> A <sub>2 </sub> in Figure 1 ), whereas subjects with all three bands were considered heterozygous (A <sub>1 </sub> A <sub>2 </sub> ). It was evident from the family studies that the alleles were inherited in a mendelian fashion. The polymorphic A <sub>2 </sub> allele was due to a change of a single base in an intron, in which thymidine was substituted for cytidine (CCTAGA to TCTAGA), creating a new XbaI site 302 base pairs upstream from position 1970 of the cDNA.\n\n【33】Table 1. Frequencies of Alleles and Genotypes with XbaI Polymorphism of the Glycogen Synthase Gene in Nondiabetic Subjects with No Family History of NIDDM and in Patients with NIDDM.\n\n【34】The A <sub>2 </sub> allele was four times more frequent among the patients with NIDDM than among the nondiabetic subjects (17 percent vs. 4 percent; P<0.001) . Among the 164 nondiabetic subjects, the frequency of the A <sub>1 </sub> A <sub>1 </sub> genotype was 92 percent, whereas that of the A <sub>1 </sub> A <sub>2 </sub> genotype was 8 percent (P<0.001) . None of the nondiabetic subjects were homozygous for A <sub>2 </sub> A <sub>2 </sub> . Thirty percent of the patients with NIDDM had the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype, as compared with 8 percent of the nondiabetic subjects (P<0.001).\n\n【35】Table 2. Characteristics of the Study Subjects, According to the Presence or Absence of the A <sub>2 </sub> Allele.\n\n【36】The onset of NIDDM occurred at approximately the same age in the patients with the A <sub>1 </sub> A <sub>1 </sub> genotype and those with either the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype (49 vs. 51 years, respectively). The patients with the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype reported a history of NIDDM in at least two other family members more often than did the patients with the A <sub>1 </sub> A <sub>1 </sub> genotype (81 percent vs. 35 percent; P = 0.019) . In addition, hypertension was more common among the patients with NIDDM who had the A <sub>2 </sub> allele than among those who did not (72 percent vs. 37 percent; P = 0.008). Hypertension was also more common among the nondiabetic subjects with the A <sub>2 </sub> allele than among those without it (36 percent vs. 9 percent; P = 0.013). Furthermore, there was a tendency toward higher values for systolic blood pressure in the nondiabetic subjects with the A <sub>1 </sub> A <sub>2 </sub> genotype than in those with the A <sub>1 </sub> A <sub>1 </sub> genotype (P = 0.056). The mean (±SE) C-peptide concentrations at base line (0.8 ±0.1 vs. 0.8 ±0.1 nmol per liter) and after stimulation with glucagon (1.3 ±0.1 vs. 1.4 ±0.3 nmol per liter) were similar, respectively, in the patients with NIDDM with the A <sub>1 </sub> A <sub>1 </sub> genotype and those with the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype. Among the patients who had the A <sub>1 </sub> A <sub>2 </sub> genotype and those who had the A <sub>2 </sub> A <sub>2 </sub> genotype, the values for these measurements, as well as those for glucose metabolism , were similar.\n\n【37】Glucose Metabolism\n------------------\n\n【38】Figure 2. Rates of Glucose Disposal and Glucose Storage as Glycogen in 32 Patients with NIDDM, According to Allelic Status.\n\n【39】Glucose metabolism is expressed as milligrams per kilogram of lean body mass per minute. The horizontal bars indicate mean values.\n\n【40】Studies using the euglycemic-hyperinsulinemic clamp were performed in 32 patients with NIDDM, 16 of whom had the A <sub>1 </sub> A <sub>1 </sub> genotype and 16 of whom had either the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype. These groups were matched with respect to sex, age (56 and 58 years, respectively), and body-mass index (27.4 ±1.0 and 27.9 ±1.0). The mean total glucose disposal was lower in the patients with the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype than in those with the A <sub>1 </sub> A <sub>1 </sub> genotype (4.0 ±0.3 vs. 6.0 ±0.6 mg \\[22.3 ±1.8 vs. 33.4 ±3.6 micromoles\\] per kilogram of lean body mass per minute; P = 0.026) . Among the patients with NIDDM, the glucose oxidation rate did not differ between those with the A <sub>2 </sub> allele and those without it (2.7 ±0.2 vs. 2.5 ±0.2 mg \\[15.5 ±1.0 vs. 14.1 ±0.8 micromoles\\] per kilogram of lean body mass per minute). The rate of nonoxidative glucose metabolism (glycogen synthesis) was reduced by half in the diabetic patients with the A <sub>1 </sub> A <sub>2 </sub> or the A <sub>2 </sub> A <sub>2 </sub> genotype as compared with those with the A <sub>1 </sub> A <sub>1 </sub> genotype (1.3 ±0.3 vs. 3.4 ±0.5 mg \\[7.0 ±1.4 vs. 19.0 ±2.8 micromoles\\] per kilogram of lean body mass per minute; P<0.001).\n\n【41】Glycogen Synthase Protein\n-------------------------\n\n【42】Figure 3. Concentrations of Glycogen Synthase Protein in Skeletal-Muscle-Biopsy Specimens from 15 Patients with the A <sub>1 </sub> A <sub>1 </sub> or the A <sub>1 </sub> A <sub>2 </sub> Genotype.\n\n【43】Values for glycogen synthase protein are expressed in relative units per nanogram of DNA. The horizontal bars indicate mean values.\n\n【44】Glycogen synthase protein was measured in muscle-biopsy specimens from eight diabetic patients with the A <sub>1 </sub> A <sub>1 </sub> genotype and seven patients with the A <sub>1 </sub> A <sub>2 </sub> genotype. The protein content was similar in the patients with the two genotypes . The mean concentrations of glycogen synthase protein in the patients with the A <sub>1 </sub> A <sub>1 </sub> and A <sub>1 </sub> A <sub>2 </sub> genotypes were 2.4 ±0.5 and 3.4 ±0.9 relative units per nanogram of DNA, respectively.\n\n【45】Discussion\n----------\n\n【46】The rationale for including the glycogen synthase gene in the list of candidate genes for NIDDM was the finding that activation of glycogen synthase by insulin was impaired in relatives of patients with NIDDM  . The glycogen synthase cDNA used in this study includes the vast majority of the coding sequence of the gene (nucleotides 414 to 3535). The restriction enzyme XbaI has two cleavage sites, corresponding to nucleotides 3260 and 3333. Both sites are outside the coding region of the gene; the stop codon is at position 2374 of the cDNA  . By subcloning and direct sequencing of genomic DNA, the XbaI polymorphism was found to be due to a point mutation (CCTAGA to TCTAGA) in an intron 302 base pairs upstream from position 1970 of the cDNA. A recent screening of eight Danish patients with NIDDM to identify mutations in the glycogen synthase gene that used single-stranded conformational polymorphism with primers covering the entire coding sequence of the gene did not reveal any mutations,  but it is not known whether any of the Danish patients had the A <sub>2 </sub> allele. It is therefore unlikely that the mutation we found would result in any structural change of the glycogen synthase enzyme. This does not exclude the possibility that it could influence expression of the gene in skeletal muscle.\n\n【47】To address this question, we measured the concentrations of the glycogen synthase protein in skeletal-muscle-biopsy specimens. The results suggest that expression of the glycogen synthase gene is normal in skeletal muscle of patients with the A <sub>1 </sub> A <sub>2 </sub> allele. There was a fourfold variation in protein concentrations among these patients and a four- to fivefold variation in rates of glucose uptake. It should be noted that these results only suggest that the A <sub>2 </sub> allele is a marker associated with NIDDM, not that a mutation in the glycogen synthase gene is the cause of NIDDM in these patients. The possibility that the association with the restriction-fragment-length polymorphism is due to a disequilibrium of linkage with another gene should also be kept in mind.\n\n【48】Hypertension was twice as frequent in the patients with NIDDM who had the A <sub>2 </sub> allele as in the patients who did not. More important, 36 percent of the normal subjects with the A <sub>2 </sub> allele had hypertension, as compared with 9 percent of those with the A <sub>1 </sub> allele. Both hypertension and NIDDM have been considered components of the so-called metabolic syndrome, the common denominator of which is insulin resistance  . In fact, normal subjects with hypertension have insulin resistance that is caused by impaired glycogen synthesis  . In patients with NIDDM who had the A <sub>2 </sub> allele, total glucose metabolism was reduced, mainly because of a reduction in the rate of glycogen synthesis in skeletal muscle. It should be remembered, however, that all the diabetic patients were severely insulin-resistant as compared with normal subjects  .\n\n【49】In conclusion, the A <sub>2 </sub> allele of the human glycogen synthase gene on chromosome 19 identifies a subgroup of patients with NIDDM who have a strong family history of NIDDM and in whom hypertension and insulin resistance are prevalent. The A <sub>2 </sub> allele can thus be considered a genetic marker for NIDDM.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9db1c818-43df-4874-8013-e687fe228e3d", "title": "Belimumab for Systemic Lupus Erythematosus", "text": "【0】Belimumab for Systemic Lupus Erythematosus\nA 20-year-old woman with SLE presents with disease flares and receives belimumab, a monoclonal antibody that binds to B-cell activating factor, inhibiting B-cell stimulation. Belimumab is considered for patients who do not have a response or have adverse effects with first-line therapies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9e3f8284-0838-4a5c-bdc7-05de9ce69b91", "title": "Alpha", "text": "【0】Alpha\nA 60-year-old white man presents for evaluation of progressive dyspnea. He is a former smoker with a 20-pack-year smoking history and a 10-year history of chronic obstructive pulmonary disease (COPD). There is no family history of COPD. Severe airflow obstruction is seen on spirometry, with a forced expiratory volume in 1 second that is 40% of the predicted value. Should he be evaluated for alpha <sub>1 </sub> \\-antitrypsin (AAT) deficiency? If AAT deficiency is documented, how should his case be managed?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "73470bb4-67ff-410c-b8a9-115f94f7fbcf", "title": "Twice-Daily Compared with Once-Daily Thoracic Radiotherapy in Limited Small-Cell Lung Cancer Treated Concurrently with Cisplatin and Etoposide", "text": "【0】Twice-Daily Compared with Once-Daily Thoracic Radiotherapy in Limited Small-Cell Lung Cancer Treated Concurrently with Cisplatin and Etoposide\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】For small-cell lung cancer confined to one hemithorax (limited small-cell lung cancer), thoracic radiotherapy improves survival, but the best ways of integrating chemotherapy and thoracic radiotherapy remain unsettled. Twice-daily accelerated thoracic radiotherapy has potential advantages over once-daily radiotherapy.\n\n【3】Methods\n-------\n\n【4】We studied 417 patients with limited small-cell lung cancer. All the patients received four 21-day cycles of cisplatin plus etoposide. We randomly assigned these patients to receive a total of 45 Gy of concurrent thoracic radiotherapy, given either twice daily over a three-week period or once daily over a period of five weeks.\n\n【5】Results\n-------\n\n【6】Twice-daily treatment beginning with the first cycle of chemotherapy significantly improved survival as compared with concurrent once-daily radiotherapy (P=0.04 by the log-rank test). After a median follow-up of almost 8 years, the median survival was 19 months for the once-daily group and 23 months for the twice-daily group. The survival rates for patients receiving once-daily radiotherapy were 41 percent at two years and 16 percent at five years. For patients receiving twice-daily radiotherapy, the survival rates were 47 percent at two years and 26 percent at five years. Grade 3 esophagitis was significantly more frequent with twice-daily thoracic radiotherapy, occurring in 27 percent of patients, as compared with 11 percent in the once-daily group (P<0.001).\n\n【7】Conclusions\n-----------\n\n【8】Four cycles of cisplatin plus etoposide and a course of radiotherapy (45 Gy, given either once or twice daily) beginning with cycle 1 of the chemotherapy resulted in overall two- and five-year survival rates of 44 percent and 23 percent, a considerable improvement in survival rates over previous results in patients with limited small-cell lung cancer.\n\n【9】Introduction\n------------\n\n【10】Of the approximately 170,000 cases of lung cancer diagnosed each year in the United States, 20 percent are small-cell cancers.  Staging systems divide small-cell lung cancer into two categories: limited and extensive. The former is clinically confined to one side of the chest and is treatable by radiotherapy field sizes (portals) tolerated by normal tissues.\n\n【11】The main treatment for limited small-cell lung cancer is radiotherapy and chemotherapy. Cisplatin plus etoposide has largely supplanted the older regimens of cyclophosphamide, doxorubicin, and vincristine. Advantages of the cisplatin–etoposide regimen over the older regimen include the absence of toxic effects on intrathoracic organs and the ability to use thoracic radiotherapy concurrently.\n\n【12】A meta-analysis of trials comparing chemotherapy alone with combined chemotherapy and thoracic radiotherapy found that combined treatment improved survival among patients with limited small-cell lung cancer,  but the best method of integrating thoracic radiotherapy with chemotherapy remained undefined. The optimal total dose, volume, duration, and timing of thoracic radiotherapy have not been tested in prospective trials. Fractionation of the customary once-daily radiotherapy dose into two treatments each day has biologic advantages and has been successful in pilot studies. In vitro, small-cell lung-cancer cell lines have marked radiosensitivity even to small doses of radiation.  The dose–response curves for small-cell lung-cancer cell lines lack a shoulder, which means that even at relatively low doses per fraction, small cells are killed exponentially; by contrast, radiation spares cell populations that have a shoulder. For these reasons, multiple small fractions of radiotherapy can kill small-cell cancer while reducing permanent damage to normal tissues. In addition, the use of small fractions may diminish the risks of late effects of radiation.\n\n【13】Pilot studies of twice-daily thoracic radiotherapy suggested that this therapy might have excellent results when combined with cisplatin and etoposide. The two-year survival rate was approximately 40 percent, and the rates of myelosuppression and esophagitis were tolerable: grade 3 granulocytopenia occurred in 70 to 80 percent of the treated patients and grade 3 esophagitis in 35 to 40 percent. \n\n【14】Cisplatin–etoposide combined with once-daily radiotherapy was also examined in pilot studies.  The Southwest Oncology Group, using daily fractionated thoracic radiotherapy at a total dose of 45 Gy, reported a two-year survival rate of 40 percent.  Toxic effects were equally reversible in pilot studies of twice-daily or once-daily thoracic radiotherapy.\n\n【15】In this study of limited small-cell lung cancer, we compared once-daily and twice-daily thoracic radiotherapy while holding other variables constant.\n\n【16】Methods\n-------\n\n【17】Patients\n--------\n\n【18】We enrolled 419 patients in the study, which began in May 1989 and ended in July 1992. Two patients were found to have been enrolled twice. The primary analysis thus included 417 patients and was conducted on an intention-to-treat basis. Of the 417 patients, 36 (21 receiving once-daily radiation and 15 receiving twice-daily radiation) were excluded from the analysis of eligible patients: 7 withdrew from treatment and never received any therapy according to the protocol, and 29 were found to be ineligible. The reasons for ineligibility were the absence of pretreatment tumor measurements (eight patients), extensive disease (six), histologic findings of non–small-cell cancer (six), incomplete staging studies (five), elevated serum aspartate aminotransferase level (one), incorrect diagnosis (one), inadequate performance status  , and absence of on-study data (one). Thus, 381 patients (185 receiving once-daily treatment and 196 receiving twice-daily treatment) were eligible for a secondary analysis.\n\n【19】For patients to be eligible the small-cell lung cancer had to be confined to one hemithorax, the ipsilateral supraclavicular fossa, or both. Patients with pleural effusions found on chest films were excluded, regardless of cytologic findings, as were patients with contralateral hilar or supraclavicular adenopathy. Staging was done by computed tomography (CT) or magnetic resonance imaging (MRI) of the chest, abdomen, and brain; radionuclide bone scanning; and bilateral iliac-crest bone marrow aspiration and biopsy. Adequate organ function was defined as a white-cell count of at least 4000 per cubic millimeter, a platelet count of at least 100,000 per cubic millimeter, a serum creatinine level of less than 1.5 mg per deciliter (130 μmol per liter), serum aspartate aminotransferase and alanine aminotransferase levels less than two times the upper limit of normal, a serum bilirubin level of less than 0.5 mg per deciliter (8.6 μmol per liter), and a forced expiratory volume in one second of at least 1.0 liter. Symptomatic cardiac disease or a myocardial infarction within the previous six months was cause for exclusion. Patients had to be available for follow-up. In all cases, histologic or cytologic findings confirmed the diagnosis of small-cell lung cancer. Patients with prior cancer or prior treatment with either chemotherapy or radiotherapy were ineligible. All patients enrolled in the study gave informed consent.\n\n【20】Chemotherapy\n------------\n\n【21】The patients received four cycles of chemotherapy. Each three-week cycle consisted of 60 mg of cisplatin per square meter of body-surface area on day 1 and 120 mg of etoposide per square meter on days 1, 2, and 3. No dose adjustments were permitted for the first two cycles. During cycles 3 and 4, the dose of etoposide was reduced for patients with grade 4 toxic effects, febrile neutropenia or documented infection, or thrombocytopenia associated with bleeding. The dose of cisplatin was reduced during cycles 3 and 4 for patients with serum creatinine levels of 1.6 to 2.5 mg per deciliter (140 to 220 μmol per liter) and was further reduced if the levels were 2.6 mg per deciliter (230 μmol per liter) or higher.\n\n【22】Thoracic Radiotherapy\n---------------------\n\n【23】In both groups, the total dose of thoracic radiotherapy was 45 Gy for each patient. Patients receiving once-daily therapy received 1.8 Gy daily in 25 treatments over a period of five weeks. Accelerated twice-daily thoracic radiotherapy involved the administration of 1.5 Gy in 30 treatments over a period of three weeks. In both groups, thoracic radiotherapy began concurrently with the first cycle of chemotherapy.\n\n【24】The target volume for thoracic radiotherapy, which was similar in both groups, included the gross tumor, as defined by the chest CT scan, and the bilateral mediastinal and ipsilateral hilar lymph nodes. Irradiation of uninvolved supraclavicular fossae was forbidden. The inferior border extended 5 cm below the carina or to a level including ipsilateral hilar structures, whichever was lower. The clinically determined volume was expanded by a margin of 1 to 1.5 cm.\n\n【25】Radiotherapy treatment used linear accelerators; no cobalt-60 machines were allowed. Patients underwent treatment setup with radiotherapy simulators to mark field borders before treatment. Reduction of the field to conform to a smaller target volume after treatment was not allowed.\n\n【26】Interruptions of thoracic radiotherapy were discouraged, but it was interrupted when patients had platelet counts under 50,000 per cubic millimeter, weight loss of 4.5 kg (10 lb) or more (grade 2), or hospitalization for neutropenic fever or sepsis, but not when patients had difficulty swallowing or fever with low white-cell counts.\n\n【27】Prophylactic Cranial Irradiation\n--------------------------------\n\n【28】Systemic therapy was scheduled to last 12 weeks. The stage of disease was then determined again according to the results of chest radiography and head and chest CT. Because of the high frequency of brain metastases (50 percent), patients with a complete response were offered prophylactic cranial irradiation, despite reports of neurotoxicity.  This treatment consisted of 10 doses of 2.5 Gy to the midplane of the brain over a two-week period, for a total of 25 Gy. \n\n【29】Measurement of Response\n-----------------------\n\n【30】A complete response was defined as the disappearance of all clinical evidence of tumor. A decrease of 50 percent or more in the product of the length and width of any measurable tumor for at least four weeks was counted as a partial response. The disease was considered to have progressed if the patient lost more than 10 percent of body weight, if there was a 25 percent increase in the diameter of any tumor 2.0 cm or more in diameter or a 50 percent increase in the diameter of any tumor less than 2.0 cm in diameter, or if any new tumor appeared.\n\n【31】End Points\n----------\n\n【32】Overall survival, the primary end point of the trial, was measured from the date of entry into the study to the date of death from any cause. Treatment was considered to have failed if there was objective evidence of disease progression, regardless of tumor response, or death without clear-cut evidence of tumor progression. Failure was considered local when an intrathoracic relapse occurred after a complete response or when there was no complete response.\n\n【33】Statistical Analysis\n--------------------\n\n【34】The target enrollment was 400 patients. This sample size would give the study an 82 percent power to detect an absolute difference in two-year survival rates of 15 percent (25 percent for patients receiving once-daily treatment and 40 percent for patients receiving twice-daily treatment) at the 0.05 level with a two-sided test. This difference is equivalent to a 50 percent increase in median survival under the assumption of exponential distribution of survival.\n\n【35】With the exponential distribution of survival, we expected to detect a hazard ratio of 1.4 after adjustment for variations in radiotherapy and a total of 353 deaths by the end of the study.\n\n【36】Patients were randomized according to a permuted-block scheme, stratified according to Eastern Cooperative Oncology Group performance status (0 or 1 vs. 2), sex, and weight loss during the six months before entry (less than 5 percent of body weight vs. 5 percent or more). \n\n【37】The survival distributions for overall survival and time to treatment failure were estimated according to the method of Kaplan and Meier.  For comparison of ordinal data, such as the incidence of toxic effects, an exact Kruskal–Wallis test was used.  For comparison of binary data, such as response rates, Fisher's exact test was used.  For comparison of the survival distributions, a Mantel log-rank test was used for discrete covariates,  with stratification according to the stratification variables unless otherwise indicated. All P values are based on two-sided tests.\n\n【38】In our analysis of prognostic factors, the proportional-hazards regression model was used to estimate the joint effect of prognostic factors on survival.  A step-down procedure that consisted of dropping the least significant covariates, one at a time, was used to obtain a more parsimonious model.\n\n【39】Results\n-------\n\n【40】Table 1. Characteristics of the Study Patients According to Assigned Treatment.\n\n【41】Table 1 shows the main characteristics of the 417 patients, of whom 206 received once-daily therapy and 211 twice-daily therapy. The median age was 63 years (range, 34 to 80) for the patients receiving once-daily therapy, and 61 years (range, 30 to 82) for the patients receiving twice-daily therapy. Forty percent of the patients assigned to once-daily therapy were over 65 years of age, as compared with 31 percent of those assigned to twice-daily therapy (P= 0.07). All other variables were well balanced. Only 8 (2 percent) of the 356 patients for whom histologic data were available had variant histologic findings, an admixture of large-cell and small-cell lung cancer thought to be associated with a poorer prognosis. \n\n【42】Toxic Effects\n-------------\n\n【43】Table 2. Treatment Complications According to the Frequency of Radiotherapy. Table 3.  Table 3. Incidence of Toxic Effects According to the Frequency of Radiotherapy.\n\n【44】Table 2 and Table 3 show the toxic effects observed among 409 patients, including 28 ineligible patients. Despite major myelosuppression (in approximately 90 percent of patients in both groups), there was only one death from myelotoxicity. Overall, there were only 11 treatment-related deaths (6 in the group receiving twice-daily therapy and 5 in the group receiving once-daily therapy). No hematopoietic growth factors were used.\n\n【45】There were significant differences between the groups in the incidence of esophagitis (P<0.001). Fifty-six percent of the patients receiving once-daily therapy and 37 percent of those receiving twice-daily therapy had no esophageal toxic effects (grade 0). Grade 3 toxicity, defined as an inability to swallow solids, requiring narcotic analgesics or the use of a feeding tube, occurred in 11 percent of patients receiving once-daily therapy and 27 percent of those receiving twice-daily therapy. There was no difference between the groups in the incidence of grade 4 toxicity (hospitalization of the patient or perforation of the esophagus). There were no reports of permanent stricture due to the acute esophagitis. The duration of esophagitis was not a study end point.\n\n【46】Response Rates\n--------------\n\n【47】Table 4. Results According to the Frequency of Radiotherapy.\n\n【48】Table 4 shows the response rates of the 381 eligible patients. Nearly 90 percent had objective responses. There was no significant difference in the response rate between the groups, nor did the rate differ significantly between the eight patients with variant histologic findings and the patients with standard histologic findings.\n\n【49】Survival\n--------\n\n【50】Figure 1. Kaplan–Meier Estimates of Overall Survival for All 417 Patients Assigned to Treatment Groups.\n\n【51】As of this writing, the median follow-up was almost eight years, and the minimal potential follow-up was approaching five years. Of the 417 patients, 335 had died: 175 patients who received once-daily therapy (85 percent) and 160 patients who received twice-daily therapy (76 percent). The median survival was 20 months for all patients, 19 months for those receiving once-daily therapy, and 23 months for those receiving twice-daily therapy. The two-year survival rate was 44 percent for all patients, 41 percent for those receiving once-daily therapy, and 47 percent for those receiving twice-daily therapy (standard error for both groups, 3 percent). The five-year survival rate was 23 percent for all patients, 16 percent for those receiving once-daily therapy, and 26 percent for those receiving twice-daily therapy (standard error for both groups, 3 percent). The difference in survival between the two groups was statistically significant (P=0.04 by the log-rank test). The estimated hazard ratio for death with once-daily treatment as compared with twice-daily treatment was 1.2 (95 percent confidence interval, 1.0 to 1.6). Figure 1 shows the estimated survival distribution according to treatment group.\n\n【52】The rate of failure-free survival at two years was 24 percent for patients receiving once-daily therapy and 29 percent for those receiving twice-daily therapy (P=0.10). According to a proportional-hazards regression model, male sex (P=0.01) and a performance status of 2 (P=0.005) were associated with shorter failure-free survival.\n\n【53】Pattern of Treatment Failure\n----------------------------\n\n【54】Twice-daily thoracic radiotherapy reduced the rate of local failure: the rate was 52 percent in the group receiving once-daily therapy and 36 percent in the group receiving twice-daily therapy (P=0.06). The rates of simultaneous local and distant failure were significantly different between the groups: both local and distant failure occurred in 23 percent of the patients receiving once-daily therapy and 6 percent of those receiving twice-daily therapy (P=0.01).\n\n【55】Discussion\n----------\n\n【56】In this trial of chemoradiotherapy for small-cell lung cancer, we gave four cycles of cisplatin–etoposide chemotherapy concurrently with 45 Gy of thoracic radiation administered twice daily or once daily. The survival rate among the 417 patients exceeded that in any previously reported large, randomized trial of chemotherapy and radiotherapy for this disease.  After five years of follow-up, only 335 deaths have been reported, even though 353 deaths were anticipated at two years. Survival was significantly better in the group receiving twice-daily radiotherapy than in the group receiving once-daily radiotherapy (P=0.04). The magnitude of the difference between the groups at two years was quite small and clinically insignificant, but with further follow-up to five years, the difference between the treatments favored the twice-daily treatment group by 10 percent (standard error, 4 percent).\n\n【57】Many assert that adding thoracic radiotherapy to chemotherapy increases toxicity without improving survival. A meta-analysis of chemotherapy alone as compared with chemotherapy and radiotherapy found that the addition of radiotherapy improved the survival rate at three years only slightly.  The trials included in the meta-analysis all used cyclophosphamide-based or doxorubicin-based regimens; in none did initial treatment include cisplatin and etoposide.\n\n【58】Although it was introduced in the late 1970s,  the combination of cisplatin and etoposide emerged as primary therapy only in the early 1980s.  A clear advantage of cisplatin plus etoposide is that the combination can be given concurrently with relatively full doses of thoracic radiotherapy, with less morbidity than occurs with doxorubicin-based  or cyclophosphamide-based  regimens. The meta-analysis  identified no differences regarding the timing of thoracic radiotherapy and chemotherapy.\n\n【59】The best method of integrating chemotherapy and thoracic radiotherapy remains unknown. Because small-cell lung cancer responds well to thoracic radiotherapy, only moderate doses of radiation (40 to 50 Gy) have been used in most trials. Choi et al.  reported that esophagitis limited treatment when the total dose from twice-daily treatment exceeded 45 Gy, and that a total dose of 70 Gy could be tolerated with once-daily treatments. Papac and colleagues  reported a rate of local failure of only 3 percent with 60 Gy fractionated once daily, but with only a small gain in median survival. Without radiotherapy, local failure occurs in 90 percent of patients.  Our study verifies that local failure remains an important problem, but we found that improved local therapy contributes to both local control and survival.\n\n【60】The timing of concurrent radiotherapy and chemotherapy may be an important therapeutic variable. We initiated therapy at the same time as the first cycle of cisplatin plus etoposide. Others have begun radiotherapy at the time of later cycles of chemotherapy. Murray et al.  reported that cisplatin–etoposide therapy in combination with radiotherapy beginning with cycle 2 was superior to concurrent radiotherapy beginning with cycle 6. Recently, Takada and colleagues  verified that beginning radiotherapy concurrently with etoposide was superior to beginning radiotherapy after the completion of four cycles of chemotherapy. The Cancer and Leukemia Group B trial compared radiotherapy starting with cycle 1 of chemotherapy and radiotherapy starting with cycle 4.  This 1987 trial used cyclophosphamide-based chemotherapy. It found the best survival when the radiotherapy began with cycle 4. Others, particularly in Europe,  found that sequential strategies were superior to concurrent treatment, which was associated with excess toxicity. Cyclophosphamide-based or doxorubicin-based chemotherapy continues to be used in these studies, which may explain the inability to integrate concurrent thoracic radiotherapy successfully. The two-year survival in these trials is about half the rate in our study.\n\n【61】The number of deaths we initially projected to occur in two years has not occurred after a minimal follow-up of five years. Esophagitis after twice-daily radiotherapy did not lead to stricture, and all the affected patients recovered their ability to swallow. Only 1 death was attributable to hematologic toxicity, and there were only 11 treatment-related deaths. Although there was an imbalance in age between the groups, age did not influence survival significantly when isolated as a variable (data not shown).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b77223ba-6484-44bc-b65a-a37d45e4de12", "title": "Current Concepts: Chronic Thromboembolic Pulmonary Hypertension", "text": "【0】Current Concepts: Chronic Thromboembolic Pulmonary Hypertension\nChronic thromboembolic pulmonary hypertension can develop after acute pulmonary embolus but is often overlooked until pulmonary hypertension has led to dyspnea and right ventricular dysfunction. This review provides a guide to early diagnosis and management.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "88e21c62-3b97-4346-86fa-5beb3b0896aa", "title": "Radical Prostatectomy versus Watchful Waiting in Early Prostate Cancer", "text": "【0】Radical Prostatectomy versus Watchful Waiting in Early Prostate Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】In 2002, we reported the initial results of a trial comparing radical prostatectomy with watchful waiting in the management of early prostate cancer. After three more years of follow-up, we report estimated 10-year results.\n\n【3】Methods\n-------\n\n【4】From October 1989 through February 1999, 695 men with early prostate cancer (mean age, 64.7 years) were randomly assigned to radical prostatectomy (347 men) or watchful waiting (348 men). The follow-up was complete through 2003, with blinded evaluation of the causes of death. The primary end point was death due to prostate cancer; the secondary end points were death from any cause, metastasis, and local progression.\n\n【5】Results\n-------\n\n【6】During a median of 8.2 years of follow-up, 83 men in the surgery group and 106 men in the watchful-waiting group died (P=0.04). In 30 of the 347 men assigned to surgery (8.6 percent) and 50 of the 348 men assigned to watchful waiting (14.4 percent), death was due to prostate cancer. The difference in the cumulative incidence of death due to prostate cancer increased from 2.0 percentage points after 5 years to 5.3 percentage points after 10 years, for a relative risk of 0.56 (95 percent confidence interval, 0.36 to 0.88; P=0.01 by Gray's test). For distant metastasis, the corresponding increase was from 1.7 to 10.2 percentage points, for a relative risk in the surgery group of 0.60 (95 percent confidence interval, 0.42 to 0.86; P=0.004 by Gray's test), and for local progression, the increase was from 19.1 to 25.1 percentage points, for a relative risk of 0.33 (95 percent confidence interval, 0.25 to 0.44; P<0.001 by Gray's test).\n\n【7】Conclusions\n-----------\n\n【8】Radical prostatectomy reduces disease-specific mortality, overall mortality, and the risks of metastasis and local progression. The absolute reduction in the risk of death after 10 years is small, but the reductions in the risks of metastasis and local tumor progression are substantial.\n\n【9】Introduction\n------------\n\n【10】Radical prostatectomy is becoming one of the most common major surgical procedures in many Western countries. In the United States alone, an estimated 60,000 men undergo this operation each year. However, to our knowledge, only one randomized trial quantified the benefit of radical prostatectomy. \n\n【11】In 2002, we presented the results of a clinical trial in which radical prostatectomy was compared with watchful waiting in the management of early prostate cancer.  Our analysis was based on a mean follow-up time of 6.2 years, a relatively short period in relation to the often long natural history of early prostate cancer.  We found that, as compared with watchful waiting, radical prostatectomy reduced the risk of death due to prostate cancer by 50 percent and the risk of distant metastasis by 37 percent, but there was no statistically significant reduction in overall mortality.\n\n【12】We now present a second analysis after an additional three years of follow-up, in accordance with the study protocol. Our main purpose was to analyze two hypotheses: first, that the relative reduction in the risk of death due to prostate cancer after surgery increases over time because removal of the primary tumor prevents metastasis and, second, that radical prostatectomy significantly improves overall survival.\n\n【13】Methods\n-------\n\n【14】Study Design\n------------\n\n【15】Details concerning study design and methods have been published previously.\n\n【16】Table 1. Baseline Characteristics of the 695 Men Enrolled in the Study.\n\n【17】From 1989 to 1999, 695 men from 14 centers in Sweden, Finland, and Iceland were enrolled. The eligibility criteria included an age under 75 years; the presence of newly diagnosed, untreated, localized prostate cancer, as verified by cytologic or histologic examination, with a tumor stage of T0d (later changed to T1b), T1, or T2 (T1c was included in 1994)  ; a health status that would permit radical prostatectomy; and a life expectancy of more than 10 years. The tumor had to be well differentiated to moderately well differentiated, according to the definition established by the World Health Organization.  Patients had to have a bone scan that showed no abnormalities and a prostate-specific antigen (PSA) level of less than 50 ng per milliliter. If the diagnosis had been established after transurethral resection, at least six blocks of prostate tissue should have been examined. After oral informed consent was received from eligible patients, they were randomly assigned to undergo either radical prostatectomy or watchful waiting through a telephone service outside the clinics. Stratification was made according to tumor grade and randomization center.\n\n【18】For men assigned to the radical-prostatectomy group, surgery started with dissection of the pelvic lymph nodes.  If there were no signs of metastasis in frozen sections, the operation was continued with retropubic radical prostatectomy.  The men in the watchful-waiting group received no initial treatment other than the transurethral resection some of them had already undergone.\n\n【19】Hormonal treatment was recommended for men with symptomatic local progression in the radical-prostatectomy group and for those with disseminated disease in both groups. Transurethral resection was recommended as the initial treatment for men with urinary obstruction in the watchful-waiting group. In January 2003, an amendment to the protocol allowed men in both groups to begin hormonal therapy if their physicians advised it.\n\n【20】Follow-up and Definition of Clinical Events\n-------------------------------------------\n\n【21】The participants were seen every six months during the first two years and then annually for a clinical examination and blood tests (to evaluate hemoglobin, PSA, alkaline phosphatase, and creatinine levels). A bone scan and a chest radiograph were obtained annually until 1997; thereafter, chest radiographs were obtained only once a year for the first two years after randomization. The clinical follow-up continued for all patients except nine, who underwent blood tests (including tests for PSA levels) and bone scanning when possible but who did not have clinical visits owing to old age and coexisting illnesses. Beginning in 2003, bone scans were allowed every second year if the patient had no biochemical or clinical signs of progression. In 2001, a pathological review of cytologic and histologic data that were available at inclusion was carried out. For the purpose of this analysis, all patient records were retrieved and individually reviewed for new events.\n\n【22】An independent end-point committee determined the cause of death on the basis of standardized extractions from the patient files; for this determination, the treatment group was not revealed. The committee used six categories of cause of death: prostate cancer; another main cause but with distant metastases, regardless of local status; another main cause but with local progression, without distant metastases; another main cause, but with local progression and unknown status concerning distant metastases; another main cause, with no evidence of tumor recurrence, tumor progression, or metastases; and another main cause within the first month after randomization.\n\n【23】In the radical-prostatectomy group, local progression was defined as the presence of a histologically confirmed local tumor. In the watchful-waiting group, men with palpable transcapsular tumor growth or with symptoms of urinary obstruction that necessitated intervention were classified as having local progression. Distant metastases were considered present when bone scans, skeletal radiographs, computed tomographic scans, or chest radiographs revealed metastases or if lymph nodes at sites other than the regional sites showed cytologic or histologic evidence of prostate cancer.\n\n【24】Statistical Analysis\n--------------------\n\n【25】There were four main end points: disease-specific death, with death due to prostate cancer (the first cause-of-death category) considered the event and death from other causes treated as a competing risk; distant metastasis, with its diagnosis considered the event and death from other causes treated as a competing risk; local progression, with death treated as a competing risk; and death from any cause.\n\n【26】All analyses were carried out in accordance with the intention-to-treat principle. Relative risks (with 95 percent confidence intervals) and differences in cumulative incidence (with 95 percent confidence intervals) were used as measures of effect for each end point. Gray's test  was used to test the hypothesis that there was no difference between the treatment groups; a P value of less than 0.05 (two-sided) was considered to indicate statistical significance. The relative risks were estimated from the Cox proportional-hazards model. Cumulative incidence (calculated in terms of integrated subdensity) rather than cumulative hazard (integrated subhazard) was used in the acknowledgment that the end points constitute competing events.  The results presented here and in the previous report  involved low absolute risks for disease-specific death and death from any cause, with no sensitivity to whether cumulative incidence rates or cumulative hazard rates were used.\n\n【27】Effect modification was first investigated through simple stratified analyses. For all end points, three prespecified subgroup analyses were carried out: analysis according to age at diagnosis — less than 65 years of age as compared with 65 years of age or older; analysis according to PSA level at diagnosis — 10 ng per milliliter or lower as compared with more than 10 ng per milliliter; and analysis according to the Gleason score of the prerandomization biopsy specimen — lower than 7 as compared with 7 or more (on a scale of 2 to 10, with 10 indicating the most poorly differentiated tumors). Any modification of the effect of radical prostatectomy according to subgroup was tested by a Cox proportional-hazards model, which included an interaction term between subgroup category and randomization group. In a second step, we further explored the interaction by including the possible effect modifier (age, PSA level at diagnosis, or Gleason score) as a continuous variable. When there was an indication of effect modification, we further controlled for the PSA level at diagnosis, the tumor stage, the Gleason score, and the year at inclusion by adding these as additional covariates in the Cox proportional-hazards model.\n\n【28】Results\n-------\n\n【29】We randomly assigned 347 men to radical prostatectomy and 348 to watchful waiting. Relevant characteristics at the time of inclusion were similar in the two groups. Most patients (76 percent) had stage T2 tumors (i.e., the tumor was confined within the prostate), and in only 12 percent were T1c (nonpalpable) tumors detected by means of PSA testing . At the end of 2003, 21 men assigned to radical prostatectomy had not undergone surgery, and 43 assigned to watchful waiting had undergone curative treatment. Lymph-node metastases, which precluded surgery, were found in frozen sections from 23 men in the radical-prostatectomy group.\n\n【30】Table 2. Causes of Death, According to the Final Consensus of the End-Point Committee.\n\n【31】During follow-up, fewer men in the radical-prostatectomy group than in the watchful-waiting group died of prostate cancer (30 vs. 50, P=0.01). As for causes of death other than prostate cancer, the numbers were similar in the two groups (53 and 56, respectively). However, among men who died from causes other than prostate cancer, a larger number in the watchful-waiting group had metastases or local progression. In terms of death from any cause, 23 more men in the watchful-waiting group than in the radical-prostatectomy group died (106 vs. 83, P=0.04) .\n\n【32】Disease-Specific Mortality\n--------------------------\n\n【33】Figure 1. Cumulative Incidence of Death from Prostate Cancer in the Two Study Groups Overall  and According to Age . Table 3.  Table 3. Cumulative Incidence of the Main End Points and Corresponding Relative Risks.\n\n【34】The difference between the two groups in the cumulative incidence of death from prostate cancer increased over time, from 2 percentage points (95 percent confidence interval, –0.6 to 4.7) after five years of follow-up to 5.3 percentage points (95 percent confidence interval, –0.3 to 11.0) after 10 years, in favor of radical prostatectomy. The relative risk among men assigned to radical prostatectomy, as compared with those assigned to watchful waiting, was 0.56 (95 percent confidence interval, 0.36 to 0.88) .\n\n【35】Distant Metastases\n------------------\n\n【36】Figure 2. Cumulative Incidence of Distant Metastasis  and of Death from Any Cause .\n\n【37】The cumulative incidence of distant metastases was similar in the two groups during the first five years (8.1 percent in the radical-prostatectomy group and 9.8 percent in the watchful-waiting group, P=0.42). However, a difference emerged after that time: at the second follow-up, 50 of the 347 men in the radical-prostatectomy group had distant metastases, as compared with 79 of the 348 men in the watchful-waiting group. In the radical-prostatectomy group, the absolute risk reduction at 10 years was 10.2 percentage points (95 percent confidence interval, 3.1 to 17.2), corresponding to a relative risk of 0.60 (95 percent confidence interval, 0.42 to 0.86) .\n\n【38】Local Progression\n-----------------\n\n【39】The difference between the two groups in the cumulative incidence of local progression was statistically significant after five years of follow-up (8.1 percent in the radical-prostatectomy group vs. 27.2 percent in the watchful-waiting group, P<0.01); the difference increased over time, to 64 men with local progression among the 347 in the prostatectomy group, as compared with 149 men among the 348 in the watchful-waiting group, at the second follow-up. The difference in the absolute risk reduction after 10 years was 25.1 percentage points (19.2 percent vs. 44.3 percent), corresponding to a relative risk in the radical-prostatectomy group of 0.33 (95 percent confidence interval, 0.25 to 0.44) .\n\n【40】Overall Mortality\n-----------------\n\n【41】The cumulative incidence of death from any cause was similar in the two groups during the first five years (7.8 percent for radical prostatectomy vs. 9.8 percent for watchful waiting). At the last follow-up, 83 of 347 men in the radical-prostatectomy group and 106 of 348 in the watchful-waiting group had died. After randomization to radical prostatectomy, the absolute reduction in the risk of death from any cause after 10 years was 5.0 percentage points, corresponding to a relative risk of 0.74 (95 percent confidence interval, 0.56 to 0.99; P=0.04 by Gray's test) .\n\n【42】Other Treatments\n----------------\n\n【43】Hormonal treatment was administered less often in the radical-prostatectomy group than in the watchful-waiting group (110 of 347 patients vs. 177 of 348, P<0.01). The mean time to hormonal treatment was 4.5 years in the radical-prostatectomy group and 4.8 years in the watchful-waiting group. Palliative radiation was also administered less often in the radical-prostatectomy group than in the watchful-waiting group (29 patients vs. 38 patients, P=0.30), as was laminectomy (4 patients vs.11 patients, P=0.04).\n\n【44】Subgroup Analyses\n-----------------\n\n【45】In planned, simple stratified analyses, we found that the benefit of radical prostatectomy in terms of disease-specific mortality differed according to age group but not according to the PSA level at diagnosis or the Gleason score. A further investigation of disease-specific mortality with the use of a Cox proportional-hazards model that included the randomization group, the patient's age as a continuous variable, and an interaction term showed that the interaction term was statistically significant (P=0.03). When the same model was augmented with the PSA level at diagnosis, the tumor stage, the Gleason score, and the year at inclusion, the P value for the interaction term shifted to 0.08. For overall mortality, the P value for the interaction term in the corresponding two analyses shifted only marginally and remained less than 0.01. The cumulative incidence of death from prostate cancer in men under 65 years of age in the watchful-waiting group was 19.2 percent at 10 years. This was markedly higher than the cumulative incidence of death in the other subgroups defined according to randomization group and age, for which the incidence varied from 8.5 percent to 11.5 percent .\n\n【46】Discussion\n----------\n\n【47】In this comparison of radical prostatectomy with watchful waiting for patients with prostate cancer, the 10-year absolute differences in disease-specific and overall mortality were statistically significant, by 5.3 (P=0.01) and 5.0 (P=0.04) percentage points, respectively, in favor of radical prostatectomy. In addition, the cumulative incidence of distant metastasis was 10.2 percentage points lower in the surgery group than in the watchful-waiting group. Because clinical manifestations of disseminated disease virtually always precede death,  this finding might herald a further lowering of the risk of death due to prostate cancer in the radical-prostatectomy group after a longer period of follow-up. We found no evidence that the benefit of radical prostatectomy was exaggerated by more frequent administration of hormonal treatment, since hormonal therapy was given less often in the radical-prostatectomy group than in the watchful-waiting group (110 patients vs. 177 patients).\n\n【48】We found that the reduction in disease-specific mortality as a result of radical prostatectomy was greatest among, or even limited to, patients younger than 65 years. The multivariate analyses indicated that this finding was attributable, only to a limited extent, to differences between younger and older men in the distribution of PSA levels or Gleason scores; however, there may have been other differences in characteristics between younger and older men at the time of inclusion. These results have limited interpretability for two additional reasons: they were based on small numbers, since the study was not powered to analyze subgroups, and the analysis was exploratory rather than based on any a priori biologic hypothesis. Therefore, the results of the subgroup analyses should be an incentive to conduct further research rather than to introduce an immediate change in clinical practice.\n\n【49】Observational cohort studies have analyzed survival rates among patients whose cancer was managed by watchful waiting. After 10 years of follow-up, such studies yielded disease-specific survival rates of 87 percent,  86 percent,  and 83 percent.  In our trial, the corresponding 10-year figure was 85 percent. The similar prognosis among patients randomly assigned to watchful waiting in our trial and those analyzed in observational studies indicates that our findings are generalizable to patients in similar settings. If watchful waiting with curative intervention in patients with rising PSA levels (as detected with active monitoring) yields better survival than does traditional watchful waiting, the difference between watchful waiting and primary surgery should diminish. As yet, however, there is no evidence from a randomized trial that the monitoring of PSA levels with accompanying curative intervention will yield better results than will watchful waiting as used in this trial and in the observational studies.\n\n【50】In this follow-up period, we found a substantial absolute difference between the two groups in terms of local progression (which can cause problems with the micturition, pain, and anxiety). Moreover, the need for hormonal treatment increased in frequency in the watchful-waiting group, as did the need for palliative radiation; both types of treatment were associated with side effects that influenced patients' quality of life and well-being. Thus, the more immediate, though stable, side effects associated with surgery  (predominantly, impotence and incontinence) and reported previously for this study,  should be weighed against the increasing incidence of symptoms and use of treatments after the progression of disease in the watchful-waiting group. However, for several reasons, a reevaluation of the costs and benefits of radical prostatectomy in the era of widespread screening is necessary: the number of patients needed to treat may be high, and the lead time to the onset of symptoms and treatment may be long in those undergoing monitoring, but the removal of small tumors may facilitate surgery and result in fewer side effects.\n\n【51】Our 10-year estimates show that radical prostatectomy is associated with a statistically significant reduction in all the end points that we investigated, with a relative reduction of 44 percent in mortality due to prostate cancer, of 26 percent in overall mortality, of 40 percent in the risk of distant metastasis, and of 67 percent in local progression. Since, in absolute terms, the reduction in mortality is moderate, clinical decision making and patient counseling will remain difficult. The additional finding that radical prostatectomy substantially reduces the risk of metastasis and symptomatic local tumor growth may, however, be of some help in guiding therapy, and we expect that the benefits of this surgery will increase during longer periods of follow-up.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4ea68d7d-b869-4908-b07e-0d1cd48dc5b2", "title": "Meningococcal Serogroup ACWYX Conjugate Vaccine in Malian Toddlers", "text": "【0】Meningococcal Serogroup ACWYX Conjugate Vaccine in Malian Toddlers\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】_Neisseria meningitidis_ serogroups A, B, C, W, X, and Y cause outbreaks of meningococcal disease. Quadrivalent conjugate vaccines targeting the A, C, W, and Y serogroups are available. A pentavalent vaccine that also includes serogroup X (NmCV-5) is under development.\n\n【3】Methods\n-------\n\n【4】We conducted a phase 2, observer-blinded, randomized, controlled trial involving Malian children 12 to 16 months of age. Participants were assigned in a :1 ratio to receive nonadjuvanted NmCV-5, alum-adjuvanted NmCV-5, or the quadrivalent vaccine MenACWY-D, administered intramuscularly in two doses 12 weeks apart. Participants were followed for safety for 169 days. Immunogenicity was assessed with an assay for serum bactericidal antibody (SBA) with rabbit complement on days 0, 28, 84, and 112.\n\n【5】Results\n-------\n\n【6】A total of 376 participants underwent randomization, with 150 assigned to each NmCV-5 group and 76 to the MenACWY-D group; 362 participants received both doses of vaccine. A total of 1% of the participants in the nonadjuvanted NmCV-5 group, 1% of those in the adjuvanted NmCV-5 group, and 4% of those in the MenACWY-D group reported local solicited adverse events; 6%, 5%, and 7% of the participants, respectively, reported systemic solicited adverse events. An SBA titer of at least 128 was seen in 91 to 100% (for all five serotypes) of the participants in the NmCV-5 groups and in 36 to 99% (excluding serogroup X) of those in the MenACWY-D group at day 84 (before the second dose); the same threshold was met in 99 to 100% (for all five serotypes) of the participants in the NmCV-5 groups and in 92 to 100% (excluding serogroup X) of those in the MenACWY-D group at day 112. Immune responses to the nonadjuvanted and adjuvanted NmCV-5 formulations were similar.\n\n【7】Conclusions\n-----------\n\n【8】No safety concerns were identified with two doses of NmCV-5. A single dose of NmCV-5 elicited immune responses that were similar to those observed with two doses of MenACWY-D. Adjuvanted NmCV-5 provided no discernible benefit over nonadjuvanted NmCV-5. \n\n【9】Introduction\n------------\n\n【10】_Neisseria meningitidis_ is known to cause outbreaks of meningitis in many parts of the world. The sub-Saharan African meningitis belt, which stretches from Senegal to Ethiopia, has the highest annual incidence of invasive meningococcal disease, with outbreaks occurring on an annual basis and with recurrent large epidemics every 7 to 14 years.  Among the several serogroups that have been classified on the basis of their polysaccharide capsule structure, six (A, B, C, W, X, and Y) are known to cause most cases of invasive disease. The serogroup distribution varies regionally, with serotypes A, C, and W being the most common in sub-Saharan Africa until recently. A proportional increase in the incidence of serogroup X disease has been reported, with an increase from 3% of 4150 confirmed meningitis cases during the period from 2013 through 2016 to 22% of 1410 cases in 2017, mainly in Burkina Faso, Niger, and Togo. \n\n【11】The two serious manifestations of invasive meningococcal disease are meningitis and septicemia, which can be fatal in 50% of cases if untreated. Even when meningococcal disease is treated, mortality is approximately 10%. Among survivors, 10 to 20% have sequelae such as severe permanent brain damage, mental retardation, deafness, epilepsy, and other neurologic disorders. \n\n【12】To tackle the problem of large epidemics of serogroup A disease in the meningitis belt, MenAfriVac, a meningococcal A conjugate vaccine, was developed in a partnership involving the Serum Institute of India, the World Health Organization, and PATH, with funding from the Bill and Melinda Gates Foundation.  MenAfriVac has been used extensively in mass immunization campaigns and has virtually eliminated serogroup A disease from the meningitis belt  ; however, other serogroups, such as C, W, and X, continue to be reported. \n\n【13】Currently, three A, C, W, and Y meningococcal conjugate vaccines are commercially available (Menactra, Menveo, and Nimenrix)  ; however, no licensed vaccine includes serogroup X. Therefore, a pentavalent meningococcal conjugate vaccine incorporating the A, C, W, Y, and X serogroups (NmCV-5) was developed by the Serum Institute of India in partnership with PATH. In a phase 1 clinical study involving healthy adults in the United States, two formulations of NmCV-5 (with and without adjuvant) showed an acceptable safety profile and encouraging immunogenicity.  Given these results, we conducted a phase 2 trial involving Malian toddlers to assess the safety and immunogenicity of this vaccine in this age group, to determine the need for an adjuvant in the formulation, and to determine whether one or two doses would be needed to elicit immune responses.\n\n【14】Methods\n-------\n\n【15】Trial Design and Oversight\n--------------------------\n\n【16】We conducted a phase 2 randomized, controlled, observer-blinded, single-center trial involving healthy Malian children 12 to 16 months of age. The trial was designated to be observer-blinded, which meant that the vaccine preparation and administration in masked syringes were handled by site staff who were aware of the trial group assignments but did not participate in any of the clinical trial evaluations. The parents or legal guardians of the participants, the site staff involved in the evaluation of trial end points, and the laboratory staff who performed the immunogenicity assays were unaware of which trial vaccine was administered to each participant.\n\n【17】The trial was approved by the research ethics committee of the Faculté de Médecine, de Pharmacie et d’Odonto-Stomatologie, Bamako, Mali, and by the institutional review board of the University of Maryland, Baltimore. The trial was conducted in accordance with the Good Clinical Practice guidelines of the International Council for Harmonisation and the principles of the Declaration of Helsinki. Full written informed consent was provided by each participant’s parent or legal guardian before enrollment. An independent data and safety monitoring board was constituted to assess the safety of the vaccines during the trial.\n\n【18】Trial Population\n----------------\n\n【19】Children 12 to 16 months of age were recruited from the urban Bamako area. Children were excluded from the trial if they had a history of any meningococcal vaccination or disease or had had any intimate contact with a person with meningococcal infection in the previous 60 days. Other major exclusion criteria were a history of hypersensitivity to any of the vaccine components, any clinically significant disorders, immune deficiency, malnutrition, known hepatitis B or C, human immunodeficiency virus infection, or malaria at the time of vaccination.\n\n【20】Trial Vaccines\n--------------\n\n【21】NmCV-5 is a lyophilized powder containing meningococcal serogroup A and X polysaccharides conjugated to tetanus toxoid and meningococcal serogroup C, W, and Y polysaccharides conjugated to recombinant CRM197 (cross-reactive material 197, a nontoxic mutant of diphtheria toxin) protein. The vaccine was reconstituted with either normal saline (nonadjuvanted formulation) or with normal saline containing aluminum phosphate (adjuvanted formulation) just before administration. After reconstitution, each 0.5-ml dose of NmCV-5 contained 5 μg each of the meningococcal serogroup A, C, W, Y, and X polysaccharides. The licensed MenACWY-D conjugate vaccine (Menactra, Sanofi Pasteur) was supplied as a single 0.5-ml dose that contained 4 μg each of the meningococcal serogroup A, C, W, and Y polysaccharides conjugated to diphtheria toxoid.\n\n【22】Objectives and Procedures\n-------------------------\n\n【23】The primary objective was to assess severe reactogenicity (i.e., grade 3 local and systemic solicited adverse events) of the nonadjuvanted and adjuvanted formulations of NmCV-5 as compared with MenACWY-D. The secondary objectives were to assess safety further (all other grades of solicited adverse events, unsolicited adverse events, adverse events that led to withdrawal from the trial, and serious adverse events) and to assess the immunogenicity of the two NmCV-5 formulations as compared with MenACWY-D.\n\n【24】The participants visited the trial clinic on day 0 (baseline) and on days 7, 28, 84, 91, 112, and 168. On day 0, eligible participants were randomly assigned, in a :1 ratio, to receive two doses of nonadjuvanted NmCV-5, adjuvanted NmCV-5, or MenACWY-D. Two doses of each vaccine were to be administered intramuscularly, with the first injection on day 0 and the second on day 84.\n\n【25】Local and systemic solicited adverse events occurring within 7 days after each vaccination were assessed daily by site staff and recorded in the home-visit worksheets. Data on unsolicited adverse events with an onset within 28 days after each vaccination were collected weekly by site staff during home and site visits. Data on serious adverse events and adverse events that led to withdrawal from the trial were collected during the entire trial period.\n\n【26】Solicited local adverse events included tenderness and swelling or induration at the injection site. Solicited systemic adverse events included irritability, drowsiness, decreased eating, vomiting, and fever. Secondary safety end points were mild and moderate grades of solicited adverse events with an onset during the 7 days after each vaccination, unsolicited adverse events with an onset during the 28 days after each vaccination, and serious adverse events and adverse events leading to premature withdrawal from the trial. The severity of all adverse events was graded according to the Division of AIDS Table for Grading the Severity of Adult and Pediatric Adverse Events, version 2.1.\n\n【27】For the assessment of secondary immunogenicity end points, approximately 5 ml of blood was obtained from the participant just before each vaccination (on days 0 and 84) and at 28 days after each vaccination (on days 28 and 112). The serum samples were tested with an assay for the serum bactericidal antibody (SBA) with baby rabbit complement at the central Public Health England laboratory (Manchester, United Kingdom) to measure functional antibodies against meningococcal serogroups A, C, W, Y, and X. \n\n【28】Statistical Analysis\n--------------------\n\n【29】We calculated that the enrollment of 375 participants (150 participants in each NmCV-5 group and 75 participants in the MenACWY-D group) would provide the trial with 80% power to detect a difference of 15.4 percentage points in the incidence of severe solicited adverse events between each NmCV-5 group and the MenACWY-D group, with the use of Fisher’s exact test at a significance level of 0.05, under the assumption that the incidence of severe reactions would be 5% in the MenACWY-D group and that 10% of the participants would withdraw from the trial. The percentage of participants who had at least one severe solicited adverse event within 7 days after any trial vaccination (in the periods of days 0 to 6 and days 84 to 90) was the primary safety end point to be compared between each NmCV-5 group and the MenACWY-D control group in a descriptive manner. The safety population included all the participants who were enrolled, received at least one dose of trial vaccine, and had any safety data available. The between-group differences in the percentages were provided, along with two-sided 95% confidence intervals obtained by the Miettinen and Nurminen method  ; similar analyses were conducted for all applicable secondary end points.\n\n【30】The comparisons of immunogenicity between the NmCV-5 groups and the MenACWY-D group and between the nonadjuvanted NmCV-5 group and the adjuvanted NmCV-5 group were performed with the use of a noninferiority method. Noninferiority was assessed by examining whether the lower limit of the 95% confidence interval for the between-group difference in percentages was above −10 percentage points (a commonly accepted noninferiority margin) for each serogroup (A, C, W, Y, and X). The immunogenicity analysis was performed in the per-protocol population, which included all the participants who correctly received their assigned trial vaccine with no major protocol deviations that were determined to potentially interfere with the immunogenicity assessment and who provided, at least at one time point after vaccination, a serum sample that could be evaluated. A supportive analysis of immunogenicity was also conducted in the full analysis population, which included all the participants who underwent randomization, received at least one dose of trial vaccine, and provided, at least at one time point after vaccination, a serum sample that could be evaluated; this analysis was conducted according to randomly assigned trial group.\n\n【31】The immunogenicity analyses in each group included the geometric mean titer (GMT) of SBA for each meningococcal serogroup (A, C, W, Y, and X) at day 0 (prevaccination baseline value) and at days 28, 84, and 112. The percentages of participants with SBA titers of at least 128 at each of these trial days were calculated, as were the percentages of participants who had an increase of at least 4 times the baseline SBA titers (i.e., among participants with a prevaccination SBA titer of <8, a postvaccination titer of ≥32; among participants with a prevaccination SBA titer of ≥8, a postvaccination titer of ≥4 times the prevaccination titer) that were obtained at days 28 and 112 (i.e., 4 weeks after each vaccination).\n\n【32】The exact two-sided 95% confidence intervals for the percentages were calculated by means of the Clopper–Pearson method for all applicable end points. The GMTs were provided, with two-sided 95% confidence intervals, by exponentiation of the corresponding means of log <sub>2 </sub> \\-transformed SBA titers and their 95% confidence intervals. Missing values were treated as missing at random. A post hoc sensitivity analysis was performed to examine the effect of missing data. The last-observation-carried-forward and worst-case methods were used to impute missing data for the primary end point. For immunogenicity end points, the mean value and worst-case methods were used for imputation. All the statistical analyses were performed with the use of SAS software, version 9.3 (SAS Institute).\n\n【33】Results\n-------\n\n【34】Trial Population\n----------------\n\n【35】The trial was conducted from November 2017 through August 2018. Among the 379 children who underwent screening, 2 had a screening failure and 1 had early withdrawal of informed consent. Therefore, 376 participants underwent randomization; 150 children were assigned to each of the NmCV-5 groups and 76 to the MenACWY-D group. Informed consent was withdrawn before the receipt of the first dose for 1 participant who had been assigned to the nonadjuvanted NmCV-5 group. The remaining 375 participants received the first dose of trial vaccine, 362 received the second dose, and 360 completed the trial. Among the participants who received any dose of vaccine, 15 discontinued or withdrew from the trial; 3 discontinuations were due to death, 10 due to protocol deviations, 1 due to withdrawal of informed consent, and 1 due to travel. At the end of the trial, there were 375 participants in the safety population and 360 in the per-protocol population .\n\n【36】All the participants were Black African, and there was a slight preponderance of male participants in each trial group (range, 51 to 55%). Overall, the mean age of the participants was 12.1 months, and the median age was 12.0 months. The distributions of length (mean \\[±SD\\], 72.9±2.7 cm) and weight (mean, 8.8±1.1 kg) were similar across all three groups.\n\n【37】Safety Results\n--------------\n\n【38】No immediate adverse events were reported within 30 minutes after vaccination. None of the participants had any severe solicited adverse events. After the first vaccine dose, 1 of 149 participants (1%) in the nonadjuvanted NmCV-5 group, 2 of 150 (1%) in the adjuvanted NmCV-5 group, and 3 of 76 (4%) in the MenACWY-D group had at least one local solicited adverse event. No local solicited adverse events were reported after the second dose.\n\n【39】Overall, 6% of the participants in the nonadjuvanted NmCV-5 group, 5% of those in the adjuvanted NmCV-5 group, and 7% of those in the MenACWY-D group reported systemic solicited adverse events. Systemic solicited adverse events were reported after the first dose in 8 of 149 participants (5%) in the nonadjuvanted NmCV-5 group, in 6 of 150 (4%) in the adjuvanted NmCV-5 group, and in 3 of 76 (4%) in the MenACWY-D group; systemic solicited adverse events after the second dose were reported in 1 of 144 participants (1%), in 2 of 145 (1%), and in 2 of 73 (3%), respectively. Only fever and vomiting were reported as solicited adverse events after the second dose. Other than two events of fever of moderate severity (one each in the nonadjuvanted NmCV-5 group and the adjuvanted NmCV-5 group), all the solicited adverse events were mild in severity .\n\n【40】During the 28 days after receipt of the first dose, 37 of 149 participants (25%) in the nonadjuvanted NmCV-5 group, 39 of 150 (26%) in the adjuvanted NmCV-5 group, and 23 of 76 (30%) in the MenACWY-D group reported 48, 46, and 28 unsolicited adverse events, respectively. None of these 122 adverse events, other than 2 events of diarrhea in participants in the nonadjuvanted NmCV-5 group, were assessed by the investigator as being related to a trial vaccine.\n\n【41】During the 28 days after receipt of the second dose, 23 of 144 participants (16%) in the nonadjuvanted NmCV-5 group, 17 of 145 (12%) in the adjuvanted NmCV-5 group, and 16 of 73 (22%) in the MenACWY-D group reported 24, 17, and 17 unsolicited adverse events, respectively. None of these 58 adverse events were assessed by the investigator as being related to a trial vaccine.\n\n【42】Almost all the unsolicited adverse events that were reported during the trial were respiratory tract infections, gastroenteritis, or diarrhea. All the unsolicited adverse events were mild or moderate in severity, and all resolved without any sequelae .\n\n【43】Three serious adverse events, one in each group, occurred during the trial, and all led to death. These events were pneumonia (in the nonadjuvanted NmCV-5 group), _Escherichia coli_ sepsis (in the adjuvanted NmCV-5 group), and second-degree burns (in the MenACWY-D group). All three serious adverse events were assessed by the investigator as being unrelated to a trial vaccine .\n\n【44】Immunogenicity Results\n----------------------\n\n【45】Figure 1. Geometric Mean Titers of Serum Bactericidal Antibody, According to Meningococcal Serogroup (Per-Protocol Population).\n\n【46】Shown are the geometric mean titers and 95% confidence intervals (𝙸 bars) of the serum bactericidal antibody with baby rabbit complement that were observed in participants who received nonadjuvanted NmCV-5, adjuvanted NmCV-5, or MenACWY-D at baseline (day 0), at 28 days and 84 days after receipt of the first dose (days 28 and 84), and at 28 days after receipt of the second dose (day 112). The per-protocol population included all the participants who correctly received their assigned trial vaccine with no major protocol deviations that were determined to potentially interfere with the immunogenicity assessment of the trial vaccines and who provided, at least at one time point after vaccination, a serum sample that could be evaluated.\n\n【47】The SBA GMTs for each meningococcal serogroup in the per-protocol population are presented in Figure 1 and Table S4. The GMTs at baseline (day 0) were low (≤4.4) across all three trial groups. At all subsequent time points and for all serogroups, the GMTs were higher in both the nonadjuvanted NmCV-5 group and the adjuvanted NmCV-5 group than in the MenACWY-D group.\n\n【48】At 84 days after receipt of the first dose, the GMTs were proportionally reduced from those at 28 days after receipt of the first dose for all serogroups in both the NmCV-5 groups and the MenACWY-D group. GMTs were higher at 28 days after receipt of the first dose of either formulation of NmCV-5 than they were 28 days after receipt of the second dose of MenACWY-D, despite an increase in the GMTs after the second dose of MenACWY-D.\n\n【49】Table 1. Participants with Serum Bactericidal Antibody Titers of at Least 128 at Days 0, 28, 84, and 112 (Per-Protocol Population).\n\n【50】At baseline, for all five serogroups, 15% or less of the participants in each of the three groups had an SBA titer of at least 128 . At 28 days after receipt of the first dose, 97% or more of the participants (lowest lower boundary of the 95% confidence interval \\[CI\\], 93.0) in the two NmCV-5 groups had SBA titers of at least 128 against all five serogroups, whereas in the MenACWY-D group, SBA titers of at least 128 in at least 90% of participants were limited to serogroups A and W. The response in the MenACWY-D group against serogroup C was 54% (95% CI, 42 to 66), whereas it was 99% (95% CI, 95 to >99) in the nonadjuvanted NmCV-5 group.\n\n【51】Just before the receipt of the second dose (at day 84), at least 91% of the participants in the two NmCV-5 groups still had SBA titers of at least 128 against all five serogroups. In the MenACWY-D group, this threshold (≥128) was maintained in more than 90% of participants only for serogroup A; for serogroup C, 36% of the participants (95% CI, 25 to 48) had an SBA titer of at least 128.\n\n【52】At 28 days after receipt of the second dose (day 112), at least 99% of the participants in the two NmCV-5 groups had an SBA titer of at least 128 against all five serogroups. In the MenACWY-D group, this threshold was met in at least 91% of the participants for serogroups A, C, W, and Y.\n\n【53】Table 2. Participants with an Increase in Serum Bactericidal Antibody Titers of at Least 4 Times the Baseline Value on Days 28 and 112 (Per-Protocol Population).\n\n【54】At 28 days after receipt of the first vaccine dose, 97% or more of the participants (lowest lower boundary of the 95% CI, 93) in each NmCV-5 group had a minimum increase of 4 times the baseline SBA titers against all five serogroups, whereas in the MenACWY-D group a minimum increase of 4 times the baseline SBA titers was observed in at least 90% of the participants only for serogroups A and W and was observed in 69% of the participants (95% CI, 58 to 80) for serogroup C . At the day 112 visit, which was scheduled to be 28 days after receipt of the second vaccine dose, more than 98% of the participants in all three groups had a minimum increase of 4 times the baseline SBA titers against all the serogroups included in the respective vaccines.\n\n【55】There were no notable differences between the nonadjuvanted NmCV-5 group and the adjuvanted NmCV-5 group for any serogroup and at any time point with regard to the percentage of participants with SBA titers of at least 128  or the percentage of participants with a minimum increase of 4 times the baseline SBA titers  after each trial vaccination. For the percentage of participants with SBA titers of at least 128 for all five serogroups on days 28, 84, and 112, the lowest lower boundary of the 95% confidence intervals for the between-group differences (between each of the NmCV-5 groups and the MenACWY-D group) was −3 percentage points. For the percentage of participants who had an increase of at least 4 times the baseline titer for all five serogroups on days 28 and 112, the lowest lower boundary of the 95% confidence intervals for the between-group differences was −4 percentage points. The immunogenicity results in the full analysis population (369 participants) did not differ substantially from those in the per-protocol population (360 participants). The primary safety end-point results and immunogenicity results from the post hoc sensitivity analysis were similar to those observed in the planned analysis.\n\n【56】Discussion\n----------\n\n【57】This phase 2 trial involving Malian toddlers was conducted to assess the safety and immunogenicity of nonadjuvanted and adjuvanted formulations of NmCV-5 as compared with MenACWY-D. The local reactogenicity profiles of the two NmCV-5 formulations were similar to that of MenACWY-D, and no safety concerns were identified.\n\n【58】The currently licensed polyvalent meningococcal conjugate vaccines for children younger than 2 years of age (Menactra, Menveo, and Nimenrix) do not contain any adjuvant.  This trial showed that NmCV-5 also does not need an adjuvant, because both formulations elicited similar immune responses to all serogroups at all time points.\n\n【59】MenACWY-D is recommended at a two-dose schedule in children 9 through 23 months of age,  and the findings of this trial support this schedule. In contrast, a single dose of NmCV-5 elicited immune responses that were similar to or higher than those observed with the two-dose schedule of MenACWY-D, which indicates that a single dose of NmCV-5 may protect children who receive the vaccine at 12 through 16 months of age. For this age group, Menveo, which is conjugated to CRM197, is recommended at a two-dose schedule and Nimenrix, which is conjugated to tetanus toxoid, at a one-dose schedule.  Menactra is conjugated to diphtheria toxoid. The single-dose schedule of NmCV-5 is an important consideration in the sub-Saharan African meningitis belt, given considerations of cost and the issues involved in having people return for a second vaccine dose.\n\n【60】NmCV-5 was immunogenic for serogroup X, which is an important observation because there is no currently licensed vaccine against serogroup X. In the past two decades, the incidence of serogroup X disease has increased in the meningitis belt, so an X-containing vaccine has a potential benefit over existing meningococcal conjugate vaccines.  The MenACWY-D group also had small increases in responses against serogroup X. Possible explanations include subclinical infections with serogroup X or cross-reactive antibodies.\n\n【61】At 3 months after the first dose (day 84), the percentage of participants in the two NmCV-5 groups who had SBA titers of at least 128 was at least 91% for all five serogroups. By comparison, the same threshold was maintained in 36% of the participants in the MenACWY-D group for serogroup C. The SBA GMTs also remained high 3 months after receipt of the first dose of NmCV-5.\n\n【62】For the percentage of participants with SBA titers of at least 128 for all five serogroups on days 28, 84, and 112, the lower boundaries of the 95% confidence intervals for the between-group differences (between each of the NmCV-5 groups and the MenACWY-D group) were above −10 percentage points. Similar results were observed with regard to the percentage of participants who had an increase of at least 4 times the baseline titer on days 28 and 112. These results suggest the noninferiority of NmCV-5 to MenACWY-D regarding these end points and are in line with the results observed with Menveo, which has also been shown to elicit noninferior or superior immune responses to those of MenACWY-D for all four serogroups included in these two vaccines. \n\n【63】At baseline, the percentage of participants with SBA titers of at least 128 was 15% or less for all five serogroups. This finding indicates a decrease in maternal antibodies as well as increasing susceptibility to meningococcal infections by 12 months of age.\n\n【64】Since there was no observed benefit of the adjuvanted formulation over the nonadjuvanted formulation, as well as no notable safety concerns, the nonadjuvanted NmCV-5 formulation was selected for further evaluation. Two phase 3 trials are ongoing to target mass immunization campaigns  and vaccination in travelers to high-risk regions .\n\n【65】The results of this trial suggest that NmCV-5 has the potential to affect outbreaks of serotype A, C, W, Y, or X in the sub-Saharan African meningitis belt, a finding that is analogous to the success seen with MenAfriVac. Several studies have shown that expanding the coverage of a monovalent serogroup A to a multivalent vaccine would be cost-effective. \n\n【66】Table 1. Participants with Serum Bactericidal Antibody Titers of at Least 128 at Days 0, 28, 84, and 112 (Per-Protocol Population). \n\n| Trial Day and Serogroup | Nonadjuvanted NmCV-5(N=144) | Nonadjuvanted NmCV-5(N=144) | Adjuvanted NmCV-5(N=144) | Adjuvanted NmCV-5(N=144) | MenACWY-D(N=72) | MenACWY-D(N=72) |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | no. | % (95% CI) | no. | % (95% CI) | no. | % (95% CI) |\n| Day 0 |  |  |  |  |  |  |\n| A | 16 | 11 (6 to 17) | 18 | 12 (8 to 19) | 10 | 14 (7 to 24) |\n| C | 2 | 1 (<1 to 5) | 0 | 0 (0 to 3) | 0 | 0 (0 to 5) |\n| W | 8 | 6 (2 to 11) | 5 | 3 (1 to 8) | 2 | 3 (<1 to 10) |\n| Y | 16 | 11 (6 to 17) | 21 | 15 (9 to 21) | 10 | 14 (7 to 24) |\n| X | 15 | 10 (6 to 17) | 12 | 8 (4 to 14) | 6 | 8 (3 to 17) |\n| Day 28 |  |  |  |  |  |  |\n| A | 144 | 100 (97 to 100) | 144 | 100 (97 to 100) | 71 | 99 (92 to >99) |\n| C | 142 | 99 (95 to >99) | 140 | 97 (93 to 99) | 39 | 54 (42 to 66) |\n| W | 142 | 99 (95 to >99) | 141 | 98 (94 to >99) | 65 | 90 (81 to 96) |\n| Y | 140 | 97 (93 to 99) | 143 | 99 (96 to >99) | 64 | 89 (79 to 95) |\n| X | 144 | 100 (97 to 100) | 143 | 99 (96 to >99) | 15 | 21 (12 to 32) |\n| Day 84 |  |  |  |  |  |  |\n| A | 144 | 100 (97 to 100) | 143 | 99 (96 to >99) | 71 | 99 (92 to >99) |\n| C | 133 | 92 (87 to 96) | 131 | 91 (85 to 95) | 26 | 36 (25 to 48) |\n| W | 137 | 95 (90 to 98) | 139 | 97 (92 to 99) | 57 | 79 (68 to 88) |\n| Y | 138 | 96 (91 to 98) | 141 | 98 (94 to >99) | 60 | 83 (73 to 91) |\n| X | 144 | 100 (97 to 100) | 143 | 99 (96 to >99) | 14 | 19 (11 to 30) |\n| Day 112 |  |  |  |  |  |  |\n| A | 144 | 100 (97 to 100) | 144 | 100 (97 to 100) | 72 | 100 (95 to 100) |\n| C | 144 | 100 (97 to 100) | 144 | 100 (97 to 100) | 68 | 94 (86 to 98) |\n| W | 144 | 100 (97 to 100) | 144 | 100 (97 to 100) | 69 | 96 (88 to 99) |\n| Y | 144 | 100 (97 to 100) | 144 | 100 (97 to 100) | 66 | 92 (83 to 97) |\n| X | 143 | 99 (96 to >99) | 144 | 100 (97 to 100) | 22 | 31 (20 to 43) |\n\n【68】 The titers of serum bactericidal antibody with rabbit complement were assessed at baseline (day 0), at 28 days and 84 days after receipt of the first dose (days 28 and 84), and 28 days after receipt of the second dose (day 112). The per-protocol population included all the participants who correctly received their assigned trial vaccine with no major protocol deviations that were determined to potentially interfere with the immunogenicity assessment and who provided, at least at one time point after vaccination, a serum sample that could be evaluated. For percentage calculations, the denominator is the number of participants with nonmissing values at baseline and postvaccination visits. Data were missing at day 0 for serogroup C in 1 participant in the nonadjuvanted NmCV-5 group and for serogroup W in 1 participant in each NmCV-5 group. CI denotes confidence interval.\n\n【69】Table 2. Participants with an Increase in Serum Bactericidal Antibody Titers of at Least 4 Times the Baseline Value on Days 28 and 112 (Per-Protocol Population). \n\n| Trial Day and Serogroup | Nonadjuvanted NmCV-5(N=144) | Nonadjuvanted NmCV-5(N=144) | Adjuvanted NmCV-5(N=144) | Adjuvanted NmCV-5(N=144) | MenACWY-D(N=72) | MenACWY-D(N=72) |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | no. | % (95% CI) | no. | % (95% CI) | no. | % (95% CI) |\n| Day 28 |  |  |  |  |  |  |\n| A | 143 | 99 (96 to >99) | 144 | 100 (97 to 100) | 70 | 97 (90 to >99) |\n| C | 141 | 99 (95 to >99) | 141 | 98 (94 to >99) | 50 | 69 (57 to 80) |\n| W | 140 | 98 (94 to >99) | 140 | 98 (94 to >99) | 65 | 90 (81 to 96) |\n| Y | 140 | 97 (93 to 99) | 142 | 99 (95 to >99) | 63 | 88 (78 to 94) |\n| X | 144 | 100 (97 to 100) | 142 | 99 (95 to >99) | 12 | 17 (9 to 27) |\n| Day 112 |  |  |  |  |  |  |\n| A | 143 | 99 (96 to >99) | 144 | 100 (97 to 100) | 71 | 99 (92 to >99) |\n| C | 142 | 99 (96 to >99) | 144 | 100 (97 to 100) | 72 | 100 (95 to 100) |\n| W | 142 | 99 (96 to >99) | 143 | 100 (97 to 100) | 71 | 99 (92 to >99) |\n| Y | 144 | 100 (97 to 100) | 142 | 99 (95 to >99) | 71 | 99 (92 to >99) |\n| X | 143 | 99 (96 to >99) | 143 | 99 (96 to >99) | 19 | 26 (17 to 38) |\n\n【71】 A serum bactericidal antibody titer of at least 4 times the baseline value was defined as follows: a postvaccination titer of at least 32 in participants with a prevaccination titer of less than 8, and a postvaccination titer of at least 4 times the prevaccination titer in participants with a prevaccination titer of 8 or higher. For percentage calculations, the denominator is the number of participants with nonmissing values at baseline and postvaccination visits. Data were missing at days 28 and 112 for serogroup C in 1 participant in the nonadjuvanted NmCV-5 group and for serogroup W in 1 participant in each NmCV-5 group.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e1eb5a73-3705-4970-9daa-fd2fb79f8975", "title": "Base-Edited CAR7 T Cells for Relapsed T-Cell Acute Lymphoblastic Leukemia", "text": "【0】Base-Edited CAR7 T Cells for Relapsed T-Cell Acute Lymphoblastic Leukemia\nAbstract\n--------\n\n【1】### Background\n\n【2】Cytidine deamination that is guided by clustered regularly interspaced short palindromic repeats (CRISPR) can mediate a highly precise conversion of one nucleotide into another — specifically, cytosine to thymine — without generating breaks in DNA. Thus, genes can be base-edited and rendered inactive without inducing translocations and other chromosomal aberrations. The use of this technique in patients with relapsed childhood T-cell leukemia is being investigated.\n\n【3】### Methods\n\n【4】We used base editing to generate universal, off-the-shelf chimeric antigen receptor (CAR) T cells. Healthy volunteer donor T cells were transduced with the use of a lentivirus to express a CAR with specificity for CD7 (CAR7), a protein that is expressed in T-cell acute lymphoblastic leukemia (ALL). We then used base editing to inactivate three genes encoding CD52 and CD7 receptors and the β chain of the αβ T-cell receptor to evade lymphodepleting serotherapy, CAR7 T-cell fratricide, and graft-versus-host disease, respectively. We investigated the safety of these edited cells in three children with relapsed leukemia.\n\n【5】### Results\n\n【6】The first patient, a 13-year-old girl who had relapsed T-cell ALL after allogeneic stem-cell transplantation, had molecular remission within 28 days after infusion of a single dose of base-edited CAR7 (BE-CAR7). She then received a reduced-intensity (nonmyeloablative) allogeneic stem-cell transplant from her original donor, with successful immunologic reconstitution and ongoing leukemic remission. BE-CAR7 cells from the same bank showed potent activity in two other patients, and although fatal fungal complications developed in one patient, the other patient underwent allogeneic stem-cell transplantation while in remission. Serious adverse events included cytokine release syndrome, multilineage cytopenia, and opportunistic infections.\n\n【7】### Conclusions\n\n【8】The interim results of this phase 1 study support further investigation of base-edited T cells for patients with relapsed leukemia and indicate the anticipated risks of immunotherapy-related complications.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e0c133a2-b953-47f7-8bd1-4eee06c0447b", "title": "Recognizing Conscience in Abortion Provision", "text": "【0】Recognizing Conscience in Abortion Provision\nThe exercise of conscience in health care is generally considered synonymous with refusal to participate in contested medical services, especially abortion. This depiction neglects the fact that the provision of abortion care is also conscience-based.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ca61e932-77d9-4e94-a915-52e56c875871", "title": "Addressing Child Hunger When School Is Closed — Considerations during the Pandemic and Beyond", "text": "【0】Addressing Child Hunger When School Is Closed — Considerations during the Pandemic and Beyond\nArticle\n-------\n\n【1】The Covid-19 pandemic has moved hunger out of the shadows in the United States. Record numbers of Americans, including one in four families with school-age children, don’t have reliable access to food.  Congress has authorized several innovative programs and substantial appropriations to respond to this crisis. Despite these efforts, food insecurity — a long-standing problem that disproportionately places children of color and those living in households under the federal poverty line at risk for physical, cognitive, and emotional harm — remains an important issue. Key factors driving increased food insecurity include school closures, transitions to hybrid learning, and suspensions of out-of-school programs, which have cut off children’s access to the food sources they once relied on during the school week. While policy attention is focused on shoring up the nutrition safety net to support pandemic recovery, it’s critical to consider whether children’s nutritional needs are being met throughout the week and where gaps may exist, both during the pandemic and beyond.\n\n【2】 Overview of the Largest USDA Nutrition-Assistance Programs Serving Children.\n\n【3】Before the pandemic, the federal nutrition safety net included five major U.S. Department of Agriculture (USDA) programs aimed at alleviating child hunger. Collectively, these programs accounted for $86.3 billion in federal spending in 2019 . Three of these programs support meals provided in educational settings: the National School Lunch Program (NSLP), the School Breakfast Program (SBP), and the Child and Adult Care Food Program (CACFP). The other two programs provide benefits designed to supplement the purchase of foods that children consume at home: the Supplemental Nutrition Assistance Program (SNAP) and the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). Participation in these programs helps reduce food insecurity. But more could be done to close gaps in children’s access to healthy meals, especially during weekends and instructional breaks.\n\n【4】Covid-19 has exposed the problems associated with relying largely on educational and enrichment settings as the primary delivery channels for food-security programs. To help feed the 30 million children who have lost access to meals during school closures and periods of hybrid learning, Congress authorized the USDA to approve waivers supporting innovative approaches organized by schools, child care centers, charitable food organizations, government agencies, and other partner organizations.\n\n【5】Perhaps the most comprehensive approach to feeding children during the pandemic has been the Pandemic Electronic Benefits Transfer (P-EBT) program. This program provided funds equivalent to the cost of meals that would otherwise have been consumed at school to families with children who were eligible for free or reduced-price meals during the spring of 2020. Congress extended this program for the 2020–2021 school year and expanded it to include children at child care centers. Evidence suggests that P-EBT mitigated hunger for 3 million children when schools were closed at the beginning of the pandemic.  P-EBT benefits cover the cost of weekday meals during the school year, but not the cost of meals consumed during weekends or instructional breaks.\n\n【6】Congress also included emergency SNAP benefits in the Families First Coronavirus Response Act. This policy allowed states to temporarily increase SNAP benefits for all current beneficiaries to the maximum amount ($680 per month for a family of four). It didn’t help the 40% of SNAP participants who were already receiving the maximum benefit. The stimulus relief passed in December as part of the fiscal year 2021 appropriations included a 15% increase in the maximum monthly benefit (equivalent to about $100 for a family of four) for 6 months. SNAP is considered the first line of defense against hunger; the program reaches more than 40 million Americans and provides food assistance directly to households. Half the households participating in SNAP include children.\n\n【7】The Coronavirus Aid, Relief, and Economic Security Act provided stimulus checks to eligible households ($1,200 per adult and $500 per child). Fewer Black and Hispanic adults than White adults report that their households received checks, however.  The latest stimulus package included a second round of checks, this time worth $600 for each eligible adult and child.\n\n【8】In addition, charitable food networks and antihunger organizations have adopted strategies for distributing food safely by means of home deliveries and drop-offs at designated locations to meet increased demand. These efforts have been especially important for children who aren’t eligible for or enrolled in other federal nutrition assistance and against the backdrop of recent restrictive immigration policies that have had a chilling effect on participation in public programs. \n\n【9】Gaps remain when it comes to feeding children. The majority of policy efforts both before and during the pandemic have focused on weekday meals, which has left millions of children hungry when schools aren’t in session. And despite considerable policy progress, high rates of food insecurity among families  suggest that recent efforts have been insufficient.\n\n【10】One important limitation is the size of the SNAP benefit, which has been widely recognized as inadequate. Increasing food prices over the past year and loss of access to other reliable sources of food (e.g., free school meals) and financial resources (e.g., because of job loss) have also meant that limited SNAP benefits need to stretch further than usual during the pandemic.\n\n【11】A second limitation is that patchwork efforts to feed children on weekends and during instructional breaks don’t reach all children. Before Covid-19, charitable food networks and antihunger organizations were the primary providers of weekend backpack programs, which supply income-eligible children with a weekend’s worth of food on Fridays during the school year. But data are lacking on the number of weekend backpack programs that existed before the pandemic and the number that are still operational. The CACFP At-Risk Afterschool Meals Program reimburses eligible organizations that provide meals for children after school hours or during weekends or instructional breaks during the school year. To increase weekend food security amid the pandemic, the USDA issued waivers that provide flexibility to schools and child-focused organizations when it comes to providing meals on weekends and during instructional breaks. During spring and summer breaks in 2020, sites participating in the NSLP, SBP, and CACFP were permitted to use summer food programs to serve to-go meals without offering enrichment activities (a prepandemic requirement). Under current waivers, approved states can allow summer food program sites to distribute multiple meals at once during holiday breaks; for example, some sites provided 1 or 2 weeks’ worth of meals on the last school day before the winter break. But not all sites’ kitchens or student households can safely store this much food.\n\n【12】Recent efforts highlight a third limitation: low participation rates in programs that provide food during weekends and instructional breaks. The reach of these programs is typically limited to children who live close to participating sites. During the pandemic, schools have struggled to maintain adequate participation in programs offering to-go meals, possibly because they have tried to implement modified meal programs with short notice while communicating changing USDA guidance to families.\n\n【13】What can be done to address these limitations? During the pandemic, one evidence-based strategy for alleviating child hunger would be to increase the amount of monthly P-EBT benefits. Another option would be to further expand the overall size of the SNAP benefit or at least extend the temporary SNAP benefit increase just passed.\n\n【14】Health care workers can play an essential role in connecting families with existing federal nutrition programs. Many health systems throughout the country screen patients for food insecurity, which could increase participation in such programs and raise awareness about additional community resources. Since food insecurity is estimated to cost the health system $53 billion annually  and many people with food insecurity have public insurance, increasing participation in nutrition-assistance programs could result in savings elsewhere in the federal safety net.\n\n【15】Although these actions would help alleviate food insecurity, they would probably be temporary. Long-term federal approaches are needed, and continued innovation will be vital for reducing food insecurity among children. Pandemic-related waivers and innovations create an opportunity to permanently strengthen the federal nutrition safety net. Any changes should be based on careful evaluation of the effectiveness of pandemic-related programs and with an eye toward feeding as many hungry children as possible. The ultimate goal should be to ensure that children have adequate access to food on all days of the week, all year long, and in all settings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4bcdc8e1-2072-4425-9bc2-97847d0a1992", "title": "Mechanisms of Disease: Genetic Disorders of Renal Phosphate Transport", "text": "【0】Mechanisms of Disease: Genetic Disorders of Renal Phosphate Transport\nThis review recounts the molecular mechanisms that control serum phosphate levels and describes the clinical consequences of abnormalities of these mechanisms. Several proteins in the kidney participate in reabsorption of urinary phosphate; the review describes mutations in the genes that encode these proteins, and the syndromes they produce.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "75ac1fd4-840c-4bf6-b513-74c5b8cb11d7", "title": "Thromboembolic Events in the South African Ad26.COV2.S Vaccine Study", "text": "【0】Thromboembolic Events in the South African Ad26.COV2.S Vaccine Study\nTo the Editor:\n--------------\n\n【1】Rare thromboembolic events have been observed during the vaccination rollout, which have prompted cautionary pauses in vaccinations by some regulatory authorities, including the South Africa Health Products Regulatory Authority.  Here, we report interim safety data from the first 288,368 participants who were vaccinated with Ad26.COV2.S in the Sisonke study — an open label, single-group, phase 3b implementation study to monitor the effectiveness of the single-dose Ad26.COV2.S vaccine among 500,000 health care workers in South Africa . Enrollment in the study began on February 17, 2021, and as of April 12, 2021, a total of 288,368 health care workers had received the Ad26.COV2.S vaccine, among whom 5898 (2%) reported adverse events.\n\n【2】The majority (81%) of adverse events reported were expected mild-to-moderate reactogenicity events. Among these 50 workers, 12 (24%) had coronavirus disease 2019 (Covid-19), which occurred within 28 days after vaccination; 12 (24%) had allergic reactions, of which one met the criteria for anaphylaxis; and 6 (12%) had neurologic conditions, including a 40-year-old man who received a diagnosis of Guillain–Barré syndrome and a 53-year-old woman with Bell’s palsy.\n\n【3】Five arterial, venous thrombotic, or embolic events were reported in 5 health care workers with known risk factors for thromboembolism (1.7 events per 100,000 participants). One case of pulmonary embolus occurred 23 days after vaccination in a 63-year-old woman who was overweight and had hypertension, diabetes mellitus, and a history of venous thrombosis; this event was fatal. A second case occurred in a 64-year-old woman who received a diagnosis of cor pulmonale 17 days after vaccination; this case had features consistent with chronic and recurrent pulmonary emboli. Two cerebrovascular accidents (infarcts on imaging) were reported — one case involved a 45-year-old woman who had underlying rheumatic heart disease and a history of human immunodeficiency virus infection, cerebrovascular accident, and aortic valve replacement, in whom left-sided weakness developed the day after vaccination, and the other case involved a 38-year-old woman who had given birth to twins 9 months before vaccination and presented with features of transient ischemic attack 8 days after vaccination. A 65-year-old woman with chronic diabetes mellitus had deterioration and blurring of vision 8 days after vaccination and received a diagnosis of retinal vein occlusion and macular hemorrhage. To date, no case of vaccine-induced immune thrombotic thrombocytopenia has been documented.\n\n【4】In South Africa, since the start of the pandemic, Covid-19 has been reported in 1.58 million persons, including more than 55,000 health care workers. The rate of adverse events with vaccination is low, and thromboembolic events have occurred mainly in persons with risk factors for thromboembolism.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "defd760f-49ac-4386-aaa3-1c5f6092c57f", "title": "Chenodeoxycholic Acid Treatment of Gallstones — A Follow-up Report and Analysis of Factors Influencing Response to Therapy", "text": "【0】Chenodeoxycholic Acid Treatment of Gallstones — A Follow-up Report and Analysis of Factors Influencing Response to Therapy\nAbstract\n--------\n\n【1】We treated 70 patients with gallstones with chenodeoxycholic acid over 3 1/2 years and analyzed the factors influencing the outcome of therapy.\n\n【2】This treatment was unsuccessful in 11 patients with radiopaque gallstones and in seven with nonfunctioning gallbladders, but 64 per cent with radiolucent gallstones treated for six months or more showed partial or complete gallstone dissolution, and of those whose bile became unsaturated with cholesterol, 100 per cent had evidence of dissolution.\n\n【3】In patients with partial or complete gallstone dissolution, the mean post-treatment biliary cholesterol saturation index — 0.78±0.04 (S.E.M.) — was significantly less (P<0.001), and the dose of chenodeoxycholic acid (14.4 ± 1.0 mg per kilogram of body weight per day) significantly more (P <0.025) than in those whose gallstones did not change (1.15±0.04 and 10.6±1.2 respectively).\n\n【4】In patients with radiolucent gallstones, the dose of chenodeoxycholic acid should be based on body weight; 14 to 15 mg per kilogram of body weight per day effectively lowers the saturation index and dissolves gallstones.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9f39e773-0a4c-44ec-bb39-e351701081b1", "title": "Prospective Study of 352 Young Patients with Chemical Diabetes", "text": "【0】Prospective Study of 352 Young Patients with Chemical Diabetes\nAbstract\n--------\n\n【1】Three criteria for \"chemical diabetes\" were evaluated by determination of their accuracy in predicting the subsequent development of classic diabetes, as measured by decompensation of carbohydrate control.\n\n【2】Three hundred and fifty-two subjects with the diagnosis of chemical diabetes made on the basis of these three criteria, were followed for a maximum of 12 years, and the data observed were analyzed by the life-table technic. Fifty-two per cent of the cases diagnosed according to the United States Public Health Service criteria (fasting blood sugar level of 110, one-hour levels of 170, two-hours levels of 120 and three-hours levels of 110 mg per 100 ml) progress to decompensation of carbohydrate control within 10 years of the diagnosis, as estimated by life-table analysis, whereas only 32 per cent diagnosed by the Fajans and Conn criteria, and 21 per cent diagnosed by the Mosenthal and Barry, show similar progression. Body weight and a family history of diabetes had no appreciable effect on these findings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2d18d54f-483e-47d4-871b-32243c2550be", "title": "Unilateral Lung Transplantation for Pulmonary Fibrosis", "text": "【0】Unilateral Lung Transplantation for Pulmonary Fibrosis\nAbstract\n--------\n\n【1】Improvements in immunosuppression and surgical techniques have made unilateral lung transplantation feasible in selected patients with end-stage Interstitial lung disease. We report two cases of successful unilateral lung transplantation for end-stage respiratory failure due to pulmonary fibrosis. The patients, both oxygen-dependent, had progressive disease refractory to all treatment, with an anticipated life expectancy of less than one year on the basis of the rate of progression of the disease. Both patients were discharged six weeks after transplantation and returned to normal life. They are alive and well at 26 months and 14 months after the procedure. Pulmonary-function studies have shown substantial improvement in their lung volumes and diffusing capacities. For both patients, arterial oxygen tension is now normal and there is no arterial oxygen desaturation with exercise.\n\n【2】This experience shows that unilateral lung transplantation, for selected patients with end-stage interstitial lung disease, provides a good functional result. Moreover, it avoids the necessity for cardiac transplantation, as required by the combined heart–lung procedure, and permits the use of the donor heart for another recipient.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b9613299-9a76-40fa-9b4f-6552cd610bc3", "title": "Objective Estimates of the Probability of Death from Burn Injuries", "text": "【0】Objective Estimates of the Probability of Death from Burn Injuries\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Over the past 20 years, there has been remarkable improvement in the chances of survival of patients treated in burn centers. A simple, accurate system for objectively estimating the probability of death would be useful in counseling patients and making medical decisions.\n\n【3】Methods\n-------\n\n【4】We conducted a retrospective review of all 1665 patients with acute burn injuries admitted from 1990 to 1994 to Massachusetts General Hospital and the Shriners Burns Institute in Boston. Using logistic-regression analysis, we developed probability estimates for the prediction of mortality based on a minimal set of well-defined variables. The resulting mortality formula was used to determine whether changes in mortality have occurred since 1984, and it was tested prospectively on all 530 patients with acute burn injuries admitted in 1995 or 1996.\n\n【5】Results\n-------\n\n【6】Of the 1665 patients (mean \\[±SD\\] age, 21±20 years; mean burn size, 14±20 percent of body-surface area), 1598 (96 percent) lived to discharge. The mean length of stay was 21±29 days. Three risk factors for death were identified: age greater than 60 years, more than 40 percent of body-surface area burned, and inhalation injury. The mortality formula we developed predicts 0.3 percent, 3 percent, 33 percent, or approximately 90 percent mortality, depending on whether zero, one, two, or three risk factors are present. The results of the prospective test of the formula were similar. A large increase in the proportion of patients who chose not to be resuscitated complicated comparisons of mortality over time.\n\n【7】Conclusions\n-----------\n\n【8】The probability of death after burns is low and can be predicted soon after injury on the basis of simple, objective clinical criteria.\n\n【9】Introduction\n------------\n\n【10】Accurate, objective estimates of the probability of death from burn injuries would, as envisioned by Knaus et al.,  provide clinicians with an explicit basis for clinical decisions, help them understand the relative contributions of specific prognostic criteria, and reduce reliance on clinical intuition. These estimates would also be useful to patients and to others making medical and financial decisions about their care. The most widely used formulas for the prediction of mortality from burns are based on a minimal set of easily obtained variables. A classic example  calculated the percent likelihood of mortality as the patient's age in years plus the percentage of the body-surface area that was burned. This formula, which was useful for triage and early assessment of outcome because it was easily remembered and applied, has become obsolete because of the remarkable improvement in survival rates in major burn centers in the past 20 years.  In contrast, more recent formulas have limited clinical use because they are difficult to remember or apply or they require more sophisticated clinical variables. \n\n【11】In this study we examined data on mortality and length of stay among patients with burns who were treated in two centers from 1990 to 1996. We then developed estimates of the probability of death and length of stay based on information available on admission. The goal was to develop simple, objective probability estimates, thereby reducing reliance on subjective estimates of outcome.\n\n【12】Methods\n-------\n\n【13】We retrospectively reviewed the records of all 1665 patients with acute burn injuries admitted to the Shriners Burns Institute, Boston, and Massachusetts General Hospital from 1990 to 1994. Information collected included age, sex, admission and discharge dates, length of stay, extent of burn (sum of the extent of second- and third-degree burns), type of burn, presence or absence of inhalation injury, need for escharotomy, and mortality. Burn types were classified as flame, scald, contact, chemical, electrical, and other (e.g., radiation). Inhalation injury was considered present if the fire had occurred in a closed space, if bronchoscopy on admission found soot below the level of the vocal cords, or if the blood carboxyhemoglobin concentration was elevated on admission. We included all patients in the study who died before discharge, including those who died within 72 hours after admission (defined as early death), those for whom there were do-not-resuscitate orders, those who died while in a persistent vegetative state, and those who died of causes other than burns. Permission for record review was obtained from the Subcommittee for Human Studies at Massachusetts General Hospital.\n\n【14】Stepwise logistic-regression analysis was performed with forward selection for evaluation of risk factors for death. Age, sex, presence or absence of inhalation injury, need for escharotomy, year of admission, and type and extent of burn were used as mortality and length-of-stay covariates. Age and burn size were coded by a method that allowed the data to dictate how many age and burn-size classifications were necessary.  Regression analysis was used for length of stay of survivors. The resulting models were simplified to use as few variables as possible on the basis of statistical judgment justified by the data to predict risk. The mortality model was tested prospectively by application to data for patients admitted in 1995 or 1996.\n\n【15】Changes in mortality rates over the past decade were assessed by entering the data for 1990 to 1994 into a formula developed for estimates of the probability of death from burns based on 1984 data. \n\n【16】Values are reported as means ±SD. Comparisons were made by Student's t-test. Confidence intervals were calculated for estimated probabilities from the logistic model.\n\n【17】Results\n-------\n\n【18】Of the 1665 patients (910 children and 755 adults), 1143 (69 percent) were male and 522 (31 percent) were female, with a mean age of 21±20 years (range, 1 month to 99 years). The mean burn size was 14±20 percent of body-surface area. Two hundred forty-four patients (15 percent) had inhalation injury, and 131 (8 percent) required escharotomy. Treatment consisted of initial fluid resuscitation, early excision and grafting of burn wounds, topical antimicrobial therapy, and critical care support by a multidisciplinary team. \n\n【19】The mean length of the hospital stay was 21±29 days. Of the 1665 patients, 1598 (96 percent) lived to discharge. Sixty-seven patients died (4 percent); 22 of these (33 percent) had multiple-organ-system failure, and 13 (19 percent) had early deaths. Twenty-two of the 67 patients (33 percent) had do-not-resuscitate orders, 5 (7 percent) died with a diagnosis of persistent vegetative state, and 5 (7 percent) died of non–burn-related causes. To our knowledge, only 2 of the 1598 patients who lived to discharge (0.1 percent) died within three months after discharge, both of underlying disease processes unrelated to the burn injury. These patients were considered survivors for the purposes of this study.\n\n【20】A Model for Estimating the Probability of Death\n-----------------------------------------------\n\n【21】Table 1. Mortality among Patients According to Age, Burn Size, and Presence or Absence of Inhalation Injury.\n\n【22】Sex, admission and discharge dates, type of burn, and need for escharotomy were not significantly associated with mortality. The identified risk factors for death were an age greater than 60 years, a burn covering more than 40 percent of body-surface area, and inhalation injury. Table 1 shows the distribution of patients according to these risk factors and actual mortality.\n\n【23】Table 2. Frequency of Each Risk Factor and Mortality Rates.\n\n【24】Table 2 condenses these data and shows that the presence of any one or two risk factors had a similar effect on mortality, allowing greater simplification of the mortality model. When the number of risk factors was used as a single covariate, no individual factor remained significant except an age of more than 90 years. Only four patients were older than 90; three of them had do-not-resuscitate orders. This model is described by equation 1:\n\n【25】> logit = -5.89+2.58n,\n\n【26】Table 3. Actual and Estimated Mortality from Burns According to the Number of Risk Factors.\n\n【27】where n is the number of risk factors and logit is the natural logarithm of the ratio of the probability of dying to the probability of living. The frequency of these three factors and the actual mortality and estimated probability of death based on this model are shown in Table 3 .\n\n【28】With this model, mortality estimates can be easily remembered according to a simple clinical rule. When burn size of more than 40 percent of body-surface area, age greater than 60 years, and presence of inhalation injury are taken as risk factors, patient mortality is 0.3 percent with no risk factors, 3 percent with one risk factor, 33 percent with two risk factors, and approximately 90 percent with all three risk factors. This rule is applicable to all patients younger than 90.\n\n【29】Prospective Evaluation of Model\n-------------------------------\n\n【30】A total of 530 patients with burns were admitted during 1995 and 1996. Their mean age was 25±23 years, their mean burn size was 13±16 percent of body-surface area, and 13 percent had inhalation injury. Thirty-two of the 530 patients (6 percent) died. The mortality rates were 0.7 percent, 14 percent, 39 percent, and 90 percent for patients with zero, one, two, and three risk factors, respectively, percentages similar to those predicted by equation 1.\n\n【31】Assessment of Changes in Mortality from the 1980s to the 1990s\n--------------------------------------------------------------\n\n【32】The development of a model for estimating the probability of death from burns permits comparison of the probability estimates over time. The log of the odds of death (logit) was found previously  to be described by equation 2:\n\n【33】> logit = -7.37 + 0.05(age) - 0.15(year) + 0.11(% body-surface area) -\n\n【34】> 6.61 × 10  (% body-surface area - mean % body-surface area)  +\n\n【35】> 1.04 × 10  (age - mean age)  ,\n\n【36】where age is the patient's age in years and year is the year of admission. Mortality decreased from 1974 to 1984. To test whether mortality decreased after 1984, the probability of mortality for each risk group in the period from 1990 to 1994 was estimated with equation 2 (with year set at 1984). The observed mortality was not significantly different from the expected mortality, indicating that mortality had not changed since 1984. Equations 1 and 2 functioned similarly as predictors.\n\n【37】In the previous study,  early deaths (14 of 1103 patients, or 1.3 percent) and 1 death in a patient with a do-not-resuscitate order were excluded, and patients in a persistent vegetative state were not considered. In 1990 to 1994, the analysis included patients with early deaths (13 of 1665 patients, or 0.8 percent) and those who died while under a do-not-resuscitate order or in a persistent vegetative state (27 patients, or 1.6 percent). If these patients are excluded, the overall difference in mortality between the decades remains similar.\n\n【38】Analysis of Choice of Do-Not-Resuscitate Treatment Plan\n-------------------------------------------------------\n\n【39】There was a 12-fold increase in the percentage of patients with do-not-resuscitate orders between the period from 1975 to 1984  and the period from 1990 to 1994 (from 0.1 percent to 1.3 percent); all these patients died. In 1995 and 1996, this percentage increased again; 12 of 530 patients (2.3 percent) chose not to be resuscitated, 1 of whom survived. We asked whether there were characteristics that led to the decision not to resuscitate.\n\n【40】Equation 2 predicted that 37 patients in the period from 1990 to 1994 had an intermediate risk of death (40 to 80 percent). Among them, 11 patients (30 percent) had do-not-resuscitate orders, all of whom died, and 26 patients (70 percent) had orders to resuscitate. Among the patients for whom resuscitation was ordered, there were five deaths from multiple-organ-system failure, two early deaths, and one death from cardiac arrhythmia. There were 18 survivors (69 percent of those resuscitated).\n\n【41】The average age of the patients with do-not-resuscitate orders (78±10 years) and their burn size (43±27 percent of body-surface area) were not significantly different from those of the resuscitated patients (67±15 years and 58±31 percent of body-surface area). Eighty-two percent of the patients with do-not-resuscitate orders had inhalation injury, as compared with 77 percent of the resuscitated patients. Therefore, the reasons for the decision whether to resuscitate were not fully explained by the risk data presented. Notably, surviving intermediate-risk patients often had serious preexisting conditions and frequently had untoward events during hospitalization. Thus, some patients with do-not-resuscitate orders might have survived.\n\n【42】Prediction of Length of Stay\n----------------------------\n\n【43】Table 4. Estimates of the Length of the Hospital Stay According to Interquartile Ranges for Burn-Size Groups.\n\n【44】An analysis of variance was performed to determine the factors contributing to length of stay. The variables tested (age, sex, year of admission, burn size, type of burn, presence or absence of inhalation injury, and need for escharotomy) accounted for 63 percent of the variance. Many variables were significantly associated with length of stay, but burn size accounted for most of the variance. When mean length of stay was plotted against burn size, it was evident that a patient's mean length of stay could be characterized by membership in one of four groups: burn size of less than 20 percent, between 20 and 39 percent, between 40 and 89 percent, and 90 percent or more of body-surface area. The variability of length of stay within these groups is illustrated by the interquartile ranges .\n\n【45】Discussion\n----------\n\n【46】The mortality rate among patients with burns at our two hospitals (4 percent) is consistent with those recently reported from other centers.  Three risk factors for death after burns were identified: an age of more than 60 years, burn size of more than 40 percent of body-surface area, and inhalation injury. Mortality is a function of the number of risk factors present. Patients with three risk factors had very high mortality, even considering that most patients with three risk factors had do-not-resuscitate orders. Only one of seven patients (14 percent) with three risk factors who received full resuscitation efforts survived.\n\n【47】The mortality rate among the patients who survived one week was 2.5 percent, which was substantially lower than the overall rate of 4 percent. By the third week, the mortality rate was 1.1 percent. Multiple-organ-system failure, which occurred between the second and the eighth weeks, accounted for one third of the deaths due to burns. After the initial hospitalization for burns, life expectancy is not significantly different from that of the age-adjusted general population. \n\n【48】Although there was no significant difference in mortality rates between 1984 and the 1990s, the remarkably small number of deaths, the increase in the number of patients with do-not-resuscitate orders, and differences in patient-exclusion criteria between the data sets limit the sensitivity of our analysis. Given these limitations, the analysis can only suggest that no large improvements in survival have occurred. To decrease mortality further, attention must be focused on factors operating before hospitalization, including field management, early transfer, and prevention. Clearly, prevention efforts are effective  and must target older adults.\n\n【49】One advantage of objective estimates of the probability of death is that they help in understanding the relative influences of specific prognostic elements. Others have reported the age of the patient and the extent of the burns as contributors to the mortality rate.  However, few studies have emphasized the contribution of inhalation injury to mortality.  This study emphasizes the marked degree to which inhalation injury contributes to mortality. The depth of severe burns, as indicated by the need for escharotomy, did not contribute significantly to mortality, although it was previously implicated in mortality among patients with large burns.  Although many older women died of burns from cooking fires and most children had scald burns, sex and differences in burn type did not contribute to mortality rates once burn size, the patient's age, and the presence or absence of inhalation injury were considered.\n\n【50】The probability estimate developed for the length of stay is based on burn size. Much of the variance was not accounted for by the data collected and is probably due to social factors. Estimates of length of stay are important for financial reasons, and accurate early estimates facilitate better financial planning by the payer. With $2 billion spent yearly in the United States on burn care, information on costs is important. For example, an estimated $30 million per year is billed for the care of older women with cooking-related ignition of their clothing.  Knowledge of the huge costs of specific injuries in terms of dollars as well as lives could help identify potential sources of funding for directed prevention programs.\n\n【51】The objective estimate of the probability of death from burn injury enables clinicians to predict mortality with greater precision. The best measure of the predictive power of the equation is the number of patients whose estimated risk of death would change appreciably from the overall risk of 4 percent, since this risk is assigned to all, regardless of risk factors. Our simple equation identified a substantial majority (1314 of 1665 patients, or 79 percent) for whom the risk of death was very low (0.2 percent) and assigned a high risk of death (87 percent) to 22 of 1665 patients (1.3 percent).\n\n【52】Certain other considerations must be emphasized with respect to the application of estimates of the probability of death to individual patients, particularly when one is deciding whether resuscitation should proceed. Each estimate is a major component of, but cannot substitute wholly for, clinical decisions. Our estimates were developed on the basis of a large number of patients who underwent a particular treatment regimen, including early burn excision and grafting, topical antimicrobial therapy, and critical care support.  For some patients, the situation may not be adequately described by the data — for example, a young person with a burn on 100 percent of the body-surface area, but without inhalation injury. It would be unwise to conclude that the probability of mortality for that patient was 3 percent. Furthermore, there may be clinical variables not tested as covariates that are important for prognosis. New therapies could make current estimates obsolete.\n\n【53】Finally, the outcome must not be viewed solely as death or survival, without consideration of the patient's quality of life. When resuscitation decisions are considered, communication between doctors and patients or their surrogates is critical for the achievement of acceptable outcomes. These considerations should be free from age-based discrimination, monetary pressures, and determinations of the quality of life by outsiders. Objective estimates of the probability of death provide clinicians with valuable information for use in making these decisions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "11d39c8d-c87d-4ada-87f8-79991e7faef2", "title": "Chronic Traumatic Encephalopathy in the Brains of Military Personnel", "text": "【0】Chronic Traumatic Encephalopathy in the Brains of Military Personnel\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Persistent neuropsychiatric sequelae may develop in military personnel who are exposed to combat; such sequelae have been attributed in some cases to chronic traumatic encephalopathy (CTE). Only limited data regarding CTE in the brains of military service members are available.\n\n【3】Methods\n-------\n\n【4】We performed neuropathological examinations for the presence of CTE in 225 consecutive brains from a brain bank dedicated to the study of deceased service members. In addition, we reviewed information obtained retrospectively regarding the decedents’ histories of blast exposure, contact sports, other types of traumatic brain injury (TBI), and neuropsychiatric disorders.\n\n【5】Results\n-------\n\n【6】Neuropathological findings of CTE were present in 10 of the 225 brains (4.4%) we examined; half the CTE cases had only a single pathognomonic lesion. Of the 45 brains from decedents who had a history of blast exposure, 3 had CTE, as compared with 7 of 180 brains from those without a history of blast exposure (relative risk, 1.71; 95% confidence interval \\[CI\\], 0.46 to 6.37); 3 of 21 brains from decedents with TBI from an injury during military service caused by the head striking a physical object without associated blast exposure (military impact TBI) had CTE, as compared with 7 of 204 without this exposure (relative risk, 4.16; 95% CI, 1.16 to 14.91). All brains with CTE were from decedents who had participated in contact sports; 10 of 60 contact-sports participants had CTE, as compared with 0 of 165 who had not participated in contact sports (point estimate of relative risk not computable; 95% CI, 6.16 to infinity). CTE was present in 8 of 44 brains from decedents with non–sports-related TBI in civilian life, as compared with 2 of 181 brains from those without such exposure in civilian life (relative risk, 16.45; 95% CI, 3.62 to 74.79).\n\n【7】Conclusions\n-----------\n\n【8】Evidence of CTE was infrequently found in a series of brains from military personnel and was usually reflected by minimal neuropathologic changes. Risk ratios for CTE were numerically higher among decedents who had contact-sports exposure and other exposures to TBI in civilian life than among those who had blast exposure or other military TBI, but the small number of CTE cases and wide confidence intervals preclude causal conclusions. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nCTE in Military Personnel  \n\n【11】Since 2001, more than 2.7 million U.S. military personnel have been deployed overseas, often encountering attacks with high explosives such as improvised explosive devices and suicide bombs.  Owing to advances in protective equipment and military medicine, service members commonly survive these attacks, often without apparent physical wounds. However, survivors frequently report neuropsychiatric symptoms, including chronic headaches, nausea, sleep disorders, concentration and memory problems, mood and behavioral disturbances, substance abuse, and suicidality, which are often diagnosed as postconcussive syndrome or post-traumatic stress disorder (PTSD). \n\n【12】Chronic traumatic encephalopathy (CTE) is a disorder with specific neuropathological characteristics that are seen in autopsied brains of decedents with histories of repetitive traumatic brain injury (TBI), particularly impact TBI (defined as injury caused by the head striking a physical object, often from blunt force). CTE has most often been reported after longstanding participation in contact sports, including American football, combat sports (e.g., boxing and wrestling), rugby, hockey, soccer, and even baseball.  CTE has also been identified in other circumstances of repetitive impact TBI, including domestic abuse, epileptic seizures, and head-banging behavior in persons with autism.  However, CTE-related lesions have also been reported to be associated with single, moderate-to-severe TBI events. \n\n【13】Much of the persistent symptomatology among service members who have been exposed to combat — and specifically those with blast exposure — overlaps with clinical presentations that are attributed to CTE, including cognitive dysfunction, behavioral changes, mood disorders and disturbances, substance abuse, and suicidality.  There have been anecdotal reports of pathologic features of CTE in the brains of military veterans, and these reports have led to the suggestion that military service, and blast exposure specifically, is associated with CTE, and that pathological features of CTE may underlie neuropsychiatric outcomes that service members have had after exposure to combat.  Military service, including that involving blast exposure, has also been incorporated into the National Institute of Neurological Disorders and Stroke diagnostic criteria for traumatic encephalopathy syndrome, which is the proposed clinical diagnosis that corresponds to CTE. \n\n【14】We performed neuropathological examinations of a consecutive series of brains from the collection of the Department of Defense–Uniformed Services University Brain Tissue Repository , a bank dedicated to the study of donated brains from deceased service members.  We assessed the incidence of TBI in civilian and military life (including blast exposure), psychiatric disease, alcohol and substance abuse, and suicidality in service members with or without CTE.\n\n【15】Material and Methods\n--------------------\n\n【16】Brain Donation and Decedent History\n-----------------------------------\n\n【17】The Department of Defense–Uniformed Services University Brain Tissue Repository, located in Bethesda, Maryland, receives offers of brain donations for neuropathological examination and research from the next of kin or legal representatives of deceased military personnel. The brain bank is not allowed to solicit brain donations from families of the deceased but instead uses an outreach campaign that includes brochures, media coverage, public service announcements, social media, and word-of-mouth recommendations from other donor families. The current study was conducted according to a protocol approved by the Uniformed Services University institutional review board.\n\n【18】Inclusion of a brain into the brain bank, with a few exceptions, is based simply on the donor’s service in the military, whether the person was on active duty or retired from the military at the time of death, and is irrespective of the person’s TBI history, neuropsychiatric disease, or other factors. Decedents’ histories are gathered retrospectively from semistructured interviews with family or next of kin, from other medical or legal representatives, and from available medical records, including postdeployment questionnaires, clinical records, autopsy reports, and death certificates.\n\n【19】Neuropathological and Immunohistochemical Examinations\n------------------------------------------------------\n\n【20】Each brain in our study was fixed in 10% buffered formalin for 2 to 5 weeks and, according to our routine neuropathological diagnostic procedures, was examined for gross pathological findings by one or more neuropathologists. Tissue samples were obtained from regions of the brain chosen at the discretion of the neuropathologist at the time of the initial assessment. An average of 13 (range, 5 to 23) tissue block samples were obtained from different regions containing cerebral cortex from one or both cerebral hemispheres of each brain, including from some or all these locations: the frontal, temporal, parietal, and occipital neocortex; the entorhinal and parahippocampal cortex; the cingulate cortex; and the insular cortex.\n\n【21】Immunohistochemical analysis for phosphorylated tau, which is the essential diagnostic test for CTE, was performed as part of the assessment of each case. Staining for beta-amyloid, amyloid precursor protein, ionized calcium-binding adapter molecule 1 (IBA-1, for microglia), and glial fibrillary acidic protein was performed in each case, and staining for other proteins, such as α-synuclein, ubiquitin, and TDP-43, was performed, as indicated, in some cases; these stains were not considered to be directly informative in the assessment for CTE and thus were not examined systematically for this study and are not reported here. Tissue processing and sectioning, histologic preparation, and immunohistochemical protocols were performed on all samples by means of identical procedures. Samples were prepared with the use of an automated tissue processor (ASP6025, Leica Biosystems). Each tissue block was paraffin-embedded and cut into 5-μm–thick sections for staining and evaluation according to immunohistochemical protocols, including assessment for phosphorylated tau with the use of antibody AT8 (mouse antihuman monoclonal antibody; dilution, ; epitope retrieval time, 10 minutes) (MN1020, Thermo Fisher Scientific), with the Leica Bond III automated immunostainer with a diaminobenzidine chromogen detection system (DS9800, Leica Biosystems). All slides were digitally scanned with the use of the Aperio AT2 whole-slide scanner (Leica Biosystems) and were stored on a Biolucida (MBF Bioscience) Web-based server. Slides were reviewed either digitally (with the use of Aperio ImageScope \\[Leica Biosystems\\]) or with the use of bright-field microscopy (or both).\n\n【22】Assessment for CTE neuropathologic changes was performed by the first and last authors, who were unaware of decedent case histories and interpretations from previous neuropathological assessments, which were made before the publication of the most recent diagnostic criteria for CTE. We assessed brains for CTE by examination of immunohistochemical staining for phosphorylated tau according to published consensus criteria, requiring phosphorylated tau aggregates in neurons, with or without glial tau in thorn-shaped astrocytes, at the depth of a cortical sulcus around a small blood vessel, in deeper cortical layers not restricted to subpial and superficial regions of the sulcus.  For the purpose of this study, the presence of a single pathognomonic lesion was sufficient for a diagnosis of CTE.\n\n【23】Assessment of TBI Exposures and Clinical Characteristics of the Decedents\n-------------------------------------------------------------------------\n\n【24】A history of TBI was classified as TBI that had occurred during the decedent’s military or civilian life. A military TBI was defined as a TBI that had occurred during activity related to military service and was further classified as either impact TBI or blast exposure. A military impact TBI was defined as a reported TBI from impact head trauma during military service without associated blast exposure. The history was considered positive for blast exposure if it included at least one specific report of blast exposure or if blast exposure was inferred by the nature of the person’s military career, such as service in the Special Forces or in explosive ordnance disposal. Civilian TBI was classified either as related to contact-sports participation or as non–sports-related (e.g., motor vehicle accidents or assaults). A contact-sports history was defined as participation in recognized or organized athletic activities that have the potential for TBI resulting from repeated impact with other participants, playing equipment or surfaces, or both. \n\n【25】Statistical Analysis\n--------------------\n\n【26】This study had no prespecified statistical analysis plan. Demographic characteristics, psychiatric diagnoses, alcohol or substance abuse (excluding tobacco), and manner of death are presented descriptively. Relative risks were calculated as the ratio of the percentage of decedents with CTE who had a given type of TBI exposure to the percentage of those with CTE who did not have the exposure. We calculated confidence intervals for relative risks using the Taylor series estimate of the variance, except for one ratio with a denominator of zero (contact-sports exposure), for which we calculated an exact confidence interval using the IRI command in Stata software, version 15.0 (StataCorp). The widths of the confidence intervals have not been adjusted for multiplicity and cannot be used to infer causal effects.\n\n【27】Results\n-------\n\n【28】Characteristics of Decedents and Brain Samples\n----------------------------------------------\n\n【29】The brains were obtained from 2013 through 2021 and were consecutive except for brains that were excluded owing to incomplete sampling or staining at the time of review (9 brains), nonmilitary status (5 brains), absence of decedent information (1 brain), or poor specimen condition (1 brain). The neuropathological study included brains from 217 men and 8 women, with an average age of 48.2 years (median, 49; range, 18 to 87) for men and 47.8 years (median, 52.5; range, 20 to 63) for women. The decedents comprised both retired and active-duty members of all military branches; 20 served in the Special Forces, most commonly as Navy SEALs (13 decedents). Race or ethnic group was reported in 199 of the 225 cases; 171 decedents were White, 15 were Black, 8 were Hispanic or Latino, 3 were Asian or Pacific Islander, 1 was Native American, and 1 was biracial (not otherwise specified).\n\n【30】Lesions diagnostic of CTE were found in 10 of 225 cases (4.4%); half the CTE cases had only one pathognomonic lesion, 2 had four lesions, and 1 had five lesions. Two of the brains with CTE (from decedents who were 72 and 87 years of age) had severe neuropathologic changes characteristic of Alzheimer’s disease, including widespread neurofibrillary tangles and senile plaques throughout the cerebral cortex, that precluded accurate assessment of the number of CTE-related lesions. The degree of neuropathologic changes characteristic of Alzheimer’s disease in these two cases suggested a high likelihood of cognitive impairment or dementia. \n\n【31】Table 1. Clinical Characteristics and Exposures in Military Decedents with Chronic Traumatic Encephalopathy (CTE). Table 2.  Table 2. Clinical Features in CTE and Non-CTE Groups.\n\n【32】Military and Civilian TBI\n-------------------------\n\n【33】A total of 60 decedents (26.7%) had a history of participation in one or more contact sports, most commonly American football (in 27) and combat sports (in 15). Other sports included rugby, hockey, soccer, lacrosse, baseball or softball, basketball, and flag football. One person each was a skateboarder, a competitive skier, a mountain biker, and a bull rider; all of them had associated head injuries.\n\n【34】A total of 44 decedents (19.6%) had one or more civilian non–sports-related impact TBIs from motor vehicle accidents, falls, assaults, and other causes. As a result of civilian non–sports-related impact TBI, 29 decedents (including 7 of the 10 with CTE) had had one or more episodes of loss of consciousness or coma (in 14 decedents), skull injury (in 8), concussion (in 5), post-traumatic mental or cognitive changes or deficits (in 4), post-traumatic seizure disorder (in 3), or intracranial bleeding (in 1) in association with the TBI. One decedent in the CTE group had hereditary hemorrhagic telangiectasia (the Osler–Weber–Rendu syndrome); in the brain of that decedent, the only pathognomonic CTE lesion that was identified was adjacent to the site of a previous resection of a vascular malformation.\n\n【35】Table 3. CTE and Exposure to Traumatic Brain Injury (TBI) among the 225 Military Decedents.\n\n【36】Of the 45 decedents with a history of blast exposure, 3 had CTE, as compared with 7 of the 180 decedents without blast exposure (relative risk, 1.71; 95% confidence interval \\[CI\\], 0.46 to 6.37) . Of the 21 decedents who had a history of military impact TBI, 3 had CTE, as compared with 7 of the 204 decedents without military impact TBI (relative risk, 4.16; 95% CI, 1.16 to 14.91). Among the 44 brains from decedents who had a history of civilian non–sports-related TBI, 8 had CTE, as compared with 2 of 181 brains from decedents without a history of civilian non–sports-related TBI (relative risk, 16.45; 95% CI, 3.62 to 74.79). In all 10 cases of CTE, the decedents had a history of playing contact sports. Of the 60 decedents who had a history that included contact sports, 10 had CTE, as compared with none of the 165 decedents who did not have a history of playing contact sports (point estimate of relative risk not computable; 95% CI, 6.16 to infinity). In the CTE group, the most common contact sports were American football (in 5 cases, including 3 in combination with other contact sports) and combat sports (in 5 cases, including 3 in combination with other contact sports).\n\n【37】Psychiatric Disorders, Alcohol and Substance Abuse, and Suicidality\n-------------------------------------------------------------------\n\n【38】A total of 88 decedents (39.1%) had a reported history of at least one psychiatric disorder, most commonly PTSD (in 52 decedents), depression (in 44), or anxiety (in 25) . Of the decedents with CTE, 6 (60%) had received at least one diagnosis of a psychiatric disorder, as compared with 82 (38%) of the decedents without CTE; PTSD was the most common diagnosis in both groups. A total of 6 decedents with CTE (60%) had a reported history of alcohol or substance abuse (or both), as compared with 91 of those without CTE (42%).\n\n【39】Data regarding a definitive cause and manner of death were available in 216 cases. Most deaths (124) were from natural causes. There were 49 suicides (22.7%), and 40 deaths (18.5%) were categorized as accidental (including 12 deaths due to drug overdose). The average age at death was 55.4 years in cases of death from natural causes, 37.8 years in the cases of suicide, and 38.5 years in the cases of accidental death. The remaining deaths were by homicide (1 death) or occurred in manners that remained undetermined (2 deaths) after investigation by medical examiners. Suicide was the manner of death in 4 of the CTE cases (40%) and in 45 of the 206 non-CTE cases (22%) in which there was a confirmed cause and manner of death.\n\n【40】Discussion\n----------\n\n【41】In our assessment for the presence of CTE in the brains of 225 deceased active-duty and retired military personnel, there was a broad age range among the decedents and various exposures to head injuries sustained in military and civilian settings. We found CTE in 4.4% of the brains, half of which had only one CTE-related lesion, and it is unclear whether those minimal histopathological findings were of clinical significance. Two CTE cases involved the brains of elderly decedents that also had severe neuropathologic changes consistent with Alzheimer’s disease, which made interpretation of the extent of CTE pathologic changes and clinical features difficult.\n\n【42】The small number of CTE cases makes the study underpowered to conclude that blast exposure was or was not associated with CTE. However, all decedents who had military-related blast or impact exposures and had CTE also had participated in contact sports that had the potential for head injuries, and in addition often had a history of non–sports-related civilian TBI. The estimates of the relative risk of these two types of civilian exposures with respect to CTE have wide confidence intervals and are unadjusted for multiple comparisons, so no causal conclusions can be drawn; however, the relative risks were numerically higher with civilian exposures than with military exposures.\n\n【43】Few community-based studies have been performed to assess CTE, and most have used brains from banks that focus on aging or neurodegenerative disease, with average ages at death of over 70 years.  In those studies in which a history of contact-sports participation was available, findings of CTE were uncommon in decedents with no history of contact-sports participation. One study reported CTE in 21 of 66 (31.8%) deceased elderly former contact-sports athletes and in none of 198 age-matched controls without contact-sports history.  In that study, 35 of the 66 former athletes had served in the military, and there was no significant difference in CTE frequency in the brains of military athletes as compared with nonmilitary athletes. In a convenience sample of 202 brains of former American football players from a brain bank focused on contact sports, CTE was found in 87% of the brains.  This included 21% of brains from decedents who only played high school football, 91% from those who played football through college, and 99% of those who played in the National Football League. Although there may be unique circumstances in the military that confer a predisposition to repetitive or severe impact TBI that could lead to CTE, we found only 10 cases of CTE lesions in 225 brains of military personnel. The difference in CTE frequency among football players as compared with the military series reported here may provide information regarding the extent of the problem that CTE represents in the two populations. We did not have data to explore the duration or number of trauma exposures, which makes a comparison of our results with other series of CTE difficult.\n\n【44】High rates of psychiatric disorders, alcohol and substance abuse, and suicide have been found among active-duty and retired military personnel, and these complex problems were represented in our series in proportions that are similar to those in the general military population.  Because of the small number of cases of CTE in our series, our study was underpowered to explore relationships between CTE and these factors. Moreover, we also could not establish the temporal relationship between these factors and CTE. For example, we cannot confirm or refute whether psychiatric disease developed before or after CTE. Nevertheless, pathologic features of CTE were not found in most of the cases of suicide, alcohol or substance abuse, or psychiatric disease in our series.\n\n【45】This study has limitations. First, gathering accurate TBI history represents a challenge. Some aspects of TBI history in our study, including blast exposure, other military TBI, and both sports and non–sports-related civilian TBI, were obtained retrospectively from next of kin as a primary means of information in many cases and were dependent on the knowledge and recall of those sources. Obtaining accurate military TBI history is also limited by the fact that blast exposure and impact TBI often occur simultaneously from blast winds propelling objects and debris or from the person being thrown by a blast onto a stationary object.  Furthermore, certain training exercises are associated with repeated, subconcussive blast exposures (e.g., firing shoulder-mounted rocket ordnances and participating in breaching exercises).  We attempted to identify blast exposure in our series by using a combination of brains from decedents with positive reported history along with those who we inferred had had blast exposure from the nature of the their military activity (e.g., Special Forces operators and explosive ordnance disposal personnel, who have a high incidence of these exposures). These limitations are potential sources of measurement error.\n\n【46】Our series represents a convenience sample of brains donated by families. There may be acquisition bias that favors TBI history and decedents with psychiatric or behavioral symptoms that may not be representative of the general military community. The brains in this study were also not sampled in a completely uniform way; however, they were sampled extensively, with an average of 13 cortical samples per case, a process consistent with the protocols in other similar CTE studies.  Consensus guidance for CTE diagnosis recommends sampling five sites containing — or often containing — cerebral cortex,  which we exceeded. Several donated brains were not analyzed owing to poor preservation or other impediments.\n\n【47】Among 225 brains of deceased military personnel donated to a military brain bank, we found 10 cases of CTE, half of which were characterized by minimal neuropathologic changes. All the decedents with CTE had a history of participation in contact sports, with or without additional civilian impact TBI unrelated to sports. The small number of CTE cases and the wide confidence intervals for relative risk prevent definitive conclusions with regard to associations or causality of blast exposure and CTE.\n\n【48】Table 1. Clinical Characteristics and Exposures in Military Decedents with Chronic Traumatic Encephalopathy (CTE). \n\n| Case No. | Age | Military Branch (Special Forces) | Psychiatric Disorder | Alcohol or Substance Abuse | Traumatic Brain Injury | Traumatic Brain Injury | Traumatic Brain Injury | Contact Sports | Manner of Death | Lesions Related to CTE |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  | Military Blast | Military Impact | Civilian Non–Sports-Related Impact |  |  |  |\n|  |  |  |  |  |  |  |  |  |  | no. (no. of cortical slides) |\n| 1 | 31 yr | Marines (NR) | Depression | Alcohol abuse | NR | NR | MVA with LOC | American football (high school, multiple concussions), martial arts | Natural | 1 (14) |\n| 2  | 41 yr | Navy (NR) | NR | NR | NR | NR | MVA with LOC | Soccer | Natural | 1 (19) |\n| 3 | 44 yr | Navy (SEAL) | PTSD, depression, anxiety | NR | Yes | Yes | Two MVAs, one with LOC | Wrestling, boxing | Suicide | 4 (15) |\n| 4 | 46 yr | Navy (SEAL) | NR | Alcohol abuse | Yes | Yes | Assault with LOC, MVA, skydiving accident | American football (high school, college, arena), martial arts | Suicide | 5 (13) |\n| 5  | 54 yr | Army (NR) | PTSD, depression | Alcohol abuse | NR | NR | Assault with LOC | American football | Suicide | 5 (9) |\n| 6 | 57 yr | Navy (NR) | NR | Polysubstance abuse | NR | NR | MVA without LOC | Skateboarding, with associated head injury | Natural | 1 (14) |\n| 7 | 57 yr | Marines (NR) | PTSD | Alcohol abuse | NR | NR | NR | American football, basketball, baseball | Natural | 1 (14) |\n| 8  | 59 yr | Air Force (NR) | PTSD, delusional disorder | Alcohol abuse | Yes | Yes | MVA | Competitive skiing, wrestling | Suicide | 1 (18) |\n| 9  | 72 yr | Navy (NR) | NR | NR | NR | NR | MVA | Boxing with history of knockout | Natural | NA (17) |\n| 10  | 87 yr | Navy (NR) | NR | NR | NR | NR | NR | American football (junior high school through military league) | Natural | NA (11) |\n\n【50】 All cases involved brains of male service members. LOC denotes loss of consciousness, MVA motor vehicle accident, NA not assessed, NR not reported, and PTSD post-traumatic stress disorder.\n\n【51】 The history included hereditary hemorrhagic telangiectasia (the Osler–Weber–Rendu syndrome), with brain vascular malformation and previous resection. A CTE-related lesion was noted only adjacent to the resection site of a previous vascular malformation.\n\n【52】 The assault was associated with chronic post-assault mental changes.\n\n【53】 The MVA was associated with severe traumatic brain injury (closed head injury and intracranial bleeding). Skiing was associated with multiple head injuries.\n\n【54】 The MVA was associated with coma that lasted for 19 hours, temporary anterograde amnesia that lasted for 18 days, and the development of post-traumatic seizure disorder and mild cognitive impairment. The history also included dementia (diagnosed 13 years before death) with progressive cognitive decline. CTE-related lesions could not be quantified owing to severe background neuropathologic change characteristic of Alzheimer’s disease.\n\n【55】 The history included dementia. CTE-related lesions could not be quantified owing to severe background neuropathologic change characteristic of Alzheimer’s disease.\n\n【56】Table 2. Clinical Features in CTE and Non-CTE Groups. \n\n| Variable | CTE(N=10) | Non-CTE(N=215) | Total(N=225) |\n| --- | --- | --- | --- |\n| Age at death — yr | 54.8±16.0 | 47.9±13.9 |  |\n| Psychiatric disorder — no. (%) | 6 (60) | 82 (38) | 88 (39) |\n| PTSD — no. (%) | 5 (50) | 47 (22) | 52 (23) |\n| Alcohol or substance abuse — no. (%) | 6 (60) | 91 (42) | 97 (43) |\n| Suicide — no./total no. (%) | 4/10 (40) | 45/206 (22)  | 49/216 (23) |\n\n【58】 Plus–minus values are means ±SD. CTE was identified in 10 of 225 brains that underwent autopsy (4.4%; 95% confidence interval, 2.2 to 8.0).\n\n【59】 Data on cause and manner of death were available in 206 of 215 non-CTE cases.\n\n【60】Table 3. CTE and Exposure to Traumatic Brain Injury (TBI) among the 225 Military Decedents. \n\n| Exposure Variable | Total Decedents with Exposure(N=225) | Cases of CTE | Cases of CTE | Relative Risk (95% CI) |\n| --- | --- | --- | --- | --- |\n|  |  | With TBI Exposure | Without TBI Exposure |  |\n|  | no. (%) | no./total no. (%) | no./total no. (%) |  |\n| Military blast | 45 (20.0) | 3/45 (6.7) | 7/180 (3.9) | 1.71 (0.46 to 6.37) |\n| Military impact TBI | 21 (9.3) | 3/21 (14.3) | 7/204 (3.4) | 4.16 (1.16 to 14.91) |\n| History of contact sports | 60 (26.7) | 10/60 (16.7) | 0/165 | Not computable (6.16 to infinity) |\n| Civilian non–sports-related impact TBI | 44 (19.6) | 8/44 (18.2) | 2/181 (1.1) | 16.45 (3.62 to 74.79) |\n\n【62】 Confidence intervals for risk ratios were calculated with the use of the Taylor series estimate of the variance, except for one ratio (history of contact sports) with a numerator of zero, for which a confidence interval was calculated using the IRI command in STATA statistical software, version 15.0 (StataCorp). The widths of the confidence intervals have not been adjusted for multiplicity and cannot be used to infer effects.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4c036696-e482-4d8a-9271-cf47bc630e5e", "title": "Case 26-2018: A 48-Year-Old Man with Fever, Chills, Myalgias, and Rash", "text": "【0】Case 26-2018: A 48-Year-Old Man with Fever, Chills, Myalgias, and Rash\nA 48-year-old man presented with fever, chills, myalgias, and diarrhea. On evaluation, he had generalized lymphadenopathy, hepatosplenomegaly, diffuse rash, and moderate eosinophilia. Diagnostic test results were received.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "448acc5f-1a85-448c-a961-00ebf280993a", "title": "A Placebo-Controlled Trial of Oral Cladribine for Relapsing Multiple Sclerosis", "text": "【0】A Placebo-Controlled Trial of Oral Cladribine for Relapsing Multiple Sclerosis\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Cladribine provides immunomodulation through selective targeting of lymphocyte subtypes. We report the results of a 96-week phase 3 trial of a short-course oral tablet therapy in patients with relapsing–remitting multiple sclerosis.\n\n【3】Methods\n-------\n\n【4】We randomly assigned 1326 patients in an approximate :1 ratio to receive one of two cumulative doses of cladribine tablets (either 3.5 mg or 5.25 mg per kilogram of body weight) or matching placebo, given in two or four short courses for the first 48 weeks, then in two short courses starting at week 48 and week 52 (for a total of 8 to 20 days per year). The primary end point was the rate of relapse at 96 weeks.\n\n【5】Results\n-------\n\n【6】Among patients who received cladribine tablets (either 3.5 mg or 5.25 mg per kilogram), there was a significantly lower annualized rate of relapse than in the placebo group (0.14 and 0.15, respectively, vs. 0.33; P<0.001 for both comparisons), a higher relapse-free rate (79.7% and 78.9%, respectively, vs. 60.9%; P<0.001 for both comparisons), a lower risk of 3-month sustained progression of disability (hazard ratio for the 3.5-mg group, 0.67; 95% confidence interval \\[CI\\], 0.48 to 0.93; P=0.02; and hazard ratio for the 5.25-mg group, 0.69; 95% CI, 0.49 to 0.96; P=0.03), and significant reductions in the brain lesion count on magnetic resonance imaging (MRI) (P<0.001 for all comparisons). Adverse events that were more frequent in the cladribine groups included lymphocytopenia (21.6% in the 3.5-mg group and 31.5% in the 5.25-mg group, vs. 1.8%) and herpes zoster (8 patients and 12 patients, respectively, vs. no patients).\n\n【7】Conclusions\n-----------\n\n【8】Treatment with cladribine tablets significantly reduced relapse rates, the risk of disability progression, and MRI measures of disease activity at 96 weeks. The benefits need to be weighed against the risks. \n\n【9】Introduction\n------------\n\n【10】Multiple sclerosis is a chronic and debilitating autoimmune disorder of the central nervous system, in which T and B cells are believed to play a major pathophysiological role.  Treatment benefits and disease modification can be obtained with the currently approved parenteral immunomodulatory and immunosuppressant therapies: interferon beta, glatiramer acetate, mitoxantrone, and natalizumab. However, treatment responses are often less than complete, and concern regarding safety and side-effect profiles may limit the general use of these drugs. The need for parenteral administration may present relative or absolute barriers to access, limiting treatment adherence and long-term outcomes. \n\n【11】Intracellular accumulation of the active metabolite of cladribine, 2-chlorodeoxyadenosine triphosphate, results in the disruption of cellular metabolism, the inhibition of DNA synthesis and repair, and subsequent apoptosis.  Cladribine preferentially affects lymphocytes because these cells have a relatively high ratio of deoxycytidine kinase to 5′-nucleotidase and are dependent on adenosine deaminase activity to maintain the equilibrium of cellular concentrations of triphosphorylated nucleotides. The accumulation of the cladribine nucleotide produces rapid and sustained reductions in CD4+ and CD8+ cells and rapid, though more transient, effects on CD19+ B cells, with relative sparing of other immune cells.  Cladribine also has been shown to cause a reduction in the levels of proinflammatory cytokines and serum and cerebrospinal fluid chemokines, in adhesion molecule expression, and in mononuclear-cell migration. \n\n【12】In the Cladribine Tablets Treating Multiple Sclerosis Orally (CLARITY) study, we investigated the efficacy and safety of cladribine in a 96-week, phase 3, double-blind, placebo-controlled, multicenter trial involving patients with relapsing–remitting multiple sclerosis. The two doses of cladribine that we evaluated were based on the results of previous clinical studies that used a parenteral formulation of the drug in various regimens.  In order to provide an extended interim hematopoietic recovery period before subsequent retreatment, we administered cladribine in short courses within separate 48-week periods rather than administering the aggregate treatment as six to eight consecutive monthly courses.\n\n【13】Methods\n-------\n\n【14】Patients\n--------\n\n【15】From April 20, 2005, to January 18, 2007, we recruited patients from 155 clinical centers in 32 countries . Patients were eligible if they had received a diagnosis of relapsing–remitting multiple sclerosis (according to the McDonald criteria),  had lesions consistent with multiple sclerosis on magnetic resonance imaging (MRI) (according to the Fazekas criteria),  had had at least one relapse within 12 months before study entry, and had a score of no more than 5.5 on the Kurtzke Expanded Disability Status Scale (EDSS, which ranges from 0 to 10, with higher scores indicating a greater degree of disability). \n\n【16】Patients were excluded from the study if two or more previous disease-modifying therapies had failed or if they had received immunosuppressive therapy at any time before study entry or cytokine-based therapy, intravenous immune globulin therapy, or plasmapheresis within 3 months before study entry. Patients were also excluded if they had abnormal results on hematologic testing (a platelet or neutrophil count below the lower limit of the normal range or a leukocyte count of half the lower limit of the normal range) within 28 days before study entry, had a disorder that could compromise immune function (including systemic disease or infection with the human immunodeficiency virus or human T-cell lymphotropic virus), or had had a relapse within 28 days before study entry. For any patient who had received a disease-modifying drug for multiple sclerosis, a washout period of at least 3 months before study entry was required.\n\n【17】Study Design\n------------\n\n【18】Eligible patients were assigned in an approximate :1 ratio to receive one of two cumulative doses of cladribine over 96 weeks (either 3.5 mg or 5.25 mg per kilogram of body weight) or matching placebo. Randomization was performed with the use of a central system and a computer-generated treatment randomization code, with dynamic allocation by site in permuted blocks of six. The study drugs were administered orally as short courses, each consisting of one or two 10-mg cladribine tablets or matching placebo given once daily for the first 4 or 5 days of a 28-day period.\n\n【19】In the first 48-week treatment period, patients received either two courses of cladribine, followed by two courses of placebo (in the 3.5-mg group); four courses of cladribine (in the 5.25-mg group); or four courses of placebo (in the placebo group), starting at day 1 and at weeks 5, 9, and 13 (8 to 20 days of treatment). In the second 48-week period, both cladribine groups received two courses of cladribine, and the placebo group received two courses of placebo, starting at weeks 48 and 52 (8 to 10 days of treatment) . After week 24, rescue therapy with subcutaneous interferon beta-1a (at a dose of 44 μg three times per week) was available if a patient had more than one relapse or a sustained increase in the EDSS score.\n\n【20】The study was conducted in accordance with relevant clinical guidelines . All patients provided written informed consent.\n\n【21】Study Oversight\n---------------\n\n【22】The protocol was reviewed and approved by the local review board or ethics committee at each study center. An independent data and safety monitoring board reviewed the study conduct and all safety data. Data were gathered by an independent commercial research organization and analyzed by the sponsor (Merck Serono) in accordance with the statistical plan. MRI data were analyzed by an independent commercial research organization at a central reading center. The authors were involved in all stages of development and finalization of the manuscript and were assisted by an independent medical-writing-services agency paid by Merck Serono. The first draft of the manuscript was cowritten by the lead academic author and a representative of the sponsor, with the medical-writing-services agency providing support as directed. The authors vouch for the completeness and accuracy of the data and analyses.\n\n【23】Study Procedures\n----------------\n\n【24】To maintain the double-blind nature of the study, all patients within a weight range received the same number of tablets (cladribine or matched placebo). In addition, at each study site, a treating physician reviewed clinical laboratory results and assessed treatment-emergent adverse events and safety information, and an independent evaluating physician who was unaware of study-group assignments performed neurologic examinations and determined whether a clinical event fulfilled criteria consistent with a relapse. Evaluators at a central neuroradiology center assessed MRI evaluations in a blinded fashion.\n\n【25】Neurologic examinations included the EDSS evaluation,  which was conducted at the prestudy evaluation and at day 1 and at weeks 13, 24, 36, 48, 60, 72, 84, and 96. MRI scans were obtained at the prestudy evaluation and at weeks 24, 48, and 96. Clinical laboratory tests, including chemical and hematologic analyses and urinalysis, were performed by a central laboratory at frequent intervals during the 96-week study (for details, see the Supplementary Appendix ). For suspected relapses occurring between study visits, patients were required to attend the study site within 7 days after the onset of neurologic symptoms for objective assessment by the evaluating physician in a blinded fashion. Relapses could be treated with intravenous corticosteroids at the discretion of the treating physician.\n\n【26】Primary and Secondary End Points\n--------------------------------\n\n【27】The primary end point was the rate of relapse at 96 weeks. A relapse was defined as an increase of 2 points in at least one functional system of the EDSS or an increase of 1 point in at least two functional systems (excluding changes in bowel or bladder function or cognition) in the absence of fever, lasting for at least 24 hours and to have been preceded by at least 30 days of clinical stability or improvement.\n\n【28】Key clinical secondary efficacy end points were the proportion of patients who were relapse-free and the time to sustained progression of disability, which was defined as the time to a sustained increase (for at least 3 months) of at least 1 point in the EDSS score or an increase of at least 1.5 points if the baseline EDSS score was 0. Additional clinical efficacy end points included the time to the first relapse and the proportion of patients receiving rescue therapy with interferon beta-1a. Secondary MRI end points were the mean number of lesions per patient per scan at 96 weeks for gadolinium-enhancing T <sub>1 </sub> \\-weighted lesions, active T <sub>2 </sub> \\-weighted lesions, and combined unique lesions, which were defined as new gadolinium-enhancing T <sub>1 </sub> \\-weighted lesions or new nonenhancing or enlarging T <sub>2 </sub> \\-weighted lesions (without double-counting).\n\n【29】The safety assessment included a review of the incidence of treatment-emergent adverse events in each study group, physical examination, and laboratory measurements. A strict protocol was established for the management of hematologic events .\n\n【30】Statistical Analysis\n--------------------\n\n【31】We determined that 1290 patients (approximately 430 in each group) were required to provide a power of 90% to detect a clinically meaningful relative reduction of 25% in the relapse rate in the cladribine groups, as compared with the placebo group, at 96 weeks (the primary end point). This was calculated with the use of a two-sided t-test on the assumption that a mean number of 2.1 relapses would occur in the placebo group, that the standard deviation for the number of relapses in each group would be 2.02, that the proportion of patients who could not be evaluated would be 10%, and that the two-sided type I error rate for the comparison between each cladribine group and the placebo group would be 2.5%.\n\n【32】The intention-to-treat population included all patients who underwent randomization, and the safety population included all patients who received at least one dose of a study drug and for whom follow-up safety data were available. The primary efficacy measurement was analyzed with the use of a Poisson regression model including effects for treatment and region and the log of time in the study as the offset variable. The study groups were compared by means of an approximate chi-square test on the basis of Wald statistics and Hochberg's step-up method for multiple comparisons to protect the type I error.\n\n【33】For patients who received rescue therapy, the primary and secondary efficacy analyses included the prerescue data and imputed data from the time of rescue onward, according to prespecified methods in the statistical analysis plan. For the primary end point, imputed data were derived only from patients in the placebo group.\n\n【34】In the analysis of secondary end points, the proportions of patients who were relapse-free and progression-free were analyzed with the use of a logistic-regression model that included study-group and region effects, and odds ratio and 95% confidence intervals were estimated for each study group. The three study groups were compared with the use of an approximate chi-square test on the basis of Wald statistics. The time to the first relapse and the time to a 3-month sustained change in the EDSS score were analyzed with the use of a Cox proportional-hazards model that included study-group and region effects, and the hazard ratio for the time to the first relapse and the time to a 3-month sustained change in the EDSS score in each group and associated 95% confidence intervals were estimated. Kaplan–Meier plots of the time to the first relapse and the time to a 3-month sustained change in the EDSS score were also generated.\n\n【35】Secondary end points that were related to lesion counts on MRI were analyzed with the use of a nonparametric analysis-of-covariance model on ranked data with effects for study group and region adjusted for baseline counts of gadolinium-enhancing T1 lesions. To protect the overall family-wise type I error rate of 5%, dose groups that differed significantly from placebo for the primary efficacy measures were compared with placebo for the three secondary MRI measurements with the use of a hierarchical testing procedure that was based on the Hochberg procedure. The sequential testing of the measurements was carried out only if the test for the previous measurement was significant.\n\n【36】Sensitivity analyses were conducted to assess the effect of baseline differences in disease characteristics on efficacy outcome measures. The results of these analyses are not reported here, since no material effects were shown. No interim analyses were conducted for this study.\n\n【37】Results\n-------\n\n【38】Patients\n--------\n\n【39】Table 1. Demographic and Clinical Characteristics of the Patients at Baseline (Intention-to-Treat Population).\n\n【40】The demographic and clinical characteristics of the intention-to-treat population of 1326 patients was generally well balanced across the three study groups, although patients receiving 3.5 mg of cladribine per kilogram had a shorter mean duration of disease (P=0.005 for the overall comparison) . Almost one third of patients had previously received disease-modifying therapy. Overall, 1184 patients (89.3%) completed the 96-week study (91.9% in the cladribine 3.5-mg group, 89.0% in the cladribine 5.25-mg group, and 87.0% in the placebo group) . A total of 1165 patients (87.9%) completed treatment (91.2%, 86.2%, and 86.3%, respectively). The mean time of participation in the study was 89.4 weeks (91.0, 89.4, and 87.8 weeks, respectively).\n\n【41】Primary and Secondary End Points\n--------------------------------\n\n【42】Table 2. Clinical and Imaging End Points and Relapses during the 96-week Study (Intention-to-Treat Population). Figure 1.  Figure 1. Efficacy Outcome Measures Relating to Relapse and Progression of Disability during the 96-Week Study Period (Intention-to-Treat Population).\n\n【43】Shown are the annualized rates of relapse , Kaplan–Meier curves of the time to the first relapse , the cumulative number of relapses over time , and Kaplan–Meier curves of the time to 3-month sustained progression of disability, according to scores on the Expanded Disability Status Scale (EDSS) . In Panel A, the T bars represent 95% confidence intervals. P values that are shown in Panels B and D are for hazard ratios and 95% confidence intervals during the 96-week period, as estimated with the use of a Cox proportional-hazards model with fixed effects for study group and region.\n\n【44】The annualized relapse rate at 96 weeks was significantly reduced in both cladribine groups, as compared with the placebo group (0.14 in the cladribine 3.5-mg group and 0.15 in the cladribine 5.25-mg group, vs. 0.33 in the placebo group), for relative reductions of 57.6% and 54.5%, respectively (P<0.001 for both comparisons) . The proportion of patients who remained relapse-free at 96 weeks was significantly higher in both cladribine groups (79.7% and 78.9%, respectively), as compared with the placebo group (60.9%) (P<0.001 for both comparisons) . In addition, the time to the first relapse was longer in both cladribine groups (hazard ratio in the 3.5-mg group, 0.44; 95% confidence interval \\[CI\\], 0.34 to 0.58; P<0.001; and hazard ratio in the 5.25-mg group, 0.46; 95% CI, 0.36 to 0.60; P<0.001 for both comparisons) . These improvements were achieved with a reduction in the odds of receiving rescue therapy with interferon beta-1a in the cladribine 3.5-mg group (60%) and the cladribine 5.25-mg group (69%), as compared with placebo (P=0.01 and P=0.003, respectively) .\n\n【45】During the 96-week study, there was a relative reduction in the risk of 3-month sustained progression of disability in both cladribine groups, as compared with placebo, with a 33% reduction in the cladribine 3.5-mg group (hazard ratio, 0.67; 95% CI, 0.48 to 0.93; P=0.02) and a 31% reduction in the cladribine 5.25-mg group (hazard ratio, 0.69; 95% CI, 0.49 to 0.96; P=0.03) . There were corresponding increases in the odds for remaining free of 3-month sustained disability progression in both cladribine groups, as compared with placebo (P=0.02 for the 3.5-mg group and P=0.03 for the 5.25-mg group) .\n\n【46】Cladribine treatment resulted in significant reductions in measures of MRI activity, as compared with placebo. Patients in the cladribine 3.5-mg group and cladribine 5.25-mg group had fewer lesions per patient per scan than those in the placebo group for gadolinium-enhancing T1 lesions (mean number, 0.12 and 0.11, respectively, vs. 0.91), active T2 lesions (mean number, 0.38 and 0.33, respectively, vs. 1.43), and combined unique lesions (mean number, 0.43 and 0.38, respectively, vs. 1.72) (P<0.001 for all comparisons vs. placebo) .\n\n【47】Adverse Events\n--------------\n\n【48】Table 3. Adverse Events and Investigator-Assessed Severity at 96 Weeks (Safety Population). Table 4.  Table 4. Effects of Cladribine on Lymphocyte Counts, According to Time Point (Safety Population).\n\n【49】Lymphocytopenia (mostly graded as mild or moderate) was reported more frequently among patients receiving cladribine than among those receiving placebo . Severe neutropenia (as rated by the investigators) was reported in three patients receiving cladribine (one in the 3.5-mg group and two in the 5.25-mg group), with severe thrombocytopenia and pancytopenia in one of the patients in the latter group, who also had an exacerbation of latent tuberculosis . There were no cases of severe anemia. The effects of cladribine on lymphocyte counts in the first and second 48-week periods are presented in Table 4 . Fig. 3 in the Supplementary Appendix shows the effects of therapy on lymphocyte and neutrophil counts over the duration of the study. Maximal effects on lymphocyte, neutrophil, and platelet counts and hemoglobin levels are presented, according to laboratory criteria of the National Cancer Institute's Common Terminology Criteria for Adverse Events, in Table 1 in the Supplementary Appendix .\n\n【50】Infections or infestations were reported in 47.7% of the patients in the cladribine 3.5-mg group, 48.9% of those in the cladribine 5.25-mg group, and 42.5% of those in the placebo group, with most of the events graded as mild or moderate by investigators (99.6% and 98.6%, respectively, vs. 99.0%). Herpes zoster infections developed in 20 patients who received cladribine (including 1 with herpes zoster oticus): 8 patients in the 3.5-mg group and 12 in the 5.25-mg group. All cases of herpes zoster were restricted and dermatomal in nature. There were three cases of primary varicella (one in each study group), all of which resolved without complication. Correlation analysis suggested that the lowest absolute lymphocyte counts in patients receiving cladribine were inversely correlated with the occurrence of infection (Spearman's correlation coefficient for both cladribine groups combined, –0.10; P=0.003).\n\n【51】Adverse events led to treatment discontinuation for 3.5% of patients in the cladribine 3.5-mg group, 7.9% of those in the cladribine 5.25-mg group, and 2.1% of those in the placebo group. The occurrence of lymphocytopenia and leukocytopenia led to treatment discontinuation in 0.9%, 4.2%, and no patients, respectively. The three study groups had similar rates of other events leading to the discontinuation of a study drug.\n\n【52】The incidence of serious adverse events was 8.4% in the cladribine 3.5-mg group, 9.0% in the cladribine 5.25-mg group, and 6.4% in the placebo group . Infections or infestations were reported as serious adverse events in 2.3%, 2.9%, and 1.6% of patients, respectively. Herpes zoster was reported as a serious adverse event for three patients receiving cladribine (two in the 5.25-mg group). The occurrence of neoplasms (including those that were found to be benign, malignant, or unspecified) was reported as a serious adverse event in 1.4% of patients in the cladribine 3.5-mg group and in 0.9% of those in the cladribine 5.25-mg group, as compared with no patients in the placebo group. These included five benign uterine leiomyomas that required inpatient hospital visits for treatment. There were three cases of cancer in the cladribine 3.5-mg group: a melanoma and carcinomas of the pancreas and ovary. A case of stage 0 cervical carcinoma in situ (considered a precancerous condition) was also reported in the cladribine 5.25-mg group. The time from the last course of therapy to diagnosis in these four patients was 2 months, 6 months, <9 months, and 7 months, respectively. For the patient with cervical carcinoma in situ, the relevant medical history included a positive test for human papillomavirus type 16 at 3 years before diagnosis. A choriocarcinoma was diagnosed in one patient in the cladribine 5.25-mg group approximately 9 months after completion of the study.\n\n【53】There were four deaths during the study and two after study discontinuation, equally distributed across the three study groups. Causes of death were acute myocardial infarction and metastatic pancreatic carcinoma in the cladribine 3.5-mg group, drowning and cardiopulmonary arrest that was considered secondary to exacerbation of latent tuberculosis in the cladribine 5.25-mg group, and suicide and hemorrhagic stroke in the placebo group (for details, see the Supplementary Appendix ).\n\n【54】Discussion\n----------\n\n【55】Our study showed that short-course therapy with cladribine tablets provided rapid and sustained treatment benefits for patients with relapsing–remitting multiple sclerosis during a 96-week period. As compared with placebo, treatment with cladribine resulted in reductions in the rates of clinical relapse and in the risk of disability progression and in the suppression of active inflammatory lesions, as visualized on MRI. Both the regimens of 3.5 mg and 5.25 mg per kilogram appeared to be equally efficacious.\n\n【56】The most commonly reported adverse event was lymphocytopenia. There was an inverse correlation between the incidence of infection and a patient's lowest absolute lymphocyte count in the combined cladribine groups. Activation of latent herpes zoster occurred in 20 cladribine-treated patients. One patient who was treated with cladribine had reactivation of latent tuberculosis and died. The use of cladribine may have contributed to this reactivation, and tuberculosis screening measures were subsequently implemented in ongoing clinical trials to rule out latent or active infection before treatment or retreatment. Cancers were isolated cases across different organ systems, and given the small number, it is not possible to establish a risk for the use of cladribine.\n\n【57】In conclusion, our study showed that short-course treatment with cladribine tablets for only 8 to 20 days per year provided a significant benefit for patients with relapsing–remitting multiple sclerosis with respect to the rate of relapse, disability progression, and MRI measures of disease activity during the 96-week study period. The benefits of treatment will need to be weighed against the risks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e559f350-b70a-4b92-921b-42f6dd92597e", "title": "Pharmacologic Control of Thromboembolic Complications of Cardiac-Valve Replacement", "text": "【0】Pharmacologic Control of Thromboembolic Complications of Cardiac-Valve Replacement\nAbstract\n--------\n\n【1】Continued experience with the use of dipyridamole as an antithrombotic agent confirmed earlier impressions of the clinical effectiveness of this agent. Dipyridamole (400 mg daily) or a placebo was given to 163 patients surviving prosthetic cardiac-valve replacement. All patients received anticoagulation with warfarin sodium. Eighty-four patients entered the placebo group. Within one year, 11.9 per cent died, 14.3 per cent suffered arterial emboli, and 9.5 per cent discontinued their medication because of possible side effects. The dipyridamole group contained 79 patients, of whom 13.9 per cent died; arterial emboli developed in 1.3 per cent and 15.2 per cent discontinued their medication. The frequency of embolization in the dipyridamole group was significantly lower than that in the placebo group. There was no significant difference between the death rates or between the drug discontinuation rates of the two groups.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3c54d2be-b57b-4928-b92d-0285eac1b043", "title": "Hereditary Thrombocytopenia with an Intrinsic Platelet Defect", "text": "【0】Hereditary Thrombocytopenia with an Intrinsic Platelet Defect\nAbstract\n--------\n\n【1】Thrombocytopenia, inherited as a dominant trait, was present in three generations of a kindred. The clinical picture was quite similar to that of idiopathic thrombocytopenic purpura, but thrombokinetic studies clearly distinguished the two diseases. Platelet-survival studies after labeling with  Cr demonstrated shortened life-spans of the patients' platelets both in themselves and in normal volunteers. Platelets from healthy volunteers, however, survived normally in the patients, showing that the accelerated platelet destruction resulted from an intrinsic platelet defect and not one extrinsic to the cell. Morphologic and biochemical studies have not as yet elucidated the nature of the defect. Splenectomy in two patients was followed by improvement in the thrombocytopenia, but the postoperative platelet survival remained short.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8ccefeb7-67b1-4466-af8e-f5cc39b0e08c", "title": "Influenza and the Rates of Hospitalization for Respiratory Disease among Infants and Young Children", "text": "【0】Influenza and the Rates of Hospitalization for Respiratory Disease among Infants and Young Children\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Young children may be at increased risk for serious complications from influenzavirus infection. However, in population-based studies it has been difficult to separate the effects of influenzavirus from those of respiratory syncytial virus. Respiratory syncytial virus often circulates with influenzaviruses and is the most frequent cause of hospitalization for lower respiratory tract infections in infants and young children. We studied the rates of hospitalization for acute respiratory disease among infants and children during periods when the circulation of influenzaviruses predominated over the circulation of respiratory syncytial virus.\n\n【3】Methods\n-------\n\n【4】For each season from October to May during the period from 1992 to 1997, we used local viral surveillance data to define periods in Washington State and northern California when the circulation of influenzaviruses predominated over that of respiratory syncytial virus. We calculated the rates of hospitalization for acute respiratory disease, excess rates attributable to influenzavirus, and incidence-rate ratios for all infants and children younger than 18 years of age who were enrolled in either the Kaiser Permanente Medical Care Program of Northern California or the Group Health Cooperative of Puget Sound.\n\n【5】Results\n-------\n\n【6】The rates of hospitalization for acute respiratory disease among children who did not have conditions that put them at high risk for complications of influenza (e.g., asthma, cardiovascular diseases, or premature birth) and who were younger than two years of age were 231 per 100,000 person-months at Northern California Kaiser sites (from 1993 to 1997) and 193 per 100,000 person-months at Group Health Cooperative sites (from 1992 to 1997). These rates were approximately 12 times as high as the rates among children without high-risk conditions who were 5 to 17 years of age (19 per 100,000 person-months at Northern California Kaiser sites and 16 per 100,000 person-months at Group Health Cooperative sites) and approached the rates among children with chronic health conditions who were 5 to 17 years of age (386 per 100,000 person-months and 216 per 100,000 person-months, respectively).\n\n【7】Conclusions\n-----------\n\n【8】Infants and young children without chronic or serious medical conditions are at increased risk for hospitalization during influenza seasons. Routine influenza vaccination should be considered in these children.\n\n【9】Introduction\n------------\n\n【10】Annual vaccination against influenza is recommended for all persons six months of age or older who have chronic conditions that increase their risk of complications from influenza.  During past epidemics of influenza, hospitalization rates among high-risk children have ranged from 200 to 500 per 100,000 persons.  During the 1970s and 1980s, Mullooly and Barker  and Glezen et al.  showed that hospitalization rates for children younger than five years of age who had no known high-risk conditions were elevated during winter months when influenzaviruses were in circulation. However, these studies did not address the possibility that some of the hospitalizations resulted from other respiratory virus infections, most notably respiratory syncytial virus. Respiratory syncytial virus frequently circulates with influenzaviruses in the winter,  is the primary cause of lower respiratory tract disease among young children,  and results in an estimated 84,000 to 144,000 hospitalizations annually for lower respiratory tract disease among U.S. children younger than five. \n\n【11】Our objective was to determine the effect of influenza on hospitalizations for acute respiratory disease in young children. We studied a period of several years because the impact of influenza can vary substantially between seasons depending on several factors, including the overall prevalence of infections, the proportion of circulating influenzavirus types and subtypes, the virulence of circulating strains, and the protective antibody levels in the population.  We studied hospitalizations for acute respiratory disease because influenzavirus infections frequently remain undiagnosed, even in hospitalized patients, and can precipitate secondary complications, including bacterial infections and exacerbations of chronic conditions that lead to hospitalization. \n\n【12】Methods\n-------\n\n【13】Study Period and Population\n---------------------------\n\n【14】We evaluated data for the period from 1992 to 1997 and included children younger than 18 years of age who had been enrolled continuously for at least one year before the start of the study or since birth in either the Kaiser Permanente Medical Care Program of Northern California, Oakland (Northern California Kaiser), or the Group Health Cooperative of Puget Sound, Seattle (Group Health Cooperative). The average annual numbers of participants from Northern California Kaiser and Group Health Cooperative were 250,892 and 71,705, respectively.\n\n【15】Northern California Kaiser has sites in the San Francisco Bay area and serves over 2,300,000 members annually. Twenty-five percent of this population is younger than 18 years of age; 40 percent are white, 19 percent Hispanic, 19 percent of mixed racial or ethnic background, 13 percent Asian, and 8 percent black; the racial or ethnic background of 1 percent is unknown. Most Group Health Cooperative sites are located in the Seattle area, and the organization serves over 340,000 members. Twenty-six percent of this population is younger than 18 years; 81 percent are white, 1 percent Hispanic, 8 percent Asian, 7 percent black, and 1 percent Native American; the racial or ethnic background of 2 percent is unknown.\n\n【16】Hospitalization and Medical Data\n--------------------------------\n\n【17】Hospitalization data were obtained through the Vaccine Safety Datalink. This project connects automated clinical data bases from Northern California Kaiser, Group Health Cooperative, and two other West Coast managed-care organizations and was started in 1991 by the National Immunization Program, Centers for Disease Control and Prevention (CDC). \n\n【18】Data on patients included demographic characteristics, vaccination status, prescribed medications, hospital discharges, emergency room visits, and visits to outpatient clinics. Complete data were obtained for Group Health Cooperative participants for the entire study period. For Northern California Kaiser participants, complete data were available for the influenza seasons from 1995 to 1996 and from 1996 to 1997. For the influenza seasons from 1993 to 1994 and from 1994 to 1995, data were limited to children who were younger than seven years of age and who were seen at three clinics (that serve about 15 percent of the total membership); no data were available for the period from 1992 to 1993.\n\n【19】Determination of Health Status\n------------------------------\n\n【20】Children were considered to be at high risk for serious complications from influenza if they had been hospitalized or had visited an outpatient clinic or emergency room during the previous year for any chronic or serious condition (including chronic pulmonary, cardiovascular, metabolic, rheumatic, renal, neurologic, immunosuppressive, and hematologic diseases and premature birth).  Pharmacologic data, which were available for Group Health Cooperative members for the entire study period and for Northern California Kaiser members for the period from 1993 to 1996, were used to identify children with asthma.  We considered children without identifiable high-risk conditions at the time of hospitalization to be otherwise healthy.\n\n【21】Definition of Study Periods\n---------------------------\n\n【22】Local virologic surveillance data were used to determine when influenzaviruses and respiratory syncytial virus were in circulation at each site. Data for the San Francisco Bay area were obtained from the National Respiratory and Enteric Virus Surveillance System of the CDC,  the Northern California Kaiser virology laboratory, Stanford Health Services, and the University of California, San Francisco, Mt. Zion Medical Center. Data for the Seattle area were obtained from the National Influenza Virologic Surveillance System of the CDC  and the National Respiratory and Enteric Virus Surveillance System.\n\n【23】### _Period When Influenzavirus Predominated_\n\n【24】For each season from October to May during the study period, we identified all periods of two or more consecutive weeks in which each week accounted for at least 5 percent of the season's total number of influenzavirus isolates and less than 5 percent of the total number of positive tests for respiratory syncytial virus. All such weeks during the entire study were combined and together were defined as the period in which influenzavirus was predominant.\n\n【25】### _Period of Extended Influenzavirus Circulation_\n\n【26】We also identified a longer period during which influenzaviruses were in circulation and the circulation of other respiratory viruses was not considered. We identified all periods of two or more consecutive weeks in which each week accounted for at least 5 percent of the season's total number of influenzavirus isolates. All such weeks during the entire study were combined and together were defined as the extended period of influenzavirus.\n\n【27】### _Period When Respiratory Syncytial Virus Predominated_\n\n【28】For each season from October to May during the study period, we identified all periods of two or more consecutive weeks in which each week accounted for at least 5 percent of the season's total number of positive tests for respiratory syncytial virus and less than 5 percent of the season's total number of influenzavirus isolates. All such weeks during the entire study were combined and together were defined as the period in which respiratory syncytial virus predominated.\n\n【29】### _Peri-Seasonal Base-Line Period_\n\n【30】For each season from October to May during the study period, we identified all periods of two or more consecutive weeks in which each week accounted for less than 5 percent of the season's total number of influenzavirus isolates and less than 5 percent of the total number of positive tests for respiratory syncytial virus and in which no isolates of parainfluenza virus type 1 or 3 were identified. All such weeks during the entire study were combined and together were defined as the peri-seasonal base-line period.\n\n【31】### _Summer Base-Line Period_\n\n【32】For each interval from June to September during the study period, we identified all periods of two or more consecutive weeks in which no isolates of influenzavirus, respiratory syncytial virus, or parainfluenza virus type 1 or 3 were detected. All such weeks during the entire study were combined and together were defined as the summer base-line period.\n\n【33】Study Outcomes\n--------------\n\n【34】The main outcomes of the study were hospitalizations for acute respiratory disease in which codes 460 to 496 or 510 to 519 from the _International Classification of Diseases, 9th revision, Clinical Modification_ were listed as a discharge diagnosis.  These codes excluded respiratory tract diseases resulting from the inhalation of asbestos, dust, or chemical fumes or the aspiration of food.\n\n【35】Statistical Analysis\n--------------------\n\n【36】For each study site and year, we computed rates of hospitalization for acute respiratory disease and incidence-rate ratios per 100,000 person-months according to age and health status for periods in which influenzavirus predominated and periods in which respiratory syncytial virus predominated. As reference values, we used hospitalization rates for children 5 to 17 years of age who had no identifiable high-risk conditions during the same period. We used exact two-sided P values and 95 percent confidence intervals to evaluate differences between the groups.  For each age group, we also calculated the excess rates of hospitalization attributable to influenzavirus by subtracting peri-seasonal rates and rates during the summer base-line period from the rates during periods in which influenzavirus predominated. We performed statistical analyses using StatXact software. \n\n【37】Results\n-------\n\n【38】Periods When Influenzavirus Predominated and Extended Periods of Influenzavirus\n-------------------------------------------------------------------------------\n\n【39】Figure 1. Isolates of Influenzavirus and Positive Tests for Respiratory Syncytial Virus (RSV) as a Percentage of All Positive Isolates and Tests in the Seattle Area  and the San Francisco Bay Area  from October 1993 to May 1994 and October 1996 to May 1997.\n\n【40】The horizontal lines over the graphs represent the consecutive periods in which influenzavirus and respiratory syncytial virus predominated (i.e., accounted for at least 5 percent of all viral isolates in a season).\n\n【41】In the San Francisco Bay area from 1993 to 1997, a total of 3638 respiratory viruses were identified, of which 515 were influenzavirus isolates (ranging from 41 in the period from 1994 to 1995 to 228 in the period from 1996 to 1997) and 3029 were identified as respiratory syncytial virus (mostly through the use of rapid antigen tests). During the entire period from 1993 to 1997, there were 13 weeks during which the circulation of influenzaviruses predominated relative to that of respirato-ry syncytial virus. During individual seasons from October to May, the length of the periods in which influenzavirus predominated ranged from two weeks in the period from 1994 to 1995 (data not shown) to six weeks in the period from 1993 to 1994 .\n\n【42】For the Seattle area from 1992 to 1997, a total of 4883 respiratory viruses were identified, of which 1285 were influenzavirus isolates (ranging from 101 in the period from 1994 to 1995 to 454 in the period from 1996 to 1997) and 2584 were respiratory syncytial virus. During the entire period from 1992 to 1997, there were 24 weeks during which the circulation of influenzaviruses predominated relative to that of respiratory syncytial virus. The length of the periods in which influenzavirus predominated ranged from two weeks in the period from 1994 to 1995 (data not shown) to nine weeks in the period from 1996 to 1997 .\n\n【43】When data from both sites were combined, the average annual duration of the extended period of influenzavirus was 7.3 weeks (51 days).\n\n【44】Prevalence of High-Risk Conditions\n----------------------------------\n\n【45】Table 1. Distribution of High-Risk Conditions in the Study Populations.\n\n【46】Among all participants at both sites, 9.7 percent had at least one identifiable high-risk condition . Asthma was the most common condition and was diagnosed in 8.3 percent of all participants .\n\n【47】Hospitalization Rates during Periods When Influenzavirus Predominated\n---------------------------------------------------------------------\n\n【48】During the periods in which influenzavirus predominated, hospitalization rates for acute respiratory disease among children with high-risk conditions at Northern California Kaiser sites were 1181 per 100,000 person-months for children younger than 2 years of age, 713 per 100,000 person-months for children 2 to 4 years of age, and 386 per 100,000 person-months for children 5 to 17 years of age. The rates among high-risk children at Group Health Cooperative sites were 772 per 100,000 person-months for children younger than 2 years of age, 458 per 100,000 person-months for children 2 to 4 years of age, and 216 per 100,000 person-months for children 5 to 17 years of age.\n\n【49】Table 2. Rates of Hospitalization for Acute Respiratory Disease among Children without High-Risk Conditions.\n\n【50】During the periods in which influenzavirus predominated, hospitalization rates for acute respiratory disease among children without high-risk conditions at Northern California Kaiser sites were 231 per 100,000 person-months for children younger than 2 years of age, 53 per 100,000 person-months for children 2 to 4 years of age, and 19 per 100,000 person-months for children 5 to 17 years of age . The rates among children without high-risk conditions at Group Health Cooperative sites were 193 per 100,000 person-months for children younger than 2 years of age, 21 per 100,000 person-months for children 2 to 4 years of age, and 16 per 100,000 person-months for children 5 to 17 years of age .\n\n【51】Table 3. Relative Risk of Hospitalization for Acute Respiratory Disease among Children without High-Risk Conditions during Periods in Which Influenzavirus Predominated.\n\n【52】At each site, rates of hospitalization for acute respiratory disease among children without high-risk conditions who were younger than 2 years of age were 12 times as high as those for children who were 5 to 17 years of age, and these differences were statistically significant . Among children without high-risk conditions who were 2 to 4 years of age, hospitalization rates were significantly higher than those among children 5 to 17 years of age at Northern California Kaiser sites but not at Group Health Cooperative sites . There were no significant differences between the sexes in the relative risk of hospitalization at any age (data not shown).\n\n【53】Hospitalization Rates of Children without High-Risk Conditions during Other Periods\n-----------------------------------------------------------------------------------\n\n【54】During the periods of extended influenzavirus circulation, the rates of hospitalization for acute respiratory disease among children without high-risk conditions who were younger than 2 years of age were 350 per 100,000 person-months at Northern California Kaiser sites and 225 per 100,000 person-months at Group Health Cooperative sites. During the periods in which respiratory syncytial virus predominated, the respective rates were 309 and 372 per 100,000 person-months.\n\n【55】Comparison of Hospitalization Rates\n-----------------------------------\n\n【56】In each age group, children with high-risk conditions were significantly more likely to be hospitalized than children without high-risk conditions. During the periods in which influenzavirus predominated, relative risks of hospitalizations for acute respiratory disease among children with high-risk conditions, as compared with children without high-risk conditions, were 5 (95 percent confidence interval, 4 to 7) at Northern California Kaiser sites and 4 (95 percent confidence interval, 3 to 6) at Group Health Cooperative sites for children younger than 2 years of age; 13 (95 percent confidence interval, 9 to 19) and 21 (95 percent confidence interval, 11 to 41), respectively, for children 2 to 4 years of age; and 20 (95 percent confidence interval, 15 to 26) and 13 (95 percent confidence interval, 9 to 19), respectively, for children 5 to 17 years of age.\n\n【57】Excess Rates of Hospitalization Attributable to Influenzavirus\n--------------------------------------------------------------\n\n【58】Table 4. Excess Rates of Hospitalization for Acute Respiratory Disease Attributable to Influenzavirus among Children without High-Risk Conditions during Periods in Which Influenzavirus Predominated.\n\n【59】Among children without high-risk conditions at Northern California Kaiser sites and Group Health Cooperative sites, the excess rates of hospitalization attributable to influenzavirus with use of the base-line rates from the summer periods were 151 and 127 per 100,000 person-months, respectively, for children younger than 2 years of age; 26 and 5 per 100,000 person-months, respectively, for children 2 to 4 years of age; and 0 and 5 per 100,000 person-months, respectively, for children 5 to 17 years of age . When peri-seasonal base-line rates were used, the excess rates of hospitalization attributable to influenzavirus were significantly elevated for children younger than two years of age at both sites but not children in other age groups .\n\n【60】Discussion\n----------\n\n【61】Seasonal epidemics of influenza cause a disproportionate number of serious complications among the elderly and among persons of any age who have certain chronic conditions.  In our study, children with chronic medical conditions were 4 to 21 times as likely to be hospitalized for an acute respiratory disease as children of the same age without such conditions during periods when influenzaviruses predominated. These findings strongly support current recommendations of the Advisory Committee on Immunization Practices and the American Academy of Pediatrics to vaccinate children with high-risk conditions against influenza annually.  Despite such recommendations, a recent study found that only 8.9 percent of children with asthma, the predominant high-risk condition in children, had received an influenza vaccination in the period from 1993 to 1995. \n\n【62】In contrast to the situation in children with high-risk conditions, it has been uncertain whether young age alone increases the risk of serious complications from influenza.  Although earlier studies demonstrated increased hospitalization rates among children during winter months when influenzaviruses were in circulation, these studies did not consider the possible effect of other respiratory virus infections on population-based rates of hospitalization.\n\n【63】Among the noninfluenza respiratory viruses, respiratory syncytial virus has been associated most frequently with lower respiratory tract disease in children.  The potential confounding effect of the parainfluenza viruses was of less concern to us, because type 1 parainfluenza viruses circulate in odd-numbered years and primarily during the fall,  whereas type 3 parainfluenza viruses circulate annually but usually during the early spring.  In addition, less than 1 percent of the respiratory viruses identified through local surveillance were type 2 parainfluenza viruses, and most were identified outside the periods in which we defined influenzavirus to be predominant (CDC: unpublished data).\n\n【64】In contrast, hospitalizations associated with respiratory syncytial virus infections were of great concern to us.  In several studies, both respiratory syncytial virus and influenzaviruses have been recovered from young children hospitalized for acute respiratory infections during the same periods.  In many of these studies, respiratory syncytial virus was detected more often than influenzaviruses; however, none of these studies used population-based denominators. We attempted to minimize the potential confounding from respiratory syncytial virus in our analysis by focusing on periods when the circulation of influenzaviruses predominated over the circulation of respiratory syncytial virus.\n\n【65】During the periods in which influenzaviruses predominated, the rates of hospitalization for acute respiratory disease among children without high-risk conditions were approximately 12 times as high among those younger than 2 years of age than among older children and were similar to the rates among children with high-risk medical conditions who were 5 to 17 years of age. In contrast, our data did not convincingly demonstrate that children without high-risk conditions who were two to four years of age had an elevated risk of hospitalization.\n\n【66】The validity and strength of our findings are supported by several considerations. We found similar results in two managed-care organizations located in different areas and serving memberships with different racial and ethnic compositions. Data were collected over a period of five years, which was important because the effect of influenza can vary considerably from season to season.  Most of the hospitalizations occurred during the periods from 1993 to 1994 and from 1996 to 1997, when influenza A/Beijing/32/92-like (H3N2) viruses and A/Wuhan/359/95-like (H3N2) viruses, respectively, predominated in the United States.  Both these viruses have been associated with high levels of influenza-associated morbidity and mortality.  Nonetheless, our study design reduced but could not eliminate the effect of hospitalizations associated with noninfluenzavirus infections.\n\n【67】To assess the relevance of our findings for influenza-vaccine policy, we estimated the excess rates of hospitalization attributable to influenzavirus among children without high-risk conditions. These rates represent hospitalizations that might have been avoided by influenza vaccination. We used both summer and peri-seasonal base-line rates to estimate rates of hospitalization attributable to influenzavirus, because the choice of the base-line period can substantially affect such results. Various studies have used summer periods,  winter weeks with low levels of influenzavirus in circulation,  or entire winters during years in which the levels of influenzavirus in circulation were low as base-line periods.  On the basis of the rates of hospitalization attributable to influenzavirus, the average length of the extended periods of influenzavirus, and the estimated number of children without high-risk conditions who were between the ages of 6 and 24 months (according to 1999 U.S. Census Bureau population estimates), we estimate that 8400 (using a peri-seasonal base line) to 11,700 (using a summer base line) children might have been hospitalized annually due to influenzavirus infections.\n\n【68】Although these estimates of potentially preventable hospitalizations are compelling, any modification of the national policy of influenza vaccination requires a balanced assessment of all relevant considerations. First, the vaccine schedule for children is already complicated and crowded and may become more so in the future, making it more difficult for all to comply with vaccine recommendations. Second, there are substantial logistic issues surrounding a requirement to vaccinate approximately 5.5 million children 6 to 24 months of age with either one or two doses of vaccine during a relatively brief period each fall. Third, issues related to cost effectiveness and safety must be seriously assessed. Discussions of such considerations are ongoing, further prompted by the promising results of efficacy studies of a live attenuated influenzavirus vaccine in children.  Our study demonstrates increased rates of influenza-related hospitalization among children younger than two years of age and suggests that routine influenza vaccination should be considered in these children.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "17c497ca-35ea-4ca0-b0ff-f202fa7e07b4", "title": "Rotavirus Vaccine — A Powerful Tool to Combat Deaths from Diarrhea", "text": "【0】Rotavirus Vaccine — A Powerful Tool to Combat Deaths from Diarrhea\nArticle\n-------\n\n【1】Rotavirus infection, the leading cause of severe childhood diarrhea in both developed and developing countries, results in over half a million deaths each year.  Currently, two rotavirus vaccines (Rotarix \\[GlaxoSmithKline Biologicals\\] and RotaTeq \\[Merck\\]) are licensed in many countries and used routinely in several. Until recently, available efficacy data were from developed and developing countries with relatively low mortality rates among children younger than 5 years of age.\n\n【2】In this issue of the _Journal,_ efficacy trials conducted in Africa by Madhi and colleagues  and a postmarketing study conducted in Mexico by Richardson and colleagues  are described. The data support the use of rotavirus vaccines in the poorest countries in the world, a finding that is consistent with previous reports.  Recently, the World Health Organization (WHO) Strategic Advisory Group of Experts on Immunization recommended the use of rotavirus vaccines worldwide.  The widespread use of these vaccines has the potential to prevent about 2 million deaths over the next decade.  Madhi and colleagues report a pooled efficacy of 61.2% in South Africa and Malawi; the country-specific efficacy was 76.9% and 49.4%, respectively. Despite the lower efficacy in Malawi, the vaccine prevented many more episodes of severe gastroenteritis due to rotavirus in that country than in South Africa or other regions where the vaccine has been evaluated, because of the higher rates of severe gastroenteritis in Malawi. Another important finding of the study was a 30% decrease in the incidence of severe gastroenteritis from any cause; similar findings have been reported in other studies.  This decrease in severe gastroenteritis from any cause suggests that the available tests for detecting rotavirus are failing to detect some cases of rotavirus disease.\n\n【3】The Mexico study provides data suggesting that the introduction of the rotavirus vaccine resulted in a substantial reduction in deaths from rotavirus. Documentation of the reduction in mortality for most interventions is extremely difficult. It is commendable that the excellent surveillance system in Mexico enabled the investigators to make this observation.\n\n【4】Can the data from Malawi and South Africa be extrapolated to other countries in the region? The available data suggest that vaccine efficacy is inversely correlated with mortality among children younger than 5 years of age, populations in which the burden of disease from rotavirus infection is often highest.  Thus, as suggested by the WHO Strategic Advisory Group of Experts, it is reasonable to assume that the efficacy would be similar in regions with socioeconomic characteristics, social structure, nutritional conditions, and risks contributing to mortality like those in Malawi and South Africa.  Although not evaluated in the studies in this issue of the _Journal,_ herd immunity has been shown to be induced by rotavirus vaccines (as an indirect effect) by reducing the exposure of unvaccinated persons to the organism.  Thus, introduction of the vaccine into countries is likely to have a greater effect than that predicted on the basis of the efficacy trials.\n\n【5】Despite this potential for rotavirus vaccines to substantially reduce the risk of death from diarrhea, there are considerable challenges to implementing their use in the poorest countries of the world. First, the storage and shipment requirements to avert cold-chain breaks of rotavirus vaccines are far greater than those of typical childhood vaccines, which will make the logistics of vaccination programs in developing countries more difficult.\n\n【6】Second, the rotavirus vaccines are currently recommended for administration during a narrow window: the first dose between 6 and 15 weeks of age, and the third dose no later than 32 weeks of age.  This recommendation is a serious impediment to the widespread use of rotavirus vaccines, especially in countries with the highest child mortality, which tend to have the lowest vaccine coverage and the lowest rate of on-time immunization.  This recommendation for age-restricted vaccine administration is based on the age-dependent occurrence of intussusception with the use of the RotaShield vaccine (manufactured by Wyeth Lederle Vaccines), which was withdrawn from the market a year after it was licensed in 1998. There is no evidence of increased frequency of intussusception with either of the two current rotavirus vaccines; thus, additional data regarding the safety and efficacy of the current vaccines as administered within wider age windows are urgently needed to ensure optimal coverage.\n\n【7】A report by Patel and colleagues  in this issue of the _Journal_ describes prolonged rotavirus disease and viral shedding in three infants with severe combined immunodeficiency after receipt of a rotavirus vaccine. Diarrhea caused by rotavirus acquired through transmission of a vaccine-type virus is likely to be much less severe than disease caused by the wild-type virus. To date, there is no evidence that rotavirus vaccine causes disease in children infected with the human immunodeficiency virus or that the vaccine is tolerated less well by such children than by other children, although this possibility will need to be carefully monitored in the future.\n\n【8】Third, the current cost of the rotavirus vaccines per dose in the United States is far beyond the means of most middle-income countries and the poorest countries. Fortunately, cofinancing is available, at least for the short term, from GAVI (formerly known as the Global Alliance for Vaccines and Immunization) — resulting in a cost of 15 to 30 cents per dose, depending on the economic status of the country. The remainder of the cost is absorbed by GAVI; therefore, it is imperative that the global donor community continues to support programs such as GAVI to ensure that the poorest children have access to these new lifesaving vaccines.\n\n【9】The WHO currently has regional surveillance systems in place to document rates of disease from rotavirus.  It is critical to maintain such surveillance systems to track the safety and effectiveness of the vaccine and shifts in serotype distribution.\n\n【10】In the 10-year period since RotaShield was withdrawn from the market, more than 5 million children have died from rotavirus disease. Thus, current vaccines should be widely used now, while trials of other vaccine candidates are continued in various populations and mechanisms to improve vaccine efficacy are investigated. Since rotavirus is only one of many pathogens that cause diarrhea, the use of rotavirus vaccine will need to be supplemented by other preventive and treatment strategies to reduce the high mortality from diarrheal diseases. Unfortunately, the coverage for known effective interventions, such as oral rehydration therapy, in parts of Africa and South Asia is less than 35%. \n\n【11】We now have another powerful weapon to add to our armamentarium to combat deaths from diarrhea — rotavirus vaccines. The vaccines should be introduced immediately in areas with high mortality from rotavirus infection, and their introduction should be used to energize diarrhea-control programs and improve coverage for all the proven interventions for diarrhea. It is time to act to combat the 1.8 million unnecessary deaths from diarrhea that continue to occur each year.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d4ee940d-03f9-4be8-a3d0-69862bae142e", "title": "Immunogenicity of a Fourth Homologous Dose of NVX-CoV2373", "text": "【0】Immunogenicity of a Fourth Homologous Dose of NVX-CoV2373\nTo the Editor:\n--------------\n\n【1】The emergence and rapid propagation of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants — in particular, the B.1.1.529 (omicron) sublineages, which have mutations that increase transmissibility and evasiveness against vaccine-induced immunity — can substantially reduce the efficacy of approved vaccines. The degree to which ancestral strain–based vaccines induced immunity against variants depends on the ability of the vaccine to induce broadly cross-reactive antibodies. The NVX-CoV2373 vaccine (Novavax) consists of full-length, prefusion, recombinant spike (rS) protein trimers with epitopes conserved across variants and is coformulated with a saponin-based adjuvant, Matrix-M, which may enhance antibody avidity, affinity maturation, and epitope spreading.\n\n【2】As part of an ongoing phase 2, randomized, placebo-controlled clinical trial of NVX-CoV2373 that is being conducted in Australia and the United States , two doses of NVX-CoV2373 were administered to participants 21 days apart, followed initially by a single booster dose after approximately 6 months. In a continuation of this trial, a second booster dose of the vaccine was administered after another 6 months. Only participants who received four doses of NVX-CoV2373 were included in this analysis.\n\n【3】Figure 1. Immunogenicity of the NVX-CoV2373 Vaccine against Ancestral and Variant Strains of SARS-CoV-2 According to Dose, as Assessed by Anti-rS IgG and Neutralization Titers.\n\n【4】Immune response was assessed as previously described before and after each dose of NVX-CoV2373, with the use of a validated anti–recombinant spike protein (rS) IgG assay for the ancestral strain and the BA.1 and BA.5 variants of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)  and with the use of serum neutralizing antibodies in validated pseudovirus neutralization assays for the ancestral strain and BA.1 and BA.4 or BA.5 variants .  Antigenic cartography maps were constructed with the use of cartography software . Anti-rS IgG and neutralizing titers that were obtained after two, three, and four doses were input into the software to construct an antigenic map for SARS-CoV-2. SARS-CoV-2 antigens are indicated by colored circles, and serum samples by small squares. Each grid square represents one antigenic unit, which is equal to a change in titer by a factor of 2. The antigenic distances from the ancestral vaccine strain (green) to the omicron variants (BA.1 \\[red\\] and BA.4 or BA.5 \\[pink\\]) were calculated after two, three, and four doses of vaccine, followed by conversion of antigenic distances to factor differences, as previously described.  Red arrows indicate the antigenic distance from the ancestral strain to the BA.5 variant  or to the BA.4 or BA.5 variant , and dashed circles represent the overall decrease in antigenic distances. Data on anti-rS IgG were not available for the BA.1 variant for 3 of the 34 participants. CI denotes confidence interval, GMT geometric mean titer, and ID <sub>50 </sub> 50% inhibitory dilution.\n\n【5】Antigenic cartography, a method that is frequently used to help evaluate the potential effectiveness of influenza vaccination against variant strains and to guide vaccine-strain composition, provides a direct visualization of the effect of multiple doses on vaccine-induced immunogenicity. The anti-rS IgG and neutralization titers increased after the third and fourth doses of NVX-CoV2373 and showed a reduction in the antigenic distance between the ancestral SARS-CoV-2 strain and the forward-drifted BA.1 and BA.4 or BA.5 variants . When we compared the antigenic distance between omicron subvariants and the ancestral strain, the factor difference in IgG titers against the ancestral strain and the BA.5 variant (4.4) after the second dose decreased to 1.8 after the fourth dose, and the factor difference in neutralization titers against the ancestral strain and the BA.4 or BA.5 variant (33.5) after the second dose decreased to 3.5 after the fourth dose.\n\n【6】These data indicate that boosting with the NVX-CoV2373 vaccine resulted in enhanced cross-reactive immunity to SARS-CoV-2 variants, a decreased gap between immune recognition of the variants and the ancestral strain, and the induction of a potentially more universal-like response against SARS-CoV-2 variants. We believe that this phenomenon may be driven by the conserved epitopes found on the recombinant protein vaccine, whereby expression of the full-length trimers of the S protein present epitopes that are conserved across variants for recognition by the immune system.  This process may be further enhanced by the saponin-based Matrix-M adjuvant by means of epitope spreading.  Whether the observed antibody responses to additional boosting with the ancestral sequence that was used in the NVX-CoV2373 vaccine translate into meaningful protection against emerging SARS-CoV-2 variants will require clinical studies of efficacy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "16984630-854b-45ee-93fd-506268b80e20", "title": "Unfolding the Diagnosis", "text": "【0】Unfolding the Diagnosis\nA previously healthy, 25-year-old man was admitted to the hospital because of abdominal pain, nausea, vomiting, and weight loss. Fever, chills, and weakness developed 2 weeks before his admission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "36dd9946-67b4-4156-9d28-aaeaeef734b2", "title": "An mRNA Vaccine against SARS-CoV-2 — Preliminary Report", "text": "【0】An mRNA Vaccine against SARS-CoV-2 — Preliminary Report\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emerged in late 2019 and spread globally, prompting an international effort to accelerate development of a vaccine. The candidate vaccine mRNA-1273 encodes the stabilized prefusion SARS-CoV-2 spike protein.\n\n【3】Methods\n-------\n\n【4】We conducted a phase 1, dose-escalation, open-label trial including 45 healthy adults, 18 to 55 years of age, who received two vaccinations, 28 days apart, with mRNA-1273 in a dose of 25 μg, 100 μg, or 250 μg. There were 15 participants in each dose group.\n\n【5】Results\n-------\n\n【6】After the first vaccination, antibody responses were higher with higher dose (day 29 enzyme-linked immunosorbent assay anti–S-2P antibody geometric mean titer \\[GMT\\], 40,227 in the 25-μg group, 109,209 in the 100-μg group, and 213,526 in the 250-μg group). After the second vaccination, the titers increased (day 57 GMT, 299,751, 782,719, and 1,192,154, respectively). After the second vaccination, serum-neutralizing activity was detected by two methods in all participants evaluated, with values generally similar to those in the upper half of the distribution of a panel of control convalescent serum specimens. Solicited adverse events that occurred in more than half the participants included fatigue, chills, headache, myalgia, and pain at the injection site. Systemic adverse events were more common after the second vaccination, particularly with the highest dose, and three participants (21%) in the 250-μg dose group reported one or more severe adverse events.\n\n【7】Conclusions\n-----------\n\n【8】The mRNA-1273 vaccine induced anti–SARS-CoV-2 immune responses in all participants, and no trial-limiting safety concerns were identified. These findings support further development of this vaccine\\.\n\n【9】Introduction\n------------\n\n【10】The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emerged in December 2019 and spread globally, causing a pandemic of respiratory illness designated coronavirus disease 2019 (Covid-19).  The urgent need for vaccines prompted an international response, with more than 120 candidate SARS-CoV-2 vaccines in development within the first 5 months of 2020.  The candidate vaccine mRNA-1273 is a lipid nanoparticle–encapsulated, nucleoside-modified messenger RNA (mRNA)–based vaccine that encodes the SARS-CoV-2 spike (S) glycoprotein stabilized in its prefusion conformation. The S glycoprotein mediates host cell attachment and is required for viral entry  ; it is the primary vaccine target for many candidate SARS-CoV-2 vaccines. \n\n【11】We conducted a first-in-human phase 1 clinical trial in healthy adults to evaluate the safety and immunogenicity of mRNA-1273. Here we report interim results of the trial.\n\n【12】Methods\n-------\n\n【13】Trial Design and Participants\n-----------------------------\n\n【14】We conducted a phase 1, dose-escalation, open-label clinical trial designed to determine the safety, reactogenicity, and immunogenicity of mRNA-1273. Eligible participants were healthy adults 18 to 55 years of age who received two injections of trial vaccine 28 days apart at a dose of 25 μg, 100 μg, or 250 μg. On the basis of the results obtained in patients at these dose levels, additional groups were added to the protocol; those results will be reported in a subsequent publication. Participants were not screened for SARS-CoV-2 infection by serology or polymerase chain reaction before enrollment. The trial was conducted at the Kaiser Permanente Washington Health Research Institute in Seattle and at the Emory University School of Medicine in Atlanta. The protocol , available with the full text of this article at NEJM.org, permitted interim analyses to inform decisions regarding vaccine strategy and public health; this interim analysis reports findings through day 57. Full details of the trial design, conduct, oversight, and analyses can be found in the protocol and statistical analysis plan .\n\n【15】The trial was reviewed and approved by the Advarra institutional review board, which functioned as a single board and was overseen by an independent safety monitoring committee. All participants provided written informed consent before enrollment. The trial was conducted under an Investigational New Drug application submitted to the Food and Drug Administration. The vaccine was codeveloped by researchers at the National Institute of Allergy and Infectious Diseases (NIAID, the trial sponsor) and at Moderna (Cambridge, MA). Moderna was involved in discussions of the trial design, provided the vaccine candidate, and, as part of the writing group, contributed to drafting the manuscript. The Emmes Company, as a subcontractor to the NIAID, served as the statistical and data coordinating center, developed the statistical analysis plan, and performed the analyses. The manuscript was written entirely by the authors, with the first author as the overall lead author, the fourth author as the lead NIAID author, and the last two authors as senior authors . The authors had full access to the data reports, which were prepared from the raw data by the statistical and data coordinating center, and vouch for the completeness and accuracy of the data and for the fidelity of the trial to the protocol.\n\n【16】Vaccine\n-------\n\n【17】The mRNA-1273 vaccine candidate, manufactured by Moderna, encodes the S-2P antigen, consisting of the SARS-CoV-2 glycoprotein with a transmembrane anchor and an intact S1–S2 cleavage site. S-2P is stabilized in its prefusion conformation by two consecutive proline substitutions at amino acid positions 986 and 987, at the top of the central helix in the S2 subunit.  The lipid nanoparticle capsule composed of four lipids was formulated in a fixed ratio of mRNA and lipid. The mRNA-1273 vaccine was provided as a sterile liquid for injection at a concentration of 0.5 mg per milliliter. Normal saline was used as a diluent to prepare the doses administered.\n\n【18】Trial Procedures\n----------------\n\n【19】The vaccine was administered as a 0.5-ml injection in the deltoid muscle on days 1 and 29; follow-up visits were scheduled for 7 and 14 days after each vaccination and on days 57, 119, 209, and 394. The dose-escalation plan specified enrollment of four sentinel participants in the 25-μg group, followed by four sentinel participants in the 100-μg group, followed by full enrollment of those two dose groups. If no halting rules were met after all participants in those two dose groups completed day 8, four sentinel participants in the 250-μg group were enrolled, followed by the remainder of that dose group.\n\n【20】Participants recorded local and systemic reactions, using a memory aid, for 7 days after each vaccination. Participants were not instructed to routinely use acetaminophen or other analgesics or antipyretics before or after the vaccinations but were asked to record any new medications taken. Adverse events were graded according to a standard toxicity grading scale . \n\n【21】Assessment of SARS-CoV-2 Binding Antibody and Neutralizing Responses\n--------------------------------------------------------------------\n\n【22】Binding antibody responses against S-2P and the isolated receptor-binding domain, located in the S1 subunit, were assessed by enzyme-linked immunosorbent assay (ELISA). Vaccine-induced neutralizing activity was assessed by a pseudotyped lentivirus reporter single-round-of-infection neutralization assay (PsVNA) and by live wild-type SARS-CoV-2 plaque-reduction neutralization testing (PRNT) assay. ELISA and PsVNA were performed on specimens collected from all participants on days 1, 15, 29, 36, 43, and 57. Because of the time-intensive nature of the PRNT assay, for this report of the interim analysis, results were available only for the day 1 and day 43 time points in the 25-μg and 100-μg dose groups.\n\n【23】For comparison of the participants’ immune responses with those induced by SARS-CoV-2 infection, 41 convalescent serum specimens were also tested. The assays were performed at the NIAID Vaccine Research Center (ELISA and PsVNA) and the Vanderbilt University Medical Center (PRNT).\n\n【24】Assessment of SARS-CoV-2 T-Cell Responses\n-----------------------------------------\n\n【25】T-cell responses against the spike protein were assessed by an intracellular cytokine–staining assay, performed on specimens collected at days 1, 29, and 43. For this report of the interim analysis, results were available only for the 25-μg and 100-μg dose groups. These assays were performed at the NIAID Vaccine Research Center. \n\n【26】Statistical Analysis\n--------------------\n\n【27】Results of immunogenicity testing of the 45 enrolled participants excluded findings for day 36, day 43, and day 57 for 3 participants who did not receive the second vaccination and for time points at which specimens were not collected (in the 100-μg group: 1 participant at day 43 and day 57; in the 250-μg group: 1 participant at day 29 and 1 at day 57). Confidence intervals of the geometric means were calculated with the Student’s t distribution on log-transformed data. Seroconversion as measured by ELISA was defined as an increase by a factor of 4 or more in antibody titer over baseline.\n\n【28】Results\n-------\n\n【29】Trial Population\n----------------\n\n【30】Table 1. Characteristics of the Participants in the mRNA-1273 Trial at Enrollment.\n\n【31】The 45 enrolled participants received their first vaccination between March 16 and April 14, 2020 . Three participants did not receive the second vaccination, including one in the 25-μg group who had urticaria on both legs, with onset 5 days after the first vaccination, and two (one in the 25-μg group and one in the 250-μg group) who missed the second vaccination window owing to isolation for suspected Covid-19 while the test results, ultimately negative, were pending. All continued to attend scheduled trial visits. The demographic characteristics of participants at enrollment are provided in Table 1 .\n\n【32】Vaccine Safety\n--------------\n\n【33】No serious adverse events were noted, and no prespecified trial halting rules were met. As noted above, one participant in the 25-μg group was withdrawn because of an unsolicited adverse event, transient urticaria, judged to be related to the first vaccination.\n\n【34】Figure 1. Systemic and Local Adverse Events.\n\n【35】The severity of solicited adverse events was graded as mild, moderate, or severe .\n\n【36】After the first vaccination, solicited systemic adverse events were reported by 5 participants (33%) in the 25-μg group, 10 (67%) in the 100-μg group, and 8 (53%) in the 250-μg group; all were mild or moderate in severity . Solicited systemic adverse events were more common after the second vaccination and occurred in 7 of 13 participants (54%) in the 25-μg group, all 15 in the 100-μg group, and all 14 in the 250-μg group, with 3 of those participants (21%) reporting one or more severe events.\n\n【37】None of the participants had fever after the first vaccination. After the second vaccination, no participants in the 25-μg group, 6 (40%) in the 100-μg group, and 8 (57%) in the 250-μg group reported fever; one of the events (maximum temperature, 39.6°C) in the 250-μg group was graded severe. (Additional details regarding adverse events for that participant are provided in the Supplementary Appendix .)\n\n【38】Local adverse events, when present, were nearly all mild or moderate, and pain at the injection site was common. Across both vaccinations, solicited systemic and local adverse events that occurred in more than half the participants included fatigue, chills, headache, myalgia, and pain at the injection site. Evaluation of safety clinical laboratory values of grade 2 or higher and unsolicited adverse events revealed no patterns of concern .\n\n【39】SARS-CoV-2 Binding Antibody Responses\n-------------------------------------\n\n【40】Table 2. Geometric Mean Humoral Immunogenicity Assay Responses to mRNA-1273 in Participants and in Convalescent Serum Specimens. Figure 2.  Figure 2. SARS-CoV-2 Antibody and Neutralization Responses.\n\n【41】Shown are geometric mean reciprocal end-point enzyme-linked immunosorbent assay (ELISA) IgG titers to S-2P  and receptor-binding domain , PsVNA ID <sub>50 </sub> responses , and live virus PRNT <sub>80 </sub> responses . In Panel A and Panel B, boxes and horizontal bars denote interquartile range (IQR) and median area under the curve (AUC), respectively. Whisker endpoints are equal to the maximum and minimum values below or above the median ±1.5 times the IQR. The convalescent serum panel includes specimens from 41 participants; red dots indicate the 3 specimens that were also tested in the PRNT assay. The other 38 specimens were used to calculate summary statistics for the box plot in the convalescent serum panel. In Panel C, boxes and horizontal bars denote IQR and median ID <sub>50 </sub> , respectively. Whisker end points are equal to the maximum and minimum values below or above the median ±1.5 times the IQR. In the convalescent serum panel, red dots indicate the 3 specimens that were also tested in the PRNT assay. The other 38 specimens were used to calculate summary statistics for the box plot in the convalescent panel. In Panel D, boxes and horizontal bars denote IQR and median PRNT <sub>80 </sub> , respectively. Whisker end points are equal to the maximum and minimum values below or above the median ±1.5 times the IQR. The three convalescent serum specimens were also tested in ELISA and PsVNA assays. Because of the time-intensive nature of the PRNT assay, for this preliminary report, PRNT results were available only for the 25-μg and 100-μg dose groups.\n\n【42】Binding antibody IgG geometric mean titers (GMTs) to S-2P increased rapidly after the first vaccination, with seroconversion in all participants by day 15 . Dose-dependent responses to the first and second vaccinations were evident. Receptor-binding domain–specific antibody responses were similar in pattern and magnitude . For both assays, the median magnitude of antibody responses after the first vaccination in the 100-μg and 250-μg dose groups was similar to the median magnitude in convalescent serum specimens, and in all dose groups the median magnitude after the second vaccination was in the upper quartile of values in the convalescent serum specimens. The S-2P ELISA GMTs at day 57 (299,751 \\[95% confidence interval {CI}, 206,071 to 436,020\\] in the 25-μg group, 782,719 \\[95% CI, 619,310 to 989,244\\] in the 100-μg group, and 1,192,154 \\[95% CI, 924,878 to 1,536,669\\] in the 250-μg group) exceeded that in the convalescent serum specimens (142,140 \\[95% CI, 81,543 to 247,768\\]).\n\n【43】SARS-CoV-2 Neutralization Responses\n-----------------------------------\n\n【44】No participant had detectable PsVNA responses before vaccination. After the first vaccination, PsVNA responses were detected in less than half the participants, and a dose effect was seen (50% inhibitory dilution \\[ID <sub>50 </sub> \\]: Figure 2C , Fig. S8, and Table 2 ; 80% inhibitory dilution \\[ID <sub>80 </sub> \\]: Fig. S2 and Table S6). However, after the second vaccination, PsVNA responses were identified in serum samples from all participants. The lowest responses were in the 25-μg dose group, with a geometric mean ID <sub>50 </sub> of 112.3 (95% CI, 71.2 to 177.1) at day 43; the higher responses in the 100-μg and 250-μg groups were similar in magnitude (geometric mean ID <sub>50 </sub> , 343.8 \\[95% CI, 261.2 to 452.7\\] and 332.2 \\[95% CI, 266.3 to 414.5\\], respectively, at day 43). These responses were similar to values in the upper half of the distribution of values for convalescent serum specimens.\n\n【45】Before vaccination, no participant had detectable 80% live-virus neutralization at the highest serum concentration tested  dilution) in the PRNT assay. At day 43, wild-type virus–neutralizing activity capable of reducing SARS-CoV-2 infectivity by 80% or more (PRNT <sub>80 </sub> ) was detected in all participants, with geometric mean PRNT <sub>80 </sub> responses of 339.7 (95% CI, 184.0 to 627.1) in the 25-μg group and 654.3 (95% CI, 460.1 to 930.5) in the 100-μg group . Neutralizing PRNT <sub>80 </sub> average responses were generally at or above the values of the three convalescent serum specimens tested in this assay. Good agreement was noted within and between the values from binding assays for S-2P and receptor-binding domain and neutralizing activity measured by PsVNA and PRNT , which provides orthogonal support for each assay in characterizing the humoral response induced by mRNA-1273.\n\n【46】SARS-CoV-2 T-Cell Responses\n---------------------------\n\n【47】The 25-μg and 100-μg doses elicited CD4 T-cell responses  that on stimulation by S-specific peptide pools were strongly biased toward expression of Th1 cytokines (tumor necrosis factor α > interleukin 2 > interferon γ), with minimal type 2 helper T-cell (Th2) cytokine expression (interleukin 4 and interleukin 13). CD8 T-cell responses to S-2P were detected at low levels after the second vaccination in the 100-μg dose group .\n\n【48】Discussion\n----------\n\n【49】We report interim findings from this phase 1 clinical trial of the mRNA-1273 SARS-CoV-2 vaccine encoding a stabilized prefusion spike trimer, S-2P. Experience with the mRNA platform for other candidate vaccines and rapid manufacturing allowed the deployment of a first-in-human clinical vaccine candidate in record time. Product development processes that normally require years  were finished in about 2 months. Vaccine development was initiated after the SARS-CoV-2 genome was posted on January 10, 2020; manufacture and delivery of clinical trials material was completed within 45 days, and the first trial participants were vaccinated on March 16, 2020, just 66 days after the genomic sequence of the virus was posted. The accelerated timeline generated key interim data necessary to launch advanced large-scale clinical trials within 6 months after initial awareness of a new pandemic threat.\n\n【50】The two-dose vaccine series was generally without serious toxicity; systemic adverse events after the first vaccination, when reported, were all graded mild or moderate. Greater reactogenicity followed the second vaccination, particularly in the 250-μg group. Across the three dose groups, local injection-site reactions were primarily mild. This descriptive safety profile is similar to that described in a report of two trials of avian influenza mRNA vaccines (influenza A/H10N8 and influenza A/H7N9) that were manufactured by Moderna with the use of an earlier lipid nanoparticle capsule formulation  and is consistent with an interim report of a phase 1–2 evaluation of a Covid-19 mRNA vaccine encoding the S receptor-binding domain.  Those studies showed that solicited systemic adverse events tended to be more frequent and more severe with higher doses and after the second vaccination.\n\n【51】The mRNA-1273 vaccine was immunogenic, inducing robust binding antibody responses to both full-length S-2P and receptor-binding domain in all participants after the first vaccination in a time- and dose-dependent fashion. Commensurately high neutralizing antibody responses were also elicited in a dose-dependent fashion. Seroconversion was rapid for binding antibodies, occurring within 2 weeks after the first vaccination, but pseudovirus neutralizing activity was low before the second vaccination, which supports the need for a two-dose vaccination schedule. It is important to note that both binding and neutralizing antibody titers induced by the two-dose schedule were similar to those found in convalescent serum specimens. However, interpretation of the significance of those comparisons must account for the variability in Covid-19 convalescent antibody titers according to factors such as patient age, disease severity, and time since disease onset and for the number of samples in the panel. \n\n【52】Though correlates of protection from SARS-CoV-2 infection have not yet been determined, measurement of serum neutralizing activity has been shown to be a mechanistic correlate of protection for other respiratory viruses, such as influenza  and respiratory syncytial virus,  and is generally accepted as a functional biomarker of the in vivo humoral response.  In rhesus macaques given DNA vaccine candidates expressing different forms of the SARS-CoV-2 spike protein, post-vaccination neutralizing antibody titers were correlated with protection against SARS-CoV-2 challenge.  Humoral and cell-mediated immune responses have been associated with vaccine-induced protection against challenge  or subsequent rechallenge after SARS-CoV-2 infection in a rhesus macaque model.  We found strong correlations between the binding and neutralization assays and between the live virus and pseudovirus neutralization assays. The latter finding suggests that the pseudovirus neutralization assay, performed under biosafety level 2 containment, may, when validated, serve as a relevant surrogate for live virus neutralization, which requires biosafety level 3 containment. In humans, phase 3 efficacy trials will allow assessment of the correlation of vaccine-induced immune responses with clinical protection.\n\n【53】In this interim report of follow-up of participants through day 57, we were not able to assess the durability of the immune responses; however, participants will be followed for 1 year after the second vaccination with scheduled blood collections throughout that period to characterize the humoral and cellular immunologic responses. This longitudinal assessment is relevant given that natural history studies suggest that SARS-CoV and MERS-CoV (Middle East respiratory syndrome coronavirus) infections, particularly mild illnesses, may not generate long-lived antibody responses. \n\n【54】The rapid and robust immunogenicity profile of the mRNA-1273 vaccine most likely results from an innovative structure-based vaccine antigen design,  coupled with a potent lipid-nanoparticle delivery system, and the use of modified nucleotides that avoid early intracellular activation of interferon-associated genes. These features of the mRNA composition and formulation have been associated with prolonged protein expression, induction of antigen-specific T-follicular helper cells, and activation of germinal center B cells.  Stabilizing coronavirus spike proteins by substituting two prolines at the top of heptad repeat 1 prevents structural rearrangements of the fusion (S2) subunit. This has enabled the determination of atomic-level structure for the prefusion conformation of spike from both endemic and pandemic strains, including HKU1,  SARS-CoV,  and MERS-CoV.  Moreover, S-2P conformational stability translates into greater immunogenicity,  based on preservation of neutralization-sensitive epitopes at the apex of the prefusion molecule, as shown for respiratory syncytial virus F glycoprotein,  and improved protein expression,  which is particularly advantageous for gene-based antigen delivery. Thus, presentation of the naturally folded prefusion conformation of the S glycoprotein to the immune system from an mRNA template enables efficient within-host antigen production and promotes both high-quality and high-magnitude antibody responses to SARS-CoV-2.\n\n【55】Previous experience with veterinary coronavirus vaccines and animal models of SARS-CoV and MERS-CoV infection have raised safety concerns about the potential for vaccine-associated enhanced respiratory disease. These events were associated either with macrophage-tropic coronaviruses susceptible to antibody-dependent enhancement of replication or with vaccine antigens that induced antibodies with poor neutralizing activity and Th2-biased responses.  Reducing the risk of vaccine-associated enhanced respiratory disease or antibody-dependent enhancement of replication involves induction of high-quality functional antibody responses and Th1-biased T-cell responses. Studies of mRNA-1273 in mice show that the structurally defined spike antigen induces robust neutralizing activity and that the gene-based delivery promotes Th1-biased responses, including CD8 T cells that protect against virus replication in lung and nose without evidence of immunopathology.  It is important to note that mRNA-1273 also induces Th1-biased CD4 T-cell responses in humans. Additional testing in animals and ongoing T-cell analysis of clinical specimens will continue to define the safety profile of mRNA-1273.\n\n【56】These safety and immunogenicity findings support advancement of the mRNA-1273 vaccine to later-stage clinical trials. Of the three doses evaluated, the 100-μg dose elicits high neutralization responses and Th1-skewed CD4 T cell responses, coupled with a reactogenicity profile that is more favorable than that of the higher dose. A phase 2 trial of mRNA-1273 in 600 healthy adults, evaluating doses of 50 μg and 100 μg, is ongoing . A large phase 3 efficacy trial, expected to evaluate a 100-μg dose, is anticipated to begin during the summer of 2020.\n\n【57】Table 1. Characteristics of the Participants in the mRNA-1273 Trial at Enrollment. \n\n| Characteristic | 25-μg Group(N=15) | 100-μg Group(N=15) | 250-μg Group(N=15) | Overall(N=45) |\n| --- | --- | --- | --- | --- |\n| Sex — no. (%) |  |  |  |  |\n| Male | 9 (60) | 7 (47) | 6 (40) | 22 (49) |\n| Female | 6 (40) | 8 (53) | 9 (60) | 23 (51) |\n| Age — yr | 36.7±7.9 | 31.3±8.7 | 31.0±8.0 | 33.0±8.5 |\n| Race or ethnic group — no. (%)  |  |  |  |  |\n| American Indian or Alaska Native | 0 | 1 (7) | 0 | 1 (2) |\n| Asian | 0 | 0 | 1 (7) | 1 (2) |\n| Black | 0 | 2 (13) | 0 | 2 (4) |\n| White | 15 (100) | 11 (73) | 14 (93) | 40 (89) |\n| Unknown | 0 | 1 (7) | 0 | 1 (2) |\n| Hispanic or Latino — no. (%) | 1 (7) | 3 (20) | 2 (13)  | 6 (13) |\n| Body-mass index  | 24.6±3.4 | 26.7±2.6 | 24.7±3.1 | 25.3±3.2 |\n\n【59】 Plus–minus values are means ±SD.\n\n【60】 Race or ethnic group was reported by the participants.\n\n【61】 One participant did not report ethnic group.\n\n【62】 The body-mass index is the weight in kilograms divided by the square of the height in meters. This calculation was based on the weight and height measured at the time of screening.\n\n【63】Table 2. Geometric Mean Humoral Immunogenicity Assay Responses to mRNA-1273 in Participants and in Convalescent Serum Specimens. \n\n| Time Point | 25-μg Group | 25-μg Group | 100-μg Group | 100-μg Group | 250-μg Group | 250-μg Group | Convalescent Serum | Convalescent Serum |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | no. | GMT (95% CI) | no. | GMT (95% CI) | no. | GMT (95% CI) | no. | GMT (95% CI) |\n| ELISA anti–S-2P |  |  |  |  |  |  | 38 | 142,140(81,543–247,768) |\n| Day 1 | 15 | 116(72–187) | 15 | 131(65–266) | 15 | 178(81–392) |  |  |\n| Day 15  | 15 | 32,261(18,723–55,587) | 15 | 86,291(56,403–132,016) | 15 | 163,449(102,155–261,520) |  |  |\n| Day 29 | 15 | 40,227(29,094–55,621) | 15 | 109,209(79,050–150,874) | 14 | 213,526(128,832–353,896) |  |  |\n| Day 36 | 13 | 391,018(267,402–571,780) | 15 | 781,399(606,247–1,007,156) | 14 | 1,261,975(973,972–1,635,140) |  |  |\n| Day 43 | 13 | 379,764(281,597–512,152) | 14 | 811,119(656,336–1,002,404) | 14 | 994,629(806,189–1,227,115) |  |  |\n| Day 57 | 13 | 299,751(206,071–436,020) | 14 | 782,719(619,310–989,244) | 13 | 1,192,154(924,878–1,536,669) |  |  |\n| ELISA anti–receptor-binding domain |  |  |  |  |  |  | 38 | 37,857(19,528–73,391) |\n| Day 1 | 15 | 55(44–70) | 15 | 166(82–337) | 15 | 576(349–949) |  |  |\n| Day 15  | 15 | 6567(3651–11,812) | 15 | 34,073(21,688–53,531) | 15 | 87,480(51,868–147,544) |  |  |\n| Day 29 | 15 | 18,149(11,091–29699) | 15 | 93,231(59,895–145,123) | 14 | 120,088(71,013–203,077) |  |  |\n| Day 36 | 13 | 208,652(142,803–304,864) | 15 | 499,539(400,950–622,369) | 14 | 720,907(591,860–878,090) |  |  |\n| Day 43 | 13 | 233,264(164,756–330,259) | 14 | 558,905(462,907–674,810) | 14 | 644,395(495,808–837,510) |  |  |\n| Day 57 | 13 | 183,652(122,763–274,741) | 14 | 371,271(266,721–516,804) | 13 | 582,259(404,019–839,134) |  |  |\n|  |  | _GMR (95% CI)_ |  | _GMR (95% CI)_ |  | _GMR (95% CI)_ |  | _GMR (95% CI)_ |\n| PsVNA ID 50  |  |  |  |  |  |  | 38 | 109.2(59.6–199.9) |\n| Day 1 | 15 | 10 | 15 | 10 | 15 | 10 |  |  |\n| Day 15  | 15 | 14.5(9.8–21.4) | 15 | 23.7(13.3–42.3) | 15 | 26.1(14.1–48.3) |  |  |\n| Day 29 | 15 | 11.7(9.7–14.1) | 15 | 18.2(12.1–27.4) | 14 | 20.7(13.3–32.2) |  |  |\n| Day 36 | 13 | 105.8(69.8–160.4) | 15 | 256.3(182.0–361.1) | 14 | 373.5(308.6–452.2) |  |  |\n| Day 43 | 13 | 112.3(71.2–177.1) | 14 | 343.8(261.2–452.7) | 14 | 332.2(266.3–414.5) |  |  |\n| Day 57 | 13 | 80.7(51.0–127.6) | 14 | 231.8(163.2–329.3) | 14 | 270.2(221.0–330.3) |  |  |\n| Live virus PRNT 80 |  |  |  |  |  |  | 3 | 158.3(15.1–1663.0) |\n| Day 1  | 15 | 4 | 15 | 4 |  | NA |  |  |\n| Day 43 | 13 | 339.7(184.0–627.1) | 14 | 654.3(460.1–930.5) |  | NA |  |  |\n\n【65】 ELISA denotes enzyme-linked immunosorbent assay, GMT geometric mean titer, GMR geometric mean response, ID <sub>50 </sub> 50% inhibitory dilution, NA not available, PRNT <sub>80 </sub> plaque-reduction neutralization testing assay that shows reduction in SARS-CoV-2 infectivity by 80% or more, and PsVNA pseudotyped lentivirus reporter neutralization assay.\n\n【66】 Seroconversion occurred in all participants at day 15.\n\n【67】 Samples that did not neutralize at the 50% level are expressed as less than 20 and plotted at half that dilution (i.e., 10).\n\n【68】 All day 1 specimens exhibited less than 80% inhibitory activity at the lowest dilution tested  and were assigned a titer of 4.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "011210ee-f2dc-4b80-8f5d-da7c04fdafa6", "title": "Lenalidomide plus Dexamethasone for Relapsed or Refractory Multiple Myeloma", "text": "【0】Lenalidomide plus Dexamethasone for Relapsed or Refractory Multiple Myeloma\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Lenalidomide is a structural analogue of thalidomide with similar but more potent biologic activity. This phase 3, placebo-controlled trial investigated the efficacy of lenalidomide plus dexamethasone in the treatment of relapsed or refractory multiple myeloma.\n\n【3】Methods\n-------\n\n【4】Of 351 patients who had received at least one previous antimyeloma therapy, 176 were randomly assigned to receive 25 mg of oral lenalidomide and 175 to receive placebo on days 1 to 21 of a 28-day cycle. In addition, all patients received 40 mg of oral dexamethasone on days 1 to 4, 9 to 12, and 17 to 20 for the first four cycles and subsequently, after the fourth cycle, only on days 1 to 4. Patients continued in the study until the occurrence of disease progression or unacceptable toxic effects. The primary end point was time to progression.\n\n【5】Results\n-------\n\n【6】The time to progression was significantly longer in the patients who received lenalidomide plus dexamethasone (lenalidomide group) than in those who received placebo plus dexamethasone (placebo group) (median, 11.3 months vs. 4.7 months; P<0.001). A complete or partial response occurred in 106 patients in the lenalidomide group (60.2%) and in 42 patients in the placebo group (24.0%, P<0.001), with a complete response in 15.9% and 3.4% of patients, respectively (P<0.001). Overall survival was significantly improved in the lenalidomide group (hazard ratio for death, 0.66; P=0.03). Grade 3 or 4 adverse events that occurred in more than 10% of patients in the lenalidomide group were neutropenia (29.5%, vs. 2.3% in the placebo group), thrombocytopenia (11.4% vs. 5.7%), and venous thromboembolism (11.4% vs. 4.6%).\n\n【7】Conclusions\n-----------\n\n【8】Lenalidomide plus dexamethasone is more effective than high-dose dexamethasone alone in relapsed or refractory multiple myeloma. \n\n【9】Introduction\n------------\n\n【10】Multiple myeloma, the second most common hematologic cancer, caused more than 19,000 deaths in Europe in 2004.  To improve the outcome of treatment, new agents are needed.  The immunomodulatory drug thalidomide has activity in about one third of patients with relapsed or refractory multiple myeloma; response rates are increased when thalidomide is combined with dexamethasone or chemotherapy.  Treatment with thalidomide is associated with sedation, fatigue, constipation, rash, deep-vein thrombosis, and peripheral neuropathy. These toxic effects often require dose reduction and, in some instances, discontinuation of the drug. \n\n【11】Lenalidomide, a derivative of thalidomide, is less toxic and more potent than the parent drug.  In patients with relapsed or refractory multiple myeloma, lenalidomide can overcome resistance not only to conventional chemotherapy but also to thalidomide,  and dexamethasone plus lenalidomide is more effective than either agent alone in refractory multiple myeloma.  We report on a randomized, phase 3 trial in which lenalidomide plus dexamethasone was compared with placebo plus dexamethasone in patients with relapsed or refractory multiple myeloma.\n\n【12】Methods\n-------\n\n【13】Patients\n--------\n\n【14】Patients with multiple myeloma in Europe, Israel, and Australia were eligible to participate in the study if they were at least 18 years of age and had been treated with at least one previous antimyeloma regimen. Patients were excluded if they had had disease progression during previous therapy containing high-dose dexamethasone (total monthly dose, >200 mg). Measurable disease was defined as a level of serum monoclonal protein (M protein) of at least 0.5 g per deciliter or a level of urinary Bence Jones protein of at least 0.2 g per day. Additional eligibility criteria included an Eastern Cooperative Oncology Group performance status of 2 or less, a serum aspartate aminotransferase or alanine aminotransferase level that was no more than three times the upper limit of the normal range, a serum bilirubin level that was no more than two times the upper limit of the normal range, a serum creatinine level of less than 2.5 mg per deciliter (221 μmol per liter), and an absolute neutrophil count of at least 1000 per cubic millimeter. The platelet count needed to be more than 75,000 per cubic millimeter for patients with less than 50% bone marrow plasma cells and more than 30,000 per cubic millimeter for patients with 50% or more bone marrow plasma cells.\n\n【15】Women of childbearing potential were eligible if they agreed to use contraception during the study, had a negative pregnancy test before enrollment, and agreed to undergo pregnancy testing every 4 weeks from enrollment until 4 weeks after discontinuation of the assigned study drug. Patients were excluded if they had previously had hypersensitivity to or uncontrollable side effects associated with previous use of thalidomide or dexamethasone. All patients gave written informed consent, and an ethics committee at each study site approved the protocol.\n\n【16】Study Design\n------------\n\n【17】The primary end point of this multicenter, randomized, placebo-controlled, phase 3 trial was the time to disease progression. Secondary end points included overall survival, the rate of response, and safety.\n\n【18】Patients received either 25 mg of oral lenalidomide or placebo on days 1 to 21 of a 28-day cycle; all patients received 40 mg of oral dexamethasone on days 1 to 4, 9 to 12, and 17 to 20 for the first four cycles. After the fourth cycle, 40 mg of dexamethasone was administered only on days 1 to 4. Patients continued to receive the assigned drug regimen until the occurrence of disease progression or unacceptable toxic effects. Patients were stratified according to the baseline serum β <sub>2 </sub> \\-microglobulin level (<2.5 mg per liter or ≥2.5 mg per liter), previous stem-cell transplantation (none or ≥1), and the number of previous antimyeloma regimens (1 or ≥2).\n\n【19】Toxic effects were graded according to the National Cancer Institute's Common Toxicity Criteria, version 2. For grade 4 adverse events, treatment was withheld and restarted at the next lower dose after resolution of the toxic effects. The dexamethasone dose was modified because of toxic effects at the investigator's discretion as follows: 40 mg daily for 4 days every 2 weeks (dose level, −1) or every 4 weeks (dose level, −2) or 20 mg daily for 4 days every 4 weeks (dose level, −3). For grade 3 or 4 neutropenia without other toxic effects, the first dose-modification step was dose level −1 (daily subcutaneous injection of 5 μg of granulocyte colony-stimulating factor per kilogram of body weight and 25 mg of lenalidomide); sequential reductions of the lenalidomide dose were 15 mg (dose level, −2), 10 mg (dose level, −3), and 5 mg (dose level, −4), with 5 μg of granulocyte colony-stimulating factor per kilogram daily at the investigator's discretion.\n\n【20】Patients underwent a blood count and physical examination on days 1 and 15 (and day 8 of cycle 1) during cycles 1 to 3 and on day 1 of each cycle thereafter. Serum and urinary levels of M protein were assessed on day 1 of each cycle and at the end of treatment. Transfusion of platelets and red cells and the administration of neutrophil growth factors and epoetin alfa were allowed as needed. All patients were allowed to receive bisphosphonates. Prophylactic anticoagulation was not recommended.\n\n【21】The study was designed as a collaborative effort by Dr. Dimopoulos, the coinvestigators, and the sponsor, Celgene. The sponsor collected the data and performed the final analysis, in collaboration with an independent data monitoring committee and Dr. Dimopoulos. All authors had full access to the primary data and the final analysis. Dr. Dimopoulos vouches for the accuracy and completeness of the published results. The first draft was written by Dr. Dimopoulos; subsequent drafts were written by Dr. Dimopoulos with editorial assistance from an employee of the sponsor. All authors had input before submission of both the original and revised manuscripts. An independent data monitoring committee reviewed safety and efficacy data throughout the study.\n\n【22】Assessments\n-----------\n\n【23】The response of patients to treatment was assessed according to the criteria of the European Group for Blood and Marrow Transplantation.  The time to progression was measured from randomization to the date of the first assessment showing progression. Progressive disease was defined as any of the following: an absolute increase of more than 500 mg of serum M protein per deciliter, as compared with the nadir value, or an absolute increase of more than 200 mg of urinary M protein in 24 hours; a new bone lesion or plasmacytoma or an increase in the size of such lesions; or the development of hypercalcemia (serum calcium level, >11.5 mg per deciliter \\[2.9 mmol per liter\\]). Responses were designated as complete (defined as the absence of M protein in serum and urine, as confirmed by immunofixation in two samples, and <5% marrow plasma cells) or partial (defined as a reduction in the level of M protein in serum of at least 50% and a reduction in urine of at least 90%). Complete and partial responses were confirmed by repeated measurements of M protein in serum and urine after 6 weeks, and progressive disease was confirmed by repeated measurements of M protein in serum and urine after 1 to 3 weeks. Near-complete response, a subcategory of partial response, was defined as a complete response without confirmation of a decrease in marrow plasma cells to less than 5% by bone marrow biopsy, confirmation of the disappearance of M protein in serum or urine by repeat immunofixation, or both.\n\n【24】Statistical Analysis\n--------------------\n\n【25】The accrual goal was the enrollment of 302 patients (151 in each group), which would provide a statistical power of 85% to detect a hazard ratio of 1.5 for the time to progression with the use of a two-sided log-rank test with an overall significance level of 0.05, adjusted for one interim analysis. Full information that was necessary for a log-rank test to have a power of 85% would be achieved when approximately 222 patients had disease progression in the two study groups. For overall survival, the trial had a power of approximately 80% to detect a difference of 50% in median overall survival for an analysis performed when at least 194 patients had died.\n\n【26】An interim analysis of safety and efficacy was planned when disease had progressed in 111 patients. If the predetermined O'Brien–Fleming boundary for the superiority of lenalidomide over placebo was crossed, the study would be unblinded, and patients would be allowed to receive lenalidomide at the time of disease progression or at the investigator's discretion. All primary analyses were based on the intention-to-treat population, and subgroup analyses were planned on the basis of stratification variables. An unstratified log-rank test was used to compare time-to-event variables between groups. Both the time to progression and overall survival were estimated with the use of Kaplan–Meier methods. Continuity-corrected Pearson chi-square tests were used to compare the proportions of patients in the two groups who had a response to treatment.\n\n【27】Results\n-------\n\n【28】Patients and Treatments\n-----------------------\n\n【29】Table 1. Demographic and Clinical Characteristics of the Patients.\n\n【30】Between September 22, 2003, and September 15, 2004, a total of 351 patients (intention-to-treat population) were enrolled at 41 centers in Europe, 6 centers in Australia, and 3 centers in Israel. Of these patients, 176 were randomly assigned to receive lenalidomide plus dexamethasone (the lenalidomide group) and 175 to receive placebo plus dexamethasone (the placebo group). Baseline characteristics were well balanced between the two groups . Previous therapies included stem-cell transplantation, thalidomide, dexamethasone, melphalan, doxorubicin, and bortezomib. Patients in the two groups had received a median of two previous therapies. The average time from initial diagnosis to study entry was more than 4 years. At the time of the analysis, which included all data obtained before unblinding in August 2005, the median follow-up was 16.4 months. Median daily doses of study drugs in the two groups were 25 mg of lenalidomide and 40 mg of dexamethasone. The O'Brien–Fleming boundary for the superiority of lenalidomide had been passed at the time of the interim analysis in September 2004.\n\n【31】Efficacy\n--------\n\n【32】Figure 1. Kaplan–Meier Curves for the Time to Disease Progression among All Patients and in Subgroups with and without Previous Exposure to Thalidomide.\n\n【33】Panel A shows estimates of the median time to disease progression for the intention-to-treat population (11.3 months in the lenalidomide group and 4.7 months in the placebo group) (P<0.001 by the log-rank test). Panel B shows the median time to disease progression among patients in the two study groups who received thalidomide before study entry and those who did not receive thalidomide (in the lenalidomide group, 13.5 months among patients who did not receive thalidomide and 8.4 months among those who did receive thalidomide; in the placebo group, 4.7 months and 4.6 months, respectively; P<0.001 by the log-rank test for both between-group comparisons of patients who did and those who did not receive thalidomide).\n\n【34】The median time to progression was 11.3 months in the lenalidomide group and 4.7 months in the placebo group (P<0.001) . The hazard ratio for time to progression was 2.85 (95% confidence interval \\[CI\\], 2.16 to 3.76; P<0.001) in favor of the lenalidomide group. Patients in the lenalidomide group had a significantly longer time to progression than patients in the placebo group in all stratified subgroups. The median time to progression for patients who had undergone one previous therapy was not reached in the lenalidomide group and was 4.7 months in the placebo group. Among patients who had undergone at least two previous therapies, the median time to progression was 11.1 months in the lenalidomide group and 4.7 months in the placebo group (P<0.001).\n\n【35】The median time to progression for patients who had previously undergone treatment with thalidomide (30.1% of the patients in the lenalidomide group and 38.3% of those in the placebo group) was 8.4 months in the lenalidomide group and 4.6 months in the placebo group (P<0.001). Among patients with no previous exposure to thalidomide, the median time to progression was 13.5 months in the lenalidomide group and 4.7 months in the placebo group (P<0.001) . In the lenalidomide group, the median time to progression was not significantly related to previous exposure to thalidomide (hazard ratio, 0.65; 95% CI, 0.42 to 1.02; P=0.06). Among patients in whom the disease had progressed during previous thalidomide treatment, the median time to progression was 9.5 months in the lenalidomide group and 3.7 months in the placebo group (P<0.001). The median time to progression was 11.3 months among patients who had undergone stem-cell transplantation and 11.4 months among patients who had not undergone stem-cell transplantation.\n\n【36】Table 2. Response among Patients in the Intention-to-Treat Population and in Selected Subgroups.\n\n【37】A total of 106 patients (60.2%) in the lenalidomide group and 42 patients (24.0%) in the placebo group had at least a partial response (P<0.001); 28 patients (15.9%) in the lenalidomide group and 6 patients (3.4%) in the placebo group had a complete response (P<0.001) . The median time to the first response was 2.1 months in the lenalidomide group and 1.6 months in the placebo group. The median time to a complete or near-complete response was 5.1 months in the lenalidomide group and 6.9 months in the placebo group. The median duration of the response was longer in the lenalidomide group (16.5 months) than in the placebo group (7.9 months, P=0.02).\n\n【38】The analysis of overall response rates (complete, near-complete, and partial responses) for each stratification group and for patients with and those without previous thalidomide treatment showed a higher response rate among patients receiving lenalidomide in all the subgroups . In the lenalidomide group, the overall response rate was higher in patients who had not received thalidomide than in those who had received thalidomide (65.0% vs. 49.1%, P=0.07).\n\n【39】Survival\n--------\n\n【40】Figure 2. Kaplan–Meier Curves for Overall Survival among All Patients and in Subgroups with and without Previous Exposure to Thalidomide.\n\n【41】Panel A shows the estimates of median overall survival for the intention-to-treat population (not yet reached in the lenalidomide group and 20.6 months in the placebo group, P<0.001 by the log-rank test). Panel B shows the median overall survival among patients in the two study groups who received thalidomide before study entry and those who did not receive thalidomide (in the lenalidomide group, the median was not yet reached in either subgroup; in the placebo group, 23.5 months among those who did not receive thalidomide and 18.2 months among those who did receive thalidomide; P=0.04 by the log-rank test for the between-group comparison of patients who received thalidomide and P=0.21 for the between-group comparison of patients who did not receive thalidomide).\n\n【42】As of May 2006, 47 patients (26.7%) in the lenalidomide group had died (30 from progressive disease), as had 60 patients (34.3%) in the placebo group (49 from progressive disease). At the time of the last analysis, median overall survival had not been reached in the lenalidomide group and was 20.6 months in the placebo group (hazard ratio for death in the lenalidomide group, 0.66; 95% CI, 0.45 to 0.96; P=0.03) . Overall survival was also significantly improved in the lenalidomide group among patients who had previously received thalidomide (hazard ratio, 2.07; 95% CI, 1.02 to 4.21; P=0.04) .\n\n【43】Adverse Events\n--------------\n\n【44】Table 3. Grade 3 and 4 Adverse Events.\n\n【45】The most frequently reported adverse events were neutropenia, muscle cramps, constipation, nausea, tremor, and dizziness. Table 3 lists all grade 3 or 4 adverse events. Patients in the lenalidomide group had a higher incidence of grade 3 neutropenia (25.0%) than did those in the placebo group (2.3%). However, grade 3 or 4 febrile neutropenia was rare (occurring in 3.4% of the patients in the lenalidomide group and in none of those in the placebo group). Grade 3 or 4 thrombocytopenia was twice as frequent in the lenalidomide group as in the placebo group (11.4% vs. 5.7%). The incidence of grade 3 or 4 somnolence, constipation, or peripheral neuropathy (all toxic effects of thalidomide) was less than 10% in the two groups and rarely resulted in a dose reduction.\n\n【46】Lenalidomide was associated with higher incidences of deep-vein thrombosis (4.0% vs. 3.5%) and pulmonary embolism (4.5% vs. 1.2%) than was placebo. The rate of grade 3 or 4 thromboembolic complications was unrelated to the concomitant administration of erythropoietin. Such events occurred in 3 of 38 patients (7.9%) who received erythropoietin and 17 of 138 patients (12.3%) who did not receive erythropoietin in the lenalidomide group (P=0.57) and in 3 of 36 patients (8.3%) who received erythropoietin and 5 of 139 patients (3.6%) who did not receive erythropoietin in the placebo group (P=0.36).\n\n【47】The proportion of patients who required more than one dose reduction or interruption of lenalidomide or dexamethasone was similar in the two groups. The mean time until the first dose reduction or interruption of a study drug was also similar in the two groups (125 days in the lenalidomide group and 128 days in the placebo group). Dose reduction or interruption because of adverse events was more common in the lenalidomide group (occurring in 76.1% of the patients) than in the placebo group (56.9%, P<0.001). There were 11 deaths that were possibly related to a study drug: 5 in the lenalidomide group (1 from cardiac arrest, 1 from pulmonary embolism, 1 from leukoencephalopathy, 1 from pneumonia bacteria, and 1 from sudden death) and 6 in the placebo group (3 from sepsis, 1 from hepatic failure, 1 from a cerebrovascular event, and 1 from gastrointestinal hemorrhage). The primary reason for the discontinuation of treatment in the two groups was disease progression; 31 patients in the two groups (8.8%) discontinued treatment early because of adverse events.\n\n【48】Granulocyte colony-stimulating factor was administered if only grade 3 or 4 myelosuppression occurred; with other grade 3 or 4 adverse events, the lenalidomide dose was reduced. In the lenalidomide group, 38 patients (21.6%) received granulocyte colony-stimulating factor during the study. Of these patients, 23 (60.5%) received granulocyte colony-stimulating factor as the first step after having grade 3 or 4 neutropenia to maintain the 25-mg dose level. Among these 23 patients, 12 (52.2%) were able to continue with the 25-mg dose level of lenalidomide from the time of the first episode of grade 3 or 4 neutropenia until the last follow-up visit, as long as that period of time was at least 3 months.\n\n【49】Discussion\n----------\n\n【50】We found that in patients with relapsed or refractory multiple myeloma, lenalidomide plus dexamethasone increased the time to progression, the rate of response (both overall and complete responses), and overall survival, as compared with placebo plus dexamethasone. The median time to progression of 11.3 months and the 60.2% response rate in the lenalidomide group are among the highest values that have been reported in the treatment of relapsed or refractory multiple myeloma. These findings support the early use of lenalidomide in patients with progressive multiple myeloma.\n\n【51】In addition, nearly one third of patients in our trial had previously received thalidomide. Our data indicate that lenalidomide can be administered to patients who have received previous treatment with thalidomide without deterioration of preexisting thalidomide-related neuropathy; in our study, nearly 50% of such patients had a response to lenalidomide, with a median time to progression of 8.4 months. As compared with patients who had not been exposed to thalidomide, patients who had previously been treated with the drug had received an additional year of treatment and had also received one more previous therapy. Given that current therapy includes thalidomide and dexamethasone, this finding is encouraging for patients with disease that is resistant to thalidomide treatment.\n\n【52】The primary toxic effects of the lenalidomide regimen were hematologic, and we found them to be manageable with adjustment of the dose of lenalidomide. Neutropenia was managed with dose adjustments, the administration of granulocyte colony-stimulating factor, or both, whereas thromboembolic events were managed with anticoagulants. Lenalidomide was not associated with the peripheral neuropathy that is caused by long-term treatment with thalidomide. In summary, we found that therapy with lenalidomide plus dexamethasone was more effective in patients with relapsed or refractory multiple myeloma than was therapy with placebo plus dexamethasone.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ae3b5473-e3a6-4b14-9a2a-7040808c7430", "title": "Albuterol–Budesonide Fixed-Dose Combination Rescue Inhaler for Asthma", "text": "【0】Albuterol–Budesonide Fixed-Dose Combination Rescue Inhaler for Asthma\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】As asthma symptoms worsen, patients typically rely on short-acting β <sub>2 </sub> \\-agonist (SABA) rescue therapy, but SABAs do not address worsening inflammation, which leaves patients at risk for severe asthma exacerbations. The use of a fixed-dose combination of albuterol and budesonide, as compared with albuterol alone, as rescue medication might reduce the risk of severe asthma exacerbation.\n\n【3】Methods\n-------\n\n【4】We conducted a multinational, phase 3, double-blind, randomized, event-driven trial to evaluate the efficacy and safety of albuterol–budesonide, as compared with albuterol alone, as rescue medication in patients with uncontrolled moderate-to-severe asthma who were receiving inhaled glucocorticoid-containing maintenance therapies, which were continued throughout the trial. Adults and adolescents (≥12 years of age) were randomly assigned in a :1 ratio to one of three trial groups: a fixed-dose combination of 180 μg of albuterol and 160 μg of budesonide (with each dose consisting of two actuations of 90 μg and 80 μg, respectively \\[the higher-dose combination group\\]), a fixed-dose combination of 180 μg of albuterol and 80 μg of budesonide (with each dose consisting of two actuations of 90 μg and 40 μg, respectively \\[the lower-dose combination group\\]), or 180 μg of albuterol (with each dose consisting of two actuations of 90 μg \\[the albuterol-alone group\\]). Children 4 to 11 years of age were randomly assigned to only the lower-dose combination group or the albuterol-alone group. The primary efficacy end point was the first event of severe asthma exacerbation in a time-to-event analysis, which was performed in the intention-to-treat population.\n\n【5】Results\n-------\n\n【6】A total of 3132 patients underwent randomization, among whom 97% were 12 years of age or older. The risk of severe asthma exacerbation was significantly lower, by 26%, in the higher-dose combination group than in the albuterol-alone group (hazard ratio, 0.74; 95% confidence interval \\[CI\\], 0.62 to 0.89; P=0.001). The hazard ratio in the lower-dose combination group, as compared with the albuterol-alone group, was 0.84 (95% CI, 0.71 to 1.00; P=0.052). The incidence of adverse events was similar in the three trial groups.\n\n【7】Conclusions\n-----------\n\n【8】The risk of severe asthma exacerbation was significantly lower with as-needed use of a fixed-dose combination of 180 μg of albuterol and 160 μg of budesonide than with as-needed use of albuterol alone among patients with uncontrolled moderate-to-severe asthma who were receiving a wide range of inhaled glucocorticoid-containing maintenance therapies. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nAlbuterol–Budesonide Rescue Inhaler in Asthma  \n\n【11】Asthma is a heterogeneous disease that manifests as variable airflow obstruction with recurring symptoms driven by underlying persistent, yet fluctuating, airway inflammation.  During a loss of asthma control, patients often focus on obtaining immediate symptom relief by relying on their rescue medication, typically a short-acting β <sub>2 </sub> \\-agonist (SABA).  However, SABAs have little effect on underlying airway inflammation,  and overreliance on SABAs serves as a metric for poor asthma control, with an associated risk of severe asthma exacerbation.  Because severe asthma exacerbations contribute to considerable morbidity and mortality, the prevention of exacerbations is imperative in the management of asthma.\n\n【12】Concerns regarding adverse consequences associated with overreliance on SABAs and evidence that rescue use of a fixed-dose combination of an inhaled glucocorticoid and formoterol, as compared with a SABA, significantly reduced the risk of exacerbation among patients with a range of asthma severity  led the Global Initiative for Asthma (GINA)  and the National Asthma Education and Prevention Program  to generally recommend as-needed use of this combination as the preferred rescue-treatment strategy. Rapid-onset bronchodilators, such as formoterol and albuterol, are ideal for a rescue fixed-dose combination with an inhaled glucocorticoid, as compared with slower-onset bronchodilators, such as salmeterol.  However, among patients with moderate-to-severe asthma, data regarding this strategy are limited to the as-needed use of the same agents (budesonide plus formoterol) in the same inhaler device the patients had been using for maintenance therapy.  Therefore, evaluation of an inhaled glucocorticoid with a fast-acting bronchodilator in a fixed-dose combination as a rescue medication that could be used in addition to any inhaled glucocorticoid-containing maintenance therapy, as compared with SABA as a rescue medication, is warranted. A fixed-dose combination of inhaled albuterol and budesonide, as compared with albuterol alone, as a rescue medication was considered to be most appropriate, because albuterol is the most commonly used rescue medication worldwide, and SABAs are the only class of rescue medication approved by the Food and Drug Administration in the United States. \n\n【13】A new formulation of albuterol and budesonide  in a single pressurized metered-dose inhaler was developed as albuterol–inhaled glucocorticoid rescue therapy for the control of acute asthma symptoms, the treatment and prevention of bronchoconstriction, and the prevention of exacerbations. The primary objective of the MANDALA trial was to evaluate the efficacy and safety of as-needed use of albuterol–budesonide, as compared with as-needed use of albuterol alone, in patients with moderate-to-severe asthma.\n\n【14】Methods\n-------\n\n【15】Patients\n--------\n\n【16】Symptomatic patients with asthma who were 4 years of age or older and had had at least one severe asthma exacerbation in the previous 12 months were recruited. Severe asthma exacerbation was defined as clinical deterioration of asthma, with a worsening or a new onset of symptoms leading to at least one of the following events: 3 or more consecutive days of treatment with a systemic glucocorticoid to treat worsening symptoms of asthma (a single depot injection was considered to be equivalent to a 3-day burst); an emergency department or urgent care visit of less than 24 hours during which systemic glucocorticoids were used to treat worsening symptoms of asthma; or an in-patient hospitalization for 24 hours or more because of asthma. Additional inclusion criteria were a forced expiratory volume in 1 second (FEV <sub>1 </sub> ) of 40 to less than 90% of the predicted normal value (with no upper limit for patients 4 to 17 years of age); FEV <sub>1 </sub> reversibility of at least 12%, as measured during an in-clinic screening visit; and a score on the Asthma Control Questionnaire–5 (ACQ-5) of 1.5 or greater at visit 2 (day 1 of the double-blind treatment period), which indicates poorly controlled asthma. \n\n【17】The patients had been receiving a medium-to-high dose of inhaled glucocorticoid or a low-to-high dose of inhaled glucocorticoid–long-acting β <sub>2 </sub> \\-agonist combination, as defined by GINA,  with or without another controller, for at least 3 months with stable dosing for at least 4 weeks before screening. They continued to receive their maintenance medications throughout the trial. Major exclusion criteria were chronic obstructive pulmonary disease or other notable lung disease, use of a systemic glucocorticoid within 3 months before screening, and use of biologic treatments within 3 months or for a duration of 5 half-lives before screening.\n\n【18】Trial Design\n------------\n\n【19】The MANDALA trial was a multinational, phase 3, double-blind, randomized, parallel-group, event-driven trial with a minimum duration of 24 weeks. The trial was conducted at 295 sites in North America, South America, Europe, and South Africa  and was continued until at least 570 first events of severe asthma exacerbation had been reported . \n\n【20】Adults and adolescents were randomly assigned in a :1 ratio to one of three treatment groups: a fixed-dose combination of 180 μg of albuterol and 160 μg of budesonide (with each dose consisting of two actuations of 90 μg and 80 μg, respectively \\[the higher-dose combination group\\]), a fixed-dose combination of 180 μg of albuterol and 80 μg of budesonide (with each dose consisting of two actuations of 90 μg and 40 μg, respectively \\[the lower-dose combination group\\]), or 180 μg of albuterol (with each dose consisting of two actuations of 90 μg \\[the albuterol-alone group\\]). The trial medications were delivered through a pressurized metered-dose inhaler. Children 4 to 11 years of age were randomly assigned to the lower-dose combination group or to the albuterol-alone group owing to concerns about higher doses of inhaled glucocorticoids in this younger population. The patients were instructed on the proper technique for using the pressurized metered-dose inhaler, and their technique was checked and confirmed by the staff at the trial site. The patients were told to use the trial medications as needed in response to symptoms and that the trial medications could be used before exercise. Rescue use was limited to the trial medications throughout the trial; additional fast-acting bronchodilators, including nebulizers, were prohibited for rescue use. Changes in maintenance therapy were discouraged unless clinically indicated. Details on permitted and prohibited medication use during the trial are provided in the Supplementary Appendix .\n\n【21】The trial procedures have been described previously.  Adherence to maintenance therapy and the use of the assigned trial medication were documented by the patients or their parents or guardians with the use of an electronic diary and were monitored by the investigators and staff at the trial site and by the sponsor (Avillion). The maximum daily dose of a trial medication was 12 inhalations (i.e., 6 doses) for all the patients. Patients who had three or more severe asthma exacerbations within a 3-month period or a total of five or more severe asthma exacerbations were assessed for possible discontinuation of the trial medication.\n\n【22】Trial Oversight\n---------------\n\n【23】The trial design was approved by the appropriate institutional and national regulatory authorities and ethics committees; all the patients or their guardians provided written informed consent (and assent, if appropriate). An independent data and safety monitoring board reviewed unblinded data every 3 months to monitor safety. Avillion coordinated data management and the statistical analyses in conjunction with the responsible contract research organizations (Syneos Health and Phastar, respectively). All the authors contributed to the design of the trial and the interpretation of the data. The first draft of the manuscript was written by a medical writer (funded by AstraZeneca) under the direction of the authors and in accordance with Good Publication Practice guidelines. All the authors provided critical feedback on the first and subsequent drafts of the manuscript and, along with the sponsor, made the decision to submit the manuscript for publication.\n\n【24】End Points\n----------\n\n【25】The primary efficacy end point was the first event of severe asthma exacerbation in a time-to-event analysis. Secondary efficacy end points were the annualized rate of severe asthma exacerbations, total systemic glucocorticoid exposure for asthma during the treatment period, and “response” at week 24 on the ACQ-5 (validated for persons ≥6 years of age),  the Asthma Quality of Life Questionnaire (AQLQ+12, validated for persons ≥12 years of age), and the Pediatric Asthma Quality of Life Questionnaire (PAQLQ, validated for persons aged 7 to 11 years); patients 4 to 6 years of age completed the PAQLQ with the help of a caregiver.  Scores on the ACQ-5 range from 0 to 6, with lower scores indicating better asthma control (minimum clinically important difference, −0.5 points).  Scores on both the AQLQ+12 and the PAQLQ range from 1 to 7, with higher scores indicating better asthma-related quality of life (minimum clinically important difference, 0.5 points).  A response was defined as a decrease (in the ACQ-5 score) or an increase (in the AQLQ+12 or PAQLQ score) of at least 0.5 points from the baseline score.\n\n【26】Safety end points were incident adverse events and incident serious adverse events. These events were assessed from the time that written informed consent (and assent, as appropriate) was obtained through the end of the safety follow-up period (2 weeks after the last in-clinic visit).\n\n【27】Statistical Analysis\n--------------------\n\n【28】The preplanned efficacy analyses were designed to answer the clinical question of interest, that is, whether budesonide, administered in a fixed-dose combination with albuterol on an as-needed basis, provides a benefit that is greater than that with as-needed use of albuterol alone in patients who continued to receive their prescribed maintenance therapy. These analyses used data that were collected during the on-treatment period before treatment discontinuation or a change in maintenance therapy; in the primary end-point analysis, data were censored at the date of treatment discontinuation or a change in maintenance therapy.  The results of an alternative, prespecified intention-to-treat analysis that was consistent with the _Journal’s_ statistical guidelines and included all the data, regardless of a change in maintenance therapy or treatment discontinuation, are presented first in the article. The term “preplanned” is used when referring to the primary efficacy analysis stated in the statistical analysis plan (with type 1 error control), and the term “prespecified” is used when referring to the intention-to-treat analysis.\n\n【29】According to the statistical analysis plan (available with the protocol ) for the preplanned efficacy analyses, the type I error for the primary end point was controlled for comparisons between each albuterol–budesonide dose group and the albuterol-alone group with the use of the Hochberg step-up procedure. Secondary end points were controlled with the use of a hierarchical testing sequence for treatment comparisons between the higher-dose combination group and the albuterol-alone group and between the lower-dose combination group and the albuterol-alone group with respect to each secondary end point. In the alternative intention-to-treat analysis, these type 1 error control procedures were applied in this same manner as in the preplanned efficacy analyses of the primary and secondary end points.\n\n【30】We estimated that a sample of 1000 adults and adolescents per trial group and 570 first events of severe asthma exacerbation would provide the trial with 87% power to detect a 25% lower risk of severe asthma exacerbation with the fixed-dose combination of albuterol–budesonide than with albuterol alone, assuming a two-sided significance level of 5% and a probability of a first severe exacerbation event of 0.22 with the use of albuterol alone.  In addition, our aim was to recruit 100 children 4 to 11 years of age in accordance with regulatory input.\n\n【31】All patients who had undergone randomization and received any amount of a trial medication, which was classified according to the trial medication they had been assigned to receive, were included in both efficacy analyses comparing the lower-dose combination group with the albuterol-alone group. Children 4 to 11 years of age were not assigned to the higher-dose combination group. Therefore, children 4 to 11 years of age were not included in the albuterol-alone group in the comparison with the higher-dose combination group. The safety analyses included all the patients who had received any amount of a trial medication, which was classified according to the trial medication they had actually received.\n\n【32】The time-to-event analysis of the primary end point of the first severe asthma exacerbation was performed with the use of a Cox proportional-hazards regression model that adjusted for the randomization stratification factors of age group (≥4 to <12, ≥12 to <18, ≥18 to <65, and ≥65 years), geographic region (North America, Western Europe, or South Africa vs. South America and the rest of Europe), and the number of severe asthma exacerbations in the 12 months before screening. The ratio of the hazard rates (hazard ratio) obtained in the primary end-point analysis was used as a measure of the (relative) risk of a severe asthma exacerbation event in order to make it distinct from the annualized rate of severe asthma exacerbations obtained in the secondary analysis.\n\n【33】The annualized rate of severe asthma exacerbations was analyzed with the use of a negative binomial regression model that adjusted for age, geographic region, number of severe asthma exacerbations in the 12 months before screening, and person-time at risk. Total systemic glucocorticoid exposure per patient was calculated as the annualized total dose of systemic glucocorticoids (in milligrams per year), which was analyzed with the use of a Wilcoxon rank-sum test. The response variables with respect to the ACQ-5, the AQLQ+12, and the PAQLQ at week 24 were compared among the trial groups with the use of a logistic-regression model that adjusted for baseline values, the randomization stratification factors, and the number of severe asthma exacerbations in the 12 months before screening.\n\n【34】Results\n-------\n\n【35】Patients\n--------\n\n【36】Figure 1. Screening, Randomization, and Analyses.\n\n【37】All 37 patients who remained in the treatment phase of the trial after the primary database lock to complete 24 weeks across the three trial groups were children or adolescents; the 1 patient in the lower-dose combination group who remained in the 2-week safety follow-up period after the primary database lock was an adolescent.\n\n【38】A total of 5620 patients were enrolled in the trial between December 27, 2018, and July 30, 2021. Among these patients, 3132 underwent randomization, 3123 were assessed with respect to the efficacy end points (5 patients who had not received any trial medication and 4 who had been withdrawn because of randomization at more than one site were excluded), and 3127 were assessed with respect to the safety end points . Data are presented up to August 23, 2021, the time of the primary database lock; a total of 37 children and adolescents remained in the treatment phase of the trial after database lock to complete 24 weeks, and 1 adolescent remained in the 2-week safety follow-up period after database lock.\n\n【39】Table 1. Demographic and Clinical Characteristics of the Patients at Screening.\n\n【40】The characteristics of the patients at screening are provided in Table 1 and Table S1. At baseline, the mean ACQ-5 score was 2.6 across the three trial groups, a result indicating poorly controlled asthma. During the trial, patients reported that the mean (±SD) percentage of days they had taken their maintenance medication was 74.7±25.6% (median, 84.6%). Adherence was similar in the three trial groups . Overall, 39 (1.2%) of the patients had a change in maintenance therapy during the trial .\n\n【41】Primary End Point\n-----------------\n\n【42】### _Intention-to-Treat Analysis_\n\n【43】Figure 2. Time-to-Event Analysis of the First Event of Severe Asthma Exacerbation (Primary End Point).\n\n【44】Data are presented for all the patients. Children 4 to 11 years of age were excluded in the comparison between the higher-dose combination group and the albuterol-alone group; therefore, the number of patients in the albuterol-alone group in this comparison (1014 patients) was lower than that in the comparison with the lower-dose combination group, in which children 4 to 11 years of age were included. The anticipated probability of a first event of severe asthma exacerbation among the patients who received albuterol alone was 0.22.\n\n【45】The intention-to-treat analysis showed that the risk of a severe asthma exacerbation, in a time-to-event analysis, was significantly lower, by 26%, in the higher-dose combination group than in the albuterol-alone group (hazard ratio, 0.74; 95% confidence interval \\[CI\\], 0.62 to 0.89; P=0.001) . The hazard ratio in the lower-dose combination group, as compared with the albuterol-alone group, was 0.84 (95% CI, 0.71 to 1.00; P=0.052). Further inferential testing was not performed, in accordance with the hierarchical testing strategy applied to this alternative, prespecified analysis to control for type I error.\n\n【46】### _Preplanned On-Treatment Efficacy Analysis_\n\n【47】In the preplanned efficacy analysis that included data collected during the on-treatment period before treatment discontinuation or a change in maintenance therapy, the hazard ratio for severe asthma exacerbation in the higher-dose combination group, as compared with the albuterol-alone group, was 0.73 (95% CI, 0.61 to 0.88). The hazard ratio in the lower-dose combination group, as compared with the albuterol-alone group, was 0.83 (95% CI, 0.70 to 0.99) . The results of the preplanned time-to-event analyses of the first severe asthma exacerbation according to subgroups are provided in Figure S2.\n\n【48】Secondary End Points\n--------------------\n\n【49】### _Intention-to-Treat Analysis_\n\n【50】Table 2. Intention-to-Treat and Preplanned On-Treatment Efficacy Analyses of the Secondary End Points.\n\n【51】The annualized rate of severe asthma exacerbations was 0.43 (95% CI, 0.33 to 0.58) in the higher-dose combination group and 0.58 (95% CI, 0.44 to 0.77) in the albuterol-alone group (rate ratio, 0.75; 95% CI, 0.61 to 0.91) . The annualized rate of severe asthma exacerbations was 0.48 (95% CI, 0.37 to 0.63) in the lower-dose combination group and 0.60 (95% CI, 0.46 to 0.79) in the albuterol-alone group (rate ratio, 0.81; 95% CI, 0.66 to 0.98).\n\n【52】The mean (±SD) annualized total dose of systemic glucocorticoid (in prednisone equivalents) was 83.6±247.7 mg in the higher-dose combination group and 130.0±630.3 mg in the albuterol-alone group. The mean annualized total dose of systemic glucocorticoid was 94.7±318.2 mg in the lower-dose combination group and 127.6±619.8 mg in the albuterol-alone group. Post hoc results of the intention-to-treat analyses of response on the ACQ-5 and the AQLQ+12 are provided in Table 2 .\n\n【53】### _Preplanned On-Treatment Efficacy Analysis_\n\n【54】The annualized rate of severe asthma exacerbations was 0.45 (95% CI, 0.34 to 0.60) in the higher-dose combination group and 0.59 (95% CI, 0.44 to 0.78) in the albuterol-alone group (rate ratio, 0.76; 95% CI, 0.62 to 0.93). The annualized rate of severe asthma exacerbations was 0.49 (95% CI, 0.37 to 0.64) in the lower-dose combination group and 0.61 (95% CI, 0.46 to 0.80) in the albuterol-alone group (rate ratio, 0.80; 95% CI, 0.66 to 0.98) . The number of patients with at least one severe asthma exacerbation that led to hospitalization was 9 in the higher-dose combination group, 10 in the lower-dose combination group, and 17 in the albuterol-alone group . The number of patients with at least one severe asthma exacerbation that led to emergency department or urgent care visits was 49 in the higher-dose combination group, 50 in the lower-dose combination group, and 66 in the albuterol-alone group.\n\n【55】The mean annualized total dose of systemic glucocorticoid (in prednisone equivalents) was 86.2±262.9 mg in the higher-dose combination group and 129.3±657.2 mg in the albuterol-alone group . The mean annualized total dose of systemic glucocorticoid was 95.5±335.4 mg in the lower-dose combination group and 127.1±646.2 mg in the albuterol-alone group.\n\n【56】At week 24, a response on the ACQ-5 (i.e., a decrease of at least 0.5 points from the baseline score) was observed in 66.8% of the patients in the higher-dose combination group and in 62.1% of those in the albuterol-alone group, for an odds ratio of 1.22 (95% CI, 1.02 to 1.47). A response on the ACQ-5 was observed in 64.7% of the patients in the lower-dose combination group and in 61.6% of those in the albuterol-alone group, for an odds ratio of 1.13 (95% CI, 0.95 to 1.35). A response on the AQLQ+12 at week 24 (i.e., an increase of at least 0.5 points from the baseline score) was observed in 51.1% of the patients in the higher-dose combination group, in 49.5% of those in the lower-dose combination group, and in 46.4% of those in the albuterol-alone group, for an odds ratio of 1.23 (95% CI, 1.02 to 1.48) in the comparison between the higher-dose combination group and the albuterol-alone group and an odds ratio of 1.11 (95% CI, 0.93 to 1.34) in the comparison between the lower-dose combination group and the albuterol-alone group. The results of the ACQ-5, AQLQ+12, and PAQLQ response analyses are provided in Table 2 and Table S4. The results with respect to the preplanned exploratory end points of prebronchodilator FEV <sub>1 </sub> , morning and evening peak expiratory flow, and daytime and night-time symptoms of asthma are summarized in the Supplementary Appendix .\n\n【57】Trial Medication Use\n--------------------\n\n【58】The overall pattern of as-needed use of trial medications was similar in the three trial groups, with increased use around the time of clinical deterioration of asthma . Patients reported using 2 or fewer inhalations on the majority of trial days (mean percentage of days with ≤2 inhalations: 53.7% in the higher-dose combination group, 52.6% in the lower-dose combination group, and 51.0% in the albuterol-alone group), and patients reported using more than 8 inhalations on less than 2% of trial days . Average daily as-needed use was similar in the three trial groups, with a mean of 2.6 inhalations per day in the higher-dose combination group, 2.7 inhalations per day in the lower-dose combination group, and 2.8 inhalations per day in the albuterol-alone group, which is equal to approximately 1.3, 1.3, and 1.4 doses, respectively, of trial medication per day.\n\n【59】Safety End Points\n-----------------\n\n【60】The percentage of patients with any adverse event was similar in the three trial groups: 46.2% in the higher-dose combination group, 47.1% in the lower-dose combination group, and 46.4% in the albuterol-alone group . The percentage of patients with serious adverse events, including deaths, was 5.2% in the higher-dose combination group, 3.8% in the lower-dose combination group, and 4.5% in the albuterol-alone group. The percentage of patients with adverse events leading to discontinuation of the trial medication was 1.0% in the higher-dose combination group, 0.9% in the lower-dose combination group, and 0.9% in the albuterol-alone group. Seven patients had died — four in the higher-dose combination group (two from coronavirus disease 2019 \\[Covid-19\\], one from an elevated glucose level, and one from cardiac arrest), two in the lower-dose combination group (one from Covid-19 and one from lung metastasis with pneumothorax), and one in the albuterol-alone group (from Covid-19). No deaths were considered by the trial investigators to be related to the trial medication.\n\n【61】Table 3. Adverse Events Occurring in at Least 2% of Patients in Any Trial Group.\n\n【62】The most common adverse events were nasopharyngitis, headache, and upper respiratory tract infections; Covid-19 was recorded in 4.2 to 4.9% of patients . The percentage of patients with adverse events associated with the use of inhaled glucocorticoids was similar in the three trial groups, ranging from 1.3% in the albuterol-alone group to 2.0% in the higher-dose combination group . The three most common adverse events associated with the use of inhaled glucocorticoids were oral candidiasis (1.0% in the higher-dose combination group, 0.9% in the lower-dose combination group, and 0.5% in the albuterol-alone group), dysphonia (0.4%, 0.6%, and 0.4%, respectively), and oropharyngeal candidiasis (0.3%, 0.3%, and 0.1%, respectively).\n\n【63】Discussion\n----------\n\n【64】Among patients with uncontrolled moderate-to-severe asthma who were receiving inhaled glucocorticoid-containing maintenance therapy, the risk of severe asthma exacerbation was significantly lower with a fixed-dose combination of 180 μg of albuterol and 160 μg of budesonide, administered on an as-needed basis in two actuations of 90 μg and 80 μg, respectively, than with as-needed albuterol alone. The results were similar in the intention-to-treat analysis and in the analysis that included data collected during the on-treatment period before treatment discontinuation or a change in maintenance therapy. The findings in both analyses showed that the annualized rate of severe asthma exacerbations was numerically lower with each albuterol–budesonide dose than with albuterol and that the mean total systemic glucocorticoid exposure among the patients in the higher-dose combination group was numerically lower than that in the albuterol-alone group. Both doses of albuterol–budesonide had an acceptable safety profile that was consistent with that of the active components, with no safety concerns identified.\n\n【65】The pattern of rescue use of the trial medications was similar in the three trial groups, as assessed on the basis of the percentage of trial days with rescue use and use around the time of clinical deterioration of asthma, which was increased in all three groups. The finding of a mean number of medication doses per day of less than 1.5 across the three trial groups shows that the patients used the albuterol–budesonide combination as they used albuterol alone. Unlike albuterol monotherapy, the fixed-dose combination allows patients to adjust the dose of inhaled glucocorticoid according to their own symptom-driven use of a bronchodilator in response to worsening asthma episodes.\n\n【66】The findings from the MANDALA trial with respect to a reduction in the risk of exacerbations are consistent with those from previous trials of the inclusion of an inhaled glucocorticoid when rescue medication is taken, as shown in a trial of as-needed use of albuterol–beclomethasone as compared with as-needed use of albuterol alone in patients with mild asthma  and in a more recent real-world, open-label trial of a free combination of beclomethasone in addition to a rescue medication in Black patients and Latinx patients with uncontrolled moderate-to-severe asthma.  Similarly, multiple trials that evaluated the single maintenance and reliever therapy (SMART) strategy, in which the same agents (budesonide plus formoterol) used as maintenance therapy are used as rescue therapy, as compared with rescue therapy with a SABA in addition to budesonide plus formoterol as maintenance therapy, showed a reduction in the risk of exacerbation in patients with moderate-to-severe asthma.  The use of albuterol–budesonide as rescue medication in the current trial addresses the limited data regarding the rescue use of inhaled glucocorticoid–formoterol, for which data are lacking in patients receiving maintenance treatment with an alternative inhaled glucocorticoid–long-acting β <sub>2 </sub> \\-agonist combination or inhaled glucocorticoid alone. Given the risks and limitations of SABA alone as rescue therapy, national and international recommendations call for an inhaled glucocorticoid-containing rescue medication as the preferred as-needed treatment; the data from this trial support that approach. Given its acceptable safety profile, the greater efficacy of the fixed-dose combination than of albuterol alone, as well as the absence of a need to change underlying maintenance therapy, indicates that this fixed-dose combination could replace SABA alone as rescue therapy in patients with moderate-to-severe asthma.\n\n【67】The strengths of our trial are the low dropout rate, with 93% of the patients completing at least 24 weeks of the treatment period, despite the trial having been conducted during the global Covid-19 pandemic, and the multinational and double-blind trial design, which increased external and internal validity, respectively. Albuterol–budesonide was used in addition to the usual maintenance therapy the patients were receiving, which included a range of inhaled glucocorticoid-containing medications, with the aim of reflecting a real-world population and improving the generalizability of our results. The limitations of our trial are the lack of measurements of the fraction of exhaled nitric oxide level, which would have allowed for a direct assessment of antiinflammatory effects; the small number of children, which precludes conclusions being drawn in this important subpopulation; and the fact that growth indexes could not be assessed because of the small numbers and short period of observation of children in this trial.\n\n【68】In the current phase 3 trial involving patients with uncontrolled moderate-to-severe asthma, the risk of severe asthma exacerbation was significantly lower with as-needed use of a fixed-dose combination of 180 μg of albuterol and 160 μg of budesonide than with as-needed use of albuterol alone.\n\n【69】Table 1. Demographic and Clinical Characteristics of the Patients at Screening. \n\n| Characteristic | Albuterol (180 μg)–Budesonide (160 μg)(N=1013) | Albuterol (180 μg)–Budesonide (80 μg)(N=1054) | Albuterol(180 μg)(N=1056) | All Patients(N=3123) |\n| --- | --- | --- | --- | --- |\n| Age |  |  |  |  |\n| Mean — yr | 50.6±15.1 | 48.5±16.7 | 49.1±17.2 | 49.4±16.4 |\n| Distribution — no. (%) |  |  |  |  |\n| ≥4 to <12 yr | 0 | 41 (3.9) | 42 (4.0) | 83 (2.7) |\n| ≥12 to <18 yr | 34 (3.4) | 32 (3.0) | 34 (3.2) | 100 (3.2) |\n| ≥18 to <65 yr | 787 (77.7) | 804 (76.3) | 783 (74.1) | 2374 (76.0) |\n| ≥65 yr | 192 (19.0) | 177 (16.8) | 197 (18.7) | 566 (18.1) |\n| Female sex — no. (%) | 645 (63.7) | 685 (65.0) | 694 (65.7) | 2024 (64.8) |\n| Race or ethnic group — no. (%) |  |  |  |  |\n| White | 818 (80.8) | 847 (80.4) | 868 (82.2) | 2533 (81.1) |\n| Black | 139 (13.7) | 141 (13.4) | 137 (13.0) | 417 (13.4) |\n| Asian | 29 (2.9) | 33 (3.1) | 23 (2.2) | 85 (2.7) |\n| American Indian or Alaska Native | 1 (0.1) | 1 (0.1) | 0 | 2 (0.1) |\n| Other | 26 (2.6) | 32 (3.0) | 28 (2.7) | 86 (2.8) |\n| Hispanic or Latinx — no. (%) |  |  |  |  |\n| Yes | 233 (23.0) | 260 (24.7) | 315 (29.8) | 808 (25.9) |\n| No | 780 (77.0) | 794 (75.3) | 741 (70.2) | 2315 (74.1) |\n| Geographic region — no. (%) |  |  |  |  |\n| North America, Western Europe, and South Africa | 536 (52.9) | 556 (52.8) | 563 (53.3) | 1655 (53.0) |\n| South America and rest of Europe | 477 (47.1) | 498 (47.2) | 493 (46.7) | 1468 (47.0) |\n| Prebronchodilator FEV 1 |  |  |  |  |\n| Mean volume — liters  | 1.9±0.6 | 1.9 ±0.6 | 1.9±0.6 | 1.9±0.6 |\n| Mean percent of predicted value  | 63.4±12.8 | 64.0±13.7 | 64.4±13.3 | 63.9±13.3 |\n| Mean reversibility in FEV 1 — %  | 27.7±17.2 | 27.2±14.2 | 27.8±15.9 | 27.6±15.8 |\n| Maintenance treatment — no. (%) |  |  |  |  |\n| Low-dose inhaled glucocorticoid–LABA or medium-dose inhaled glucocorticoid | 314 (31.0) | 334 (31.7) | 308 (29.2) | 956 (30.6) |\n| Medium-dose inhaled glucocorticoid–LABA or high-dose inhaled glucocorticoid | 385 (38.0) | 435 (41.3) | 441 (41.8) | 1261 (40.4) |\n| High-dose inhaled glucocorticoid–LABA | 295 (29.1) | 267 (25.3) | 285 (27.0) | 847 (27.1) |\n| Missing | 19 (1.9) | 18 (1.7) | 22 (2.1) | 59 (1.9) |\n| Severe asthma exacerbations in the 12 mo before screening — no. (%) |  |  |  |  |\n| 1 | 788 (77.8) | 822 (78.0) | 840 (79.5) | 2450 (78.5) |\n| 2 | 185 (18.3) | 185 (17.6) | 164 (15.5) | 534 (17.1) |\n| 3 | 27 (2.7) | 38 (3.6) | 45 (4.3) | 110 (3.5) |\n| ≥4 | 13 (1.3) | 9 (0.9) | 7 (0.7) | 29 (0.9) |\n\n【71】 Plus–minus values are means ±SD. The analysis includes 3123 patients; 5 patients who had not received any amount of a trial medication and 4 who had been withdrawn because of randomization at more than one site were excluded. Medications for rescue use were limited to the trial medications throughout the trial; additional fast-acting bronchodilators, including nebulizers, were prohibited. Additional controller medications were used by approximately 15% of the patients: approximately 10% used a leukotriene-receptor antagonist, 4% a long-acting muscarinic antagonist, and 2% a xanthine. Changes to maintenance therapy were allowed when clinically indicated . FEV <sub>1 </sub> denotes forced expiratory volume in 1 second, and LABA long-acting β <sub>2 </sub> \\-agonist.\n\n【72】 The results were obtained from the visit in which the prebronchodilator FEV <sub>1 </sub> was assessed for eligibility.\n\n【73】 The results were obtained from the visit in which reversibility in FEV <sub>1 </sub> was assessed for eligibility. The reversibility in FEV <sub>1 </sub> was calculated as the postbronchodilator FEV <sub>1 </sub> (in liters) minus the prebronchodilator FEV <sub>1 </sub> (in liters) divided by the prebronchodilator FEV <sub>1 </sub> (in liters).\n\n【74】Table 2. Intention-to-Treat and Preplanned On-Treatment Efficacy Analyses of the Secondary End Points. \n\n| Analysis | Intention-to-Treat Analysis | Intention-to-Treat Analysis | Intention-to-Treat Analysis | Intention-to-Treat Analysis | Preplanned On-Treatment Efficacy Analysis | Preplanned On-Treatment Efficacy Analysis | Preplanned On-Treatment Efficacy Analysis | Preplanned On-Treatment Efficacy Analysis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | Adults and Adolescents  | Adults and Adolescents  | Adults, Adolescents, and Children  | Adults, Adolescents, and Children  | Adults and Adolescents  | Adults and Adolescents  | Adults, Adolescents, and Children  | Adults, Adolescents, and Children  |\n|  | Albuterol(180 μg)–Budesonide(160 μg) | Albuterol(180 μg) | Albuterol(180 μg)–Budesonide(80 μg) | Albuterol(180 μg)Alone | Albuterol(180 μg)–Budesonide(160 μg) | Albuterol(180 μg) | Albuterol(180 μg)–Budesonide(80 μg) | Albuterol(180 μg) |\n| Annualized rate of severe asthma exacerbation |  |  |  |  |  |  |  |  |\n| Patients — no. | 1013 | 1014 | 1054 | 1056 | 1013 | 1014 | 1054 | 1056 |\n| Severe exacerbations — no. | 345 | 427 | 372 | 441 | 334 | 413 | 354 | 426 |\n| Annualized rate (95% CI) | 0.43 (0.33–0.58) | 0.58 (0.44–0.77) | 0.48 (0.37–0.63) | 0.60 (0.46–0.79) | 0.45 (0.34–0.60) | 0.59 (0.44–0.78) | 0.49 (0.37–0.64) | 0.61 (0.46–0.80) |\n| Rate ratio (95% CI) | 0.75 (0.61–0.91) | Reference | 0.81 (0.66–0.98) | Reference | 0.76 (0.62–0.93) | Reference | 0.80 (0.66–0.98) | Reference |\n| Annualized total dose of systemic glucocorticoid  |  |  |  |  |  |  |  |  |\n| Patients — no. | 1012 | 1011 | 1052 | 1052 | 1012 | 1011 | 1052 | 1052 |\n| Median value (5th–95th percentile) — mg/yr | 0.0 (0.0–459.2) | 0.0 (0.0–484.3) | 0.0 (0.0–494.4) | 0.0 (0.0–600.8) | 0.0 (0.0–496.1) | 0.0 (0.0–622.1) | 0.0 (0.0–487.0) | 0.0 (0.0–615.9) |\n| Mean value — mg/yr | 83.6±247.7 | 130.0±630.3 | 94.7±318.2 | 127.6±619.8 | 86.2±262.9 | 129.3±657.2 | 95.5±335.4 | 127.1±646.2 |\n| Response analysis at wk 24  |  |  |  |  |  |  |  |  |\n| ACQ-5 |  |  |  |  |  |  |  |  |\n| Patients — no. | 1013 | 1014 | 1052 | 1055 | 1013 | 1014 | 1052 | 1055 |\n| Patients with response — no. (%) | 682 (67.3) | 636 (62.7) | 690 (65.6) | 656 (62.2) | 677 (66.8) | 630 (62.1) | 681 (64.7) | 650 (61.6) |\n| Odds ratio (95% CI) | 1.22 (1.01–1.46) | Reference | 1.15 (0.96–1.37) | Reference | 1.22 (1.02–1.47) | Reference | 1.13 (0.95–1.35) | Reference |\n| AQLQ+12 |  |  |  |  |  |  |  |  |\n| Patients — no. | 994 | 993 | 987 | NA | 994 | 993 | 987 | NA |\n| Patients with response — no. (%) | 515 (51.8) | 464 (46.7) | 496 (50.3) | NA | 508 (51.1) | 461 (46.4) | 489 (49.5) | NA |\n| Odds ratio (95% CI) | 1.25 (1.04–1.50) | Reference | 1.13 (0.94–1.36) | NA | 1.23 (1.02–1.48) | Reference | 1.11 (0.93–1.34) | NA |\n\n【76】 Plus–minus values are means ±SD. The hierarchical testing procedure was stopped after the comparison between the lower-dose combination group and the albuterol-alone group in the intention-to-treat analysis of the primary end point; therefore, 95% confidence intervals cannot be used to infer treatment effects. A total of 3123 patients were assessed with respect to the efficacy end points (5 patients who had not received any trial medication and 4 who had been withdrawn because of randomization at more than one site were excluded). The comparisons between the higher-dose combination group and the albuterol-alone group excluded children 4 to 11 years of age; therefore, the number of patients in the albuterol-alone group in these comparisons was less than that in the comparisons between the lower-dose combination group and the albuterol-alone group. NA denotes not applicable.\n\n【77】 Analyses included patients 12 years of age or older.\n\n【78】 Analyses included patients 4 years of age or older, except for the Asthma Control Questionnaire–5 (ACQ-5) response analysis, in which patients 4 and 5 years of age were excluded, and the Asthma Quality of Life Questionnaire (AQLQ+12) response analysis, in which patients 4 to 11 years of age were excluded.\n\n【79】 The annualized total dose of systemic glucocorticoid is given in prednisone equivalents.\n\n【80】 Intention-to-treat analyses for the ACQ-5 response and the AQLQ+12 response are post hoc. Scores on the ACQ-5 range from 0 to 6, with lower scores indicating better asthma control (minimum clinically important difference, 0.5 points). Scores on the AQLQ+12 range from 1 to 7, with higher scores indicating better asthma-related quality of life (minimum clinically important difference, 0.5 points). A response was defined as a decrease (in the ACQ-5 score) or an increase (in the AQLQ+12 score) of at least 0.5 points from the baseline score.\n\n【81】Table 3. Adverse Events Occurring in at Least 2% of Patients in Any Trial Group. \n\n| Event | Albuterol (180 μg)–Budesonide (160 μg)(N=1015) | Albuterol (180 μg)–Budesonide (80 μg)(N=1055) | Albuterol(180 μg)(N=1057) |\n| --- | --- | --- | --- |\n|  | number of patients (percent) | number of patients (percent) | number of patients (percent) |\n| Any adverse event | 469 (46.2) | 497 (47.1) | 490 (46.4) |\n| Nasopharyngitis | 76 (7.5) | 61 (5.8) | 54 (5.1) |\n| Headache | 44 (4.3) | 50 (4.7) | 50 (4.7) |\n| Covid-19 | 43 (4.2) | 52 (4.9) | 46 (4.4) |\n| Upper respiratory tract infection | 26 (2.6) | 31 (2.9) | 26 (2.5) |\n| Bronchitis | 25 (2.5) | 27 (2.6) | 28 (2.6) |\n| Hypertension | 22 (2.2) | 27 (2.6) | 26 (2.5) |\n| Asthma | 18 (1.8) | 20 (1.9) | 35 (3.3) |\n| Back pain | 27 (2.7) | 23 (2.2) | 20 (1.9) |\n| Influenza | 21 (2.1) | 23 (2.2) | 14 (1.3) |\n| Sinusitis | 15 (1.5) | 17 (1.6) | 24 (2.3) |\n\n【83】 Adverse events are sorted in decreasing total frequency of preferred term in the _Medical Dictionary for Regulatory Activities_ , version 24.0. Patients with multiple events in the same category are counted only once in that category.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4bbc7cf9-186a-4161-9c3c-a5715c096cd3", "title": "A Novel Influenza A (H1N1) Vaccine in Various Age Groups", "text": "【0】A Novel Influenza A (H1N1) Vaccine in Various Age Groups\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】There is an urgent need for a vaccine that is effective against the 2009 pandemic influenza A (H1N1) virus.\n\n【3】Methods\n-------\n\n【4】A split-virus, inactivated candidate vaccine against the 2009 H1N1 virus was manufactured, and we evaluated its safety and immunogenicity in a randomized clinical trial. Subjects were between 3 and 77 years of age, stratified into four age groups. The immunization schedule consisted of two vaccinations, 21 days apart. Subjects were injected with placebo or with vaccine, with or without alum adjuvant, at doses of 7.5 μg, 15 μg, or 30 μg. Serologic analysis was performed at baseline and on days 21 and 35.\n\n【5】Results\n-------\n\n【6】A total of 2200 subjects received one dose, and 2103 (95.6%) received the second dose, of vaccine or placebo. No severe adverse side effects associated with the vaccine were noted. In the nonadjuvanted-vaccine groups, injection-site or systemic reactions, most mild in nature, were noted in 5.5 to 15.9% of subjects. Among the subjects receiving 15 μg of nonadjuvanted vaccine, a hemagglutination-inhibition titer of  or more was achieved by day 21 in 74.5% of subjects between 3 and 11 years of age, 97.1% of subjects between 12 and 17 years, 97.1% of subjects between 18 and 60 years, and 79.1% of subjects 61 years of age or older; by day 35, the titer had been achieved in 98.1%, 100%, 97.1%, and 93.3% of subjects, respectively. The proportion with a titer of  or more was generally highest among the subjects receiving 30 μg of vaccine, with or without adjuvant. Vaccine without adjuvant was associated with fewer local reactions and greater immune responses than was vaccine with adjuvant.\n\n【7】Conclusions\n-----------\n\n【8】These data suggest that a single dose of 15 μg of hemagglutinin antigen without alum adjuvant induces a typically protective immune response in the majority of subjects between 12 and 60 years of age. Lesser immune responses were seen after a single dose of vaccine in younger and older subjects. \n\n【9】Introduction\n------------\n\n【10】Recently, a novel swine-origin influenza A (H1N1) virus was identified as the cause of large numbers of febrile respiratory illnesses in Mexico and the United States.  It rapidly spread to many countries around the world, prompting the World Health Organization to declare a pandemic on June 11, 2009.  An important method of controlling this pandemic will be large-scale immunization. Currently used trivalent seasonal-influenza vaccines are unlikely to provide protection against the new 2009 pandemic influenza A (H1N1) virus  ; hence, there is an urgent need to develop a vaccine against it. On July 29, 2009, the Advisory Committee on Immunization Practices tentatively concluded that children under 9 years of age may need to be given two doses of vaccine to elicit sufficient immunogenic reactivity against the 2009 H1N1 virus. \n\n【11】In response to the pandemic, a novel vaccine against the virus strain A/California/07/2009 (H1N1) has been developed and recently was approved for sale in China. This report details the findings of a randomized, placebo-controlled, double-blind clinical trial of the safety and immunogenicity profile of this influenza A (H1N1) 2009 monovalent vaccine. In addition, we evaluated the role of alum adjuvant in the vaccine formulation, the optimal amount of antigen, and the need for second doses in children or elderly people.\n\n【12】Methods\n-------\n\n【13】Study Design and Objective\n--------------------------\n\n【14】From July 2009 through August 2009, we enrolled a total of 2200 subjects between the ages of 3 years and 77 years in a stratified, randomized, double-blind, placebo-controlled, clinical trial in Taizhou, Jiangsu Province, China. The study was sponsored by Hualan Biological Bacterin Company. The study was conducted and the data were gathered by the nonindustry investigators, and the data analysis was conducted by Southeast University; all the authors drafted the manuscript and made the decision to submit it for publication. The sponsor did not influence these activities. All the authors had full access to the data, which were held by Southeast University, and all vouch for the accuracy and completeness of the data and the analysis.\n\n【15】Approval for the study protocol was obtained from the ethics committee of the Jiangsu Provincial Center for Disease Control and Prevention, and the study was conducted in accordance with the principles of the Declaration of Helsinki, the standards of Good Clinical Practice (as defined by the International Conference on Harmonization), and Chinese regulatory requirements, as stipulated by the Chinese Food and Drug Administration. Written informed consent was obtained from each subject or his or her legal representative.\n\n【16】The study subjects received either vaccine or placebo. The vaccine was administered at various doses, with or without alum adjuvant. The primary immunologic end points were the proportions of subjects with an increase in the hemagglutination-inhibition titer by a factor of 4 or more on day 21 after the first dose and on day 35 (14 days after the second dose). The second dose was administered on day 21, after data for the primary end point had been collected. The primary safety end points were the presence of any systemic reaction or injection-site reaction 21 days after the first dose and 14 days after the second dose.\n\n【17】Vaccine\n-------\n\n【18】The influenza A (H1N1) 2009 monovalent, split-virus vaccine was developed by Hualan Biological Bacterin Company, and the seed virus was prepared from reassortant vaccine virus A/California/7/2009 NYMC X-179A (New York Medical College, New York), distributed by the Centers for Disease Control and Prevention in the United States. This strain was recommended by the World Health Organization and obtained from the Chinese Food and Drug Administration.  The vaccine was prepared in embryonated chicken eggs according to the same standard techniques that are used for the production of trivalent inactivated vaccine against seasonal influenza. In brief, the virus was harvested from the egg cultures and inactivated with the use of formaldehyde. The virus was concentrated, purified, and further sterilized by means of chromatography (on a Sepharose 4FF column) with Triton X-100.\n\n【19】The five experimental vaccines produced were split-virus products of 30 μg of hemagglutinin antigen with or without adjuvant, 15 μg of hemagglutinin antigen with or without adjuvant, and 7.5 μg of hemagglutinin antigen with adjuvant. The adjuvant formulation contained 1.2 mg of alum per milliliter of solution. The placebo consisted of phosphate-buffered saline.\n\n【20】Study Subjects\n--------------\n\n【21】Subjects were eligible to participate in the study if they were healthy, were 3 years of age or older, did not have a history of infection with the 2009 H1N1 virus, and had not received an influenza A (H1N1) 2009 monovalent vaccine, and if they or their guardians confirmed that they understood the study procedures, provided written informed consent, and agreed to keep a daily record of symptoms. Adverse events were coded according to the requirements of the Chinese Food and Drug Administration. These requirements are based on documents published by the Division of Microbiology and Infectious Diseases of the National Institutes of Allergy and Infectious Diseases that provide a grading system to classify adverse effects in adult and pediatric patients.  Women between 16 and 50 years of age were also required to have a negative pregnancy test at the time of screening and before each vaccination.\n\n【22】The study subjects were classified into four age categories: 61 years of age or older, 18 to 60 years of age, 12 to 17 years of age, and 3 to 11 years of age. The general rationale for the age divisions was as follows: young children, defined as those 3 to 11 years of age, attend kindergartens or primary schools; adolescents, defined as those 12 to 17 years of age, attend junior high school or high school; adults, defined as those 18 to 60 years, work outside the home; and the elderly, defined as those 61 years or older, are retired and often live with family. Hence, the four age groups have different socialization patterns and thus different risks of exposure to the virus.\n\n【23】Figure 1. Enrollment and Follow-up of the Study Subjects.\n\n【24】 The reasons why 97 of the 2200 subjects who received the first vaccine dose did not receive the second dose are listed in Table 2 in the Supplementary Appendix .\n\n【25】Each treatment group had 110 subjects . Subjects in the elderly and adolescent cohorts were each randomly assigned to receive one of five doses of vaccine: adjuvant vaccine with 7.5, 15, or 30 μg of hemagglutinin antigen per dose or nonadjuvant vaccine with 15 or 30 μg of hemagglutinin antigen per dose. The adult subjects were randomly assigned to receive placebo or one of five doses of vaccine: adjuvant vaccine with 7.5, 15, or 30 μg of hemagglutinin antigen per dose or nonadjuvant vaccine with 15 or 30 μg of hemagglutinin antigen per dose. The subjects who were 3 to 11 years of age were randomly assigned to receive one of four doses of vaccine: adjuvant vaccine with 7.5 or 15 μg of hemagglutinin antigen per dose or nonadjuvant vaccine with 15 or 30 μg of hemagglutinin antigen per dose.\n\n【26】With 110 subjects per treatment group, the study had a statistical power of at least 80% to detect a seroconversion rate of more than 40% in each group. The randomization schedule was created with the use of SAS software . The block size was defined by an independent statistician at Southeast University (Nanjing, China). Randomization-code numbers were assigned to subjects in chronological order by the investigator.\n\n【27】Subjects and investigators were unaware of the formulation of the study vaccine that was administered. Injections were given intramuscularly, in the deltoid muscle. After an on-site safety observation of 30 minutes' duration, subjects or their guardians were asked to record underarm body temperature and data on injection-site reactions and systemic reactions at 6, 24, 48, and 72 hours and at 7, 14, and 21 days, in a diary provided by the investigators. Serum samples were collected three times: on day 0 (before the first vaccination), day 21 (before the second vaccination), and day 35 (14 days after the second vaccination).\n\n【28】Assays\n------\n\n【29】The titer of antibodies against the vaccine strain was measured in all samples by means of hemagglutination-inhibition assays, which were performed in accordance with established procedures and with the use of turkey erythrocytes.  In brief, samples were treated with cholera filtrate at 36°C for 16 hours and were then tested at dilutions of  and . Titers of anti–hemagglutinin antigen antibodies that were below the detection limit (i.e., < were assigned a value of , and titers above  were assigned a value of . All samples were assayed in a blinded manner, in duplicate, and were double-checked by two investigators at the Chinese National Institute for the Control of Pharmaceuticals and Biological Products.\n\n【30】The results of immunogenicity assays performed 21 days after the first vaccination and 14 days after the second vaccination were compared with the results obtained at baseline. In addition, the numbers of subjects who underwent seroconversion were noted and compared with the baseline numbers. Seroconversion was defined as an increase in the hemagglutination-inhibition titer of at least four times the baseline titer, according to international guidelines used to evaluate influenza vaccines.  No microneutralization tests were performed.\n\n【31】Statistical Analysis\n--------------------\n\n【32】The likelihood-ratio chi-square test or Fisher's exact test was used to compare the number of subjects in each treatment group who had a local (injection-site) or systemic reaction within 21 days after vaccination.\n\n【33】The primary immunologic end points were the proportions of subjects with an increase in the hemagglutination-inhibition titer by a factor of 4 or more after the first and second vaccine doses. Specifically, seroconversion was considered to have occurred when the hemagglutination-inhibition titer was less than  before vaccination and was  or more after vaccination or the titer was  or more before vaccination and increased to at least four times the baseline level after vaccination. Other end points regarding hemagglutination inhibition included the geometric mean titer, the geometric mean increase (i.e., the ratio of the titer after vaccination to the titer before vaccination), and the proportion of subjects with a titer of  or more. Modeling of the geometric mean titers and geometric mean increases was conducted with the use of generalized linear models, which included the effects of adjuvant status, dose, and age. Logistic regression was used to model the rate of adverse reactions, the proportion of subjects with an increase in the hemagglutination-inhibition titer by a factor of 4 or more, and the proportion with a titer of  or more, according to adjuvant status, dose, and age. Safety data were summarized descriptively.\n\n【34】Hypothesis testing was conducted with the use of two-sided tests, with an alpha value of 0.05 considered to indicate statistical significance. All statistical analyses were performed by means of SAS software .\n\n【35】Results\n-------\n\n【36】Study Subjects\n--------------\n\n【37】A total of 2200 subjects between 3 and 77 years of age received the first dose of vaccine, and 2103 (95.6%) received the second dose, 21 days later . Data from all vaccinated subjects were included in the safety analysis. Immunogenicity data on the effects of the first and second vaccinations were available for 2104 and 2069 subjects, respectively.\n\n【38】Safety\n------\n\n【39】Table 1. Injection-Site and Systemic Reactions within 21 Days after the First or Second Dose of Nonadjuvanted Vaccine or Placebo, According to Age Group.\n\n【40】Injection-site and systemic reactions are shown in Table 1 . Most reactions were mild; grade 3 reactions — corresponding to an inability to perform usual social and functional activities  — occurred in 0.0 to 1.8% of the study subjects . Only one severe adverse event was reported, in a subject who received placebo: hospitalization for atrial fibrillation on the day after the first injection. Thus, no serious, vaccine-related adverse events or unexpected events were recorded. Analysis of the factors associated with the rate of local or systemic reactions suggested that the presence of adjuvant and receipt of the second dose were associated with a higher rate of local reactions (P<0.001 and P=0.002, respectively) . For systemic reactions, age was the only factor with significant effects on the rate: subjects 12 to 17 years of age had systemic reactions most frequently, followed by subjects 3 to 11 years of age, those 18 to 60 years of age, and those 61 years of age or older . The most common injection-site reaction after vaccination was pain, which occurred in 10.5 to 26.7% of subjects; the most frequently reported systemic reaction was fever, which occurred in 11.5 to 18.0% of subjects . The prevalence of an adverse event was similar among subjects with detectable hemagglutination-inhibition antibodies against the vaccine strain and in those without detectable antibodies.\n\n【41】Immune Response\n---------------\n\n【42】Table 2. Hemagglutination-Inhibition Titer of  or More among Subjects Receiving Nonadjuvanted Vaccine, According to Age Group.\n\n【43】The proportion with a hemagglutination-inhibition titer of  or more was 1.1 to 6.0% at baseline in the four age groups . Hemagglutination-inhibition antibodies against the vaccine strain were detected on day 21 after the first dose and also 14 days after the second dose (day 35). The proportion with a titer of  or more and the proportion with an increase in the hemagglutination-inhibition titer by a factor of 4 or more, according to treatment group, are listed in Table 10 in the Supplementary Appendix . Logistic-regression modeling showed that dose, adjuvant status, and age significantly affected the proportion of subjects with a titer of  or more and the proportion with an increase in the hemagglutination-inhibition titer by a factor of 4 or more after the first dose (P<0.001 for all comparisons) . Vaccine formulations without adjuvant were more immunogenic than formulations with adjuvant. There was also a dose-dependent antibody response; the dose of 30 μg was the most immunogenic, with the proportion of subjects with a titer of  or more highest among subjects 12 to 17 years of age, followed by subjects 18 to 60 years of age, subjects 3 to 11 years of age, and subjects 61 years of age or older. The proportion of subjects with a titer of  or more among those receiving nonadjuvant vaccine was 74.5 to 97.1% after the first dose and increased to 93.3 to 100% after the second dose .\n\n【44】Table 3. Geometric Mean Titer of Hemagglutination-Inhibition Antibodies among Subjects Receiving Nonadjuvanted Vaccine, According to Age Group.\n\n【45】The results were similar for the geometric mean titer and the geometric mean increase from baseline: the highest values in the adolescent and adult cohorts were seen with formulations without adjuvant and at higher doses . Among the children, both the geometric mean titer and geometric mean increase rose significantly after the second vaccination. In contrast, in the other age groups, the antibody levels did not rise significantly after the second dose among the subjects who received the nonadjuvanted vaccine. After the first and second vaccinations, antibody levels did not differ significantly between the groups receiving the 15-μg dose of nonadjuvanted vaccine and the groups receiving the 30-μg dose of nonadjuvanted vaccine .\n\n【46】Figure 2. Cumulative Distribution Curves for Titers of Hemagglutination-Inhibition Antibodies on Day 21 (after the First Dose of Vaccine) and Day 35 (after the Second Dose), According to Treatment Group and Age Group.\n\n【47】The percentages of subjects with hemagglutination-inhibition titers are shown for children , adolescents , adults , and elderly subjects . The limit of detection was a titer of . Titers are expressed as the reciprocal of the dilution.\n\n【48】The antibody titers after the first and second doses of vaccine support the findings of higher immunogenicity of the vaccine formulations without adjuvant and of the higher doses, among adolescents and adults .\n\n【49】Discussion\n----------\n\n【50】Recently, Greenberg et al.  reported that a single 15-μg dose of split-virus 2009 H1N1 vaccine was immunogenic in healthy adults 18 to 64 years of age, with mild-to-moderate vaccine-associated reactions. However, because children and the elderly were not included in the study, it not known whether a second dose of 2009 H1N1 vaccine would be necessary to induce sufficient immunity in these populations.\n\n【51】The results of our study show that 2009 H1N1 vaccine is associated with an acceptable safety profile for adults as well as children and elderly people. The level of immunity induced by the first dose of the vaccine appears to be influenced by the presence or absence of alum adjuvant and the age of the recipients . The vaccines formulated without alum adjuvant were more effective in inducing an immune reaction in subjects than were vaccines with adjuvant. This lack of enhancement by the use of alum adjuvant was consistent with data from previous studies of other influenza vaccines.  There were no significant differences in the immunogenicity of the 15-μg and 30-μg doses of nonadjuvant vaccine , in line with the results reported by Greenberg et al.  The immune response to the vaccine varied among the age groups. As in studies of seasonal influenza vaccine,  age was an important factor associated with the level of induced immunity in our study.\n\n【52】The immune responses in children and the elderly, but not adults or adolescents, can be substantially boosted by a second dose of vaccine . This finding is consonant with the results of studies evaluating the effectiveness of seasonal influenza vaccination, which showed that one dose of vaccine was highly immunogenic in healthy adults under 65 years of age and that a second dose did not substantially enhance the antibody response. \n\n【53】In our study, depending on the age of the subjects, the administration of a single 15-μg dose of vaccine is associated with a proportion of subjects with a titer of  or more of 74.5% (95% confidence interval \\[CI\\], 65.1 to 82.5) to 97.1% (95% CI, 91.9 to 99.4) and a geometric mean titer of 64.1 (95% CI, 51.1 to 80.3) to 430.7 (95% CI, 330.0 to 562.1). These results are consistent with the statutory or regulatory requirements of most governments for the use of vaccines. \n\n【54】Although one dose of 15 μg of vaccine without adjuvant protects the majority of persons, another dose given 21 days later will increase the antibody response in children. The decision about whether to administer two doses of vaccine in children will need to be made by public health officials. One argument for a two-dose vaccine schedule in children is the unusual epidemiology of the 2009 H1N1 pandemic: it affects younger age groups, including young children, and significant morbidity and mortality appear to occur in these younger age groups.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f643550f-fd14-4517-b6d6-bc99e654861f", "title": "Postherpetic Neuralgia", "text": "【0】Postherpetic Neuralgia\nPostherpetic neuralgia is more common with older age. Recommended treatments include topical agents (lidocaine or capsaicin) and systemic agents (in particular, gabapentin, pregabalin, or tricyclic antidepressants), but their efficacy tends to be suboptimal.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "27835ff2-0e95-469f-86f7-d90c8cccb659", "title": "Xylazine — Medical and Public Health Imperatives", "text": "【0】Xylazine — Medical and Public Health Imperatives\nArticle\n-------\n\n【1】Increasing use of xylazine, most often in combination with other drugs such as fentanyl, is a rapidly growing threat to human health in the United States. Xylazine is an α <sub>2 </sub> \\-agonist in the same drug class as clonidine, lofexidine, and dexmedetomidine. It was initially studied for use in humans as an antihypertensive agent, but development for human use was discontinued because of adverse effects. Xylazine was approved by the Food and Drug Administration for use as a sedative in veterinary medicine in 1972 but isn’t approved for use in humans.\n\n【2】Centrally acting α <sub>2 </sub> \\-agonist medications inhibit the release of norepinephrine and epinephrine. The effects on the central nervous system include sedation, analgesia, and euphoria. Reduced sympathetic outflow from the central nervous system causes decreased peripheral vascular resistance, heart rate, and blood pressure. Some α <sub>2 </sub> \\-agonists are approved for use in humans as antihypertensive agents, for sedation, and for mitigation of opioid-withdrawal symptoms to facilitate abrupt opioid discontinuation. This class of drugs is not regulated under the federal Controlled Substances Act and historically has been thought to be associated with a low risk of illicit use. People who use xylazine, however, may develop physiological dependence, have symptoms consistent with a xylazine-related substance use disorder, and have severe withdrawal symptoms (e.g., irritability, anxiety, and dysphoria) after abrupt discontinuation. Xylazine-withdrawal symptoms are not alleviated by the administration of opioids. The severity of such symptoms, combined with uncertainty about effective treatment options, may compel people to continue to use xylazine, since discontinuation without assistance often isn’t feasible.\n\n【3】Xylazine appears to have entered the illicit drug supply in the northeastern United States as an additive to fentanyl. It can be consumed orally or by smoking, snorting, or intramuscular, subcutaneous, or intravenous injection. The drug’s reported duration of effect is longer than that of fentanyl; adulteration of fentanyl with xylazine therefore probably enhances the euphoria and analgesia induced by fentanyl and reduces the frequency of injections.\n\n【4】 Estimated Xylazine-Involved Drug-Poisoning Deaths in the United States, 2018–2021.\n\n【5】Data are from the Centers for Disease Control and Prevention WONDER data set and include drug-poisoning cases with codes T42.7 or T46.5 in the _International Classification of Diseases_ , 10th revision.\n\n【6】The first illicit use of xylazine was reported in Puerto Rico around 2001. Xylazine was initially used in combination with a polydrug mixture, commonly referred to as a speedball, containing a stimulant (e.g., cocaine or amphetamine) and an opioid (e.g., heroin, morphine, or fentanyl). Xylazine was identified intermittently in drug samples collected in the continental United States between 2006 and 2018. Philadelphia and Connecticut appear to have been the epicenters of xylazine use in the continental United States, but use is rapidly spreading throughout the country.  The drug is known as “anesthesia de caballo” in Puerto Rico and “tranq” in Philadelphia (or “tranq dope” when combined with opioids, most often fentanyl). Xylazine was found in more than 90% of illicit drug samples tested in Philadelphia in 2021  and was identified in forensic toxicology samples from 36 of 49 states that were tested in June 2021.  As of March 2023, fentanyl mixed with xylazine had been found in drug seizures in 48 states. According to data from the Centers for Disease Control and Prevention, the estimated number of drug-poisoning deaths in the United States involving xylazine grew from 260 in 2018 to 3480 in 2021, an increase of 1238% , with the highest numbers of such deaths during that period reported in Pennsylvania, Maryland, New York, and Connecticut.\n\n【7】Patients presenting with xylazine intoxication may have central nervous system depression, hypotension, and bradycardia. Clinicians may not recognize the contribution of xylazine to a person’s symptoms, since many aren’t aware of growing use and rapid point-of-care testing for xylazine isn’t widely available. Respiratory depression has been reported in people using xylazine, probably because the drug increases the risk of opioid-induced respiratory depression. Naloxone can reverse opioid-induced respiratory depression but doesn’t reverse the effects of xylazine. Although naloxone administration remains a vital treatment for any overdose that may involve opioids, additional supportive care may therefore be necessary in the treatment of xylazine overdose. Such care may include maintaining a patent airway, administering supplemental oxygen, performing rescue breathing when indicated, and treating hypotension as needed. There is no xylazine-reversal agent currently approved for use in humans.\n\n【8】Limited data are available to guide clinical decision making related to the treatment of xylazine withdrawal in inpatient settings. Some institutions are exploring the use of dexmedetomidine infusions for xylazine-withdrawal symptoms in the intensive care unit,  whereas others are exploring the use of clonidine and lofexidine in inpatient units. As compared with clonidine, lofexidine may be associated with a lower incidence of adverse effects, including hypotension, when used to treat symptoms of withdrawal. Although tapering these medications over 5 to 7 days may be appropriate, there are limited data to guide clinical decision making regarding treatment duration.\n\n【9】 Xylazine-Associated Skin Injury.\n\n【10】Photograph courtesy of Sarah Laurel, the founder of Savage Sisters, a Philadelphia-based organization that provides support services, including wound care, to people with active substance use disorder.\n\n【11】In addition to its acute effects, xylazine is associated with severe necrotic skin ulcerations .  Patients who use xylazine may present to the emergency department seeking care for these wounds. Such wounds are different from the wounds commonly seen in people who inject drugs; tissue injury may occur at or remote from an injection site and irrespective of the mode of use. The pathophysiology of tissue injury is unclear and probably multifactorial. Patients may seek care late in the progression of such injuries because of fear that clinicians will be unable to effectively diagnose and treat the symptoms associated with xylazine withdrawal. Once engaged in care, patients may also leave care prematurely because of severe withdrawal symptoms or the fear of such symptoms. Such challenges may ultimately result in further progression of tissue injury.\n\n【12】Concurrent use of xylazine and fentanyl and other opioids can complicate addiction treatment. Treatment programs may not include xylazine testing in their testing protocols and therefore may not identify concurrent xylazine use. In addition, programs may not be prepared to treat xylazine withdrawal when it occurs. The presence of xylazine-associated wounds requiring care that is not commonly available in outpatient or residential addiction treatment programs may make access to addiction treatment difficult. As has been observed in patients taking multiple substances, patients taking opioids and xylazine may require more intensive treatment than those taking opioids alone to increase the likelihood of a successful outcome.\n\n【13】Xylazine poses a threat to public health, and the people being harmed by this drug deserve rapid, comprehensive, and high-quality health care. The White House Office of National Drug Control Policy (where we work) declared xylazine, particularly the use of fentanyl adulterated or associated with xylazine (FAAX), an emerging threat on April 12, 2023. This first-of-its-kind declaration triggered the development of an emerging threat response plan, which will be published within 90 days after the emerging threat designation, as required by the SUPPORT (Substance Use-Disorder Prevention that Promotes Opioid Recovery and Treatment for Patients and Communities) Act, and will include content focused on testing, treatment, harm reduction, comprehensive data collection and analysis, source identification and supply reduction, possible regulatory actions, and rapid conduct of basic and applied research. We expect that this designation will trigger a vigorous national response with targeted actions that will save lives.\n\n【14】Addressing use of FAAX will present critical challenges. Collection and dissemination of comprehensive data on xylazine use, including on the harm caused by such use and potential disparities in outcomes based on race, ethnicity, and socioeconomic status, are needed. Similarly, basic science and clinical research are necessary to better understand the effects of xylazine in humans, including the pharmacokinetics of xylazine and the mechanisms of injury associated with its use. There is an urgent need for more robust evidence on treatment options for acute xylazine intoxication, management of withdrawal symptoms, wound care, and long-term management of xylazine-related substance use disorder. It will also be important to support rapid development and distribution of reliable point-of-care tests for both biologic specimens and drug products, as well as widespread implementation of hospital- and community-based xylazine testing. Education for the public and clinicians will be needed as additional data are collected on the consequences of xylazine use and treatment options. Finally, as more is learned about the sources and supply of xylazine used by humans, effective strategies for disrupting and reducing this supply will be necessary.\n\n【15】FAAX is associated with increasing harm to people living in the United States. Our goal is for the designation of xylazine as an emerging threat and subsequent actions to begin to address this threat before it worsens and undermines efforts to reduce illicit fentanyl use in the United States.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "147b4a7c-54d2-495b-bbe4-d76c1364ea8d", "title": "Current Concepts: Disclosing Harmful Medical Errors to Patients", "text": "【0】Current Concepts: Disclosing Harmful Medical Errors to Patients\nHistorically, fear of malpractice litigation made clinicians cautious about informing patients when they made mistakes in their care. This article reviews recent efforts by regulators, hospitals, accreditation organizations, and legislators to encourage and facilitate discussions between health care providers and patients when patients are harmed by medical errors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "373f0877-2dcf-45d2-bf8f-29c12934f44e", "title": "Case 24-2007 — A 20-Year-Old Pregnant Woman with Altered Mental Status", "text": "【0】Case 24-2007 — A 20-Year-Old Pregnant Woman with Altered Mental Status\nA 20-year-old pregnant woman was admitted at 26 weeks' gestation because of dizziness, confusion, and difficulty walking. Six weeks earlier, she was confused and had odd head movements. Four days before admission, she had dizziness and weakness; she began to fall to her left and vomited. On admission, examination of the cerebrospinal fluid showed a lymphocytic pleocytosis with mildly elevated protein and normal glucose levels. During the next 18 days, her condition worsened.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "01f07ccb-7d85-4f11-852a-f57543cf76c1", "title": "Double Environmental Injustice — Climate Change, Hurricane Dorian, and the Bahamas", "text": "【0】Double Environmental Injustice — Climate Change, Hurricane Dorian, and the Bahamas\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with Drs. James Shultz and Krista Nottage on climate-change–driven storms and their disproportionate effects on marginalized populations. \n\n【3】Climate change has been linked to changes in Atlantic hurricane behavior, making storms more destructive to the built environment and vital infrastructure, more harmful to the physical and mental health of island-based and coastal populations, and more deadly in their aftermath.  These escalating effects on population health represent a double environmental injustice: socioeconomically disadvantaged and marginalized populations sustain disproportionate harm and loss, with more hazardous storms exacerbating the inequity; and while the populations most vulnerable to Atlantic hurricanes, especially those in small-island states, contribute virtually nothing to climate change,  they are among those most exposed to risks that are worsened by the carbon emissions from higher-income countries. \n\n【4】Anthropogenic climate change amplifies storm hazards through such intermediaries as anomalously warm ocean and air temperatures, rising ocean heat content, and increasing atmospheric moisture capacity, compounded by a rise in sea levels. Atlantic hurricanes have become stronger, wetter, and slower-moving over the past few decades, with likely contributions from human actions. \n\n【5】Widespread Destruction in Marsh Harbour, Abaco Islands, Bahamas, after Hurricane Dorian.\n\n【6】Hurricane Dorian’s catastrophic trajectory across the northwest Bahamas dramatically highlighted these trends. Dorian intensified at an unprecedented rate just before reaching the Bahamas and slammed into Great Abaco Island with sustained winds of 185 mph, tying the record for the greatest Atlantic-hurricane wind speed at landfall. It produced deluges of rain and dragged a two-story-high storm surge onshore, submerging entire neighborhoods. And its forward motion stalled as it moved onto Grand Bahama Island, where residents were battered by the strongest eye-wall winds for 40 hours, resulting in one of the most prolonged known population exposures to extreme hurricane hazards. Yet despite its distinguishing features, Hurricane Dorian fits into a discernible pattern of increasingly harmful Atlantic storms. Dorian was the fifth Category 5 Atlantic hurricane to make landfall over four consecutive seasons, preceded by Matthew (2016), Irma (2017), Maria (2017), and Michael (2018).\n\n【7】Destruction of Buildings and Trees in Central Abaco.\n\n【8】All hurricanes have public health effects. In addition to living through the storm hazards during impact, affected populations face health consequences arising from widespread power outages, water contamination, inoperative sanitation systems, and acute food insecurity in the aftermath. Storm-affected citizens grapple with the severe structural damage that leaves many homes uninhabitable and triggers population displacement. Survivors are often left to swelter in health-compromising heat and humidity and are vulnerable to vectorborne diseases. Health services are disrupted for many people with acute or chronic medical conditions.  Patients with special medical needs, including those who are immunosuppressed, recovering from surgery or undergoing rehabilitation, or dependent on electronic devices or life-sustaining therapies, may face real threats to survival. School closures, damage to worksites, unemployment, and financial losses challenge the resilience of storm-ravaged communities. \n\n【9】Mental health consequences are among the most salient, and they are a focus of the current recovery initiatives in the Bahamas. Survivors who were directly exposed to extreme storm hazards during Dorian’s impact are at elevated risk for developing post-traumatic stress disorder.  Those who sustained massive losses may develop major depression or anxiety disorders. \n\n【10】Climate-change–driven hurricanes tend to inflict two types of environmental injustice. One is that socioeconomically disadvantaged and racial or ethnic minority populations experience disproportionate harm, loss, and life changes. The increased severity of climate-change–related storms portends a worsening impact on marginalized people that can exacerbate preexisting health gaps and social inequities. As Dorian moved over the northwest Bahamas, for instance, the most severe destruction affected thousands dwelling in shantytowns on Great Abaco Island. Many of those affected were undocumented migrants. Similarly, after Hurricane Harvey, flooding around Houston was worst in socioeconomically deprived neighborhoods with higher proportions of black and Latino residents. And after Hurricane Maria, many rural, disadvantaged Puerto Rican municipalities struggled without electricity for as long as a year. The death toll rose steadily into the thousands, as frail, elderly, and chronically ill people died preventable deaths. Disparities in health, as measured by multiple indicators, were magnified. \n\n【11】At a more fundamental level of environmental injustice, the contribution of island-based populations to global carbon emissions is negligible.  Collectively, the Caribbean’s 44 million residents generate just 0.4% of total global emissions, and the Bahamas’ share is well below 0.01%. The countries designated by the United Nations as small-island states face climate-change–associated hurricane risks that are not of their own making. Rather, high-income countries that produce fossil fuels and expend massive amounts of energy generate most of the emissions that elevate hurricane risks for small-island populations.\n\n【12】Dorian’s devastation of the northwest Bahamas is the latest installment in a series of Atlantic storm impacts on the 29 United Nations–designated Caribbean small-island states. In 2016, Hurricane Matthew ransacked Haiti’s breadbasket, destroying staple crops. In 2017, Hurricane Irma brought Category 4 or 5 winds to eight small-island states and tropical storm winds to six more.  Hurricane Maria affected 16 island states, bringing major hurricane winds to 5 of them, including Dominica and Puerto Rico. All told, 22 of 29 Caribbean small-island states were affected by at least one 2017 storm. \n\n【13】These states are geographically vulnerable because they’re in warm Atlantic Ocean latitudes where hurricanes form and flourish. Most residents of these islands are unable to evacuate in order to avoid traumatizing exposure to hurricanes.  In the aftermath of a hurricane, recovery support must be physically transported to geographically isolated populations of storm survivors who may be spread across multiple islands. \n\n【14】The ferocity of cyclonic activity may have additive effects: the vulnerable are disproportionately affected, while the capacity of small-island states is decimated. Cars, trucks, and supplies intended to provide relief are destroyed. Relief workers, security officers, disaster coordinators, and health team members themselves become victims. Often the most climate-resilient structures end up being repurposed as shelters, which renders them unavailable to serve their usual functions. These multiplicative effects further magnify social injustice. \n\n【15】Ultimately, this whole dynamic contributes to widening health gaps. We believe that recognition of this reality compels action on several fronts.\n\n【16】First, communities need to prepare strategically for the onslaught of stronger storms by revising building codes and redesigning neighborhoods to redress existing socioeconomic inequities in affordable, safe housing. Also critical is making infrastructure more resilient, with a particular focus on electrical power systems. Cuba, for instance, completely redesigned its electrical system, creating a decentralized microgrid architecture that facilitated islandwide restoration of power less than 3 weeks after Irma’s impact in 2017.\n\n【17】Second, the health care system will have to be undergirded to withstand stronger hurricanes in order to maintain access to lifesaving services and operability of life-sustaining treatments.  Fortification of hospital facilities, auxiliary electrical power, and workforce protection for health care staff are priorities. Lessons learned from U.S. health care systems can be applied in island states. \n\n【18】Third, enhancing warning systems, upgrading the structural integrity of hurricane shelters, and ensuring citizen compliance with evacuation protocols are lifesaving actions. Special attention must be paid to making these resources equitably available to marginalized populations.\n\n【19】Fourth, citizen engagement in disaster preparedness and response is generally lacking, although Cuba is an exception. Personal and family disaster planning and participation in community emergency-response teams, with opportunities for active practice, has been a vision in the United States — and a well-rehearsed reality in Cuba. Family preparedness is especially crucial for households with members who have special medical needs. The building of citizen cohesion and social connectedness is one of the hallmarks of community resilience that pays dividends in disaster situations. \n\n【20】Fifth, health professionals can help increase collaborations among climate scientists, population health scientists, and clinicians to better plan the health response to hurricane-affected communities. In particular, health professionals have an important voice and can use it to advance the public conversation about climate change and resulting disasters. Both individually and collectively, health professionals can advocate for policies that constrain carbon emissions and seek alternative energy sources that diminish population health risks.\n\n【21】Sea levels will not recede, average global temperatures will not decline, and hurricane hazards will not moderate. We need to prepare now for future Dorian-like scenarios in a manner that redresses environmental injustice.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ef5df1b6-769a-4b45-adc4-7cc4e3b43907", "title": "Mechanisms of Disease: Alpha", "text": "【0】Mechanisms of Disease: Alpha\nAlpha <sub>1 </sub> \\-antitrypsin is a member of a family of protease inhibitors known as the serpins. Mutations in these molecules can lead to disease, not only because the biologic activity of the protease in tissue is increased, but also because the mutations result in misfolded (i.e., conformationally abnormal) protease molecules that accumulate in tissue. This review article summarizes the action of these protease inhibitors and how mutations lead to their accumulation in particular neurodegenerative disorders such as prion encephalopathies and Alzheimer's disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "c0c9554e-3372-4125-9695-9a670279d0a2", "title": "Normalization of the Blunted Ventilatory Response to Acute Hypoxia in Congenital Cyanotic Heart Disease", "text": "【0】Normalization of the Blunted Ventilatory Response to Acute Hypoxia in Congenital Cyanotic Heart Disease\nAbstract\n--------\n\n【1】Patients with congenital cyanotic heart disease have a blunted ventilatory response to hypoxia, but the permanence of the blunting is disputed. To determine how early the blunted ventilatory response develops and whether it is reversible, we studied three groups of children and young adults: five (seven to 13 years of age) with acyanotic heart disease, eight (seven to 16) with cyanotic congenital heart disease (arterial oxygen saturation, 55 to 83 per cent), and 13 (seven to 17) whose cardiac defects were repaired (arterial oxygen saturation, 93 to 98 per cent). The ventilatory response to acute hypoxia was subnormal in the hypoxemic children in that their ventilation showed little increase when arterial oxygen saturation fell by 10 to 20 per cent, compared to a 150 to 300 per cent increase in the control subjects. This characteristic even appeared in a seven-year-old patient, indicating that the disorder occurs in early life. The appearance of blunted ventilatory response is delayed when hypoxia from birth is less severe. After operation, with return of the arterial hypoxemia to normal, the response was in the normal range. We conclude that the blunted response is reversible.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "19eede4c-0b8b-49fd-b65b-ba1bee956991", "title": "Normal Erythropoietic Helper T Cells in Congenital Hypoplastic (Diamond-Blackfan) Anemia", "text": "【0】Normal Erythropoietic Helper T Cells in Congenital Hypoplastic (Diamond-Blackfan) Anemia\nAbstract\n--------\n\n【1】To examine the erythropoietic function of T and null cells in congenital hypoplastic (Diamond-Blackfan) anemia, we fractionated the peripheral blood of three normal subjects and three affected patients into subclasses of null, T and B cells. Mixtures of these cells were co-cultured in plasma clots in the presence of erythropoietin. Erythroid colonies grew in cultures of normal null cells if either normal or patient T cells were co-cultured with them. Null cells of patients with hypoplastic anemia did not produce erythroid colonies under any culture conditions. We conclude that in this disorder, T cells function normally as helper cells in erythropoiesis and do not suppress colony formation, whereas the erythroid progenitor cells in the peripheral blood null-cell fractions are deficient in either number or function.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d3237083-d3d2-4c0e-99f6-3a24cb3a1d43", "title": "Breast Cancer and Cigarette Smoking", "text": "【0】Breast Cancer and Cigarette Smoking\nAbstract\n--------\n\n【1】It has been suggested that cigarette smoking may reduce the incidence of breast cancer, perhaps by as much as 20 per cent. To evaluate the relation between breast-cancer risk and smoking, we studied 2160 women with breast cancer and 717 controls who had been admitted to the hospital for cancer of the ovary, cancer of the colon or rectum, malignant melanoma, or lymphoreticular cancers. As compared with women who had never smoked, the estimated relative risk of breast cancer was 1.1 for current smokers of any amount (95 per cent confidence interval, 0.9 to 1.3), and 1.0 (0.8 to 1.3) for heavy smokers (15 or more cigarettes per day). Allowance for all identified potential confounding factors did not materially alter the results. There was no indication that age at commencement of smoking was related to the risk, nor was there evidence of an effect of smoking within the categories of age at first pregnancy or age at menopause. The data provide evidence against the hypothesis that smoking may reduce the incidence of breast cancer by 20 per cent.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "7255af96-1d6b-4dc2-a619-dae73cda9230", "title": "Effects of Dexamethasone in Primary Supratentorial Intracerebral Hemorrhage", "text": "【0】Effects of Dexamethasone in Primary Supratentorial Intracerebral Hemorrhage\nAbstract\n--------\n\n【1】To evaluate the efficacy of dexamethasone for treatment of primary supratentorial intracerebral hemorrhage, we studied 93 patients 40 to 80 years old, using a double-blind randomized block design. After the subjects were stratified according to their level of consciousness (Glasgow Coma Scale), those with objectively documented primary supratentorial intracerebral hemorrhage were randomly assigned to either dexamethasone or placebo. For ethical reasons, three interim analyses were planned, to permit early termination of the trial if one study group did better than the other.\n\n【2】During the third interim analysis, the death rate at the 21st day was identical in the two groups (dexamethasone vs. placebo, 21 of 46 vs. 21 of 47; chi-square = 0.01, P = 0.93). In contrast, the rate of complications (mostly infections and complications of diabetes) was much higher in the dexamethasone group (chi-square = 10.89, P<0.001), leading to early termination of the study. In the light of the absence of a demonstrable beneficial effect and the presence of a significant harmful effect, current practices of using dexamethasone for treatment of primary supratentorial hemorrhage should be reconsidered.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ef062243-a083-4d22-af4d-93b927203692", "title": "Cryptogenic Organizing Pneumonia", "text": "【0】Cryptogenic Organizing Pneumonia\nCOP (formerly bronchiolitis obliterans organizing pneumonia) causes respiratory symptoms, with pulmonary infiltrates that usually respond well to therapy, but it is often misdiagnosed. This review discusses the pathobiology, diagnosis, and treatment of COP.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8acac431-fc1f-4bb5-bc0d-2d60e3c9b562", "title": "Whole-Genome Sequencing and Social-Network Analysis of a Tuberculosis Outbreak", "text": "【0】Whole-Genome Sequencing and Social-Network Analysis of a Tuberculosis Outbreak\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】An outbreak of tuberculosis occurred over a 3-year period in a medium-size community in British Columbia, Canada. The results of mycobacterial interspersed repetitive unit–variable-number tandem-repeat (MIRU-VNTR) genotyping suggested the outbreak was clonal. Traditional contact tracing did not identify a source. We used whole-genome sequencing and social-network analysis in an effort to describe the outbreak dynamics at a higher resolution.\n\n【3】Methods\n-------\n\n【4】We sequenced the complete genomes of 32 _Mycobacterium tuberculosis_ outbreak isolates and 4 historical isolates (from the same region but sampled before the outbreak) with matching genotypes, using short-read sequencing. Epidemiologic and genomic data were overlaid on a social network constructed by means of interviews with patients to determine the origins and transmission dynamics of the outbreak.\n\n【5】Results\n-------\n\n【6】Whole-genome data revealed two genetically distinct lineages of _M. tuberculosis_ with identical MIRU-VNTR genotypes, suggesting two concomitant outbreaks. Integration of social-network and phylogenetic analyses revealed several transmission events, including those involving “superspreaders.” Both lineages descended from a common ancestor and had been detected in the community before the outbreak, suggesting a social, rather than genetic, trigger. Further epidemiologic investigation revealed that the onset of the outbreak coincided with a recorded increase in crack cocaine use in the community.\n\n【7】Conclusions\n-----------\n\n【8】Through integration of large-scale bacterial whole-genome sequencing and social-network analysis, we show that a socioenvironmental factor — most likely increased crack cocaine use — triggered the simultaneous expansion of two extant lineages of _M. tuberculosis_ that was sustained by key members of a high-risk social network. Genotyping and contact tracing alone did not capture the true dynamics of the outbreak. \n\n【9】Introduction\n------------\n\n【10】_Mycobacterium tuberculosis_ is an important infectious disease even in developed countries with extensive control programs. This is the case in British Columbia, Canada, where the 2007 incidence rate of 6.4 cases per 100,000 population exceeded the national average of 4.7 cases per 100,000 population. \n\n【11】In May 2006, a case of smear-negative pleural tuberculosis was diagnosed in an adult in a medium-size community in British Columbia. A second case, manifested as disseminated tuberculosis, was reported in an infant in July 2006. Reverse contact tracing identified nine additional cases between August and October 2006, when the British Columbia Centre for Disease Control (BCCDC) initiated an epidemiologic investigation.\n\n【12】Cases were identified throughout the remainder of 2006 and peaked in 2007, resulting in a 2007 incidence rate of 72 cases per 100,000 population in the region. By December 31, 2008, a total of 41 cases of tuberculosis had been identified, with all cultured isolates having an identical pattern of mycobacterial interspersed repetitive unit–variable-number tandem repeats (MIRU-VNTRs). Traditional contact tracing and social-network analysis were used to identify key persons, places, and behaviors that contributed to the spread of the outbreak. Epidemiologic data were later complemented with whole-genome sequencing of 36 _M. tuberculosis_ isolates from the region.\n\n【13】Methods\n-------\n\n【14】Case Definition and Outcomes\n----------------------------\n\n【15】Laboratory-confirmed cases of tuberculosis were defined as those with _M. tuberculosis_ complex present on culture. Clinical cases were defined as those for which _M. tuberculosis_ or other pathogens could not be cultured but that were characterized by a radiologic, pathologic, or therapeutic response that was consistent with active tuberculosis.  Of the 41 cases, 37 were laboratory confirmed, and 4 were clinical.\n\n【16】Therapeutic outcomes were recorded and classified according to standardized treatment outcomes of the World Health Organization.  All isolates were susceptible to all first-line antituberculosis antibiotics.\n\n【17】Epidemiologic Investigation\n---------------------------\n\n【18】Case finding was carried out from May to October 2006 by local nurses with support from the BCCDC. Traditional contact-tracing approaches were used, in accordance with Canadian guidelines.  In October 2006, two BCCDC investigators were used to assist local health authorities in investigation of the outbreak. The work was deemed part of public health's response to the outbreak and was not subject to institutional review.\n\n【19】A social-network questionnaire (SNQ)  was developed to identify shared socialization settings and to prioritize case finding. The SNQ focused on drug and alcohol use, residential and travel history, places of social aggregation, and identification of contacts in the context of high-risk behaviors and locations, through the use of name-generator questions such as “Who else hangs out there?” The SNQ was administered by trained interviewers in the form of an open-ended interview and was used retrospectively to examine 9 of the 11 cases diagnosed before October 31, 2006 (1 of which occurred in an infant, so the parent was interviewed). With respect to the remaining 2 cases, the patients died before October 2006.\n\n【20】The field investigation concluded in November 2006. Case finding with the use of both traditional contact tracing and the SNQ was continued by the local health authorities. The resulting data were collated into a single network visualized in Cytoscape software  .\n\n【21】Genomic Investigation\n---------------------\n\n【22】_M. tuberculosis_ isolates were cultured at the BCCDC Public Health Microbiology and Reference Laboratory. Restriction-fragment–length polymorphism (RFLP) analysis based on insertion sequence 6110 was performed on the first 13 outbreak isolates, and 24-loci MIRU-VNTR analysis was performed on all 37 laboratory-confirmed, culture-positive isolates at the National Reference Centre for Mycobacteriology of the National Microbiology Laboratory in Winnipeg, Manitoba.\n\n【23】A total of 36 _M. tuberculosis_ isolates (32 of the 37 outbreak isolates and 4 historical isolates with identical MIRU-VNTR patterns) were sequenced (Genome Analyzer II sequencer, Illumina). The number of 50-bp reads ranged from 9.3 million to 21.6 million after filtering, and reads were deposited in the National Center for Biotechnology Information's Sequence Read Archive under accession number SRA020129. Reads were aligned to the reference genome (of _M. tuberculosis_ strain CDC1551), with an average of 99.21% of the reference genome being covered by at least one read. Alignment identified 204 single-nucleotide polymorphisms (SNPs) unique to the 36 isolates. The SNPs were concatenated and used to construct a distance matrix, which was hierarchically clustered to generate a dendrogram. Phylogenetic analysis of the SNPs was also carried out, with the use of maximum-likelihood methods and Bayesian Markov chain Monte Carlo (MCMC) methods. A subgroup of SNPs, not including those from repetitive genes, was also analyzed. Further details are provided in the Supplementary Appendix .\n\n【24】Results\n-------\n\n【25】Clinical and Epidemiologic Characteristics of the Patients\n----------------------------------------------------------\n\n【26】Figure 1. Cases of Tuberculosis in the Outbreak Community and Cocaine-Related Police Files.\n\n【27】The 41 outbreak cases are shown, along with 2 cases before the outbreak, according to the year and calendar quarter (Q) of diagnosis. Cocaine-related Royal Canadian Mounted Police (RCMP) files are files related to charges of cocaine possession or trafficking (or both).Table 1.  Table 1. Characteristics of 41 Patients with Laboratory-Confirmed or Clinical Tuberculosis in a British Columbia Community from May 2006 through December 2008, According to Tuberculosis Lineage.\n\n【28】Between May 2006 and December 2008, a total of 41 cases of tuberculosis were diagnosed in the outbreak community , elevating the annual incidence rate for the region by a factor of more than 10. The majority of the patients were adults (mean age, 36 years) presenting with pulmonary tuberculosis (68%) or pleural tuberculosis (24%) . Two pediatric cases were diagnosed, including one in an infant presenting with disseminated tuberculosis. None of the patients were born outside Canada, and none were positive for the human immunodeficiency virus.\n\n【29】Outcomes were recorded through December 31, 2009, with a minimum of 12 months of follow-up for all patients. The majority of patients had favorable outcomes, with 35 patients (85%) meeting the World Health Organization criteria for “cure or treatment completed.”  One patient (2%) did not complete treatment, and 1 patient (2%) had a relapse of pulmonary tuberculosis. Four deaths (10% of patients) were recorded during the follow-up period, including one death from complications of disseminated tuberculosis and one death from hepatic failure during receipt of antituberculosis medication. Two patients died from unrelated causes (motor-vehicle accident and drug overdose).\n\n【30】Social-Network Analysis to Identify a Source Patient\n----------------------------------------------------\n\n【31】Figure 2. Community Networks of the First 11 Outbreak Cases, Derived from Traditional Contact Tracing and Social-Network Questionnaires.\n\n【32】Panel A shows the network based on contact tracing of the first 11 cases; Panel B shows the network based on the social-network questionnaires administered to 8 patients and the parent of 1 pediatric patient (the remaining 2 patients died before administration of the questionnaire). Orange circles indicate smear-positive outbreak cases, green circles smear-negative outbreak cases, blue circles named contacts, and white circles named places of social aggregation. Purple arrows indicate the most likely source case (identified on the arrow) suggested by the network structure. Heavy lines connect active cases to one another, light lines connect active cases to named contacts, and dashed lines connect active cases to places.\n\n【33】Traditional contact tracing was used early in the outbreak for purposes of secondary case finding and source-case determination. However, a single source case could not be clearly identified, since the contact network was biased toward the pediatric case (MT0009) ; this reflected the use of reverse contact tracing, in which the contacts of the child were screened in an effort to find the source of the child's infection.\n\n【34】A social-network approach was subsequently used. On the basis of a network derived from interviews of patients from early in the outbreak, one case (MT0001) was identified as the most likely source case . MT0001 was the fifth case identified in the outbreak; the patient was an adult with cavitary, smear-positive pulmonary tuberculosis that had been symptomatic and untreated for at least 8 months before the detection of the first case. MT0001 was connected to all but two early cases through direct contact or a shared social setting. Use of the SNQ improved subsequent case-finding efforts by revealing previously unreported social interactions and identifying several locations frequented by infectious patients, including two hotels, a meal center, two community centers, and a series of crack houses.\n\n【35】As in past studies involving SNQs,  use of the SNQ in this study identified demographic characteristics associated with an increased risk of tuberculosis transmission in low-incidence regions. Many SNQ respondents reported transient living arrangements, crack cocaine use (61%), and alcohol use (66%) . The social network was highly transitive (one of several measures of interconnectedness within a social network) , with each patient reporting, on average, social contact with six other patients.\n\n【36】Whole-Genome Sequencing to Identify Genetic Lineage\n---------------------------------------------------\n\n【37】Figure 3. Hierarchical Clustering of Patterns of Mycobacterial Interspersed Repetitive Unit–Variable Number Tandem Repeats (MIRU-VNTRs) and Whole-Genome–Derived Genetic Distances of 36 _Mycobacterium tuberculosis_ Isolates.\n\n【38】The 36 cases of tuberculosis that had been sequenced were analyzed and are presented according to the year and calendar quarter (Q) of the onset of symptoms. In addition, 10 isolates from regional cases (RC) outside the outbreak community that were genotyped between 2004 and 2007 were used to root the MIRU-VNTR tree (left), which illustrates the clonality of the outbreak isolates at low resolution. MIRU loci are listed along the top of the panel, with the values in each column indicating the allele present. In contrast, the higher-resolution whole-genome analysis of single-nucleotide polymorphisms of the outbreak isolates reveals the existence of two distinct cocirculating lineages, A and B.\n\n【39】RFLP and MIRU-VNTR typing of the outbreak isolates yielded identical patterns, suggesting a clonal outbreak . Review of RFLP data from the same geographic area in the years preceding the outbreak indicated that the same clone had been detected sporadically in the region since 1995.\n\n【40】To examine transmission dynamics of the outbreak at a higher resolution, we analyzed the 204 SNPs that differentiated the 36 isolates from each other, most of which were nonsynonymous (44%, with lesser percentages in the other categories), in agreement with previous reports.  Unexpectedly, hierarchical clustering of the SNPs revealed two distinct lineages of _M. tuberculosis_ cocirculating within the population , suggesting not one outbreak but two simultaneous outbreaks. Thirteen outbreak isolates clustered to form lineage A, whereas 19 outbreak isolates formed lineage B. The presence of two lineages was observed regardless of the similarity measure implemented during hierarchical clustering and was also observed when clustering was replaced with a Bayesian MCMC phylogenetic method. A maximum-likelihood method yielded a nearly identical tree, with 28 of 32 outbreak isolates belonging to the expected lineage. Four isolates (MT0031, MT0033, MT0034, and MT0036) whose lineage membership varied in the maximum-likelihood tree may reflect cases of coinfection with lineage A and B strains, resulting in a mixed signal that was difficult to resolve through maximum-likelihood approaches. Analysis of a data set filtered to remove SNPs in repetitive regions, as well as those appearing in a single isolate, recapitulated the dual-lineage observation.\n\n【41】No significant differences in demographic characteristics, clinical presentation, outcomes, or risk factors were detected between patients with lineage A tuberculosis and those with lineage B tuberculosis . Examination of the complete community network revealed that most outbreak cases (80%) involved direct contact with both lineages, suggesting a single heterogeneous social network rather than two independent subnetworks associated with each lineage .\n\n【42】Origins of the Outbreak\n-----------------------\n\n【43】The historical isolates MT0005 (from 1995), MT0006 (from 2000), and MT0007 (from 2001) were most similar to lineage A (in maximum-likelihood and MCMC analyses), whereas the historical isolate MT0035 (from 2001) clustered with lineage B (in clustering, maximum-likelihood, and MCMC analyses), indicating that divergence of the two lineages preceded the current outbreak. The observation that both lineages had been present in the region for at least 5 years before the outbreak suggested that a social or environmental factor operating before 2006 triggered the outbreak, rather than a genetic change in the organism.\n\n【44】Epidemiologic follow-up revealed that of the 41 patients with tuberculosis during the outbreak, 25 (61%) had a self-reported history of crack cocaine use. Cocaine-related police files in the outbreak community mirrored the epidemic curve, peaking in 2006 before declining over the subsequent years , and a number of crack houses proliferated in the region during the same period (Murray K, Royal Canadian Mounted Police: personal communication), suggesting that the use of crack cocaine may have played a role in triggering and sustaining the outbreak.\n\n【45】Construction of Putative Transmission Networks\n----------------------------------------------\n\n【46】By applying three rules, social contacts linked to putative transmission events can be identified from a social network: first, transmission moves forward in time; second, a person with only a single previously infectious contact must have been infected by that contact; and third, in cases in which a person has multiple upstream infectious contacts, smear-positive contacts are more likely to transmit disease than smear-negative contacts.\n\n【47】Figure 4. Putative Transmission Networks Constructed from Genotyping Data versus Whole-Genome Data for 32 Patients.\n\n【48】Genotyping data from analyses of mycobacterial interspersed repetitive unit–variable number tandem repeats (MIRU-VNTRs) were used in Panel A, and whole-genome data were used in Panel B. Each panel shows patients (identified by case number) represented by circles colored according to smear status and clinical presentation as an index of infectivity: black circles indicate smear-positive pulmonary disease, gray circles smear-positive miliary disease or smear-negative pulmonary disease, and white circles smear-negative extrapulmonary disease. The cases are connected by arrows on the basis of reported social relationships representing plausible transmission attributable to a single case (purple arrows) or multiple potential sources of transmission (light blue lines), with dashed arrows indicating moderately infective patients and solid lines highly infective patients. In Panel A, the case with the earliest symptom onset was MT0001 (center), and the second-earliest case is shown at the 12 o'clock position, with the remaining cases listed in the clockwise direction in order of increasing time since symptom onset. When a clonal outbreak is assumed, the epidemiologic interpretation of the data suggests that most transmission events can be traced to the source case, MT0001. In Panel B, the cases are shown according to circular dendrograms based on whole-genome data reflecting the tuberculosis lineage (A in blue and B in pink). This network provides a more accurate picture of transmission, with transmission restricted to each lineage, facilitating epidemiologic interpretation of the underlying social-network data and revealing the role of the second and third source cases (MT0010 and MT0011).\n\n【49】In our study, on the basis of MIRU-VNTR genotyping alone, in which all isolates are assumed to be genetically equivalent, the majority of putative transmission events (13 events) were linked to MT0001 . Of these, 9 are biologically impossible if the genomic data are considered, since MT0001, a case of lineage A tuberculosis, could not have been the source of infection with lineage B.\n\n【50】To construct a more accurate transmission network, the social-network data were visualized in the context of the dendrogram generated from the genomic data . Social relationships between patients with tuberculosis of the other lineage were removed (since lineage A cases could not have caused lineage B cases and vice versa), and the three rules of transmission were applied.\n\n【51】Social-network analysis had identified MT0001 as the most probable source case. This result was recapitulated in the genomics-based transmission network, with MT0001 acting as a “superspreader,” probably causing six cases within lineage A. In lineage B, two cases, MT0010 and MT0011, were most likely to have been responsible for four and three subsequent cases, respectively. Clinically, the three source cases were characterized by smear-positive disease with pronounced delays (2 to 10 months) between the onset of symptoms and diagnosis.\n\n【52】Examining clades of closely related isolates revealed that within a clade, most patients did not have social relationships with other patients in the clade; rather, relationships consisted of a shared contact who was the source of transmission (e.g., Z to X and Z to Y), probably within a short time frame. Pairwise genetic distances between strains from source patients and strains from their exposed contacts may be affected by multiple variables, including length of time between infection and method of specimen collection. Strains present in patients with a high bacillary burden for several months before diagnosis and specimen collection have most likely undergone microevolution, manifested as greater genetic diversity in strains isolated from persons they infect after longer periods.\n\n【53】Discussion\n----------\n\n【54】Between May 2006 and December 2008, a total of 41 cases of tuberculosis were diagnosed in a British Columbia community struggling with the challenges of alcoholism, drug use, and transient housing arrangements. Because of the recognized limitations of contact tracing,  field epidemiologists used social-network analysis early in the outbreak. Social-network analysis can improve case finding in vulnerable populations through structured interviews that identify high-risk behaviors, places of social aggregation, and persons not specifically named in traditional contact tracing.  In the outbreak we studied, social-network analysis outperformed contact tracing in identifying a probable source case as well as several locations and persons subsequently targeted for follow-up.\n\n【55】Throughout the field investigation, it was assumed that the outbreak was clonal because all isolates exhibited identical MIRU-VNTR patterns. Identical genetic fingerprints are frequently used to identify clusters of tuberculosis cases that may be epidemiologically linked  and as a surrogate for identity at the whole-genome level. Follow-up genome sequencing revealed this assumption to be false, showing that conventional molecular epidemiologic techniques cannot capture the full level of genetic diversity within a single genotype of _M. tuberculosis,_ which may comprise multiple distinct lineages. The higher-resolution SNP patterns afforded by whole-genome sequencing revealed that the outbreak was the coalescence of two outbreaks, each with its own causative lineage of _M. tuberculosis_ .\n\n【56】Association of historical isolates with each lineage suggested that divergence of the two lineages from a common ancestor occurred before 2001. Retrospective review of contact-tracing data around the four sequenced historical isolates did not yield direct contacts between patients with historical cases and those with outbreak cases of the same lineage, but all these patients were known to frequent common locations in the community. Had social-network analysis and location-based screening been performed for the historical cases rather than contact tracing alone, the present outbreak might have been prevented.\n\n【57】The highly transitive nature of the outbreak community's social network made transmission patterns difficult to identify with the use of epidemiologic techniques alone. Our findings show that genome sequencing allows the larger social network to be divided into subnetworks associated with specific genetic lineages of disease. Removal of social relationships that could not have led to transmission according to the genomic data greatly reduced the complexity of the network, increased the usefulness of the existing epidemiologic data, and allowed for the identification of individual transmission events and three coprimary source patients. The source patients all had smear-positive tuberculosis, were highly connected within the social network, and had been symptomatic for several months before treatment. In the social network we studied, the contribution of these “superspreader” cases to the overall prevalence of disease played a larger role than secondary transmission through person-to-person contact.\n\n【58】The simultaneous reappearance of two extant lineages suggests that a social or environmental factor, not a genetic change in the organism, most likely triggered the outbreak. Our observations suggest that a rise in crack cocaine use within the community, peaking in 2006, may have been this trigger. Previous work has established that social networks can mediate behaviors related to addictive substances.  However, a lack of longitudinal network data precludes delineation of the spread of crack cocaine in the network described here. Surveys of crack cocaine users describe findings that are consistent with high rates of tuberculosis exposure,  and several tuberculosis outbreaks have been reported in crack-using communities.  A constellation of impaired host immunity, lack of access to health care, and crowded, poorly ventilated environments most likely predisposes users to both exposure to and development of tuberculosis.  Poverty and the highly transitive social networks established by crack users may also contribute to tuberculosis transmission.\n\n【59】In summary, we describe the integration of social-network analysis with high-resolution bacterial genome sequencing to enhance the investigation of a tuberculosis outbreak. Our data show that epidemiologic inferences are greatly improved by interpretation in the context of whole-genome sequencing and that a single _M. tuberculosis_ genotype contains enough genetic diversity to refine epidemiologic linkages on the basis of genetic lineage or clade membership.\n\n【60】Although whole-genome sequencing may not currently be feasible as a routine tool for outbreak investigation, data generated by this and other sequencing projects will identify the most informative panel of markers of genomic variability in _M. tuberculosis,_ facilitating the development of higher-resolution genotyping methods. As the cost of whole-genome sequencing continues to decrease and next-generation sequencing platforms become integrated into public health practice, combined microbial genomic and epidemiologic approaches like those described here will become an important and tractable first step toward a systems approach to tuberculosis control.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9eda8dc3-201a-4bf9-9d48-04a6a3ca40d7", "title": "Narcolepsy", "text": "【0】Narcolepsy affects approximately 1 in 2000 people but is often not correctly diagnosed. This review article summarizes recent discoveries regarding the neurobiologic features of this condition and describes a practical clinical approach to effective treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1c5c6757-cb7f-4eef-89ee-1ef3cd8e8c7a", "title": "Intrauterine Infection with Varicella-Zoster Virus after Maternal Varicella", "text": "【0】Intrauterine Infection with Varicella-Zoster Virus after Maternal Varicella\nAbstract\n--------\n\n【1】We investigated the consequences of maternal infection with Varicella Zoster virus in a prospective study of 43 pregnancies complicated by varicella and 14 pregnancies complicated by herpes zoster. Nine of 43 pregnant women with varicella had associated morbidity — pneumonia (4 women), death (1), premature labor (4 of 42), premature delivery (2 of 42), and herpes zoster (1). Intrauterine varicella infection was identified on the basis of clinical evidence (anomalies characteristic of the congenital varicella syndrome, acute varicella at birth, or herpes zoster in infancy) or immunologic evidence (IgM antibody to Varicella Zoster in the neonatal period, persistent IgG antibody to Varicella Zoster at one to two years of age, or in vitro lymphocyte proliferation in response to varicella–zoster virus antigen). The congenital varicella syndrome occurred in 1 of 11 infants of women with first-trimester varicella. Immunologic evidence of intrauterine varicella infection was present in 7 of 33 infants tested; 4 of these infants were asymptomatic. According to clinical or immunologic criteria, 8 of 33 infants had evidence of intrauterine varicella infection. These observations show that varicella during pregnancy was associated with maternal morbidity and evidence of fetal infection, but that herpes zoster was not.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5fa4211e-c1e4-4ade-8c19-3a9806002e50", "title": "Efficacy and Safety of Degludec versus Glargine in Type 2 Diabetes", "text": "【0】Efficacy and Safety of Degludec versus Glargine in Type 2 Diabetes\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Degludec is an ultralong-acting, once-daily basal insulin that is approved for use in adults, adolescents, and children with diabetes. Previous open-label studies have shown lower day-to-day variability in the glucose-lowering effect and lower rates of hypoglycemia among patients who received degludec than among those who received basal insulin glargine. However, data are lacking on the cardiovascular safety of degludec.\n\n【3】Methods\n-------\n\n【4】We randomly assigned 7637 patients with type 2 diabetes to receive either insulin degludec (3818 patients) or insulin glargine U100 (3819 patients) once daily between dinner and bedtime in a double-blind, treat-to-target, event-driven cardiovascular outcomes trial. The primary composite outcome in the time-to-event analysis was the first occurrence of an adjudicated major cardiovascular event (death from cardiovascular causes, nonfatal myocardial infarction, or nonfatal stroke) with a prespecified noninferiority margin of 1.3. Adjudicated severe hypoglycemia, as defined by the American Diabetes Association, was the prespecified, multiplicity-adjusted secondary outcome.\n\n【5】Results\n-------\n\n【6】Of the patients who underwent randomization, 6509 (85.2%) had established cardiovascular disease, chronic kidney disease, or both. At baseline, the mean age was 65.0 years, the mean duration of diabetes was 16.4 years, and the mean (±SD) glycated hemoglobin level was 8.4±1.7%; 83.9% of the patients were receiving insulin. The primary outcome occurred in 325 patients (8.5%) in the degludec group and in 356 (9.3%) in the glargine group (hazard ratio, 0.91; 95% confidence interval, 0.78 to 1.06; P<0.001 for noninferiority). At 24 months, the mean glycated hemoglobin level was 7.5±1.2% in each group, whereas the mean fasting plasma glucose level was significantly lower in the degludec group than in the glargine group (128±56 vs. 136±57 mg per deciliter, P<0.001). Prespecified adjudicated severe hypoglycemia occurred in 187 patients (4.9%) in the degludec group and in 252 (6.6%) in the glargine group, for an absolute difference of 1.7 percentage points (rate ratio, 0.60; P<0.001 for superiority; odds ratio, 0.73; P<0.001 for superiority). Rates of adverse events did not differ between the two groups.\n\n【7】Conclusions\n-----------\n\n【8】Among patients with type 2 diabetes at high risk for cardiovascular events, degludec was noninferior to glargine with respect to the incidence of major cardiovascular events. \n\n【9】Introduction\n------------\n\n【10】Cardiovascular complications remain two to four times more common among patients with type 2 diabetes than among persons without diabetes.  Observational studies have suggested that patients with type 2 diabetes who require insulin have increased rates of cardiovascular events.  However, a large clinical trial involving patients with impaired fasting glucose levels, impaired glucose tolerance, or type 2 diabetes reported cardiovascular outcomes among those who received basal insulin glargine that were similar to outcomes among patients who received standard care. \n\n【11】Degludec is an ultralong-acting, once-daily basal insulin approved for use in adults, adolescents, and children with diabetes.  Previous open-label studies have shown lower day-to-day variability in the glucose-lowering effect and lower rates of hypoglycemia among the patients who received degludec than among those who received glargine.  The Food and Drug Administration (FDA) required that a dedicated preapproval trial of cardiovascular outcomes be conducted to assess the cardiovascular safety of degludec, as compared with glargine. Consequently, we conducted the Trial Comparing Cardiovascular Safety of Insulin Degludec versus Insulin Glargine in Patients with Type 2 Diabetes at High Risk of Cardiovascular Events (DEVOTE).\n\n【12】Methods\n-------\n\n【13】Trial Design and Oversight\n--------------------------\n\n【14】Detailed methods of the trial have been published previously.  Briefly, the trial was a treat-to-target, randomized, double-blind, active comparator–controlled cardiovascular outcomes trial that was conducted at 438 sites in 20 countries. The trial was designed to continue until the occurrence of at least 633 primary outcome events, as confirmed by central, blinded review by an independent event-adjudication committee.\n\n【15】The trial was conducted in accordance with the provisions of the Declaration of Helsinki and the International Conference on Harmonisation Good Clinical Practice Guidelines.  The protocol  was approved by the independent ethics committee or institutional review board at each trial center. Written informed consent was obtained from each patient before any trial-related activities.\n\n【16】The trial was funded and conducted by Novo Nordisk. Statogen Consulting and Novo Nordisk both independently analyzed the data only after the database lock. The steering committee, which was composed of the authors, participated in designing the trial, analyzing the data, editing an earlier version of the manuscript, and making the decision to submit the manuscript for publication. Medical writing and editorial support were funded by the sponsor. The authors had full access to all the trial data and vouch for the completeness and integrity of the data and for the fidelity of the trial to the protocol.\n\n【17】A prespecified interim analysis was planned, for regulatory purposes as agreed with the FDA, to assess the noninferiority of degludec versus glargine for cardiovascular safety after the occurrence of 150 primary outcome events, as confirmed by the event-adjudication committee.  Per regulatory guidance, the confirmation of an upper limit of the confidence interval below 1.8 at the interim analysis was required to establish noninferiority and allow confidential FDA review.  On the basis of the results of the submitted interim analysis, the FDA approved the use of degludec in the United States in September 2015.\n\n【18】  The conduct of the trial was overseen by a steering committee that consisted of academic investigators and Novo Nordisk employees. In addition, an independent external data and safety monitoring committee was established to review accumulated data and evaluate the risk–benefit balance at planned intervals. An external independent statistics group, Statistics Collaborative, provided unblinded data to the data and safety monitoring committee, which could recommend to continue, modify, or terminate the trial prematurely on the basis of criteria developed before the initiation of the trial. Operational advice for the trial was provided by the global expert panel throughout the trial.\n\n【19】Patients and Treatments\n-----------------------\n\n【20】Patients with type 2 diabetes who were at high risk for cardiovascular events were randomly assigned in a  ratio to receive either degludec or glargine (both in identical 10-ml vials containing 100 U per milliliter), with each drug added to standard care and administered once daily between dinner and bedtime. Eligible patients included those who were being treated with at least one oral or injectable antihyperglycemic agent. Also required was a glycated hemoglobin level of 7% or more while the patients were receiving the antihyperglycemic agent; if the level was less than 7%, treatment with at least 20 units of basal insulin per day was required. Two groups of patients were eligible for the trial: those who were 50 years of age or older who had at least one coexisting cardiovascular or renal condition and those who were 60 years of age or older who had at least one cardiovascular risk factor. A complete list of inclusion and exclusion criteria is provided in the Supplementary Appendix .\n\n【21】Patients could continue their pretrial antihyperglycemic therapy except for basal and premix insulins, which were discontinued. Patients adjusted their dose of basal insulin weekly on the basis of the lowest of three self-measured blood-glucose values, as measured before breakfast 2 days before and on the day of dose adjustment, with the aim of reaching a target of 71 to 90 mg per deciliter (4.0 to 5.0 mmol per liter) . To safeguard vulnerable patients, an alternative blood-glucose target of 90 to 126 mg per deciliter (5.0 to 7.0 mmol per liter) was available for these patients. Bolus insulin (aspart) was provided by Novo Nordisk for patients who were either continuing or initiating bolus treatment during the trial, with weekly adjustments based on the lowest of three preprandial or bedtime self-measured blood-glucose values measured on the 3 days before dose adjustment and aiming to reach a target of 71 to 126 mg per deciliter . Higher targets were allowed at the discretion of the investigator.\n\n【22】The following events were adjudicated by the event-adjudication committee in a blinded manner: acute coronary syndrome (defined as myocardial infarction or unstable angina pectoris leading to hospitalization), stroke, death, and severe hypoglycemia. The definitions that were used for the clinical-event adjudication are provided in the Supplementary Appendix . Neoplasms were classified by a blinded independent committee as malignant, benign, or not classifiable. For neoplasms that were classified as malignant, a further subclassification was performed to assess the primary organ site.\n\n【23】Outcomes\n--------\n\n【24】All outcomes were prespecified unless otherwise stated. The primary composite outcome in the time-to-event analysis was the first occurrence of death from cardiovascular causes, nonfatal myocardial infarction, or nonfatal stroke. The multiplicity-adjusted confirmatory secondary outcomes were the number and incidence of adjudicated events of severe hypoglycemia, which was defined by the American Diabetes Association as an episode requiring the assistance of another person to actively administer carbohydrate or glucagon or to take other corrective actions.  According to this definition, plasma glucose levels may not be available during an event, but neurologic recovery after the return of plasma glucose to a normal level is considered to be sufficient evidence that the event was induced by a low plasma glucose level.\n\n【25】Other secondary outcomes included an expanded composite cardiovascular outcome (the primary composite outcome or unstable angina leading to hospitalization) and the time from randomization to death from any cause, along with serious adverse events or adverse events leading to discontinuation of the intervention, levels of glycated hemoglobin and fasting plasma glucose, blood pressure, pulse, lipid measurements, weight, body-mass index, estimated glomerular filtration rate, nocturnal severe hypoglycemia (occurring between  a.m. and  a.m.), and basal and bolus insulin dose. Glycated hemoglobin was measured at randomization, at months 3, 6, 9, and 12, and yearly thereafter. Other laboratory tests were performed at randomization and yearly thereafter.\n\n【26】Statistical Analysis\n--------------------\n\n【27】The statistical analysis plan is available in the Supplementary Appendix . Details regarding the sample-size estimates and statistical analyses have been published previously.  We estimated that the follow-up of 7500 patients for approximately 5 years with an assumed event rate of 2.1 per 100 patient-years of exposure would produce 633 events and hence a power of 91% to rule on the null hypothesis. A Cox proportional-hazards regression model was used to analyze the intention-to-treat population for the primary composite outcome to test for the noninferiority of degludec as compared with glargine. Noninferiority would be confirmed if the upper boundary of the 95% confidence interval was less than 1.3. If noninferiority was established, we then tested for superiority with respect to severe hypoglycemic episodes using a negative binomial-regression model that was adjusted for observation time and treatment group to test for the number of events and a logistic-regression model that was adjusted for treatment group to test for incidence. Superiority of these secondary outcomes would be confirmed if the upper boundary of the 95% confidence interval was less than 1.0. Selected sensitivity analyses, including the per-protocol analysis, were performed to address the robustness of the results. The rationale for the use of a noninferiority threshold of 1.3 in the primary analysis and a threshold of 1.8 in the interim analysis is described in the Supplementary Appendix .\n\n【28】Results\n-------\n\n【29】Patients\n--------\n\n【30】From November 2013 through November 2014, a total of 7637 patients were randomly assigned to receive either degludec (3818 patients) or glargine (3819 patients) once daily . Of these patients, 98% completed the final follow-up visit or died during the trial . The vital status was known for 99.9% of the patients. Five patients (0.06%) were lost to follow-up, and three patients (0.04%) had withdrawn consent at the time of the database lock. The median observation time was 1.99 years, and the median exposure time was 1.83 years.\n\n【31】The characteristics of the patients at baseline were similar in the two groups . Of the 7637 patients, 6509 (85.2%) had established cardiovascular disease or moderate chronic kidney disease. The mean age was 65.0 years, the mean duration of diabetes was 16.4 years, and the mean (±SD) glycated hemoglobin level was 8.4±1.7%. Of the 6409 patients (83.9%) who were receiving insulin at baseline, 3515 (54.8%) were receiving a basal–bolus regimen.\n\n【32】Cardiovascular Outcomes\n-----------------------\n\n【33】Table 1. Primary Outcomes. Figure 1.  Figure 1. Kaplan–Meier Analysis of the Composite Primary Outcome.\n\n【34】Shown are plots of time until the primary outcome  and its composite events — death from cardiovascular causes , nonfatal myocardial infarction , and nonfatal stroke  — in the degludec group and the glargine group. The noninferiority of degludec as compared with glargine was confirmed because the upper limit of the two-sided 95% confidence interval for the hazard ratio was less than 1.3. The results were determined by the event-adjudication committee on the basis of Cox proportional-hazards regression analysis in the intention-to-treat population. Data for patients without an event were censored at the time of the last contact (telephone or visit). The inset graphs show the same data on expanded y axes.\n\n【35】The primary composite outcome occurred in 325 patients (8.5%) in the degludec group and in 356 patients (9.3%) in the glargine group (hazard ratio, 0.91; 95% confidence interval \\[CI\\], 0.78 to 1.06; P<0.001 for noninferiority in a one-sided test) . Individual components of the composite cardiovascular outcome are provided in Table 1 and Figures 1B, 1C, and 1D . There was no significant difference in the incidence of death in the degludec and glargine groups (202 patients \\[5.3%\\] vs. 221 patients \\[5.8%\\]; hazard ratio, 0.91; 95% CI, 0.76 to 1.11; P=0.35).\n\n【36】The results of various sensitivity analyses that used alternative censoring methods were aligned with the findings of the primary analysis and are shown, along with the subgroup analyses, in Figures S3 and S4 in the Supplementary Appendix . Findings for the remaining adjudicated cardiovascular outcomes and the expanded composite outcome are shown in Figure S5 in the Supplementary Appendix .\n\n【37】Severe Hypoglycemia\n-------------------\n\n【38】Table 2. Secondary Outcomes. Figure 2.  Figure 2. Severe Hypoglycemia and Glucose Control.\n\n【39】Shown are the observed cumulative number of events of severe hypoglycemia  and nocturnal severe hypoglycemia  per patient in the degludec group and the glargine group. For severe hypoglycemia, the superiority of degludec over glargine was confirmed because the upper limit of the two-sided 95% confidence interval for the estimated rate ratio was below 1.0. Nocturnal severe hypoglycemia was defined as an episode with an investigator-reported onset between  a.m. and  a.m. Also shown are measures of treatment efficacy, according to the glycated hemoglobin level  and the fasting plasma glucose level  in the two groups, with both comparisons performed in post hoc analyses.\n\n【40】A total of 752 severe hypoglycemic events occurred, 280 events in 187 patients in the degludec group and 472 events in 252 patients in the glargine group; the rate was 3.70 events per 100 patient-years in the degludec group and 6.25 events per 100 patient-years in the glargine group (rate ratio, 0.60; 95% CI, 0.48 to 0.76; P<0.001 for superiority) . One or more events of severe hypoglycemia  occurred in 187 patients (4.9%) in the degludec group and in 252 (6.6%) in the glargine group, for an absolute difference of 1.7 percentage points (odds ratio, 0.73; 95% CI, 0.60 to 0.89; P<0.001 for superiority) . Of the 752 severe hypoglycemic events that occurred in the two groups, blood-glucose measurements were available for 637 events (84.7%) . In addition, there was a lower rate of nocturnal severe hypoglycemia in the degludec group than in the glargine group (0.65 vs. 1.40 events per 100 patient-years) for a rate ratio of 0.47 (95% CI, 0.31 to 0.73; P<0.001) .\n\n【41】The results of the on-treatment analyses were similar to those in the primary analyses . The results of subgroup analyses are shown in Figure S9 in the Supplementary Appendix . Treatment ratios differed significantly in subgroups defined according to sex, ethnic group (Hispanic or Latino vs. not Hispanic or Latino), cardiovascular risk group (established cardiovascular disease vs. risk factors), and trial center (United States vs. other countries in DEVOTE).\n\n【42】Glycemic Control\n----------------\n\n【43】There was no significant between-group difference in total and bolus insulin dose levels over time . For basal insulin, the estimated dose of degludec was 2 units higher than the dose of glargine (estimated treatment ratio, 1.04; 95% CI, 1.00 to 1.08; P=0.04) at 24 months . Overall initiation of concomitant antihyperglycemic medications during the trial was similar in the two groups .\n\n【44】There also was no significant between-group difference in changes in glycated hemoglobin levels throughout the trial . At 24 months, the glycated hemoglobin level was 7.5% (58 mmol per mole) in the two groups, with an estimated treatment difference of 0.01 percentage points (95% CI, –0.05 to 0.07; P=0.78 in post hoc analysis). Over 24 months, plasma glucose values that were measured by the patients before breakfast were similar in the two groups; the median value for all patients was 95 mg per deciliter (5.3 mmol per liter) .\n\n【45】At 24 months, the mean laboratory-measured fasting plasma glucose level was significantly lower in the degludec group than in the glargine group (128±56 vs. 136±57 mg per deciliter \\[7.1±3.1 vs. 7.5±3.2 mmol per liter\\]). Laboratory-measured fasting plasma glucose levels decreased more in the degludec group than in the glargine group (–39.9 mg per deciliter vs. –34.9 mg per deciliter \\[–2.2 mmol per liter vs. –1.9 mmol per liter\\]) after 24 months (estimated treatment difference, −7.2 mg per deciliter; 95% CI, –10.3 to –4.1 \\[–0.4 mmol per liter; 95% CI, –0.6 to –0.2\\]; P<0.001 in post hoc analysis) .\n\n【46】Cardiovascular Risk Factors\n---------------------------\n\n【47】The observed mean change in cardiovascular risk factors from baseline to month 24 did not differ between treatment groups for the following variables: weight, body-mass index, blood pressure, pulse, estimated glomerular filtration rate, and all blood lipid levels (high-density lipoprotein cholesterol, low-density lipoprotein cholesterol, total cholesterol, and triglycerides) . Changes in the overall use of cardiovascular medications during the trial were similar in the two groups .\n\n【48】Safety and Adverse Events\n-------------------------\n\n【49】Table 3. Selected Adverse Events Reported during the Trial.\n\n【50】The rate of adverse events was 44.7 events per 100 patient-years in the degludec group and 50.1 events per 100 patient-years in the glargine group; the corresponding rates of serious adverse events were 44.2 events versus 49.6 events per 100 patient-years . The rate of events leading to permanent discontinuation of a trial drug was 3.7 events per 100 patient-years in the degludec group and 4.0 events per 100 patient-years in the glargine group. The numbers of malignant, benign, and unclassifiable neoplasms were similar in the two groups . Serious adverse events that occurred in at least 1% of the patients and critical symptoms associated with severe hypoglycemic episodes (as confirmed by the event-adjudication committee) are described in Tables S6, S7, and S8 in the Supplementary Appendix . There were no confirmed fatal events associated with hypoglycemia.\n\n【51】Discussion\n----------\n\n【52】In this cardiovascular outcomes trial of basal insulin therapy in patients with type 2 diabetes at high cardiovascular risk, we found that degludec was noninferior to glargine in terms of cardiovascular events and superior with regard to hypoglycemia risk, with a lower rate of both severe and nocturnal severe hypoglycemia (by 40% and 53%, respectively; P<0.001 for both comparisons). These results were achieved at equivalent glycemic control in the two groups. The demonstrated safety of degludec with respect to cardiovascular outcomes was reflected in the individual components of the primary composite outcome and was consistent across multiple prespecified subgroups.\n\n【53】Patients with diabetes have a greater risk of cardiovascular disease and cardiovascular-related death than do persons without diabetes.  Several trials have consequently investigated the effect of an intensive reduction in glycemic levels on the risk of cardiovascular outcomes in patients with type 2 diabetes.  The results of these trials have been varied, with UKPDS, ADVANCE, and VADT showing a neutral effect of reducing glycemic levels on the risk of cardiovascular events, whereas ACCORD showed a significantly increased risk of death both from cardiovascular causes and from any cause associated with more intensive glycemic control.  The focus on cardiovascular outcomes related to diabetes treatments was intensified when the FDA issued guidance in 2008 that described the need to establish the cardiovascular safety of new antihyperglycemic therapies.  This recommendation led to the conduct of numerous cardiovascular outcomes trials involving patients with diabetes.  Although the FDA guidance did not specifically include various types of insulin, the ORIGIN trial, which was designed before the issuing of the FDA guidance and specifically sought to evaluate the cardiovascular safety of glargine, showed no significant difference in cardiovascular outcomes with glargine as compared with standard care.  In the context of this complex landscape of cardiovascular outcomes trials, we found that degludec was not associated with a greater risk of cardiovascular outcomes than was glargine at the same level of glycemic control.\n\n【54】The development of basal insulins with more stable pharmacodynamic profiles has allowed patients to aim safely for fasting glucose levels in the normal range by providing a consistent glucose-lowering effect with a half-life of more than 24 hours and thereby reducing the occurrence of hypoglycemia.  The reduction in severe hypoglycemia that is reported in our trial and in previous trials that have compared degludec with glargine probably results from the improved pharmacodynamic profile of degludec. \n\n【55】The incidence and rates of severe hypoglycemia across cardiovascular outcomes trials  are difficult to compare owing to differences in the definitions that were used and to factors such as frailty, diabetes duration, frequency of insulin use, treatment regimens, and treatment targets at baseline and during the trial. Among all the patients in our trial, the incidence of severe hypoglycemia (2.90 events per 100 patient-years) and rates (4.97 events per 100 patient-years) were within the range that was evident in studies in which the use of insulin was a prominent component of therapy, which had a range of incidences from 0.53 to 5.05 events per 100 patient-years and a range of rates from 0.70 to 8.25 events per 100 patient-years.  Severe hypoglycemia is associated with broad negative consequences for patients with diabetes.  The number of patients who would need to be treated with degludec rather than glargine to avert 1 severe hypoglycemic event is 40.\n\n【56】Our trial has several strengths, including its double-blind design, large enrollment of patients at high cardiovascular risk, and high retention rate of patients. The primary limitation of the trial is its intermediate duration (2 years). Whether these findings can be extrapolated to longer exposure, to patients with a lower risk of cardiovascular events, or both is uncertain. Furthermore, no adjustments were made for multiplicity in the exploratory analysis beyond the prespecified hierarchical analyses of the cardiovascular outcomes and severe hypoglycemia. Overall, the exploratory analyses support the results for the primary and secondary outcomes. However, it is important to emphasize that these analyses are exploratory and have not been adjusted for multiple testing. \n\n【57】In conclusion, we found that in patients with type 2 diabetes at high risk for cardiovascular events, degludec was noninferior to glargine in terms of the incidence of cardiovascular events.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9a29110f-2f4d-4b48-a4e2-d6d2033b3114", "title": "Delaying Pregnancy during a Public Health Crisis — Examining Public Health Recommendations for Covid-19 and Beyond", "text": "【0】Delaying Pregnancy during a Public Health Crisis — Examining Public Health Recommendations for Covid-19 and Beyond\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with Dr. Sonja Rasmussen on guidance related to pursuing or delaying pregnancy during a public health emergency. \n\n【3】During previous public health emergencies, the issue of whether public health agencies should recommend that women avoid becoming pregnant because of potential risks to themselves and their newborns has been controversial. The ongoing Covid-19 pandemic has again led to questions regarding whether women should consider postponing pregnancy because of potential virus-related risks. Such discussions involve important ethical considerations.\n\n【4】The issue of pregnancy avoidance emerged early in the HIV epidemic, after the recognition that perinatal transmission occurred in a quarter of pregnancies among women with HIV and that both perinatally infected children and their mothers had poor outcomes. Some public health and professional organizations, including the Centers for Disease Control and Prevention (CDC), initially recommended that women with HIV postpone becoming pregnant until more was known. This recommendation raised a number of ethical concerns, including concern about the potential for moral censure of women, coercive policies, constraints on reproductive autonomy, and discrimination. In 1996, a multidisciplinary working group on HIV and reproduction recommended a policy of “contextualized counseling” that wouldn’t rule out a thoughtful decision to proceed with plans for pregnancy while acknowledging the known risks of untreated HIV for women and their offspring. \n\n【5】As former CDC employees who played leadership roles in the responses to the 2009 H1N1 influenza and Zika virus outbreaks, two of us (S.A.R., D.J.J.) became aware of appeals for the CDC to recommend postponing pregnancy during these public health emergencies. During the H1N1 influenza pandemic, as data emerged showing that pregnant women with H1N1 influenza were at increased risk for complications and death, some clinicians called for the CDC to recommend that women wait until the pandemic was over to become pregnant. Given the ability to mitigate the risk of harm to pregnant women by means of influenza prevention, antiviral prophylaxis, and early treatment, the CDC instead focused its efforts on educating women and their clinicians on ways to prevent infection, the importance of early treatment, and the need for vaccination once a vaccine became available.\n\n【6】The issue of delaying conception was again raised during the Zika outbreak in 2016 and 2017. Because of substantial risks to the fetus, the question of whether women living in areas with active Zika virus transmission should avoid becoming pregnant was discussed. Given that such risks could be mitigated by avoiding mosquito bites and protecting against sexual transmission, the CDC recommended that clinicians discuss with patients the risks of Zika virus infection during pregnancy and ways to avoid transmission and ask patients about their reproductive life plans, rather than recommending postponing pregnancy. To better support women who wanted to avoid becoming pregnant during the outbreak, the CDC implemented efforts to improve access to contraception. Some experts, however, criticized the agency’s decision not to recommend that women living in areas with active Zika transmission avoid pregnancy.\n\n【7】The subject of delaying conception has once again arisen during the Covid-19 pandemic. Although data on Covid-19–related risks to pregnant women and newborns are limited, a recent study found that pregnant women with Covid-19 have 1.5 times the risk of being admitted to an intensive care unit (ICU) and 1.7 times the risk of requiring mechanical ventilation faced by nonpregnant women of childbearing age with Covid-19 but that pregnant women aren’t at increased risk for death.  Information on adverse pregnancy outcomes associated with Covid-19 is also limited. Intrauterine transmission of SARS-CoV-2 appears to occur rarely,  and no evidence has suggested an association between Covid-19 and birth defects; however, data suggest that preterm birth and admission to a neonatal ICU are common among infants born to SARS-CoV-2–infected women. \n\n【8】In clinical contexts, advice regarding pregnancy avoidance has tended to rely on a doctrine of nondirective counseling, in which a clinician offers information about risks and approaches to minimizing them and supports patients in making informed decisions. In a few situations, however, clinicians may be more directive about pregnancy avoidance, such as when there are severe maternal risks associated with pregnancy (e.g., among women with Marfan’s syndrome with a dilated aortic root). In such cases, the issue of pregnancy avoidance has been addressed with individualized counseling rather than with public health recommendations.\n\n【9】The exercise of public authority in an area as deeply personal and private as the decision about whether and when to have a child requires strong justification, given the many ethical issues it raises. There are several potential areas of concern. The first relates to reproductive autonomy. Respect for autonomy is a guiding principle in medicine; reproductive autonomy has been given particular priority because of the importance that decisions about childbearing may have for a person’s self-determination, personal security, and life course. Past efforts to coerce people to avoid pregnancy (e.g., compulsory-sterilization policies) are widely considered to represent egregious ethics violations. Another concern is the potential for discrimination. Even objectively neutral policies can translate into differential experiences according to race, ethnic group, or social class. Public advice discouraging pregnancy may also inappropriately shift responsibility for pregnancy outcomes to parents and away from institutions that are ultimately responsible for mitigating harm and have the power to do so. A final concern pertains to the interests of people with disabilities. As the disability-rights community has argued, advice about avoidance of pregnancies that might result in offspring with disabilities can send a harmful message about what or who is valued. There is a morally important difference between preventing disease and preventing the birth of a child with a disability.\n\n【10】We believe that before public health agencies make a recommendation to avoid pregnancy during a public health emergency, several criteria should be met. First, the pregnancy-related risks associated with the emergency should be well understood. Early in the response to a new pathogen, these risks are often unknown. In some situations, such as in the event of a large-scale radiation release, however, risk would be high and potentially well understood.\n\n【11】Second, pregnancy-related risk should be high and well above the risk associated with other conditions or exposures that are fairly common among pregnant women. For example, the risk of congenital malformations is two to four times as high among babies born to women with pregestational diabetes as among babies born to women without diabetes. Diabetes is also associated with an increased risk of miscarriage, prematurity, growth restriction, perinatal mortality, and macrosomia, and pregnant women with diabetes are at risk for acute myocardial infarction, progression of retinopathy, nephropathy, and diabetic ketoacidosis.  Despite these risks, many women with diabetes choose to become pregnant.\n\n【12】A third criterion should be that pregnancy-related risks cannot be reasonably minimized or mitigated. During the 2009 H1N1 pandemic, women could mitigate their risk of harm by taking steps to avoid infection or, if they were exposed, by receiving antiviral prophylaxis or early treatment. Similarly, during the Zika outbreak, the risk of infection could be reduced by the woman and her partner avoiding mosquito bites and after conception by protecting against sexual transmission.\n\n【13】Fourth, effective contraception should be readily available. Women who want to avoid pregnancy should be able to do so. Finally, educational programming that carefully and effectively lays out the risks and benefits associated with becoming pregnant during the public health emergency as compared with waiting until it ends to conceive should be widely available. Programming should be accessible to people with various educational backgrounds, be available in languages spoken by affected people, and be culturally sensitive, and it should address the potential role of partners and others in risk mitigation.\n\n【14】Ultimately, given the ethical concerns raised by public health recommendations regarding pregnancy avoidance, strong justification for any such advice is needed. The criteria outlined above might be fulfilled during certain public health emergencies (e.g., a radiation emergency with continuing exposure), but we don’t believe that the risks associated with Covid-19 meet the bar. The pandemic further emphasizes the need to provide information and support to women related to their decisions to pursue or delay pregnancy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "858f3960-8356-4cb3-917e-f3ab9cde28af", "title": " Mutation and Atypical Femoral Fractures with Bisphosphonates", "text": "【0】 Mutation and Atypical Femoral Fractures with Bisphosphonates\nTo the Editor:\n--------------\n\n【1】Atypical femoral fractures have been associated with long-term bisphosphonate treatment.  However, the underlying mechanisms remain obscure. We studied three sisters who had atypical femoral fractures after receiving various oral bisphosphonates for 6 years. Two of the sisters had a single fracture (at the ages of 64 and 73 years), and one had bilateral fractures (one at the age of 60 years and the other at the age of 61 years). Given the low incidence of atypical femoral fractures in the general population (5.9 per 10,000 person-years),  we hypothesized that these sisters might have an underlying genetic background that contributed to these fractures.\n\n【2】Figure 1. High Degree of Conservation of the Relevant Amino Acid Sequence of GGPPS in Multiple Species.\n\n【3】Shown is the alignment of the region containing the D188 residue (arrow) in geranylgeranyl pyrophosphate synthase protein encoded by _GGPS1_ in different species. This mutation would be expected to severely impair enzyme activity by disrupting a Mg <sup>2+ </sup> binding site that is critical for binding of the farnesyl pyrophosphate substrate and for catalysis.  This mutation would be expected to disrupt the α-helix secondary structure (shown above the corresponding sequences). The numbers after each species indicate the first identified residue of the corresponding protein.\n\n【4】We performed whole-exome sequencing to detect possible shared genetic variants involved in their apparent increased risk. In addition, we performed whole-exome sequencing in three unrelated patients with atypical femoral fractures who each had received bisphosphonates for more than 5 years. We prioritized rare nonsynonymous mutations in the variant filtering, and only mutations that were shared among the three sisters were considered. No mutation was found to be homozygous or in any gene containing mutations in both chromosomes (compound heterozygous). Assuming that a dominant model was involved, we detected 37 rare mutations (in 34 genes), among them a novel p.Asp188Tyr substitution in the enzyme geranylgeranyl pyrophosphate synthase (GGPPS), which is a site of inhibition by bisphosphonates in the mevalonate pathway.  The variant that is located in the genomic position g.235505746G→T on chromosome 1 (GRCh37/hg19) in _GGPS1_ had the best conservation score and was not described in any of the available databases. This variant would be expected to severely impair the enzyme activity . Furthermore, the gene encoding cytochrome P-450 family 1 subfamily A member 1 ( _CYP1A1_ ), which is involved in steroid metabolism, was also mutated in all three sisters and in one of the unrelated patients, which suggests that it could be another potential susceptibility gene for bisphosphonate-related atypical femoral fractures. An additional mutation in the gene encoding mevalonate diphosphate decarboxylase ( _MVD_ ) was detected in one unrelated patient.\n\n【5】Pathway analysis of the mutated genes showed enrichment of the isoprenoid biosynthetic pathway (GO:0008299), which includes _GGPS1, CYP1A1,_ and _MVD_ (P<0.001). We speculate that other variants that have been identified might also be involved in susceptibility to bisphosphonate-related atypical femoral fractures. Such variants include missense changes in the gene encoding fibronectin 1 ( _FN1_ ) and in the genes encoding synapse defective Rho GTPase homolog 2 ( _SYDE2_ ) and neuronal guanine nucleotide exchange factor ( _NGEF_ ); the latter two proteins are regulators of small GTPases. We speculate that our results may support a model in which accumulation of susceptibility variants (including some in relevant genes, notably _GGPS1_ ) may lead to a possible genetic component of predisposition to atypical femoral fractures.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "72274597-fdbc-45f5-9f21-6e9c140b63af", "title": "Recovery of Herpes-Simplex Virus from Human Trigeminal Ganglions", "text": "【0】Recovery of Herpes-Simplex Virus from Human Trigeminal Ganglions\nAbstract\n--------\n\n【1】In six of seven patients, herpes-simplex virus was recovered from trigeminal ganglions obtained at autopsy from unselected cadavers less than 12 hours after death and maintained in culture for 10 to 45 days. The virus was not obtained from similarly handled cultures of trigeminal nerve or root from the same patients. The identity of the virus was confirmed by fluorescent-antibody and neutralization tests and by electron microscopy. The results indicate that herpes-simplex virus is present in a high proportion of human trigeminal ganglions and suggest this as a possible site of latent virus causing recurrent oral infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6ee8bdde-e9a1-4c80-9f37-5732c128793a", "title": "Surgical or Transcatheter Aortic-Valve Replacement in Intermediate-Risk Patients", "text": "【0】Surgical or Transcatheter Aortic-Valve Replacement in Intermediate-Risk Patients\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Although transcatheter aortic-valve replacement (TAVR) is an accepted alternative to surgery in patients with severe aortic stenosis who are at high surgical risk, less is known about comparative outcomes among patients with aortic stenosis who are at intermediate surgical risk.\n\n【3】Methods\n-------\n\n【4】We evaluated the clinical outcomes in intermediate-risk patients with severe, symptomatic aortic stenosis in a randomized trial comparing TAVR (performed with the use of a self-expanding prosthesis) with surgical aortic-valve replacement. The primary end point was a composite of death from any cause or disabling stroke at 24 months in patients undergoing attempted aortic-valve replacement. We used Bayesian analytical methods (with a margin of 0.07) to evaluate the noninferiority of TAVR as compared with surgical valve replacement.\n\n【5】Results\n-------\n\n【6】A total of 1746 patients underwent randomization at 87 centers. Of these patients, 1660 underwent an attempted TAVR or surgical procedure. The mean (±SD) age of the patients was 79.8±6.2 years, and all were at intermediate risk for surgery (Society of Thoracic Surgeons Predicted Risk of Mortality, 4.5±1.6%). At 24 months, the estimated incidence of the primary end point was 12.6% in the TAVR group and 14.0% in the surgery group (95% credible interval \\[Bayesian analysis\\] for difference, −5.2 to 2.3%; posterior probability of noninferiority, >0.999). Surgery was associated with higher rates of acute kidney injury, atrial fibrillation, and transfusion requirements, whereas TAVR had higher rates of residual aortic regurgitation and need for pacemaker implantation. TAVR resulted in lower mean gradients and larger aortic-valve areas than surgery. Structural valve deterioration at 24 months did not occur in either group.\n\n【7】Conclusions\n-----------\n\n【8】TAVR was a noninferior alternative to surgery in patients with severe aortic stenosis at intermediate surgical risk, with a different pattern of adverse events associated with each procedure. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nSURTAVI Study: TAVR versus Open Surgery  \n\n【11】Transcatheter aortic-valve replacement (TAVR) with the use of a self-expanding prosthesis is superior to medical therapy in patients with severe, symptomatic aortic stenosis in whom surgical aortic-valve replacement has been associated with prohibitive risk.  Among patients who are at high risk for standard surgery, TAVR may be the preferred option.  The adoption of TAVR in patients with aortic stenosis at high risk for surgery has been rapid, as shown by enrollment in the ongoing Society of Thoracic Surgeons–American College of Cardiology Transcatheter Valve Therapy Registry. \n\n【12】The comparative efficacy of TAVR and surgery has been less well studied among patients with aortic stenosis who are at lower surgical risk.  A randomized trial comparing balloon-expandable TAVR and surgery among intermediate-risk patients showed that TAVR was noninferior to surgery 2 years after randomization.  Given the higher rates of residual aortic-valve regurgitation and pacemaker use in TAVR patients,  and more frequent stroke, atrial fibrillation, acute kidney injury, and blood transfusions in surgical patients,  a randomized comparison of TAVR and surgery among intermediate-risk patients was warranted.\n\n【13】The purpose of the Surgical Replacement and Transcatheter Aortic Valve Implantation (SURTAVI) trial was to compare the safety and efficacy of TAVR performed with the use of a self-expanding bioprosthesis with surgical aortic-valve replacement in patients who were deemed to be at intermediate risk for surgery.\n\n【14】Methods\n-------\n\n【15】Trial Design\n------------\n\n【16】The SURTAVI trial was a multinational, randomized, noninferiority clinical trial designed to compare the safety and efficacy of TAVR and surgery in patients with symptomatic, severe aortic stenosis at intermediate surgical risk. Eligible patients were recruited at 87 centers and underwent randomization in a  ratio to undergo TAVR with the use of a self-expanding bioprosthesis or surgery . The trial was conducted in compliance with the International Conference on Harmonisation and the Declaration of Helsinki. It was approved by the local institutional review board or medical ethics committee at each center. All the patients provided written informed consent.\n\n【17】Medtronic funded the trial and developed the protocol  in collaboration with the executive committee. Medtronic representatives were responsible for site selection, data monitoring, and trial management. An independent academic clinical-events committee (Cardialysis) adjudicated all end points using standard definitions . Paradigm Biostatistics performed the Bayesian analysis for all end-point comparisons; an independent statistical consultant (Berry Consultants) validated the primary Bayesian end-point analysis. The data and safety monitoring board provided study oversight with periodic safety review and recommendations relating to trial design and conduct.\n\n【18】The first and third authors prepared all drafts of the manuscript, and all the authors made the decision to submit the manuscript for publication. No one who is not an author contributed to the writing of the manuscript. The authors attest that the trial was performed according to the protocol and vouch for the accuracy and completeness of the reported data.\n\n【19】Patient Selection\n-----------------\n\n【20】Eligible patients with symptomatic, severe aortic stenosis were determined by the local multidisciplinary heart team to be at intermediate surgical risk, which was defined as an estimated risk of 30-day surgical death of 3 to 15%, according to the criteria of the Society of Thoracic Surgeons Predicted Risk of Mortality (STS-PROM), as well as such nontraditional factors as coexisting illnesses, frailty, and disability. Severe aortic-valve stenosis was defined as an initial aortic-valve area of 1.0 cm  or less or an aortic-valve area index of less than 0.6 cm  per square meter of body-surface area and a mean gradient of more than 40 mm Hg or a maximum aortic velocity of more than 4 m per second at rest or with dobutamine provocation in patients with a left ventricular ejection fraction of less than 55% or a Doppler velocity index of less than 0.25 on resting echocardiography. A detailed list of inclusion and exclusion criteria is provided in Table S3 in the Supplementary Appendix . An international screening committee confirmed patient eligibility .\n\n【21】Trial Procedures\n----------------\n\n【22】The randomization of patients was stratified according to clinical site and the need for surgical coronary revascularization, as recommended by the multidisciplinary heart team. The choice and size of the surgical bioprosthesis were left to the discretion of the surgeon. Patients in the surgery group underwent coronary revascularization at the time of aortic-valve replacement if needed. After the procedure, a daily regimen of at least 81 mg of aspirin was prescribed indefinitely, including for patients who were receiving warfarin.\n\n【23】Among the patients in the TAVR group, the selection of the bioprosthesis size and access site were based on preprocedural computed tomography. The CoreValve bioprosthesis was used in 724 of 863 patients (84%); the Evolut R bioprosthesis was used in 139 (16%) . Transfemoral access was preferred; subclavian or direct aortic approaches were used in patients with unsuitable iliofemoral anatomy. The use of embolic protection during the TAVR procedure was not permitted. Percutaneous coronary intervention, when indicated, was performed either as a staged procedure before TAVR or at the time of TAVR as a concomitant procedure. Dual antiplatelet therapy with aspirin (at a dose of 81 to 100 mg) and clopidogrel (75 mg) was recommended for 3 months after the procedure; thereafter, the same dose of either aspirin or clopidogrel was recommended as indefinite monotherapy. Patients requiring warfarin or another anticoagulant were treated with antiplatelet monotherapy after the procedure.\n\n【24】Trial End Points\n----------------\n\n【25】The primary end point was a composite of death from any cause or disabling stroke at 24 months. (Trial end-point definitions are provided in Table S4 in the Supplementary Appendix .) Disabling stroke was defined according to the criteria of the Valve Academic Research Consortium-2 (VARC-2).  All the patients were seen by a trained neurologist or stroke specialist, and neurologic events were adjudicated by a neurologist on the clinical-events committee.\n\n【26】Prespecified analyses of death from any cause or disabling stroke at 12 months were completed for selected subgroups . Secondary end points included major adverse cardiovascular and cerebrovascular events, which consisted of death from any cause, myocardial infarction, all types of strokes, and any reintervention. Additional secondary safety and efficacy end points are described in the protocol and in the Methods section in the Supplementary Appendix .\n\n【27】An independent echocardiographic core laboratory at the Mayo Clinic provided serial echocardiographic assessments with the use of VARC-2 criteria,  which include aortic-valve hemodynamics and total aortic and paravalvular regurgitation through 24 months. Health-related quality-of-life assessments through 24 months were provided by the clinical sites with the use of the Kansas City cardiomyopathy questionnaire (KCCQ).  (KCCQ summary scores range from 0 to 100, with a correlation between scores of >60 and New York Heart Association class I or II and a 10-point increase corresponding to moderate clinical improvement.)\n\n【28】Statistical Analysis\n--------------------\n\n【29】The trial design called for the use of Bayesian statistical methods. We determined that TAVR would be declared noninferior to surgery for the primary outcome if the posterior probability of noninferiority with a margin of 0.07 was more than 0.971, as calculated by means of Bayesian analysis. The prespecified value of 0.971 was selected empirically through simulation to achieve a type I error at an alpha level of less than 0.05. A sample size of 1600 attempted aortic-valve procedures was chosen on the basis of an assumed 17% incidence of death from any cause or disabling stroke at 24 months among the patients undergoing surgery. A Bayesian interim analysis was prespecified when 1400 patients had reached the 12-month follow-up.\n\n【30】We evaluated the primary and secondary end points in a modified intention-to-treat population of patients who had undergone randomization and an attempted procedure. We imputed the outcome of patients without a known outcome at 24 months according to the prespecified statistical model, which was based on the patient’s last known status at the latest known time point: at the time of the procedure or at 1 month, 6 months, 12 months, or 18 months. A sensitivity analysis was performed to account for missing data, including the patients who were lost to follow-up or withdrew from the study. Secondary end points were tested with the use of a hierarchical testing procedure. Primary and secondary end points were also analyzed in the intention-to-treat population. Details with respect to the analysis populations, sensitivity analyses, and hierarchical testing methods are provided in the Methods section in the Supplementary Appendix .\n\n【31】We used a Bayesian analogue of a two-sample t-test to compare continuous variables as means (±SD) and a Bayesian version of a comparison of proportions to compare categorical variables as frequencies and percentages. Event rates are summarized as Bayesian posterior medians with 95% credible intervals, which were calculated from the 2.5th and 97.5th percentiles. We also performed Kaplan–Meier survival analyses.\n\n【32】Results\n-------\n\n【33】Baseline Characteristics\n------------------------\n\n【34】A total of 1746 patients underwent randomization at 87 centers in the United States, Europe, and Canada from June 19, 2012, to June 30, 2016 . The modified intention-to-treat population included 1660 patients (864 in the TAVR group and 796 in the surgery group). In this population, 2 patients in the TAVR group and 1 in the surgery group did not undergo implantation. In addition, TAVR was performed in 2 patients in the surgery group and surgery was performed in 1 patient in the TAVR group, which resulted in 863 patients who underwent the assigned procedure in the TAVR group and 794 who underwent the assigned procedure in the surgery group . Revascularization was recommended in 332 of 1660 patients (20%) in the two groups.\n\n【35】Table 1. Characteristics of the Patients at Baseline.\n\n【36】Baseline demographic and clinical characteristics of the patients are provided in Table 1 . The mean age was 79.8±6.2 years. All the patients were at intermediate risk for surgery (STS-PROM value, 4.5±1.6%), and most had coexisting illnesses, including diabetes (in 34.5%), chronic lung disease (in 34.5%), and frailty (5-meter gait speed of >6 seconds, 52.3%; falls within 6 months, 12.2%). A complete list of coexisting illnesses, including frailty and disability, is provided in Table S5 in the Supplementary Appendix .\n\n【37】A total of 71 patients in the intention-to-treat population who were assigned to the surgery group did not undergo the procedure . A comparison of these patients with the 796 patients who underwent surgery identified no differences in baseline demographic characteristics, surgical frailty, disability, or coexisting illnesses .\n\n【38】Procedural Outcomes\n-------------------\n\n【39】Table 2. Procedure-Related Complications at 30 Days (Modified Intention-to-Treat Population).\n\n【40】Early (≤30 day) acute kidney injury stage 2 or 3 and new or worsening atrial fibrillation occurred more often in the surgery group than in the TAVR group, whereas major vascular complications and the need for permanent pacemaker implantation occurred more often in the TAVR group . There were no significant differences in 24-month mortality among the patients who required a new pacemaker . Transfusions were more common in the surgery group than in the TAVR group, including an increase by a factor of 3.5 in the need for four or more red-cell units . Other outcomes were similar in the two groups. A detailed description of procedural outcomes is provided in the Supplementary Appendix .\n\n【41】Primary End Point\n-----------------\n\n【42】Table 3. Clinical Outcomes at 30 Days, 12 Months, and 24 Months (Modified Intention-to-Treat Population). Figure 1.  Figure 1. Noninferiority Analysis and Time-to-Event Curves for the Primary End Point.\n\n【43】In this Bayesian analysis, the posterior probability distribution for the difference in the primary end point (death from any cause or disabling stroke at 24 months) between patients who underwent transcatheter aortic-valve replacement (TAVR) and those who underwent surgical replacement confirmed that the noninferiority margin for TAVR was met . Also shown are time-to-event curves for the primary end point , death from any cause , and disabling stroke , findings that were similar in the two groups. In Panels B, C, and D, the insets show the same data on an enlarged y axis.\n\n【44】The primary Bayesian analysis was performed in 1660 patients in the modified intention-to-treat population when 1400 patients had reached 12 months of follow-up. The incidence of the primary end point at 24 months was 12.6% in the TAVR group and 14.0% in the surgery group (95% credible interval \\[Bayesian analysis\\] for difference, −5.2 to 2.3%; posterior probability of noninferiority, >0.999) . Similar results were found in the intention-to-treat population (13.2% in the TAVR group and 14.1% in the surgery group; 95% credible interval for difference, −4.7 to 2.7%; posterior probability of noninferiority, >0.999) . A sensitivity analysis that was performed to account for patients who were lost to follow-up showed no important difference in the primary conclusions (Results section in the Supplementary Appendix ). At 24 months, the rate of death from any cause was 11.4% in the TAVR group and 11.6% in the surgery group (95% credible interval for difference, −3.8 to 3.3%); the rate of disabling stroke was also similar in the two groups . No significant differences with respect to geographic region or trial site were found for the primary outcome. Prespecified subgroup analyses of death from any cause or disabling stroke at 12 months identified no significant differences in the treatment effect between TAVR and surgery .\n\n【45】Secondary End Points\n--------------------\n\n【46】Results of hierarchical analyses of the secondary end points are provided in Table S9 in the Supplementary Appendix . New York Heart Association symptoms improved significantly in the two groups from baseline, an improvement that persisted throughout the 24-month follow-up period . Quality of life, as measured by the KCCQ summary score, improved significantly in the two groups through 24 months of follow-up; the TAVR group had a higher proportion of patients with improvement at 1 month than did the surgery group .\n\n【47】Echocardiographic Findings\n--------------------------\n\n【48】Figure 2. Echocardiographic Findings.\n\n【49】The mean aortic-valve (AV) gradient was significantly lower (dashed lines) and the effective AV orifice area was significantly larger (solid lines) in the TAVR group than in the surgery group at all time points after the procedure.\n\n【50】Aortic-valve hemodynamics improved in both the TAVR group and the surgery group . The TAVR group had lower mean aortic-valve gradients and larger aortic-valve areas than did the surgery group. Moderate or severe residual paravalvular regurgitation was more common in the TAVR group at 1 year (5.3% in the TAVR group vs. 0.6% in the surgery group; 95% credible interval for difference, 2.8 to 6.8%) .\n\n【51】Discussion\n----------\n\n【52】In this trial, we found that TAVR was statistically noninferior to surgery in patients who were deemed to be at intermediate surgical risk by a multidisciplinary heart team. We found that the risk of death or disabling stroke at 24 months ranged from 12.6 to 14.0% among the patients in our trial. Surgery was associated with higher rates of acute kidney injury, atrial fibrillation, and transfusion requirements, whereas TAVR had higher rates of residual aortic regurgitation and need for pacemaker implantation. TAVR resulted in better aortic-valve hemodynamics than surgery, and neither TAVR nor surgery showed evidence of structural valve deterioration at 24 months.\n\n【53】A portfolio of randomized clinical trials compared TAVR with surgery in patients at varying surgical risk.  This expanding evidence base suggests that the highest mortality benefit for TAVR over surgery (or medical therapy) is seen in patients at high surgical risk.  Among the patients at high risk, those in the TAVR group had a lower rate of death than did those in the surgery group,  owing to the delayed recovery from surgery-related complications.  It is less certain that a mortality benefit of TAVR over surgery will be identified among patients at lower surgical risk. Although a comparison between randomized trials carries inherent risks, both our trial and the previously reported Placement of Aortic Transcatheter Valves (PARTNER) IIA trial achieved their noninferiority end points of death from any cause or disabling stroke in intermediate-risk populations. The mean STS-PROM value was higher in PARTNER IIA than in our trial (5.8% vs. 4.5%), as was the observed-to-expected 30-day surgical mortality ratio (0.71 vs. 0.38).  The observed-to-expected ratio in our trial was one of the lowest such ratios for surgical mortality that have been reported in randomized studies to date.  We attribute this result to the best practices of our cardiac surgical teams, which underscores the importance of the similar 30-day rates of death in the TAVR group and the surgery group (2.2% and 1.7%, respectively). The rates of death from any cause at 24 months were similar in the TAVR group and the surgery group (11.4% and 11.6%, respectively), which supports the similarity of the two techniques at the time of this midterm follow-up. This finding also suggests that the patients in our trial were at lower risk than those in the PARTNER IIA trial, which showed 24-month mortality of 16.7% with TAVR and 18.0% with surgery. \n\n【54】Surgical risk assessment in intermediate-risk patients is often problematic, even for an experienced multidisciplinary heart team. Conventional risk scores, such as the STS-PROM,  may be supplemented with other nontraditional surgical risk factors, such as coexisting conditions, frailty, and disability.  We defined our lower threshold for the heart-team assessment of 30-day surgical risk at 3%, and our results provide reassurance that TAVR is an alternative to surgery in patients at the lower boundaries of intermediate risk.\n\n【55】Neurologic complications associated with aortic-valve replacement are increasingly recognized as critical outcome measures in comparative trials. At 24 months, we found a numerically lower rate of disabling stroke in the TAVR group than in the surgery group, although the difference was not significant; these findings were similar to those in a previous randomized trial involving patients at increased risk for surgery.  We performed neurologic assessments before and after the procedures in the two groups, although detailed cognitive testing was not performed and embolic protection devices were not allowed during the procedure. Similar to the findings of the pivotal study involving patients at high surgical risk,  we found that the rates of acute kidney injury and atrial fibrillation were higher in the surgery group, whereas the rates of residual aortic regurgitation and permanent pacemaker implantation were higher in the TAVR group. Although we might have expected a lower rate of permanent pacemaker implantation with the introduction of the Evolut R valve on the basis of rates of 11.7% and 16.4% in previous studies,  the rates among patients who received the CoreValve (25.5%) and the Evolut R valve (26.7%) were similar. Whether this finding is related to the small number of Evolut R valves that were implanted late in the trial is unknown and will require further study.  The 24-month mortality among patients who required a new pacemaker was similar to that in the overall population.\n\n【56】Aortic-valve hemodynamics were substantially improved in both the TAVR group and the surgery group and probably contributed to the reduction in symptoms and improvement in health-related outcomes that we observed. We identified lower aortic-valve gradients and larger aortic-valve areas in patients treated with TAVR, findings that probably stemmed from the supraannular design of the self-expanding bioprosthesis. Long-term follow-up will be needed to determine the clinical effect of the improved hemodynamics in the TAVR group. Although we did not find evidence of structural valve deterioration at this midterm follow-up, more extended follow-up is needed.\n\n【57】Our study has several limitations. A relatively high frequency of unplanned withdrawals occurred in the surgery group, primarily because of the withdrawal of patient consent after randomization. We could not identify differences in baseline demographic characteristics among the patients who underwent the assigned surgery and those who did not. The next-generation Evolut R bioprosthesis was used in less than 20% of the patients. We also recognized that long-term follow-up is needed, since a 24-month end-point analysis provides incomplete information about the life cycle of TAVR as compared with surgical bioprostheses.\n\n【58】In conclusion, in a comparison between TAVR and surgical replacement in patients with symptomatic, severe aortic stenosis at intermediate risk for surgery, TAVR was a statistically noninferior alternative to surgery with respect to death from any cause or disabling stroke at 24 months. However, each procedure had a different pattern of adverse events.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "dcae4397-958e-449a-8f6d-e5446952829f", "title": "U.S. Outpatient Antibiotic Prescribing, 2010", "text": "【0】U.S. Outpatient Antibiotic Prescribing, 2010\nTo the Editor:\n--------------\n\n【1】Antibiotic use is an important factor in the spread of antibiotic resistance. It is estimated that 50% of antibiotic prescriptions may be unnecessary.  We analyzed data on the prescription of antibiotics to outpatients in the United States to identify the areas in which interventions addressing appropriate use could have the greatest effect.\n\n【2】Data on oral antibiotic prescriptions dispensed during 2010 in the United States were extracted from the IMS Health Xponent database, which represents a 100% projection of prescription activity on the basis of a sample of more than 70% of U.S. prescriptions. Antibiotics were categorized according to the IMS Health Uniform System of Classification. The numbers of prescriptions and census denominators were used to calculate prescribing rates. Prescriptions were totaled for 17 provider specialty groups on the basis of the self-designated specialty of the prescriber (as defined by the American Medical Association) associated with each prescription. The Xponent database provided the number of prescribers in each specialty for the calculation of the number of prescriptions per provider.\n\n【3】Figure 1. Antibiotic Prescriptions per 1000 Persons of All Ages According to State, 2010.\n\n【4】Health care providers prescribed 258.0 million courses of antibiotics in 2010, or 833 prescriptions per 1000 persons . Penicillins and macrolides were the most common categories prescribed. The most frequently prescribed antibiotic agent was azithromycin. Prescribing rates were higher among persons younger than 10 years of age and persons 65 years of age or older. Rates were also higher in the South (936 prescriptions per 1000 persons, as compared with the 639 prescriptions per 1000 persons in the West). Prescribing rates varied considerably according to provider specialty.\n\n【5】Our analyses were subject to limitations. We were unable to directly assess the appropriateness of prescribing because our data did not capture patient visits and diagnoses. We were also unable to determine how many unique patients received a prescription. Finally, prescribing data may not accurately represent actual antibiotic consumption, since patient adherence to treatment regimens varies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b96b13de-1c23-402b-8aaf-1ea5afbc9f73", "title": "Medical Progress: Focal Segmental Glomerulosclerosis", "text": "【0】Medical Progress: Focal Segmental Glomerulosclerosis\nFocal segmental glomerulosclerosis, which is characterized by progressive glomerular scarring, accounts for about 20% of cases of the nephrotic syndrome in children and 40% in adults. This review considers current approaches to diagnosis and management of the disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d14354b3-0645-4287-95a8-f63a53a27d6e", "title": "Health Care Reform and Equity for Undocumented Immigrants — When Crisis Meets Opportunity", "text": "【0】Health Care Reform and Equity for Undocumented Immigrants — When Crisis Meets Opportunity\nBasic, high-value medical care remains largely inaccessible to undocumented immigrants. Expansion of the current public health insurance system could narrow gaps.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9a758fec-1a0d-4046-90e1-863ba71c2a9d", "title": "A Randomized Trial of Psychosocial Support during High-Risk Pregnancies", "text": "【0】A Randomized Trial of Psychosocial Support during High-Risk Pregnancies\nAbstract\n--------\n\n【1】Background.\n-----------\n\n【2】It is often suggested that psychological and social support and health education for women at high risk for delivering a low-birth-weight infant can improve the outcomes of pregnancy, but the evidence is inconclusive. We undertook this prospective trial to evaluate a program of home visits designed to provide psychosocial support during pregnancy.\n\n【3】Methods.\n--------\n\n【4】At four centers in Latin America, 2235 women at higher-than-average risk for delivering a low-birth-weight infant were recruited before the 20th week of pregnancy. The women were randomly assigned either to an intervention group (n = 1115) that received four to six home visits from a nurse or social worker in addition to routine prenatal care or to a control group (n = 1120) that received only routine prenatal care (with a mean of eight prenatal visits). The principal measures of outcome were low birth weight (<2500 g), preterm delivery (<37 weeks of gestation), and specified categories of maternal and neonatal morbidity.\n\n【5】Results.\n--------\n\n【6】The women who received the home visits as well as routine prenatal care had outcomes that differed little from those of the women who received only routine care. The risks of low birth weight (odds ratio for the intervention group as compared with the control group, 0.93; 95 percent confidence interval, 0.68 to 1.28), preterm delivery (odds ratio, 0.88; 95 percent confidence interval, 0.67 to 1.16), and intrauterine growth retardation (odds ratio, 1.08; 95 percent confidence interval, 0.83 to 1.40) were similar in the two groups. There was no evidence that the intervention had any significant effect on the type of delivery, the length of hospital stay, perinatal mortality, or neonatal morbidity in the first 40 days. There was no protective effect of the psychosocial-support program even among the mothers at highest risk.\n\n【7】Conclusions.\n------------\n\n【8】Interventions designed to provide psychosocial support and health education during high-risk pregnancies are unlikely to improve maternal health or to reduce the incidence of low birth weight among infants. \n\n【9】Introduction\n------------\n\n【10】PRETERM delivery and intrauterine growth retardation remain major health problems in both developing and developed societies.    Although several epidemiologic studies in Latin America    and elsewhere  have consistently identified risk factors associated with low birth weight, programs aimed at controlling or preventing these risk factors have had only limited effects,    unless intensive and long-term interventions were undertaken  or the target populations were extremely underprivileged. \n\n【11】Most of the interventions that have been formally tested in randomized controlled trials have been medically oriented, and it has been suggested that too little consideration has been given to pregnant women's psychological and social well-being. These aspects of maternal health could be as important as biologic factors, it has been argued, particularly among women living in poverty who are at high risk for poor outcomes of pregnancy. Psychological stress and lack of social support, together with the adverse effects of high-risk medical conditions, could interact with an unhealthy lifestyle and a reduced access to adequate health care,    thereby adversely affecting the outcome of pregnancy.\n\n【12】In a large, randomized multicenter trial, we compared the outcomes of pregnancy among women at high risk who received standard prenatal care with those among women whose prenatal care was supplemented with four to six home visits designed to provide psychosocial support and health education.\n\n【13】Methods\n-------\n\n【14】The trial was conducted from January 1989 through March 1991 under the auspices of the Latin American Network for Perinatal and Reproductive Research    in its centers in the cities of Rosario, Argentina; Pelotas, Brazil; Havana, Cuba; and Mexico City. The research protocol was approved by the human studies committees of the four institutions.\n\n【15】Subjects\n--------\n\n【16】The principal criteria for entry into the trial were the presence of one or more of the following risk factors for delivering a low-birth-weight infant: previous delivery of a low-birth-weight or preterm infant; previous fetal or infant death; age <18 years; body weight ≤50 kg, height ≤1.5 m; low family income defined according to locally adapted cutoff points; less than three years of school; smoking or heavy alcohol consumption; and residence apart from the child's father.\n\n【17】Women who met these criteria were recruited if they started prenatal care during weeks 15 to 22 of a singleton pregnancy and had no history of major mental illness, cervical cerclage, or Rh isoimmunization. Randomization was carried out by the Data Coordinating Center in Pelotas; a computer-generated code was used for randomization within balanced blocks of 20 women, stratified according to center. Formal entry into the trial was delayed until a base-line interview had been completed and the woman had given informed consent. A sequence of sealed, opaque envelopes was used by a single investigator in each hospital to assign the women to treatment groups, and a particular woman's group assignment was known only to this investigator, the study supervisor, and the home visitor.\n\n【18】Treatment\n---------\n\n【19】The intervention was aimed primarily at increasing social support and reducing stress and anxiety. Four home visits were made during or near weeks 22, 26, 30, and 34 of gestation, with the option for two more visits at the discretion of the woman and the study staff.\n\n【20】Specially trained female social workers in Argentina, Brazil, and Mexico and obstetrical nurses in Cuba carried out the home visits. Before the study, training was provided in 26 sessions of four hours each; it included theoretical and practical instructions and the use of role playing that focused on how to conduct the visits and respond to possible personal or social situations. The home visitors were paid a full-time salary equivalent to that received by social workers or nurses in the public health sector.\n\n【21】Schedules for the visits were set in advance with the patient and her family or designated \"support person.\" Each visit was planned to last between one and two hours; the home visitor used a standard manual in conducting the visits. The manual and the proposed activities for the visits were based on information from ethnographic studies conducted in each site before the initiation of the trial. The manual included detailed descriptions of various situations that home visitors were likely to encounter, with suggested interventions. It was intended that home visitors at the different study sites would act in a similar way, with adaptations appropriate to the local culture.\n\n【22】The main objective of the visits was the strengthening of the pregnant woman's social network. A support person (husband or partner, mother, sister, friend, or neighbor) was selected by the patient to share all intervention activities beginning with the first visit. The support person was strongly encouraged to remain involved with the woman throughout the pregnancy, to participate in the decision-making process, to help the woman resolve personal problems, to promote healthful behavior, and to encourage attendance at visits for prenatal care. The home visitor provided direct emotional support to the woman and helped her to cope with problems related to medical recommendations or prenatal care.\n\n【23】The first part of each visit was devoted to encouraging the pregnant woman and her support person to discuss the pregnancy, especially any changes, worries, or doubts. Using this information as background, the home visitor adapted the themes defined in the study protocol, focusing the discussion on information relevant to each woman. The home visitor discussed her proposed strategy with the study supervisor after the first visit and designed an individual plan for each woman. Changes were made, if necessary, during subsequent visits.\n\n【24】To complement the provision of psychosocial support during the home visits, a special support office for women in the intervention group was established in each hospital. Women could visit without prior appointments or call on a telephone line reserved for such calls. The office was intended to help women with problems related to attendance at prenatal care appointments and compliance with treatment or laboratory tests, and to monitor adherence to doctors' recommendations. Women close to term had a guided tour of the hospital that focused on the labor and delivery unit and the emergency room.\n\n【25】Finally, health education was provided during the home visit; this included education about nutrition and suggestions about reducing smoking and alcohol or drug use. A carefully designed poster and booklet were given to each woman during the first home visit. Using familiar terms and situations, these materials gave advice and encouraged healthful behavior and good nutrition. No attempt was made to provide medical care during the home visits.\n\n【26】The control group was provided with the routine prenatal care available at each of the participating institutions. Physicians and nurses in the prenatal clinics were not selected for the study, and the women were assigned for care according to the standard hospital procedures. The clinic personnel were never told whether women were in the trial, although some may have identified themselves as participants.\n\n【27】Evaluation of the Intervention\n------------------------------\n\n【28】To evaluate whether the planned material was covered during the home visits, a precoded form was completed by the home visitor at the end of each visit and reviewed by the project coordinator. Five percent of all home visits, selected randomly, were observed by the project coordinator, without previous notification, and a second set of forms identical to those used by the home visitor was independently completed by the project coordinator. Similarly, a 5 percent random sample of the postpartum questionnaires was also independently repeated. Results of comparisons between the two sets of data were used to identify social workers and interviewers to undergo retraining.\n\n【29】Data for the evaluation of procedures and treatment effects were collected by a team of independent professional interviewers who were not informed about the nature of the study. The interviews were carried out for the intervention and control groups at home during the 36th week of gestation, in the hospital immediately after delivery, and 40 days post partum at home. At the 36-week interview, information was obtained on social support, knowledge about pregnancy and delivery and health-related behavior, as well as levels of psychological distress. After delivery, data were collected from medical records on prenatal care, labor and delivery, and the health status of the newborn. During the 40-day postpartum visit information was collected on maternal and infant morbidity and feeding practices.\n\n【30】Base-line information on several indicators of psychological distress and social support was used to construct two summary variables. The psychological-distress variable was a weighted normalized score, including the results of the Spielberger anxiety test,  reported feelings toward the pregnancy, expectations about delivery, and fears about being a bad mother. The social-support variable was obtained from a factor analysis, including information on whether the father was present, his attitude, and any help he provided, support provided at home by others, and the woman's degree of satisfaction with this help. Women were classified as having a low level of distress or social support if their scores were at or below the median value for these variables in the total study sample, and as having a high level of distress or social support if their scores were above the corresponding median.\n\n【31】Statistical Analysis\n--------------------\n\n【32】It was calculated before the study began that a sample size of 1200 women in each group would provide 90 percent power (α = 0.05) of identifying a reduction in the frequency of low birth weight, from 20 percent in the control group to 15 percent in the intervention group, allowing for attrition of 20 percent. However, the actual rate of low birth weight in the control group was 9.4 percent. The actual sample size of 2235 gave the study 80 percent power (α = 0.05) of detecting a reduction from this rate to 6 percent in the intervention group. The data were analyzed on an intention-to-treat basis according to the group to which the women were randomly assigned.\n\n【33】We compared the two groups using the chi-square test or z-test. Crude analyses and analyses adjusted for potential confounding variables were carried out when appropriate. Logistic regression was used to calculate odds ratios and 95 percent confidence intervals for dichotomous outcome variables, and linear regression for estimating differences in continuous outcome variables.\n\n【34】Stratified analyses were conducted to compare the results among previously designated subgroups in the intervention and control groups, specifically women with high levels of psychological distress and low levels of social support at base line, those who smoked, nulliparous women, women with a history of adverse pregnancy outcome, and those less than 18 years of age.\n\n【35】This report focuses on the effect of the intervention on the incidence of low birth weight (<2500 g), preterm delivery (<37 weeks), intrauterine growth retardation (<10th percentile for a reference population  ), intrapartum interventions, and neonatal morbidity and mortality. We have not included any effects of the intervention on the women's psychological well-being in this report.\n\n【36】Results\n-------\n\n【37】A total of 5990 women were screened for the risk factors required for trial entry, and 2235 (37 percent) were considered eligible and agreed to participate in the study. Of these women, 1115 were randomly assigned to the intervention group and 1120 to the control group.\n\n【38】Table 1. Distribution of Risk Factors at Entry, According to Treatment Group. Table 2.  Table 2. Base-Line Characteristics of the Women, According to Treatment Group.\n\n【39】The distribution of risk factors at entry was similar in the two groups . The mean (±SD) gestational age at randomization was 18.3±2.3 weeks for both groups. Overall, the two groups had similar demographic, obstetrical, and psychological characteristics at base line . Despite the good overall comparability of the intervention and control groups, there were differences in some variables when the two groups were considered on a country-by-country basis. Those variables and country of residence were treated as potential confounding variables in the adjusted analyses.\n\n【40】Most (83 percent) of the women randomly assigned to the intervention group received the planned number of home visits, and 90 percent were visited at least once. The various planned components of the home visit, including health education, discussions of specific topics, and encouragement of the use of health services, were included in more than 80 percent of the visits. The support person was present at 66 percent of the first visits, 57 percent of the second visits, 51 percent of the third visits, and 52 percent of the fourth visits. The groups had similar numbers of visits to the medical center for prenatal care (8.1 ±3.7 in the control group vs. 8.0±3.8 in the intervention group). By the 37th week of gestation, the women in the intervention group had significantly greater knowledge of the signs and symptoms of pregnancy complications and more favorable expectations about delivery. The proportion of smokers in the intervention group declined from 24 percent at entry to 19 percent at 37 weeks. The comparable proportions were 22 percent and 20 percent, respectively, in the control group.\n\n【41】Low Birth Weight, Preterm Delivery, and Intrauterine Growth Retardation\n-----------------------------------------------------------------------\n\n【42】Table 3. Effect of Psychosocial Support during Pregnancy on Primary Outcome Measures.\n\n【43】No differences were observed between the groups in any of the principal measures of outcome . The rate of low birth weight was 8.7 percent in the intervention group and 9.4 percent in the control group (odds ratio, 0.93; 95 percent confidence interval, 0.68 to 1.28), and the rate of preterm delivery was 11.1 percent and 12.5 percent (odds ratio, 0.88; 95 percent confidence interval, 0.67 to 1.16). The rate of intrauterine growth retardation was 14.3 percent in the intervention group and 13.3 percent in the control group (odds ratio, 1.08; 95 percent confidence interval, 0.83 to 1.40). Adjustments for smoking, maternal age, previous cesarean section, obstetrical history, parity, interval since the birth of the previous child, alcohol consumption, and study site did not alter these results.\n\n【44】The infants born to the women in the intervention group had a crude mean birth weight that was 19 g heavier (95 percent confidence interval, -29.3 to 67.3 g) than that of the infants of the women in the control group. After adjustment, the infants in the intervention group were 20.2 g heavier and 0.27 cm taller, and they had a mean gestational age that was 0.24 week longer; these differences were not statistically significant.\n\n【45】Table 4. Effect of Psychosocial Support during Pregnancy on Birth Weight and Gestational Age. Table 5.  Table 5. Effect of Psychosocial Support during Pregnancy on Birth Weight and Gestational Age, According to the Mother's Level of Psychosocial Support and Psychological Distress at Entry.\n\n【46】Analyses after stratification according to the preselected variables did not identify any statistically significant differences between the intervention and control groups in any of the principal outcome variables . Women with high levels of psychological distress, low levels of social support, or both also received no apparent benefit from the intervention in terms of the frequency of low birth weight, preterm delivery, or intrauterine growth retardation . An exploratory post hoc analysis included only women from each group who received fewer than four prenatal visits. There were still no significant differences between the groups or patterns in the rate of low birth weight, prematurity, or intrauterine growth retardation. The sample sizes were very small, however (43 women in the intervention group and 46 women in the control group).\n\n【47】Maternal and Neonatal Morbidity\n-------------------------------\n\n【48】The proportion of women admitted to the hospital for reasons other than delivery and their diagnoses were similar in the two groups, as were the rates of overall morbidity. The duration of postpartum hospitalization was 2.8±2.4 days in the intervention group and 2.8±2.5 days in the control group. Among the women who had vaginal deliveries, the proportion hospitalized for more than four days because of infection was 8.2 percent in the intervention group and 6.4 percent in the control group. The frequency of maternal complications and the use of health services at two and six weeks post partum were not significantly different in the two groups.\n\n【49】Evaluation of the infants at 40 days post partum demonstrated similar rates of respiratory infection, gastrointestinal diseases, nutritional complications, diarrhea, and dehydration in both groups. The overall frequency of hospital admission or visits to the hospital emergency department was 3.2 percent for the 953 infants born to women in the intervention group for whom follow-up data were available and 4.5 percent for the 949 comparable infants born to women in the control group (odds ratio, 0.68; 95 percent confidence interval, 0.41 to 1.13).\n\n【50】Discussion\n----------\n\n【51】In this large, randomized multicenter trial, intense formal psychosocial support offered through home visits to socially disadvantaged pregnant women did not improve the biologic outcomes of the pregnancies. The women had risk factors for the delivery of a low-birth-weight infant, high base-line anxiety scores, and low levels of social support — all presumably making them more responsive to this type of intervention.\n\n【52】The study was reasonably successful in carrying out the intervention as planned, and the intensity of the intervention was probably as high as is possible for programs conducted by public prenatal care services in most developing and developed countries. Furthermore, the intervention also attempted to increase the women's knowledge and use of health services, and efforts were made to make it easier for them to carry out the recommendations of health care workers. The intensity of the social-support intervention, the presence of a support person, and the strong elements of education and encouragement and facilitation of the use of health care resources made this intervention different from those described in previous reports.    It is evident, however, that the intervention was not sufficient to overcome a lifetime of disadvantage and poor health.\n\n【53】Contamination of the control group by the education component of the intervention is unlikely. There were few opportunities for the women to contact each other because of the large number of patients served by these hospitals. Similarly, the hospital residents and staff rotated frequently, making it unlikely that they deduced which women were in the control group and provided them with additional care or psychosocial support.\n\n【54】Neither the comparisons between the groups after stratification according to the main predictive base-line variables nor the adjusted analyses changed our results in any significant way. Moreover, when we considered only women with high base-line levels of psychological distress, low levels of social support, or both — the group most likely to benefit from the intervention — the intervention still had no detectable effect.\n\n【55】We also calculated the magnitude of the effect of the social-support intervention, including data from a previously reported review,  a recent report,  and the results of this study and using a meta-analysis technique.  There were 10 studies that provided data on preterm delivery and 11 that included low birth weight and type of delivery as outcome variables. These analyses yielded odds ratios of 0.93 (95 percent confidence interval, 0.82 to 1.05) for preterm delivery, 0.93 (95 percent confidence interval, 0.81 to 1.07) for low birth weight, and 0.96 (95 percent confidence interval, 0.86 to 1.07) for forceps delivery or cesarean section. Finally, eight studies with information on intrapartum use of anesthesia had a typical odds ratio of 0.91 (95 percent confidence interval, 0.81 to 1.03).\n\n【56】These findings therefore confirm that psychosocial support offered to pregnant women in various formats and settings does not appear to improve their neonates' birth weight or gestational age at delivery.        Conversely, interventions involving the provision of supportive companions for the mother during labor have been shown to affect perinatal outcomes    and could have a role in the care of pregnant women in specific short-term stressful situations.\n\n【57】Our intervention included a strong element of health and nutrition education, including culturally adapted, easy-to-follow graphic material. Information and advice were systematically presented during all home visits and focused on topics about which each woman needed the most information. Unfortunately, this component was not effective or, at least, was not sufficiently strong to overcome the negative effect of the preexisting risk factors.\n\n【58】Health education has been consistently recommended as an integral component of health services, particularly for disadvantaged populations and in developing countries. It is assumed that poor people fail to act or engage in risky behavior because they lack knowledge. However, a recent methodologic review found only very limited support for this recommendation in the literature, with only 3 of the 67 studies reviewed containing the four methodologic attributes required; only 21 percent were randomized studies with more than 60 cases or two clusters.   \n\n【59】Thus, before any special effort is made to incorporate major health-education components into prenatal care, there should be strong evidence of their benefits. If such evidence were a prerequisite, some elements of current prenatal care might not have been incorporated into clinical practice.\n\n【60】It could be argued that limiting entry into our study to women who sought prenatal care before 22 weeks of gestation and who had an adequate number of prenatal visits may have excluded women with the greatest need for this type of intervention. Our study was designed to evaluate the effect of psychosocial support in supplementing prenatal medical care. Women had to be contacted early and had to continue in the program for the intervention to be carried out as planned. It is possible that similar intervention targeted to women who received prenatal care only late in pregnancy, although necessarily less intensive, could be more effective because of these women's greater need. This possibility could be evaluated in future trials.\n\n【61】Although our intervention provided the opportunity for close interaction between the pregnant woman and the home visitor, no specific psychological treatment was provided, and no attempt was made to resolve major family or social problems. A more individualized program that provides, for example, long-term and intense psychological services, may be needed for a selected group of women. Long-term interventions appear to be necessary in other programs as well, such as nutritional supplementation during pregnancy. \n\n【62】On the basis of data on large numbers of women studied in randomized trials, we conclude that there is no evidence that supplementing prenatal care with formal psychosocial support prevents preterm delivery and intrauterine growth retardation. Any new programs must be demonstrated to be effective before they can be recommended as part of routine prenatal care, even if the theory behind such interventions appears to be logically sound. In the meantime, clinics and hospitals should concentrate on consistently providing the components of prenatal care that have been proved to be effective.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e0b10455-d6fc-497c-a8bc-122e416c6410", "title": "Diet and 20-Year Mortality from Coronary Heart Disease — The Ireland–Boston Diet–Heart Study", "text": "【0】Diet and 20-Year Mortality from Coronary Heart Disease — The Ireland–Boston Diet–Heart Study\nAbstract\n--------\n\n【1】In a prospective epidemiologic study of 1001 middle-aged men, we examined the relation between dietary information collected approximately 20 years ago and subsequent mortality from coronary heart disease. The men were initially enrolled in three cohorts: one of men born and living in Ireland, another of those born in Ireland who had emigrated to Boston, and the third of those born in the Boston area of Irish immigrants. There were no differences in mortality from coronary heart disease among the three cohorts.\n\n【2】In within-population analyses, those who died of coronary heart disease had higher Keys (P = 0.06) and modified Hegsted (P = 0.02) dietary scores than did those who did not (a high score indicates a high intake of saturated fatty acids and cholesterol and a relatively low intake of polyunsaturated fatty acids). These associations were significant (P = 0.03 for the Keys and P = 0.04 for the modified Hegsted scores) after adjustment for other risk factors for coronary heart disease. Fiber intake (P = 0.04) and a vegetable-foods score, which rose with increased intake of fiber, vegetable protein, and starch (P = 0.02), were lower among those who died from coronary heart disease, though not significantly so after adjustment for other risk factors. A higher Keys score carried an increased risk of coronary heart disease (relative risk, 1.60), and a higher fiber intake carried a decreased risk (relative risk, 0.57).\n\n【3】Overall, these results tend to support the hypothesis that diet is related, albeit weakly, to the development of coronary heart disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "fe2afe77-a570-4d2e-9956-9300cb805834", "title": "Evidence of SARS-CoV-2 Infection in Returning Travelers from Wuhan, China", "text": "【0】Evidence of SARS-CoV-2 Infection in Returning Travelers from Wuhan, China\nTo the Editor:\n--------------\n\n【1】As the number of cases of infection with the novel coronavirus (SARS-CoV-2) has continued to increase, many countries have established restrictions regarding travelers who have recently visited China.  With lockdown measures imposed in Hubei Province, China,  and a public health emergency of international concern declared by the World Health Organization,  foreign nationals have sought to return to their home countries from China, and public health authorities are racing to contain the spread of Covid-19 (the disease caused by SARS-CoV-2 infection) around the world. This process is complicated by epidemiologic uncertainty regarding possible transmission of the virus by asymptomatically or subclinically symptomatic infected persons. It is unclear whether persons who show no signs or symptoms of respiratory infection shed SARS-CoV-2.\n\n【2】Figure 1. Evaluation and Testing of Passengers.\n\n【3】Passengers were evaluated to detect infection with the novel coronavirus (SARS-CoV-2) after their flight from Wuhan, China, to Frankfurt, Germany. All tests were targeted to detect SARS-CoV-2.\n\n【4】In this context, a group of predominantly German nationals who had stayed in Hubei Province was evacuated to Frankfurt, Germany, on February 1, 2020. They were to be transferred to Germersheim, Germany, and quarantined for 14 days, since this period is thought to be the upper limit of the incubation period of SARS-CoV-2. Screening for symptoms and clinical signs of infection was performed before their departure from China. A total of 126 travelers were allowed to board an aircraft operated by the German air force .\n\n【5】During the flight, 10 passengers were isolated. Two passengers had had contact with 1 person who had a confirmed case of SARS-CoV-2 infection, 6 had reported symptoms, were deemed to be clinically symptomatic, or both, and 2 passengers had accompanied family members who had been isolated on the flight because of suspected SARS-CoV-2 infection or because of other symptoms (i.e., symptoms related to pregnancy). These 10 passengers were transferred to University Hospital Frankfurt immediately after arrival. All 10 tested negative for SARS-CoV-2 by real-time reverse-transcription–polymerase-chain-reaction (RT-PCR) assays  of throat swabs and sputum.\n\n【6】The remaining 116 passengers (5 months to 68 years of age), including 23 children, were sent to the medical assessment center at Frankfurt Airport, where each was evaluated by a medical team of physicians. Each passenger was asked to report current symptoms of fever, fatigue, sore throat, cough, runny nose, muscle aches, and diarrhea, and each one was screened for signs of infection in the nose and throat. The temperature of all passengers was taken. All were afebrile except for 1 passenger who had a temperature of 38.4°C and reported dyspnea and cough. He was transferred to University Hospital Frankfurt for evaluation. However, testing to detect SARS-CoV-2 by RT-PCR of a throat swab and sputum was negative.\n\n【7】In addition to the preplanned multistep process of screening for signs and symptoms of infection and observing the asymptomatic cohort in quarantine, we decided to offer a throat swab to test for SARS-CoV-2 in each of the 115 travelers who had passed triage. A total of 114 passengers consented to the test.\n\n【8】Two of the 114 persons (1.8%) in this cohort of travelers who had passed the symptoms-based screening tested positive for SARS-CoV-2 by RT-PCR (cycle threshold value in the two samples, 24.39 and 30.25, respectively). Testing with a second protocol consisting of two commercial sets (LightMix Modular SARS and Wuhan CoV E-gene, and LightMix Modular Wuhan CoV RdRP-gene, both produced by TIB MOLBIOL) and retesting of the positive samples at the Institute of Virology, Philipps University Marburg, in Marburg, Germany, confirmed the results. In addition, the isolation of SARS-CoV-2 from both samples in cell culture of Caco-2 cells indicated potential infectivity .\n\n【9】These two persons were subsequently isolated from the cohort and transferred to the Infectious Disease Unit at University Hospital Frankfurt for further evaluation and observation on the following day. After a thorough evaluation in the hospital ward, a faint rash and minimal pharyngitis were observed in one patient. Both patients remained well and afebrile 7 days after admission.\n\n【10】In this effort to evacuate 126 people from Wuhan to Frankfurt, a symptom-based screening process was ineffective in detecting SARS-CoV-2 infection in 2 persons who later were found to have evidence of SARS-CoV-2 in a throat swab. We discovered that shedding of potentially infectious virus may occur in persons who have no fever and no signs or only minor signs of infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "610b4031-1b4a-4e2d-b064-2d7910076191", "title": "Trastuzumab Emtansine for HER2-Positive Advanced Breast Cancer", "text": "【0】Trastuzumab Emtansine for HER2-Positive Advanced Breast Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Trastuzumab emtansine (T-DM1) is an antibody–drug conjugate incorporating the human epidermal growth factor receptor 2 (HER2)–targeted antitumor properties of trastuzumab with the cytotoxic activity of the microtubule-inhibitory agent DM1. The antibody and the cytotoxic agent are conjugated by means of a stable linker.\n\n【3】Methods\n-------\n\n【4】We randomly assigned patients with HER2-positive advanced breast cancer, who had previously been treated with trastuzumab and a taxane, to T-DM1 or lapatinib plus capecitabine. The primary end points were progression-free survival (as assessed by independent review), overall survival, and safety. Secondary end points included progression-free survival (investigator-assessed), the objective response rate, and the time to symptom progression. Two interim analyses of overall survival were conducted.\n\n【5】Results\n-------\n\n【6】Among 991 randomly assigned patients, median progression-free survival as assessed by independent review was 9.6 months with T-DM1 versus 6.4 months with lapatinib plus capecitabine (hazard ratio for progression or death from any cause, 0.65; 95% confidence interval \\[CI\\], 0.55 to 0.77; P<0.001), and median overall survival at the second interim analysis crossed the stopping boundary for efficacy (30.9 months vs. 25.1 months; hazard ratio for death from any cause, 0.68; 95% CI, 0.55 to 0.85; P<0.001). The objective response rate was higher with T-DM1 (43.6%, vs. 30.8% with lapatinib plus capecitabine; P<0.001); results for all additional secondary end points favored T-DM1. Rates of adverse events of grade 3 or above were higher with lapatinib plus capecitabine than with T-DM1 (57% vs. 41%). The incidences of thrombocytopenia and increased serum aminotransferase levels were higher with T-DM1, whereas the incidences of diarrhea, nausea, vomiting, and palmar–plantar erythrodysesthesia were higher with lapatinib plus capecitabine.\n\n【7】Conclusions\n-----------\n\n【8】T-DM1 significantly prolonged progression-free and overall survival with less toxicity than lapatinib plus capecitabine in patients with HER2-positive advanced breast cancer previously treated with trastuzumab and a taxane. \n\n【9】Introduction\n------------\n\n【10】Amplification of human epidermal growth factor receptor 2 (HER2, also called ErbB2) occurs in approximately 20% of breast cancers and is associated with shortened survival.  Combining HER2-targeted agents with standard chemotherapy is an effective therapeutic approach for patients with HER2-positive metastatic breast cancer. When combined with first-line chemotherapy, trastuzumab increases the time to progression and overall survival among patients with metastatic disease.  The addition of lapatinib to capecitabine increases the time to progression in patients previously treated with trastuzumab, an anthracycline, and a taxane,  and this combination is a standard option for disease progression with trastuzumab.\n\n【11】Trastuzumab emtansine (T-DM1) is an antibody–drug conjugate that incorporates the HER2-targeted antitumor properties of trastuzumab with the cytotoxic activity of the microtubule-inhibitory agent DM1 (derivative of maytansine); the antibody and the cytotoxic agent are conjugated by means of a stable linker.  T-DM1 allows intracellular drug delivery specifically to HER2-overexpressing cells, thereby improving the therapeutic index and minimizing exposure of normal tissue. Phase 2 studies have shown the clinical activity of T-DM1 in patients with HER2-positive advanced breast cancer. \n\n【12】The EMILIA study, a phase 3 trial, assessed the efficacy and safety of T-DM1, as compared with lapatinib plus capecitabine, in patients with HER2-positive advanced breast cancer previously treated with trastuzumab and a taxane.\n\n【13】Methods\n-------\n\n【14】Study Design\n------------\n\n【15】The EMILIA study is a randomized, open-label, international trial involving patients with HER2-positive, unresectable, locally advanced or metastatic breast cancer who were previously treated with trastuzumab and a taxane. The study was conducted in accordance with the International Conference on Harmonization Good Clinical Practice standards and the Declaration of Helsinki. Patients provided written informed consent; the study was approved by the relevant institutional review board or independent ethics committee.\n\n【16】Patients were randomly assigned in a  ratio to T-DM1 or lapatinib plus capecitabine with the use of a hierarchical, dynamic randomization scheme through an interactive voice-response system. Stratification factors were world region (United States, Western Europe, or other), the number of prior chemotherapy regimens for unresectable, locally advanced or metastatic disease (0 or 1 vs. >1), and disease involvement (visceral vs. nonvisceral).\n\n【17】The primary end points were progression-free survival assessed by independent review, overall survival, and safety. Progression-free survival was defined as the time from randomization to progression or death from any cause. Overall survival was defined as the time from randomization to death from any cause. Prespecified secondary end points included progression-free survival (investigator-assessed), the objective response rate, the duration of response, and the time to symptom progression. The objective response rate was determined according to modified RECIST on the basis of an independent review of patients with measurable disease at baseline; responses were confirmed at least 28 days after the initial documentation of a response. The time to symptom progression was defined as the time from randomization to the first decrease of 5 points or more from baseline scores on the Trial Outcome Index of the patient-reported Functional Assessment of Cancer Therapy–Breast (FACT-B TOI, on which scores range from 0 to 92, with higher scores indicating a better quality of life)  in women with a baseline score and at least one postbaseline score. Safety was monitored by an independent data monitoring committee and a cardiac review committee.\n\n【18】Study Oversight\n---------------\n\n【19】The study was designed by the academic investigators, the trial steering committee, and representatives of the sponsor, F. Hoffmann–La Roche/Genentech. The first author prepared the initial draft of the manuscript with support from a medical writer who was paid by Genentech. All the authors contributed to subsequent drafts and made the decision to submit the manuscript for publication.\n\n【20】Patients\n--------\n\n【21】Eligible patients had documented progression of unresectable, locally advanced or metastatic HER2-positive breast cancer previously treated with a taxane and trastuzumab. Inclusion criteria were progression during or after the most recent treatment for locally advanced or metastatic disease or within 6 months after treatment for early-stage disease, and a centrally confirmed HER2-positive status, assessed by means of immunohistochemical analysis (with 3+ indicating positive status), fluorescence in situ hybridization (with an amplification ratio ≥2.0 indicating positive status), or both. Patients with measurable disease (according to modified RECIST) and those with nonmeasurable disease were included. Other eligibility criteria were a left ventricular ejection fraction of 50% or more (determined by echocardiography or multiple-gated acquisition \\[MUGA\\] scanning) and an Eastern Cooperative Oncology Group performance status of 0 (asymptomatic) or 1 (restricted in strenuous activity but ambulatory and able to do light work).\n\n【22】Major exclusion criteria were prior treatment with T-DM1, lapatinib, or capecitabine; peripheral neuropathy of grade 3 or higher   ; symptomatic central nervous system (CNS) metastases or treatment for these metastases within 2 months before randomization; a history of symptomatic congestive heart failure or serious cardiac arrhythmia requiring treatment; and a history of myocardial infarction or unstable angina within 6 months before randomization.\n\n【23】Procedures\n----------\n\n【24】Patients in the control group self-administered oral lapatinib at a dose of 1250 mg daily plus oral capecitabine at a dose of 1000 mg per square meter of body-surface area every 12 hours (maximum planned daily dose, 2000 mg per square meter) on days 1 through 14 of each 21-day treatment cycle and recorded their doses in a patient diary. Dose delays, reductions, and discontinuations owing to toxic effects were defined in the protocol. For capecitabine, the first dose reduction was to 75% of the total daily dose, and the second to 50% of that dose . For lapatinib, the first dose reduction was to 1000 mg daily, and the second to 750 mg daily. Patients could continue to take lapatinib if capecitabine was discontinued and vice versa. If treatment with both drugs was delayed for more than 42 consecutive days, the drugs were discontinued.\n\n【25】Patients randomly assigned to T-DM1 received 3.6 mg per kilogram of body weight intravenously every 21 days. Dose delays, reductions, and discontinuations owing to toxic effects were defined in the protocol. The first dose reduction was to 3.0 mg per kilogram and the second to 2.4 mg per kilogram . Dose escalation was not allowed after a dose reduction. If a toxic event did not resolve to a grade 1 level or to baseline status within 42 days after the most recent dose, the study treatment was discontinued. Patients continued to receive the study treatment until disease progression (investigator-assessed) or the development of unmanageable toxic effects.\n\n【26】Assessments\n-----------\n\n【27】Tumor assessments were performed by the study investigators and by the independent review committee at baseline and every 6 weeks thereafter until investigator-assessed disease progression; an additional assessment was required 6 weeks after progression. The left ventricular ejection fraction was measured by means of echocardiography (the preferred method) or MUGA scanning at baseline, week 6, week 12, and every 12 weeks thereafter until discontinuation of the study treatment; an additional assessment was performed 30 days after the last dose of the study drug. Local laboratory assessments were performed at baseline, on day 1 of each treatment cycle, on days 8 and 15 of cycles 1 through 4, and 30 days after the last dose of the study drug. Adverse events were monitored continuously and graded according to the CTCAE, version 3.0.\n\n【28】Statistical Analysis\n--------------------\n\n【29】The trial was originally designed with progression-free survival, as assessed by independent review, as the primary efficacy end point, with a planned sample of 580 patients. In October 2010, with all data still masked to the investigators, the protocol was amended to add overall survival as a coprimary efficacy end point, with an increase in the planned sample to 980 patients. The trial had 90% power to detect a hazard ratio of 0.75 for progression or death from any cause with T-DM1 as compared with lapatinib plus capecitabine and 80% power to detect a hazard ratio of 0.80 for death from any cause, with a two-sided alpha level of 0.05.\n\n【30】The primary analysis of progression-free survival was to be performed after 508 independently assessed events, and the final analysis of overall survival after 632 deaths. The first interim analysis of overall survival was to be performed at the time of the primary analysis of progression-free survival. A second interim analysis was added to the statistical analysis plan after the completion of the first interim analysis and was conducted when 50% of the targeted events had occurred. Stopping boundaries for efficacy were determined by means of the Lan–DeMets alpha-spending function with an O'Brien–Fleming boundary and the actual number of observed deaths. To adjust for multiple comparisons, a fixed-sequence hypothesis-testing procedure was implemented. The hypothesis test for progression-free survival was conducted at a two-sided alpha level of 0.05. If the result was statistically significant, overall survival was to be tested at a two-sided alpha level of 0.05, which was spent at the interim and final analyses according to the Lan–DeMets spending function. If both primary end points were statistically significant, the secondary end points were to be tested in a prespecified order. The statistical analysis plan is included in the protocol.\n\n【31】The primary end points were assessed in the intention-to-treat population and tested by means of two-sided log-rank tests, with stratification according to the factors used for randomization. A sensitivity analysis was performed for progression-free survival among patients who received nonprotocol breast-cancer treatment before documented disease progression, with censoring of data at the last tumor assessment before the initiation of such therapy. Kaplan–Meier methods were used to estimate medians for the primary end points, 1- and 2-year survival rates, and corresponding 95% confidence intervals. Analyses of progression-free survival in 16 prespecified subgroups were performed. Ten post hoc analyses to assess potential effects of prior therapy were performed. All post hoc analyses had similar results, and the results of one representative analysis (line of therapy) are therefore reported here. We used a Cox proportional-hazards model, with the same stratification factors as those used for randomization, to estimate hazard ratios and 95% confidence intervals for the primary efficacy end points and for subgroup analyses.\n\n【32】Investigator-assessed progression-free survival and the time to symptom progression were analyzed with the same methods as those used for independent review. The objective response rate was compared between groups with the use of the Mantel–Haenszel chi-square test, with stratification according to the factors used for randomization. For patients with an objective response, the median duration of the response was estimated with the use of the Kaplan–Meier approach.\n\n【33】Results\n-------\n\n【34】Study Population\n----------------\n\n【35】Table 1. Selected Demographic and Baseline Characteristics of the Patients.\n\n【36】From February 2009 through October 2011, a total of 991 patients were enrolled at 213 centers in 26 countries; 496 patients were assigned to lapatinib plus capecitabine, and 495 were assigned to T-DM1 . Baseline demographic and disease characteristics were similar in the two groups . The first data-cutoff date of January 14, 2012 (median duration of follow-up, approximately 13 months), was used for all analyses in this report except the second interim analysis of overall survival, which had a data-cutoff date of July 31, 2012 (median duration of follow-up, approximately 19 months).\n\n【37】Primary Analysis\n----------------\n\n【38】Figure 1. Progression-free Survival, as Assessed by an Independent Review Committee.\n\n【39】Shown are Kaplan–Meier estimates of progression-free survival in the intention-to-treat population, stratified according to world region, number of prior chemotherapy regimens (0 or 1 vs. >1), and site of disease involvement (visceral vs. nonvisceral). Median progression-free survival was 3.2 months longer in the trastuzumab emtansine (T-DM1) group than in the lapatinib–capecitabine group. CI denotes confidence interval.\n\n【40】Treatment with T-DM1 significantly improved progression-free survival as assessed by independent review (median survival, 9.6 months, vs. 6.4 months with lapatinib plus capecitabine; stratified hazard ratio for progression or death from any cause, 0.65; 95% confidence interval \\[CI\\], 0.55 to 0.77; P<0.001)  and in the sensitivity analysis with censoring for nonprotocol therapy . This benefit was consistently observed across clinically relevant subgroups, with a less definitive benefit among patients 75 years of age or older and those with nonvisceral or nonmeasurable disease .\n\n【41】Figure 2. Second Interim Analysis of Overall Survival.\n\n【42】Shown are Kaplan–Meier estimates of overall survival in the intention-to-treat population, stratified according to world region, number of prior chemotherapy regimens (0 or 1 vs. >1), and site of disease involvement (visceral vs. nonvisceral). The second interim analysis was conducted on the basis of 331 deaths and met the predefined O'Brien–Fleming stopping boundary. The data-cutoff date was July 31, 2012. Median follow-up was 18.6 months (range, 0 to 41) in the lapatinib–capecitabine group and 19.1 months (range, 0 to 40) in the T-DM1 group.\n\n【43】At the first interim analysis of overall survival (223 deaths), the stratified hazard ratio for death from any cause with T-DM1 versus lapatinib plus capecitabine was 0.62 (95% CI, 0.48 to 0.81; P=0.0005) and did not cross the predefined O'Brien–Fleming stopping boundary (P=0.0003). At the second interim analysis of overall survival (331 deaths), T-DM1 significantly increased median overall survival (30.9 months, vs. 25.1 months with lapatinib plus capecitabine; hazard ratio for death from any cause, 0.68; 95% CI, 0.55 to 0.85; P<0.001) . Estimated 1-year survival rates were 85.2% (95% CI, 82.0 to 88.5) in the T-DM1 group and 78.4% (95% CI, 74.6 to 82.3) in the lapatinib–capecitabine group; rates at 2 years were 64.7% (95% CI, 59.3 to 70.2) and 51.8% (95% CI, 45.9 to 57.7), respectively.\n\n【44】Prespecified Secondary Efficacy End Points\n------------------------------------------\n\n【45】Table 2. Objective-Response Rate and Duration of Response, as Assessed by the Independent Review Committee.\n\n【46】Treatment with T-DM1 improved investigator-assessed progression-free survival (median, 9.4 months with T-DM1 vs. 5.8 months with lapatinib plus capecitabine; hazard ratio for progression or death from any cause, 0.66; 95% CI, 0.56 to 0.77; P<0.001). The objective-response rate was higher in the T-DM1 group (43.6%; 95% CI, 38.6 to 48.6) than in the lapatinib–capecitabine group (30.8%; 95% CI, 26.3 to 35.7; P<0.001), and the median duration of response was longer (12.6 months vs. 6.5 months) . The median time to a decrease of 5 points or more in the FACT-B TOI score was delayed in the T-DM1 group (7.1 months, vs. 4.6 months with lapatinib plus capecitabine; hazard ratio, 0.80; 95% CI, 0.67 to 0.95; P=0.012).\n\n【47】Treatment Exposure\n------------------\n\n【48】More patients in the lapatinib–capecitabine group than in the T-DM1 group required a dose reduction (lapatinib, 27.3% of patients; capecitabine, 53.4%; T-DM1, 16.3%). As a result, the median daily dose received was 1250.0 mg per day (range, 250.0 to 1332.3) for lapatinib, 1729.8 mg per square meter per day (range, 781.6 to 2338.4) for capecitabine, and 3.5 mg per kilogram every 21 days (range, 2.7 to 4.0) for T-DM1. In the safety population, 37 of 488 patients (7.6%) discontinued treatment with lapatinib, 46 of 488 patients (9.4%) discontinued treatment with capecitabine, and 29 of 490 patients (5.9%) discontinued treatment with T-DM1 because of adverse events .\n\n【49】Safety\n------\n\n【50】Table 3. Adverse Events in the Safety Population.\n\n【51】Serious adverse events in the safety population were reported for 88 patients (18.0%) in the lapatinib–capecitabine group and for 76 patients (15.5%) in the T-DM1 group. The incidence rates of adverse events of grade 3 or above were higher in the lapatinib–capecitabine group than in the T-DM1 group (57.0% vs. 40.8%) . Diarrhea and palmar–plantar erythrodysesthesia were the most commonly reported grade 3 or 4 events in the lapatinib–capecitabine group, affecting 20.7% and 16.4% of patients, respectively. The most commonly reported grade 3 or 4 events with T-DM1 were thrombocytopenia (12.9%) and elevated serum concentrations of aspartate aminotransferase (4.3%) and alanine aminotransferase (2.9%).\n\n【52】For most patients, the first occurrence of grade 3 or 4 thrombocytopenia was reported during the first two cycles of T-DM1 treatment; with dose modifications, the majority of these patients were able to continue treatment (10 patients \\[2.0%\\] discontinued T-DM1 because of thrombocytopenia). The overall incidence of bleeding events was higher with T-DM1 (29.8%, vs. 15.8% with lapatinib plus capecitabine); rates of grade 3 or 4 bleeding events were low in both groups (1.4% and 0.8%, respectively). The only grade 4 bleeding event was a gastrointestinal hemorrhage in a patient treated with T-DM1 whose platelet counts were within the normal range during the study treatment. Reports of hyperbilirubinemia of any grade were more frequent in the lapatinib–capecitabine group than in the T-DM1 group (8.2% vs. 1.2%). With appropriate dose modifications, the majority of patients with grade 3 or 4 elevations in serum aminotransferase levels were able to continue treatment (3 patients discontinued T-DM1 because of grade 3 elevations in aspartate aminotransferase levels), and no patients met Hy's law criteria for drug-induced liver injury. \n\n【53】In the majority of patients, a left ventricular ejection fraction of 45% or more was maintained during the study treatment (in 97.1% of patients in the T-DM1 group and 93.0% of patients in the lapatinib–capecitabine group). Three patients in each group had a decrease from baseline to less than 40%. Of 481 patients in the T-DM1 group and 445 in the lapatinib–capecitabine group who could be evaluated, 8 patients (1.7%) and 7 patients (1.6%), respectively, had an ejection fraction that was less than 50% and at least 15 percentage points below the baseline value. To date, grade 3 left ventricular systolic dysfunction has developed in 1 patient in the T-DM1 group and in no patients in the lapatinib–capecitabine group.\n\n【54】Most of the deaths that occurred during the study period were attributed to disease progression (123 deaths \\[96.1%\\] in the lapatinib–capecitabine group and 91 deaths \\[96.8%\\] in the T-DM1 group). Five deaths were attributed to adverse events that occurred within 30 days after the last dose of a study drug: 4 deaths in the lapatinib–capecitabine group (due to coronary artery disease, multiorgan failure, coma, and hydrocephalus) and 1 death in the T-DM1 group (due to metabolic encephalopathy after CNS progression).\n\n【55】Discussion\n----------\n\n【56】In this phase 3 study, the antibody–drug conjugate T-DM1, as compared with lapatinib plus capecitabine, significantly improved progression-free and overall survival among patients with HER2-positive metastatic breast cancer who had previously received trastuzumab and a taxane. The benefit was observed regardless of the line of therapy in patients with metastatic disease and was seen in patients with a disease-free interval of less than 6 months after completion of trastuzumab-based therapy in the adjuvant or neoadjuvant setting.\n\n【57】The consistent and favorable outcomes with T-DM1 with regard to the primary and secondary end points in this trial indicate that this antibody–drug conjugate has efficacy in the treatment of HER2-positive advanced breast cancer. The safety profile of T-DM1 and the improved progression-free and overall survival with this agent, as compared with standard HER2-directed therapy, provide clinical evidence that intracellular delivery of the cytotoxic agent specifically to HER2-overexpressing cells improves the therapeutic index by minimizing exposure of normal tissue. The adverse events associated with T-DM1 were generally low grade, and patients were largely able to continue treatment after protocol-specified dose modification, with a continued treatment benefit. In addition, the time to symptom progression was significantly delayed with T-DM1.\n\n【58】In conclusion, our study shows that T-DM1 has therapeutic potential, across a heterogeneous population of patients, for the treatment of advanced, HER2-positive breast cancer that has progressed during or after treatment with trastuzumab and a taxane.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "243440c0-cf6f-45ef-ba8f-f6e222f7f7b3", "title": "Infantile Metachromatic Leukodystrophy — Heterozygote Detection in Skin Fibroblasts and Possible Applications to Intrauterine Diagnosis", "text": "【0】Infantile Metachromatic Leukodystrophy — Heterozygote Detection in Skin Fibroblasts and Possible Applications to Intrauterine Diagnosis\nAbstract\n--------\n\n【1】A profound deficiency of arylsulfatase A activity (ARA) is detectable in skin fibroblasts developed from patients with infantile metachromatic leukodystrophy (about 3 to 4 per cent of normal). Fibroblasts were cultivated from skin biopsies of parents and siblings in four unrelated families in which a child with infantile metachromatic leukodystrophy had been documented. Fibroblast ARA activities of seven parents ranged from three to 15 times higher than those found in the affected patients, but were below the activity measured in control fibroblasts, confirming an autosomal recessive mode of inheritance for this disease.\n\n【2】ARA specific activity in cultured cells from normal mid-trimester amniotic-fluid samples is significantly lower than that found in control skin fibroblast cultures. Hence values found in amniotic-fluid cell cultures do not necessarily identify the fetus with metachromatic leukodystrophy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b751279e-ab55-43b3-aa35-630b24352fe7", "title": "A Girl with a Birth Weight of 280 g, Now 14 Years Old", "text": "【0】A Girl with a Birth Weight of 280 g, Now 14 Years Old\nTo the Editor:\n--------------\n\n【1】Figure 1. The Patient at 14 Years of Age.\n\n【2】One of us and several colleagues previously reported in the _Journal_ the survival, at 18 months of age, of a girl with extreme symmetrical intrauterine growth restriction; she had a birth weight of 280 g and a length of 25 cm at a gestational age of 26 weeks and 6 days.  To our knowledge, her birth weight remains the lowest in the world literature. We now report her growth and development at 14 years of age, as she enters high school .\n\n【3】At two years of age, our patient had a Mental Development Index score of 86 (normal range, 84 to 116) on the Bayley Scales of Infant Development and was walking independently. Toilet training took place at three years of age. At five years of age, her visual acuity was 20/200 (in the right eye) and 20/100 (in the left) and was corrected with eyeglasses. Her only hospitalization was at four years of age, for pneumonia. She continues to have reactive airway disease. She started kindergarten at six years of age. A workup for failure to thrive and short stature at three and nine years of age, respectively, revealed no abnormalities. Menarche occurred at 13 years of age. She attends a regular school and has a cumulative grade-point average of 3.70 (of a possible 4.00) for the previous eight years. Since her birth, she has gained an average of 1.8 kg in weight and 9.7 cm in height annually. The 50th percentile for weight and height for girls at the age of 14 years are 50 kg and 163 cm, respectively. Despite her present weight of 25.4 kg and height of 136.5 cm, no psychosocial maladaptations have been reported. The results of her high-school entrance examinations were in the 83rd percentile nationally.\n\n【4】Neonatal survival improves dramatically from 5 percent at a gestational age of 23 weeks to 90 percent at a gestational age of 27 weeks. Despite the routine use of antenatal and postnatal corticosteroids, surfactants, and aggressive ventilation, prospective studies have demonstrated that newborns delivered before 24 weeks of gestation have been completed are less likely to survive and to survive without deficits than are those delivered after a longer gestation. Girls generally have a better prognosis than boys.  The normal cognitive development of our patient is more remarkable than her survival. A significant number of newborns with an extremely low birth weight (<1000 g) and intrauterine growth restriction who have been followed to school age have suboptimal neurodevelopmental outcomes and cognitive function.  Fifty-two newborns with a birth weight of less than 400 g have been described in the literature. Their average gestational age was 25 weeks and 6 days, and 83 percent were girls. All had symmetrical intrauterine growth restriction.  These extremely low-birth-weight, “miracle” newborns can propagate false expectations for families, caregivers, and the medicolegal community alike. Gestational age and female sex are critical characteristics in newborns at the threshold of viability.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8c450c3d-0613-4b58-ac0d-0b681c3ff93b", "title": "Use of Cytomegalovirus Immune Globulin to Prevent Cytomegalovirus Disease in Renal-Transplant Recipients", "text": "【0】Use of Cytomegalovirus Immune Globulin to Prevent Cytomegalovirus Disease in Renal-Transplant Recipients\nAbstract\n--------\n\n【1】We undertook a prospective randomized trial to examine whether an intravenous cytomegalovirus (CMV) immune globulin would prevent primary CMV disease in renal-transplant recipients. Fifty-nine CMV-seronegative patients who received kidneys from donors who had antibodies against CMV were assigned to receive either intravenous CMV immune globulin or no treatment. The immune globulin was administered in multiple doses over the first four months after transplantation.\n\n【2】The incidence of virologically confirmed CMV-associated syndromes was reduced from 60 percent in controls to 21 percent in recipients of CMV immune globulin (P<0.01). Fungal or parasitic superinfections were not seen in globulin recipients but occurred in 20 percent of controls (P = 0.05). Only 4 percent of globulin recipients had marked leukopenia (reflecting serious CMV disease), as compared with 37 percent of the controls (P<0.01). There was a concomitant but not statistically significant reduction in the incidence of CMV pneumonia (17 percent of controls as compared with 4 percent of globulin recipients). A significant reduction in serious CMV-associated disease was observed even when patients were stratified according to therapy for transplant rejection (P = 0.04). We observed no effect of immune globulin on rates of viral isolation or seroconversion, suggesting that treated patients often harbored the virus but that clinically evident disease was much less likely to develop in them.\n\n【3】We conclude that CMV immune globulin provides effective prophylaxis in renal-transplant recipients at risk for primary CMV disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d376b14e-593e-4d97-ad64-fc40e1b86322", "title": "Case 30-2010 — A 15-Year-Old Boy with a Recurrent Skin Lesion", "text": "【0】Case 30-2010 — A 15-Year-Old Boy with a Recurrent Skin Lesion\nA 15-year-old boy was seen in the outpatient cancer center because of a skin lesion that had recurred 13 months after excision. The recurrent lesion was excised, and a diagnosis of an atypical melanocytic lesion with features of melanoma was made.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "601c6fe7-f04b-4739-a780-43ad76e5248b", "title": "Responding to Evolving Abortion Regulations — The Critical Role of Primary Care", "text": "【0】Responding to Evolving Abortion Regulations — The Critical Role of Primary Care\nArticle\n-------\n\n【1】On March 4, 2019, the Department of Health and Human Services published a final rule containing updated regulations for providers participating in the Title X family planning program. These regulations would limit patients’ ability to obtain abortion-related information, counseling, and services. The rule is scheduled to go into effect on May 3, 2019, pending the outcome of lawsuits filed in federal courts by more than 20 states seeking to block its implementation. The changes it would make to the Title X program represent the latest in a series of state and federal regulations restricting access to abortion that have been implemented over the past 5 years. We believe that this emerging reality creates an imperative for the broader primary care community to accelerate uptake and implementation of a feasible, safe, and effective innovation that can democratize access to these services: medication abortion.\n\n【2】The Title X program, enacted in 1970, was designed to ensure access to comprehensive contraceptive care and preventive reproductive health services, especially for low-income and uninsured people. Its overarching objective is to provide information to promote informed decision making and autonomy in family planning. Although Title X funding has never covered abortion services, the new rule would require that there be “clear financial and physical separation” between projects and programs funded by Title X and “facilities where abortion is a method of family planning.”  Physical separation could be achieved by constructing separate entrances for the two spaces, for example. In addition, the rule would no longer require that Title X providers offer abortion counseling, and it would finalize the prohibition against using funds from the program to refer patients for abortion. The rule is clear that Title X providers cannot “refer for abortion (even) when requested by a client.”\n\n【3】What we find striking about this move is that a program never intended to cover abortion services is now being hijacked to restrict information about, and access to, abortion services for vulnerable populations. The new regulations would have a substantial effect on the 4 million people who are served by the 3858 currently extant Title X–funded clinics in the United States each year. Engaging primary care clinicians not affiliated with the Title X program in routinely offering medication abortion thus becomes critically important.\n\n【4】Each year in the United States, more than 600,000 pregnancies result in induced abortion; the majority of these abortions (roughly 90%) occur in the first trimester. Barriers to obtaining abortion services are increasing, and 90% of U.S. women live in a county without an abortion clinic. As of 2014, the United States had only 1671 facilities that provided abortions,  but there are more than 200,000 practicing primary care physicians. Pregnant people who are unable to obtain a wanted abortion are more likely to live in households with incomes below the federal poverty level, to be unemployed, and to have fewer aspirational plans than those who are able to obtain abortion services. Legislation in Texas that created onerous requirements for abortion clinics — and was ultimately struck down by the Supreme Court in _Whole Woman’s Health v. Hellerstedt_ — resulted in the closure of more than half the state’s clinics. The requirements increased the distance that residents needed to travel to reach the nearest abortion facility by an average of 51 miles. \n\n【5】Prior to the availability of mifepristone for medication abortion in the United States, nearly all first-trimester abortions were performed using vacuum aspiration, and referral to clinicians trained in surgical-abortion services was standard primary care practice. When the Food and Drug Administration (FDA) approved mifepristone in 2000, many abortion-rights supporters believed that more clinicians would offer medication abortion and access would greatly expand. In 2006, the percentage of first-trimester abortions in the United States that were medical was 10%; by 2015, the percentage of medical abortions was still only 24%. By contrast, in parts of Europe, such as France, the United Kingdom, and Scandinavia, such rates are between 60 and 90%.\n\n【6】What’s more, estimates suggest that only 1% of abortions in the United States are performed in physicians’ offices.  Nearly all abortion services remain siloed in stand-alone clinics or particular specialties, such as obstetrics and gynecology. Barriers that may explain the lack of dissemination of medication abortion into primary care include misperceptions about the complexity and safety of dispensing mifepristone, lack of training opportunities, organizational policies prohibiting abortion, ambivalence or ethical or moral discomfort among providers, stringent FDA regulations that require both provider registration in a central database and in-clinic storage and direct dispensing of mifepristone to patients, and concerns for the safety of clinic patients and staff because of intimidation and threats of violence. \n\n【7】Protocols for providing medication abortion are well aligned with the structure and functions of primary care. Mifepristone, a progesterone-receptor antagonist that aids in endometrial detachment of a pregnancy, is dispensed in the clinic as one 200-mg pill. Patients are then instructed to take 800 μg of misoprostol, a prostaglandin analogue, at home, to induce uterine contractions. This regimen is effective in completing an abortion in 97 to 98% of cases. Medication abortions can be safely offered at up to 10 weeks’ gestation. Serious complications, such as hemorrhage or infection, occur in an estimated 0.23% of cases.  Common effects of the medications (abdominal cramping and nausea) can be easily managed, and follow-up to ensure completion of the abortion can be performed safely and effectively in person or over the phone. The National Academies of Sciences, Engineering, and Medicine affirmed in a 2018 report that medication abortion lends itself to widespread integration into primary care, since it doesn’t require procedural training or special equipment, the medications can be dispensed and patients can be monitored in an outpatient setting, and counseling skills can be acquired.\n\n【8】In order to integrate medication abortion into primary care, we believe that greater efforts should be made to educate primary care clinicians about this intervention. Information could be provided by means of didactics and workshops in residency training and continuing medical education courses. More personalized “academic detailing” could be delivered by regional experts or experienced peers. Health system leaders and health plans could support primary care practices by using readily available implementation tool kits that inform best practices and reduce barriers for integration of medication abortion services.\n\n【9】Given mounting regulations, many people who would formerly have been able to obtain comprehensive reproductive health care in Title X family planning clinics may be forced to seek care in other settings or carry an unwanted pregnancy to term. Many patients have expressed a strong desire to receive abortion services from their primary care providers. Primary care clinicians could be an important resource for providing care to people in need of reproductive health and abortion services, given their breadth of training and commitment to providing comprehensive care. Performing medication abortions requires a clinical evaluation to confirm an intrauterine pregnancy, basic knowledge about the medications involved, comfort with providing patient-centered counseling, and leadership to champion the integration of new clinic protocols. Such tools are in keeping with those that primary care clinicians already use to optimize care for patients with a range of acute and chronic conditions.\n\n【10】Reproductive health care is a well-established component of primary care. Nearly every primary care clinician will care for women who become pregnant. We believe that updates to the Title X program that would restrict access to abortion-related information, counseling, and services create an imperative to shift away from regarding medication abortion as a niche technique toward considering it the standard of care for people seeking abortion services. Regardless of whether the final rule is ultimately allowed to stand, we believe that the evolving regulations that threaten to further restrict access to abortion in the United States mean the time has come for the primary care community to turn the promise offered by medication abortion into a reality.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d92f5eb6-c0ff-475c-bfba-7a6760f83786", "title": "Expression of H-Y Antigen in Human Males with Two Y Chromosomes", "text": "【0】Expression of H-Y Antigen in Human Males with Two Y Chromosomes\nAbstract\n--------\n\n【1】To determine whether the gene that controls the expression of H-Y (\"male\") antigen on human cells is Y-linked, we have compared the H-Y antigen level in normal males with that in three males with two Y chromosomes. Leukocytes from one XXYY and two XYY males express more H-Y antigen than leukocytes from normal XY males. We conclude that a structural gene or positive regulatory gene for H-Y antigen is on the human Y chromosome. Testing for the H-Y antigen may be of benefit in patients who have signs of masculinization but who lack an identifiable Y chromosome. Positive results for the H-Y antigen would be tentative evidence that the corresponding region of the Y chromosome was present, perhaps as part of a translocation, despite the absence of a typical Y chromosome.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0729541b-0663-4a9c-bc8c-d0450bf95eac", "title": "Toxicity in Chemotherapy — When Less Is More", "text": "【0】Toxicity in Chemotherapy — When Less Is More\nNormal cells and cancer cells respond differently to fasting: normal cells switch to maintenance pathways, but cancer cells do not and are thus sensitized to oxidative stress. A recent study suggests that fasting also renders cancer cells more susceptible to chemotherapy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "58b9e677-4e69-4d05-8ba7-6e81b3d99a08", "title": "Resumption of Cardiac Activity after Withdrawal of Life-Sustaining Measures", "text": "【0】Resumption of Cardiac Activity after Withdrawal of Life-Sustaining Measures\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The minimum duration of pulselessness required before organ donation after circulatory determination of death has not been well studied.\n\n【3】Methods\n-------\n\n【4】We conducted a prospective observational study of the incidence and timing of resumption of cardiac electrical and pulsatile activity in adults who died after planned withdrawal of life-sustaining measures in 20 intensive care units in three countries. Patients were intended to be monitored for 30 minutes after determination of death. Clinicians at the bedside reported resumption of cardiac activity prospectively. Continuous blood-pressure and electrocardiographic (ECG) waveforms were recorded and reviewed retrospectively to confirm bedside observations and to determine whether there were additional instances of resumption of cardiac activity.\n\n【5】Results\n-------\n\n【6】A total of 1999 patients were screened, and 631 were included in the study. Clinically reported resumption of cardiac activity, respiratory movement, or both that was confirmed by waveform analysis occurred in 5 patients (1%). Retrospective analysis of ECG and blood-pressure waveforms from 480 patients identified 67 instances (14%) with resumption of cardiac activity after a period of pulselessness, including the 5 reported by bedside clinicians. The longest duration after pulselessness before resumption of cardiac activity was 4 minutes 20 seconds. The last QRS complex coincided with the last arterial pulse in 19% of the patients.\n\n【7】Conclusions\n-----------\n\n【8】After withdrawal of life-sustaining measures, transient resumption of at least one cycle of cardiac activity after pulselessness occurred in 14% of patients according to retrospective analysis of waveforms; only 1% of such resumptions were identified at the bedside. These events occurred within 4 minutes 20 seconds after a period of pulselessness. \n\n【9】Introduction\n------------\n\n【10】### Video\n\n【11】 Cessation and Resumption of Cardiac Activity \n\n【12】A principle of organ donation from deceased donors is that the donor must be declared dead before initiation of organ retrieval.  Donation most commonly occurs after neurologic determination of death by standard criteria for brain death  but can also occur after circulatory determination of death. The practice of donation after circulatory determination of death after removal of life-sustaining measures has increased,  and the criteria used for determining death in this context have varied.  Most protocols for organ donation after circulatory determination of death recommend 5 minutes of observation of apnea and pulselessness as determined by arterial catheter monitor, although practices vary from 2 to 10 minutes.  After this period, without attempts to restart circulation and without spontaneous resumption of circulation, loss of circulation is considered permanent and organ recovery may begin.\n\n【13】Instances of organ recovery after 75 seconds of pulselessness in infants have led to debate on the minimum acceptable duration of observation to ensure that permanent loss of circulation has occurred.  Concerns about the potential for autoresuscitation, or return of spontaneous cardiac activity, are based on reports in terminated cardiopulmonary resuscitation (CPR).  The longest reported period of pulselessness between terminated CPR and observed autoresuscitation is 10 minutes.  Limited prospective evidence suggests that return of cardiac activity occurs less frequently after withdrawal of life-sustaining measures and circulatory determination of death than after terminated CPR.  Observational studies involving patients dying after withdrawal of life-sustaining measures have described transient resumption of circulation or cardiac activity occurring seconds to minutes after pulselessness, with no reports that consciousness was regained or that the patient survived to hospital discharge.  We conducted a prospective and retrospective observational study, the Death Prediction and Physiology after Removal of Therapy (DePPaRT) study, to describe the incidence and timing of resumption of cardiac electrical and pulsatile activity in critically ill adults who died after withdrawal of life-sustaining measures.\n\n【14】Methods\n-------\n\n【15】Study Design\n------------\n\n【16】This study was conducted at 20 adult intensive care units in Canada (16 sites), the Czech Republic (3 sites), and the Netherlands (1 site) . The study was designed and led by a core team (whose members are listed in the Supplementary Appendix ) with input from a steering committee and family partners.  Funding was from the Canadian Institutes for Health Research as part of the Canadian Donation and Transplantation Research Program, the CHEO Research Institute, and the Karel Pavlík Foundation. The research protocol was approved by the relevant institutional review board or ethics committee at each site, and all patients’ surrogate decision makers provided written informed consent for participation in the study.\n\n【17】Procedures\n----------\n\n【18】Patients in intensive care at participating hospitals were eligible if surrogate decision makers had agreed on a care plan of withdrawal of life-sustaining measures without CPR and imminent death was anticipated. Patients with neurologic determination of death or a functioning cardiac pacemaker or without an arterial catheter were excluded. Clinicians provided palliation, withdrew ventilation and medications, and determined death according to their usual practices. \n\n【19】We recorded demographic characteristics, eligibility for organ donation, palliative interventions, withdrawal of life-sustaining measures (e.g., extubation and changes in mechanical ventilation, circulatory interventions, or both), and time-of-death determination. Data on blood pressure (recorded with an arterial catheter), heart rhythm (3- or 5-lead electrocardiography \\[ECG\\]), and oxygen saturation (plethysmography) were collected continuously for at least 15 minutes before commencement of withdrawal of life-sustaining measures and up to 30 minutes after death determination according to institution-specific criteria . In a subgroup of planned organ donors, monitoring devices were removed 5 minutes after pulselessness according to regional practice. Deidentified electronic data on ECG and arterial blood-pressure waveforms for all the patients were uploaded to the secure study website. This was an observational study; health care providers were asked to continue routine end-of-life practices, including monitoring and death determination, as per usual. They were asked to document their method of death determination with the use of a standardized checklist and to record and describe any observations of unassisted resumption of cardiac activity on a case-report form. \n\n【20】Clinicians at the bedside reported resumption of circulation or cardiac activity prospectively by identifying activity on bedside ECG, arterial pressure catheter monitors, palpated arterial pulse, breaths, or physical movements. We subsequently determined the incidence of resumptions of cardiac activity independently of clinical reports using retrospective adjudicated review of ECG and arterial pressure catheter waveforms. These waveforms were used both to affirm the presence of cardiac activity as observed at the bedside and to identify additional instances that were not reported by bedside clinicians.\n\n【21】For the retrospective part of the study, we defined resumption of cardiac electrical and pulsatile activity as a return of arterial pulse pressure of at least 5 mm Hg corresponding to at least one QRS complex, after a period of pulse pressure of less than 5 mm Hg for at least 60 seconds, as detected by indwelling arterial pressure catheter monitor. This definition was developed by an expert clinical advisory committee with agreement from 8 intensivists and cardiac physiology experts, who were also the group retrospectively adjudicating the waveforms, and 12 members of the steering committee .\n\n【22】We collected, processed, and analyzed digitized data on heart rate and blood-pressure waveforms  using software that was developed to identify cessation of ECG activity, pulselessness, and resumptions of cardiac electrical and pulsatile activity . Two or more adjudicators, who were unaware of patient demographic characteristics and clinical history, reviewed waveform data with a custom-made waveform viewer. Using data on ECG and arterial pressure catheter waveforms, adjudicators located periods of absence of circulation and identified if and when activity returned. Discrepancies of more than 2 seconds in the duration of a cessation or resumption of activity were resolved at review meetings, observed by a steering committee member to ensure equity of discussion. A third adjudicator was consulted when consensus was not achieved .\n\n【23】Statistical Analysis\n--------------------\n\n【24】Without previously published rates of autoresuscitation or resumptions of circulation or cardiac activity, we arbitrarily estimated that a sample size of 500 patients with 0 observed events would be consistent with an incidence of less than 0.7% using a binomial one-tailed 97.5% confidence interval.  MATLAB software (MathWorks) and R software, version 3.6.1 ,  were used for descriptive analyses. The incidence and timing of resumption of electrical and pulsatile cardiac activity are provided descriptively. Because of the low number of events, we calculated confidence intervals for binomial distribution for the main results using either Agresti–Coull  (number of events >0) or Clopper–Pearson (number of events=0) confidence intervals.  We adhered to the STROBE checklist  for reporting of data. Interrater correlation and intrarater correlation among adjudicators were determined .\n\n【25】Results\n-------\n\n【26】Patients\n--------\n\n【27】Figure 1. Study Enrollment.\n\n【28】DCD denotes donation after circulatory determination of death, ECG electrocardiographic, and ICU intensive care unit.\n\n【29】We screened 1999 adult patients between May 1, 2014, and May 1, 2018, and determined that 695 were eligible for the study; 48 families of eligible patients declined participation and 16 patients were ineligible after enrollment, which resulted in the inclusion of 631 patients . The characteristics of enrolled and nonenrolled patients are shown in Table S2. Of enrolled patients, 205 (32%) were eligible for organ donation after circulatory determination of death, of whom 67 (33%) became donors and had at least one organ retrieved. The median time from the start of withdrawal of life-sustaining measures to determination of death by cardiac criteria was 60 minutes (interquartile range, 21 to 283; range, 1 minute to 11 days 5 hours 54 minutes).\n\n【30】Prospective Bedside Observations\n--------------------------------\n\n【31】Table 1. Characteristics of Enrolled Patients at Baseline.\n\n【32】Table 1 shows the characteristics of enrolled patients. A total of 13 of 631 patients had bedside observation of return of cardiac activity, but only 5 of these instances (1%; 95% confidence interval \\[CI\\], 0 to 2) were corroborated by retrospective waveform analysis. Three of these 5 instances were described as a return of heart rhythm and blood pressure, the fourth was described as a return of heart rate (without comment on blood pressure), and the fifth was described as a return of respiration (without comment on heart rate or blood pressure). The 5 reports that were confirmed by waveform showed resumptions of cardiac electrical and pulsatile activity at 64 seconds, 66 seconds, 2 minutes 30 seconds, 2 minutes 31 seconds, and 2 minutes 56 seconds after a period of pulselessness. There were 2 clinical reports of resumption of activity for which waveform vital-signs data were not available for review; 1 was described as transient resumption of heart rate after a 3-minute cessation, and 1 was reported as transient resumption of heart rate and blood pressure after a cessation of 1 minute 42 seconds.\n\n【33】Retrospective Waveform Analysis\n-------------------------------\n\n【34】Figure 2. Duration of Cessation of Cardiac Activity as Compared with the Duration of Resumption of Cardiac Activity.\n\n【35】Panel A shows a scatterplot of retrospective waveform analysis indicating the duration of cessation of cardiac activity (arterial blood pressure, <5 mm Hg) as compared with the duration of resumption of cardiac activity (arterial blood pressure, ≥5 mm Hg concurrent with ECG activity). There were a total of 77 cessations and resumptions in 67 patients (of 480), with 7 patients having more than 1 cessation and resumption. Panel B shows a histogram of the number of cessations of cardiac activity, with the x axis of the scatterplot used for binning intervals. Panel C shows a histogram of the number of resumptions of cardiac activity, with the y axis of the scatterplot used for binning intervals. (Note that the scales on the y axes in Panel B and Panel C are not the same.)\n\n【36】Of the 631 patients enrolled, 151 did not have complete waveform data, resulting in a subgroup of 480 with both bedside clinical observations and complete data on ECG and arterial pressure catheter waveforms that were available for retrospective analysis. Review of the data from the 480 patients with complete ECG and arterial pressure catheter waveforms and at least 5 minutes of continuous waveform monitoring after pulselessness showed 67 of 480 patients (14%; 95% CI, 11 to 17) with resumption of cardiac activity, including the 5 who had a resumption identified by bedside observation and 7 (1%) who had more than one cessation followed by a resumption of cardiac activity. The durations of cessation of cardiac activity and subsequent resumptions are shown in Figure 2 . The majority of resumptions of cardiac activity (55 resumptions in 45 patients) followed pulselessness of 1 to 2 minutes. The longest duration of pulselessness before a resumption of cardiac electrical and pulsatile activity was 4 minutes 20 seconds . Among 32 potential organ donors with waveform data that could be interpreted, there were two resumptions of cardiac activity, one after 1 minute 4 seconds and one at 2 minutes 31 seconds after pulselessness, both reported by bedside clinicians.\n\n【37】The median duration of resumed cardiac activity was 3.9 seconds (range, 1 second to 13 minutes 14 seconds) . Of the 67 patients who had a resumption of cardiac activity, 33 (49%) had a resumption lasting only one cardiac cycle .\n\n【38】In patients with waveform recordings of 30 minutes (432 of 480) after determination of death, all instances of resumption of cardiac activity occurred within 5 minutes after pulselessness. We estimated the probability of observing resumptions after more than 5 minutes to be less than 1% (3.7 of 432, with the use of a binomial one-tailed 97.5% confidence interval). \n\n【39】Cessation of cardiac electrical activity coincided within 2 seconds with the last arterial pulse of at least 5 mm Hg in 93 patients (19%). The median time between final arterial pulse and final QRS complex was 3 minutes 37 seconds (range, 0 seconds to 83 minutes 28 seconds). Cardiac electrical activity after the last arterial pulse was observed for more than 30 minutes in 33 of 480 patients (7%) and until the end of recording in 23 of 480 patients (5%) .\n\n【40】Discussion\n----------\n\n【41】After a period of loss of cardiac activity that followed the planned withdrawal of life-sustaining measures, 1% of the patients in our study had transient resumption of cardiac activity observable by bedside reports that were corroborated by retrospectively identified ECG and arterial pulse catheter waveform activity. Retrospective waveform review showed resumption of cardiac activity in 14% of the patients, including resumptions identified at the bedside. The longest period of pulselessness that was followed by resumption of cardiac activity was 4 minutes 20 seconds. Activity on ECG after pulselessness often continued past cessation of arterial catheter pressure.\n\n【42】A systematic review,  which included one prospective study involving 30 patients,  showed a return of cardiac activity in 0 to 3% of patients after withdrawal of life-sustaining measures; the longest duration of pulselessness was 1 minute 42 seconds before observed resumption of cardiac activity. Concerns about this type of autoresuscitation can potentially limit the implementation of the practice of donation after circulatory determination of death. The term “autoresuscitation” used in this context may be a misnomer if it is interpreted as a return to viable life. In the current study, no patients who had a resumption of cardiac activity regained consciousness or survived. However, transient resumption of cardiac activity did occur, which suggests that the physiologic processes of somatic death after removal of life-sustaining measures occasionally include periods of cessation and resumption of cardiac electrical and pulsatile arterial activity. These transient resumptions of cardiac activity after withdrawal of life-sustaining measures are not equivalent to autoresuscitation as observed after terminated CPR, in which returns of circulation have rarely resulted in return of consciousness and survival.\n\n【43】Our analysis of clinical reports by bedside clinicians and vital-sign waveform recordings from a large international sample supports the current 5-minute observation period required by most protocols and guidelines for proceeding with organ donation after circulatory determination of death. Our results also confirm the known phenomenon of electrical cardiac activity continuing in the absence of pulsatile cardiac activity. Waiting for cessation of ECG activity to determine circulatory death is a recommendation in some protocols. \n\n【44】This study has limitations. Of 13 clinically reported resumptions of circulation, 2 did not have waveform data available for confirmation of cardiac activity and were not included in our estimates. The generalizability of our results may be limited because we excluded patients without arterial pressure catheters and 24% of enrolled patients could not be included in the retrospective waveform analysis owing to incomplete data. The group of patients that we studied included 67 who proceeded to organ recovery, 32 of whom had waveform data. These patients were monitored for only 5 minutes of pulselessness, consistent with organ-donation protocols; this limited the observation period for return of circulatory activity. There were 2 clinical reports of return of cardiac activity within the 5-minute observation period in organ donors, both confirmed by waveform analysis.\n\n【45】Our study definition of cardiac activity used an arbitrary threshold of pulse pressure (≥5 mm Hg) that does not imply meaningful circulation. This conservative consensus definition may have been partially responsible for the ostensibly high incidence (14%) of transient resumptions of cardiac activity identified through waveform adjudication.\n\n【46】After a period of pulselessness that followed planned withdrawal of life-sustaining measures, clinically reported resumption of cardiac activity that was confirmed by waveform analysis occurred in 1% of the patients. Retrospective analysis of continuous ECG and arterial pressure monitoring identified resumption of cardiac activity in 14% of the patients, all occurring within 5 minutes after pulselessness.\n\n【47】Table 1. Characteristics of Enrolled Patients at Baseline. \n\n| Variable | Clinical Observations | Clinical Observations | Retrospective Waveform Adjudication | Retrospective Waveform Adjudication |\n| --- | --- | --- | --- | --- |\n|  | All Patients(N=631) | Confirmed Bedside Report of Resumption of Cardiac Activity(N=5) | All Patients(N=480) | Resumption of Cardiac Activity Identified in Waveforms(N=67) |\n| Demographic characteristics |  |  |  |  |\n| Age — yr |  |  |  |  |\n| Mean | 63±16 | 66±6 | 65±15 | 66±13 |\n| Range | 18–95 | 58–74 | 18–95 | 22–94 |\n| Female sex — no. (%) | 241 (38) | 3 (60) | 184 (38) | 26 (39) |\n| Chronic condition — no./total no. (%) | 518/630 (82) | 3/5 (60) | 411/479 (86) | 56/67 (84) |\n| Primary reason for ICU admission — no. (%) |  |  |  |  |\n| Neurologic | 307 (49) | 2 (40) | 226 (47) | 27 (40) |\n| Cardiac | 23 (4) | 0 | 18 (4) | 2 (3) |\n| Respiratory | 96 (15) | 2 (40) | 74 (15) | 15 (22) |\n| Sepsis or infection | 95 (15) | 1 (20) | 78 (16) | 12 (18) |\n| Trauma | 28 (4) | 0 | 18 (4) | 1 (1) |\n| Other  | 82 (13) | 0 | 66 (14) | 10 (15) |\n| CPR in previous 24 hr — no. (%) | 84 (13) | 0 | 71 (15) | 7 (10) |\n| Glasgow Coma Scale score at ICU admission  |  |  |  |  |\n| Patients evaluated | 626 | 5 | 476 | 66 |\n| Median score (range) | 4 (3–15) | 9 (3–15) | 4 (3–15) | 4 (3–15) |\n| APACHE II score  |  |  |  |  |\n| Patients evaluated | 627 | 5 | 477 | 66 |\n| Mean | 28±9 | 23±6 | 28±8 | 28±8 |\n| Range | 5–55 | 17–31 | 5–55 | 9–43 |\n| Traumatic brain injury — no./total no. (%) | 86/630 (14) | 0/5 | 65/479 (14) | 4/67 (6) |\n| Length of stay in ICU |  |  |  |  |\n| Patients evaluated | 630 | 5 | 479 | 67 |\n| Median stay (range) — days | 4 (0–61) | 5 (2–23) | 3 (0–61) | 5 (0–30) |\n| DCD donor — no. (%) | 67 (11) | 2 (40) | 32 (7) | 2 (3) |\n| Bedside report of resumption of cardiac activity — no. (%) | 13 (2) | 5 (100) | 12 (2)  | 7 (10)  |\n| Determination-of-death form completed — no. (%) | 596 (94) | 5 (100) | 463 (96) | 63 (94) |\n| Life-sustaining measures |  |  |  |  |\n| Receiving invasive mechanical ventilation — no. (%) | 552 (87) | 5 (100) | 416 (87) | 63 (94) |\n| Extubated during withdrawal of life-sustaining measures — no./total no. (%) | 389 (62) | 4 (80) | 277 (58) | 43 (64) |\n| No. of vasopressors or inotropes — no. of patients (%) |  |  |  |  |\n| 0 | 270 (43) | 3 (60) | 189 (39) | 26 (39) |\n| 1 | 232 (37) | 1 (20) | 192 (40) | 29 (43) |\n| 2 | 78 (12) | 0 | 62 (13) | 7 (10) |\n| ≥3 | 51 (8) | 1 (20) | 37 (8) | 5 (7) |\n| Receiving sedation — no. (%) | 463 (73) | 4 (80) | 346 (72) | 58 (87) |\n| Receiving analgesia — no. (%) | 588 (93) | 5 (100) | 445 (93) | 65 (97) |\n\n【49】 Plus–minus values are means ±SD. Percentages may not total 100 because of rounding. CPR denote cardiopulmonary resuscitation, DCD donation after circulatory determination of death, and ICU intensive care unit.\n\n【50】 Other reasons include gastrointestinal bleeding, abdominal aortic aneurysm, multiple causes, hypovolemic shock, and multiorgan failure.\n\n【51】 Scores on the Glasgow Coma Scale range from 3 to 15, with lower scores indicating a reduced level of consciousness.\n\n【52】 Score on the Acute Physiology and Chronic Health Evaluation (APACHE) II range from 0 to 71, with higher scores indicating more severe disease.\n\n【53】 One clinical report of resumption of circulation did not have waveform data available because it occurred more than 24 hours after withdrawal of life-sustaining measures. A second report had waveform data but a loss of signal in the data coincided with the clinical report of resumption of circulation.\n\n【54】 Two patients had clinical reports of resumption of circulation that did not correspond to our definition of a resumption of circulation but later had a resumption of circulation verified by waveform adjudication that was not reported clinically.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f3b26a6a-be15-45df-9321-0826573ba469", "title": "The Meningococcal Vaccine — Public Policy and Individual Choices", "text": "【0】The Meningococcal Vaccine — Public Policy and Individual Choices\nEvery year there are approximately 2200 to 3000 cases of invasive meningococcal disease in the United States. Because of its low cost effectiveness and other limitations, routine use of the meningococcal vaccine is not recommended. However, individual choices may differ from public policy considerations. If parents were aware of this option, many might choose to pay for vaccination in order to protect adolescents and young adults from this devastating infection. Parents and patients need information about the availability of all effective vaccines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "72116d58-5bd2-418c-898f-ea5a37b18475", "title": "Case 35-2006 — A Newborn Boy with Hypotonia", "text": "【0】Case 35-2006 — A Newborn Boy with Hypotonia\nA 2-day-old boy was hospitalized because of hypotonia. Neurologic examination showed diffuse hypotonia, no tongue fasciculations, absent deep-tendon reflexes in the arms, trace reflexes at the knees, and bilateral ankle clonus. The serum creatine kinase level was 14,528 U per liter. A diagnostic procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f4fe65b1-efa2-4b12-9225-fbb231ab4892", "title": "Risks of Cancer of the Colon and Rectum in Relation to Serum Cholesterol and Beta-Lipoprotein", "text": "【0】Risks of Cancer of the Colon and Rectum in Relation to Serum Cholesterol and Beta-Lipoprotein\nAbstract\n--------\n\n【1】We studied the risk of colorectal cancer in relation to serum cholesterol and beta-lipoprotein in more than 92,000 Swedish subjects less than 75 years old. The cohort was examined between 1963 and 1965 and followed by means of the Swedish Cancer Register until 1979. During this period, 528 colon cancers and 311 rectal cancers developed. A positive association was observed between the serum cholesterol level and the risk of rectal cancer among men (P<0.05), with a relative risk of 1.65 in men with levels ≥276 mg per deciliter (7.1 mmol per liter). An association was also observed between the serum betalipoprotein level and the risk of rectal cancer among men (P<0.05). When cholesterol and beta-lipoprotein levels were considered together, they were associated with both rectal and colon cancer in men. The relative risk in men with both cholesterol ≥250 mg per deciliter (6.5 mmol per liter) and beta-lipoprotein ≥12 units (2.2 g per liter) was 1.62 for colon cancer (95 percent confidence interval, 1.18 to 2.22) and 1.70 for rectal cancer (1.18 to 2.44). Similar trends were observed in women, although they were not statistically significant.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "74706f60-3656-4c2f-add5-57362024d5dd", "title": "Glycemic Management of Type 2 Diabetes Mellitus", "text": "【0】Glycemic Management of Type 2 Diabetes Mellitus\nThis article provides a framework for establishing glycemic targets for patients with type 2 diabetes, taking into account both psychosocial and clinical factors, and discusses strategies to achieve the targets. First-line treatments and additional therapies are discussed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "66856a13-094c-44ef-ac92-a9b36586a821", "title": "Antioxidant Vitamins and Colorectal Adenoma", "text": "【0】Antioxidant Vitamins and Colorectal Adenoma\nTo the Editor\n-------------\n\n【1】Greenberg et al. (July 21 issue)  report negative results when three antioxidant vitamins (beta carotene, vitamin C, and vitamin E) were given over a four-year period to test their efficacy in preventing colorectal adenoma, a surrogate outcome for colorectal cancer. The authors suggest that the three-year period between the colonoscopic examinations at year 1 and year 4 may not have been long enough for a change in the incidence of adenoma to be observed and that the interventions used may not have been the correct ones to reduce the incidence. A third explanation is that the interventions may affect the incidence of colorectal carcinoma by a method other than a reduction in adenomas. This implies that the wrong surrogate outcome was selected.\n\n【2】In order to determine before the study that the appropriate surrogate outcome has been selected, the connection between that outcome and the outcome of ultimate interest must be determined -- that is, the accuracy with which the three-year occurrence of adenomas predicts colorectal cancer must be assessed. If this predictive accuracy is low, it is unlikely that an intervention whose benefit is indexed by this surrogate outcome will produce a detectable benefit.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e72baee3-a34b-49a2-9842-eefef493f5fd", "title": "Cystathioninuria in Two Healthy Siblings", "text": "【0】Cystathioninuria in Two Healthy Siblings\nAbstract\n--------\n\n【1】Tests in two siblings with cystathioninuria indicated that the disorder is congenital and not secondary. Yet both children enjoy good physical and mental health. Their parents excrete substantial amounts of cystathionine in urine only after methionine loading, and thus appear to be heterozygous for the cystathioninuria gene. It is possible that the mental defect and other serious disorders that have been described in six of the seven previously reported cases of cystathioninuria were coincidental, and in no way the result of the genetically determinded deficiency of the cystathionine-cleaving enzyme. Cystathioninuria may prove to be a benign disorder requiring no treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1e23db92-cb09-4c75-b88c-acfc1345b126", "title": "Effect of Dengue Serostatus on Dengue Vaccine Safety and Efficacy", "text": "【0】Effect of Dengue Serostatus on Dengue Vaccine Safety and Efficacy\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】In efficacy trials of a tetravalent dengue vaccine (CYD-TDV), excess hospitalizations for dengue were observed among vaccine recipients 2 to 5 years of age. Precise risk estimates according to observed dengue serostatus could not be ascertained because of the limited numbers of samples collected at baseline. We developed a dengue anti–nonstructural protein 1 (NS1) IgG enzyme-linked immunosorbent assay and used samples from month 13 to infer serostatus for a post hoc analysis of safety and efficacy.\n\n【3】Methods\n-------\n\n【4】In a case–cohort study, we reanalyzed data from three efficacy trials. For the principal analyses, we used baseline serostatus determined on the basis of measured (when baseline values were available) or imputed (when baseline values were missing) titers from a 50% plaque-reduction neutralization test (PRNT <sub>50 </sub> ), with imputation conducted with the use of covariates that included the month 13 anti-NS1 assay results. The risk of hospitalization for virologically confirmed dengue (VCD), of severe VCD, and of symptomatic VCD according to dengue serostatus was estimated by weighted Cox regression and targeted minimum loss–based estimation.\n\n【5】Results\n-------\n\n【6】Among dengue-seronegative participants 2 to 16 years of age, the cumulative 5-year incidence of hospitalization for VCD was 3.06% among vaccine recipients and 1.87% among controls, with a hazard ratio (vaccine vs. control) through data cutoff of 1.75 (95% confidence interval \\[CI\\], 1.14 to 2.70). Among dengue-seronegative participants 9 to 16 years of age, the cumulative incidence of hospitalization for VCD was 1.57% among vaccine recipients and 1.09% among controls, with a hazard ratio of 1.41 (95% CI, 0.74 to 2.68). Similar trends toward a higher risk among seronegative vaccine recipients than among seronegative controls were also found for severe VCD. Among dengue-seropositive participants 2 to 16 years of age and those 9 to 16 years of age, the cumulative incidence of hospitalization for VCD was 0.75% and 0.38%, respectively, among vaccine recipients and 2.47% and 1.88% among controls, with hazard ratios of 0.32 (95% CI, 0.23 to 0.45) and 0.21 (95% CI, 0.14 to 0.31). The risk of severe VCD was also lower among seropositive vaccine recipients than among seropositive controls.\n\n【7】Conclusions\n-----------\n\n【8】CYD-TDV protected against severe VCD and hospitalization for VCD for 5 years in persons who had exposure to dengue before vaccination, and there was evidence of a higher risk of these outcomes in vaccinated persons who had not been exposed to dengue. \n\n【9】Introduction\n------------\n\n【10】The first dengue vaccine — the recombinant, live, attenuated, tetravalent dengue vaccine (CYD-TDV) — was licensed on the basis of three efficacy trials in the Asia-Pacific region and Latin America.  After an excess of hospitalizations for dengue among children who had been vaccinated at 2 to 5 years of age was observed in the third year of the phase 3 trial in Asia (the CYD14 trial), the potential effects of baseline dengue serostatus and age on vaccine safety and efficacy required reconsideration.  One hypothesis for these excess cases was that CYD-TDV in recipients without previous dengue infection (i.e., dengue-unexposed vaccine recipients) mimics primary infection and, similar to natural secondary infection, places these people at an increased risk for severe disease on subsequent infection. \n\n【11】The CYD-TDV efficacy trials assessed baseline dengue serostatus in a subset of participants (7.5% to 20%, depending on the trial) by measuring antibodies to each serotype with a 50% plaque-reduction neutralization test (PRNT <sub>50 </sub> ). This limited subset, referred to as the immunogenicity subset, did not allow for precise estimates of the risk of hospitalization for dengue or the risk of severe dengue in seronegative vaccine recipients.  In an effort to overcome this limitation, blood samples that had been collected after the third vaccination were used to retrospectively determine baseline serostatus in a post hoc study. In the case–cohort study reported here, we used a newly developed dengue anti–nonstructural protein 1 (NS1) IgG enzyme-linked immunosorbent assay (ELISA)  to differentiate between anti-NS1 antibodies induced by wild-type dengue infection and those induced by vaccination (since CYD-TDV contains genes encoding NS1 from the yellow fever 17D vaccine virus rather than from dengue virus) to infer baseline dengue serostatus and reanalyze vaccine safety and efficacy according to serostatus.\n\n【12】Methods\n-------\n\n【13】Study Design\n------------\n\n【14】The findings of the CYD-TDV efficacy trials included in this analysis (CYD14 in the Asia-Pacific region, CYD15 in Latin America, and CYD23 \\[and its long-term follow-up extension study, CYD57\\] in Thailand) have been reported elsewhere.  The trials were similar to one another in design, with participants randomly assigned in a  ratio to the vaccine group or the control group at months 0, 6, and 12 and actively followed for disease to month 25. All participants who were randomly assigned to the control group in CYD14 and CYD15 received 0.9% saline placebo. In CYD23, all participants who were randomly assigned to the control group received 0.9% saline placebo, with the exception of the first 50 participants, who received inactivated rabies vaccine (Verorab, Sanofi Pasteur) for the first injection and 0.9% saline placebo for all other injections. Follow-up continued in order to monitor for disease leading to hospitalization (hospital phase), and active surveillance has subsequently been reinstated from approximately month 50 onward. Blood samples were collected during the acute phase of illness to virologically confirm dengue infection.\n\n【15】In this case–cohort study, we reassessed all cases of symptomatic virologically confirmed dengue (VCD), hospitalization for VCD, and severe VCD according to serostatus; this assessment included all cases occurring in participants in the immunogenicity subsets. From each trial, a subcohort of 10% of the participants was randomly selected after stratification according to age group and trial site. Details of the sampling strategy for the subcohort are provided in the analysis plan . The case–cohort design provides power similar to that obtained with retesting of the entire cohort and enables efficient evaluation of multiple outcomes . \n\n【16】Sanofi Pasteur was the sponsor of the clinical trials and funded the work reported here and the development of the manuscript that was submitted. The sponsor was involved in all aspects of the trials and related analyses. The authors vouch for the accuracy and completeness of the data reported in this study.\n\n【17】Assessment Methods\n------------------\n\n【18】Month 13 anti-NS1 titers were measured in all participants in this case–cohort study for whom samples were available . For the principal analyses, dengue serostatus at the time of vaccination was defined on the basis of the baseline PRNT <sub>50 </sub> serostatus as measured in the original trials (for participants with baseline values) or was imputed in analyses in which variables, including month 13 anti-NS1 titers as a continuous variable, were used as predictors (for participants in the case cohort with missing baseline values). Two imputation methods were used: logistic regression for multiple imputation and super learner  for targeted minimum loss–based estimation . For complementary analyses, cohorts were defined according to the month 13 anti-NS1 titer (month 13 NS1 method) with cutoff thresholds for positivity of 9 ELISA units (EUs) per milliliter or 20 EUs per milliliter. The threshold of 9 EUs per milliliter (the lower limit of quantitation) was chosen to minimize rates of false seronegative results.\n\n【19】End Points\n----------\n\n【20】The primary objective of the study was to assess the risk of hospitalization for VCD in seronegative vaccine recipients who were 9 years of age or older at enrollment (the primary end point). Prespecified secondary objectives included an assessment of this risk in seronegative participants in prespecified age groups (2 to 8 years and 2 to 16 years). The primary safety end point was hospitalization for VCD, and the other safety end point was severe VCD (as defined by an independent data and safety monitoring committee). Assessment of the efficacy of the vaccine against symptomatic VCD up to month 25 in prespecified age groups (2 to 8 years, 9 to 16 years, and 2 to 16 years) in seronegative participants was a secondary objective. Exploratory objectives included assessment of these end points among dengue-seropositive participants and a serotype-specific analyses. The definitions and methods of assessment of the safety and efficacy end points were the same as previously reported for the individual trials . \n\n【21】Statistical Analysis\n--------------------\n\n【22】Cumulative incidence, hazard ratios, relative risks of severe VCD or hospitalization for VCD, and vaccine efficacy against symptomatic VCD were estimated on the basis of measured or imputed month 0 PRNT <sub>50 </sub> serostatus (for the multiple imputation and targeted minimum loss–based estimation methods) and month 13 NS1 serostatus. In accordance with the original protocols , efficacy was evaluated to month 25, and safety was evaluated over a long-term follow-up period (up to 6 years).\n\n【23】The multiple-imputation and NS1 methods involved a weighted Cox regression model with study group (vaccine or control) as a covariate. The regression models were not stratified according to serostatus or age group. Instead, separate subgroup analyses were performed with the use of the regression models for each category of serostatus and age group. Wald 95% confidence intervals of hazard ratios and P values were calculated.  With the multiple-imputation approach, estimates from 10 iterations were combined with the use of Rubin’s variance rule.  Attributable risks were calculated as between-group differences in estimated cumulative incidence over 5 years. A parametric bootstrap approach (with 1000 samples) in the subcohort under the assumption of a Poisson distribution was used to estimate 95% confidence intervals for attributable risk. In analyses based on super learner to predict baseline serostatus, targeted minimum loss–based estimation was used to estimate cumulative incidences of dengue, from which vaccine efficacy, relative risk, and attributable risk were calculated.\n\n【24】Analyses of data from the individual trials and pooled data were conducted (phase 3 trials for efficacy and all trials for safety). Reported P values are two-sided. Although the prespecified analysis plan stated that P values were not going to be adjusted for multiplicity, we are presenting Holm–Bonferroni adjustment of P values for multiple safety end points, as well. \n\n【25】We anticipated that imputation of baseline serostatus from month 13 anti-NS1 titers might be affected by dengue infection occurring between month 0 and month 13. Therefore, prespecified principal analyses were performed: from month 0 onward (including participants who had VCD between month 0 and month 13), and from month 13 onward (excluding participants who had VCD between month 0 and month 13). Our reasoning was based on the fact that the analysis from month 0 onward accounts for potential vaccine protection against events between month 0 and month 13 in cumulative efficacy and risk estimates and maximizes the benefit of randomization. Complementary analyses in which month 13 NS1 serostatus was used assessed outcomes only from month 13 onward.\n\n【26】Results\n-------\n\n【27】Study Population\n----------------\n\n【28】The case cohort included the 3578 participants in the subcohort (2384 in the vaccine group and 1194 in control group), as well as all the participants from the trials who had symptomatic VCD (1258 cases), hospitalization for VCD (644 cases), or severe VCD (142 cases) . The distribution of baseline demographic characteristics in the overall, seropositive, and seronegative populations in the subcohort was balanced between the vaccine group and the control group; 24.5% of the participants were classified as seronegative by logistic regression (multiple-imputation), 24.0% by super learner (targeted minimum loss–based estimation), and 23.4% by measurement of anti-NS1 titers (with the cutoff of 9 EUs per milliliter). In the subcohort, data on month 0 PRNT <sub>50 </sub> and month 13 anti-NS1 titers were missing for 66.7% (1591 of 2384) and 3.9% (93 of 2384), respectively, of vaccine recipients and 68.5% (818 of 1194) and 4.8% (57 of 1194) of controls. If (as is plausible when this sampling design is used) the probabilities of missing data are random after measured variables used in the analysis have been accounted for, then the inferences are expected to be valid. Additional details on the baseline characteristics of the participants and on missing data are provided in Tables S1, S3 through S6, and S46 in the Supplementary Appendix .\n\n【29】Validation of Imputed Baseline Dengue Serostatus\n------------------------------------------------\n\n【30】The accuracy of the logistic-regression and super learner models was cross-validated for the predictability of measured baseline PRNT <sub>50 </sub> serostatus; the methods produced similar results, with 79% of the participants who had been predicted to be seronegative by each method confirmed to be seronegative on the basis of measured PRNT <sub>50 </sub> titers . The analysis of concordance between the anti-NS1 titers at month 13 and PRNT <sub>50 </sub> titers at month 0 and the analysis of the effect of CYD-TDV on the anti-NS1 titers at month 13 are provided in Table S8 and pages 19 through 23, respectively, in the Supplementary Appendix .\n\n【31】Safety and Efficacy Estimates\n-----------------------------\n\n【32】Because the estimates calculated by the different analytic approaches were generally consistent with one another, pooled estimates based on the multiple-imputation approach (which is more commonly used for handling issues with missing data) from month 0 onward are reported unless indicated otherwise; estimates from all the methods we used are provided in the tables, figures, and Supplementary Appendix . Exploratory analyses showed statistical evidence of interaction between serostatus and treatment effect for the safety and efficacy end points (P<0.01 for all comparisons) and support the separate presentation of estimates for seronegative and seropositive populations. No statistical evidence of interaction between age groups (2 to 8 years and 9 to 16 years of age) and treatment effect on the safety end points in seronegative participants was observed in exploratory analyses, and therefore P values for these end points are reported only for the analyses involving participants 2 to 16 years of age.\n\n【33】### _Risk Associated with Vaccination at 9 to 16 Years of Age_\n\n【34】Figure 1. Risk of Hospitalization for Virologically Confirmed Dengue (VCD) and of Severe VCD in Participants 9 to 16 Years of Age, According to Baseline Serostatus.\n\n【35】Dengue serostatus was assigned on the basis of multiple imputation (MI, month 0 onward), targeted minimum loss–based estimation (TMLE, month 0 onward), and measured anti–nonstructural protein 1 (NS1) titer (threshold for positivity, 9 enzyme-linked immunosorbent assay \\[ELISA\\] units per milliliter \\[T9\\]; month 13 onward). Hazard ratios (MI and NS1) and relative risks (TMLE) are shown with corresponding 95% confidence intervals. For NS1, numerators represent the number of participants who were hospitalized for VCD or had severe VCD, and the denominators are the total numbers of participants selected in the subcohort. For MI, the numerators and denominators are the means of 10 iterations of MI, with the numerator representing the number of participants who were hospitalized for VCD or had severe VCD and the denominator representing the total number of participants selected in the subcohort. For TMLE, the numerators are the predicted numbers of study group–specific events among participants of the given serostatus within the subcohort, and the denominators are the predicted numbers of participants of a given serostatus within the subcohort. The analysis involved the as-treated population, in which participants were classified as being in the vaccine group if they had received at least one injection of CYD-TDV. Data were pooled from the CYD14, CYD15, and CYD23 (and CYD57) trials.\n\n【36】Among seronegative participants 9 to 16 years of age, the hazard ratio (vaccine vs. control) for hospitalization for VCD was 1.41 (95% confidence interval \\[CI\\], 0.74 to 2.68) and that for severe VCD was 2.44 (95% CI, 0.47 to 12.56); the point estimates of the hazard ratio and relative risk were greater than 1 for all methods in the pooled analyses . Through month 60 among seronegative participants, the cumulative incidence of hospitalization for VCD was 1.57% (95% CI, 1.13 to 2.19) in the vaccine group and 1.09% (95% CI, 0.53 to 2.27) in the control group, and the incidence of severe VCD was 0.40% (95% CI, 0.22 to 0.75) in the vaccine group and 0.17% (95% CI, 0.04 to 0.83) in the control group. Additional details of the results for seronegative participants, as well as exploratory analyses for each trial, are provided in Tables S10 through S12 in the Supplementary Appendix .\n\n【37】Among seropositive participants, the hazard ratio (vaccine vs. control) for hospitalization for VCD was 0.21 (95% CI, 0.14 to 0.31) and that for severe VCD was 0.16 (95% CI, 0.07 to 0.37); the point estimates of the hazard ratios and relative risk were less than 1 with all methods in the pooled analyses  and in the individual trials . Table S14 in the Supplementary Appendix shows exploratory analyses involving seropositive participants who were 9 to 11 years and 12 to 16 years of age. Through month 60 among seropositive participants, the cumulative incidence of hospitalization for VCD was 0.38% (95% CI, 0.26 to 0.54) in the vaccine group and 1.88% (95% CI, 1.54 to 2.31) in the control group, and the cumulative incidence of severe VCD was 0.08% (95% CI, 0.03 to 0.17) in the vaccine group and 0.48% (95% CI, 0.34 to 0.69) in the control group .\n\n【38】The attributable risk over a 60-month period per 1000 seronegative vaccine recipients was 4.78 (95% CI, −13.99 to 24.00) for hospitalization for VCD and 2.30 (95% CI, −7.00 to 10.67) for severe VCD. The corresponding attributable risk per 1000 seropositive vaccine recipients was −15.08 (95% CI, −25.44 to −4.97) and −4.05 (95% CI, −9.59 to 0.63), respectively.\n\n【39】### _Risk Associated with Vaccination at 2 to 8 Years of Age_\n\n【40】Figure 2. Risk of Hospitalization for VCD and of Severe VCD in Participants 2 to 8 Years and 2 to 16 Years of Age, According to Baseline Serostatus.\n\n【41】Dengue serostatus was categorized on the basis of MI (month 0 onward), TMLE (month 0 onward), and NS1 (threshold for positivity, 9 ELISA units per milliliter \\[T9\\]; month 13 onward). Hazard ratios (MI and NS1) and relative risks (TMLE) are shown with corresponding 95% confidence intervals. For NS1, numerators represent the number of participants who were hospitalized for VCD or had severe VCD, and the denominators are the total numbers of participants selected in the subcohort. For MI, the numerators and denominators are means from 10 iterations of MI, with the numerator representing the number of participants who were hospitalized for VCD or had severe VCD and the denominator representing the total number of participants selected in the subcohort. For TMLE, the numerators are the predicted numbers of study group–specific events among participants of the given serostatus within the subcohort, and the denominators are the predicted numbers of participants of a given serostatus within the subcohort. The analysis involved the as-treated population, in which participants were classified as being in the vaccine group if they had received at least one injection of CYD-TDV. Data were pooled from the CYD14, CYD15, and CYD23 (and CYD57) trials. For the analysis involving participants who were 2 to 16 years of age, the unadjusted and adjusted P values for the comparisons between the vaccine group and the control group among seronegative participants were as follows: MI method for hospitalization for VCD, P=0.01 (unadjusted) and P=0.02 (Holm–Bonferroni adjusted); MI method for severe VCD, P=0.03 (unadjusted) and P=0.03 (Holm–Bonferroni adjusted); TMLE method for hospitalization for VCD, P=0.07 (unadjusted) and P=0.09 (Holm–Bonferroni adjusted); and TMLE method for severe VCD, P=0.04 (unadjusted) and P=0.09 (Holm–Bonferroni adjusted). Holm–Bonferroni adjustment was performed for the two safety end points (two tests) independently for each method.\n\n【42】Among seronegative participants 2 to 8 years of age, the hazard ratio (vaccine vs. control) for hospitalization for VCD was 1.95 (95% CI, 1.19 to 3.19) and that for severe VCD was 3.31 (95% CI, 0.87 to 12.54) ; the point estimates were greater than 1 for all methods in pooled analyses and in individual trials. Among seropositive participants, the corresponding hazard ratios were 0.50 (95% CI, 0.33 to 0.77) and 0.58 (95% CI, 0.26 to 1.30) , and the point estimates were less than 1 in all analyses. Additional results, including the estimates of cumulative incidence and attributable risk and the results of exploratory analyses involving seronegative participants who were 2 to 5 years and 6 to 8 years of age, are shown in Tables S11 and S14 through S18 in the Supplementary Appendix ).\n\n【43】### _Risk Associated with Vaccination at 2 to 16 Years of Age_\n\n【44】Among seronegative participants who were 2 to 16 years of age, the hazard ratio (vaccine vs. control) for hospitalization for VCD was 1.75 (95% CI, 1.14 to 2.70) and that for severe VCD was 2.87 (95% CI, 1.09 to 7.61) ; all point estimates were greater than 1. Among seronegative participants, the cumulative incidence of hospitalization for VCD through month 60 was 3.06% (95% CI, 2.53 to 3.61) among vaccine recipients and 1.87% (95% CI, 1.23 to 2.86) among controls. Among seropositive participants, the corresponding hazard ratios were 0.32 (95% CI, 0.23 to 0.45) and 0.31 (95% CI, 0.17 to 0.58) , and point estimates were less than 1 in all analyses. Among seropositive participants, the cumulative incidence of hospitalization for VCD through month 60 was 0.75% (95% CI, 0.56 to 1.00) among vaccine recipients and 2.47% (95% CI, 2.09 to 2.92) among controls. Additional results are provided in Tables S19 through S22 in the Supplementary Appendix .\n\n【45】### _Risk over Time_\n\n【46】Figure 3. Cumulative Incidence Curves of Hospitalization for VCD from Month 0 According to Baseline Serostatus as Classified by PRNT <sub>50 </sub> at Baseline (Multiple-Imputation Approach) in Different Age Groups.\n\n【47】Data are from a pooled analysis of the CYD14, CYD15, and CYD23 (and CYD57) trials. The cumulative incidence curves are curtailed at month 66 to ensure that at least 20% of the participants remained at risk in each subcohort. Insets show the same data on an enlarged y axis.\n\n【48】Among seronegative participants, the hazard ratio (vaccine vs. control) for hospitalization for VCD was greater than 1 as estimated with most methods during the hospital phase (month 25 onward) in participants who were 9 to 16 years of age . The risk estimates according to time period among participants who were 2 to 8 years or 2 to 16 years of age are shown in Tables S24 and S25 in the Supplementary Appendix . There was an excess risk of hospitalization for VCD in seronegative vaccine recipients as compared with seronegative controls from month 30 onward among those who were 9 to 16 years of age and from month 18 onward among those who were 2 to 8 years of age . Among seropositive participants, the cumulative risk was lower in the vaccine group than in the control group throughout follow-up .\n\n【49】### _Clinical Profile of Cases and Risk According to Dengue Virus Serotype_\n\n【50】Table 1. Clinical Signs and Symptoms in All Hospitalizations for Virologically Confirmed Dengue (VCD) Occurring from Month 13 to the End of the Follow-up Period (Month 60 to Month 72) among Seronegative Participants.\n\n【51】In all age groups, the median duration of fever, symptoms, and hospitalization did not differ between hospitalized patients with VCD in the vaccine group and those in the control group. A higher risk of plasma leakage and severe thrombocytopenia (platelet count, <50×10  per liter) was found in the vaccine group . The overall clinical picture among patients with severe VCD was similar in the two study groups, and most cases were dengue hemorrhagic fever (DHF) grade I or II, as defined in the World Health Organization (WHO) 1997 classification . All the affected participants recovered. There were no dengue-related deaths. The all-cause mortality rate in all the trials combined was 0.24% (0.21% in the vaccine group and 0.30% in the control group), and no deaths from any cause were judged by the investigators and the sponsor to be related to the vaccine. No differences in the symptomatology between cases in seronegative vaccine recipients and those in seropositive controls were found . Among seronegative participants, the hazard ratio or relative risk (vaccine vs. control) of hospitalization for VCD caused by serotype 1 or 3 dengue virus was greater than 1 in some analyses, and these ratios were consistently greater than 1 for hospitalization for VCD caused by serotype 2 dengue virus. Among seropositive participants, the hazard ratios and relative risks of hospitalization for VCD were less than 1 for dengue due to each of the four serotypes .\n\n【52】### _Vaccine Efficacy against Symptomatic VCD_\n\n【53】Figure 4. Vaccine Efficacy against Symptomatic VCD up to Month 25 According to Baseline Serostatus in Different Age Groups.\n\n【54】Dengue serostatus was categorized on the basis of MI (month 0 onward), TMLE (month 0 onward), and NS1 (threshold for positivity, 9 ELISA units per milliliter \\[T9\\]; month 13 onward). Vaccine efficacy estimates are shown with corresponding 95% confidence intervals. For NS1, the numerators represent the number of participants who had symptomatic VCD and the denominators represent the total participants selected in the subcohort; estimates are from month 13 to month 25. For MI, the numerators and denominators are means from 10 iterations of MI, with the numerator representing the number of participants with symptomatic VCD and the denominator representing the total number of participants selected in the subcohort; estimates are from month 0 to month 25. For TMLE, the numerators are the predicted numbers of study group–specific events among participants of the given serostatus within the subcohort and the denominators are the predicted numbers of participants of a given serostatus within the subcohort; estimates are from month 0 to month 25. The analysis involved the intention-to-treat population, with participants included in the group (vaccine or control) to which they had been randomly assigned. Data were pooled from the CYD14 and CYD15 trials.\n\n【55】Among seronegative participants, vaccine efficacy against symptomatic VCD (up to month 25) was 39% (95% CI, −1 to 63) among those who were 9 to 16 years of age, 19% (95% CI, −47 to 55) among those who were 2 to 8 years of age, and 32% (95% CI, −9 to 58) among those who were 2 to 16 years of age. Among seropositive participants, the corresponding values were 76% (95% CI, 64 to 84), 60% (95% CI, 31 to 76), and 73% (95% CI, 59 to 82) . Vaccine efficacy as determined by all other methods and according to trial, serotype, and age stratum are shown in Tables S34 through S43 in the Supplementary Appendix .\n\n【56】Discussion\n----------\n\n【57】Using a new assay and several analytic methods, we reanalyzed serum samples from three efficacy trials to further characterize the safety and efficacy of CYD-TDV according to dengue serostatus at the time of vaccination. We found that among dengue-seropositive participants, vaccination conferred protection against subsequent disease for at least 5 years; the rates of severe VCD and hospitalization for VCD over a 5-year period for all the ages considered (2 to 16 years of age) were approximately 70% lower in the vaccine group than in the control group, and among those 9 years of age or older at vaccination (the vaccine-indicated age group), the rates of severe VCD and hospitalization for VCD were approximately 80% lower in the vaccine group than in the control group.\n\n【58】Among dengue-seronegative participants, however, over the same period, the rates of hospitalization for VCD and of severe VCD were higher in the vaccine group than in the control group. A trend toward a higher risk of hospitalization for VCD in association with vaccination was found among seronegative participants who were 9 to 16 years of age, and a significantly higher risk was found among seronegative participants who were 2 to 8 years of age. Among seronegative vaccine recipients 9 to 16 years of age, the onset of a higher risk of hospitalization for VCD occurred mainly during the third year after the first vaccination, whereas among younger seronegative vaccine recipients, the higher risk seemed to start earlier. Our findings indicate a major role for previous dengue exposure in modifying vaccine performance and provide some evidence of a possible age effect. However, since age is associated with dengue exposure, it remains unclear whether these findings reflect undetected dengue exposure that was not captured by the assays (i.e., false seronegatives) or age-specific differences after vaccination or infection. \n\n【59】Most participants did not have severe VCD during the trial; the rates of severe VCD were low among both seronegative vaccine recipients and seropositive controls, which indicated the rarity of severe dengue even in areas of high endemicity. On the basis of our data, the population-level effect of vaccination would be an excess of hospitalizations for dengue and of severe cases in seronegative persons, as well as a lower rate of these events among seropositive persons. Extrapolating the estimates of attributable risk to a cohort of 1 million people 9 to 16 years of age with an 80% rate of seropositivity suggests that, over a period of 5 years, vaccination would prevent approximately 11,000 hospitalizations (12,000 avoided among seropositive persons with 1000 excess among seronegative persons) and approximately 2500 severe cases (3000 avoided among seropositive persons and 500 excess among seronegative persons). However, these numbers should be interpreted cautiously, since they reflect the epidemiologic contexts of the clinical trials and are expected to differ in other contexts and over time (because factors affecting population-level vaccine performance, such as the baseline rate of dengue seropositivity, force of infection, and incidence of infection, may change). Dynamic transmission models can help provide an understanding of the interactions among incidence, seroprevalence, and time-dependent factors. \n\n【60】Symptomatic, nonsevere dengue is an important condition with a substantial outpatient burden.  Among seropositive persons, we found high efficacy of the vaccine against symptomatic VCD (up to month 25), with low-to-modest efficacy suggested among seronegative vaccine recipients. These findings are consistent with previous observations based on measured PRNT <sub>50 </sub> titers at baseline in the immunogenicity subset from the trials. \n\n【61】Our findings support the hypothesis that, in the absence of previous dengue exposure, the CYD-TDV vaccine partially mimics primary infection and increases the risk of severe dengue during subsequent infection, similar to the risk that is observed epidemiologically in association with a natural second dengue infection. One notable difference is that the risk of natural secondary dengue infection is associated with naturally acquired monotypic antibodies, whereas the observed risk with CYD-TDV vaccination occurs after induction of multitypic antibody responses. Nevertheless, the pattern of risk we found is consistent with the previous hypothesis of a clustering of the risk related to vaccination time.  We found no meaningful clinical differences in the symptomatology of severe cases among seronegative vaccine recipients (37 cases among those who were 2 to 16 years of age), seronegative controls (5 cases), and seropositive controls (44 cases); the small numbers make these comparisons fragile at best.\n\n【62】The immunopathogenic mechanisms underlying these findings remain unknown. Although antibody-dependent enhancement has been proposed as a mechanistic basis of the increased risk of severe dengue associated with a subsequent second dengue infection,  and recent evidence from a longitudinal cohort study of natural dengue infection suggests an increased risk in the presence of low antibody titers,  our study did not specifically investigate whether antibody-dependent enhancement, other pathogens, or host or environmental factors played a role. However, these observations may be partly explained by differences in vaccine performance according to dengue serotype.\n\n【63】Overall, the general consistency of the results obtained with different analytic methods supports our findings. However, each approach relies on assumptions. Both multiple imputation and targeted minimum loss–based estimation rely on the “missing-at-random” assumption, which, although unverifiable, we think is likely to hold by design.  Multiple imputation has well established operating characteristics but relies on correctly specifying a model for the month 0 serostatus probabilities. Targeted minimum loss–based estimation is “doubly robust” in that it incorporates both the conditional month 0 serostatus probability and the probability of having missing data while relying on only one of these probabilities being correctly specified. However, in relying on weaker assumptions, it can produce larger standard errors than multiple imputation.\n\n【64】Unlike multiple imputation and targeted minimum loss–based estimation, the month 13 NS1 method uses a measured proxy for month 0 serostatus that does not rely on correct model specification for the missing data. However, the categorization of month 13 NS1 serostatus according to a defined threshold is subject to differential misclassification and underestimation of vaccine efficacy and cannot account for events that occurred before month 13 . Our cross-validation results indicate that both logistic regression and super learner were highly predictive of observed month 0 serostatus. Although serostatus predictions were most accurate for analyses that excluded events that occurred from month 0 to month 13, analyses from month 0 onward account for vaccine effects between month 0 and month 13 and preserve the benefits of randomization.\n\n【65】Beyond the methodologic considerations mentioned, there are some important caveats: although we used a new assay after characterization and qualification, neither the anti-NS1 assay nor the PRNT is validated or registered for the purpose of determining baseline dengue serostatus. Serostatus classification relies heavily on the performance characteristics of the assays used, as well as on pretest probabilities of the condition of interest. Therefore, predictive values are expected to vary in different epidemiologic contexts. In this post hoc study, power was predetermined by the number of cases that had been observed in the original trials, and small-to-moderate effects might have been missed because of the limited numbers of some safety end points.\n\n【66】Our findings could affect the implementation of dengue vaccination programs. In December 2017, the WHO published an interim position addressing this new information, and the WHO Strategic Advisory Group and Experts (SAGE) made recommendations available after their April 2018 meeting.  A reliable, rapid test to determine previous dengue exposure would be ideal; however, no such test has been widely registered for this indication, and prevaccination screening in large programs could be challenging to implement.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "72ada470-9d02-4376-9b2e-c5fccda271d8", "title": "Circulating Osteoblast-Lineage Cells in Humans", "text": "【0】Circulating Osteoblast-Lineage Cells in Humans\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Although current evidence suggests that only a minuscule number of osteoblast-lineage cells are present in peripheral blood, we hypothesized that such cells circulate but that their concentration has been vastly underestimated owing to the use of assays that required adherence to plastic. We further reasoned that the concentration of these cells is elevated during times of increased bone formation, such as during pubertal growth.\n\n【3】Methods\n-------\n\n【4】We used flow cytometry with antibodies to bone-specific proteins to identify circulating osteoblast-lineage cells in 11 adolescent males and 11 adult males (mean \\[±SD\\] age, 14.5±0.7 vs. 37.7±7.6 years). Gene expression and in vitro and in vivo bone-forming assays were used to establish the osteoblastic lineage of sorted cells.\n\n【5】Results\n-------\n\n【6】Cells positive for osteocalcin and cells positive for bone-specific alkaline phosphatase were detected in the peripheral blood of adult subjects (1 to 2 percent of mononuclear cells). There were more than five times as many cells positive for osteocalcin in the circulation of adolescent boys (whose markers of bone formation were clearly increased as a result of pubertal growth) as compared with adult subjects (P<0.001). The percentage of cells positive for osteocalcin correlated with markers of bone formation. Sorted osteocalcin-positive cells expressed osteoblastic genes, formed mineralized nodules in vitro, and formed bone in an in vivo transplantation assay. Increased values were also found in three adults with recent fractures.\n\n【7】Conclusions\n-----------\n\n【8】Osteoblast-lineage cells circulate in physiologically significant numbers, correlate with markers of bone formation, and are markedly higher during pubertal growth; therefore, they may represent a previously unrecognized circulatory component to the process of bone formation.\n\n【9】Introduction\n------------\n\n【10】Bone marrow contains both osteoblast and osteoclast precursors that can differentiate into the mature osteoblasts and osteoclasts, respectively,  that are believed to be needed for normal bone remodeling on trabecular surfaces contiguous to bone marrow. In addition to residing in the bone marrow, substantial numbers of osteoclast precursors are detectable in the peripheral circulation,  and these cells may be able to travel to sites of active bone remodeling distant from red marrow. However, it is unclear whether a parallel process involving circulating osteoblast-lineage cells exists and, if so, what role these osteoblastic cells might play in normal or pathologic bone remodeling.\n\n【11】Osteoblastic cells in the bone marrow have traditionally been identified by their adherence to standard tissue-culture plastic and by the subsequent formation of mineralized nodules.  Using these criteria, previous investigators have provided evidence that osteoblastic cells that adhere to plastic are, indeed, present in the circulation in some species,  but in exceedingly low numbers (less than 1 in 10  peripheral-blood mononuclear cells in adult humans  ).\n\n【12】In addition to these adherent osteoblastic cells, Long et al.  have identified a population of cells in the bone marrow that have osteogenic potential and are nonadherent and positive for osteocalcin, bone-specific alkaline phosphatase, or both. Moreover, a recent study showed that bone marrow cells that do not adhere to plastic have bone-repopulating activity in lethally irradiated mice that is more than 10 times as great as that of adherent cells from the marrow.  If nonadherent osteogenic bone marrow cells are present in the peripheral circulation, assays that rely on adherence to plastic would detect only the rare adherent circulating osteoblastic cell,  resulting in a substantial underestimation of the actual number of osteoblastic cells in the circulation.\n\n【13】We reasoned that the use of flow cytometry with antibodies to bone-specific proteins (osteocalcin and alkaline phosphatase) would better identify circulating osteoblast-lineage cells. Furthermore, if these circulating osteoblastic cells play a physiologic role in bone formation, their concentration in peripheral blood would increase under conditions of increased bone formation. Therefore, we determined the concentration of these cells in the circulation in adult men as compared with adolescent boys, who had markedly increased bone-formation indexes, since they were going through the pubertal growth spurt.\n\n【14】Methods\n-------\n\n【15】Study Subjects\n--------------\n\n【16】Study subjects included 11 consecutively recruited healthy adult males 28 to 49 years of age and 11 consecutively recruited adolescent boys 13 to 15 years of age. For some studies, buffy coats were obtained from male subjects at the Mayo Transfusion Center. All studies were approved by the Mayo Clinic's institutional review board, and all subjects, or their parents, provided written informed consent; subjects less than 18 years of age provided assent. The study was conducted between May 2003 and January 2005.\n\n【17】Serum Biochemical and Hormonal Measurements\n-------------------------------------------\n\n【18】Serum osteocalcin was measured with a two-site immunoradiometric assay (ELSA-OSTEO, CisBio) (interassay coefficient of variation, less than 12 percent), and serum bone alkaline phosphatase was measured by immunoassay (Quidel) (interassay coefficient of variation, less than 8 percent). Serum insulin-like growth factor I (IGF-I) and IGF-binding protein 3 were measured with immunoradiometric assays (Diagnostic Systems Laboratories) (interassay coefficient of variation, less than 15 percent for both). Serum testosterone was measured by a competitive immunoassay (Access Immunoassay System, Beckman Coulter) (coefficient of variation, less than 10 percent), and serum estradiol was measured with a double antibody radioimmunoassay (Diagnostic Products) (coefficient of variation, less than 8 percent).\n\n【19】Extraction and Immunostaining of Mononuclear Cells\n--------------------------------------------------\n\n【20】Samples of whole blood or buffy coats were layered over Ficoll-Paque density gradients, and mononuclear cells were isolated and subsequently processed for immunostaining as previously described.  Primary antibodies were a monoclonal anti–bone alkaline phosphatase (B4-78 hybridoma, Developmental Studies Hybridoma Bank, University of Iowa) or a goat polyclonal antihuman osteocalcin (Santa Cruz Biotechnology) antibody. Control isotype antibodies were used at the same concentrations as the primary antibodies. An anti-CD15 antibody (Becton Dickinson), a granulocyte-specific marker, excluded contamination of isolated cells with granulocyte lineages. After incubation with primary antibodies, cells were stained with conjugated secondary antibodies including phycoerythrin-conjugated AffinityPure f(ab') <sub>2 </sub> fragment donkey antimouse IgG and fluorescein isothiocyanate–conjugated AffinityPure IgG f(ab') <sub>2 </sub> fragment donkey antigoat (Jackson ImmunoResearch) antibodies.\n\n【21】Flow Cytometry and Fluorescence-Activated Cell-Sorter Analysis\n--------------------------------------------------------------\n\n【22】We used the procedure for flow cytometry that was previously described by our laboratory.  For some studies, mononuclear cells stained with the anti-osteocalcin or anti–bone alkaline phosphatase antibody and the corresponding secondary antibody were also sorted by flow cytometry (FACSVantage SE, Becton Dickinson) directly into a growth medium consisting of MesenCult Basal Medium supplemented with 15 percent osteogenic fetal-calf serum (StemCell Technologies).\n\n【23】Primary Culture of Osteoblastic Cells\n-------------------------------------\n\n【24】Sorted cells were suspended in growth medium and plated at a density of 1×10  cells per well in fibronectin-coated plates (Becton Dickinson). On days 8 and 14 after plating, supernatants that contained nonadherent cells were centrifuged, the media were changed, and nonadherent cell pellets were returned to the original wells. On day 21 and then weekly thereafter, media were changed to the differentiation medium (MesenCult Basal Medium with 15 percent osteogenic stimulatory supplements, further supplemented with a 3.5 mM final concentration of β-glycerophosphate, 10 <sup>–8 </sup> M dexamethasone, and 50 μg per milliliter of ascorbic acid \\[StemCell Technologies\\]), each time preserving nonadherent cells. After three weeks in the differentiation medium, cells were fixed and stained with von Kossa's stain to detect any calcified nodules.\n\n【25】RNA Extraction and Real-Time Reverse-Transcriptase–Polymerase-Chain-Reaction (Rt-PCR) Analysis\n----------------------------------------------------------------------------------------------\n\n【26】Total RNA was extracted from cells collected by fluorescence-activated cell sorting with Absolutely RNA Microprep kit (Stratagene). A complementary-DNA template was constructed with the use of AMV Reverse Transcriptase, Random Primer p(dN) <sub>6 </sub> , RNase inhibitor, deoxynucleoside triphosphates, and oligodT (Roche). PCR analyses were performed on the iCycler iQ/Real-Time PCR Detection System apparatus with a 2X iQ SYBR Green Supermix (Bio-Rad). Gene expression was normalized to glyceraldehyde-3-phosphate dehydrogenase.\n\n【27】In Vivo Bone-Forming Assay\n--------------------------\n\n【28】Osteocalcin-positive or osteocalcin-negative peripheral-blood cells that had been sorted by fluorescence-activated cell sorting, as well as unsorted peripheral-blood mononuclear cells (10  cells each), were suspended in 1 ml of alpha-Dulbecco's modified Eagle's medium (Invitrogen) and mixed with 40 mg of hydroxyapatite and tricalcium phosphate powder (in a  ratio; particle size, 0.1 to 0.5 mm; Berkeley Advanced Biomaterials). A secondary matrix was formed by mixing 15 μl of mouse fibrinogen (3.2 mg per milliliter in phosphate-buffered saline) and 15 μl of mouse thrombin (25 U per milliliter in 2 percent calcium chloride; Sigma-Aldrich) to form a solid fibrin gel. The transplants were placed in subcutaneous pockets in an eight-week-old immunocompromised mouse (Beige Nude XID, Harlan). After eight weeks, we looked at radiographs of the pellets, and then the pellets were recovered, placed in 70 percent ethanol, and analyzed with the use of quantitative computed tomography  (Stratec XCT Research SA+, Orthometrix).\n\n【29】Histology\n---------\n\n【30】The pellets were dehydrated in 95 percent ethanol for one day and then in 100 percent ethanol for five days before they were immersed for four days in glycol methylmethacrylate and embedded by controlled polymerization. The samples were then cut into 5-μm sections and stained with Goldner's stain. \n\n【31】Statistical Analysis\n--------------------\n\n【32】All data are presented as means ±SD. Comparisons between the adult and adolescent subjects were performed with the use of unpaired t-tests. Spearman rank correlations were used to relate serum biochemical and hormonal measurements to the percentage of cells positive for osteocalcin or bone alkaline phosphatase. A P value of less than 0.05 was considered significant, and all reported P values are two-sided.\n\n【33】Results\n-------\n\n【34】Table 1. Age and Serum Biochemical and Hormonal Data in the 11 Adult and 11 Adolescent Males.\n\n【35】Table 1 shows the biochemical and hormonal data of the study subjects. Owing to the dramatic increase in bone formation during pubertal growth, the adolescent boys had significantly increased indexes of bone formation as well as increased serum levels of IGF-I and IGF-binding protein 3, reflecting increased secretion of growth hormone.  Serum testosterone levels were similar in the two groups, whereas serum estradiol levels were lower in the adolescent boys than in the adult men.\n\n【36】Figure 1. Mean (±SD) Percentage of Circulating Cells Positive for Osteocalcin  and Bone Alkaline Phosphatase  in Adolescent and Adult Males.\n\n【37】As shown in Figure 1 , cells positive for osteocalcin, but not cells positive for bone alkaline phosphatase, were more than five times as great in the circulation of the adolescent boys as in the adult men (mean percentage, 5.13 vs. 0.93). Moreover, in the two groups of subjects combined, the percentage of osteocalcin-positive cells significantly correlated with serum levels of osteocalcin and bone alkaline phosphatase (r=0.72 and r=0.77, respectively; P<0.001 for both). The percentage of osteocalcin-positive cells was also positively correlated with serum levels of IGF-I and IGF-binding protein 3 (r=0.74, P<0.001, and r=0.61, P<0.01, respectively). Although serum testosterone levels were not correlated with the percentage of osteocalcin-positive cells (r=–0.12, P=0.62), there was a suggestion of an inverse association between these cells and serum estradiol levels (r=–0.40, P=0.09). No significant associations were noted between markers of bone turnover and the percentage of cells positive for bone alkaline phosphatase, or between the hormonal variables and the percentage of cells positive for bone alkaline phosphatase (data not shown).\n\n【38】In addition to the increases in bone formation during the adolescent growth spurt, there are also significant increases after a fracture.  One adult subject in this study had a severe fracture of the distal forearm six weeks after his initial data were collected. Thus, we had the serendipitous opportunity to examine sequential changes in his osteocalcin-positive and bone alkaline phosphatase–positive cells before and after the fracture. A blood sample obtained 20 days after the fracture showed a more than ninefold increase in circulating osteocalcin-positive cells (from 0.6 to 5.9 percent) and a twofold increase in circulating cells positive for bone alkaline phosphatase (from 1.9 to 4.1 percent) as compared with the percentages at baseline.\n\n【39】We subsequently studied two additional men, 40 and 34 years of age, 10 days after fractures of the distal forearm and tibial shaft, respectively. The proportions of circulating osteocalcin-positive cells in these subjects were 3.7 and 2.2 percent, respectively, and those of bone alkaline phosphatase–positive cells were 3.0 and 8.0 percent, respectively. The combined mean values for cells positive for osteocalcin and bone alkaline phosphatase in the three patients studied after fractures were 3.9 percent and 5.0 percent, respectively. Relative to the normal means for these cells in age-matched subjects , these percentages represent approximately fourfold and twofold increases, respectively.\n\n【40】We performed a number of studies to characterize more completely the cells that were positive for osteocalcin and bone alkaline phosphatase. Since platelets can express low levels of osteocalcin,  the possibility that platelets confounded the results was addressed by testing a sample of platelet-rich plasma and applying the appropriate gates to the flow cytometry data for mononuclear cells, as was done in all analyses.  None of the cells in this gate setting expressed osteocalcin (data not shown). Similarly, granulocyte-lineage cells in peripheral blood express alkaline phosphatase.  However, the gating around mononuclear cells effectively excluded virtually all CD15+ cells (a granulocyte marker), and double staining with an anti-CD15 antibody and the anti–bone alkaline phosphatase antibody indicated that less than 0.1 percent of the mononuclear cells coexpressed both markers.\n\n【41】Figure 2. Formation of Mineralized Nodules in Vitro by Osteocalcin-Positive Cells.\n\n【42】Sorted osteocalcin-positive cells  and unsorted cells  were cultured for three weeks to expand cell numbers. Nonadherent cells were preserved with each change of medium. The osteocalcin-positive and unsorted cells were then placed in osteoblast-differentiation medium, where the osteocalcin-positive cells formed mineralized nodules, shown as the dark areas in Panels A and B (von Kossa's stain).Figure 3.  Figure 3. Expression of the Bone-Related Genes — Osteocalcin, Bone Alkaline Phosphatase, and Collagen Type I (α2 Chain) — in Osteocalcin-Positive Cells, Unsorted Cells, and Osteocalcin-Negative Cells.\n\n【43】The bands are shown after amplification with use of conventional reverse-transcriptase polymerase chain reaction (RT-PCR). Also shown is the simultaneous quantitation of the messenger RNAs with real-time RT-PCR normalized to glyceraldehyde-3-phosphate dehydrogenase (GAPDH) and expressed as the enrichment ratio in osteocalcin-positive cells as compared with unsorted cells  and in osteocalcin-positive cells as compared with osteocalcin-negative cells .\n\n【44】Since the percentages of circulating osteocalcin-positive cells were markedly increased during the adolescent growth spurt, we performed further studies of these cells. The ability of sorted osteocalcin-positive cells to form mineralized nodules in vitro was assessed. Culture of these cells required the preservation of nonadherent cells. However, the majority of the cells gradually became adherent, and when osteocalcin-positive cells were placed in osteoblast-differentiation medium, they could form mineralized nodules . In addition, freshly sorted osteocalcin-positive cells, as compared with unsorted cells and as compared with osteocalcin-negative cells, were markedly enriched for expression of the bone-related genes — osteocalcin, bone alkaline phosphatase, and collagen type I (α2 chain) .\n\n【45】Figure 4. Evidence of the Osteogenic Potential of Osteocalcin-Positive Cells Implanted into Immunocompromised Mice.\n\n【46】Panel A shows a radiograph of an immunocompromised mouse that was implanted with cell suspensions containing osteocalcin-positive cells, osteocalcin-negative cells, and unsorted cells. The numbers indicate the volumetric bone mineral density of the cell suspensions, measured by quantitative computed tomography. In Panel B, Goldner's staining, in which turquoise indicates mineralized bone, highlights histologic sections of the cell suspensions containing osteocalcin-positive cells (left image), osteocalcin-negative cells (center image), and unsorted cells (right image). Panel C shows a higher-power view of the histologic sections of the cell suspensions containing the osteocalcin-positive cells, showing clear areas of bone formation, which, under polarized light , demonstrate the presence of lamellar bone.\n\n【47】An in vivo transplantation assay in which osteocalcin-positive, osteocalcin-negative, or unsorted peripheral-blood mononuclear cells were implanted subcutaneously into immunocompromised mice was used to obtain further evidence of the osteogenic potential of the osteocalcin-positive cells. As shown in Figure 4A , both radiography and quantitative computed tomography showed higher radiodensity and volumetric bone mineral density in the area of the osteocalcin-positive cell suspensions than in the osteocalcin-negative cells. Histologic examination of the implanted cell suspensions showed clear evidence of mineralized bone formation by the osteocalcin-positive cells, which was lacking in the osteocalcin-negative cells . Figure 4C is a higher-power view of the mineralized bone formed in the osteocalcin-positive implant. In the same sample, polarized light shows the presence of lamellar bone .\n\n【48】Finally, additional studies were performed to characterize circulating cells positive for bone alkaline phosphatase. Gene expression studies using real-time RT-PCR indicated that cells positive for bone alkaline phosphatase were also enriched for bone-related genes (2-fold for osteocalcin, >200-fold for bone alkaline phosphatase, and 5-fold for collagen type I, relative to unsorted cells). In addition, in preliminary studies, these cells seem to be able to be induced to form mineralized nodules in vitro, similar to the osteocalcin-positive cells (data not shown).\n\n【49】Discussion\n----------\n\n【50】The present study provides clear evidence that osteoblast-lineage cells are present in the human circulation in significant numbers and that the percentage of circulating osteocalcin-positive cells is higher during the adolescent growth spurt, which is associated with a marked stimulation of bone formation, than in adulthood.  Previous studies showed the presence of these cells in peripheral blood in humans,  but at exceedingly low concentrations. Indeed, in this study, we detected concentrations of these cells in the human circulation approximately six orders of magnitude greater than previously measured.  The principal reason for this difference is that earlier studies required adherence to plastic as a criterion for selecting these cells.  Since the majority of circulating osteoblast-lineage cells may lack the capacity to adhere to plastic, we used a different approach, involving the detection of bone-related proteins on the cell surface.\n\n【51】The number of circulating osteocalcin-positive cells correlated positively with serum levels of osteocalcin and bone alkaline phosphatase. Moreover, they correlated with serum levels of IGF-I and IGF-binding protein 3. Since IGF-I and IGF-binding protein 3 reflect growth hormone secretion,  it is possible that increases in circulating osteocalcin-positive cells during puberty are driven, at least in part, by IGF-I, growth hormone, or both. Although these associations collectively provide evidence in support of the biologic relevance of increases in circulating osteocalcin-positive cells during adolescence, further studies are needed to define more precisely the factors regulating the concentration of these cells in peripheral blood. In addition, whether the increase in circulating osteocalcin-positive cells during growth is due to an increase in the pool of precursors or to less programmed cell death is, at present, an open question.\n\n【52】The data from the three patients with fractures, which included sequential changes in the percentage of circulating osteoblastic cells before and after fracture in one patient, indicate that the concentration of both circulating cells positive for osteocalcin and cells positive for bone alkaline phosphatase may also increase after fractures. Why both populations of cells may be elevated after fracture but only populations of osteocalcin-positive cells increase in adolescent boys is unclear. Our findings suggest that the cells positive for osteocalcin and bone alkaline phosphatase probably represent somewhat different subpopulations of cells, indicating a need for further characterization.\n\n【53】Of the three patients with fractures, the one with the most clinically severe fracture had the highest percentage of osteocalcin-positive cells. Although studies are lacking that correlate the severity of fracture with the increase in circulating osteoblastic cells, as well as the time course of changes in these cells after fracture, the present findings raise the possibility that circulating cells positive for bone alkaline phosphatase, osteocalcin, or both play a functional role in healing fractures in portions of the peripheral skeleton not adjacent to red marrow. Indeed, studies in mice have shown that systemically infused bone marrow stromal cells can localize to the fracture callus and potentially participate in fracture repair. \n\n【54】The present findings, although new, might be anticipated, given the context of extensive work by Long and colleagues,  who showed that a population of nonadherent bone marrow cells expressing osteocalcin, bone alkaline phosphatase, or both on their surface also have the capacity to differentiate into mature osteoblasts. In addition, there is recent evidence of highly osteogenic cells in the nonadherent cell population from bone marrow.  It is plausible that the cells we have detected in peripheral blood are identical to the nonadherent osteogenic cells from the bone marrow that have previously been identified,  but further detailed characterization of both cell populations would be required to test this possibility. In addition, how the nonadherent osteocalcin-positive and bone alkaline phosphatase–positive cells differ from the classic, plastic-adherent osteoblastic cells from the bone marrow  remains to be better defined.\n\n【55】Our observations may resolve a fundamental inconsistency in the traditional view of bone remodeling that has arisen from the work of Hauge and colleagues.  It has long been known that remodeling in cancellous bone occurs on the surfaces of trabeculae, at the interface of the bone and bone marrow. Remodeling involves both osteoclasts and osteoblasts, which form the basic multicellular unit.  Since osteoblasts are known to differentiate from mesenchymal precursors in the bone marrow,  the traditional view held that osteoblasts traveled directly from the marrow to bone surfaces as the need arose in each basic multicellular unit. However, Hauge et al.  provided convincing histologic evidence that each basic multicellular unit has a roof of flattened cells that form a protective compartment over most sites of remodeling in cancellous bone, which raises the question of how osteoblastic cells in the marrow are able to gain access to the bone surface. Since the protective canopy of cells is often penetrated by capillary sinusoids of the marrow,  our data suggest that the previously identified nonadherent osteogenic cells from the marrow  may traverse these sinusoids and thus gain access to the basic multicellular unit. As part of this process, these cells probably also access the peripheral circulation.\n\n【56】In conclusion, the present study indicates that osteoblast-lineage cells are present in the human circulation in physiologically relevant numbers. Collectively, our data suggest a circulatory component to the process of bone formation and raise the possibility that these cells may also be involved in the healing of fractures. Moreover, circulating osteoblast-lineage cells might also have therapeutic potential. Developing a method to harvest these cells from peripheral blood, expand them in culture, and subsequently implant them into sites of impaired bone healing might lead to new approaches to bone regeneration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "87b40ac2-5a82-46f9-aeb6-c05bf30fa037", "title": "Association between Lupus Psychosis and Antiribosomal P Protein Antibodies", "text": "【0】Association between Lupus Psychosis and Antiribosomal P Protein Antibodies\nAbstract\n--------\n\n【1】In 18 of 20 patients with psychosis secondary to systemic lupus erythematosus (SLE), autoantibodies to ribosomal P proteins were detected by immunoblotting and measured with a new radioimmunoassay using a synthetic peptide as antigen.\n\n【2】The frequency of anti-P was not increased in patients with other central nervous system manifestations of SLE (3 of 20, by radioimmunoassay), in patients with transient behavioral abnormalities due to SLE (none of 8), in patients with psychosis who did not have SLE (none of 13), or in normal controls (none of 20). In four of five paired serum samples, anti-P-peptide antibody levels increased 5-fold to 30-fold during the active phase of lupus psychosis. Longitudinal studies of anti-P activity in two patients with psychosis revealed that anti-P levels increased before and during the active phases of psychosis but not during sepsis or other exacerbations of SLE, and that the elevations were selective for anti-P antibodies, as opposed to anti-DNA antibodies. Longitudinal studies of anti-P activity in two patients with anti-P but without psychosis showed less than threefold changes in anti-P levels despite exacerbations of disease.\n\n【3】We conclude that anti-P is associated with lupus psychosis and that synthetic peptide antigens may be useful for the detection and measurement of autoantibodies to intracellular proteins.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "89d85e57-a776-498e-96a0-61e4107c7c0f", "title": "Bilateral Atrial Myxomas Associated with Hyperpigmented Skin Lesions", "text": "【0】Bilateral Atrial Myxomas Associated with Hyperpigmented Skin Lesions\nTo the Editor:\n--------------\n\n【1】Figure 1. Lentigines and Ephelides (Freckles) on the Dorsum of the Hand of a 51-Year-Old Man. Figure 2.  Figure 2. Echocardiogram Showing Bilateral Atrial Tumors (Arrows), with Protrusion (Upper Arrow) into the Left Ventricle (LV) during Diastole.\n\n【2】RV denotes right ventricle, RA right atrium, and LA left atrium.\n\n【3】A 51-year-old white man was admitted to the hospital because of fever, weight loss, and several episodes of systemic embolization, manifested as sharp, left-sided flank pain with macrohematuria and episodes of painful purple discoloration of his fingers and toes. Seven years earlier, a pigmented nevus (0.7 by 0.4 cm) had been excised from his upper back and diagnosed as malignant melanoma, with invasion of the skin to the level of the papillary dermis (Clark level III). On examination, the patient had skin phototype 2 (i.e., skin that usually burns and sometimes tans), with various hyperpigmented cutaneous lesions, including nevi, lentigines, and ephelides (freckles), on his lips, hands , and torso. On cardiac examination, there was an early diastolic sound that was compatible with a tumor plop. Echocardiography revealed bilateral atrial tumors arising from the interatrial septum and protruding into the left ventricle during diastole . The tumors, which were 9 by 5 by 3 cm, were surgically removed, and the diagnosis of bilateral atrial myxomas was confirmed by histologic studies. Screening of the patient's family revealed that an 18-year-old daughter had hyperpigmented skin lesions. She underwent echocardiographic studies, which showed no abnormalities.\n\n【4】The association between hyperpigmented skin lesions and atrial myxoma has been reported previously.  Affected patients tend to have multiple myxomas, and there is a familial tendency. Acronyms for the combination of skin lesions and atrial myxoma have been suggested, such as the NAME syndrome (nevi, atrial myxoma, myoid tumor, and ephelides) and the LAMB syndrome (lentigines, atrial myxoma, and blue nevi), and it has also been referred to as Carney's complex and the “cutaneous lentiginosis with atrial myxomas syndrome.”  The findings in our patient provide further evidence of an association between cardiac myxomas and hyperpigmented skin lesions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b261a5c1-c11e-4f1a-8858-56030c720685", "title": "Current Concepts: The Autosomal Recessive Cerebellar Ataxias", "text": "【0】Current Concepts: The Autosomal Recessive Cerebellar Ataxias\nAutosomal recessive cerebellar ataxia must be considered in any child or young adult with a progressive disorder of gait or balance or with hypotonia or excessive clumsiness. This review presents a practical approach to these neurodegenerative diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "79642350-e173-4105-8ffa-6a69360ac4ea", "title": "Eltrombopag Added to Immunosuppression in Severe Aplastic Anemia", "text": "【0】Eltrombopag Added to Immunosuppression in Severe Aplastic Anemia\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】A single-group, phase 1–2 study indicated that eltrombopag improved the efficacy of standard immunosuppressive therapy that entailed horse antithymocyte globulin (ATG) plus cyclosporine in patients with severe aplastic anemia.\n\n【3】Methods\n-------\n\n【4】In this prospective, investigator-led, open-label, multicenter, randomized, phase 3 trial, we compared the efficacy and safety of horse ATG plus cyclosporine with or without eltrombopag as front-line therapy in previously untreated patients with severe aplastic anemia. The primary end point was a hematologic complete response at 3 months.\n\n【5】Results\n-------\n\n【6】Patients were assigned to receive immunosuppressive therapy (Group A, 101 patients) or immunosuppressive therapy plus eltrombopag (Group B, 96 patients). The percentage of patients who had a complete response at 3 months was 10% in Group A and 22% in Group B (odds ratio, 3.2; 95% confidence interval \\[CI\\], 1.3 to 7.8; P=0.01). At 6 months, the overall response rate (the percentage of patients who had a complete or partial response) was 41% in Group A and 68% in Group B. The median times to the first response were 8.8 months (Group A) and 3.0 months (Group B). The incidence of severe adverse events was similar in the two groups. With a median follow-up of 24 months, a karyotypic abnormality that was classified as myelodysplastic syndrome developed in 1 patient (Group A) and 2 patients (Group B); event-free survival was 34% and 46%, respectively. Somatic mutations were detected in 29% (Group A) and 31% (Group Β) of the patients at baseline; these percentages increased to 66% and 55%, respectively, at 6 months, without affecting the hematologic response and 2-year outcome.\n\n【7】Conclusions\n-----------\n\n【8】The addition of eltrombopag to standard immunosuppressive therapy improved the rate, rapidity, and strength of hematologic response among previously untreated patients with severe aplastic anemia, without additional toxic effects. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nEltrombopag in Severe Aplastic Anemia  \n\n【11】Acquired aplastic anemia is a disease that involves primary bone marrow failure and manifests with pancytopenia. The best evidence of an autoimmune pathogenesis comes from the patients’ response to immunosuppressive treatment and from laboratory studies.  The introduction of antithymocyte globulin (ATG) in the late 1970s  and the addition of cyclosporine to ATG in the 1980s  led to considerable improvements in hematopoietic recovery and longer survival among patients with severe or very severe aplastic anemia (a distinction based on the neutrophil count and thus on susceptibility to infection).\n\n【12】Overall, two thirds of patients have a response to standard immunosuppressive treatment with horse ATG plus cyclosporine.  The quality and timing of hematologic response are the best predictors of long-term survival.  Over the past three decades, many studies have shown the failure of methods to improve the results of standard therapy; the tested methods included replacing horse ATG with rabbit ATG, alemtuzumab, or cyclophosphamide; adding a third immunosuppressive drug such as mycophenolate mofetil or sirolimus; and adding hematopoietic growth factors to standard therapy.  Moreover, the development of myeloid cancers remains a troublesome complication after immunosuppressive therapy, and it accounts for 10 to 15% of late treatment failures. \n\n【13】Eltrombopag, an oral thrombopoietin-receptor agonist, initially was shown to have efficacy in patients with aplastic anemia that was refractory to immunosuppressive therapy.  An open-label, nonrandomized, phase 1–2 study showed that eltrombopag in combination with standard horse ATG plus cyclosporine had efficacy in untreated patients with severe or very severe aplastic anemia.  We report the results of a phase 3, prospective, investigator-led, multicenter, open-label, randomized trial comparing horse ATG plus cyclosporine with or without eltrombopag as first-line therapy in patients with severe or very severe aplastic anemia.\n\n【14】Methods\n-------\n\n【15】Trial Design and Oversight\n--------------------------\n\n【16】RACE (Randomized, Multicenter Trial Comparing Horse ATG plus Cyclosporine with or without Eltrombopag as First-Line) was conducted at 24 sites in six European countries by the European Society for Blood and Marrow Transplantation (EBMT). Ethics committees at the participating institutions approved the trial, and an independent data and safety monitoring board provided oversight. Patients were randomly assigned to receive either horse ATG plus cyclosporine or horse ATG plus cyclosporine and eltrombopag. Randomization was stratified according to age (≥15 to <40 years or ≥40 years), disease severity (severe or very severe), and center. Adverse events were classified according to the Common Terminology Criteria for Adverse Events, version 4.03. \n\n【17】 All the patients or their legal guardians provided written informed consent. The first and last authors conceived of the trial, which was designed in collaboration with the other authors. No one who is not an author contributed to the writing of the manuscript. Pfizer provided horse ATG, and Novartis provided eltrombopag; both companies also provided research support to EBMT under a Cooperative Research and Development Agreement but had no role in writing the manuscript.\n\n【18】Patients\n--------\n\n【19】From July 2015 through April 2019, a total of 285 patients who were 15 years of age or older, who had a new diagnosis of acquired severe or very severe aplastic anemia,  and who were not eligible for front-line hematopoietic stem-cell transplantation underwent screening. Of these patients, 205 untreated patients were enrolled. After enrollment, 2 patients died and 6 were later found to have diagnoses other than aplastic anemia, leaving 197 patients with a confirmed diagnosis of severe or very severe aplastic anemia .\n\n【20】Treatment\n---------\n\n【21】Patients in Group A received standard immunosuppressive therapy consisting of horse ATG (Atgam, Pfizer) administered at a dose of 40 mg per kilogram of body weight per day on 4 consecutive days and oral cyclosporine at a dose of 5 mg per kilogram of body weight per day from day 1 for a minimum of 12 months; cyclosporine was subsequently tapered during the following 12 months and was discontinued by 24 months . Patients in Group B received experimental therapy consisting of standard immunosuppressive therapy plus eltrombopag administered orally at a dose of 150 mg per day from day 14 through 6 months or through 3 months in patients who had a complete response (defined below) at 3 months. All patients in Group B who had a partial response (defined below) at 3 months continued to receive eltrombopag through 6 months, in accordance with the protocol.\n\n【22】Analyses of Somatic Mutations\n-----------------------------\n\n【23】Samples of bone marrow were obtained at baseline, 6 months, and 2 years in order to analyze the frequency and variant allele frequency of somatic myeloid cancer–associated mutations. A 31-gene targeted molecular bar-coded panel was used .\n\n【24】End Points\n----------\n\n【25】The primary end point of the trial was a hematologic complete response at 3 months, defined as a hemoglobin level greater than 10 g per deciliter, an absolute neutrophil count greater than 1000 per cubic millimeter, and a platelet count greater than 100,000 per cubic millimeter in patients who had not received transfusions.  The criteria for a partial response were transfusion independence (both red cells and platelets), with a blood lineage that did not meet the criteria of severe aplastic anemia but was insufficient for a complete response. Secondary end points included overall response (defined as a complete response or partial response); the time to first response, best response, and complete response; overall survival; event-free survival; relapse; clonal evolution; hemolytic paroxysmal nocturnal hemoglobinuria; discontinuation of immunosuppression; and quality of life as reported by the patient .\n\n【26】Statistical Analysis\n--------------------\n\n【27】The trial was based on the hypothesis that the hematologic complete response rate at 3 months would be 3 times as high in the experimental group (estimated at 21%) as in the standard-therapy group (estimated at 7%).  We estimated that a sample of 96 patients in each treatment group would provide the trial with 80% power (two-sided test) to reject the null hypothesis at a 5% significance level; this sample was increased to 100 patients to compensate for patients with data that could not be evaluated. The cutoff date for analysis was March 1, 2020. All efficacy end points were evaluated on an intention-to-treat basis. The comparison of the complete response rate and the overall response rate was performed with the Mantel–Haenszel pooled odds ratio, stratified according to the factors used for randomization (age, severity of aplastic anemia, and center) .\n\n【28】Results\n-------\n\n【29】Patients\n--------\n\n【30】Table 1. Characteristics of the Patients at Baseline.\n\n【31】No significant differences between the two groups with respect to demographic and clinical features were noted . The median follow-up among the patients in both groups was 24 months (95% confidence interval \\[CI\\], 23 to 24).\n\n【32】Hematologic Response\n--------------------\n\n【33】Table 2. Hematologic Response, According to Treatment Group.\n\n【34】The percentage of patients with a complete response at 3 months was 10% in Group A and 22% in Group B (pooled odds ratio, 3.2; 95% CI, 1.3 to 7.8; P=0.01), which represented a significant between-group difference in the primary end point . The overall response rate at 3 months was lower in Group A (31%) than in Group B (59%).\n\n【35】Of the 70 patients in Group A who did not have a response at 3 months, 14 had an overall response at 6 months (4 had a complete response, and 10 had a partial response). Of the 39 patients in Group B who did not have a response at 3 months, 11 had an overall response at 6 months (4 had a complete response, and 7 had a partial response). At 6 months, the overall response rate was 41% in Group A and 68% in Group B. Better responses were observed in Group B than in Group A at each time point and in all strata (i.e., the severity of aplastic anemia and age) . The superiority of the experimental therapy over standard therapy was also confirmed when National Institutes of Health (NIH) criteria for partial response (which do not include transfusion independence  ) were used, with an overall response rate of 66% in Group A and 77% in Group B at 3 months and 66% and 79%, respectively, at 6 months .\n\n【36】Figure 1. Kinetics of Hematologic Response.\n\n【37】The cumulative incidences of a response are plotted according to treatment group. The shaded areas indicate 95% confidence intervals. Competing events were hematopoietic stem-cell transplantation, any additional treatment for aplastic anemia, clonal evolution, and death. Panel A shows the time to first response. The cumulative incidence of a first response at 3 months was 25% (95% CI, 16 to 33) in Group A and 53% (95% CI, 43 to 63) in Group B; at 6 months, the incidences were 45% (95% CI, 35 to 54) and 68% (95% CI, 58 to 77), respectively. Panel B shows the time to complete response. The median time to complete response was 9.1 months in Group B, whereas in Group A less than 50% of the patients had a complete response during the trial observation period.\n\n【38】The median time to a first response was 8.8 months in Group A and 3.0 months in Group B . At 12 months, the complete response rate was 33% in Group A and 52% in Group B . The time from partial response to complete response was 5.1 months in Group A (32 patients) and 2.7 months in Group B (43 patients). The median time to best response was 8.9 months in Group A and 3.9 months in Group B.\n\n【39】Among the patients who had a response, the time to platelet transfusion independence was 68 days (interquartile range, 34 to 151) in Group A and 40 days (interquartile range, 20 to 80) in Group B. The time to red-cell transfusion independence was 140 days (interquartile range, 62 to 252) in Group A and 51 days (interquartile range, 23 to 122) in Group B.\n\n【40】Predictors of Response\n----------------------\n\n【41】Details of the univariate and multivariable analyses are provided in Table S5A and S5B. In the multivariable analysis, randomization group, age, and disease severity were the only three factors associated with a response. Patients in Group B had a higher probability of a complete response at 3 months and an overall response at 6 months. More severe disease (very severe vs. severe) was a negative predictor for both a complete response at 3 months and an overall response at 6 months. Older age (≥40 years) was associated with a lower overall response rate at 6 months but not with a lower complete response rate at 3 months.\n\n【42】Adverse Events\n--------------\n\n【43】One patient in Group A who died prematurely did not begin to receive horse ATG according to the protocol; all other patients received horse ATG. Six patients (3 in each group) had an interrupted course of horse ATG because of safety reasons or the physician’s decision. Cyclosporine was permanently discontinued within the first 6 months in 18 patients (11 in Group A and 7 in Group B), predominantly because of renal toxicity (data not shown). All patients who were randomly assigned to Group B received eltrombopag, which was discontinued before 6 months in 10 patients  because of elevated liver enzyme levels (in 4 patients); a slight increase in reticulin deposition in the bone marrow (in 2 patients) ; or other reasons (in 4 patients). The incidence of all adverse events, including infectious and hepatic complications, was similar in the two groups .\n\n【44】Patient-Reported Outcomes\n-------------------------\n\n【45】Outcomes reported by the patients were assessed with the use of the European Organization for Research and Treatment of Cancer core Quality of Life of Cancer Patients questionnaire at baseline and at 6, 12, and 24 months after randomization. Scores improved from baseline over time with respect to global health status, as well as on physical, social, and emotional scales, with minimal differences between the groups .\n\n【46】Karyotypic Abnormalities, Myeloid Cancers, and Somatic Mutations\n----------------------------------------------------------------\n\n【47】Only three confirmed karyotypic abnormalities met the definition of karyotypic evolution according to the protocol (1 patient in Group A had monosomy 7 and 2 patients in Group B had del\\[13q\\]). There was no morphologic evidence of myelodysplastic syndrome.\n\n【48】Figure 2. Somatic Mutations.\n\n【49】Panel A shows the frequency and variant allele frequency (VAF) of mutations at baseline, 6 months, and 24 months. The variant allele frequency of the mutations is shown on a logarithmic scale. The box-and-whisker plots of the specific gene mutations are shown; the whiskers indicate the range, the sides of the boxes indicate the interquartile range, and the vertical line within each box indicates the median. The gray dots indicate individual mutations, and the vertical lines over some of the gray dots indicate the range (minimum and maximum). Panel B shows the frequency of mutations as a measure of 1 or 2 or more mutations in each group at different time points. Panel C shows the variant allele frequency of mutations in each group in patients who were screened (45 patients) and had detectable mutations (34 patients) at all three time points.\n\n【50】Next-generation sequencing was available at the time of analysis for 156 patients at baseline, 121 patients at 6 months, and 53 patients at 24 months of follow-up. At baseline, with the exclusion of _PIGA_ mutations, 47 patients (30%) had somatic mutations; 36 patients (23%) had 1 mutation, 10 patients (6%) had 2 mutations, and 1 patient (<1%) had more than 2 mutations . The most frequently mutated genes were _DNMT3A, BCOR, BCORL1_ , and _PIGA_ . Overall, patients with mutations were older, had severe aplastic anemia (as compared with very severe aplastic anemia), and had a higher neutrophil count than those without mutations. The median age of patients with _PIGA, BCOR_ , and _BCORL1_ mutations was lower than that of patients with other mutations (41 years vs. 59 years).\n\n【51】At baseline, the frequency of mutations, the mutated genes, and the median variant allele frequency did not differ significantly between the two treatment groups . Baseline mutations were not significantly associated with overall survival  or response. Complete response rates at 3 months were 14% among patients without mutations and 21% among those with mutations. The overall response rate at 6 months was 49% among patients without mutations and 60% among those with mutations .\n\n【52】During the disease course, the frequency of mutations in patients increased from approximately 30% in both groups at baseline to 66% in Group A and 55% in Group B at 6 months, and to 77% and 52%, respectively, at 24 months . Fluctuations in the variant allele frequency of mutations were noted in both groups at three different time points .\n\n【53】Irrespective of baseline mutations, at 6 months, new or additional mutations were acquired in 30 patients (53%) in Group A and in 22 patients (39%) in Group B. At 24 months, new or additional mutations were acquired in 16 patients (62%) in Group A and in 6 patients (27%) in Group B . These mutations did not correlate with hematologic response or with overall survival .\n\n【54】Long-Term Outcomes\n------------------\n\n【55】The 2-year overall survival was similar in Group A (85%; 95% CI, 78 to 92) and Group B (90%; 95% CI, 82 to 97) . Twenty-two patients died during the trial — 14 in Group A and 8 in Group B . A total of 23 patients underwent hematopoietic stem-cell transplantation — 12 in Group A and 11 in Group B; details of other additional treatments according to group are provided in Table S18. The cumulative incidence of hemolytic paroxysmal nocturnal hemoglobinuria at 24 months was 7% in Group A and 1% in Group B . The cumulative incidence of relapse 18 months after response did not differ significantly between Group A (11%; 95% CI, 2 to 20) and Group B (19%; 95% CI, 9 to 29).\n\n【56】Figure 3. Event-free Survival and Stacked Cumulative Incidence Curves.\n\n【57】Panel A shows Kaplan–Meier curves for event-free survival according to treatment group. The shaded areas indicate 95% confidence intervals. In Group A, event-free survival at 6 months was 59% (95% CI, 50 to 69); at 12 months, 41% (95% CI, 31 to 50); and at 24 months, 34% (95% CI, 24 to 44); in Group B, these rates were 79% (95% CI, 71 to 87), 56% (95% CI, 46 to 66), and 46% (95% CI, 36 to 57), respectively. Events were no response at 6 months, hematopoietic stem-cell transplantation, any additional treatment for aplastic anemia, clonal evolution, relapse, or death. Panel B shows stacked cumulative incidence curves according to event. The overall cumulative incidence of an event (1 minus the probability of event-free survival) is shown according to the type of event that occurred. In Group B, reintroduction of eltrombopag was defined according to the protocol in case of relapse or a weak hematologic response .\n\n【58】In the multivariable analysis, the two groups had similar overall survival (hazard ratio for death in Group B as compared with Group A, 0.57; 95% CI, 0.24 to 1.37) and relapse risk (hazard ratio, 1.32; 95% CI, 0.55 to 3.21). Older age was the only factor associated with worse overall survival and relapse risk. At 2 years, more events had occurred in Group A than in Group B, which resulted in an inferior event-free survival in Group A (34%; 95% CI, 24 to 44) than in Group B (46%; 95% CI, 36 to 57) . The most common treatment failure events were no response in Group A and no response and the use of additional treatment in Group B . In the multivariable analysis, risk among patients in Group B was reduced in the first 6 months (hazard ratio for treatment failure events, 0.42; 95% CI, 0.25 to 0.72). Older age and disease severity were confirmed as risk factors. At 24 months, 19% of the patients in Group A and 28% of those in Group B had a cyclosporine-independent response.\n\n【59】Discussion\n----------\n\n【60】Over the past 30 years, efforts to improve the results of standard therapy (horse ATG plus cyclosporine) in patients with severe aplastic anemia have been largely unsuccessful.  In this prospective, randomized, multicenter trial, hematologic complete and overall responses at 3 months were significantly better with eltrombopag added to standard therapy than with standard therapy alone, and the quality and speed of hematologic recovery were better with the addition of eltrombopag as well, with no excess of toxic effects. In this trial, eltrombopag was started at day 14 (to prevent possible cumulative toxicity from concomitant administration of ATG), with no detrimental effect on the early hematologic response; in contrast, the phase 2 NIH trial initially suggested that the efficacy of standard therapy was improved with the addition of eltrombopag, with the best results when eltrombopag was introduced at day 1. \n\n【61】The mechanisms of action of eltrombopag in aplastic anemia warrant further investigation. Previous studies have shown that eltrombopag stimulates hematopoiesis despite high levels of endogenous thrombopoietin.  However, it is not clear whether this action is exerted at the level of hematopoietic stem cells or on more mature progenitor cells (i.e., by increasing the ratio of progenitor cells to stem cells). Thus, independent of its molecular mechanisms, eltrombopag appears to sustain hematopoiesis, buying time for immunosuppression to curb the immune attack on hematopoietic stem cells. Furthermore, in addition to its direct stimulatory action on hematopoiesis, eltrombopag might contribute to the immunosuppressive effect of ATG plus cyclosporine. A recent study showed that by binding to the transmembrane domain of the thrombopoietin receptor, eltrombopag prevents the inhibitory effect of interferon-γ by interrupting the interaction between endogenous thrombopoietin and its cognate receptor (i.e., serving as a decoy receptor). \n\n【62】We evaluated the hematologic complete response as a primary end point because survival and long-term outcomes are associated with the quality of hematologic response at 3 months and with the presence of early recovery after the administration of ATG.  In our trial, the median times to first response and complete response were shorter with immunosuppressive therapy plus eltrombopag than with immunosuppressive therapy alone; these faster response times accounted for the achievement of earlier red-cell and platelet transfusion independence in the experimental group. At 6 months, the overall response rate increased from 41% to 68%, with transfusion independence as a prerequisite for partial response. On the basis of the NIH criteria for partial response (i.e., improvement in blood counts but no need for transfusion independence), the overall response rate at 6 months with immunosuppressive therapy plus eltrombopag was also significantly better than with immunosuppressive therapy alone (79% and 66%). Even if the difference in the response rate decreases over time, this earlier hematologic recovery may translate into fewer patients having to switch to early hematopoietic stem-cell transplantation.\n\n【63】The ability to identify patients who have a higher probability of hematologic response is important. In our trial, less severe aplastic anemia (severe vs. very severe) and younger age (<40 years) were associated with a better response. Thus, both factors remain the two main clinical predictors, even in triple therapy for aplastic anemia.  None of the previously reported baseline hematologic characteristics  were associated with the overall response rate in our trial.\n\n【64】The results of a prespecified overall safety analysis that included infectious and hepatic complications did not differ significantly between the two groups. Two patients discontinued eltrombopag because of focal grade 1 reticulin deposition in the trephine biopsy that reversed on discontinuation of eltrombopag; these findings are consistent with the long-term follow-up of the use of a thrombopoietin-receptor agonist in immune thrombocytopenia.  The outcomes reported by the patients showed overall improvement in both treatment groups, with no significant differences between the groups.\n\n【65】The addition of eltrombopag to standard immunosuppressive therapy did not result in significantly improved overall survival, which was expected considering the additional effect of rescue treatment. The 85 to 90% 2-year overall survival rate is higher than most rates observed in multicenter studies involving patients with severe aplastic anemia. Nevertheless, eltrombopag added to standard immunosuppressive therapy significantly increased event-free survival from 34% to 46% at 2 years through the reduction in initial refractoriness to immunosuppression. However, the reintroduction of eltrombopag was the most common event in the experimental group; this finding provides justification for further studies with longer follow-up to improve long-term treatment in patients with aplastic anemia.\n\n【66】With the advent of next-generation sequencing, the presence of somatic mutations emerged as a common finding in patients with aplastic anemia; these mutations had a possible effect on progression to myeloid cancers and on long-term outcomes.  This finding was an obvious concern because of eltrombopag-associated stem-cell stimulatory properties.  In this prospective trial, we found that the prevalence of somatic mutations was not higher in the eltrombopag group (Group B) than in the standard-therapy group (Group A). We were surprised to find, however, that the percentage of patients with mutations increased from 29% at baseline to 66% at 6 months in Group A and from 31% at baseline to 55% at 6 months in Group B. Thus, we prospectively found that hematologic recovery after immunosuppressive therapy with or without eltrombopag was likely to be oligoclonal. However, clones (at baseline or later) did not negatively affect the response or 2-year outcomes. The dominance of these clones, as tracked by variant allele frequency, was largely unpredictable, with no common patterns across patients, suggesting genetic drift more than an active selection process.  Similarly, despite the prevalence of the _PIGA_ mutation, eltrombopag did not selectively induce paroxysmal nocturnal hemoglobinuria clonal expansion, as compared with standard immunosuppressive therapy.\n\n【67】Clonal hematopoiesis associated with hematopoietic recovery  was frequent but, as shown in paroxysmal nocturnal hemoglobinuria,  it should not be confused with “clonal evolution,” which is considered to be progression to a myeloid cancer.  A long-term follow-up of this trial is planned to explore the clinical relevance of this oligoclonal hematopoiesis and to evaluate the risk of myeloid malignant transformation, which usually appears in 10 to 15% of patients 5 to 10 years after diagnosis.  However, treating physicians should not overinterpret the presence of somatic mutations; therapeutic decisions (i.e., commitment to hematopoietic stem-cell transplantation) should be made only in the presence of clear clinical indications.\n\n【68】This prospective randomized trial showed that the addition of eltrombopag to horse ATG plus cyclosporine, as compared with horse ATG plus cyclosporine alone, was beneficial in patients with severe aplastic anemia. The addition of eltrombopag induced a response that was of higher quality and occurred faster without increasing toxic effects.\n\n【69】Table 1. Characteristics of the Patients at Baseline. \n\n| Characteristic | Group A: Horse ATG–Cyclosporine(N=101) | Group B: Horse ATG–Cyclosporine–Eltrombopag(N=96) | All Patients(N=197) |\n| --- | --- | --- | --- |\n| Follow-up — mo |  |  |  |\n| Median | 24 | 23 | 24 |\n| 95% CI | 23–24 | 19–24 | 23–24 |\n| Age — yr |  |  |  |\n| Median | 52 | 55 | 53 |\n| Range | 15–81 | 16–77 | 15–81 |\n| Age category — no. (%) |  |  |  |\n| ≥15 to <18 yr | 7 (7) | 2 (2) | 9 (5) |\n| ≥18 to <40 yr | 29 (29) | 27 (28) | 56 (28) |\n| ≥40 to <65 yr | 43 (43) | 43 (45) | 86 (44) |\n| ≥65 yr | 22 (22) | 24 (25) | 46 (23) |\n| Sex — no. (%) |  |  |  |\n| Male | 52 (52) | 56 (58) | 108 (55) |\n| Female | 49 (48) | 40 (42) | 89 (45) |\n| Severity of aplastic anemia — no. (%) |  |  |  |\n| Severe | 67 (66) | 62 (65) | 129 (66) |\n| Very severe | 34 (34) | 34 (35) | 68 (34) |\n| Laboratory values |  |  |  |\n| GPI-deficient neutrophils ≥1.0% — no./total no. (%) | 44/100 (44) | 33/93 (36) | 77/193 (40) |\n| Reticulocyte count — per mm 3 |  |  |  |\n| Median | 20,000 | 23,300 | 21,000 |\n| IQR | 8,900–36,000 | 12,000–46,800 | 10,000–38,000 |\n| Neutrophil count — per mm 3 |  |  |  |\n| Median | 300 | 500 | 400 |\n| IQR | 100–700 | 100–1000 | 100–800 |\n| Lymphocyte count — per mm 3 |  |  |  |\n| Median | 1,400 | 1,400 | 1,400 |\n| IQR | 1,000–1,800 | 1,000–1,700 | 1,000–1,800 |\n| Platelet count — per mm 3 |  |  |  |\n| Median | 18,000 | 15,000 | 17,000 |\n| IQR | 10,000–32,000 | 10,000–29,000 | 10,000–30,000 |\n| Cytogenetic abnormalities — no./total no. (%) |  |  |  |\n| Normal | 64/86 (74) | 61/84 (73) | 125/170 (74) |\n| Abnormal karyotype  | 7/86 (8) | 6/84 (7) | 13/170 (8) |\n| Karyotypic analysis failed | 15/86 (17) | 17/84 (20) | 32/170 (19) |\n| Somatic myeloid mutations — no. of patients/total no. evaluated (%)  | 23/78 (29) | 24/78 (31) | 47/156 (30) |\n\n【71】 ATG denotes antithymocyte globulin, CI confidence interval, GPI glycophosphatidylinositol, and IQR interquartile range.\n\n【72】 The category of abnormal karyotype included 7 patients with deletion Y (3 in Group A and 4 in Group B), 2 patients with trisomy 8 in Group A, 1 patient with deletion 20q in Group B, and 3 patients (1 in Group A and 2 in Group B) with other abnormalities .\n\n【73】 In 41 patients, mutations were missing at baseline for the following reasons: 9 minors (<18 years of age) could not be included according to the King’s College London Haemato-Oncology Tissue Bank policy, 8 patients did not consent to biosampling, 2 samples were lost during transit to the central laboratory, and 22 samples were not included for other reasons, mainly because the analysis had not been performed at the time of data lock.\n\n【74】Table 2. Hematologic Response, According to Treatment Group.\n\n| Cohort and Response | Response at 3 Mo | Response at 3 Mo | Response at 3 Mo | Response at 3 Mo | Response at 6 Mo | Response at 6 Mo |\n| --- | --- | --- | --- | --- | --- | --- |\n|  | Group A: Horse ATG–Cyclosporine(N=101) | Group B: Horse ATG–Cyclosporine–Eltrombopag(N=96) | Odds Ratio(95% CI)  | P Value | Group A: Horse ATG–Cyclosporine(N=101) | Group B: Horse ATG–Cyclosporine–Eltrombopag(N=95)  |\n| All patients — no. (%) |  |  |  |  |  |  |\n| Complete response  | 10 (10) | 21 (22) | 3.2 (1.3–7.8) | 0.01 | 20 (20) | 30 (32) |\n| Partial response | 21 (21) | 36 (38) |  |  | 21 (21) | 35 (37) |\n| No response | 70 (69) | 39 (41) |  |  | 60 (59) | 30 (32) |\n| Overall response  | 31 (31) | 57 (59) |  |  | 41 (41) | 65 (68) |\n| Patients with severe aplastic anemia — no./total no. (%) |  |  |  |  |  |  |\n| Complete response | 10/67 (15) | 17/62 (27) |  |  | 15/67 (22) | 20/62 (32) |\n| Partial response | 17/67 (25) | 27/62 (44) |  |  | 16/67 (24) | 26/62 (42) |\n| No response | 40/67 (60) | 18/62 (29) |  |  | 36/67 (54) | 16/62 (26) |\n| Patients with very severe aplastic anemia — no./total no. (%) |  |  |  |  |  |  |\n| Complete response | 0/34 | 4/34 (12) |  |  | 5/34 (15) | 10/33 (30) |\n| Partial response | 4/34 (12) | 9/34 (26) |  |  | 5/34 (15) | 9/33 (27) |\n| No response | 30/34 (88) | 21/34 (62) |  |  | 24/34 (71) | 14/33 (42) |\n| Patients ≥15 to <40 yr — no./total no. (%) |  |  |  |  |  |  |\n| Complete response | 6/36 (17) | 6/29 (21) |  |  | 11/36 (31) | 15/29 (52) |\n| Partial response | 6/36 (17) | 14/29 (48) |  |  | 7/36 (19) | 8/29 (28) |\n| No response | 24/36 (67) | 9/29 (31) |  |  | 18/36 (50) | 6/29 (21) |\n| Patients ≥40 yr — no./total no. (%) |  |  |  |  |  |  |\n| Complete response | 4/65 (6) | 15/67 (22) |  |  | 9/65 (14) | 15/66 (23) |\n| Partial response | 15/65 (23) | 22/67 (33) |  |  | 14/65 (22) | 27/66 (41) |\n| No response | 46/65 (71) | 30/67 (45) |  |  | 42/65 (65) | 24/66 (36) |\n\n【76】 The pooled odds ratios for Group B as compared with Group A and 95% confidence intervals were obtained with the use of the Mantel–Haenszel test, stratified according to the factors used at randomization (age, severity of aplastic anemia, and center).\n\n【77】 One patient in Group B did not have follow-up to the 6-month evaluation and did not have a competing event at the last follow-up.\n\n【78】 At 6 months, 4 of the patients who had had a complete response at 3 months (1 patient in Group A and 3 patients in Group B) had loss of response (i.e., they moved from complete response to no response) and 7 patients (all in Group B) had a downgrade in response from complete response to partial response.\n\n【79】 The overall response corresponded to the percentage of patients who had a partial or complete response.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "cf16bf72-290a-43d4-a998-905aa179ec40", "title": "Bioengineering of an Intraabdominal Endocrine Pancreas", "text": "【0】Bioengineering of an Intraabdominal Endocrine Pancreas\nTo the Editor:\n--------------\n\n【1】Islet transplantation can restore euglycemia and eliminate severe hypoglycemia in patients with type 1 diabetes.  Limitations of intrahepatic islet transplantation include a restricted transplant-tissue volume, bleeding with placement of the transplant, exposure to high levels of immunosuppressive drugs after transplantation, and the triggering of an immediate blood-mediated inflammatory response.  The omentum has a dense vascularized surface for islet implantation, drains into the portal system, and is easily accessible. \n\n【2】We report on a 43-year-old woman with a 25-year history of type 1 diabetes. Her weight was 53.4 kg (117.7 lb), and her body-mass index (the weight in kilograms divided by the square of the height in meters) was 21.5. She received insulin at a mean (±SD) daily dose of 32.9±1.3 units. In this patient, diabetes was complicated by unawareness of hypoglycemia and episodes of severe hypoglycemia. The stimulated C-peptide level was less than 0.3 ng per milliliter (<0.1 nmol per liter), and the glycated hemoglobin level was 6.8%.\n\n【3】As part of an ongoing study , the patient underwent islet transplantation onto the omentum. A total of 602,395 islet equivalents (an islet equivalent is the standard unit for reporting variations in the volume of islets, with the use of a standard islet diameter of 150 μm) from one deceased donor (total packed-tissue volume, 6.5 ml) were combined with autologous plasma (in a  ratio) and laparoscopically layered onto the omentum. A total of 20 ml of recombinant thrombin (Recothrom), reconstituted in a solution containing 1000 units per milliliter, was layered over the islets, followed by another layer of autologous plasma to generate a degradable biologic scaffold. The induction immunosuppression regimen consisted of antithymocyte globulin and etanercept, and the maintenance immunosuppression regimen consisted of mycophenolate sodium and tacrolimus. Tacrolimus was switched to sirolimus 8 months after transplantation because the patient had hair loss. No surgical complications were observed.\n\n【4】Figure 1. Results of Glucose Monitoring and Mixed-Meal Tolerance Tests, Homeostasis Model Assessment (HOMA2) Index Values, and Beta and BETA-2 Scores in the Patient.\n\n【5】Panel A shows the results of hourly continuous glucose monitoring (G4 Platinum, Dexcom) on the 7 days before each study visit. I bars indicate standard deviations. The shaded area shows the glucose range of 70 to 180 mg per deciliter (3.9 to 10.0 mmol per liter). The 7-day mean (±SD) glucose level in 1986 interstitial glucose measurements obtained every 5 minutes was 99±15 mg per deciliter (range, 60 to 169) (5.5±0.8 mmol per liter \\[range, 3.3 to 9.4\\]) at 75 days; in 1985 interstitial glucose measurements, it was 105±17 mg per deciliter (range, 71 to 158) (5.8±0.9 mmol per liter \\[range, 3.9 to 8.8\\]) at 6 months; and in 1966 interstitial glucose measurements, it was 109±15 mg per deciliter (range, 73 to 151) (6.0±0.8 mmol per liter \\[range, 4.1 to 8.4\\]) at 12 months. To convert the values for glucose to millimoles per liter, multiply by 0.05551. Panel B shows 2012 capillary blood glucose values obtained before transplantation and through postoperative day 365. I bars indicate standard deviations. The upper and lower orange lines show the glucose range of 70 to 180 mg per deciliter. Panel C shows variables derived from the mixed-meal tolerance test and HOMA2 indexes. Samples obtained in the mixed-meal tolerance test at 75 days for measurements of glucose and C-peptide levels included only 0 and 90 minutes. The HOMA2 estimates steady-state beta-cell function (HOMA2–%B) and insulin sensitivity (HOMA2–%S) as percentages of a normal reference population. HOMA2–IR is the reciprocal of HOMA2–%S. HOMA2 indexes were calculated with the use of the HOMA2 calculator, version 2.2.3 (Diabetes Trials Unit, University of Oxford), available at www.dtu.ox.ac.uk/homacalculator. Beta scores range from 0 to 8, with higher scores indicating better islet function, and BETA-2 scores range from 0 to 42, with lower scores indicating higher risks of glucose intolerance and insulin dependence. To convert the values for C-peptide to nanomoles per liter, multiply by 0.331.\n\n【6】Insulin was discontinued 17 days after the transplantation.\n\n【7】At 12 months, in response to a 5-hour mixed-meal tolerance test, the 90-minute glucose level was 266 mg per deciliter (14.6 mmol per liter); this level decreased to 130 mg per deciliter (7.1 mmol per liter) at 300 minutes. On continuous glucose monitoring, the 7-day mean glucose level in 1966 interstitial glucose readings was 109±15 mg per deciliter (6.0±0.8 mmol per liter), the glycated hemoglobin level was 6.0%, the Beta score (based on the fasting glucose level, the 90-minute C-peptide level after a mixed-meal tolerance test, the glycated hemoglobin level, and the use of either insulin or glucose-lowering therapies) was 7 (on a scale from 0 to 8, with higher scores indicating better islet function), and the BETA-2 score (calculated from the fasting C-peptide level, the fasting glucose level, and the glycated hemoglobin level measured in a single fasting blood sample) was 10 (on a scale from 0 to 42, with lower scores indicating higher risks of glucose intolerance and insulin dependence). After evaluation at 6 months, insulin secretion decreased and glucose levels increased, although these levels remained below those that would constitute diabetes. After transplantation, the patient exercised regularly and followed a low-carbohydrate diet, which probably contributed to her stable glycemic control.\n\n【8】In this patient, islet transplantation onto the omentum restored euglycemia and insulin independence. A functional decline was observed at 12 months with an increase in insulin sensitivity, which we speculate may have been due to the switch from tacrolimus to sirolimus. The patient continued to have stable glycemic control without exogenous insulin and without episodes of hypoglycemia.\n\n【9】Data from long-term follow-up and regarding additional omental islet transplantations are lacking. The current study is ongoing to determine the safety and long-term feasibility of this strategy of islet transplantation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3e18ee2a-b016-4f0e-865d-7f8631664154", "title": "Case 1-2023: A 49-Year-Old Man with Hypokalemia and Paranoia", "text": "【0】Case 1-2023: A 49-Year-Old Man with Hypokalemia and Paranoia\nA 49-year-old man was admitted with hypokalemia and paranoia. Small-cell lung cancer had been diagnosed 6 months earlier; he had declined medical therapy and pursued alternative treatment. A diagnostic test was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1df8fab7-7477-4842-a7aa-5aa53c316876", "title": "Prognosis of Medically Treated Patients with Coronary-Artery Disease with Profound ST-Segment Depression during Exercise Testing", "text": "【0】Prognosis of Medically Treated Patients with Coronary-Artery Disease with Profound ST-Segment Depression during Exercise Testing\nAbstract\n--------\n\n【1】Reproducible and profound (>2 mm) ST-segment depression during exercise testing in patients with coronary heart disease is associated with multivessel involvement. In these patients, Coronary Artery bypass surgery has been recommended even when symptoms are absent. However, there are few long-term follow-up data regarding the prognosis when such patients are treated medically. Among 212 men with coronary-artery disease in whom profound ST-segment depression could be reproduced with exercise, 142 who had no other type of heart disease and were not receiving digitalis drugs had a mean ST-segment depression of 2.9 mm. Follow-up has lasted an average of 59 months: 11 patients have died (annual mortality, 1.4 per cent), and nine have had bypass operations (1.3 per cent per year). Survival correlated with exercise tolerance but not with degree of ST depression, peak heart rate, or peak blood pressure during exercise. We conclude that such ST-segment depression is not associated with a poor prognosis. There is rarely a need to resort to cardiac surgery; medical management is highly successful and associated with a low mortality.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b9d79cc3-7371-46b3-a25d-3c9362f4b3c5", "title": "XDR Tuberculosis — Implications for Global Public Health", "text": "【0】XDR Tuberculosis — Implications for Global Public Health\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 Interview with Ian Smith on extensively drug-resistant tuberculosis and its implications for global public health. \n\n【3】In early 2005, physicians at a rural hospital in KwaZulu-Natal, a province of South Africa, were concerned by a high rate of rapid death among patients infected with the human immunodeficiency virus (HIV) who also had tuberculosis. A study revealed the presence not only of multidrug-resistant (MDR) tuberculosis but also what came to be called extensively drug-resistant (XDR) tuberculosis. XDR tuberculosis is caused by a strain of _Mycobacterium tuberculosis_ resistant to isoniazid and rifampin (which defines MDR tuberculosis) in addition to any fluoroquinolone and at least one of the three following injectable drugs: capreomycin, kanamycin, and amikacin. Of 53 patients with XDR tuberculosis, 55% claimed they had never been treated (implying that they had primary infection with an XDR strain of _M. tuberculosis_ ); two thirds had recently been hospitalized; and all 44 who underwent testing were HIV-positive. All but one of the patients died of tuberculosis, with a median survival period of only 16 days from the time the first sputum specimen was collected. Genotyping analysis revealed that 85% of the 46 isolates tested belonged to the KwaZulu-Natal (KZN) family of tuberculosis strains, which had been recognized in the province for a decade. \n\n【4】These alarming findings attracted much attention at the International AIDS Society conference in Toronto in August 2006. But this was not the first time that XDR tuberculosis had been identified. A March 2006 report by the Centers for Disease Control and Prevention and the World Health Organization (WHO) documented the presence of XDR tuberculosis in at least 17 countries. Though not representative, the data showed that 10% of MDR tuberculosis isolates were in fact XDR tuberculosis. More representative data from the United States, the Republic of Korea, and Latvia showed that 4%, 15%, and 19%, respectively, of MDR tuberculosis isolates were XDR strains. \n\n【5】 Prevalence of MDR Tuberculosis among New Cases of Tuberculosis, 1994–2002, and Countries with at Least One Reported Case of XDR Tuberculosis as of January 2007.\n\n【6】Data are from the World Health Organization.\n\n【7】In the fall of 2006, international experts agreed on the laboratory case definition of XDR tuberculosis; a framework for action on the clinical management of suspected XDR tuberculosis; implications for national tuberculosis-control programs; protection of health care workers; surveillance; and advocacy, communication, and social mobilization. \n\n【8】The global threat of XDR tuberculosis has great significance for the public health field. For one thing, its very existence is a reflection of weaknesses in tuberculosis management, which should minimize the emergence of drug resistance. Early, accurate diagnosis and immediate, proper curative treatment, supported and supervised so that drugs are taken for the appropriate duration, are key to tuberculosis control. Inadequate drug regimens select out drug-resistant strains, which then proliferate. Further treatment errors repeat the cycle, leading to strains that are resistant to other drugs, until MDR tuberculosis is created.\n\n【9】The recipe for XDR tuberculosis is the same — the inappropriate use of second-line drugs in a patient for whom first-line drugs are failing. Patients then spread the infection to close contacts, who acquire primary XDR tuberculosis. Multiple errors have probably contributed to the development of XDR disease in South Africa.\n\n【10】Solutions that interrupt this cycle are urgently needed. The most basic requirement is an effective disease-control infrastructure, starting with much-strengthened laboratory capacity. Diagnosis based on sputum-smear microscopy and rapid liquid-culture methods followed by the provision of appropriate support for patients and strict supervision of treatment until cure are the basis of tuberculosis control. Compliance must be maximized to prevent the emergence of drug resistance.\n\n【11】Prevention, however, is insufficient once drug-resistant tuberculosis has spread. Immediate detection through rapid drug-susceptibility testing is necessary to ensure that patients receive a quick diagnosis and adequate treatment and that transmission of the disease is thereby interrupted. Such treatment requires access to second-line drugs, which are more costly, more toxic, and weaker than first-line drugs. Second-line treatment must be given for at least 18 months under strict monitoring and supervision, and patients must receive counseling and support, since further development of drug resistance would render them virtually untreatable.  New classes of antituberculosis drugs are unlikely to become available for at least another few years.\n\n【12】XDR tuberculosis also has major implications for the care of patients with HIV and for HIV control, because a high prevalence of HIV predicts extreme vulnerability to tuberculosis. All available public health measures must be implemented when these diseases converge, starting with DOTS, the essential package of tuberculosis-control intervention based on diagnosis and treatment of infectious cases, and HIV–AIDS prevention and treatment. Antiretroviral drugs protect against tuberculosis by restoring patients' immunocompetence. Tuberculosis screening and chemoprophylaxis are essential in patients who are HIV-positive, as are antiretroviral treatment and the use of trimethoprim–sulfamethoxazole for patients with tuberculosis. Furthermore, hospital infection-control measures must be strengthened to prevent transmission among hospitalized patients and health care workers.\n\n【13】The development of XDR tuberculosis reveals weaknesses in primary care diagnostic services. Patients with cough often present at the nearest clinic, and if they have tuberculosis that is not promptly diagnosed and treated, they will spread the disease. And if tuberculosis isolates are not tested for drug susceptibility from the outset, resistance may be detected too late to permit a cure. These interventions require substantial primary care capacity and training of health care workers in recognizing suspected cases of tuberculosis, making diagnoses, supervising treatment, and counseling patients. Well-trained laboratory technicians are also of paramount importance for ensuring proper diagnosis.\n\n【14】In addition, the need to contain XDR tuberculosis places major demands on surveillance systems. Most current information on drug resistance comes from surveys, since routine drug-susceptibility testing has been the privilege of rich countries. This situation must change rapidly in areas affected by XDR tuberculosis. Information is essential for building and monitoring a response, and only computerized information systems allow sufficiently rapid exchange of information within and between countries. The 2005 International Health Regulations, which take effect in June 2007, provide a framework that identifies the roles of the WHO and national governments in identifying and responding to public health emergencies and sharing relevant information. Effective implementation of these regulations requires much more effective national surveillance and response systems — and therefore the mobilization of resources.\n\n【15】XDR tuberculosis has exposed the dearth of new tools for tuberculosis control. Although the current diagnostic tests and drugs can control tuberculosis if rigorously applied, the lack of easy-to-use tests that produce rapid results reflects a lack of awareness of the magnitude of the problem. Ideally, if tuberculosis is suspected, it should be diagnosed at the point of care, and information about drug susceptibility should be obtained rapidly to guide treatment decisions. In most countries, this ideal is not achieved because of insufficient primary care services and the lack of adequate laboratories and of tools permitting easy, prompt detection of drug resistance. To correct these deficiencies, governments and international aid partners must invest in building a proper care and laboratory infrastructure, and research on better diagnostics must be intensified without delay.\n\n【16】Similarly, the lack of new classes of antituberculosis drugs has made treatment of drug-resistant tuberculosis challenging. New treatment regimens probably will not be available for several years — hence the imperative to preserve the effectiveness of current drugs by making sure that no second-line drugs are used without proper supervision. An effective vaccine would be the most powerful tool for preventing tuberculosis and drug resistance, but a vaccine is not anticipated anytime soon. We must invest in research and development for better tools while maintaining the efficacy of the tools we have available today.\n\n【17】All evidence suggests that XDR tuberculosis reflects a failure to implement the measures recommended in the WHO's Stop TB Strategy.  This strategy emphasizes expanding high-quality DOTS programs, addressing HIV-associated tuberculosis and drug resistance, strengthening health care systems and primary care services, encouraging all providers to follow good practices, empowering patients and communities to improve health, and enabling and promoting research. These measures ultimately require political commitment and will, and in many countries, health is still not a top priority. But we now have an opportunity to prioritize tuberculosis control and research efforts, energized by the appearance of highly resistant strains that may not be halted unless immediate investments match the challenges we face.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "dd42abf6-e6ad-41b8-b9bb-d5dc7b07068a", "title": "The HA-1A Monoclonal Antibody for Gram-Negative Sepsis", "text": "【0】The HA-1A Monoclonal Antibody for Gram-Negative Sepsis\nTo the Editor\n-------------\n\n【1】Ziegler and collaborators (Feb. 14 issue)  recently reported on an impressive reduction in 28-day mortality, from 49 percent to 30 percent, in a subgroup of patients who had bacteremia due to gram-negative bacilli. The patients were treated with human anti—lipid A monoclonal antibody early in the course after the onset of symptoms. Patients with sepsis or bacteremia caused by microorganisms other than gram-negative bacilli received no measurable benefit. These results prompted the investigators to recommend the therapy as routine treatment for patients with clinical signs of bacteremia, provided that a gram-negative organism was suspected as the cause.\n\n【2】The authors deserve high praise for this important result of collaborative research. Yet we have some discomfort about recommendations for routine use of the antibody. Patients were assigned to treatment with either anti—lipid A antibody or albumin placebo, depending on the basis of a clinical diagnosis of sepsis with circulatory instability,  which did not distinguish between bacteriologic causes. Accordingly, of the cohort of 543 patients, only 37 percent had both bacteremia and gram-negative organisms as the cause of bacteremia. An equal percentage had gram-negative infections without bacteremia. In 15 percent, no source of infection was identified. Accordingly, only about one third of the patients fulfilled the criterion of bacteremia due to gram-negative enteric bacilli.\n\n【3】The authors were forthright in presenting the finding that when all patients were taken into account, there was no reduction in mortality after treatment with anti—lipid A antibody. This exposes the reality that there was no overall benefit to patients defined by the \"sepsis syndrome.\" If patients who had both bacteremia and gram-negative bacilli as the cause of the bacteremia had been identified and received anti—lipid A antibody, mortality might well have been significantly reduced. To the contrary, the failure to show an overall benefit leaves open the possibility that the demonstrated benefit to patients with gram-negative bacteremias was counter-balanced by adverse effects in some or all of the remaining patients. We therefore would be reluctant to employ this therapy on the basis of the diagnostic criteria used by Dr. Ziegler and her collaborators.\n\n【4】It is apparent that successful treatment with anti—lipid A antibody is contingent on the ability to make an early diagnosis of bacteremia and to establish that the bacteremia is caused by endotoxin-producing enteric bacilli, so as to preclude risks and avoid million-dollar expenditures for a majority of patients who would be treated without evidence of benefit. The authors would have to demonstrate such methods for purposes of early life-saving treatment with lipid A antibody.    It also prompts us to rethink the diagnostic usefulness of terms such as \"sepsis syndrome\" and even \"septicemia,\" in favor of bedside diagnoses with more clinical and microbiologic precision as previously suggested by our group.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0aa8b8fc-1f29-4c66-a184-2359f9defb17", "title": "Monoclonality of Lymphoproliferative Lesions in Cardiac-Transplant Recipients — Clonal Analysis Based on Immunoglobulin-Gene Rearrangements", "text": "【0】Monoclonality of Lymphoproliferative Lesions in Cardiac-Transplant Recipients — Clonal Analysis Based on Immunoglobulin-Gene Rearrangements\nAbstract\n--------\n\n【1】Whether lymphoproliferative disorders arising in immunosuppressed recipients of organ transplants are primarily neoplastic or hyperplastic in nature is a matter of controversy. Reports of polyclonal B-cell proliferations in these lesions suggest the presence of hyperplasia, but these disorders resemble lymphoma histologically and are clinically aggressive and often rapidly fatal, as expected of a malignant neoplastic disease. We examined tissue specimens from 10 cases of lymphoproliferative disease that occurred in immunosuppressed recipients of cardiac transplants. Specimens from nine of these patients lacked cellular immunoglobulin; however, analysis of DNA extracted from these tissues revealed that each lesion contained large numbers of cells possessing uniform, clonal rearrangements of immunoglobulin-gene DNA. Therefore, when first seen clinically these proliferations contained a notable monoclonal-cell population typical of conventional B-cell lymphomas that are not associated with immunosuppression. We therefore suggest that lymphoproliferative disorders in recipients of cardiac transplants are neoplastic at the earliest stages of detectable disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "22143bd5-3324-4cbf-86ae-85aacfffce9c", "title": "Abortion “Reversal” — Legislating without Evidence", "text": "【0】Abortion “Reversal” — Legislating without Evidence\nFour states require abortion providers to tell their patients about treatment that may reverse the effect of mifepristone if they change their mind after starting a medication abortion. But abortion reversal treatment remains experimental and unproven.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "45b5a539-70e7-48ff-893d-d9dc77bfba77", "title": "A Follow-up Study of Residents in Internal-Medicine, Pediatrics and Obstetrics-Gynecology Training Programs in Massachusetts — Implications for the Supply of Primary-Care Physicians", "text": "【0】A Follow-up Study of Residents in Internal-Medicine, Pediatrics and Obstetrics-Gynecology Training Programs in Massachusetts — Implications for the Supply of Primary-Care Physicians\nAbstract\n\n【1】A questionnaire survey of physicians who had been residents in internal medicine, pediatrics or obstetrics-gynecology in Massachusetts during the years 1967–1972 was undertaken to determine what specialties they now practice and the extent to which they deliver primary care. Over 600 physicians (74 per cent) responded. Devoting more than half their practice to a primary-care specialty were only 28 per cent of the former residents in internal medicine as compared with 56 per cent of those in pediatrics and 74 per cent of those in obstetrics-gynecology. For each group the fraction of full-time equivalent primary-care physicians produced was 0.27, 0.42 and 0.39 for the internal-medicine, pediatrics and obstetrics-gynecology programs respectively. These findings indicate that, although Massachusetts is meeting the requirements of PL 94–484 concerning the percentage of residency positions in the primary-care specialties, such compliance does not guarantee an adequate future supply of primary-care practitioners.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f02b74eb-5381-49a0-80a0-da64242c450f", "title": "The Use of in Vitro Technics to Study Drug-Induced Pancytopenia", "text": "【0】The Use of in Vitro Technics to Study Drug-Induced Pancytopenia\nAbstract\n--------\n\n【1】The confirmation of drug-induced marrow aplasia is difficult since rechallenging the patient can lead to serious morbidity. We used marrow-culture technics to challenge in vitro the bone marrow of a patient with marrow aplasia after ingestion of quinidine. There was no clinical or laboratory evidence of quinidine-mediated destruction of erythrocytes, leukocytes or platelets. By contrast, use of quinidine in combination with the patient's serum substantially inhibited in vitro growth of allogeneic marrow granulocytic and erythroid series. Furthermore, use of quinidine in combination with acute-phase serum (but not acute-phase serum alone or quinidine in combination with recovery-phase serum) inhibited growth of the patient's marrow. This observation suggests that both a transient serum factor and quinidine were responsible for the marrow aplasia.\n\n【2】These technics could be applied with minimal risk to similar patients and would permit in vitro rechallenge with the suspected drug.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "69c6e128-79fb-44c0-a568-893ad00f8ad8", "title": "Case 9-2023: A 20-Year-Old Man with Shortness of Breath and Proteinuria", "text": "【0】Case 9-2023: A 20-Year-Old Man with Shortness of Breath and Proteinuria\nA 20-year-old man was admitted with hemoptysis and hypoxemia. CT of the chest revealed pulmonary emboli; urinalysis showed proteinuria and hematuria. Diagnostic tests were performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "341536a2-de2f-4137-ae9f-fe7fd7668b03", "title": "The Patient Resident", "text": "【0】The Patient Resident\nThe patient’s eyes close as his diagnosis sinks in. The resident sits with him in a silence filled with racing fears of the unknown. It’s that moment when everything has changed for this man: he’s gone from before to after. The resident knows this moment all too well.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "61a5099f-92cf-42f6-a69d-b2091070828f", "title": "Weekly Paclitaxel in the Adjuvant Treatment of Breast Cancer", "text": "【0】Weekly Paclitaxel in the Adjuvant Treatment of Breast Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】We compared the efficacy of two different taxanes, docetaxel and paclitaxel, given either weekly or every 3 weeks, in the adjuvant treatment of breast cancer.\n\n【3】Methods\n-------\n\n【4】We enrolled 4950 women with axillary lymph node–positive or high-risk, lymph node–negative breast cancer. After randomization, all patients first received 4 cycles of intravenous doxorubicin and cyclophosphamide at 3-week intervals and were then assigned to intravenous paclitaxel or docetaxel given at 3-week intervals for 4 cycles or at 1-week intervals for 12 cycles. The primary end point was disease-free survival.\n\n【5】Results\n-------\n\n【6】As compared with patients receiving standard therapy (paclitaxel every 3 weeks), the odds ratio for disease-free survival was 1.27 among those receiving weekly paclitaxel (P=0.006), 1.23 among those receiving docetaxel every 3 weeks (P=0.02), and 1.09 among those receiving weekly docetaxel (P=0.29) (with an odds ratio >1 favoring the groups receiving experimental therapy). As compared with standard therapy, weekly paclitaxel was also associated with improved survival (odds ratio, 1.32; P=0.01). An exploratory analysis of a subgroup of patients whose tumors expressed no human epidermal growth factor receptor type 2 protein found similar improvements in disease-free and overall survival with weekly paclitaxel treatment, regardless of hormone-receptor expression. Grade 2, 3, or 4 neuropathy was more frequent with weekly paclitaxel than with paclitaxel every 3 weeks (27% vs. 20%).\n\n【7】Conclusions\n-----------\n\n【8】Weekly paclitaxel after standard adjuvant chemotherapy with doxorubicin and cyclophosphamide improves disease-free and overall survival in women with breast cancer. \n\n【9】Introduction\n------------\n\n【10】Adjuvant chemotherapy substantially reduces the risk of recurrence and death among women with operable breast cancer.  The addition of a taxane to an anthracycline-containing regimen, whether after or concurrently with anthracycline treatment, further reduces the risk of relapse. Two studies in which patients received four cycles of paclitaxel every 3 weeks after receiving four cycles of doxorubicin and cyclophosphamide every 3 weeks  established a new standard of care for operable breast cancer and led to regulatory approval of paclitaxel for axillary lymph node–positive breast cancer. Another study demonstrating that concurrent administration of docetaxel with doxorubicin and cyclophosphamide was more effective than fluorouracil, doxorubicin, and cyclophosphamide led to regulatory approval of docetaxel for node-positive breast cancer. \n\n【11】Questions remain, however, about the optimally effective taxane and the optimal schedule of administration of a taxane. Preclinical and indirect clinical evidence suggested that docetaxel was a more effective taxane than paclitaxel and that weekly paclitaxel was more effective than a conventional schedule of paclitaxel every 3 weeks.  Moreover, phase 3 trials of patients with metastatic breast cancer demonstrated that docetaxel every 3 weeks  or paclitaxel every week  was superior to paclitaxel every 3 weeks.\n\n【12】We conducted a study to compare the efficacies of two aspects of current adjuvant chemotherapy in patients with axillary lymph node–positive or high-risk, lymph node–negative breast cancer: paclitaxel versus docetaxel and a schedule of every 3 weeks versus a weekly schedule. The factorial design of the trial allowed comparison of paclitaxel every 3 weeks for 4 cycles with three experimental regimens — paclitaxel every week for 12 cycles, docetaxel every 3 weeks for 4 cycles, or docetaxel every week for 12 cycles — with each regimen given after a standard doxorubicin–cyclophosphamide regimen.\n\n【13】Methods\n-------\n\n【14】Study Patients\n--------------\n\n【15】We included in the study women who had operable, histologically confirmed adenocarcinoma of the breast with histologically involved lymph nodes (tumor stage T1, T2, or T3 and nodal stage N1 or N2) or high-risk, axillary node-negative disease (T2 or T3, N0) without distant metastases.\n\n【16】Chemotherapy\n------------\n\n【17】All women received doxorubicin (60 mg per square meter of body-surface area, given by slow intravenous push during a period of 5 to 15 minutes) and cyclophosphamide (600 mg per square meter by intravenous infusion for 30 to 60 minutes) every 3 weeks for four cycles. This therapy was followed by taxane therapy. The women were randomly assigned to 175 mg of paclitaxel per square meter by intravenous infusion for 3 hours every 3 weeks for 4 doses, 80 mg of paclitaxel per square meter by intravenous infusion for 1 hour weekly for 12 doses, 100 mg of docetaxel per square meter by intravenous infusion for 1 hour every 3 weeks for 4 doses, or 35 mg of docetaxel per square meter by intravenous infusion for 1 hour weekly for 12 doses. Guidelines for dose modification, premedication, and supportive care are given in the Supplementary Appendix .\n\n【18】Hormonal Therapy and Irradiation\n--------------------------------\n\n【19】Patients who had breast-sparing surgery received radiotherapy according to accepted standards of care after completion of all chemotherapy. Women who had a modified radical mastectomy also were permitted to receive radiotherapy after completion of all chemotherapy, at the discretion of the treating physician. Patients with hormone receptor–positive disease (defined as disease positive for estrogen receptors, progesterone receptors, or both) were required to take 20 mg of tamoxifen daily for 5 years. In June 2005, the protocol was modified to permit postmenopausal women who were taking tamoxifen to change to an aromatase inhibitor before completing 5 years of tamoxifen or to begin taking an aromatase inhibitor after completing a 5-year course of tamoxifen.\n\n【20】End Points\n----------\n\n【21】The primary end point was disease-free survival, defined as the time from randomization to disease recurrence (including death from recurrence if it was the first manifestation of recurrence), death without recurrence, or contralateral breast cancer. This end point differs from the end point of disease-free survival that was used in other trials of paclitaxel sponsored by the National Cancer Institute, which did not include contralateral breast cancer in the definition  or which included contralateral breast cancer and second primary cancers. \n\n【22】Study Protocol\n--------------\n\n【23】The protocol was coordinated by the Eastern Cooperative Oncology Group (ECOG) ; other participating groups included the Southwest Oncology Group (SWOG), the Cancer and Leukemia Group B (CALGB), and the North Central Cancer Treatment Group (NCCTG). The protocol was reviewed and approved by the institutional review board at each participating institution, and all patients provided written informed consent. All the authors vouch for the accuracy and completeness of the data.\n\n【24】Statistical Analysis\n--------------------\n\n【25】The primary comparisons were paclitaxel with docetaxel, regardless of the dosing schedule, and weekly dosing with dosing every 3 weeks, regardless of the taxane administered. The study design required enrollment of 4762 eligible patients with total information (the number of events required for a primary analysis) after 1042 observed failures (recurrences or deaths). This design had an 86% power to detect a 17.5% reduction in the hazard rate for failure among the docetaxel groups, or with weekly dosing instead of dosing every 3 weeks, at an overall two-sided significance level of 0.05.\n\n【26】Annual interim analyses were planned to begin about 2 years after initiation of the study and to continue until full information was obtained, with critical values determined from the O'Brien–Fleming boundary. All eligible patients undergoing randomization were included in the efficacy analysis. All treated patients were included in the adverse-events analyses, regardless of eligibility.\n\n【27】The primary prespecified analysis of disease-free survival and overall survival was performed with the use of the log-rank test. Also prespecified was that the test for paclitaxel versus docetaxel was stratified according to dosing schedule, number of positive nodes (none vs. one or more), and estrogen-receptor status (positive, negative, or unknown) and that the test for the weekly schedule as compared with the schedule of every 3 weeks was stratified according to the taxane administered, the number of positive nodes, and estrogen-receptor status. The tests for the comparisons between the groups receiving experimental treatment and the group receiving standard treatment (paclitaxel every 3 weeks) were stratified according to the number of positive nodes and estrogen-receptor status. Distributions of events with respect to time were estimated with the use of Kaplan–Meier analysis. Cox proportional-hazards models stratified according to the taxane administered, number of positive nodes, estrogen-receptor status, and dosing schedule were used to estimate hazard ratios and to test for significant differences in the times to events. All reported P values are two-sided. The significance level used in the pairwise comparisons between the groups receiving experimental treatment and the group receiving standard treatment was 0.017 on the basis of the Bonferroni correction for multiple comparisons, corresponding to an overall type I error rate of 0.05.\n\n【28】Because recent data suggest that patients with disease positive for hormone receptors and negative for human epidermal growth factor receptor type 2 (HER2) protein may not benefit from adjuvant paclitaxel therapy,  we performed a post hoc analysis of outcomes according to hormone-receptor status and HER2 expression. Details of this analysis are in the Supplementary Appendix .\n\n【29】Results\n-------\n\n【30】Patients\n--------\n\n【31】Table 1. Characteristics of the Patients.\n\n【32】Between October 1999 and January 2002, we enrolled 5052 patients, of whom 4950 (98%) were eligible for the study. Of the ineligible patients, 55 had a tumor margin within 1 mm of the excised specimen, 10 had fewer than six nodes removed, 6 had T4 or N3 disease, 5 had distant metastases before registration, and 26 were deemed ineligible for other reasons. The treatment groups were similar with regard to prognostic characteristics . The median age was 51 years (range, 19 to 84). Approximately 12% of the patients had no positive lymph nodes, 56% had one to three positive nodes, and 32% had four or more positive nodes. The tumor was positive for estrogen receptor, progesterone receptor, or both in 70% of patients and positive for HER2 in 19% (as determined in local institutional laboratories). Sixty percent of the patients had undergone mastectomy, and 40% had undergone breast-sparing surgery.\n\n【33】Treatment\n---------\n\n【34】The patients received a mean of 4.0 and a median of 4 (range, 1 to 5) cycles of treatment with doxorubicin and cyclophosphamide; 97% of patients received all 4 cycles. The mean number of taxane cycles received was 3.9 for the group receiving paclitaxel every 3 weeks, 11.4 for the group receiving weekly paclitaxel, 3.8 for the group receiving docetaxel every 3 weeks, and 10.8 for the group receiving weekly docetaxel. The proportion of women in whom the taxane dose was modified was 22%, 29%, 28%, and 40%, respectively. The proportion who received all doses was 95%, 88%, 87%, and 75%, respectively. The proportion who received half or less than half of the planned doses was 4%, 5%, 9%, and 11%, respectively. There were no significant differences among the four groups in the percentage of patients who received hormonal therapy; the percentages ranged from 27 to 30% for tamoxifen alone, 43 to 45% for tamoxifen followed by an aromatase inhibitor, and 3 to 4% for an aromatase inhibitor alone. The median time from randomization to initiation of an aromatase inhibitor for those who crossed over was 3.7 years.\n\n【35】Taxane Type and Schedule of Administration\n------------------------------------------\n\n【36】At the time of the analysis after a median follow-up of 63.8 months, 1048 patients had a recurrence of breast cancer or cancer in the contralateral breast, and 686 had died. There were no significant differences in disease-free survival between the paclitaxel-treated groups and the docetaxel-treated groups (odds ratio, 1.03; 95% confidence interval (\\[CI\\], 0.91 to 1.17; P=0.61) or between the groups receiving weekly treatment and those receiving treatment every 3 weeks (odds ratio, 1.06; 95% CI, 0.94 to 1.20; P=0.33). The results were similar when the data were analyzed according to definitions of disease-free survival used in other trials of treatment with doxorubicin and cyclophosphamide followed by a taxane (data not shown). However, in a Cox proportional-hazards model that included the taxane administered (paclitaxel or docetaxel), the schedule of administration (weekly or every 3 weeks), and their interaction, the interaction of docetaxel and the weekly schedule was significant for disease-free survival (P=0.003) and for overall survival (P=0.01).\n\n【37】Disease-free Survival\n---------------------\n\n【38】Figure 1. Disease-free Survival.\n\n【39】Panel A shows disease-free survival according to treatment group. Panel B shows the odds ratios for disease-free survival in the experimental groups as compared with the group receiving standard treatment (paclitaxel every 3 weeks).\n\n【40】Figure 1A shows the Kaplan–Meier curves for disease-free survival in the four treatment groups. The estimated 5-year survival rates were 76.9% for the group receiving paclitaxel every 3 weeks, 81.5% for the group receiving weekly paclitaxel, 81.2% for the group receiving docetaxel every 3 weeks, and 77.6% for the group receiving weekly docetaxel. As compared with the group receiving paclitaxel every 3 weeks, there was significantly better disease-free survival in the group receiving weekly paclitaxel (odds ratio, 1.27; P=0.006) and in the group receiving docetaxel every 3 weeks (odds ratio, 1.23; P=0.02), but not in the group receiving weekly docetaxel (odds ratio, 1.09; P=0.29) . The results were similar when the definition of end point did not include contralateral breast cancer or contralateral breast cancer and second primary nonbreast cancer .\n\n【41】Overall Survival\n----------------\n\n【42】Figure 2. Overall Survival.\n\n【43】Panel A shows overall survival according to treatment group. Panel B shows the odds ratios for overall survival in the experimental groups as compared with the group receiving standard treatment (paclitaxel every 3 weeks).\n\n【44】Figure 2A shows the Kaplan–Meier curves for overall survival in the four groups. The estimated 5-year overall survival rates were 86.5% for the group receiving paclitaxel every 3 weeks, 89.7% for the group receiving weekly paclitaxel, 87.3% for the group receiving docetaxel every 3 weeks, and 86.2% for the group receiving weekly docetaxel. As compared with the group receiving paclitaxel every 3 weeks, overall survival was significantly better in the group receiving weekly paclitaxel (odds ratio, 1.32; P=0.01), but not in the groups receiving docetaxel every 3 weeks (odds ratio, 1.13; P=0.25) or weekly docetaxel (odds ratio, 1.02; P=0.80) .\n\n【45】Expression of Hormone Receptors and HER2\n----------------------------------------\n\n【46】Because of evidence that the benefit of taxane therapy may be restricted to patients with hormone receptor–negative disease  and HER2-positive disease,  we evaluated the influence of hormone-receptor status and HER2 expression on the effectiveness of weekly paclitaxel. This analysis should be regarded as exploratory, because it was not prespecified when the study was initiated in 1999.\n\n【47】Figure 3. Exploratory Analysis of Disease-free and Overall Survival According to Expression of Human Epidermal Growth Factor Receptor Type 2 (HER2).\n\n【48】Panel A shows the odds ratios for disease-free survival and Panel B shows the odds ratios for overall survival according to HER2 expression in the experimental groups as compared with the group receiving paclitaxel every 3 weeks.Figure 4.  Figure 4. Exploratory Analysis of Disease-free and Overall Survival According to Expression of Hormone Receptors (HR).\n\n【49】The figure shows the odds ratios for disease-free and overall survival the group receiving weekly paclitaxel in as compared with the group receiving paclitaxel every 3 weeks among patients with HER2-negative disease according to whether the disease was positive or negative for hormone receptors.\n\n【50】Figure 3 shows that patients with HER2-negative disease who were treated with weekly paclitaxel had improved disease-free survival  and overall survival ; these effects were not seen in patients with HER2-positive disease or in those treated with docetaxel. Figure 4 indicates that patients with HER2-negative disease who received weekly paclitaxel had improved disease-free survival and overall survival, irrespective of their hormone-receptor status. Similar effects were not observed in the group receiving docetaxel every 3 weeks (data not shown).\n\n【51】Toxic Effects\n-------------\n\n【52】Table 2. Toxic Effects of Paclitaxel and Docetaxel.\n\n【53】During treatment with doxorubicin and cyclophosphamide, 40% of patients had grade 2 toxic effects, 13% had grade 3 toxic effects, and 39% had grade 4 toxic effects; the percentages were similar in the four treatment groups . Table 2 and Table 2 in the Supplementary Appendix show the toxic-effects profile of the taxane component of therapy for each group. Twenty-eight percent of patients receiving paclitaxel weekly had grade 3 or 4 toxic effects, as compared with 30% of those receiving paclitaxel every 3 weeks (P=0.32), 71% of those receiving docetaxel every 3 weeks (P<0.001), and 45% of those receiving docetaxel weekly (P<0.001). The higher proportions of grades 3 and 4 toxic effects in the group receiving docetaxel every 3 weeks were due to a 46% incidence of neutropenia (as compared with an incidence of ≤4% for the other groups), which resulted in higher rates of febrile neutropenia (16%) and infection (13%) than in the other groups (1% and ≤4%, respectively, for all other groups). The incidence of grade 3 or 4 neuropathy in the four groups ranged from 4 to 8%, but the group receiving weekly paclitaxel had a significantly higher incidence of grade 2, 3, or 4 neuropathy (27%) than any of the other treatment groups (P<0.001 for each comparison).\n\n【54】Discussion\n----------\n\n【55】This trial was designed to compare the efficacy of paclitaxel with that of docetaxel and to compare the standard taxane schedule (every 3 weeks) with a weekly schedule in nearly 5000 women with axillary lymph node–positive or high-risk, lymph node–negative breast cancer. We found no significant differences in survival between the groups treated with paclitaxel and those treated with docetaxel or between the groups treated weekly and those treated every 3 weeks. However, the interpretation of our results is complicated by an unanticipated interaction between the type of taxane administered and the treatment schedule for the group receiving weekly docetaxel. In a comparison of the three experimental groups with the group receiving standard paclitaxel treatment, the group receiving weekly paclitaxel and the group receiving docetaxel every 3 weeks had significantly improved disease-free survival, and the group receiving weekly paclitaxel had significantly improved overall survival. In comparison with the group receiving standard therapy, the group receiving weekly paclitaxel had significantly more moderate-to-severe neuropathy and the group receiving docetaxel every 3 weeks had significantly more severe neutropenia and its associated complications. The 32% reduction in the hazard ratio for death afforded by weekly paclitaxel is similar to that observed for anthracycline-containing chemotherapy as compared with no adjuvant cytotoxic therapy.  The results are consistent with studies of metastatic breast cancer that demonstrated a benefit of weekly paclitaxel  or docetaxel every 3 weeks,  as compared with paclitaxel every 3 weeks.\n\n【56】A similar benefit was not observed for the group receiving weekly docetaxel, a result that may be attributable in part to the less acceptable adverse-events rates and poorer adherence to treatment in this group. The findings are also consistent with the results of trials of metastatic breast and prostate cancer that indicate that docetaxel may be more effective if given every 3 weeks rather than weekly.  The results were not changed when the data were analyzed according to definitions of end points similar to those used in other trials evaluating paclitaxel.  Guidelines for standardization of end-point definitions in trials of adjuvant breast-cancer treatment have recently been proposed that we think should be routinely incorporated into future trials. \n\n【57】Other studies have suggested that the benefits of taxane-based therapy are driven largely by improved outcomes in hormone receptor–negative disease  or HER2-positive disease and that there may be little if any benefit of taxane therapy for the 50% or more of patients with hormone receptor–positive, HER2-negative disease.  In our trial, there were similar trends favoring weekly paclitaxel in patients with HER2-negative disease that was either hormone receptor–positive or hormone receptor–negative. Although hormone-receptor status was not determined centrally, analysis of an another ECOG-coordinated Intergroup trial conducted during a similar time period demonstrated 90% concordance for hormone-receptor expression between local laboratories and a central laboratory, and 95% concordance for tumors determined to be HER2-negative in local laboratories.  Another study, which compared six cycles of fluorouracil, epirubicin, and cyclophosphamide every 3 weeks with four cycles of fluorouracil, epirubicin, and cyclophosphamide followed by weekly paclitaxel for 8 weeks, also showed a better outcome in patients treated with weekly paclitaxel, including patients with hormone receptor–positive or hormone receptor–negative disease, as determined by testing in a central laboratory according to a prespecified analysis plan.  Taken together, these results suggest that the benefits are similar in hormone receptor–positive and hormone receptor–negative breast cancer.\n\n【58】In conclusion, treatment with doxorubicin and cyclophosphamide followed by weekly paclitaxel is associated with improved disease-free survival and overall survival in comparison with treatment with doxorubicin and cyclophosphamide followed by paclitaxel given every 3 weeks. We found no evidence that women with hormone receptor–positive, HER2-negative breast cancer derived less benefit than those with breast cancer negative for hormone receptors or positive for HER2.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2620ab5c-c9d4-4b74-82a7-99468ecf769e", "title": "Do We Really Need Another Time-Series Study of the PM", "text": "【0】Do We Really Need Another Time-Series Study of the PM\nArticle\n-------\n\n【1】The link between particulate pollution and mortality was originally recognized in the context of severe episodes of poor air quality in the 20th century, such as the London Fog of 1952.  These episodes showed clear evidence that the number of deaths increased in association with high levels of particulate matter (PM). The policy response to the increasing evidence of the effects of air pollution on public health was for governments to develop air-quality regulations. In the United States, the Clean Air Act of 1970 mandated that the Environmental Protection Agency (EPA) develop national ambient air-quality standards (NAAQS) to protect even the most vulnerable members of the general population from adverse health effects.  An NAAQS for PM was initially established in 1971.\n\n【2】The current primary NAAQS for PM applies to particles with an aerodynamic diameter of 2.5 μm or less (PM <sub>2.5 </sub> ) — particles that are small enough to be deposited in the alveoli. A secondary NAAQS applies to particles with an aerodynamic diameter of 10 μm or less (PM <sub>10 </sub> ) — particles that can be deposited in large airways. The epidemiologic evidence in support of the adoption of an NAAQS for PM <sub>2.5 </sub> was largely from time-series studies.  Time-series analyses include daily measures of health events (e.g., daily mortality), regressed against concentrations of PM (e.g., 24-hour average PM <sub>2.5 </sub> ) and weather variables (e.g., daily average temperature) for a given geographic area. The population serves as its own control, and confounding by population characteristics is negligible because these are stable over short time frames. Time-series studies can be confounded by time-varying factors such as influenza epidemics and temperature; however, statistical methods to reduce such confounding have been developed. \n\n【3】Many time-series studies have been conducted in cities in various countries around the world. Efforts have been made to include larger regions in time-series analyses to increase the generalizability of the reported associations.  A meta-analysis has shown that the PM <sub>2.5 </sub> –mortality association remains robust in pooled analyses.  Multiple longitudinal cohort studies of the association between long-term PM <sub>2.5 </sub> exposure and mortality, in which individual-level covariates were included in the analyses, have generally shown even stronger associations, providing important support for the evidence from time-series studies.  Moreover, experimental data from exposure studies in animals and controlled exposure studies in humans have increasingly provided mechanistic evidence in support of the epidemiologic findings.\n\n【4】Given the abundance of evidence in support of an association between short-term PM <sub>2.5 </sub> exposure and mortality, what is the contribution of the time-series study by Liu et al. in this issue of the _Journal_ ?  First, this study included almost 60 million deaths from 652 cities in 24 countries, thereby greatly increasing the generalizability of the association and decreasing the likelihood that the reported associations are subject to confounding bias. In observations consistent with previous studies, all-cause (nonaccidental), cardiovascular, and respiratory mortality were associated with short-term exposures to both PM <sub>10 </sub> and PM <sub>2.5 </sub> . The strength of the associations was reduced but remained significant in two-pollutant models that addressed potential confounding by gaseous pollutants.\n\n【5】Perhaps the most interesting result of the study by Liu et al. is from their concentration–response analysis. On the basis of studies of exposure to multiple combustion sources of PM <sub>2.5 </sub> (outdoor air pollution, secondhand tobacco smoke, and active tobacco smoking) and cardiovascular mortality, Pope et al. proposed that the shape of the concentration–response relation is curvilinear, with a lesser slope at higher exposure levels.  Although other studies have reported evidence of such curvilinearity, the current study of PM data from many regions around the world provides the strongest evidence to date that higher levels of exposure may be associated with a lower per-unit risk. Regions that have lower exposures had a higher per-unit risk. This finding has profound policy implications, especially given that no threshold of effect was found. Even high-income countries, such as the United States, with relatively good air quality could still see public health benefits from further reduction of ambient PM concentrations (i.e., below the current NAAQS).\n\n【6】The Clean Air Act requires a periodic review of the weight of evidence of adverse health effects of regulated air pollutants by an external body of scientists, called the Clean Air Scientific Advisory Committee (CASAC). Controlled exposure studies in humans, toxicologic studies in animals, and epidemiologic studies are included in the weight-of-evidence reviews by CASAC. In the context of the current review of the NAAQS for PM and the Trump Administration’s view of inconvenient scientific evidence as anathema,  Anthony Cox, the current chair of CASAC, has characterized the abundant observational epidemiologic evidence from time-series and cohort studies of the PM <sub>2.5 </sub> –mortality association as not proving causality. Rather than relying on the weight-of-the-evidence approach that the EPA has traditionally used to infer causation, Cox wants to rely on studies that use a theoretical approach called “manipulative causality.”  This theory restricts epidemiologic evidence that may be considered acceptable to assess causality to results from intervention studies or studies that have been analyzed with the use of causal inference statistical methods. The effort to exclude all observational epidemiologic data that have not been analyzed in a manipulative causality framework not only makes no sense, it would set a dangerous precedent for environmental policy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d9fc8204-6774-42c7-838a-5435192c7f96", "title": "Shared Decision Making — The Pinnacle of Patient-Centered Care", "text": "【0】Shared Decision Making — The Pinnacle of Patient-Centered Care\n### Audio Interview\n\n【1】 Interview with Michael Barry on patient-centered care and shared decision making. \n\n【2】The most important attribute of patient-centered care is the active engagement of patients when fateful health care decisions must be made — when they arrive at a crossroads of medical options, where diverging paths have different and important consequences.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5c02a70c-dcbc-4c62-bc53-680a02c036b6", "title": "Medication Reconciliation for Controlled Substances — An “Ideal” Prescription-Drug Monitoring Program", "text": "【0】Medication Reconciliation for Controlled Substances — An “Ideal” Prescription-Drug Monitoring Program\nIn the face of an epidemic of misuse of prescription opioids and related deaths, many states have created prescription-drug monitoring programs to track prescribing of controlled substances. As such programs are developed, expanded, or retooled, reanalysis is in order.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4fb68fd2-c399-4402-9b5c-21bdbe4a8016", "title": "FDA Approval of Doxylamine–Pyridoxine Therapy for Use in Pregnancy", "text": "【0】FDA Approval of Doxylamine–Pyridoxine Therapy for Use in Pregnancy\nArticle\n-------\n\n【1】In 1983, the combination-drug product Bendectin (Merrell Dow), consisting of 10 mg of doxylamine succinate and 10 mg of pyridoxine hydrochloride per tablet, was voluntarily withdrawn from the U.S. market by the manufacturer. For the next 30 years, there were no medications that had been approved by the Food and Drug Administration (FDA) for the treatment of nausea and vomiting of pregnancy. Recently, the FDA approved Diclegis (Duchesnay), a product with the same combination of doxylamine and pyridoxine that had been marketed as Bendectin. The Bendectin experience serves as an informative case study of how decisions that are not science-based may affect the marketing and availability of a drug product and lead to adverse public health consequences.\n\n【2】Nausea and vomiting occur in as many as 80% of all pregnant women between 6 and 12 weeks of gestation. Symptoms are usually self-limiting and resolve with nonpharmacologic conservative measures. Roughly one third of women with nausea and vomiting of pregnancy have symptoms that are clinically significant, resulting in diminished quality of life. About 1% of pregnant women may have progression to hyperemesis gravidarum, a condition characterized by persistent vomiting, loss of more than 5% of body weight, ketonuria, electrolyte imbalance, acidosis, nutritional deficiencies, and dehydration, all of which pose further health risks to both mother and fetus.\n\n【3】Bendectin had originally been approved in 1956 as a three-agent formulation, consisting of 10 mg of dicyclomine hydrochloride (an antispasmodic agent), 10 mg of doxylamine succinate (an antihistamine), and 10 mg of pyridoxine hydrochloride (vitamin B <sub>6 </sub> ). In the 1970s, dicyclomine hydrochloride was determined to be ineffective for treating nausea and vomiting of pregnancy, and Bendectin was consequently reformulated as a two-drug combination that the FDA approved in 1976. From 1956 to 1983, Bendectin was widely prescribed; at the peak of its use, as many as 25% of pregnant women in the United States took the product. \n\n【4】In the historical context of two notorious teratogens, thalidomide and diethylstilbestrol, initial reports questioning the safety of Bendectin ignited public fears. In the late 1960s and through the 1970s, letters to the editors of medical journals began to report an association between Bendectin use and birth defects. The mainstream media reported stories as well, and law firms launched publicity campaigns claiming that Bendectin was a teratogen. In January 1980, the first major lawsuit ( _Mekdeci v. Merrell National Laboratories, a Division of Richardson-Merrell, Inc_ .) was heard in Florida, and by the time the product was withdrawn in 1983, there were more than 300 pending lawsuits attributing various birth defects to the use of Bendectin. Limb-reduction deformities, cardiac defects, oral clefts, and genital tract malformations were among the conditions alleged to be associated with Bendectin use. However, courtroom testimony claiming that Bendectin was a human teratogen was markedly devoid of evidence-based corroboration.  Merrell Dow indicated that its decision to withdraw Bendectin was based not on safety issues but on financial concerns. In the wake of the Bendectin allegations, the company's insurance premiums had risen to $10 million per year, only $3 million less than the total income from Bendectin sales.\n\n【5】In 1979, the FDA issued a “Talk Paper” stating that studies in animals and several large epidemiologic studies had provided “no adequate evidence linking Bendectin with an increased risk of birth defects.” In September 1980, the FDA Fertility and Maternal Health Drugs Advisory Committee reviewed 13 epidemiologic studies, 11 of which had found no association of Bendectin with an increased risk of birth defects and 2 of which suggested a weak association with heart defects and cleft palate. The committee took into account the strengths and limitations of these epidemiologic studies and unanimously concluded that, overall, the data did not show an association between Bendectin and birth defects. Nevertheless, the committee recommended that product labeling be revised to include a patient package insert and to narrow the indication to nausea and vomiting of pregnancy that had not been alleviated with conservative measures. In addition, the continuation of epidemiologic studies was encouraged.\n\n【6】Two independent meta-analyses (pooled observational studies) of Bendectin and congenital birth defects, published after the product was withdrawn from the market, similarly concluded that Bendectin is not a human teratogen.  The first of these analyzed 17 cohort and case–control studies conducted between 1963 and 1985, and the second involved 27 cohort and case–control studies conducted between 1963 and 1991. In addition, data maintained by the Birth Defect Monitoring Program of the Centers for Disease Control and Prevention (CDC) did not show an association between birth defects and Bendectin use. These data show that during the period from 1985 through 1987, which was after the product was withdrawn, the incidence of birth defects was the same as that seen during the peak period (1978 through 1980) of Bendectin use. Given that as many as one quarter of U.S. pregnant women were using Bendectin by 1980, the fact that birth-defect incidence did not fall after product withdrawal is inconsistent with drug teratogenicity. \n\n【7】Aside from the fact that a considerable quantity of data, both direct and indirect, has failed to produce evidence of Bendectin-associated teratogenicity, the withdrawal of Bendectin may actually have had adverse effects on pregnant women. According to data from the National Center for Health Statistics, the number of hospitalizations in the United States for nausea and vomiting of pregnancy increased from 7 per 1000 live births (baseline data from 1974 to 1980) to 15 to 16 per 1000 live births during the period from 1981 through 1987.  Furthermore, it is not possible to know how many women, fearing that they had caused harm to their fetus, underwent elective abortions; anecdotal reports suggest that some did.\n\n【8】The decades-long history of doxylamine–pyridoxine emphasizes the importance of making clinical decisions on the basis of scientific evidence. The FDA's approval of Diclegis was based on efficacy and safety data from a randomized, placebo-controlled clinical trial and also took into account the extensive data described above showing that combined treatment with doxylamine succinate and pyridoxine hydrochloride is not teratogenic. These data reveal a favorable risk–benefit profile for Diclegis in the treatment of nausea and vomiting of pregnancy that has been refractory to nonpharmacologic treatment. Although combined doxylamine–pyridoxine treatment is already the single most studied pharmacologic therapy for use in pregnancy, the FDA will continue to carefully monitor postmarketing data related to Diclegis use. The Diclegis story reminds us that reliance on evidence-based practices, with the use of multiple streams of data, is the most appropriate way to evaluate drug safety.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "bb9eb5ad-5978-4464-9159-6cc02efdef73", "title": "Current Concepts: Responding to Medical Events during Commercial Airline Flights", "text": "【0】Current Concepts: Responding to Medical Events during Commercial Airline Flights\nPhysicians are often asked to provide assistance when symptoms develop in a passenger during a commercial flight. This Review Article identifies the most common problems that develop during air travel. The authors recommend ways to respond to such events and describe the resources that are available to physicians and flight crews while the aircraft is still airborne.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "c4b10461-6a34-4605-9cd3-221c21b3cdef", "title": "The Pocked Erythrocyte — Red-Cell Surface Alterations in Reticuloendothelial Immaturity of the Neonate", "text": "【0】The Pocked Erythrocyte — Red-Cell Surface Alterations in Reticuloendothelial Immaturity of the Neonate\nAbstract\n--------\n\n【1】A study of red-cell surface morphology using interference-contrast microscopy showed striking morphologic differences in premature and term infants, children of various ages and normal adults. In normal adults 2.6 per cent of erythrocytes gave the appearance of having small (0.2 to 0.5 μ) pits or craters on their surface. By contrast, premature and term infants had a mean of 47.2 per cent and 24.3 per cent pitted cells respectively, reaching near adult values by two months of age in term infants and taking somewhat longer in premature infants. An inverse correlation was observed between birth weight and the number of pitted cells.\n\n【2】The finding of increased numbers of pitted cells in the neonate may reflect reticuloendothelial or splenic hypofunction since this type of cell has so far been observed with such frequency only in patients without spleens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a798fd9c-5f10-4013-852c-6cceefb22dcb", "title": "Periodic Microcirculatory Flow in Patients with Sickle-Cell Disease", "text": "【0】Periodic Microcirculatory Flow in Patients with Sickle-Cell Disease\nAbstract\n--------\n\n【1】We have applied the technique of laser-Doppler velocimetry to compare patterns of cutaneous blood flow in the forearms of patients with stable sickle-cell disease, with the patterns in normal subjects matched for age, race, and sex, and in patients with anemia due to β <sup>+ </sup> \\-thalassemia. The mean resting blood flow in the patients with sickle-cell disease was comparable to that of the control groups but was associated with large, local oscillations in flow with periods of 7 to 10 seconds and peak-to-trough magnitudes about half the mean flow. Oscillations occurred simultaneously at sites separated by 1 cm but were independent in phase and frequency. Since these laser-Doppler measurements represent the average flow pattern in about 1 mm  of skin (i.e., in approximately 50 to 70 capillary loops), these results suggest that microcirculatory flow in patients with sickle-cell disease proceeds by synchronization of rhythmic flow in large domains of microvessels. These findings indicate that periodic flow may be a compensatory mechanism to offset the deleterious altered rheology of erythrocytes containing polymerized hemoglobin S, and suggest that laser-Doppler velocimetry may be a useful method to investigate microvascular physiology in patients with sickle-cell disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d4910c0b-b63c-4b2f-af25-f164c5f3fbce", "title": "Brief Report: A Novel Coronavirus from Patients with Pneumonia in China, 2019", "text": "【0】Brief Report: A Novel Coronavirus from Patients with Pneumonia in China, 2019\nSummary\n-------\n\n【1】In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. \n\n【2】Introduction\n------------\n\n【3】Emerging and reemerging pathogens are global challenges for public health.  Coronaviruses are enveloped RNA viruses that are distributed broadly among humans, other mammals, and birds and that cause respiratory, enteric, hepatic, and neurologic diseases.  Six coronavirus species are known to cause human disease.  Four viruses — 229E, OC43, NL63, and HKU1 — are prevalent and typically cause common cold symptoms in immunocompetent individuals.  The two other strains — severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) — are zoonotic in origin and have been linked to sometimes fatal illness.  SARS-CoV was the causal agent of the severe acute respiratory syndrome outbreaks in 2002 and 2003 in Guangdong Province, China.  MERS-CoV was the pathogen responsible for severe respiratory disease outbreaks in 2012 in the Middle East.  Given the high prevalence and wide distribution of coronaviruses, the large genetic diversity and frequent recombination of their genomes, and increasing human–animal interface activities, novel coronaviruses are likely to emerge periodically in humans owing to frequent cross-species infections and occasional spillover events. \n\n【4】In late December 2019, several local health facilities reported clusters of patients with pneumonia of unknown cause that were epidemiologically linked to a seafood and wet animal wholesale market in Wuhan, Hubei Province, China.  On December 31, 2019, the Chinese Center for Disease Control and Prevention (China CDC) dispatched a rapid response team to accompany Hubei provincial and Wuhan city health authorities and to conduct an epidemiologic and etiologic investigation. We report the results of this investigation, identifying the source of the pneumonia clusters, and describe a novel coronavirus detected in patients with pneumonia whose specimens were tested by the China CDC at an early stage of the outbreak. We also describe clinical features of the pneumonia in two of these patients.\n\n【5】Methods\n-------\n\n【6】Viral Diagnostic Methods\n------------------------\n\n【7】Four lower respiratory tract samples, including bronchoalveolar-lavage fluid, were collected from patients with pneumonia of unknown cause who were identified in Wuhan on December 21, 2019, or later and who had been present at the Huanan Seafood Market close to the time of their clinical presentation. Seven bronchoalveolar-lavage fluid specimens were collected from patients in Beijing hospitals with pneumonia of known cause to serve as control samples. Extraction of nucleic acids from clinical samples (including uninfected cultures that served as negative controls) was performed with a High Pure Viral Nucleic Acid Kit, as described by the manufacturer (Roche). Extracted nucleic acid samples were tested for viruses and bacteria by polymerase chain reaction (PCR), using the RespiFinderSmart22kit (PathoFinder BV) and the LightCycler 480 real-time PCR system, in accordance with manufacturer instructions.  Samples were analyzed for 22 pathogens (18 viruses and 4 bacteria) as detailed in the Supplementary Appendix . In addition, unbiased, high-throughput sequencing, described previously,  was used to discover microbial sequences not identifiable by the means described above. A real-time reverse transcription PCR (RT-PCR) assay was used to detect viral RNA by targeting a consensus RdRp region of pan β-CoV, as described in the Supplementary Appendix .\n\n【8】Isolation of Virus\n------------------\n\n【9】Bronchoalveolar-lavage fluid samples were collected in sterile cups to which virus transport medium was added. Samples were then centrifuged to remove cellular debris. The supernatant was inoculated on human airway epithelial cells,  which had been obtained from airway specimens resected from patients undergoing surgery for lung cancer and were confirmed to be special-pathogen-free by NGS. \n\n【10】Human airway epithelial cells were expanded on plastic substrate to generate passage-1 cells and were subsequently plated at a density of 2.5×10  cells per well on permeable Transwell-COL (12-mm diameter) supports. Human airway epithelial cell cultures were generated in an air–liquid interface for 4 to 6 weeks to form well-differentiated, polarized cultures resembling in vivo pseudostratified mucociliary epithelium. \n\n【11】Prior to infection, apical surfaces of the human airway epithelial cells were washed three times with phosphate-buffered saline; 150 μl of supernatant from bronchoalveolar-lavage fluid samples was inoculated onto the apical surface of the cell cultures. After a 2-hour incubation at 37°C, unbound virus was removed by washing with 500 μl of phosphate-buffered saline for 10 minutes; human airway epithelial cells were maintained in an air–liquid interface incubated at 37°C with 5% carbon dioxide. Every 48 hours, 150 μl of phosphate-buffered saline was applied to the apical surfaces of the human airway epithelial cells, and after 10 minutes of incubation at 37°C the samples were harvested. Pseudostratified mucociliary epithelium cells were maintained in this environment; apical samples were passaged in a  diluted vial stock to new cells. The cells were monitored daily with light microscopy, for cytopathic effects, and with RT-PCR, for the presence of viral nucleic acid in the supernatant. After three passages, apical samples and human airway epithelial cells were prepared for transmission electron microscopy.\n\n【12】Transmission Electron Microscopy\n--------------------------------\n\n【13】Supernatant from human airway epithelial cell cultures that showed cytopathic effects was collected, inactivated with 2% paraformaldehyde for at least 2 hours, and ultracentrifuged to sediment virus particles. The enriched supernatant was negatively stained on film-coated grids for examination. Human airway epithelial cells showing cytopathic effects were collected and fixed with 2% paraformaldehyde–2.5% glutaraldehyde and were then fixed with 1% osmium tetroxide dehydrated with grade ethanol embedded with PON812 resin. Sections (80 nm) were cut from resin block and stained with uranyl acetate and lead citrate, separately. The negative stained grids and ultrathin sections were observed under transmission electron microscopy.\n\n【14】Viral Genome Sequencing\n-----------------------\n\n【15】RNA extracted from bronchoalveolar-lavage fluid and culture supernatants was used as a template to clone and sequence the genome. We used a combination of Illumina sequencing and nanopore sequencing to characterize the virus genome. Sequence reads were assembled into contig maps (a set of overlapping DNA segments) with the use of CLC Genomics software, version 4.6.1 (CLC Bio). Specific primers were subsequently designed for PCR, and 5′- or 3′-RACE (rapid amplification of cDNA ends) was used to fill genome gaps from conventional Sanger sequencing. These PCR products were purified from gels and sequenced with a BigDye Terminator v3.1 Cycle Sequencing Kit and a 3130XL Genetic Analyzer, in accordance with the manufacturers’ instructions.\n\n【16】Multiple-sequence alignment of the 2019-nCoV and reference sequences was performed with the use of Muscle. Phylogenetic analysis of the complete genomes was performed with RAxML (13) with 1000 bootstrap replicates and a general time-reversible model used as the nucleotide substitution model.\n\n【17】Results\n-------\n\n【18】Patients\n--------\n\n【19】Figure 1. Chest Radiographs.\n\n【20】Shown are chest radiographs from Patient 2 on days 8 and 11 after the onset of illness. The trachea was intubated and mechanical ventilation instituted in the period between the acquisition of the two images. Bilateral fluffy opacities are present in both images but are increased in density, profusion, and confluence in the second image; these changes are most marked in the lower lung fields. Changes consistent with the accumulation of pleural liquid are also visible in the second image.\n\n【21】Three adult patients presented with severe pneumonia and were admitted to a hospital in Wuhan on December 27, 2019. Patient 1 was a 49-year-old woman, Patient 2 was a 61-year-old man, and Patient 3 was a 32-year-old man. Clinical profiles were available for Patients 1 and 2. Patient 1 reported having no underlying chronic medical conditions but reported fever (temperature, 37°C to 38°C) and cough with chest discomfort on December 23, 2019. Four days after the onset of illness, her cough and chest discomfort worsened, but the fever was reduced; a diagnosis of pneumonia was based on computed tomographic (CT) scan. Her occupation was retailer in the seafood wholesale market. Patient 2 initially reported fever and cough on December 20, 2019; respiratory distress developed 7 days after the onset of illness and worsened over the next 2 days , at which time mechanical ventilation was started. He had been a frequent visitor to the seafood wholesale market. Patients 1 and 3 recovered and were discharged from the hospital on January 16, 2020. Patient 2 died on January 9, 2020. No biopsy specimens were obtained.\n\n【22】Detection and Isolation of a Novel Coronavirus\n----------------------------------------------\n\n【23】Three bronchoalveolar-lavage samples were collected from Wuhan Jinyintan Hospital on December 30, 2019. No specific pathogens (including HCoV-229E, HCoV-NL63, HCoV-OC43, and HCoV-HKU1) were detected in clinical specimens from these patients by the RespiFinderSmart22kit. RNA extracted from bronchoalveolar-lavage fluid from the patients was used as a template to clone and sequence a genome using a combination of Illumina sequencing and nanopore sequencing. More than 20,000 viral reads from individual specimens were obtained, and most contigs matched to the genome from lineage B of the genus betacoronavirus — showing more than 85% identity with a bat SARS-like CoV (bat-SL-CoVZC45, MG772933.1) genome published previously. Positive results were also obtained with use of a real-time RT-PCR assay for RNA targeting to a consensus RdRp region of pan β-CoV (although the cycle threshold value was higher than 34 for detected samples). Virus isolation from the clinical specimens was performed with human airway epithelial cells and Vero E6 and Huh-7 cell lines. The isolated virus was named 2019-nCoV.\n\n【24】Figure 2. Cytopathic Effects in Human Airway Epithelial Cell Cultures after Inoculation with 2019-nCoV.\n\n【25】To determine whether virus particles could be visualized in 2019-nCoV–infected human airway epithelial cells, mock-infected and 2019-nCoV–infected human airway epithelial cultures were examined with light microscopy daily and with transmission electron microscopy 6 days after inoculation. Cytopathic effects were observed 96 hours after inoculation on surface layers of human airway epithelial cells; a lack of cilium beating was seen with light microcopy in the center of the focus . No specific cytopathic effects were observed in the Vero E6 and Huh-7 cell lines until 6 days after inoculation.\n\n【26】Figure 3. Visualization of 2019-nCoV with Transmission Electron Microscopy.\n\n【27】Negative-stained 2019-nCoV particles are shown in Panel A, and 2019-nCoV particles in the human airway epithelial cell ultrathin sections are shown in Panel B. Arrowheads indicate extracellular virus particles, arrows indicate inclusion bodies formed by virus components, and triangles indicate cilia.\n\n【28】Electron micrographs of negative-stained 2019-nCoV particles were generally spherical with some pleomorphism . Diameter varied from about 60 to 140 nm. Virus particles had quite distinctive spikes, about 9 to 12 nm, and gave virions the appearance of a solar corona. Extracellular free virus particles and inclusion bodies filled with virus particles in membrane-bound vesicles in cytoplasm were found in the human airway epithelial ultrathin sections. This observed morphology is consistent with the Coronaviridae family.\n\nTo further characterize the virus, de novo sequences of 2019-nCoV genome from clinical specimens (bronchoalveolar-lavage fluid) and human airway epithelial cell virus isolates were obtained by Illumina and nanopore sequencing. The novel coronavirus was identified from all three patients. Two nearly full-length coronavirus sequences were obtained from bronchoalveolar-lavage fluid (BetaCoV/Wuhan/IVDC-HB-04/2020, BetaCoV/Wuhan/IVDC-HB-05/2020|EPI\\_ISL\\_402121), and one full-length sequence was obtained from a virus isolated from a patient (BetaCoV/Wuhan/IVDC-HB-01/2020|EPI\\_ISL\\_402119). Complete genome sequences of the three novel coronaviruses were submitted to GISAID (BetaCoV/Wuhan/IVDC-HB-01/2019, accession ID: EPI\\_ISL\\_402119; BetaCoV/Wuhan/IVDC-HB-04/2020, accession ID: EPI\\_ISL\\_402120; BetaCoV/Wuhan/IVDC-HB-05/2019, accession ID: EPI\\_ISL\\_402121) and have a 86.9% nucleotide sequence identity to a previously published bat SARS-like CoV (bat-SL-CoVZC45, MG772933.1) genome. The three 2019-nCoV genomes clustered together within the sarbecovirus subgenus, which shows the typical betacoronavirus organization: a 5′ untranslated region (UTR), replicase complex (orf1ab), S gene, E gene, M gene, N gene, 3′ UTR, and several unidentified nonstructural open reading frames.\n\n【30】Figure 4. Schematic of 2019-nCoV and Phylogenetic Analysis of 2019-nCoV and Other Betacoronavirus Genomes.\n\n【31】Shown are a schematic of 2019-nCoV  and full-length phylogenetic analysis of 2019-nCoV and other betacoronavirus genomes in the Orthocoronavirinae subfamily .\n\n【32】Although 2019-nCoV is similar to some betacoronaviruses detected in bats , it is distinct from SARS-CoV and MERS-CoV. The three 2019-nCoV coronaviruses from Wuhan, together with two bat-derived SARS-like strains, ZC45 and ZXC21, form a distinct clade. SARS-CoV strains from humans and genetically similar SARS-like coronaviruses from bats collected from southwestern China formed another clade within the subgenus sarbecovirus. Since the sequence identity in conserved replicase domains (ORF 1ab) is less than 90% between 2019-nCoV and other members of betacoronavirus, the 2019-nCoV — the likely causative agent of the viral pneumonia in Wuhan — is a novel betacoronavirus belonging to the sarbecovirus subgenus of Coronaviridae family.\n\n【33】Discussion\n----------\n\n【34】We report a novel CoV (2019-nCoV) that was identified in hospitalized patients in Wuhan, China, in December 2019 and January 2020. Evidence for the presence of this virus includes identification in bronchoalveolar-lavage fluid in three patients by whole-genome sequencing, direct PCR, and culture. The illness likely to have been caused by this CoV was named “novel coronavirus-infected pneumonia” (NCIP). Complete genomes were submitted to GISAID. Phylogenetic analysis revealed that 2019-nCoV falls into the genus betacoronavirus, which includes coronaviruses (SARS-CoV, bat SARS-like CoV, and others) discovered in humans, bats, and other wild animals.  We report isolation of the virus and the initial description of its specific cytopathic effects and morphology.\n\n【35】Molecular techniques have been used successfully to identify infectious agents for many years. Unbiased, high-throughput sequencing is a powerful tool for the discovery of pathogens.  Next-generation sequencing and bioinformatics are changing the way we can respond to infectious disease outbreaks, improving our understanding of disease occurrence and transmission, accelerating the identification of pathogens, and promoting data sharing. We describe in this report the use of molecular techniques and unbiased DNA sequencing to discover a novel betacoronavirus that is likely to have been the cause of severe pneumonia in three patients in Wuhan, China.\n\n【36】Although establishing human airway epithelial cell cultures is labor intensive, they appear to be a valuable research tool for analysis of human respiratory pathogens.  Our study showed that initial propagation of human respiratory secretions onto human airway epithelial cell cultures, followed by transmission electron microscopy and whole genome sequencing of culture supernatant, was successfully used for visualization and detection of new human coronavirus that can possibly elude identification by traditional approaches.\n\n【37】Further development of accurate and rapid methods to identify unknown respiratory pathogens is still needed. On the basis of analysis of three complete genomes obtained in this study, we designed several specific and sensitive assays targeting ORF1ab, N, and E regions of the 2019-nCoV genome to detect viral RNA in clinical specimens. The primer sets and standard operating procedures have been shared with the World Health Organization and are intended for surveillance and detection of 2019-nCoV infection globally and in China. More recent data show 2019-nCoV detection in 830 persons in China. \n\n【38】Although our study does not fulfill Koch’s postulates, our analyses provide evidence implicating 2019-nCoV in the Wuhan outbreak. Additional evidence to confirm the etiologic significance of 2019-nCoV in the Wuhan outbreak include identification of a 2019-nCoV antigen in the lung tissue of patients by immunohistochemical analysis, detection of IgM and IgG antiviral antibodies in the serum samples from a patient at two time points to demonstrate seroconversion, and animal (monkey) experiments to provide evidence of pathogenicity. Of critical importance are epidemiologic investigations to characterize transmission modes, reproduction interval, and clinical spectrum resulting from infection to inform and refine strategies that can prevent, control, and stop the spread of 2019-nCoV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "35ff34a9-78fa-4ed8-9e49-5523703df061", "title": "Tissue-Culture Assay of Antibodies to Heat-Labile ", "text": "【0】Tissue-Culture Assay of Antibodies to Heat-Labile \nAbstract\n--------\n\n【1】Serums of volunteers challenged orally with either toxigenic or invasive strains of _Escherichia coli_ were analyzed for antibody content by means of an adrenal tumor-cell tissue-culture system and by the standard rabbit ileal-loop method. The tissue-culture assay system depends on the ability of specific antibody to neutralize the morphogenic effects of heat-labile _Esch. coli_ enterotoxins. Six of seven volunteers receiving the toxigenic strains demonstrated fourfold or greater increases in antibody titers to heat-labile _Esch. coli_ enterotoxins by the tissue-culture method, whereas eight of nine receiving the invasive _Esch. coli_ strains did not demonstrate any such increases. These results compared favorably with those obtained by the ileal-loop method.\n\n【2】The adrenal cell morphologic assay method can be used in seroepidemiologic and clinical studies designed to assess the importance of heat-labile enterotoxigenic _Esch. coli_ diarrheal disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3fc72db4-b358-4868-bfa5-fd7c90e32bd1", "title": "A Sharp Right Turn", "text": "【0】A Sharp Right Turn\nA 60-year-old man presented to the emergency department for evaluation of rectal bleeding, syncope, and pain in the right leg. Five days earlier, diffuse abdominal pain that worsened with movement had developed in association with nausea, anorexia, and malaise. Shortly before admission, the patient had a single episode of gross hematochezia and hematuria followed by syncope, as well as intense pain in the right leg.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9a1ebaaf-2332-4382-aeaf-f9aa6ada8e4d", "title": "Case 34-2006 — A 72-Year-Old Woman with Nausea Followed by Hypotension and Respiratory Failure", "text": "【0】Case 34-2006 — A 72-Year-Old Woman with Nausea Followed by Hypotension and Respiratory Failure\nA 72-year-old woman awoke from sleep with nausea that worsened during a 24-hour period. Evaluation disclosed pulmonary infiltrates, an elevated granulocyte count, hypoxemia, and hypotension; antibiotics were administered. Blood drawn later showed elevated levels of cardiac enzymes, and an electrocardiogram showed ST-segment elevation. Hypoxemia and hypotension worsened. A diagnostic procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "fba8c4e1-e7af-4033-8cf9-2d73b7888802", "title": "Case 4-2006 — A 79-Year-Old Woman with Myalgias, Fatigue, and Shortness of Breath", "text": "【0】Case 4-2006 — A 79-Year-Old Woman with Myalgias, Fatigue, and Shortness of Breath\nA 79-year-old woman had abrupt onset of pain in the shoulders, neck, and back two days after beginning treatment with ezetimibe. These symptoms persisted, and weakness developed despite discontinuing ezetimibe. The erythrocyte sedimentation rate was 90 mm per hour. Prednisone (20 mg daily) led to improvement in the muscle pain and weakness, but dyspnea and hoarseness developed, and she was admitted to the hospital. A diagnostic procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "492254a2-4a49-4fe4-9051-ade2e154beec", "title": "Cancer Risk Associated with Lorcaserin — The FDA’s Review of the CAMELLIA-TIMI 61 Trial", "text": "【0】Cancer Risk Associated with Lorcaserin — The FDA’s Review of the CAMELLIA-TIMI 61 Trial\nIn early 2020, the FDA requested that the manufacturer of lorcaserin voluntarily withdraw the drug from the market because of a signal of increased cancer risk. The FDA concluded that the drug’s benefits don’t outweigh its risks for any identifiable patient population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3f468b6b-3888-420a-9f83-601c10269fea", "title": "Primaquine Failure and Cytochrome P-450 2D6 in ", "text": "【0】Primaquine Failure and Cytochrome P-450 2D6 in \nTo the Editor:\n--------------\n\n【1】Primaquine is the only medication approved by the Food and Drug Administration to eradicate the hypnozoites of _Plasmodium vivax,_ but relapses of _P. vivax_ malaria due to drug failure occur.  Human cytochrome P-450 isoenzyme 2D6 (CYP2D6) may be a key enzyme involved in metabolizing primaquine into redox-active metabolites against hypnozoites in the liver. \n\n【2】As part of a phase 1 clinical trial of a vaccine against _P. vivax_ , 33 participants were exposed to _P. vivax_ sporozoites from the bites of infected mosquitoes. Parasitemia developed in all participants by day 13 after the challenge, and parasitemia rapidly cleared on initiation of the directly observed administration of a combination of chloroquine (at a dose of 1500 mg base by mouth over a period of 48 hours) and primaquine (at a dose of 30 mg by mouth daily for 14 days). Two participants (6%) had multiple relapses of malaria . After each relapse, parasitemia was rapidly cleared in these participants with chloroquine (at a standard dose of 1500 mg base by mouth over a period of 48 hours) and a weight-based dose of primaquine (at a total dose of 6 mg per kilogram of body weight). To our knowledge, true resistance to primaquine in _P. vivax_ hypnozoites has not been described; this suggests a role for host factors in drug failure.  We sought to identify an association between CYP2D6 activity and primaquine drug failure.\n\n【3】CYP2D6 phenotypes were ascertained in 25 available participants. The institutional review boards of the Walter Reed Army Institute of Research, the Naval Medical Research Center, and the Walter Reed Army Medical Center, as well as the Western Institutional Review Board approved the study, and all participants provided written informed consent. CYP2D6 phenotyping was performed; 21 participants had an extensive-metabolizer phenotype (at least one allele coding for an enzyme with normal activity) (84%), 3 participants had an intermediate-metabolizer phenotype (heterozygous for one null and one reduced-function allele) (12%), and 1 participant had a poor-metabolizer phenotype (two nonfunctional alleles) (4%)  .\n\n【4】Table 1. Episodes of Malaria Relapse up to 20 Months after Challenge, According to CYP2D6 Activity Phenotype.\n\n【5】There were no relapses in the extensive-metabolizer group, two relapses in one participant in the intermediate-metabolizer group, and three relapses in the one participant in the poor-metabolizer group . Although the sample size was small, there were significant associations between low-activity CYP2D6 phenotypes and the initial relapse and number of relapses up to 20 months after the challenge . Plasma concentrations of the parent primaquine were measured in most of these participants for 24 hours after oral administration of 30 mg of primaquine. Participants with relapse had significantly decreased clearance of primaquine and the highest residual plasma concentrations after 24 hours . This effect, which was consistent with decreased metabolism by CYP2D6, was most apparent in Participant 1 (who had the poor-metabolizer phenotype) and Participant 2 (who had the intermediate-metabolizer phenotype) .\n\n【6】These data support our hypothesis that a primaquine metabolite responsible for hypnozoite killing is generated by a CYP2D6-dependent pathway. We propose that these persons have polymorphisms in CYP2D6 resulting in diminished metabolism of primaquine such that sufficient levels of the active metabolite were not achieved. These data suggest that host genetics may contribute to primaquine failure in persons who have relapse of _P. vivax_ malaria. Larger study populations are necessary to further elucidate the relationships among CYP2D6 activity, geographic regional dosing requirements, and clinical failure of primaquine to eradicate _P. vivax_ hypnozoites.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "dea22665-648d-4886-bc1a-5b043036e48e", "title": "Histiocytosis-X — Demonstration of Abnormal Immunity, T-Cell Histamine H2-Receptor Deficiency, and Successful Treatment with Thymic Extract", "text": "【0】Histiocytosis-X — Demonstration of Abnormal Immunity, T-Cell Histamine H2-Receptor Deficiency, and Successful Treatment with Thymic Extract\nAbstract\n--------\n\n【1】Twelve of 17 patients with histiocytosis-X were immunologically abnormal, as shown by the presence of circulating lymphocytes spontaneously cytotoxic to cultured human fibroblasts or of antibody to autologous erythrocytes. The patients also had a notable lack of histamine H2 surface receptors on their T lymphocytes, suggesting a suppressor-cell deficiency. The lymphocyte abnormalities were reversed in vitro after incubation in a crude extract of calf thymus gland, and therefore all 17 patients were treated with daily intramuscular injections of this extract. With this therapy, 10 patients entered complete remission — a response at least as good as that observed in historical controls treated with chemotherapy. A positive clinical response was associated with an increase in the number of T-cell histamine H2 receptors to normal levels and with correction of the other immunologic abnormalities. The results of this preliminary study justify a larger prospective clinical trial of thymic extract and further investigation of the immunoregulatory mechanisms in histiocytosis-X.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0382bc13-2838-44c2-aad5-da5a828d1ef6", "title": "Chemotaxis of Polymorphonuclear Leukocytes from Patients with Diabetes Mellitus", "text": "【0】Chemotaxis of Polymorphonuclear Leukocytes from Patients with Diabetes Mellitus\nAbstract\n--------\n\n【1】With a new in vitro method of measuring the chemotaxis of polymorphonuclear leukocytes from peripheral blood, a chemotactic index was calculated. The mean chemotactic index in 31 patients with diabetes mellitus was significantly less (p less than 0.0005) than that in 31 matched controls. The defect in chemotaxis could not be correlated with plasma insulin, plasma glucose, serum carbon dioxide and blood urea nitrogen values, or with any therapeutic agents. Incubation of the leukocytes from 11 normals with glucose in concentrations of 300 to 900 mg per 100 ml did not alter indexes. The defect in chemotaxis of the diabetic leukocyte was corrected by incubation of the cells with insulin in concentrations of 100 μU to 10 mU per milliliter. Insulin with phenol was less effective than insulin without preservative. Insulin was ineffective in the absence of glucose. It is suggested that this defect in the chemotaxis of diabetic leukocytes could contribute to increased infections in these patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f3f3505f-87a6-4540-a2f2-ec7ca436cde2", "title": "Hyperfractionated Irradiation with or without Concurrent Chemotherapy for Locally Advanced Head and Neck Cancer", "text": "【0】Hyperfractionated Irradiation with or without Concurrent Chemotherapy for Locally Advanced Head and Neck Cancer\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Radiotherapy is often the primary treatment for advanced head and neck cancer, but the rates of locoregional recurrence are high and survival is poor. We investigated whether hyperfractionated irradiation plus concurrent chemotherapy (combined treatment) is superior to hyperfractionated irradiation alone.\n\n【3】Methods\n-------\n\n【4】Patients with advanced head and neck cancer who were treated only with hyperfractionated irradiation received 125 cGy twice daily, for a total of 7500 cGy. Patients in the combined-treatment group received 125 cGy twice daily, for a total of 7000 cGy, and five days of treatment with 12 mg of cisplatin per square meter of body-surface area per day and 600 mg of fluorouracil per square meter per day during weeks 1 and 6 of irradiation. Two cycles of cisplatin and fluorouracil were given to most patients after the completion of radiotherapy.\n\n【5】Results\n-------\n\n【6】Of 122 patients who underwent randomization, 116 were included in the analysis. Most patients in both treatment groups had unresectable disease. The median follow-up was 41 months (range, 19 to 86). At three years the rate of overall survival was 55 percent in the combined-therapy group and 34 percent in the hyperfractionation group (P=0.07). The relapse-free survival rate was higher in the combined-treatment group (61 percent vs. 41 percent, P=0.08). The rate of locoregional control of disease at three years was 70 percent in the combined-treatment group and 44 percent in the hyperfractionation group (P=0.01). Confluent mucositis developed in 77 percent and 75 percent of the two groups, respectively. Severe complications occurred in three patients in the hyperfractionation group and five patients in the combined-treatment group.\n\n【7】Conclusions\n-----------\n\n【8】Combined treatment for advanced head and neck cancer is more efficacious and not more toxic than hyperfractionated irradiation alone.\n\n【9】Introduction\n------------\n\n【10】Cure of locally advanced squamous-cell carcinoma of the head and neck is uncommon whether a single treatment, including high-dose external-beam irradiation, or a combination of treatments is used. The rate of relapse-free survival is approximately 25 percent, and most patients die from complications of progressive local disease. Repopulation of tumor cells during treatment, tumor hypoxia, and resistance to radiotherapy have all been implicated as causes of treatment failure after primary radiotherapy. \n\n【11】Accelerated fractionation irradiation, which shortens the total time of treatment, has been employed to increase the probability of locoregional control by reducing the risk of tumor repopulation. With hyperfractionated irradiation, multiple small fractions of radiation are given each day to increase the total dose but not the risk of long-term toxicity.  Retrospective studies of patients treated with accelerated fractionation and hyperfractionation have demonstrated an improvement in disease control of about 20 percentage points, as compared with historical controls treated with conventional daily radiation, without an increase in long-term toxicity.  A prospective, randomized trial by the European Organization for the Research and Treatment of Cancer (EORTC) showed an increase of 20 percentage points in the rate of locoregional control of oropharyngeal carcinoma at five years and an improvement of 14 percentage points in survival after treatment with 8050 cGy of radiation in hyperfractionated doses as compared with the rates achieved with once-daily radiotherapy (total dose, 7000 cGy), without any increase in acute or chronic toxic effects. \n\n【12】Combination radiotherapy and chemotherapy is another promising approach to locally advanced head and neck cancer. Most studies have used sequential chemotherapy followed by radiotherapy. However, as compared with the use of radiotherapy alone, this strategy has generally failed to improve overall survival or disease-free survival. \n\n【13】We initiated a phase 3 trial in 1990 to test the hypothesis that accelerated hyperfractionation irradiation plus concurrent chemotherapy followed by adjuvant chemotherapy (combined therapy) leads to better locoregional control, relapse-free survival, and overall survival than accelerated hyperfractionation alone. We have demonstrated the feasibility of this program in a phase 1–2 study.  Accelerated hyperfractionation alone was selected as the control treatment because it represented the most intensive approach to primary radiotherapy, even though it had not been proved at the time to be superior to once-daily treatment.\n\n【14】Methods\n-------\n\n【15】Patient Population and Eligibility Criteria\n-------------------------------------------\n\n【16】In this multicenter study, all patients were initially evaluated by a multidisciplinary team consisting of otolaryngologists, radiation oncologists, and medical oncologists. The tumors were classified according to the criteria of the American Joint Committee on Cancer Staging.  The stage of the tumor was determined on the basis of each patient's history and physical examination (including examination with the patient under anesthesia), chest X-ray films, and computed tomographic or magnetic resonance imaging studies (or both) of the head and neck. Computed tomography or magnetic resonance imaging was usually repeated four to six weeks after treatment was completed to help assess the response to treatment.\n\n【17】To be eligible for the study, patients had to have previously untreated, histologically proven squamous-cell carcinoma; a tumor that was more than 4 cm in diameter (stage T3), or a tumor that was more than 4 cm in diameter and invaded deep tissues or bone or extended into an adjacent anatomical area (stage T4), with or without massive cervical lymphadenopathy (metastasis to a lymph node measuring more than 6 cm; stage N3); and no evidence of distant metastases. Patients with tumors of the base of the tongue that were between 2 and 4 cm in diameter without palpable cervical lymph nodes (stage T2N0) were also eligible. Primary tumors were classified as resectable or unresectable. Unresectable lesions were defined as those in which a resection plane could not be created without a high probability of gross residual disease.  Resectable primary tumors were defined as those with a low probability of residual disease after surgery. Most patients with resectable primary tumors were enrolled in the protocol for the purpose of preserving the function of the affected organ. Patients had to be at least 18 years of age and no older than 75 years with a Karnofsky performance score of at least 60, a serum creatinine concentration of 2.0 mg per deciliter (177 μmol per liter) or less, a white-cell count of at least 3000 per cubic millimeter, and a platelet count of at least 100,000 per cubic millimeter.\n\n【18】Patients who had had invasive cancer within the preceding five years, synchronous primary lesions, or squamous-cell carcinoma of the skin of the head and neck were excluded from the study, as were those who were pregnant. The protocol was approved by the protocol-review committee of the Duke Cancer Center and the institutional review board of Duke University Medical Center. Written informed consent was obtained from all patients.\n\n【19】Accelerated Hyperfractionated Irradiation\n-----------------------------------------\n\n【20】The primary tumor and draining lymphatic system were treated isocentrically with 4-MV or 6-MV photons and parallel opposed lateral portals with a source-to-isocenter distance of 80 to 100 cm. Supraclavicular nodes and nodes in the lower part of the neck were treated with the use of a single anterior field, with midline blocking to prevent spinal cord overlap. The spinal cord was blocked in the inferior aspect of both lateral fields for laryngeal primary tumors. The inferior border of the lateral fields and the superior border of the supraclavicular fields coincided on the skin.\n\n【21】Lateral-field doses were prescribed at midplane, whereas a depth of 3 cm was used for the supraclavicular field. Initial fields received a total of 4000 cGy, given in dosages of 125 cGy twice a day with a six-hour interval between doses. The use of the supraclavicular field was discontinued after 4000 cGy had been administered if there were no palpable lymph nodes in the lower part of the neck. In order to shield the spinal cord, the size of the lateral fields was reduced after 4000 cGy had been administered. Electron-beam irradiation was used to boost the dose to the posterior cervical lymph-node chains. The size of the field was again reduced after 5500 to 6000 cGy had been administered. The total dose delivered to the primary tumor was 7500 cGy in a six-week period. Treatment was delivered continuously without any planned interruptions.\n\n【22】Accelerated Hyperfractionated Irradiation and Concurrent Chemotherapy\n---------------------------------------------------------------------\n\n【23】The dosage and techniques of irradiation were the same as in the group that received accelerated hyperfractionated irradiation alone. A seven-day interruption of radiotherapy was planned after 4000 cGy had been administered, since our phase 1–2 trial had shown that this was necessary to manage treatment-induced mucositis. The total dose intended for the primary tumor was 7000 cGy in a seven-week period.\n\n【24】Chemotherapy\n------------\n\n【25】Chemotherapy was administered during weeks 1 and 6 of the course of treatment. Patients received prophylactic hydration and antiemetics. Chemotherapy consisted of fluorouracil and cisplatin (Platinol, Bristol-Myers Squibb, Princeton, N.J.). Fluorouracil was administered as a continuous infusion at a dose of 600 mg per square meter of body-surface area per day for five days. Cisplatin was given as a daily bolus of 12 mg per square meter per day for five days, for a total dose of 60 mg per square meter. Two additional cycles of cisplatin and fluorouracil were planned after the completion of all local therapy, with the cisplatin again divided into five daily boluses, and the dose increased to 80 mg per square meter in cycle 3 and to 100 mg per square meter in cycle 4. There were no provisions for reductions in the doses of chemotherapy.\n\n【26】Management of the Neck\n----------------------\n\n【27】Patients with no regional lymph-node metastases of the neck (stage N0) or metastasis to a single ipsilateral lymph node measuring 3 cm or less in diameter (stage N1) were treated only with radiotherapy, or radiotherapy and chemotherapy. Patients who presented with neck disease of stage N2 (metastasis to a single ipsilateral lymph node that was more than 3 cm in diameter but not more than 6 cm) or higher were reevaluated four to six weeks after irradiation. Elective neck dissection was planned in patients who had a complete response at the primary site even if they also had a complete response in the neck. \n\n【28】Experimental Design\n-------------------\n\n【29】The primary outcome measures were the rate of complete response at the primary site and the rate of locoregional control. We estimated that 126 patients (63 in each group) were needed in order to achieve an approximate power of 0.80 to detect a difference in complete-response rates of 0.20 (0.60 in the hyperfractionation group and 0.80 in the combined-treatment group at an alpha level of 0.05 with a one-sided test). The achievement of the response rates stipulated by the experimental design would in turn lead to a power of 0.80 to detect a difference in the rate of locoregional control of 0.25.\n\n【30】Randomization Procedures\n------------------------\n\n【31】The randomization strategy was designed by the biostatistics unit of the Duke Cancer Center and executed independently by the cancer-center protocol office. The principal investigator telephoned the protocol office to receive a patient's treatment assignment, with subsequent written confirmation provided by the protocol registrar.\n\n【32】The randomization scheme was a permuted block design with an equal probability of assignment to either treatment and stratification according to the resectability of the tumor (resectable or unresectable) and the hemoglobin concentration at randomization (<12 g per deciliter vs. ≥12 g per deciliter). The hemoglobin concentration was selected because of the adverse prognostic effect of anemia in patients undergoing radiotherapy.  The block size was six.\n\n【33】Statistical Analysis\n--------------------\n\n【34】Exact tests for contingency tables, Kaplan–Meier estimates of survival,  and stratified log-rank tests were used to test for differences in response rates, estimated times to events, and differences in the distributions of these events. All P values are two-sided. Competing risk factors are a fundamental problem in the design of studies that use local control as an end point, since relapses at distant sites or deaths may occur before the end point is reached.  This problem ordinarily requires one to assume that the risk factors are independent, an untestable and potentially implausible assumption. Therefore, in addition to analyzing locoregional control, we analyzed both relapse-free survival — that is, survival free of relapse at a local or a distant site — and overall survival, which included death from any cause.\n\n【35】Results\n-------\n\n【36】Characteristics of the Patients\n-------------------------------\n\n【37】From June 1990 to December 1995, 142 eligible patients were identified. Twenty patients declined to enroll in the study. Of the 122 who underwent randomization, 6 were excluded from the analysis for the following reasons: synchronous primary tumors (2 patients), distant metastases at diagnosis (2), loss to follow-up before the start of treatment (1), and refusal of all treatment (1). Thus, only 116 patients were included in the analysis.\n\n【38】Table 1. Characteristics of the Patients at Randomization.\n\n【39】Table 1 outlines the characteristics of the patients. Fifty-five percent of the 60 patients in the hyperfractionation group and 52 percent of the 56 patients in the combined-treatment group had unresectable disease. Eighty-seven percent of the patients in the hyperfractionation group and 90 percent of those in the combined-treatment group had stage T3 or T4 primary tumors. The mean diameter of the primary tumor exceeded 5 cm in both groups. Nodal metastases were present in 73 percent of the patients in the hyperfractionation group and 70 percent of those in the combined-treatment group. Sixty-three percent of the patients in the hyperfractionation group had advanced nodal disease (stage N2 or N3), as compared with 44 percent of those in the combined-treatment group (P=0.31). Most patients (89 percent) were enrolled at Duke University Medical Center or the Durham Veterans Affairs Hospital.\n\n【40】Treatment\n---------\n\n【41】Radiotherapy in the hyperfractionation regimen was designed to be more intensive than in the combined-treatment regimen, in terms of both total dose and the time of delivery. The mean (±SD) dose delivered to the primary tumor was 7400±273 cGy in the hyperfractionation regimen and 7050±160 cGy in the combined-treatment regimen (P<0.001 by the two-tailed t-test). The average numbers of days of administration were 42±6 and 47±5, respectively (P<0.001 by the two-tailed t-test). There were no unplanned treatment breaks.\n\n【42】Fifty-five of the patients in the combined-treatment group (98 percent) received two cycles of chemotherapy concurrently with irradiation; a cardiac arrhythmia developed in one patient as a result of fluorouracil therapy, and he received only one cycle of chemotherapy. Thirty-two patients in this group (57 percent) received the third and fourth cycles of post-radiation chemotherapy. Seventeen patients refused to undergo post-radiation chemotherapy, and it was not offered to the seven patients who did not have a complete response at the primary site after combined therapy.\n\n【43】Toxicity of Treatment\n---------------------\n\n【44】Table 2. Short-Term and Long-Term Adverse Effects of Treatment.\n\n【45】Table 2 outlines the adverse effects of treatment. Sepsis, including one case that was fatal, was more frequent in the combined-treatment group, as is expected with the use of myelosuppressive agents. The incidence of confluent mucositis was virtually the same in the two treatment groups, although the mean length of time before mucositis resolved was longer in the combined-treatment group (six vs. four weeks). There were no significant differences regarding weight loss, although a higher proportion of patients in the combined-treatment group required temporary nasogastric or gastronomy feeding tubes. Severe long-term effects requiring surgery or hyperbaric oxygen — namely, osteonecrosis and soft-tissue necrosis — were rare in both groups.\n\n【46】Outcome\n-------\n\n【47】A complete response was defined as the disappearance of all clinical and radiographic evidence of disease at the primary site six weeks after radiotherapy. Of the 60 patients in the hyperfractionation group, 44 (73 percent) had a complete response, whereas 49 of the 56 patients in the combined-treatment group (88 percent) had a complete response (P=0.52). The response was pathologically confirmed in patients who subsequently underwent elective neck dissection, since the presence of persistent disease would have necessitated more comprehensive surgery than a neck dissection.\n\n【48】Of the 37 patients in the hyperfractionation group who had stage N2 or N3 cervical lymphadenopathy, 25 had a complete response. Neck dissection was performed in 16 of these patients, and residual cancer was identified in specimens from 6. Among the 25 patients in the combined-treatment group with adenopathy of stage N2 or N3, 19 had a complete response. Neck dissection was performed in 14 patients, and residual cancer was present in 3.\n\n【49】Figure 1. Kaplan–Meier Estimates of the Duration of Locoregional Control of Disease.\n\n【50】Combined treatment was better than hyperfractionation alone (P=0.01). The I bars indicate the 95 percent confidence intervals for the hyperfractionation group (0.32 to 0.58) and the combined-treatment group (0.56 to 0.82). Data on patients with distant metastases as a first event were not censored, and such patients were still considered to be at risk for local failure.Figure 2.  Figure 2. Kaplan–Meier Estimates of Relapse-free Survival.\n\n【51】Combined treatment was better than hyperfractionation alone (P=0.08). The I bars indicate the 95 percent confidence intervals for the hyperfractionation group (0.27 to 0.53) and the combined-treatment group (0.48 to 0.74). Data on patients who died of other causes were censored at the time of death.Figure 3.  Figure 3. Kaplan–Meier Estimates of Overall Survival.\n\n【52】Combined treatment was better than hyperfractionation alone (P=0.07). The I bars indicate the 95 percent confidence intervals for the hyperfractionation group (0.22 to 0.48) and the combined-treatment group (0.42 to 0.68). Death from any cause was included in the analysis.\n\n【53】The median follow-up of surviving patients was 41 months (range, 19 to 86); 82 percent of such patients were followed for at least 2 years. All relapses occurred within 18 months after enrollment. Patients in the combined-treatment group had better rates of locoregional control of disease at three years (70 percent vs. 44 percent, P=0.01) , relapse-free survival (61 percent vs. 41 percent, P=0.08) , and overall survival (55 percent vs. 34 percent, P=0.07)  than the patients in the hyperfractionation group. The respective P values for these three end points were 0.01, 0.11, and 0.12 after adjustment for differences in nodal stage at base line between the two treatment groups.\n\n【54】Patterns of Relapse\n-------------------\n\n【55】The tumor recurred in 33 patients who received only hyperfractionation. The primary site was the most common location of a first recurrence (in 21 of 33 patients, or 64 percent). Lymph nodes were involved in 15 patients (45 percent), and distant metastases were a component in 6 patients (18 percent). The percentage of recurrences totals more than 100 because some patients had recurrences at multiple sites.\n\n【56】The tumor recurred in 22 patients after combined therapy, with the most common location being the primary site (in 16 of 22 patients, or 73 percent). In the remaining 6 patients (27 percent) the recurrences consisted of distant metastases; 5 of these patients were among the 17 who refused to undergo post-radiation chemotherapy. There were no recurrences in the neck in the patients who underwent elective neck dissection in either treatment group.\n\n【57】Discussion\n----------\n\n【58】In most patients with advanced head and neck cancer, conventional radiotherapy does not result in long-term locoregional control of the tumor, and it is this failure that ultimately proves fatal. Investigators have sought to improve this situation through the use of several strategies.\n\n【59】Accelerated fractionation and hyperfractionation both use multiple daily dosages of irradiation. The purpose of accelerated fractionation is to reduce tumor proliferation during therapy by shortening the overall treatment time. With hyperfractionated irradiation, the treatment time is kept constant relative to once-daily treatment. Hyperfractionation is an attempt to improve the therapeutic ratio by increasing the total dose of radiation, but not the risk of long-term toxicity. The size of each fraction is less than that delivered with standard once-daily treatment. Phase 2 trials have reported that these approaches have considerable benefits. Phase 3 trials of the EORTC have confirmed these findings. \n\n【60】Two randomized trials of sequentially administered chemotherapy and radiation, with or without surgery depending on the resectability of the tumor, for advanced laryngeal and hypopharyngeal cancer have demonstrated that this approach can result in a high degree of organ preservation; it can obviate the need for laryngectomy and result in survival rates equivalent to those reported after surgery and postoperative radiation.  Despite its widespread use, however, this strategy has not been proved superior to radiation alone. \n\n【61】Regimens of concurrently administered radiation and chemotherapy have typically employed once-daily irradiation with doses ranging from 65 to 70 Gy. This approach has an apparent advantage as compared with radiation alone.  Moreover, a randomized study demonstrated the superiority of concurrent combined therapy over sequential combined therapy.  Recent meta-analyses have also concluded that concurrent administration is superior to sequential treatment. \n\n【62】We combined accelerated hyperfractionated irradiation with concurrent chemotherapy. When our study began in 1990, phase 2 data attested to the superiority of hyperfractionation over conventional radiotherapy.  Other results suggested the superiority of concurrent chemotherapy and radiotherapy for esophageal, anal, and rectal cancer. We therefore decided to compare maximally intensive radiotherapy alone (total dose, 7500 cGy) with a slightly less intense radiotherapy program (total dose, 7000 cGy) plus concurrent chemotherapy.\n\n【63】We found that overall survival, relapse-free survival, and locoregional control were all improved by the addition of concurrent chemotherapy to the hyperfractionation program. Moreover, these results were achieved without a corresponding increase in severe toxicity; long-term complications were the same in both groups, with a slight increase in acute adverse effects in the combined-treatment group.\n\n【64】The outcome of this trial must be interpreted with caution. Despite randomization, there were some imbalances in the two treatment groups. There were more patients with advanced nodal disease (N2 or N3) in the hyperfractionation group than in the combined-treatment group (63 percent vs. 45 percent, P=0.31), perhaps making the prognosis worse in the former group. There were other, smaller imbalances regarding the primary site, with more patients with the hypopharynx as a primary site (which is associated with a less favorable result) in the combined-treatment group and more patients with the oropharynx (tonsil and base of tongue) as a primary site (which is associated with a more favorable result) in the hyperfractionation group. The trial was, however, well balanced with respect to resectability, the most important prognostic variable, as well as the stage of the tumor, hemoglobin concentration, and Karnofsky performance status.\n\n【65】Our protocol specified that patients with stage N2 or N3 tumors could undergo elective neck dissection, but the procedure was not always performed. The absence of any relapses in the neck among patients who underwent dissection implies that the overall failure rate in the neck would have been lower had this aspect of the protocol been rigorously followed. A greater proportion of the patients in the combined-treatment group underwent neck dissection than in the hyperfractionation group, another possible contribution to the improved outcome for this group.\n\n【66】This trial was not designed to test the value of adjuvant chemotherapy. Nonetheless, the fact that five of the six recurrences at distant sites occurred among the 17 patients in the combined-treatment group who did not receive post-irradiation chemotherapy suggests that relapse-free survival in this group might have been higher had patients received all planned chemotherapy. The Intergroup Study 0034 demonstrated a reduction in distant metastases with adjuvant chemotherapy with cisplatin and fluorouracil in patients with resectable head and neck cancer, albeit with no improvement in overall survival. \n\n【67】Despite the improvements in overall survival, relapse-free survival, and locoregional control of disease with the use of hyperfractionated irradiation and concurrent chemotherapy, approximately half the patients who received this treatment ultimately died of their disease, mostly from sequelae of uncontrolled locoregional disease. Clearly, there is a need for improvement in the treatment of locoregional disease in advanced head and neck cancer.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8e62777f-f14a-4572-8e76-767f9608dd19", "title": "Invasive Pulmonary Aspergillosis Associated with High-Dose Inhaled Fluticasone", "text": "【0】Invasive Pulmonary Aspergillosis Associated with High-Dose Inhaled Fluticasone\nTo the Editor:\n--------------\n\n【1】A 44-year-old man with moderately severe asthma was first seen in the infectious-disease clinic in November 1999 because of a recent diagnosis of pulmonary aspergillosis. The patient had noted an increase in his asthma symptoms during the summer of 1999. A chest x-ray film obtained at that time had revealed bilateral cavitary lesions. An open-lung biopsy in September 1999 had revealed chronic necrotizing aspergillosis.\n\n【2】The patient had no known history of immunosuppression. His medications included fluticasone (delivered in two puffs of 220 μg four times a day \\[total daily dose, 1760 μg\\]) and zafirlukast (20 mg per day). The patient was initially treated with oral itraconazole for invasive pulmonary aspergillosis. Repeated imaging showed progression of disease. Hemoptysis developed. Treatment was switched to an investigational antifungal medication. During the course of therapy, hypertension developed and the patient's abdominal girth increased. The results of a cosyntropin (Cortrosyn) stimulation test were consistent with the presence of adrenal insufficiency. The dose of inhaled corticosteroids was gradually tapered. Recent computed tomographic scans of the thorax demonstrate improvement, the patient's cough has decreased, and he has had no further episodes of hemoptysis.\n\n【3】High-dose systemic corticosteroids are a risk factor for invasive aspergillosis. We believe that this is the first reported case of invasive pulmonary aspergillosis associated with an inhaled corticosteroid. A Medline search produced one reported case of laryngeal aspergillosis associated with inhaled fluticasone. \n\n【4】Fluticasone is the most potent inhaled corticosteroid available in the United States. It was made available because it has greater topical potency, a higher degree of retention in tissue, and a longer half-life than the other inhaled corticosteroids. Inhaled corticosteroids have been linked to adverse effects traditionally associated with systemic corticosteroids, including adrenal suppression and effects on linear growth and bone metabolism.  There have been numerous reports of adrenal insufficiency associated with the use of inhaled corticosteroids. A meta-analysis of studies of adrenal suppression among patients who used inhaled corticosteroids demonstrated that fluticasone was nearly twice as likely to cause adrenal suppression as beclomethasone, triamcinolone, or budesonide at equivalent dosages. The effects were more obvious at daily doses of more than 0.8 mg. \n\n【5】Our findings add to a growing body of evidence that in certain patients, inhaled corticosteroids can exert clinically significant systemic effects. Patients who are taking high doses may be at risk for secondary immunosuppression and opportunistic infection. Practitioners should be aware of these risks when they prescribe these medications, particularly the more potent formulations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e3e124ae-af6d-4fbc-b247-083b1252236d", "title": "Relation between Surgical Volume and Incidence of Postoperative Wound Infection", "text": "【0】Relation between Surgical Volume and Incidence of Postoperative Wound Infection\nAbstract\n\n【1】We used a statewide program for the surveillance and reporting of infections acquired in the hospital to examine the relation between volume of surgery and the incidence of postoperative wound infection. Over a 29-month period 25,941 surgical procedures performed at 22 hospitals were studied. For all procedures studied, the mean number of operations performed was directly related to hospital size (number of beds). A highly significant inverse relation was found between the logarithm of the frequency of operation and the infection rate for appendectomy, herniorrhaphy, cholecystectomy, colon resection, and abdominal hysterectomy. (The relation was borderline \\[P = 0.055\\] for laminectomy and not significant for cesarean section.) Although these data clearly demonstrate higher morbidity in hospitals performing relatively little surgery, there are several possible explanations, and no conclusions for health policy can yet be drawn.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a6d41312-5e88-4e99-87ae-f2988fe41d46", "title": "Tranexamic Acid in Patients Undergoing Noncardiac Surgery", "text": "【0】Tranexamic Acid in Patients Undergoing Noncardiac Surgery\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Perioperative bleeding is common in patients undergoing noncardiac surgery. Tranexamic acid is an antifibrinolytic drug that may safely decrease such bleeding.\n\n【3】Methods\n-------\n\n【4】We conducted a trial involving patients undergoing noncardiac surgery. Patients were randomly assigned to receive tranexamic acid (1-g intravenous bolus) or placebo at the start and end of surgery (reported here) and, with the use of a partial factorial design, a hypotension-avoidance or hypertension-avoidance strategy (not reported here). The primary efficacy outcome was life-threatening bleeding, major bleeding, or bleeding into a critical organ (composite bleeding outcome) at 30 days. The primary safety outcome was myocardial injury after noncardiac surgery, nonhemorrhagic stroke, peripheral arterial thrombosis, or symptomatic proximal venous thromboembolism (composite cardiovascular outcome) at 30 days. To establish the noninferiority of tranexamic acid to placebo for the composite cardiovascular outcome, the upper boundary of the one-sided 97.5% confidence interval for the hazard ratio had to be below 1.125, and the one-sided P value had to be less than 0.025.\n\n【5】Results\n-------\n\n【6】A total of 9535 patients underwent randomization. A composite bleeding outcome event occurred in 433 of 4757 patients (9.1%) in the tranexamic acid group and in 561 of 4778 patients (11.7%) in the placebo group (hazard ratio, 0.76; 95% confidence interval \\[CI\\], 0.67 to 0.87; absolute difference, −2.6 percentage points; 95% CI, −3.8 to −1.4; two-sided P<0.001 for superiority). A composite cardiovascular outcome event occurred in 649 of 4581 patients (14.2%) in the tranexamic acid group and in 639 of 4601 patients (13.9%) in the placebo group (hazard ratio, 1.02; 95% CI, 0.92 to 1.14; upper boundary of the one-sided 97.5% CI, 1.14; absolute difference, 0.3 percentage points; 95% CI, −1.1 to 1.7; one-sided P=0.04 for noninferiority).\n\n【7】Conclusions\n-----------\n\n【8】Among patients undergoing noncardiac surgery, the incidence of the composite bleeding outcome was significantly lower with tranexamic acid than with placebo. Although the between-group difference in the composite cardiovascular outcome was small, the noninferiority of tranexamic acid was not established. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nTranexamic Acid in Noncardiac Surgery  \n\n【11】Perioperative bleeding, a common complication in patients undergoing noncardiac surgery, is associated with increased morbidity and mortality.  Large surgical trials have shown that tranexamic acid reduces the incidence and severity of bleeding in patients undergoing cesarean section or cardiac surgery.  Encouraging evidence from small trials suggests that tranexamic acid may reduce the incidence and severity of bleeding in patients undergoing orthopedic surgery  ; there are limited data on its use in patients undergoing nonorthopedic noncardiac surgeries.  Tranexamic acid is an antifibrinolytic drug,  and trials have not been large enough to establish whether tranexamic acid increases the risk of thrombotic events in noncardiac surgery.\n\n【12】We undertook the Perioperative Ischemic Evaluation–3 (POISE-3) trial to answer the following questions. In patients undergoing noncardiac surgery who are at risk for bleeding and cardiovascular events, does tranexamic acid result in a lower incidence of life-threatening bleeding, major bleeding, or bleeding into a critical organ than placebo, and is it noninferior to placebo with respect to the incidence of major cardiovascular complications within 30 days?\n\n【13】Methods\n-------\n\n【14】Trial Design\n------------\n\n【15】In this international, randomized, controlled trial, we evaluated the effects of tranexamic acid as compared with placebo in patients undergoing noncardiac surgery. We used a partial factorial design in patients who were receiving at least one long-term antihypertensive medication to evaluate a hypotension-avoidance strategy as compared with a hypertension-avoidance strategy. We report the results of the trial of tranexamic acid here. Details of the trial design have been published previously.  Trial centers obtained ethics approval before commencing recruitment.\n\n【16】Trial Oversight\n---------------\n\n【17】The trial was funded by the Canadian Institutes of Health Research and others. The Population Health Research Institute was the trial coordinating center and was responsible for the randomization scheme, database, data validation, analyses, and trial coordination. No external funder had a role in the design or conduct of the trial, collection or analysis of the data, or preparation of the manuscript. Investigators and trial personnel collected the data. The writing committee finalized the statistical analysis plan, available with the protocol, before any investigator was made aware of the trial results. The first author wrote the first draft of the manuscript, and the writing committee made revisions and made the decision to submit the manuscript for publication. There were no agreements concerning confidentiality of the data between the sponsors (i.e., funders) and the authors or their institutions.\n\n【18】Patients\n--------\n\n【19】We recruited patients from June 2018 through July 2021 at 114 hospitals in 22 countries. Eligible patients were 45 years of age or older, were undergoing inpatient noncardiac surgery, and were at risk for bleeding and cardiovascular complications according to criteria previously associated with perioperative bleeding and cardiovascular complications (e.g., known atherosclerotic disease, undergoing major surgery, an age of ≥70 years, and a serum creatinine level of >175 μmol per liter \\[2.0 mg per deciliter\\]).  Patients were excluded if they were undergoing cardiac surgery or intracranial neurosurgery, if a physician planned to administer systemic tranexamic acid during surgery, or if the patient had a creatinine clearance of less than 30 ml per minute (Cockcroft–Gault equation) or was receiving long-term dialysis.\n\n【20】Procedures\n----------\n\n【21】After written informed consent was obtained from all the patients or a patient-designated decision maker, randomization was performed by means of a central computerized system with the use of block randomization, with stratification according to center. Patients were assigned in a  ratio to receive tranexamic acid (1-g intravenous bolus) or placebo at the start and end of surgery and, in a  ratio with the use of a partial factorial design, to a hypotension-avoidance strategy or a hypertension-avoidance strategy. Patients, health care providers, data collectors, and outcome adjudicators were unaware of the trial-group assignments. The follow-up process is described in Section S2.\n\n【22】Trial Outcomes\n--------------\n\n【23】The primary efficacy outcome was a composite of life-threatening bleeding, major bleeding, and bleeding into a critical organ — henceforth referred to as the composite bleeding outcome — at 30 days after randomization. The primary safety outcome was a composite of myocardial injury after noncardiac surgery (i.e., myocardial infarction or isolated ischemic troponin elevation), nonhemorrhagic stroke, peripheral arterial thrombosis, and symptomatic proximal venous thromboembolism — henceforth referred to as the composite cardiovascular outcome — at 30 days after randomization.\n\n【24】Secondary and tertiary outcomes  included the individual components of the composite bleeding outcome and the composite cardiovascular outcome, bleeding independently associated with death after noncardiac surgery,  myocardial infarction, a net risk–benefit outcome (death from cardiovascular causes, nonfatal life-threatening bleeding, major bleeding, bleeding into a critical organ, myocardial injury after noncardiac surgery, stroke, peripheral arterial thrombosis, or symptomatic proximal venous thromboembolism), major bleeding according to the criteria of the International Society on Thrombosis and Haemostasis (ISTH)  , transfusion of at least 1 unit of packed red cells, amputation, any symptomatic or asymptomatic proximal venous thromboembolism, and seizure. Outcome definitions, monitoring, and the adjudication process are described in Sections S4, S5, and S6, respectively.\n\n【25】Statistical Analysis\n--------------------\n\n【26】The primary efficacy hypothesis was that tranexamic acid would be superior to placebo with respect to the composite bleeding outcome. The primary safety hypothesis was that tranexamic acid would be noninferior to placebo with respect to the composite cardiovascular outcome. To satisfy the noninferiority hypothesis, the upper boundary of the one-sided 97.5% confidence interval for the hazard ratio of the composite cardiovascular outcome — which corresponds to the upper boundary of the two-sided 95% confidence interval — had to be below 1.125, and the one-sided P value had to be less than 0.025. The noninferiority margin corresponds to a relative increase in risk of 12.5%, which is half the relative decrease in risk of 25% that we looked for to establish the superiority of aspirin to placebo in the POISE-2 trial.  To inform our primary hypotheses, the POISE-3 trial was initially designed to randomly assign 10,000 patients to receive tranexamic acid or placebo .\n\n【27】Owing to a financial deficit resulting from slowed recruitment during the coronavirus disease 2019 (Covid-19) pandemic, the steering committee stopped recruitment on July 15, 2021, after at least 9500 patients had undergone randomization. This decision was made without knowledge of the trial results but with knowledge that the incidences of the aggregate composite bleeding and composite cardiovascular outcome events were higher than originally estimated. We estimated that a sample of 9500 patients would provide the trial with 90% power to detect a hazard ratio of 0.80 or less (two-sided alpha level of 0.05) for tranexamic acid as compared with placebo, assuming an incidence of composite bleeding outcome events of 9.0% in the placebo group. Stopping the trial at 9500 patients also provided the trial with 98% power for a noninferiority margin, expressed as a hazard ratio of 1.125 (one-sided alpha level of 0.025), under the assumption of an incidence of composite cardiovascular outcome events of 14.0% in the placebo group and an expected hazard ratio of 0.90 and with adjustment for the partial factorial design.\n\n【28】The noninferiority hypothesis was evaluated in the per-protocol population (i.e., patients who received both planned doses of tranexamic acid or placebo). For all the other analyses, data were analyzed in the intention-to-treat population (i.e., patients evaluated in the trial group to which they had been randomly assigned). Data for patients who were lost to follow-up were censored on the last day that their status was known. An independent data and safety monitoring committee reviewed interim analyses when 25%, 50%, and 75% of the 30-day data were available. Details of the interim analyses are provided in Section S8.\n\n【29】Most outcomes were analyzed with the use of Cox proportional-hazards models, with stratification according to trial group in the blood-pressure management factorial. For the Cox models, we calculated the hazard ratio, corresponding 95% confidence interval, and associated P value. A two-sided P value of less than 0.05 was considered to indicate statistical significance. For the primary efficacy and safety outcomes, we assessed whether the blood-pressure management factorial affected the results of tranexamic acid as compared with placebo using Cox proportional-hazards models that incorporated tests of interaction.\n\n【30】We analyzed the binary outcome of transfusion using a two-by-two table and analyzed length of hospital stay and days alive at home using quantile regression. Because we did not correct for multiplicity when conducting tests for secondary or tertiary outcomes, results are reported as point estimates and 95% confidence intervals. The widths of the confidence intervals have not been adjusted for multiplicity, so the intervals should not be used to infer definitive treatment effects for the secondary or tertiary outcomes.\n\n【31】We performed four prespecified subgroup analyses of the primary outcomes; subgroups were defined according to type of surgery, hemoglobin level, estimated glomerular filtration rate, and N-terminal pro–brain natriuretic peptide level . For the composite cardiovascular outcome, we performed a sensitivity analysis according to the intention-to-treat principle. Statistical analyses were performed with the use of SAS software, version 9.4 (SAS Institute).\n\n【32】Results\n-------\n\n【33】Patients, Follow-up, and Adherence\n----------------------------------\n\n【34】Table 1. Baseline Characteristics of the Patients, Type of Surgery, and Medications.\n\n【35】We randomly assigned 9535 patients to receive tranexamic acid (4757 patients) or placebo (4778 patients) . The 30-day follow-up visit was completed for 99.9% of the patients. The baseline characteristics of the patients, type of noncardiac surgeries, and preoperative medications were similar in the two groups . The mean age of the patients was 69.4 years, and 4183 were women (43.9%). Most patients underwent nonorthopedic noncardiac surgery (76.8%). Most patients were enrolled in North America (31.1%), Europe (39.8%), and the Asia–Pacific region (27.0%) . A total of 75.9% of the patients were White .\n\n【36】In both the tranexamic acid group and the placebo group, 96.3% of the patients received both doses of the trial agent . Administration of nontrial antifibrinolytic drugs was uncommon , and the postoperative use of drugs during the hospital stay that can increase the risk of bleeding was similar in the two groups . After surgery, 64.2% of the patients in the tranexamic acid group and 63.6% of those in the placebo group received prophylactic medication against venous thromboembolism.\n\n【37】Primary Outcomes\n----------------\n\n【38】Table 2. Effects of Tranexamic Acid on 30-Day Outcomes. Figure 1.  Figure 1. Kaplan–Meier Estimates of the Primary Outcomes.\n\n【39】The composite bleeding outcome  was a composite of life-threatening bleeding, major bleeding, and bleeding into a critical organ at 30 days. The composite cardiovascular outcome  was a composite of myocardial injury after noncardiac surgery, nonhemorrhagic stroke, peripheral arterial thrombosis, and symptomatic proximal venous thromboembolism at 30 days. The insets show the same data on an expanded y axis.\n\n【40】A composite bleeding outcome event occurred in 433 patients (9.1%) in the tranexamic acid group and in 561 patients (11.7%) in the placebo group (hazard ratio, 0.76; 95% confidence interval \\[CI\\], 0.67 to 0.87; absolute difference, −2.6 percentage points; 95% CI, −3.8 to −1.4; two-sided P<0.001 for superiority) . A composite cardiovascular outcome event occurred in 649 of 4581 patients (14.2%) in the tranexamic acid group and in 639 of 4601 patients (13.9%) in the placebo group (hazard ratio, 1.02; 95% CI, 0.92 to 1.14; upper boundary of the one-sided 97.5% CI, 1.14; absolute difference, 0.3 percentage points; 95% CI, −1.1 to 1.7; one-sided P=0.04 for noninferiority); the sensitivity analysis performed according to the intention-to-treat principle produced a similar result (hazard ratio, 1.03; 95% CI, 0.92 to 1.14).\n\n【41】The interaction term of log-time according to trial group in the models indicated that the assumptions of proportional hazards were not violated (P=0.75 for interaction in the analysis of the primary efficacy outcome and P=0.48 for interaction in the analysis of the primary safety outcome). The blood-pressure management factorial had no significant effect on the results of the comparison between tranexamic acid and placebo for the primary efficacy outcome (P=0.67 for interaction) and the primary safety outcome (P=0.74 for interaction).\n\n【42】Secondary and Tertiary Outcomes and Serious Adverse Events\n----------------------------------------------------------\n\n【43】Table 3. Effects of Tranexamic Acid on 30-Day Tertiary Outcomes.\n\n【44】With respect to secondary outcomes , bleeding independently associated with death after noncardiac surgery occurred in 416 patients in the tranexamic acid group (8.7%) and in 541 patients (11.3%) in the placebo group (hazard ratio, 0.76; 95% CI, 0.67 to 0.87); major bleeding occurred in 363 patients (7.6%) and in 496 patients (10.4%), respectively (hazard ratio, 0.72; 95% CI, 0.63 to 0.83); and a net risk–benefit outcome event occurred in 983 patients (20.7%) and in 1046 patients (21.9%), respectively (hazard ratio, 0.94; 95% CI, 0.86 to 1.02). With respect to tertiary outcomes , major bleeding according to ISTH criteria occurred in 315 patients (6.6%) in the tranexamic acid group and in 415 patients (8.7%) in the placebo group (hazard ratio, 0.75; 95% CI, 0.65 to 0.87), and transfusion of at least 1 unit of packed red cells occurred in 449 patients (9.4%) and in 574 patients (12.0%), respectively (odds ratio, 0.77; 95% CI, 0.68 to 0.88).\n\n【45】Hazard ratios for selected secondary and tertiary outcomes in the tranexamic acid group as compared with the placebo group are as follows: for myocardial infarction, 1.27 (95% CI, 0.89 to 1.82); for nonhemorrhagic stroke, 1.51 (95% CI, 0.80 to 2.84); for peripheral arterial thrombosis, 0.96 (95% CI, 0.53 to 1.72); for amputation, 0.67 (95% CI, 0.34 to 1.31); and for any symptomatic or asymptomatic proximal venous thromboembolism, 1.15 (95% CI, 0.69 to 1.91) . Serious adverse events occurred in 263 patients (5.5%) in the tranexamic acid group and in 242 patients (5.1%) in the placebo group (P=0.16) .\n\n【46】Prespecified Subgroup Analyses and Post Hoc Analyses\n----------------------------------------------------\n\n【47】Figure 2. Prespecified Subgroup Analyses.\n\n【48】The size of the boxes is proportional to the number of patients with events. GFR denotes glomerular filtration rate, and NT-proBNP N-terminal pro–brain natriuretic peptide.\n\n【49】The results in the tranexamic acid group as compared with the placebo group with respect to the primary efficacy and safety outcomes were consistent across the prespecified subgroups  and, in post hoc analyses, were generally consistent across all major surgical subgroups . The odds ratio for transfusion of at least 2 units of packed red cells in the tranexamic acid group as compared with the placebo group was 0.74 (95% CI, 0.64 to 0.86), and the odds ratio for transfusion of 2 to 4 units of packed red cells was 0.71 (95% CI, 0.60 to 0.84) . Results of a competing-risks analysis that was adjusted for death that prevented observation of the primary outcome events were consistent with the results of the primary efficacy and safety analyses .\n\n【50】Discussion\n----------\n\n【51】In this trial involving patients undergoing noncardiac surgery who met criteria for increased risk of bleeding and cardiovascular events, composite bleeding outcome events (life-threatening bleeding, major bleeding, and bleeding into a critical organ) proved common (11.7% in the placebo group), and the likelihood of composite bleeding outcome events was lower with tranexamic acid than with placebo (hazard ratio, 0.76; 95% CI, 0.67 to 0.87). The noninferiority of tranexamic acid to placebo with respect to the composite cardiovascular outcome was not shown (hazard ratio, 1.02; 95% CI, 0.92 to 1.14).\n\n【52】Large trials have shown that tranexamic acid, as compared with placebo, is associated with a lower risk of death due to bleeding among patients with postpartum hemorrhage (relative risk, 0.81; 95% CI, 0.65 to 1.00) and a lower risk of death among patients with trauma (relative risk, 0.91; 95% CI, 0.85 to 0.97).  Large surgical trials have shown that tranexamic acid resulted in a lower incidence of postpartum hemorrhage among patients undergoing cesarean section and a lower incidence of hemorrhage resulting in reoperation among patients undergoing cardiac surgery than placebo. \n\n【53】Small trials suggest that tranexamic acid reduces the risk of bleeding among patients undergoing orthopedic surgery or nonorthopedic noncardiac surgery.  Although a recent meta-analysis of trials in diverse settings showed no increased risk of thromboembolic events with tranexamic acid,  the noncardiac surgery trials were small, which limits their reliability.  POISE-3 was a large trial addressing this knowledge gap in noncardiac surgery.\n\n【54】A recent meta-analysis of trials showed no dose response with tranexamic acid as compared with control for thrombotic events (2620 total events; relative risk, 1.00; 95% CI, 0.93 to 1.08); however, there was a dose response for seizures (P=0.01).  Tranexamic acid doses of 2 g or less per day as compared with control were not associated with an increased risk of seizure (676 events; relative risk, 1.02; 95% CI, 0.88 to 1.19), whereas tranexamic acid doses of more than 2 g per day were associated with an increased risk of seizure (79 events; relative risk, 3.05; 95% CI, 1.01 to 9.02). In the POISE-3 trial, we evaluated a tranexamic acid dose of 2 g per day as compared with placebo, and seizures were uncommon (in 0.2% and <0.1% of the patients, respectively; hazard ratio, 3.35; 95% CI, 0.92 to 12.20).\n\n【55】A trial involving patients undergoing cesarean section showed that tranexamic acid was associated with a higher incidence of nausea and vomiting than placebo (relative risk, 1.19; 95% CI, 1.08 to 1.30). In the POISE-3 trial, we did not collect data on nausea and vomiting outcomes, but tranexamic acid was not associated with an increased risk of nausea or vomiting reported as a serious adverse event. We report significant between-group differences in serious adverse events for a few category outcomes (e.g., cardiovascular disorders); however, within these categories, the between-group differences in most individual serious adverse events were not significant, had contrasting results (e.g., hypertension and hypotension), and were not adjusted for multiplicity.\n\n【56】Bleeding was common in the placebo group in the POISE-3 trial irrespective of the definition used: the composite bleeding outcome (in 11.7% of the patients), bleeding independently associated with death after noncardiac surgery (in 11.3%), major bleeding (in 10.4%), major bleeding according to ISTH criteria (in 8.7%), and transfusion of packed red cells (in 12.0%). Tranexamic acid consistently lowered the relative risk by approximately 25% across these outcomes. Tranexamic acid yielded similar results for the composite bleeding outcome in patients undergoing orthopedic and nonorthopedic noncardiac surgery. Moreover, post hoc subgroup analyses showed a consistent result across all types of noncardiac surgery. We recently published a bleeding-risk calculator for patients undergoing noncardiac surgery that physicians can use to estimate the reduction in bleeding risk with tranexamic acid in individual patients. \n\n【57】Noninferiority was not established for the primary safety outcome. Health care providers and patients will have to weigh a clear beneficial reduction in the incidence of composite bleeding outcome events (absolute difference, −2.6 percentage points; 95% CI, −3.8 to −1.4) against the low probability of a small increase in the incidence of composite cardiovascular outcome events (absolute difference, 0.3 percentage points; 95% CI, −1.1 to 1.7).\n\n【58】Patients prefer to avoid bleeding and transfusions.  Although tranexamic acid can facilitate these goals, only a small proportion of patients undergoing noncardiac surgery receive it.  There is an annual global shortage of 30 million blood-product units,  and surgical bleeding accounts for up to 40% of all transfusions. Given that 300 million surgeries occur annually worldwide,  the results of the POISE-3 trial indicate the potential for large public health and clinical benefits if tranexamic acid becomes standard practice in noncardiac surgery. \n\n【59】Strengths of the trial include the participation of 114 hospitals across six continents, the blinding of trial-group assignments, high adherence to the trial agents, and follow-up of 99.9% of the patients. A limitation was that we stopped the trial early owing to a financial deficit resulting from slowed recruitment during the Covid-19 pandemic. We did, however, enroll more than 95% of our original planned sample size. Our decision to stop recruitment was made without knowledge of the trial results. There are limitations regarding our clinical ability to identify perioperative thrombotic complications  ; however, tranexamic acid did not appear to increase the risk of any outcome known to result from thrombosis.\n\n【60】In our trial involving patients undergoing noncardiac surgery, the incidence of a composite of life-threatening bleeding, major bleeding, and bleeding into a critical organ was significantly lower with tranexamic acid than with placebo. Although the difference in composite cardiovascular complications between the tranexamic acid group and the placebo group was small, the noninferiority of tranexamic acid was not established.\n\n【61】Table 1. Baseline Characteristics of the Patients, Type of Surgery, and Medications. \n\n| Characteristics | Tranexamic Acid(N=4757) | Placebo(N=4778) |\n| --- | --- | --- |\n| Age — yr | 69.5±9.5 | 69.3±9.4 |\n| Male sex — no./total no. (%) | 2669/4755 (56.1) | 2681/4778 (56.1) |\n| Eligibility criteria met — no. (%) | 4742 (99.7) | 4766 (99.7) |\n| NT-proBNP ≥200 ng/liter | 574 (12.1) | 552 (11.6) |\n| History of coronary artery disease | 1410 (29.6) | 1466 (30.7) |\n| History of peripheral artery disease | 714 (15.0) | 722 (15.1) |\n| History of stroke | 400 (8.4) | 388 (8.1) |\n| Undergoing major vascular surgery | 541 (11.4) | 544 (11.4) |\n| Risk criteria |  |  |\n| Met ≥3 of 9 criteria | 3988 (83.8) | 4003 (83.8) |\n| Undergoing major surgery  | 3741 (78.6) | 3798 (79.5) |\n| Undergoing urgent or emergency surgery | 555 (11.7) | 540 (11.3) |\n| Age ≥70 yr | 2611 (54.9) | 2588 (54.2) |\n| Current diabetes for which medication is taken | 1749 (36.8) | 1812 (37.9) |\n| Preoperative serum creatinine level >175 μmol/liter | 57 (1.2) | 73 (1.5) |\n| History of congestive heart failure | 674 (14.2) | 671 (14.0) |\n| History of transient ischemic attack | 282 (5.9) | 247 (5.2) |\n| History of hypertension | 4293 (90.2) | 4321 (90.4) |\n| History of smoking within 2 yr before surgery | 1131 (23.8) | 1128 (23.6) |\n| Other medical history — no. (%) |  |  |\n| Atrial fibrillation | 478 (10.0) | 445 (9.3) |\n| Active cancer | 1311 (27.6) | 1360 (28.5) |\n| Surgery — no./total no. (%) |  |  |\n| Any procedure | 4729/4757 (99.4) | 4740/4778 (99.2) |\n| General  | 1769/4729 (37.4) | 1773/4740 (37.4) |\n| Orthopedic | 1083/4729 (22.9) | 1063/4740 (22.4) |\n| Vascular | 699/4729 (14.8) | 700/4740 (14.8) |\n| Urologic | 598/4729 (12.6) | 624/4740 (13.2) |\n| Spinal | 237/4729 (5.0) | 206/4740 (4.3) |\n| Gynecologic | 162/4729 (3.4) | 171/4740 (3.6) |\n| Thoracic | 127/4729 (2.7) | 146/4740 (3.1) |\n| Low-risk | 39/4729 (0.8) | 34/4740 (0.7) |\n| Plastic | 14/4729 (0.3) | 23/4740 (0.5) |\n| Data missing on type of procedure performed | 1/4729 (<0.1) | 0/4740 |\n| No procedure performed | 27/4757 (0.6) | 35/4778 (0.7) |\n| Data missing on whether patient underwent surgery | 1/4757 (<0.1) | 3/4778 (0.1) |\n| Medication taken within 24 hr before surgery — no. (%) |  |  |\n| Therapeutic-dose thrombin or factor Xa inhibitor | 22 (0.5) | 28 (0.6) |\n| Therapeutic-dose vitamin K antagonist | 6 (0.1) | 8 (0.2) |\n| Therapeutic-dose intravenous or subcutaneous antithrombotic agent | 58 (1.2) | 44 (0.9) |\n| Prophylactic-dose anticoagulant | 753 (15.8) | 757 (15.8) |\n| Aspirin | 638 (13.4) | 634 (13.3) |\n| P2Y12 inhibitor | 88 (1.8) | 84 (1.8) |\n| Nonsteroidal antiinflammatory drug | 266 (5.6) | 267 (5.6) |\n| Cyclooxygenase-2 inhibitor | 132 (2.8) | 158 (3.3) |\n\n【63】 Plus–minus values are means ±SD. To convert the values for creatinine to milligrams per deciliter, divide by 88.4. NT-proBNP denotes N-terminal pro–brain natriuretic peptide.\n\n【64】 Major surgery was defined as intraperitoneal, intrathoracic, retroperitoneal, or major orthopedic surgery.\n\n【65】 General surgery was defined as complex visceral resection (i.e., surgery involving the liver, esophagus, pancreas, or multiple organs); partial or total colectomy, stomach surgery, or small-bowel resection; major head and neck resection for nonthyroid tumor; or other intraabdominal surgery (e.g., dissection of the gallbladder, appendix, adrenal glands, spleen, or regional lymph nodes).\n\n【66】Table 2. Effects of Tranexamic Acid on 30-Day Outcomes. \n\n| Outcome | Tranexamic Acid(N=4757) | Placebo(N=4778) | Hazard Ratio(95% CI)  | P Value |\n| --- | --- | --- | --- | --- |\n| Primary efficacy outcome: composite bleeding outcome — no. (%)  | 433 (9.1) | 561 (11.7) | 0.76 (0.67–0.87) | <0.001  |\n| Individual components of composite bleeding outcome — no. (%) |  |  |  |  |\n| Life-threatening bleeding  | 78 (1.6) | 79 (1.7) | 0.99 (0.73–1.36) |  |\n| Major bleeding  | 363 (7.6) | 496 (10.4) | 0.72 (0.63–0.83) |  |\n| Bleeding into a critical organ  | 12 (0.3) | 21 (0.4) | 0.57 (0.28–1.16) |  |\n| Primary safety outcome: composite cardiovascular outcome — no./total no. (%)  | 649/4581 (14.2) | 639/4601 (13.9) | 1.02 (0.92–1.14) | 0.04  |\n| Individual components of composite cardiovascular outcome — no. (%) |  |  |  |  |\n| MINS  | 608 (12.8) | 602 (12.6) | 1.02 (0.91–1.14) |  |\n| Nonhemorrhagic stroke  | 24 (0.5) | 16 (0.3) | 1.51 (0.80–2.84) |  |\n| Peripheral arterial thrombosis  | 22 (0.5) | 23 (0.5) | 0.96 (0.53–1.72) |  |\n| Symptomatic proximal venous thromboembolism  | 32 (0.7) | 28 (0.6) | 1.15 (0.69–1.91) |  |\n| Other secondary outcomes — no. (%) |  |  |  |  |\n| Bleeding independently associated with death after noncardiac surgery | 416 (8.7) | 541 (11.3) | 0.76 (0.67–0.87) |  |\n| MINS not fulfilling the universal definition of myocardial infarction | 549 (11.5) | 549 (11.5) | 1.01 (0.89–1.13) |  |\n| Myocardial infarction | 67 (1.4) | 53 (1.1) | 1.27 (0.89–1.82) |  |\n| Net risk–benefit outcome  | 983 (20.7) | 1046 (21.9) | 0.94 (0.86–1.02) |  |\n\n【68】 MINS denotes myocardial injury after noncardiac surgery.\n\n【69】 The widths of the confidence intervals for the secondary and tertiary outcomes have not been adjusted for multiplicity, so the intervals should not be used to infer definitive treatment effects for these outcomes.\n\n【70】 The composite bleeding outcome was a composite of life-threatening bleeding, major bleeding, and bleeding into a critical organ.\n\n【71】 Shown is the two-sided P value for superiority.\n\n【72】 This is a secondary outcome.\n\n【73】 The composite cardiovascular outcome was a composite of myocardial injury after noncardiac surgery, nonhemorrhagic stroke, peripheral arterial thrombosis, and symptomatic proximal venous thromboembolism.\n\n【74】 Shown is the one-sided P value for noninferiority. To show statistical significance, this P value had to be less than 0.025.\n\n【75】 This is a tertiary outcome.\n\n【76】 The net risk–benefit outcome was a composite of death from cardiovascular causes, nonfatal life-threatening bleeding, major bleeding, bleeding into a critical organ, MINS, stroke, peripheral arterial thrombosis, and symptomatic proximal venous thromboembolism.\n\n【77】Table 3. Effects of Tranexamic Acid on 30-Day Tertiary Outcomes. \n\n| Outcome | Tranexamic Acid(N=4757) | Placebo(N=4778) | Hazard Ratio(95% CI)  |\n| --- | --- | --- | --- |\n| Major bleeding according to ISTH criteria — no. (%) | 315 (6.6) | 415 (8.7) | 0.75 (0.65 to 0.87) |\n| Transfusion of ≥1 unit of packed red cells — no. (%) | 449 (9.4) | 574 (12.0) | 0.77 (0.68 to 0.88)  |\n| Death from any cause — no. (%) | 52 (1.1) | 57 (1.2) | 0.92 (0.63 to 1.33) |\n| Death from cardiovascular cause — no. (%) | 25 (0.5) | 30 (0.6) | 0.84 (0.49 to 1.42) |\n| Hemorrhagic stroke — no. (%) | 2 (<0.1) | 0 | — |\n| Amputation — no. (%) | 14 (0.3) | 21 (0.4) | 0.67 (0.34 to 1.31) |\n| Symptomatic pulmonary embolism — no. (%) | 24 (0.5) | 17 (0.4) | 1.42 (0.76 to 2.64) |\n| Symptomatic proximal deep venous thrombosis — no. (%) | 11 (0.2) | 13 (0.3) | 0.85 (0.38 to 1.90) |\n| Any symptomatic or asymptomatic proximal venous thromboembolism — no. (%) | 32 (0.7) | 28 (0.6) | 1.15 (0.69 to 1.91) |\n| Cardiac revascularization — no. (%) | 12 (0.3) | 13 (0.3) | 0.93 (0.42 to 2.03) |\n| Acute kidney injury — no. (%)  | 672 (14.1) | 655 (13.7) | 1.03 (0.93 to 1.15) |\n| New renal-replacement therapy — no. (%) | 19 (0.4) | 16 (0.3) | 1.19 (0.61 to 2.32) |\n| Rehospitalization for cardiovascular reasons — no. (%) | 84 (1.8) | 75 (1.6) | 1.13 (0.82 to 1.54) |\n| Seizure — no. (%) | 10 (0.2) | 3 (0.1) | 3.35 (0.92 to 12.20) |\n| Infection — no. (%) | 499 (10.5) | 487 (10.2) | 1.03 (0.91 to 1.17) |\n| Sepsis — no. (%) | 68 (1.4) | 63 (1.3) | 1.08 (0.77 to 1.53) |\n| Median length of hospital stay (IQR) — days | 4.0 (2.1 to 7.1) | 4.0 (2.1 to 7.1) | 0 (−0.1 to 0.1)  |\n| Median no. of days alive at home (IQR) | 25 (22 to 28) | 25 (21 to 28) | 0 (−0.4 to <0.1)  |\n| Disability — no. (%) | 1408 (31.9) | 1407 (31.6) | 1.02 (0.92 to 1.13)  |\n\n【79】 IQR denotes interquartile range, and ISTH International Society on Thrombosis and Haemostasis.\n\n【80】 The widths of the confidence intervals have not been adjusted for multiplicity, so the intervals should not be used to infer definitive treatment effects.\n\n【81】 Shown is the odds ratio.\n\n【82】 Acute kidney injury was defined as an increase in the serum creatinine concentration from the preoperative (prerandomization) concentration either by 26.5 μmol or more per liter (0.3 mg per deciliter) within 48 hours after surgery or by 50% or more within 7 days after surgery.\n\n【83】 Shown is the median difference.\n\n【84】 Shown is the odds ratio from logistic regression with a score on the 12-item World Health Organization Disability Schedule 2.0 (WHODAS 2.0) of 12 or higher at 30 days as the dependent variable, with adjustment for the WHODAS 2.0 score at baseline. WHODAS 2.0 scores range from 0 to 48, with higher scores indicating greater disability. A score of 12 or higher represents a disability level of 25% or higher. A total of 4416 patients in the tranexamic acid group and 4459 in the placebo group had baseline and follow-up data available and were included in the analyses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8d833f0d-5d86-4fa6-b830-1e7f6ae8339a", "title": "Shock-Wave Lithotripsy for Renal Calculi", "text": "【0】Shock-Wave Lithotripsy for Renal Calculi\nA 42-year-old man presents with a symptomatic kidney stone 12 mm in diameter. Treatment with extracorporeal shock-wave lithotripsy is recommended. Lithotripsy creates fluid shock waves that are transmitted through the body to cause fragmentation of kidney stones.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "14a937ea-996a-43f9-99cd-616a21fdacf8", "title": "Increased Resistance to Penicillin of Pneumococci Isolated from Man", "text": "【0】Increased Resistance to Penicillin of Pneumococci Isolated from Man\nAbstract\n--------\n\n【1】Strains of _Diplococcus pneumoniae_ relatively insensitive to penicillin were isolated from an aboriginal child in Australia and from 15 New Guineans. The concentration of penicillin required to inhibit growth was 25 times that which inhibited sensitive pneumococci. These pneumococci were also partially resistant to cephalosporin antibiotics. All the relatively insensitive pneumococci from New Guinea were identified as Type 4, suggesting that transmission of the partially resistant strain had occurred. The isolations were made from persons who lived in remote areas where penicillin had frequently been used.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a7b864a6-e8ff-4c9d-97f2-24f52c481882", "title": "Obesity", "text": "【0】Obesity\nTo the Editor\n-------------\n\n【1】In their article on obesity, Rosenbaum et al. (Aug. 7 issue)  characterize sibutramine, currently undergoing regulatory review as a drug for the treatment of obesity, as having both catecholaminergic and serotonergic agonist effects. Sibutramine, in fact, is not an agonist at catecholamine or serotonin receptors but instead acts by inhibiting the reuptake of serotonin and norepinephrine at central synapses. Thus, sibutramine's mode of action is similar to that of other monoamine-reuptake inhibitors such as venlafaxine (an inhibitor of serotonin and norepinephrine reuptake) and fluoxetine (a selective serotonin-reuptake inhibitor). The mode of action of drugs that alter appetite by enhancing central monoamine activity has several implications. For instance, primary pulmonary hypertension has been associated with certain appetite-suppressant drugs  (e.g., fenfluramine), as well as other drugs (e.g., cocaine), that act by causing monoamine release.  Rosenbaum et al. characterize the implicated anorectic drugs as reuptake inhibitors. In fact, monoamine-reuptake inhibitors that do not cause monoamine release, such as tricyclic antidepressant drugs, selective serotonin-reuptake inhibitors, and serotonin- and norepinephrine-reuptake inhibitors, have not been associated with an increased risk of primary pulmonary hypertension or neurotoxicity  or with the cardiac valvulopathy reported by Connolly et al.  Given the diversity of the satiety-enhancing drugs currently being developed, attention to the details of the mechanism of action may be even more important in the future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "91bcc52d-7596-4d70-b73c-75744e3a14a1", "title": "Acquired Bleeding Diathesis in a Patient Taking PC-SPES", "text": "【0】Acquired Bleeding Diathesis in a Patient Taking PC-SPES\nTo the Editor:\n--------------\n\n【1】PC-SPES is a commercially available nutritional supplement containing eight herbs that is used by many patients with prostate cancer. It has potent estrogenic activity and substantial antineoplastic effects in patients with prostate cancer.  We describe a patient with profound bleeding diathesis after one month of unsupervised use of this compound.\n\n【2】Table 1. Hematologic and Coagulation Variables.\n\n【3】A 62-year-old man with hormone-refractory prostate cancer and nodal metastases (stage D1 disease) presented to the emergency department after an episode of syncope. This episode had been preceded by one day of epistaxis, abdominal pain, hematuria, and the passage of maroon stools. The patient denied a family history of bleeding or bruising, and the prothrombin time and activated partial-thromboplastin time had previously been normal. The only medications were 12 capsules of PC-SPES daily (twice the manufacturer's recommended dosage) for one month, in addition to multivitamins. The initial vital signs were notable for a pulse of 120 beats per minute and a blood pressure of 112/82 mm Hg. Physical examination revealed extensive ecchymoses, and a computed tomographic scan showed a large retroperitoneal hematoma. The results of laboratory analysis and mixing studies with normal plasma are shown in Table 1 . Hepatic-function variables were within normal limits. Initial treatment included the transfusion of 2 units of packed red cells and 6 units of fresh-frozen plasma and the administration of vitamin K. On follow-up testing three weeks and again three months later, the prothrombin time and activated partial-thromboplastin time were normal. Laboratory studies on admission also unexpectedly revealed a serum warfarin level of 0.69 μg per milliliter (therapeutic range, 2 to 8).\n\n【4】The use of PC-SPES has been associated with increased rates of deep venous thrombosis and pulmonary embolism, but to our knowledge its use has never been linked to hemorrhage. Two isolates of the herb Baikal skullcap ( _Scutellaria baicalensis Georgi_ ), one of the components of PC-SPES, are coumarins (compounds structurally related to warfarin, a synthetic coumarin) and have been shown to act as vitamin K reductase inhibitors in a manner similar to that of warfarin.  In this case, the history and laboratory variables were most consistent with the presence of an acquired coagulation-factor deficiency, and the measured warfarin level was insufficient to have induced the described changes in coagulation variables.\n\n【5】To investigate the possibility that a phytocoumarin was contained in the PC-SPES, two mice were given the PC-SPES preparation procured from the patient at a dosage of 500 mg per kilogram of body weight per day, administered orally as previously described.  After three days, the mean (±SD) measured serum warfarin level was 0.87 ±0.2 μg per milliliter. These results indicate that the PC-SPES preparation that this patient took contained a component that comigrates with warfarin on high-performance liquid chromatography. The transient, severe bleeding diathesis in this patient was probably the result of unsupervised use of PC-SPES and was probably related to a phytocoumarin or other compound in the PC-SPES. Patients should be counseled that PC-SPES has multiple thrombotic and hemorrhagic side effects, that these potentially harmful complications must be balanced with the antineoplastic effects of PC-SPES, and that unsupervised use of this preparation is not recommended.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a3111968-794d-42e6-8cf0-e930f5ec2359", "title": "Aspergillus Infections", "text": "【0】Aspergillus Infections\nAspergillosis is an opportunistic fungal infection that poses a particular risk for patients with neutrophil defects and causes diverse clinical syndromes. This review addresses our current understanding of aspergillosis and advances in diagnosis and treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8992189b-42e8-4138-a477-c3c6c2e0f7d1", "title": "Evaluation of Acromegaly by Radioimmunoassay of Somatomedin-C", "text": "【0】Evaluation of Acromegaly by Radioimmunoassay of Somatomedin-C\nAbstract\n--------\n\n【1】We measured serum concentrations of somatomedin-C by radioimmunoassay in 57 acromegalic patients and compared them with various indicators of disease activity. The mean fasting somatomedin-C concentration was 6.8 U per milliliter (range, 2.6 to 21.7) for the acromegalics and 0.67 U per milliliter (range, 0.31 to 1.4) for 48 normal, fasting adults. The somatomedin-C concentration correlated significantly with: heel-pad thickness (r = 0.73), fasting glucose (r = 0.74), and one-hour postprandial glucose (r = 0.77). In contrast, \"glucose-suppressed\" growth hormone correlated weakly (r = 0.34, 0.36, 0.34) with these clinical indexes of severity. Fasting growth hormone levels showed no correlation (r = 0.14). Five active acromegalics had \"normal\" growth hormone levels after glucose suppression, but they had elevated somatomedin-C. In 15 patients studied one year after treatment, changes in somatomedin-C concentrations paralleled the degree of clinical improvement. Measurement of somatomedin-C appears to provide a reliable means for confirming the diagnosis of acromegaly and of clinical disease activity than measurement of growth hormone concentrations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d2e7c56e-536f-40ec-ab32-faa69259486e", "title": "Graphic Perspective: Screen Time", "text": "【0】Graphic Perspective: Screen Time\n*   _8_ Comments\n\n【1】Article\n-------\n\n【2】### Audio Interview\n\n【3】 Interview with Dr. Elizabeth Rourke on practicing primary care over the Internet during the Covid-19 pandemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5a458970-8707-42c0-8642-61fb9d171487", "title": "The Use of Ambulatory Testing in Prepaid and Fee-for-Service Group Practices", "text": "【0】The Use of Ambulatory Testing in Prepaid and Fee-for-Service Group Practices\nAbstract\n\n【1】To examine the influence of method of payment on ambulatory testing by internists, we compared the rate at which patients with uncomplicated hypertension were tested by 10 doctors practicing in large fee-for-service groups with that by 17 doctors in large prepaid groups. We examined the use of individual tests and asked the doctors in the fee-for-service groups what they believed about the profitability and costs of tests.\n\n【2】After correcting for the patient's age, sex, duration of disease, and severity of disease as measured by pretreatment blood pressure, and for the doctor's year of medical school graduation, we found that 50 percent more electrocardiograms were obtained among patients in fee-for-service practices (0.69 per patient per year vs. 0.45, P = 0.006), and 40 percent more chest radiographs (0.49 vs. 0.35, P = 0.11). Fee-for-service doctors believed that both tests were associated with high profit and costs.\n\n【3】These results suggest that the use of certain high-profit, high-cost tests is higher in large fee-for-service groups than in large prepaid groups. Although the generalizability of conclusions based on this limited study must be considered tentative, the findings suggest that it may be appropriate to consider changing the payments for tests as part of a more general reform of the fee schedules.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "40fbaa62-4b63-4761-8861-c976b79851fc", "title": "Cytisine versus Nicotine for Smoking Cessation", "text": "【0】Cytisine versus Nicotine for Smoking Cessation\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Placebo-controlled trials indicate that cytisine, a partial agonist that binds the nicotinic acetylcholine receptor and is used for smoking cessation, almost doubles the chances of quitting at 6 months. We investigated whether cytisine was at least as effective as nicotine-replacement therapy in helping smokers to quit.\n\n【3】Methods\n-------\n\n【4】We conducted a pragmatic, open-label, noninferiority trial in New Zealand in which 1310 adult daily smokers who were motivated to quit and called the national quitline were randomly assigned in a  ratio to receive cytisine for 25 days or nicotine-replacement therapy for 8 weeks. Cytisine was provided by mail, free of charge, and nicotine-replacement therapy was provided through vouchers for low-cost patches along with gum or lozenges. Low-intensity, telephone-delivered behavioral support was provided to both groups through the quitline. The primary outcome was self-reported continuous abstinence at 1 month.\n\n【5】Results\n-------\n\n【6】At 1 month, continuous abstinence from smoking was reported for 40% of participants receiving cytisine (264 of 655) and 31% of participants receiving nicotine-replacement therapy (203 of 655), for a difference of 9.3 percentage points (95% confidence interval, 4.2 to 14.5). The effectiveness of cytisine for continuous abstinence was superior to that of nicotine-replacement therapy at 1 week, 2 months, and 6 months. In a prespecified subgroup analysis of the primary outcome, cytisine was superior to nicotine-replacement therapy among women and noninferior among men. Self-reported adverse events over 6 months occurred more frequently in the cytisine group (288 events among 204 participants) than in the group receiving nicotine-replacement therapy (174 events among 134 participants); adverse events were primarily nausea and vomiting and sleep disorders.\n\n【7】Conclusions\n-----------\n\n【8】When combined with brief behavioral support, cytisine was found to be superior to nicotine-replacement therapy in helping smokers quit smoking, but it was associated with a higher frequency of self-reported adverse events. \n\n【9】Introduction\n------------\n\n【10】 QUICK TAKE  \nCytisine vs. Nicotine  \n\n【11】Cytisine is a plant-based alkaloid found in members of the Leguminosae family.  Like varenicline, cytisine is a partial agonist of nicotinic acetylcholine receptors (nAChRs), with an affinity for the α4β2 receptor subtype,  and a half-life of 4.8 hours.  Cytisine is a generic agent currently manufactured by Sopharma as Tabex and by Aflofarm Pharma as Desmoxan. It has been available both with and without prescription for smoking cessation since the 1960s, largely in Eastern Europe.  Four systematic reviews report cytisine to be superior to placebo for short-term and long-term abstinence.  When taken at the recommended dosage (1.5 to 9 mg per day for 25 days), cytisine is associated with no significant increase in adverse events as compared with placebo (20.5% vs. 19.6%), although gastrointestinal symptoms are more common (11.9% vs. 7.2%). \n\n【12】Cytisine remains relatively unknown outside Eastern Europe despite calls for licensing worldwide  because of its proven benefits, low cost as compared with other cessation medications (cytisine, $20 to $30 for 25 days; nicotine-replacement therapy, $112 to $685 for 8 to 10 weeks; varenicline, $474 to $501 for 12 weeks),  and low cost per quality-adjusted-life-year.  Given that no trials have compared cytisine with nicotine-replacement therapy, we designed a noninferiority trial to investigate whether cytisine was at least as effective as nicotine-replacement therapy. We hypothesized that 25 days of cytisine plus low-intensity behavioral support would be at least as effective as 8 weeks of nicotine-replacement therapy plus low-intensity behavioral support for smoking cessation.\n\n【13】Methods\n-------\n\n【14】Study Oversight\n---------------\n\n【15】 The Health Research Council of New Zealand funded the trial, including the reimbursement of Quitline, which was engaged to recruit participants. Cytisine was supplied at no cost by the manufacturer, Sopharma. The research council, Sopharma, and the Extab Corporation had no role in the design of the trial, the collection, analysis, or interpretation of the data, or the writing of the report for publication. The protocol was approved by the New Zealand Multi-Region Ethics Committee and the Standing Committee on Therapeutic Trials. All participants provided oral informed consent. All authors were involved in the design and conduct of the trial and the writing of the manuscript. The first and second authors oversaw the conduct of the study, the seventh author performed all statistical analyses, and the first author supervised the writing of the manuscript.\n\n【16】Participants\n------------\n\n【17】This parallel-group, randomized, controlled, noninferiority trial was conducted in New Zealand.  The first randomization was conducted on March 29, 2011, and the last follow-up took place on February 4, 2013. Smokers were recruited through the New Zealand national quitline. To be included in the study, participants had to be at least 18 years of age, daily smokers, and motivated to quit. Potential participants were excluded if they were pregnant or breast-feeding, were taking smoking-cessation medication, were enrolled in another cessation program or study, had self-reported pheochromocytoma, had a systolic blood pressure above 150 mm Hg, a diastolic blood pressure above 100 mm Hg, or both, had schizophrenia, or had had a self-reported cardiovascular event in the 2 weeks before study enrollment.\n\n【18】Randomization\n-------------\n\n【19】Eligible participants who had called the quitline were randomly allocated, by computer, to nicotine-replacement therapy or cytisine in a  ratio. Randomization was stratified with the use of minimization according to sex, ethnicity (Maori, Pacific Islander, or non-Maori and non–Pacific Islander), and cigarette dependence, which was determined by means of the Fagerström Test of Cigarette Dependence, in which smokers were assigned to one of two groups: those with scores of 5 or lower, indicating lower dependence, and those with scores greater than 5, indicating greater dependence.  Participants and researchers collecting outcome data were aware of treatment allocation.\n\n【20】Procedures\n----------\n\n【21】All participants were offered low-intensity telephone behavioral support (an average of three calls of 10 to 15 minutes each from Quitline advisors over a period of 8 weeks). Participants assigned to nicotine-replacement therapy received vouchers from Quitline that were redeemable from community pharmacies for nicotine patches (in doses of 7 mg, 14 mg, or 21 mg) and for gum (2 mg or 4 mg) or lozenges (1 mg or 2 mg) or both gum and lozenges at a cost of NZ$3 for an 8-week supply of each item (the equivalent of €2, or approximately $2.50 in U.S. dollars). The type and strength of nicotine-replacement therapy were determined by Quitline advisors in accordance with national smoking-cessation guidelines  and participant preference. The cytisine group received a 25-day course of tablets by courier and were asked to reduce their smoking at their own pace during the first 4 days of treatment such that they were not smoking at all by the 5th day (i.e., their “quit date”). Participants followed the manufacturer’s recommended dosing regimen: days 1 through 3, one tablet every 2 hours through the waking day (up to six tablets per day); days 4 through 12, one tablet every 2.5 hours (up to five tablets per day); days 13 through 16, one tablet every 3 hours (up to four tablets per day); days 17 through 20, one tablet every 4 to 5 hours (three tablets per day); and days 21 through 25, one tablet every 6 hours (two tablets per day). Participants in the cytisine group also received the vouchers for nicotine-replacement therapy sent to participants in the nicotine-replacement therapy group. They were asked to take the cytisine tablets for 25 days. If they had not stopped smoking by that time, or if they required ongoing support to refrain from smoking after that time, they were to redeem the vouchers for nicotine-replacement therapy.\n\n【22】At baseline, data were collected on demographics, smoking history, concomitant medication, motivation to quit smoking (with a score of 1 indicating very low motivation and a score of 5 indicating very high motivation), symptoms of withdrawal and the urge to smoke (both assessed with the Mood and Physical Symptoms Scale, with symptoms rated on a scale of 1 to 5, with 1 indicating none and 5 the most severe, and the urge to smoke scored on a scale of 0 to 10, with higher scores indicating greater strength of the urge to smoke and a greater duration of these urges),  alcohol use (assessed with the Alcohol Use Disorders Identification Test \\[AUDIT-C\\], rated on a scale of 0 to 12, with higher scores indicating a greater risk of alcohol dependence),  and satisfaction with smoking (assessed with the modified Cigarette Evaluation Questionnaire, which included 12 subscales; in each subscale, a score of 1 indicated not at all satisfied and a score of 7 indicated extremely satisfied). \n\n【23】The primary outcome was continuous abstinence from smoking (self-reported abstinence since quit day, with an allowance for smoking a total of five cigarettes or less,  including during the previous 7 days) 1 month after quit day. Secondary outcomes assessed at 1 week and at 1, 2 and 6 months after quit day were self-reported treatment compliance (total number of cytisine tablets taken or the type, strength, and amount of nicotine-replacement therapy used); alcohol use  ; motivation to quit; symptoms of tobacco withdrawal; and the strength of urges to smoke and the duration of these urges  ; 7-day point prevalence for abstinence (no cigarettes, not a single puff, in the previous 7 days)  ; continuous abstinence; smoking satisfaction  ; concomitant medication; and, if still smoking, date returned to daily smoking and the number of cigarettes smoked per day. Self-reported adverse events were recorded at each follow-up call, coded in accordance with the _International Statistical Classification of Diseases and Related Health Problems,_ Tenth Revision, Australian Modification, and classified by a medical practitioner as nonserious or serious (defined as death, life-threatening, hospitalization, or otherwise medically important) and according to severity (mild — awareness of event but easily tolerated; moderate — discomfort extensive enough to cause some interference with usual activity; or severe — inability to carry out usual activity). All adverse events were reviewed by an independent data safety and monitoring committee. At 1 week and 1 month we also asked participants in the cytisine group whether they would recommend cytisine as a cessation aid.\n\n【24】Statistical Analysis\n--------------------\n\n【25】For our sample of 1310 people (655 per group), we assumed a loss of 20% to follow-up and a power of 90% at the one-sided significance level of 0.025 (the equivalent of a two-sided significance level of 0.05) to detect a 5% difference in 1-month quit rates between groups. The 1-month quit rate in the cytisine group was assumed to be 55%, midway between the estimate of 60% for varenicline  and 50% for nicotine-replacement therapy.  A noninferiority margin of difference between the group proportions was set at 5%.\n\n【26】Analyses were performed with SAS Software, version 9.3 (SAS Institute), and were guided by a prespecified plan. Noninferiority for the primary outcome was evaluated by observing whether the lower bound of the two-sided 95% confidence intervals for the risk difference in quit rates between the groups was above the noninferiority limit of −5. The primary analyses were carried out on an intention-to-treat basis (participants for whom outcomes were missing were assumed to be smoking). In the case that noninferiority was evident, assessment as to whether cytisine had effectiveness superior to that of nicotine-replacement therapy was carried out according to the same approach but was compared with a zero difference. Per-protocol analyses excluded participants who had missing data at 1 month or who had major protocol violations (e.g., death, pregnancy, withdrawal from the study, loss to follow-up, or noncompliance). Compliance in the cytisine group was defined as having taken 80% or more of the required number of tablets within 1 month after the quit date (i.e., 80 tablets or more). Compliance in the nicotine-replacement therapy group was defined as having used nicotine-replacement therapy at both 1 week and 1 month after the quit date. Participants with missing data were assumed to be noncompliant with the study regimen. Complete case analysis was also undertaken, and quit rates, relative risk, risk difference, and the number needed to treat were calculated. Treatment groups were compared with the use of chi-square tests and unadjusted and adjusted logistic regression modeling (adjusting for minimization factors and education).\n\n【27】In prespecified subgroup analyses, the consistency of effects was assessed with tests for heterogeneity for the primary outcome according to ethnicity (Maori vs. non-Maori), age (<40 years of age vs. ≥40 years of age), sex, and level of education (<12 years of schooling or no qualification vs. ≥12 years of schooling), type of cigarettes smoked (factory-made only, roll-your-own only, or factory-made and roll-your-own), and baseline AUDIT-C score (high vs. low). Post hoc subgroup analyses for the primary outcome were undertaken according to baseline level of cigarette dependence and use of nicotine-replacement therapy in the preceding 12 months. The change from baseline in symptoms of tobacco withdrawal (for abstainers), AUDIT-C score, and number of cigarettes smoked per day over time was assessed by means of repeated-measures mixed models adjusted for baseline value. Kaplan–Meier curves, the log rank test, and Cox proportional hazards regression analysis were used to measure time to first lapse from quit date (return to daily smoking).\n\n【28】Results\n-------\n\n【29】Outcomes\n--------\n\n【30】Figure 1. Recruitment and Retention of Participants throughout the Trial.\n\n【31】At initial assessment, the main reasons for ineligibility were current use of other smoking-cessation products or services (71%), schizophrenia (9%), uncontrolled blood pressure (9%), and pregnancy or breast-feeding (8%). For the 1-month analysis, protocol violations (as defined in the study) included loss to follow-up, death, and study withdrawal. NRT denotes nicotine-replacement therapy.Table 1.  Table 1. Baseline Characteristics of the Participants.\n\n【32】Among 3001 people assessed, 1310 were eligible for study participation and underwent randomization, 655 in each group . Loss to follow-up at 1 month was 12% in both groups . Baseline characteristics were evenly balanced between the treatment groups .\n\n【33】Table 2. Continuous Abstinence and 7-Day Point-Prevalence Abstinence According to Treatment Group, According to the Intention-to-Treat Analysis.\n\n【34】Cytisine was not only noninferior to nicotine-replacement therapy but had superior effectiveness: 1-month continuous abstinence rates were significantly higher in the cytisine group (40%, 264 of 655) than in the nicotine-replacement therapy group (31%, 203 of 655) (risk difference, 9.3 percentage points; 95% confidence interval \\[CI\\], 4.2 to 14.5; number needed to treat, 11) . Complete case and per-protocol analyses revealed similar findings . Adjusted logistic-regression analysis produced an odds ratio of 1.5 for abstinence with cytisine(95% CI, 1.2 to 1.9; P=0.003). Prespecified analyses conducted according to sex showed a significantly higher 1-month continuous abstinence rate with cytisine in women and showed no significant difference (noninferiority) in men (P=0.011 for heterogeneity), with no significant differences noted for other subgroups . The secondary cessation outcomes at 1 week and at 1, 2, and 6 months were consistent with the primary outcome, with the exception of 6-month, 7-day point-prevalence abstinence rates (which were not statistically significant) .\n\n【35】At 1 week, 66% (392 of 596) of participants in the nicotine-replacement therapy group were using nicotine-replacement therapy obtained through the voucher system as compared with 4% (26 of 600) in the cytisine group. In the cytisine group, at 1 week after the quit date, 63 tablets should have been taken, but participants reported taking a mean (±SD) of 49±24 tablets. At 1 month after the quit date, all 100 tablets should have been taken, but participants reported taking a mean of 72±34 tablets. Overall, 53% (344 of 655) of participants were in compliance with the guidelines for cytisine treatment (i.e., they had taken 80 or more tablets), whereas 67% (437 of 655) in the nicotine-replacement therapy group were in compliance with treatment guidelines (i.e., they had used nicotine-replacement therapy at both 1 week and 1 month). At 1 month after the quit date, 22% of participants in the cytisine group and 55% of those in the nicotine-replacement therapy group continued their allocated treatment. In the cytisine group, there were 19 participants who used cytisine and nicotine-replacement therapy concomitantly.\n\n【36】Adverse Events\n--------------\n\n【37】Table 3. Summary of All-Cause Adverse Events\n\n【38】Self-reported adverse events occurred more frequently in the cytisine group (288 events reported by 204 participants) than in the nicotine-replacement-therapy group (174 events reported by 134 participants), with an incidence rate ratio of 1.7 (95% CI, 1.4 to 2.0; P<0.001) . Similar findings were observed for participants who were compliant with treatment (161 events reported by 107 participants in the cytisine group vs. 113 events reported by 88 participants in the nicotine-replacement-therapy group; incidence rate ratio, 1.4; 95% CI, 1.1 to 1.8; P=0.003) . In the cytisine group, 67% of adverse events were reported between randomization and 1 month after the quit date, as compared with 49% in the nicotine-replacement-therapy group. The majority of adverse events were nonserious and were mild to moderate in severity . The most frequent adverse events in the cytisine group were nausea and vomiting and sleep disorders . Serious adverse events are listed in Table S5 in the Supplementary Appendix. Among participants in the cytisine group who reported an adverse event, 89% at 1 week (2 missing) and 82% at 1 month after the quit date (13 missing) said they would recommend cytisine to someone who wanted to stop smoking.\n\n【39】Figure 2. Kaplan–Meier Curve for Time to First Relapse.\n\n【40】Twenty-one percent of participants in the cytisine group and 34% of participants in the NRT group did not quit on the quit date (Day 0). Time to relapse was defined as the number of days to return to regular (daily) smoking of cigarettes. In the cytisine group, the median time to relapse was significantly longer among those who complied with treatment (177 participants) than among those who did not (171 participants): 127 days (95% CI, 62 to unable to estimate) versus 20 days (95% CI, 8 to 25); P<0.001 by the log-rank test.\n\n【41】Overall, 21% of participants in the cytisine group and 34% of those in the nicotine-replacement-therapy group did not quit on the quit day. The median time to relapse (resumption of smoking) after the quit day was significantly longer in the cytisine group than in the nicotine-replacement therapy group: 53 days (95% CI, 36 to 100) versus 11 days (95% CI, 6 to 22) (P<0.001 for log-rank test; hazard ratio, 0.8 \\[95% CI, 0.7 to 0.9, P=0.001\\]); 348 participants in the cytisine group relapsed within 6 months versus 389 participants in the nicotine-replacement therapy group . A greater difference was observed for those who were compliant with treatment . Other outcome data are presented in the Supplementary Appendix .\n\n【42】Discussion\n----------\n\n【43】Cytisine was superior to nicotine-replacement therapy for smoking cessation among dependent smokers motivated to quit. Self-reported adverse events over 6 months were almost twice as common in the cytisine group than in the nicotine-replacement-therapy group. Types of adverse events in the cytisine group were similar to those seen in previous placebo-controlled trials of cytisine.  Our trial was neither large enough nor long enough to assess the occurrence of uncommon adverse events or those with a long time to onset. Compliance with allocated treatment was modest. Time to relapse was delayed in the cytisine group. During treatment, participants in the cytisine group reported fewer symptoms of tobacco withdrawal, found smoking less rewarding, and reduced the number of cigarettes smoked per day. The higher quit rate observed in women taking cytisine has not been previously reported in studies of nAChR partial agonists. This finding could be the result of chance (since data were not adjusted for multiplicity) but warrants further investigation, since several reviews of nicotine-replacement therapy have reported lower quit rates in women than in men,  possibly as a result of biologic and psychosocial differences.\n\n【44】We chose a noninferiority design on the basis of data available at the time, before publication of the placebo-controlled trial by West et al.  Our trial population was similar to that of New Zealand Quitline callers overall  (although Asian, Pacific Islander, and male smokers are slightly underrepresented among Quitline users),  New Zealand smokers overall,  and three previous trial populations recruited through Quitline.  The trial was pragmatic, with broad entry criteria. The criteria for exclusion on the basis of medical conditions reflected the manufacturer’s precautions for the use of cytisine and Quitline’s policy regarding contraindications for nicotine-replacement therapy. As a precaution, we excluded people who reported that they had schizophrenia, given cytisine’s similarity to varenicline (which has a boxed warning related to severe mental health problems). Users of noncigarette tobacco products were not excluded but were probably few in number given that such products are rarely used in New Zealand and that the sale of snus (a moist tobacco powder that is placed under the upper lip) is illegal. We followed the manufacturer’s recommended dosing regimen for cytisine, although we are not aware of any published studies that support the regimen.\n\n【45】Our study had several limitations. First, since researchers were aware of treatment allocation, there may have been a reporting bias in favor of cytisine. Second, although adverse events were medically reviewed, they were self-reported. The higher proportion of adverse events in the cytisine group may be due to reporting bias, since the known side effects of nicotine-replacement therapy could have been regarded as “normal” by participants in the nicotine-replacement therapy group who had previously received such therapy and could therefore have gone unreported. We did not collect long-term safety data; in the placebo-controlled trial by West et al.,  adverse events reported during a 12-month follow-up period were predominantly related to gastrointestinal effects. Third, because participants were unlikely to have prior knowledge of cytisine, some of the treatment effect might be explained by its novelty. Fourth, verification of self-reported abstinence was not undertaken owing to both the broad geographic dispersal of the study population and budget constraints. In addition, at 1 month participants in the nicotine-replacement-therapy group would have received positive test results for cotinine, a metabolite of nicotine, even if they were not smoking. Abstinence rates may therefore be overreported or underreported , but there should not be a difference in the nature of reporting between groups. Fifth, smokers who call Quitline may be more motivated to quit than other populations of smokers.  Sixth, the treatment periods for the two interventions were different; the selection of a 1-month primary outcome should help to ensure comparability. There was also a between-group difference in participants’ access to treatments (i.e., cytisine was delivered free, whereas nicotine-replacement therapy was obtained from a pharmacy at a small cost). In a previous trial with 1410 participants in which the same recruitment method was used, participants were randomly assigned either to receive free delivery of nicotine-replacement therapy by courier or to use of the voucher system.  No between-group differences in continuous abstinence at 3 weeks, 3 months, or 6 months were observed.\n\n【46】The trial shows that cytisine is an effective smoking-cessation aid for use as a first-line treatment for tobacco dependence. The most common adverse events were nausea and vomiting and sleep disorders. The effect sizes in this trial were similar to those observed in a trial of varenicline versus nicotine-replacement therapy.  Given the large difference in the market prices of varenicline and cytisine,  a head-to-head, noninferiority trial that includes cost-effectiveness analyses is justified.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8078ccde-5c5c-49ed-bd32-6c3eba339081", "title": "Cardiac Macrophages — Keeping the Engine Running Clean", "text": "【0】Cardiac Macrophages — Keeping the Engine Running Clean\nCardiomyocyte function depends on ATP, the generation of which takes place in the mitochondrion; strong performance of mitochondria is probably critical to cardiac function. A recent study delineates a mechanism through which spent mitochondria are cleared.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6e39bb64-17c8-4f88-afaf-73ac90f91705", "title": "The Nutritional Value of University-Hospital Diets", "text": "【0】The Nutritional Value of University-Hospital Diets\nTo the Editor:\n--------------\n\n【1】In 1989, the National Research Council  published a comprehensive and authoritative examination of the role of diet in disease, which on the basis of the evidence, proposed dietary guidelines for promoting health and reducing the risk of chronic disease. The council recommended reducing the percentage of dietary fat to an average of 30 percent of calories or less, the amount of saturated fat to 10 percent of calories or less, and the level of cholesterol to 300 mg per day or less; eating five or more servings of fruits and vegetables daily and six or more servings of grains; limiting the total daily intake of salt to 6 g; and maintaining a calcium intake of at least 800 mg per day. Such recommendations were incorporated by the Public Health Service as national nutrition objectives for health promotion and disease prevention to be attained by the year 2000. \n\n【2】Table 1. Nutritional Analysis of the House Diets of 57 University Hospitals.\n\n【3】We performed a computerized analysis (N-Squared Nutritionist IV, N-Squared Computing, Silverton, Oreg.) of the nutritional value of the house diets offered in 65 university teaching hospitals, which were members of the University Hospital Consortium, to determine their compliance with the above recommendations. Each institution was asked to provide three daily menus that they offered to patients who were not under any specific dietary restrictions and to indicate the menu items that would be served to patients who did not make their own selections. The dietary departments of 57 of 65 hospitals (88 percent) responded to the survey. Only four (7 percent) of the hospital menus met all seven recommendations. The content of fat, saturated fat, cholesterol, and sodium exceeded the recommended levels in 22 (39 percent), 27 (47 percent), 46 (81 percent), and 31 (54 percent) of the hospitals, respectively . The menus provided for an adequate number of servings of fruits and vegetables and grains in 50 (88 percent) and 53 (93 percent) of the hospitals, respectively. The menus of 25 hospitals (44 percent) provided information concerning the nutritional value of dietary items to patients.\n\n【4】On the basis of this study, we conclude that many teaching hospitals do not design house diets to meet nationally recognized dietary recommendations and do not supply patients with enough information to help them make healthful dietary choices. These discouraging results are consistent with those of a recent report by Israeli investigators, who found that none of that country's university hospitals served diets that met the dietary goals of the American Heart Association. \n\n【5】Hospitals should assume a greater role in promoting more healthful diets. We cannot think of a more appropriate place to encourage the nutritional health of Americans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d225650b-d40d-4446-ada1-7aa10dfeaab9", "title": "Evaluation and Management of Enuresis", "text": "【0】Evaluation and Management of Enuresis\nThe parents of an 8-year-old boy bring him to the pediatrician's office for evaluation of nightly bed-wetting. He often does not void during school hours, and when he returns home after school, he usually rushes to the bathroom. He arrives home thirsty after school, and most of his daily fluid intake is in the afternoon and evening. How should this boy be evaluated and treated?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "de88aac7-ff0d-4342-a36d-eadb61210329", "title": "Visualization of Early Engraftment in Clinical Islet Transplantation by Positron-Emission Tomography", "text": "【0】Visualization of Early Engraftment in Clinical Islet Transplantation by Positron-Emission Tomography\nTo the Editor:\n--------------\n\n【1】Although islet transplantation is an option for patients with type 1 diabetes, even those who become insulin-independent after engraftment have islet-graft function that is estimated to be less than 30% of that in a healthy person.  In fact, most patients resume insulin treatment within 2 years after transplantation. \n\n【2】Figure 1. PET–CT Images of Labeled Islets, Radioactivity Uptake, and Plasma C-Peptide Levels during and after Transplantation.\n\n【3】Panel A shows coronal (subpanel a), sagittal (subpanel b), and transaxial (subpanel c) PET–CT images of the liver 70 minutes after the initiation of the islet transplantation. The regional radioactivity concentration (Bq/cc) in the images is shown in color, as indicated in the bar, with red indicating higher uptake. The uptake is apparent predominantly in the right liver lobe; the label is distributed heterogeneously within the liver parenchyma in the form of small multifocal areas of concentrated radioactivity (“hot spots”). One of these hot spots, located laterally in the right lobe of the liver, is at the intersection of the red lines in all three images. R denotes right, L left, A anterior, and P posterior. Panel B shows the radioactivity uptake in the liver, expressed as the percentage of the total administered radioactivity plotted over time (0 to 60 minutes from the start of islet transplantation), extracted from the dynamic PET–CT data. The highest liver uptake is seen at approximately 20 minutes, just after completion of islet transplantation. At this time, only half of the administered radioactivity is found in the liver. All calculations have been corrected for the natural decay of fluorine-18. Panel C shows plasma C-peptide levels before, during, and up to 7 days after the transplantation. A sharp rise in C-peptide levels, indicating islet destruction, was observed just after islet infusion and lasted at least 210 minutes. From day 3 onward, an increased level of C peptide was found. The corresponding plasma glucose levels on day 1 to day 7 were in the range of 150 to 173 mg per deciliter. This level of C-peptide secretion has remained for more than 3 months after transplantation.\n\n【4】Islet-cell transplantation is accomplished by embolizing the islets by means of the portal vein into the liver.  Because available tools to study transplanted islets have been limited, the liver has been considered a “black box” in clinical islet transplantation. Here we demonstrate the use of scanning with positron-emission tomography (PET) combined with computed tomography (CT) to visualize the peritransplantation phase of islet transplantation in a single subject. The recipient is a 46-year-old man with a 39-year history of type 1 diabetes. Islets (430,000 islet equivalents) were isolated from the pancreas of a multiorgan donor, and 100,000 islet equivalents were allowed to internalize  F-fluorodeoxyglucose (  F-FDG) (the half-life of fluorine-18 is 110 minutes) immediately before transplantation. In vitro autoradiography indicated that 96.7% of the radioactivity was confined to the islets. Labeled and unlabeled islets, carefully mixed, were infused into the portal vein. Labeled islets were readily visualized, demonstrating a heterogeneous distribution in the liver. They were concentrated in multifocal “hot spots” in the right lobe , with no evident shunting to the lungs. Only 53% of the administered radioactivity was detected in the liver, suggesting that half the transplanted islet cells were damaged to the extent that the  F-FDG they contained was released during the first few minutes after transplantation . The residual radioactivity was evenly distributed throughout the body. A parallel, marked increase in plasma C peptide  was observed, also indicating islet destruction. \n\n【5】After 2 weeks, the patient's plasma C-peptide level after an overnight fast was 1.18 ng per milliliter (concomitant plasma glucose level, 85 mg per deciliter), and his meal-stimulated plasma C-peptide level was 1.63 ng per milliliter (plasma glucose level, 156 mg per deciliter), even with insulin treatment to optimize islet engraftment. The satisfactory outcome of the transplantation, despite the marked loss of islets immediately after transplantation, indicates that the islets were of good quality and suggests that the substantial loss of islets described here is a common phenomenon in clinical islet transplantation.  Since the submission of this letter, another patient has undergone transplantation with the same procedure and with similar results as in this case.\n\n【6】The main advantages of the PET–CT technology as shown here are that it is readily available in most centers performing islet transplantation and that it allows real-time measurement of islet survival and distribution after transplantation. Thus, this technology seems to be promising as a means for assessing various strategies to improve early islet engraftment after intraportal transplantation and for evaluating alternative sites of implantation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0e3e687a-5a7a-49cf-bef7-97d6a5bf2aec", "title": "Variations in the Use of Medical and Surgical Services by the Medicare Population", "text": "【0】Variations in the Use of Medical and Surgical Services by the Medicare Population\nAbstract\n\n【1】We measured geographic differences in the use of medical and surgical services during 1981 by Medicare beneficiaries (age ≥65) in 13 large areas of the United States. The average number of Medicare beneficiaries per site was 340,000. We found large and significant differences in the use of services provided by all medical and surgical specialties. Of 123 procedures studied, 67 showed at least threefold differences between sites with the highest and lowest rates of use. Use rates were not consistently high in one site, but rates for procedures used to diagnose and treat a specific disease varied together, as did alternative treatments for the same condition.\n\n【2】These results cannot be explained by the actions of a small number of physicians. We do not know whether physicians in high-use areas performed too many procedures, whether physicians in low-use areas performed too few, or whether neither or both of these explanations are accurate. However, we do know that the differences are too large to ignore and that unless they are understood at a clinical level, uninformed policy decisions that have adverse effects on the health of the elderly may be made.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8ced3451-09dd-40e5-a65b-120b28d09f25", "title": "Liver Transplantation to Provide Low-Density-Lipoprotein Receptors and Lower Plasma Cholesterol in a Child with Homozygous Familial Hypercholesterolemia", "text": "【0】Liver Transplantation to Provide Low-Density-Lipoprotein Receptors and Lower Plasma Cholesterol in a Child with Homozygous Familial Hypercholesterolemia\nAbstract\n--------\n\n【1】A six-year-old girl with severe hypercholesterolemia and atherosclerosis had two defective genes at the low-density-lipoprotein (LDL) receptor locus, as determined by biochemical studies of cultured fibroblasts. One gene, inherited from the mother, produced no LDL receptors; the other gene, inherited from the father, produced a receptor precursor that was not transported to the cell surface and was unable to bind LDL. The patient degraded intravenously administered  I-LDL at an extremely low rate, indicating that her high plasma LDL-cholesterol level was caused by defective receptor-mediated removal of LDL from plasma. After transplantation of a liver and a heart from a normal donor, the patient's plasma LDL-cholesterol level declined by 81 per cent, from 988 to 184 mg per deciliter. The fractional catabolic rate for intravenously administered  I-LDL, a measure of functional LDL receptors in vivo, increased by 2.5-fold. Thus, the transplanted liver, with its normal complement of LDL receptors, was able to remove LDL cholesterol from plasma at a nearly normal rate.\n\n【2】We conclude that a genetically determined deficiency of LDL receptors can be largely reversed by liver transplantation. These data underscore the importance of hepatic LDL receptors in controlling the plasma level of LDL cholesterol in human beings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "21c684ef-9673-4e80-8ba7-3030702c4891", "title": "Case 5-2017 — A 19-Year-Old Man with Hematuria and a Retroperitoneal Mass", "text": "【0】Case 5-2017 — A 19-Year-Old Man with Hematuria and a Retroperitoneal Mass\nA 19-year-old man presented with a 2-day history of hematuria, without pain, after hiking. Imaging revealed a well-circumscribed 4.6-cm retroperitoneal mass. A procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "4543ee65-17a8-4c17-8c75-f75bf8aef50a", "title": "Pleural Effusion", "text": "【0】Pleural Effusion\nA 70-year-old man with an 80-pack-year history of smoking and a history of congestive heart failure presents with increasing shortness of breath. He also has aching chest pain on the right side that worsens with deep inspiration. He is afebrile. The chest radiograph reveals asymmetrical bilateral pleural effusions, with more fluid on the right. How should this patient be evaluated?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a0e6eebf-2446-4225-9619-dabe98af906b", "title": "Evaluation of Chloramphenicol Acid Succinate Therapy of Induced Typhoid Fever and Rocky Mountain Spotted Fever", "text": "【0】Evaluation of Chloramphenicol Acid Succinate Therapy of Induced Typhoid Fever and Rocky Mountain Spotted Fever\nAbstract\n--------\n\n【1】In volunteers with induced typhoid fever, plasma levels of free biologically active chloramphenicol were approximately twice as high in those treated with oral chloramphenicol (four subjects) as in those treated with the same dose of intramuscular chloramphenicol succinate (four subjects), apparently owing to the failure of the intramuscularly administered succinate ester to be completely hydrolyzed to the active drug. Plasma chloramphenicol levels in nine volunteers infected with Rocky Mountain spotted fever and treated with chloramphenicol succinate intramuscularly revealed that approximately a third of the administered drug was present in the biologically inactive unhydrolyzed form. Although chloramphenicol succinate was effective in controlling the early toxicity of induced typhoid fever and Rocky Mountain spotted fever, the delayed response and high relapse rate made this preparation inadequate as the sole form of therapy given intramuscularly in conventional doses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5a75b30b-e5c5-4631-b9c0-1929cc2aef9b", "title": "Distribution of ", "text": "【0】Distribution of \nTo the Editor:\n--------------\n\n【1】Hong et al. (Sept. 24 issue)  present results of an early-phase clinical trial of sotorasib, which showed promising clinical benefit. Patients had non–small-cell lung cancer (NSCLC; 46%) or colorectal cancer (33%), and White patients constituted 76% of the cohort. As drugs are being developed for the previously “undruggable” _KRAS_ <sup>G12C </sup> mutation, it is imperative to study the distribution of this mutation across sex, race,  and all cancers.\n\n【2】We extracted data from the registry of the American Association for Cancer Research Project Genomics Evidence Neoplasia Information Exchange (GENIE), version 8.0  . We studied the distribution of _KRAS_ <sup>G12C </sup> mutations in 32,138 patients with cancer across race (Asian, Black, and White) and sex and in 10 cancer types . A total of 2045 patients (6.4%) were Asian, 2355 (7.3%) were Black, and 27,738 (86.3%) were White. _KRAS_ <sup>G12C </sup> mutations were compared according to race and sex, and P values were corrected by the Benjamini–Hochberg method to determine false discovery rate–corrected Q values, which were considered significant when Q was less than 0.05.\n\n【3】Figure 1. _KRAS_ <sup>G12C </sup> Mutation Landscape of 32,138 Patients.\n\n【4】Panel A shows the distribution of _KRAS_ <sup>G12C </sup> mutations in 10 cancer types. Data were obtained from the American Association for Cancer Research Project GENIE (Genomics Evidence Neoplasia Information Exchange), version 8.0. The frequency of _KRAS_ <sup>G12C </sup> mutations are shown by subgroups defined by race and sex among patients with non–small-cell lung cancer (NSCLC; Panel B) or colorectal cancer . Values shown within the bars refer to the number of patients with _KRAS_ <sup>G12C </sup> mutations; values above the bars refer to the total number of patients for each cohort. Fisher’s exact test was used to calculate the odds ratios and 95% confidence intervals. A two-sided binominal test was used to establish P values, which were corrected by the Benjamini–Hochberg method to determine Q values.\n\n【5】_KRAS_ <sup>G12C </sup> mutations were identified in 1867 samples, most frequently in patients with NSCLC (1443 of 10,444 \\[13.8%\\]). _KRAS_ <sup>G12C </sup> mutations were identified in 3.5% of patients (74 of 2105) with cancer of unknown primary site, 3.3% of patients (12 of 368) with appendiceal cancer, 3.2% of patients (234 of 7402) with colorectal cancer, and 3.1% of patients (7 of 223) with small-bowel cancer . Among patients with colorectal cancer and those with NSCLC, female patients harbored significantly more _KRAS_ <sup>G12C </sup> mutations than male patients .\n\n【6】Among patients with NSCLC, White and Black patient groups were enriched for _KRAS_ <sup>G12C </sup> mutations more than Asians (White patients, 1153 of 8892 \\[13%\\]; Black patients, 94 of 862 \\[10.9%\\]; and Asian patients, 25 of 690 \\[3.6%\\]) (P<0.001). It is striking that there were differences by sex within the same ethnic groups of patients with NSCLC. _KRAS_ <sup>G12C </sup> mutations occurred more often in White female patients than in White male patients with NSCLC (odds ratio, 1.4; 95% confidence interval \\[CI\\], 1.3 to 1.6; Q<0.001) and more often in Asian male patients than in Asian female patients (odds ratio, 5.2; 95% CI, 1.9 to 17.9; Q=0.01) . Among patients with colorectal cancer, White female patients were enriched for _KRAS_ <sup>G12C </sup> mutations more than White male patients (odds ratio, 1.4; 95% CI, 1.1 to 1.9; Q=0.04) .\n\n【7】Overall, we found that _KRAS_ <sup>G12C </sup> somatic mutations are common in NSCLC, colorectal cancer, appendiceal and small bowel cancers, and cancers of unknown primary site, with a mutation frequency of 3 to 14%. With the promising efficacy of sotorasib,  recruiting patients with all these cancer types in future clinical trials will be important.\n\n【8】These data illustrate the presence of differences according to sex and ethnic group in the prevalence of _KRAS_ <sup>G12C </sup> mutations in various cancer types and emphasize the importance of mutation analysis of larger numbers of non-White populations and in rare cancers. Such efforts will address racial disparities, aid in future clinical trial design, and ideally lead to better insight into the reasons for these differences.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6f8d728b-3837-4767-b7dd-25cfc6e0a9b1", "title": "Myasthenic Antibodies Cross-Link Acetylcholine Receptors to Accelerate Degradation", "text": "【0】Myasthenic Antibodies Cross-Link Acetylcholine Receptors to Accelerate Degradation\nAbstract\n--------\n\n【1】The decrease of acetylcholine receptors at neuromuscular junctions of myasthenic patients has been attributed to an antibody-mediated autoimmune process that accelerates receptor degradation. We studied the mechanism of this process in skeletal-muscle cultures, using intact antibodies and antibody fragments. Addition of myasthenic IgG or its divalent fragment, F(ab′) <sub>2 </sub> , to cultures accelerated the rate of acetylcholine-receptor degradation threefold. By contrast, the monovalent fragment, Fab, from myasthenic serum had no effect on degradation, although it bound to acetylcholine receptors. Addition of a second, \"piggyback\" antibody to cross-link the Fab:receptor complexes resulted in a threefold increase of the degradation rate. Similarly, when acetylcholine receptors with bound α-bungarotoxin were cross-linked by the addition of specific antibody against α-bungarotoxin, the degradation rate increased approximately threefold. The effect of myasthenic patients' antibodies in accelerating degradation of acetylcholine receptors is attributed to their ability to cross-link the receptors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "852f6fbb-c325-4636-af5e-ff619786e7ea", "title": "The Death Throes of Mercy — Our Shared Responsibility When Hospitals Close", "text": "【0】The Death Throes of Mercy — Our Shared Responsibility When Hospitals Close\nArticle\n-------\n\n【1】Hospital closures disrupt communities. They also demand that we clarify our goals: Do we come together to support endangered hospitals, or do we support the patients and communities relying on them? Sometimes the second requires the first, but not always.\n\n【2】For 245 years, the Philadelphia General Hospital served the city as a public institution. It closed in 1977, when the city could no longer afford escalating operating costs and capital expenses. The creation of Medicare and Medicaid also made it easier for private hospitals to care for the poor and the elderly. Together, private hospitals assumed responsibility for vulnerable patients and crafted a new safety net.\n\n【3】Many of those hospitals have since closed. Last year, the bankruptcy of Hahnemann University Hospital drew national attention and was portrayed by some observers as a tale of corporate greed endangering the health of vulnerable patients.  The closure complicated access to care for Hahnemann’s patients, nearly all of whom had public insurance or were uninsured. More than 2500 people lost their jobs, including 575 physicians-in-training.\n\n【4】Less than a year later, another Philadelphia institution, Mercy Philadelphia Hospital, has announced that it will close. Although Mercy has made noble efforts to remain open, including expanding its emergency department (ED), the hospital is in its death throes.\n\n【5】Hospitals are failing in communities throughout the United States. The reasons for these failures are complex but can be glimpsed in the Philadelphia market, which has previously maintained a high number of hospital beds per capita. Health care has increasingly shifted to outpatient settings, reducing demand for the inpatient care that supports higher financial margins than ambulatory care. For safety-net hospitals serving communities that rely on Medicaid, even margins for inpatient care are often negative. A loss for each patient cannot be made up by increasing volume. Distressed hospitals may provide lower-quality care with limited innovation, further reducing demand. Rural hospitals experience even greater challenges, since they need to maintain essential access to services without the volume — or clinicians — to make ends meet. \n\n【6】Calls for government action have followed each new hospital closure. Yet it is uncertain whether investments to keep hospitals open can be sustained. Market forces may make closure necessary and ultimately inescapable. However, government intervention, whether through funding or regulation, is needed to facilitate soft landings for patients after closures.\n\n【7】To ensure that patients are not forgotten, communities as well as private health care institutions often step in to help them. When Hahnemann shut down, nearby hospitals came to the rescue, absorbing patients requiring emergency and inpatient care and saving some jobs. But the costs of caring for these displaced patients can be high, and public reimbursement is low. Hospitals that have managed to stay viable also need to maintain their strength to prevent future losses to their communities.\n\n【8】Our remaining hospitals are indispensable, as the Covid-19 pandemic has shown. The current crisis has exposed declining hospital capacity in the United States. Hospitals that have weathered years of diminishing volume had to suddenly prepare for an overwhelming public health emergency. Critical care beds, in particular, are in short supply. In the coming weeks, hospitals must overcome sharp declines in revenue due to overall reduced patient volume and suspension of services, including elective surgeries. Even more hospitals are likely to close. After the pandemic, the United States will be challenged to bear the cost of maintaining sufficient hospital capacity to be prepared for future public health emergencies.\n\n【9】Effective responses may require us to think beyond traditional hospitals. For all the disruption they cause, hospital closures represent opportunities to develop new and robust ways to support patients. All strategies should be informed and driven by community needs, but a few strategies are evident in Philadelphia and could be applied more broadly.\n\n【10】One key strategy is to strengthen the outpatient safety net. A network of Federally Qualified Health Centers (FQHCs) and city-operated health centers accounts for the bulk of Philadelphia’s safety net. These facilities provide essential primary and preventive care and support services. It is imperative that they have adequate resources to care for patients and to address population health. In addition, many services, including surgeries and diagnostic procedures, have shifted from inpatient to outpatient settings. Timely access to providers performing those services is essential, as is tight coordination between primary care and specialists. All patients, but particularly those with complex medical needs, will benefit from expanded options for specialty care, which may in turn require changes in reimbursement structures or development of new care delivery models.\n\n【11】It is also essential to develop and provide alternatives for acute care. Mercy Philadelphia has provided 48,000 ED visits each year. Many alternative options can serve patients who previously sought emergency care. Some patients could receive treatment at an urgent care center or an FQHC based in the former hospital. A free-standing ED might also treat patients with low-acuity conditions while transferring patients who require admission to nearby hospitals. Currently, Pennsylvania law does not permit free-standing EDs, although such facilities have expanded access to acute care in many other states. Finally, telemedicine services are likely to gain further traction given their widespread use during the Covid-19 pandemic. Health systems, clinicians, and patients may find increasing comfort and utility in telemedicine both for triaging acute problems and for managing chronic illness.\n\n【12】Behavioral health services should also be expanded. Until this year, Mercy Philadelphia operated a crisis center for mental health emergencies, including substance use disorder. Regardless of whether they have excess acute care hospital beds, few cities have sufficient psychiatric inpatient capacity or adequate community-based services to prevent hospitalization. As many as half of medical inpatients have behavioral health diagnoses, and identification and early attention to these needs could improve clinical outcomes and reduce costs. \n\n【13】On the financial front, Medicaid subsidies should be allocated in proportion to where patients receive care. Philadelphia hospitals rely on a complex system of state and matching federal subsidies for their financial health, including supplemental funding allocated by Medicaid.  Subsidies for “disproportionate share hospitals” are threatened even as closures reveal their value. After hospitals close, these funds need to remain in communities affected by closures. Furthermore, city, state, and federal matching funds should be fairly and transparently distributed to the hospitals and community organizations that assume the care of displaced patients, rather than being diverted for other needs.\n\n【14】A longer-term, preventive strategy is to address the social determinants of health in affected communities. Hospitals are not, in and of themselves, solutions for housing and food insecurity, violence and injury prevention, prejudice and residential segregation, or education and health literacy, among a host of health-related social needs. But hospitals and health systems can be advocates and help to coordinate services that address these structural problems.\n\n【15】Finally, new strategies are required to prepare for public health emergencies. The Covid-19 pandemic has demanded far greater acute care capabilities than many U.S. communities have in place.  Surging rates of hospitalizations have compelled hospitals to convert waiting rooms into critical care wards, cities to build hospitals in convention centers, and the hospital ship _USNS Comfort_ to dock in New York Harbor. The crisis precipitated a sudden and dire need for hospital beds, but when the pandemic abates, it will not be possible to sustain this expanded capacity in order to be prepared for the next crisis — particularly if the pandemic accelerates hospital closures. A coordinated and flexible system may prove to be a better solution for mitigating surges during emergencies. Such a system would require collaboration among government, health systems, and industry to rapidly deploy hospital beds, ramp up production of necessary medical equipment, and rapidly mobilize an expanded health care workforce.\n\n【16】The closure of Mercy Philadelphia, like the closure of Hahnemann, will leave its mark on patients, employees, and the city. To alleviate these harms, we should ensure that remaining hospitals maintain their financial health so that they can share responsibility for treating displaced patients. At the same time, these closures offer an opportunity to reconsider the services that our city, and communities throughout the country, need most.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ae18f070-f442-45b4-9003-b88ee3561078", "title": "PML in a Patient with Lymphocytopenia Treated with Dimethyl Fumarate", "text": "【0】PML in a Patient with Lymphocytopenia Treated with Dimethyl Fumarate\nTo the Editor:\n--------------\n\n【1】We report the case of a 54-year-old woman with multiple sclerosis who was treated with delayed-release dimethyl fumarate (DMF; Tecfidera, Biogen Idec) and who died on October 13, 2014, from complications related to aspiration pneumonia and progressive multifocal leukoencephalopathy (PML) with severe, prolonged lymphocytopenia.\n\n【2】The patient, who had received the diagnosis of multiple sclerosis in 1996 and had been treated with glatiramer acetate, was randomly assigned to the placebo group in the 2-year Determination of the Efficacy and Safety of Oral Fumarate in Relapsing–Remitting Multiple Sclerosis (DEFINE) trial.  She subsequently received delayed-release DMF (at a dose of 240 mg three times daily) for 4.5 years in the open-label extension study. Twelve months after the initiation of delayed-release DMF, severe lymphocytopenia (lymphocyte count, 290 to 580 cells per cubic millimeter) developed and persisted for 3.5 years.\n\n【3】Although the patient had no clinical disease activity since the first month of active treatment, she presented with new neurologic signs and symptoms consistent with a relapse in multiple sclerosis (severe gait disorder, speech disorder, and difficulties in left arm coordination) on August 11, 2014. Her condition did not improve with the administration of intravenous methylprednisolone (1 g once daily on August 11 to 13, 2 g once daily on August 20 to 22, and 2 g once daily on September 20 to 22) and plasmapheresis for the suspected relapse (5 courses, 1 per day; September 26 to 30). Delayed-release DMF treatment was discontinued on August 23. PML was diagnosed on the basis of results on magnetic resonance imaging  and positive results on polymerase-chain-reaction assay for JC virus DNA in cerebrospinal fluid obtained by means of lumbar puncture on October 7. The patient had received no previous immunosuppressant drugs or natalizumab.\n\n【4】Since delayed-release DMF was approved in the United States in March 2013, more than 135,000 patients with multiple sclerosis have been treated, representing approximately 112,000 person-years of exposure as of December 31, 2014. As of September 26, 2014, a total of 3112 patients with multiple sclerosis had received delayed-release DMF in clinical trials, representing approximately 7429 person-years of exposure. To date, there has been no overall increase in the risk of serious infection, including other opportunistic infections. However, delayed-release DMF can cause lymphocytopenia,  a known risk factor for PML.  Thus, a contributory role of lymphocytopenia in this patient cannot be ruled out.\n\n【5】Figure 1. Lymphocyte Counts in Patients with Multiple Sclerosis Receiving Delayed-Release Dimethyl Fumarate (DMF).\n\n【6】Shown are lymphocyte counts in studies involving patients with multiple sclerosis during treatment with delayed-release DMF. In the open-label extension of phase 3 trials, as of May 10 2014, the incidence of lymphocytopenia (lymphocyte count, <500 per cubic millimeter) was 1.9% (47 of 2470 patients) for 6 months or longer, 0.6% (15 of 2470 patients) for 2 years or longer, and 0.3% (8 of 2470 patients) for 3 years or longer. Panel A shows mean lymphocyte counts for 47 patients in whom consecutive post-baseline lymphocyte counts were less than 500 cells per cubic millimeter for at least 6 months, as compared with 2466 patients without lymphocytopenia. All the patients received delayed-release DMF at a dose of 240 mg either two or three times daily. Counts were checked every 3 months in clinical trials, but values are shown for 6-month intervals for readability. The dashed line indicates the lower limit of the normal range (LLN). The I bars indicate standard errors. Panel B shows absolute lymphocyte counts over time in the same 47 DMF-treated patients, according to the daily dose and whether the patient was still receiving the drug. In the two dose groups (240 mg administered either two or three times daily), similar proportions had a reduction in the lymphocyte count to below 800 cells per cubic millimeter or to below 500 cells per cubic millimeter. Post-dosing lymphocyte counts (available for 9 patients) generally increased in the 4 weeks after the discontinuation of treatment.\n\n【7】In controlled and uncontrolled studies involving patients with multiple sclerosis, there was a decrease in the mean lymphocyte count of approximately 30% during the first year of treatment with delayed-release DMF. The mean lymphocyte count then plateaued and remained above the lower limit of the normal range . Approximately 2% of patients had reduced lymphocyte counts (<500 cells per cubic millimeter) that persisted for more than 6 months.  In this subgroup of patients, which was identifiable within the first year of treatment, lymphocyte counts of less than 500 per cubic millimeter were generally maintained with continued therapy .  Periodic monitoring of absolute lymphocyte counts to identify patients at increased risk for severe, prolonged lymphocytopenia and consideration of treatment interruption in these patients may mitigate the risk of PML. Studies to evaluate the effect of delayed-release DMF on lymphocyte subgroups are ongoing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "6291423c-0ac7-460f-b988-d89f5d8b2f81", "title": "Audio Interview: A New Round of Rising Covid-19 Numbers", "text": "【0】Audio Interview: A New Round of Rising Covid-19 Numbers\nArticle\n-------\n\n【1】### Audio Interview\n\n【2】 A New Round of Rising Covid-19 Numbers \n\n【3】The continuing spread of SARS-CoV-2 remains a Public Health Emergency of International Concern. What physicians need to know about transmission, diagnosis, and treatment of Covid-19 is the subject of ongoing updates from infectious disease experts at the _Journal_ .\n\n【4】In this audio interview conducted on August 31, 2021, the editors discuss the rising number of U.S. Covid-19 cases that has followed the loosening of restrictions and new data on the effectiveness of vaccination in controlling the increase.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1042f4ff-086e-40f6-84a7-3736fd511083", "title": "Further Evidence Relating Mitral-Valve Prolapse to Cerebral Ischemic Events", "text": "【0】Further Evidence Relating Mitral-Valve Prolapse to Cerebral Ischemic Events\nAbstract\n--------\n\n【1】Echocardiography demonstrates prolapse of the mitral valve in at least 5 per cent of the population. Since some observations have linked this condition to stroke, we studied its incidence in two groups of patients with cerebral ischemia. The older group contained 141 patients over 45 years of age (mean, 64.7 years) who had transient ischemia or partial stroke. Prolapse was found in eight (5.7 per cent) of these patients and in 10 (7.1 per cent) of 141 age-matched controls. The second group contained 60 patients who had transient ischemia or partial stroke and were under 45 years old (mean 33.9 years). Prolapse was detected in 24 patients (40 per cent) but in only five (6.8 per cent) of 60 age-matched controls (mean age, 33.7 years). The odds ratio, 9.33, was highly significant (P<0.001). In six of the 24 patients there were other potential causes for cerebral ischemia, leaving 18 in whom the only recognizable potential cause was a prolapsing mitral valve (odds ratio, 7.00; P<0.001). This study suggests that this entity has a role in cerebral ischemia, at least in younger patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "1d7d354a-f680-4066-8e45-b44db71963eb", "title": "Efficacy of Tocilizumab in Patients Hospitalized with Covid-19", "text": "【0】Efficacy of Tocilizumab in Patients Hospitalized with Covid-19\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】The efficacy of interleukin-6 receptor blockade in hospitalized patients with coronavirus disease 2019 (Covid-19) who are not receiving mechanical ventilation is unclear.\n\n【3】Methods\n-------\n\n【4】We performed a randomized, double-blind, placebo-controlled trial involving patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection, hyperinflammatory states, and at least two of the following signs: fever (body temperature >38°C), pulmonary infiltrates, or the need for supplemental oxygen in order to maintain an oxygen saturation greater than 92%. Patients were randomly assigned in a  ratio to receive standard care plus a single dose of either tocilizumab (8 mg per kilogram of body weight) or placebo. The primary outcome was intubation or death, assessed in a time-to-event analysis. The secondary efficacy outcomes were clinical worsening and discontinuation of supplemental oxygen among patients who had been receiving it at baseline, both assessed in time-to-event analyses.\n\n【5】Results\n-------\n\n【6】We enrolled 243 patients; 141 (58%) were men, and 102 (42%) were women. The median age was 59.8 years (range, 21.7 to 85.4), and 45% of the patients were Hispanic or Latino. The hazard ratio for intubation or death in the tocilizumab group as compared with the placebo group was 0.83 (95% confidence interval \\[CI\\], 0.38 to 1.81; P=0.64), and the hazard ratio for disease worsening was 1.11 (95% CI, 0.59 to 2.10; P=0.73). At 14 days, 18.0% of the patients in the tocilizumab group and 14.9% of the patients in the placebo group had had worsening of disease. The median time to discontinuation of supplemental oxygen was 5.0 days (95% CI, 3.8 to 7.6) in the tocilizumab group and 4.9 days (95% CI, 3.8 to 7.8) in the placebo group (P=0.69). At 14 days, 24.6% of the patients in the tocilizumab group and 21.2% of the patients in the placebo group were still receiving supplemental oxygen. Patients who received tocilizumab had fewer serious infections than patients who received placebo.\n\n【7】Conclusions\n-----------\n\n【8】Tocilizumab was not effective for preventing intubation or death in moderately ill hospitalized patients with Covid-19. Some benefit or harm cannot be ruled out, however, because the confidence intervals for efficacy comparisons were wide. \n\n【9】Introduction\n------------\n\n【10】 VISUAL ABSTRACT  \nTocilizumab in Patients Hospitalized with Covid-19\n\n【11】Infections with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes coronavirus disease 2019 (Covid-19), now number more than 7 million in the United States.  At the peak of the pandemic to date, more than 1000 Americans died from Covid-19 each day, and more than 214,000 had died as of October 13, 2020. After an incubation period, the acute viral phase in patients with symptomatic Covid-19 usually manifests as influenza-like symptoms. In some persons, the illness progresses to hypoxemic respiratory failure.  Evidence suggests that the pathophysiological basis of this profound decline is a severe inflammatory response resembling cytokine release syndrome.  In this phase, patients have markedly abnormal inflammatory markers, including elevated serum interleukin-6, ferritin, and C-reactive protein levels.  Higher concentrations of interleukin-6 in serum are associated with higher levels of SARS-CoV-2 viremia,  prolonged viral RNA shedding,  progression to mechanical ventilation,  and death.  These findings led us to hypothesize that interleukin-6 receptor blockade might interrupt this inflammatory cascade at a crucial stage.\n\n【12】Evidence from nonrandomized trials and open-label studies has been contradictory,  and published results from randomized, double-blind, placebo-controlled trials have been lacking. We performed the investigator-initiated Boston Area COVID-19 Consortium (BACC) Bay Tocilizumab Trial, a randomized, double-blind, placebo-controlled trial of tocilizumab administered relatively early in the disease course, with the aim of preventing progression of Covid-19. We hypothesized that early intervention with interleukin-6 receptor blockade might limit progression to hypoxemic respiratory failure or death, reduce the risk of clinical worsening, and decrease the duration of supplemental oxygen use.\n\n【13】Methods\n-------\n\n【14】Trial Design\n------------\n\n【15】We conducted the trial at seven Boston hospitals. The trial was approved by the Mass General Brigham institutional review board and was conducted in accordance with Good Clinical Practice guidelines and the Declaration of Helsinki. All patients provided written informed consent in keeping with institutional guidelines. The investigators designed the trial, collected the data, and performed the analysis. Genentech funded the trial and provided tocilizumab but had no role in data analysis, data interpretation, or writing of the manuscript. The trial was overseen by a data and safety monitoring board. All the authors participated in writing the manuscript that was submitted. No one who is not an author contributed to writing the manuscript.\n\n【16】Patients\n--------\n\n【17】Patients were eligible for enrollment if they were 19 to 85 years of age and had SARS-CoV-2 infection confirmed by either nasopharyngeal swab polymerase chain reaction or serum IgM antibody assay. Patients had to have at least two of the following signs: fever (body temperature >38°C) within 72 hours before enrollment, pulmonary infiltrates, or a need for supplemental oxygen in order to maintain an oxygen saturation higher than 92%. At least one of the following laboratory criteria also had to be fulfilled: a C-reactive protein level higher than 50 mg per liter, a ferritin level higher than 500 ng per milliliter, a d \\-dimer level higher than 1000 ng per milliliter, or a lactate dehydrogenase level higher than 250 U per liter. Patients were excluded if they were receiving supplemental oxygen at a rate that exceeded 10 liters per minute, if they had a recent history of treatment with biologic agents or small-molecule immunosuppressive therapy, if they were receiving other immunosuppressive therapy that the investigator believed placed them at higher risk for an infection, or if they had had diverticulitis. The full list of entry criteria is provided in the protocol.\n\n【18】Randomization and Treatment\n---------------------------\n\n【19】Patients were randomly assigned in a  ratio to receive standard care plus a single dose of either tocilizumab (8 mg per kilogram of body weight administered intravenously, not to exceed 800 mg) or placebo; randomization was performed with randomly permuted blocks of sizes 3 and 6. Randomization was stratified according to site. Administration of tocilizumab or placebo was generally complete within 3 hours after informed consent was obtained.\n\n【20】Concomitant Treatment\n---------------------\n\n【21】The results of the Adaptive Covid-19 Treatment Trial (ACTT-1) of remdesivir  became known during this trial, but the results of the Randomized Evaluation of Covid-19 Therapy (RECOVERY) trial  regarding the efficacy of dexamethasone were announced afterward. Therefore, some patients received remdesivir as concomitant treatment, whereas no patients received dexamethasone. Antiviral therapy, hydroxychloroquine, and glucocorticoids were permitted as concomitant treatment.\n\n【22】Outcomes\n--------\n\n【23】The primary outcome was intubation (or death, for patients who died before intubation) after administration of tocilizumab or placebo, assessed in a time-to-event analysis. The first secondary outcome was clinical worsening, defined on the basis of an ordinal clinical improvement scale. Scores on the scale were defined as follows: 1, discharged or ready for discharge; 2, in (or ready for) a non-ICU hospital ward and not receiving supplemental oxygen; 3, in (or ready for) a non-ICU hospital ward and receiving supplemental oxygen; 4, in the ICU or a non-ICU hospital ward and receiving noninvasive ventilation or high-flow oxygen; 5, in the ICU, intubated, and receiving mechanical ventilation; 6, in the ICU and receiving extracorporeal membrane oxygenation or mechanical ventilation and additional organ support; and 7, death . Worsening was defined as an increase by at least 1 point among patients who had been receiving supplemental oxygen at baseline or at least 2 points among those who had not been receiving supplemental oxygen at baseline. The second secondary outcome was discontinuation of supplemental oxygen among patients who had been receiving it at baseline. Both secondary outcomes were assessed in time-to-event analyses. Data from patients who were event-free at the end of follow-up were censored at 28 days (for the primary and first secondary outcome) or at 29 days (for the second secondary outcome). Data from patients who could not be reached for 28-day follow-up were censored at hospital discharge.\n\n【24】Tertiary outcomes were those related to other time-to-event analyses (e.g., improvement, discharge, or death), analyses of duration (supplemental oxygen use, receipt of mechanical ventilation), and binary outcomes (admission to the intensive care unit \\[ICU\\] or death). Exploratory analyses addressed the relationships among inflammatory biomarker levels, demographic and clinical features, and efficacy outcomes.\n\n【25】Statistical Analysis\n--------------------\n\n【26】We assumed that the risk of invasive mechanical ventilation or death within 28 days would be 30% in the placebo group  and that the risk with tocilizumab would be reduced to 15%. With a total of 243 patients, we had 80% power to detect such a difference with the use of a log-rank test, assuming two-sided tests and a significance level of 0.05. A blinded interim analysis for safety was performed.\n\n【27】 The primary efficacy analysis focused on the modified intention-to-treat population, defined as all patients who underwent randomization and received either tocilizumab or placebo before intubation or death. The safety population included all patients who underwent randomization and received either tocilizumab or placebo. For primary and secondary outcomes, the treatment groups were compared with the use of log-rank tests stratified according to site. The three smallest sites were combined. The differences between the treatment groups were estimated as hazard ratios with 95% confidence intervals from stratified Cox proportional hazards models. Estimates of the percentages of patients who had events at specific time points were based on the corresponding Kaplan–Meier curves. The proportional hazards assumption was confirmed through correlation tests between the weighted Schoenfeld residuals and event times.\n\n【28】A single primary analysis was performed with the criterion for statistical significance defined as a two-sided P value of less than 0.05. Testing of the two secondary efficacy outcomes was performed with a Bonferroni–Holm correction to ensure an overall two-sided significance level of less than 0.05. No corrections for multiplicity were used for tertiary and exploratory analyses. Confidence intervals were not adjusted for multiple comparisons and cannot be used to infer effects. We examined whether the treatment effect on the primary outcome varied within subgroups defined on the basis of demographic or clinical risk factors for poor Covid-19 outcomes. Treatment-effect modification was assessed in separate interaction models for each risk factor. Each model was stratified according to site and included age (>65 or ≤65 years), sex, Hispanic or Latino ethnic group, and race (White or non-White) as covariates. We also evaluated the association between these risk factors and the primary outcome in separate main effects models with stratification and covariate adjustment.\n\n【29】Results\n-------\n\n【30】Patients\n--------\n\n【31】Figure 1. Screening, Randomization, and Outcomes.\n\n【32】The patient who was intubated before receiving placebo was excluded from the modified intention-to-treat population but included in the safety population.Table 1.  Table 1. Baseline Characteristics of the Patients.\n\n【33】Between April 20 and June 15, 2020, a total of 243 patients were enrolled; 161 received tocilizumab, and 81 received placebo . One patient, who had been intubated before receiving placebo, was excluded from the modified intention-to-treat population but was included in the safety population. The baseline characteristics of the patients are shown in Table 1 . In the modified intention-to-treat population, 141 patients (58%) were male and 102 patients (42%) were female. The median age was 59.8 years (interquartile range, 45.3 to 69.4). The youngest patient was 21.7 years of age, and the oldest was 85.4 years of age. In the overall trial population, 45% of patients were Hispanic or Latino, 16% were Black, and 43% were White.\n\n【34】The body-mass index (the weight in kilograms divided by the square of the height in meters) was at least 30 in 51% of the patients at baseline, 49% of the patients had hypertension, and 31% had known diabetes mellitus. A total of 194 patients (80%) were hospitalized in non-ICU hospital wards and were receiving supplemental oxygen (≤6 liters per minute), delivered by nasal cannula, to maintain an oxygen saturation greater than 92%; 10 (4%) were receiving high-flow oxygen (>6 and ≤10 liters per minute delivered by any device); and 38 (16%) were not receiving supplemental oxygen at baseline. The median concentration of C-reactive protein was 110.0 mg per liter (interquartile range, 64.9 to 175.3); ferritin, 708 ng per milliliter (interquartile range, 411 to 1225); d \\-dimer, 884 ng per milliliter (interquartile range, 527 to 1730); and lactate dehydrogenase, 340 U per liter (interquartile range, 289 to 413).\n\n【35】Similar percentages of patients in the two groups received remdesivir, hydroxychloroquine, or glucocorticoids. Remdesivir was administered to 77 patients (53 \\[33%\\] in the tocilizumab group and 24 \\[29%\\] in the placebo group). Hydroxychloroquine was administered to 9 patients (6 \\[4%\\] in the tocilizumab group and 3 \\[4%\\] in the placebo group). Glucocorticoids were administered to 23 patients (18 \\[11%\\] in the tocilizumab group and 5 \\[6%\\] in the placebo group).\n\n【36】Primary Outcome\n---------------\n\n【37】Table 2. Time-to-Event Outcomes in the Modified Intention-to-Treat Population. Figure 2.  Figure 2. Kaplan–Meier Analyses of Efficacy Outcomes.\n\n【38】Shown are Kaplan–Meier curves for the time-to-event analyses of mechanical ventilation or death ; clinical worsening, defined as an increase in score on an ordinal clinical improvement scale (scores range from 1 to 7, with higher scores indicating worse clinical condition) by at least 1 point among patients who had been receiving supplemental oxygen at baseline or at least 2 points among those who had not been receiving supplemental oxygen at baseline  ; and discontinuation of supplemental oxygen among patients who had been receiving it at baseline .\n\n【39】A total of 27 patients (11.2%) were intubated within 28 days or died before intubation . At day 28, 17 patients (10.6%) in the tocilizumab group and 10 patients (12.5%) in the placebo group had been intubated or had died (11 were intubated and 6 died without being intubated in the tocilizumab group; 8 were intubated and 2 died without being intubated in the placebo group). The Kaplan–Meier curves for the time to intubation or death are shown in Figure 2A . The hazard ratio for a primary outcome event in the tocilizumab group was 0.83 (95% confidence interval \\[CI\\], 0.38 to 1.81; P=0.64 by log-rank test). The hazard ratio was also estimated with adjustment for age, sex, race, Hispanic or Latino ethnic group, diabetes status, and baseline serum interleukin-6 concentration. The adjusted hazard ratio was 0.66 (95% CI, 0.28 to 1.52). The difference between the adjusted and unadjusted hazard ratios was primarily due to the greater percentage of older patients in the tocilizumab group.\n\n【40】Secondary Outcomes\n------------------\n\n【41】The Kaplan–Meier curves for the time to worsening on the ordinal clinical improvement scale are shown in Figure 2B . The hazard ratio for worsening in the tocilizumab group as compared with the placebo group was 1.11 (95% CI, 0.59 to 2.10; P=0.73 by log-rank test) . The adjusted hazard ratio was 0.88 (95% CI, 0.45 to 1.72). At 14 days, 18.0% of the patients in the tocilizumab group and 14.9% of the patients in the placebo group had had disease worsening as measured on the ordinal scale. At 28 days, the percentages were 19.3% and 17.4%, respectively.\n\n【42】The Kaplan–Meier curves for the time to discontinuation of supplemental oxygen among patients who had been receiving it at baseline are shown in Figure 2C . The hazard ratio for discontinuation by 28 days in the tocilizumab group as compared with the placebo group was 0.94 (95% CI, 0.67 to 1.30; P=0.69 by log-rank test). The adjusted hazard ratio was 0.95 (95% CI, 0.67 to 1.33). The median time to discontinuation of supplemental oxygen among patients who had been receiving it at baseline was 5.0 days (interquartile range, 3.8 to 7.6) in the tocilizumab group and 4.9 days (interquartile range, 3.8 to 7.8) in the placebo group. At 14 days, 75.4% of patients in the tocilizumab group and 78.8% in the placebo group were no longer receiving supplemental oxygen. At 28 days, the percentages were 82.6% and 84.9%, respectively.\n\n【43】Tertiary and Exploratory Outcomes\n---------------------------------\n\n【44】Table 3. Duration Outcomes and Admission to the ICU or Death in the Modified Intention-to-Treat Population.\n\n【45】Tertiary efficacy outcomes are shown in Table 2 and Table 3 . None of the outcomes differed substantially between the treatment groups. The median time to improvement was 6.0 days (95% CI, 5.0 to 6.0) in the tocilizumab group and 5.0 days (95% CI, 4.0 to 7.0) in the placebo group. The median duration of supplemental oxygen use after administration of tocilizumab was 4.0 days; the corresponding measure in the placebo group was 3.9 days. Among the 233 patients who were not in the ICU at enrollment, 25 patients (15.9%) in the tocilizumab group and 12 patients (15.8%) in the placebo group were either admitted to the ICU or died before ICU admission. Among the 19 patients who were intubated, the duration of mechanical ventilation did not differ significantly between the groups (median duration, 15.0 days in the tocilizumab group and 27.9 days in the placebo group). The median time to discharge was 6.0 days in both groups. The results of exploratory analyses of changes in the concentrations of inflammatory markers over time are shown in Table S3.\n\n【46】Patient and Treatment Subgroup Analyses\n---------------------------------------\n\n【47】Multivariate, adjusted models showed that patients older than 65 years of age were at greater risk for progression to intubation or death than younger patients (hazard ratio, 3.11; 95% CI, 1.36 to 7.10). In addition, patients with baseline serum interleukin-6 concentrations higher than 40 pg per milliliter were more likely to have progression than those with baseline concentrations at or below 40 pg per milliliter (hazard ratio, 3.03; 95% CI, 1.34 to 6.83). Factors that were not found to affect the risk of intubation or death included male sex (hazard ratio, 1.27; 95% CI, 0.57 to 2.81), Hispanic or Latino ethnic group (hazard ratio, 1.16; 95% CI, 0.47 to 2.85), obesity (hazard ratio, 1.32; 95% CI, 0.60 to 2.90), diabetes (hazard ratio, 1.55; 95% CI, 0.69 to 3.48), and treatment with remdesivir (hazard ratio, 1.95; 95% CI, 0.86 to 4.44). No difference between the subgroups was observed for the treatment effect .\n\n【48】Safety\n------\n\n【49】Table 4. Adverse Events in the Safety Population.\n\n【50】Adverse events are shown in Table 4 . No new safety signal for tocilizumab emerged. Neutropenia developed in 22 patients in the tocilizumab group, as compared with only 1 patient in the placebo group (P=0.002), but serious infections occurred in fewer patients in the tocilizumab group (13 \\[8.1%\\] vs. 14 \\[17.1%\\]; P=0.03). There were 36 serious adverse events in the tocilizumab group, occurring in a total of 28 patients. Of these 36 serious adverse events, 25 were considered by the investigators to be unrelated to tocilizumab, and 11 were considered to be related or possibly related. There were 38 serious adverse events in the placebo group, occurring in 12 patients; 35 events were considered unrelated to placebo, and 3 were considered related or possibly related. All serious adverse events are listed in Table 4 . They are also listed separately with additional descriptions in Table S5.\n\n【51】Discussion\n----------\n\n【52】Our data do not provide support for the concept that early interleukin-6 receptor blockade is an effective treatment strategy in moderately ill patients hospitalized with Covid-19. The hypothesis underlying our trial was that interleukin-6 receptor blockade in patients with disease that had not yet led to intubation would disrupt the cytokine storm associated with Covid-19, thereby preventing the most severe disease consequences. Findings from this randomized, double-blind, placebo-controlled trial indicated that this intervention had no significant effect on the risk of intubation or death, on disease worsening, on time to discontinuation of supplemental oxygen, or on any of the efficacy outcomes we examined. Because of the width of the confidence intervals for our efficacy comparisons, however, we cannot exclude the possibility that tocilizumab treatment is associated with either some benefit or harm in some patients.\n\n【53】Our results stand in contrast to those of multiple open-label trials and nonrandomized case series, some of which have suggested that interleukin-6 receptor blockade has a substantial positive effect on patients with Covid-19. Reported but still unpublished results of a small number of other randomized trials have been discouraging.  The explanation for the failure of tocilizumab to affect clinical outcomes substantially in our trial is not clear. One possibility is that interleukin-6 and other inflammatory proteins that are observed to be present at elevated levels in patients with Covid-19 represent host responses to the infection, similar to the elevations in cytokine levels seen in patients with endocarditis, sepsis, and other infections, rather than components of a self-amplifying inflammatory loop that would benefit from suppression. Regardless of the explanation, our findings in the context of information about other randomized, blinded trials of interleukin-6 receptor blockade largely undermine the concept that this anticytokine approach is a useful treatment strategy for preventing the evolution of Covid-19 from a moderate to a severe state. It remains possible, however, that patient populations that differ from the one targeted by our trial might benefit from interleukin-6 receptor blockade. It is also possible that other anticytokine approaches or studies in which alternative antiinflammatory or antiviral strategies (or combinations of those strategies) are used will show greater efficacy than we have shown. The overall experience with interleukin-6 receptor blockade, however, underscores the point that any such approach must be subjected to randomized, blinded trials. Moreover, such investigations should be performed early in the course of evaluating each strategy, before the adoption of widespread use. \n\n【54】Our results confirmed the relationship between older age and poor outcomes in Covid-19 but did not identify separate effects of sex, diabetes, obesity, or Hispanic or Latino ethnic group on prognosis. We also confirmed that patients with higher serum interleukin-6 concentrations at baseline were more likely to have a poor outcome.  Although tocilizumab did not show efficacy in this trial, the drug was not associated with excessive high-grade toxic effects in this population, which was characterized by multiple coexisting conditions. Patients treated with tocilizumab had fewer serious infections than those who received placebo.\n\n【55】Our trial has a number of strengths. It was a randomized, double-blind, placebo-controlled trial focusing on hospitalized patients who had not been intubated. The trial population was also ethnically diverse, including substantial numbers of Hispanic or Latino patients. \n\n【56】The trial also has certain weaknesses. The primary event rate we observed was lower than anticipated, perhaps because of evolving standards of care during the trial. Remdesivir became available early in the trial, and general approaches to management were also approved, including strategies to delay intubation, if possible, rather than intubating early. Nevertheless, 12% of the patients in our trial were intubated or died, and we found little evidence that tocilizumab altered any efficacy outcomes. Despite randomization, imbalance in the percentage of older patients between the treatment groups was observed. This had some effect on the estimated treatment effect on the primary outcome, as evidenced by the difference between the unadjusted and adjusted hazard ratios. However, the confidence intervals in both analyses were wide, and the overall conclusions of the unadjusted and adjusted analyses are consistent.\n\n【57】In this randomized, double-blind, placebo-controlled trial, we did not find any efficacy of interleukin-6 receptor blockade for the treatment of hospitalized patients with Covid-19.\n\n【58】Table 1. Baseline Characteristics of the Patients. \n\n| Characteristic | Tocilizumab(N=161) | Placebo(N=82) | All Patients(N=243) |\n| --- | --- | --- | --- |\n| Enrolling hospital — no. (%) |  |  |  |\n| Boston Medical Center | 16 (10) | 7 (9) | 23 (9) |\n| Brigham and Women’s Hospital | 17 (11) | 8 (10) | 25 (10) |\n| Lahey Hospital | 2 (1) | 2 (2) | 4 (2) |\n| Massachusetts General Hospital | 62 (39) | 30 (37) | 92 (38) |\n| North Shore Medical Center | 51 (32) | 26 (32) | 77 (32) |\n| Newton–Wellesley Hospital | 13 (8) | 7 (9) | 20 (8) |\n| St. Elizabeth’s Medical Center | 0 | 2 (2) | 2 (1) |\n| Median age (IQR) — yr | 61.6 (46.4–69.7) | 56.5 (44.7–67.8) | 59.8 (45.3–69.4) |\n| Age >65 yr — no. (%) | 60 (37) | 22 (27) | 82 (34) |\n| Male sex — no. (%) | 96 (60) | 45 (55) | 141 (58) |\n| Race or ethnic group — no. (%)  |  |  |  |\n| American Indian or Alaska Native | 1 (1) | 0 | 1 (<1) |\n| Asian | 7 (4) | 2 (2) | 9 (4) |\n| Black | 24 (15) | 16 (20) | 40 (16) |\n| Native Hawaiian or Pacific Islander | 0 | 1 (1) | 1 (<1) |\n| White | 71 (44) | 33 (40) | 104 (43) |\n| Other | 35 (22) | 15 (18) | 50 (21) |\n| Unknown | 23 (14) | 15 (18) | 38 (16) |\n| Hispanic or Latino ethnic group — no. (%)  |  |  |  |\n| Hispanic or Latino | 70 (43) | 39 (48) | 109 (45) |\n| Not Hispanic or Latino | 84 (52) | 35 (43) | 119 (49) |\n| Unknown | 7 (4) | 8 (10) | 15 (6) |\n| Median BMI (IQR)  | 29.9 (26.0–34.2) | 30.2 (25.7–33.8) | 30.1 (25.9–34.2) |\n| BMI ≥30 — no. (%)  | 80 (50) | 42 (51) | 122 (50) |\n| Median time from symptom onset to randomization (IQR) — days | 9.0 (6.0–13.0) | 10.0 (7.0–13.0) | 9.0 (6.0–13.0) |\n| Hypertension — no. (%) | 80 (50) | 38 (46) | 118 (49) |\n| Heart failure — no. (%) | 17 (11) | 7 (9) | 24 (10) |\n| History of myocardial infarction — no. (%) | 15 (9) | 6 (7) | 21 (9) |\n| Chronic obstructive pulmonary disorder — no. (%) | 15 (9) | 7 (9) | 22 (9) |\n| Asthma — no. (%) | 15 (9) | 7 (9) | 22 (9) |\n| Smoking status — no. (%) |  |  |  |\n| Current smoker | 7 (4) | 0 | 7 (3) |\n| Former smoker | 46 (29) | 26 (32) | 72 (30) |\n| Lifelong nonsmoker | 99 (61) | 48 (59) | 147 (60) |\n| Unknown | 9 (6) | 8 (10) | 17 (7) |\n| Diabetes — no. (%) | 45 (28) | 30 (37) | 75 (31) |\n| Chronic kidney disease — no. (%) | 29 (18) | 13 (16) | 42 (17) |\n| History of cancer — no. (%) | 22 (14) | 8 (10) | 30 (12) |\n| Ordinal scale score — no. (%)  |  |  |  |\n| 2 | 23 (14) | 15 (18) | 38 (16) |\n| 3 | 133 (83) | 61 (74) | 194 (80) |\n| 4 | 5 (3) | 5 (6) | 10 (4) |\n| 5 | 0 | 1 (1) | 1 (<1) |\n| Median laboratory values (IQR)  |  |  |  |\n| Absolute lymphocyte count — cells/mm 3 | 1040 (700–1400) | 1030 (680–1360) | 1030 (700–1400) |\n| C-reactive protein level — mg/liter | 116.0 (67.1–190.6) | 94.3 (58.4–142.0) | 110.0 (64.9–175.3) |\n| Ferritin level — ng/ml | 723 (413–1212) | 686 (382–1228) | 708 (411–1225) |\n| d \\-Dimer level — ng/ml | 857 (536–1695) | 980 (500–1739) | 884 (527–1730) |\n| Lactate dehydrogenase level — U/liter | 351 (287–420) | 324 (290–395) | 340 (289–413) |\n| Serum interleukin-6 level — pg/ml | 23.6 (14.0–49.9) | 25.4 (14.6–40.3) | 24.4 (14.1–45.5) |\n| Erythrocyte sedimentation rate — mm/hr | 61 (42–90) | 63 (42–87) | 61 (42–88) |\n| Troponin level — ng/liter | 8 (6–22) | 9 (6–24) | 9 (6–22) |\n| NT-proBNP level — pg/ml | 110 (50–438) | 93 (33–431) | 108 (38–437) |\n| Procalcitonin level — ng/ml | 0.2 (0.1–0.4) | 0.2 (0.1–0.3) | 0.2 (0.1–0.4) |\n\n【60】 Percentages may not total 100 because of rounding. IQR denotes interquartile range, and NT-proBNP N-terminal pro–B-type natriuretic peptide.\n\n【61】 Race and ethnic group were reported by the patients.\n\n【62】 The body-mass index (BMI) is the weight in kilograms divided by the square of the height in meters.\n\n【63】 Scores on the ordinal clinical improvement scale range from 1 to 7, with higher scores indicating worse clinical condition. A score of 2 indicates that the patient was in (or ready for) a non–intensive care unit (ICU) hospital ward and was not receiving supplemental oxygen; a score of 3, that the patient was in (or ready for) a non-ICU hospital ward and was receiving supplemental oxygen; a score of 4, that the patient was in the ICU or in a non-ICU hospital ward and was receiving noninvasive ventilation or high-flow oxygen; and a score of 5, that the patient was in the ICU, intubated, and receiving mechanical ventilation.\n\n【64】 Absolute lymphocyte counts were missing for 2 patients (1 in the tocilizumab group and 1 in the placebo group), C-reactive protein levels were missing for 2 patients (1 and 1), ferritin levels were missing for 1 patient (in the placebo group), d \\-dimer levels were missing for 2 patients (1 and 1), lactate dehydrogenase levels were missing for 3 patients (1 and 2), interleukin-6 levels were missing for 9 patients (6 and 3), erythrocyte sedimentation rates were missing for 24 patients (19 and 5), troponin levels were missing for 9 patients (5 and 4), NT-proBNP levels were missing for 19 patients (12 and 7), and procalcitonin levels were missing for 16 patients (8 and 8).\n\n【65】Table 2. Time-to-Event Outcomes in the Modified Intention-to-Treat Population. \n\n| Outcome | No. of Patients with Event within 28 Days | Percentage of Patients with Event (95% CI)  | Percentage of Patients with Event (95% CI)  | Median No. of Days to Event (95% CI) | Hazard Ratio (95% CI) | Log-Rank P Value  |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  | Day 14 | Day 28 |  |  |  |\n| Measures of worsening |  |  |  |  |  |  |\n| Primary outcome: mechanical ventilation or death |  |  |  |  |  |  |\n| Tocilizumab | 17 | 9.9 (6.2–15.7) | 10.6 (6.7–16.6) | NR | 0.83 (0.38–1.81) | 0.64 |\n| Placebo | 10 | 10.0 (5.1–18.9) | 12.5 (6.9–22.0) | NR |  |  |\n| Secondary outcome: clinical worsening on ordinal scale  |  |  |  |  |  |  |\n| Tocilizumab | 31 | 18.0 (12.9–24.9) | 19.3 (14.0–26.2) | NR | 1.11 (0.59–2.10) | 0.73 |\n| Placebo | 14 | 14.9 (8.7–24.7) | 17.4 (10.7–27.7) | NR |  |  |\n| Tertiary outcome: mechanical ventilation  |  |  |  |  |  |  |\n| Tocilizumab | 11 | 6.8 (3.6–11.4) | 6.8 (3.6–11.4) | NR | 0.65 (0.26–1.62) | — |\n| Placebo | 8 | 10.0 (4.6–17.7) | 10.0 (4.6–17.7) | NR |  |  |\n| Tertiary outcome: death |  |  |  |  |  |  |\n| Tocilizumab | 9 | 4.4 (2.1–8.9) | 5.6 (3.0–10.5) | NR | 1.52 (0.41–5.61) | — |\n| Placebo | 3 | 1.3 (0.2–8.7) | 3.8 (1.2–11.3) | NR |  |  |\n| Measures of improvement |  |  |  |  |  |  |\n| Secondary outcome: discontinuation of supplemental oxygen among patients receiving it at baseline |  |  |  |  |  |  |\n| Tocilizumab | 114 | 75.4 (67.9–82.2) | 82.6 (75.9–88.4) | 5.0 (3.8–7.6) | 0.94 (0.67–1.30) | 0.69 |\n| Placebo | 56 | 78.8 (68.3–87.7) | 84.9 (75.2–92.2) | 4.9 (3.8–7.8) |  |  |\n| Tertiary outcome: clinical improvement on ordinal scale  |  |  |  |  |  |  |\n| Tocilizumab | 147 | 86.3 (80.6–91.1) | 91.3 (86.3–95.1) | 6.0 (5.0–6.0) | 1.06 (0.80–1.41) | — |\n| Placebo | 72 | 81.5 (72.4–89.0) | 88.9 (81.0–94.5) | 5.0 (4.0–7.0) |  |  |\n| Tertiary outcome: initial discharge |  |  |  |  |  |  |\n| Tocilizumab | 147 | 86.3 (80.6–91.1) | 91.3 (86.3–95.0) | 6.0 (4.0–7.0) | 1.08 (0.81–1.43) | — |\n| Placebo | 72 | 81.5 (72.4–89.0) | 88.9 (81.0–94.5) | 6.0 (5.0–6.0) |  |  |\n\n【67】 The modified intention-to-treat population included the 242 patients (161 in the tocilizumab group and 81 in the placebo group) who underwent randomization and received either tocilizumab or placebo before intubation or death. NR denotes not reached.\n\n【68】 Percentages were estimated from the Kaplan–Meier curve.\n\n【69】 P values are not reported for tertiary outcomes.\n\n【70】 Worsening was defined as an increase in score on the ordinal clinical improvement scale by at least 1 point among patients receiving supplemental oxygen at baseline or at least 2 points among those not receiving supplemental oxygen at baseline. Improvement was defined as an increase in score by at least 2 points.\n\n【71】 Results for the time-to-event analysis of mechanical ventilation were obtained with the use of competing-risks analyses with death treated as a competing event. The percentage of patients with an event was estimated from the cumulative incidence function for the event of interest (mechanical ventilation). The cause-specific hazard ratio is reported.\n\n【72】Table 3. Duration Outcomes and Admission to the ICU or Death in the Modified Intention-to-Treat Population.\n\n| Outcome | Tocilizumab(N=161) | Placebo(N=81) | Relative Risk |\n| --- | --- | --- | --- |\n| Median duration of receipt of supplemental oxygen (IQR) — days  | 4.0 (1.8–11.6) | 3.9 (1.1–9.2) | — |\n| Median duration of mechanical ventilation (IQR) — days  | 15.0 (12.6–NR) | 27.9 (16.3–NR) | — |\n| Admission to ICU or death — % | 15.9 | 15.8 | 0.97 (0.50–1.88) |\n\n【74】 Patients who did not receive supplemental oxygen were assigned a value of 0. Patients who died before discontinuation of supplemental oxygen were assigned a value equal to the number of days from when supplemental oxygen began until the end of the 28-day follow-up period.\n\n【75】 The median and IQR for duration of mechanical ventilation were estimated from Kaplan–Meier curves generated within patients who received mechanical ventilation (11 in the tocilizumab group and 8 in the placebo group). Data for patients who died without discontinuation of mechanical ventilation were censored at 28 days.\n\n【76】Table 4. Adverse Events in the Safety Population. \n\n| Event | Tocilizumab(N=161) | Placebo(N=82) | P Value |\n| --- | --- | --- | --- |\n|  | no. of patients (%) | no. of patients (%) |  |\n| Death | 9 (5.6) | 4 (4.9)  | 0.81 |\n| Hypersensitivity reaction to infusion | 2 (1.2) | 2 (2.4) | 0.52 |\n| Infection of grade ≥3 | 13 (8.1) | 14 (17.1) | 0.03 |\n| Grade 3 | 12 (7.5) | 14 (17.1) |  |\n| Grade 4 | 1 (0.6) | 0 |  |\n| Myocardial infarction | 0 | 1 (1.2) | 0.15 |\n| Deep venous thrombosis | 2 (1.2) | 3 (3.7) | 0.18 |\n| Pulmonary embolism | 2 (1.2) | 2 (2.4) | 0.47 |\n| Stroke | 2 (1.2) | 0 | 0.31 |\n| Seizure | 0 | 1 (1.2) | 0.13 |\n| Arterial ischemia | 1 (0.6) | 0 | 0.49 |\n| Gastrointestinal perforation | 0 | 0 | — |\n| Demyelinating disorder | 0 | 0 | — |\n| Elevated liver-function values |  |  |  |\n| ALT, grade ≥3 | 8 (5.0) | 4 (4.9) | 0.99 |\n| Grade 3 | 8 (5.0) | 4 (4.9) |  |\n| Grade 4 | 0 | 0 |  |\n| AST, grade ≥3 | 6 (3.7) | 3 (3.7) | 0.99 |\n| Grade 3 | 6 (3.7) | 2 (2.4) |  |\n| Grade 4 | 0 | 1 (1.2) |  |\n| Neutropenia, grade ≥3 | 22 (13.7) | 1 (1.2) | 0.002 |\n| Grade 3 | 21 (13.0) | 1 (1.2) |  |\n| Grade 4 | 1 (0.6) | 0 |  |\n| Thrombocytopenia, grade ≥3 | 1 (0.6) | 0 | 0.51 |\n| Grade 3 | 1 (0.6) | 0 |  |\n| Grade 4 | 0 | 0 |  |\n| Bleeding | 0 | 1 (1.2) | 0.15 |\n| Other  | 21 (13.0) | 14 (17.1) | 0.15 |\n\n【78】 The percentage of patients who had at least one occurrence of each type of adverse event is reported. Grade was calculated as the maximum grade reported across occurrences within a patient. The percentages of patients with adverse events were compared with the use of a Mantel–Haenszel test stratified according to enrolling site without adjustment for multiple comparisons. ALT denotes alanine aminotransferase, and AST aspartate aminotransferase.\n\n【79】 One patient who died was intubated before receiving placebo and was excluded from the modified intention-to-treat population but included in the safety population.\n\n【80】 Other events are listed in detail in Table S6.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b01f1e5a-b3a5-4fc0-b778-bec92a543ed2", "title": "Redundant Publication: A Reminder", "text": "【0】Redundant Publication: A Reminder\nArticle\n-------\n\n【1】Nobody is well served by the practice of reporting the same study in two journals, publishing a review of the same subject nearly simultaneously in two journals, or splitting a study into two or more parts and submitting each to separate journals. A recent mini-epidemic of attempts to publish redundantly in the _Journal_ has alerted us to the need to remind authors and investigators of policies that seem to be breached increasingly often now. We have not called attention to this issue for several years, and a new generation of authors may have overlooked previous commentaries. \n\n【2】Our guidelines regarding redundant publication are published each week on the Information for Authors page. In a practice followed by many journals, we ask authors to send us copies of any manuscripts closely related to the manuscript they want us to consider for publication. This allows us to decide whether there is excessive overlap between two manuscripts or whether the results of a single study are inappropriately divided into two or more papers. In the trade, the latter practice is sometimes referred to as “salami slicing.”\n\n【3】The reasons for preventing redundant publication are not arbitrary. As earlier editorials have pointed out, multiple reports of the same observations can overemphasize the importance of the findings, overburden busy reviewers, fill the medical literature with inconsequential material, and distort the academic reward system. \n\n【4】The results of huge clinical trials or epidemiologic studies with multiple and unrelated end points, such as the GUSTO (Global Utilization of Streptokinase and Tissue Plasminogen Activator for Occluded Coronary Arteries) trial of thrombolytic therapy after acute myocardial infarction, the Framingham Heart Study, or the Physicians' Health Study, could not be reported as a single study. It often takes years to collect and analyze such data, and it is legitimate to describe important outcomes of such studies separately. On the other hand, reports of studies involving several dozen patients should not be split into overlapping manuscripts. Because the line between appropriate and inappropriate practice is not always clear, it might be helpful to provide several concrete examples.\n\n【5】First, some examples of overlapping publications: Two years ago we accepted a paper on bone lesions in patients with chronic renal failure. We asked a distinguished nephrologist to write an editorial to accompany the paper. While preparing the editorial, the nephrologist came across a study published in a specialty journal several months earlier. It was written by the same authors, described the same patients, and reported virtually the same end points. The authors had not told us they had published similar data elsewhere. Although we were well along in the production process, we pulled the paper. This example of fragmenting the results of a single study and reporting them in several papers is not unique. Several months ago, for example, we received a manuscript describing a controlled intervention in a birthing center. The authors sent the results on the mothers to us, and the results on the infants to another journal. The two outcomes would have more appropriately been reported together. We also received a manuscript on a molecular marker as a prognostic tool for a type of cancer; another journal was sent the results of a second marker from the same pathological specimens. Combining the two sets of data clearly would have added meaning to the findings.\n\n【6】After we published a recent study describing diagnostic tests on 101 consecutive patients with suspected traumatic rupture of the thoracic aorta,  we learned that the report was remarkably similar to two papers that had been published in the surgical literature.  One, published only two months earlier, described 160 patients.  The other, published two years earlier, described 69 patients.  All three papers were from the same institution; three persons were listed as authors on all three papers, and two others were listed as authors on two of the papers. Before we accepted the paper for publication, we were not informed about the report on 160 patients, although it had already been accepted for publication elsewhere. The paper published two years earlier was also not brought to our attention by the authors, although we were aware of it because it was listed in the bibliography. No information was given in any of the papers about which patients were being reported on two or more times. In fact, as the letter from Drs. Smith and Kearney in the Correspondence section of this issue of the _Journal_ indicates,  some patients in fact were reported on in all three papers, providing a misleading impression of the number of patients studied and the value of the tests.\n\n【7】It is surprising that these practices still occur, despite growing attention and nearly universal disapproval. Most of the time the redundant publication is quickly exposed by readers. In addition, an investigator's peers often recognize a succession of “least publishable units.” The motivation for publishing two or more papers when one would do is not always clear. In some instances authors have argued that they were interested in getting the information to different audiences. In others, they have claimed that they perceived the overlap to be far less substantial than did the editors. Finally, there is reason to suspect that the academic incentive system fosters a desire by authors to lengthen their bibliographies. Ways of counteracting this distorted incentive have been proposed  but have not been universally implemented.\n\n【8】We are not eager to act as prior-publication police, and we do not regularly search the literature to determine whether an author has committed one of the several forms of redundant publication. But we have rewritten the relevant portion of our Information for Authors as follows: “Authors should submit to the Editor copies of any published papers or other manuscripts in preparation or submitted elsewhere that are related to the manuscript to be considered by the _Journal_ ” (we formerly asked for “copies of any related manuscripts”). We will continue to rely on the honesty and judgment of authors in informing us of any work of theirs that is related to a manuscript they are submitting to the _Journal._\n\n【9】When preparing a manuscript, authors might heed the advice offered previously.  In deciding whether reports are redundant, authors should ask themselves whether a single paper would be more cohesive and more informative than two. When there is any doubt, authors should submit with their manuscripts any other papers possibly representing duplication or fragmentation of results, whether published, submitted for publication, or already accepted for publication.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "008e4e94-dfa5-41c5-9515-a53566a91d0c", "title": "Case 38-2018: A 54-Year-Old Man with New Heart Failure", "text": "【0】Case 38-2018: A 54-Year-Old Man with New Heart Failure\nA 54-year-old man presented with severe symptoms of heart failure. An evaluation revealed a wide pulse pressure, displaced cardiac impulse, volume overload, and a diastolic murmur. A chest radiograph showed enlargement of the cardiac silhouette. Additional diagnostic tests were performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "f4067005-65ff-4440-aa21-235ac159ad28", "title": "The Urea Reduction Ratio and Serum Albumin Concentration as Predictors of Mortality in Patients Undergoing Hemodialysis", "text": "【0】The Urea Reduction Ratio and Serum Albumin Concentration as Predictors of Mortality in Patients Undergoing Hemodialysis\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Among patients with end-stage renal disease who are treated with hemodialysis, solute clearance during dialysis and nutritional adequacy are determinants of mortality. We determined the effects of reductions in blood urea nitrogen concentrations during dialysis and changes in serum albumin concentrations, as an indicator of nutritional status, on mortality in a large group of patients treated with hemodialysis.\n\n【3】Methods\n-------\n\n【4】We analyzed retrospectively the demographic characteristics, mortality rate, duration of hemodialysis, serum albumin concentration, and urea reduction ratio (defined as the percent reduction in blood urea nitrogen concentration during a single dialysis treatment) in 13,473 patients treated from October 1, 1990, through March 31, 1991. The risk of death was determined as a function of the urea reduction ratio and serum albumin concentration.\n\n【5】Results\n-------\n\n【6】As compared with patients with urea reduction ratios of 65 to 69 percent, patients with values below 60 percent had a higher risk of death during follow-up (odds ratio, 1.28 for urea reduction ratios of 55 to 59 percent and 1.39 for ratios below 55 percent). Fifty-five percent of the patients had urea reduction ratios below 60 percent. The duration of dialysis was not predictive of mortality. The serum albumin concentration was a more powerful (21 times greater) predictor of death than the urea reduction ratio, and 60 percent of the patients had serum albumin concentrations predictive of an increased risk of death (values below 4.0 g per deciliter). The odds ratio for death was 1.48 for serum albumin concentrations of 3.5 to 3.9 g per deciliter and 3.13 for concentrations of 3.0 to 3.4 g per deciliter. Diabetic patients had lower serum albumin concentrations and urea reduction ratios than nondiabetic patients.\n\n【7】Conclusions\n-----------\n\n【8】Low urea reduction ratios during dialysis are associated with increased odds ratios for death. These risks are worsened by inadequate nutrition.\n\n【9】Introduction\n------------\n\n【10】At present, more than 150,000 Americans with end-stage renal disease benefit from dialysis treatment under Medicare's disease-targeted entitlement program  . Without the widespread availability of dialysis and kidney transplantation, many lives would have ended prematurely. Despite these achievements, the annual mortality rate among patients with end-stage renal disease in the United States increased from 21.0 percent in 1981 to 24.3 percent in 1988  . In contrast, the annual mortality rates among patients with end-stage renal disease in other industrialized nations have remained stable and lower than those in the United States  .\n\n【11】The average blood urea nitrogen concentrations before and after a hemodialysis treatment and between hemodialysis treatments, the duration of individual treatments, and the adequacy of nutrition as assessed by simple laboratory variables may be critical determinants of the morbidity and mortality of patients treated with hemodialysis  . Pursuant to these observations, we undertook a retrospective analysis to define the risk of death as a function of both solute clearance by hemodialysis and serum albumin concentrations, as a measure of nutritional status, in a large group of patients treated with hemodialysis. We also evaluated the extent to which solute clearance may affect over time the relation between changes in solute clearance and changes (if any) in the patients' nutritional status.\n\n【12】Methods\n-------\n\n【13】Patients\n--------\n\n【14】National Medical Care, a large provider of hemodialysis services, maintains a patient statistical profile system  that includes selected patient data such as birth date, sex, cause of end-stage renal disease, time and frequency of dialysis treatments, frequency and duration of hospitalization, renal-transplantation status, vital status, and the results of common clinical laboratory tests, such as serum electrolyte, creatinine, albumin, and cholesterol concentrations and blood urea nitrogen concentrations. All the biochemical analyses were performed by a single clinical laboratory (LifeChem, Northvale, N.J.). This data base includes a measure of the percent reduction in the blood urea nitrogen concentration during a single dialysis treatment -- the urea reduction ratio -- as a measure of the adequacy of dialysis. The urea reduction ratio is calculated with the formula 100 × (1 - \\[C <sub>t </sub> /C <sub>o </sub> \\]), in which C <sub>t </sub> is the blood urea nitrogen measured five minutes after the end of dialysis and C <sub>o </sub> is the predialysis blood urea nitrogen  . The urea reduction ratio is a function of the clearance of urea from the blood by the dialyzer, the length of the individual dialysis treatment, and the volume of distribution of urea in a particular patient. Therefore, the urea reduction ratio is a quantitative measurement of an individual patient's urea clearance during a single hemodialysis treatment and can be used as a proxy for the adequacy of solute clearance during a treatment  .\n\n【15】The 13,473 study subjects were selected from among adult patients with end-stage renal disease who were receiving hemodialysis three times per week in the 358 National Medical Care facilities on October 1, 1990, and who were still receiving dialysis therapy six months later (March 31, 1991) or who had died. Patients who underwent kidney transplantation, transferred to other dialysis facilities or to other forms of long-term dialysis, or were lost to follow-up during that interval were excluded. A subgroup of 12,170 patients were treated from October through December 1990 (early period) and January through March 1992 (late period).\n\n【16】Laboratory-data files were examined to identify patients who had one or more determinations of the serum albumin concentration and the urea reduction ratio between June and September 1990, the four months before the start of the study. The results of all determinations during this interval were averaged for each patient. During this time, the serum albumin concentration was measured a mean (±SD) of 2.3 ±1.0 times per patient and the urea reduction ratio was determined 2.3 ±1.1 times per patient. Our retrospective analyses used these results for the patients who met the specified follow-up criteria.\n\n【17】Statistical Analysis\n--------------------\n\n【18】The initial data analysis used a forward stepwise logistic-regression process  to evaluate the odds ratios of death in terms of selected demographic characteristics, such as age, sex, race, presence of diabetes mellitus, and primary renal diagnosis. For each renal diagnosis, a separate binary variable was constructed. Demographic characteristics found to be associated with the odds ratios were then fixed in the regression model. The urea reduction ratio and the serum albumin concentration were then added to the model. In addition, the patients were grouped according to both the urea reduction ratio and serum albumin concentration as dichotomous variables, and patient profiles were constructed by treating the groups as binary variables in the model.\n\n【19】Finally, we compared the urea reduction ratios and serum albumin concentrations in 4758 patients with diabetes mellitus with those in 8715 nondiabetic patients. Group values were adjusted for age, sex, and race, and the statistical differences between them were evaluated by analysis of covariance.\n\n【20】Results\n-------\n\n【21】Patients\n--------\n\n【22】A total of 13,473 patients were undergoing hemodialysis on October 1, 1990, and fulfilled the entry criteria. Their mean age (±SD) was 59 ±15 years; 48 percent were women, 48 percent were nonwhite, and 35 percent had diabetes. The median duration of hemodialysis was 30 months. A total of 1556 of the patients (11.5 percent) died during the six-month follow-up period.\n\n【23】Demographic Characteristics, Urea Reduction Ratio, Serum Albumin Concentration, and Risk of Death\n-------------------------------------------------------------------------------------------------\n\n【24】Table 1. Relation of the Patients' Characteristics to the Odds Ratio for Death.\n\n【25】The effect of the patients' characteristics on the odds ratio for death is shown in Table 1 . After a mean age of 59 years, each year of advancing age was associated with an approximately 3 percent increase in the odds ratio. The odds ratios were higher among whites and diabetic patients. A similar analysis was performed that included the serum albumin concentration and the urea reduction ratio in the model. Both the serum albumin concentration and the urea reduction ratio were significant predictors of death, but there was no statistical interaction between these two factors. The duration of each dialysis treatment was also evaluated in this model. Given the range of times during which hemodialysis was performed (lower-quartile group, three hours; upper-quartile group, four hours), the length of treatment was not associated with a risk of death.\n\n【26】Table 2. Adjusted Risk of Death According to the Urea Reduction Ratio or Serum Albumin Concentration in Patients with End-Stage Renal Disease.\n\n【27】As compared with a reference urea reduction ratio of 65 to 69 percent, values less than 60 percent were associated with a higher odds ratio for death . The serum albumin concentration was an even more powerful predictor of risk. As compared with patients who had serum albumin concentrations ranging from 4.0 to 4.4 g per deciliter, patients with serum albumin concentrations below 4.0 g per deciliter had increased odds ratios for death .\n\n【28】Figure 1. Odds Ratios for Death for a Range of Urea Reduction Ratios and Serum Albumin Concentrations in 13,473 Patients with End-Stage Renal Disease Treated with Hemodialysis.\n\n【29】A value of 1.0 was assigned to the odds ratio for the reference group (Ref), with a urea reduction ratio of ≥ 65 percent and a serum albumin concentration of ≥ 4.0 g per deciliter. On the bar graphs, the top symbol indicates the statistical significance of this comparison, the symbol on the right the P value for the comparison with a serum albumin concentration of ≥ 4.0 g per deciliter, and the symbol on the left the P value for the comparison with a urea reduction ratio of ≥ 65 percent. NS denotes not significant (P>0.05), solid squares a P value of 0.05 to >0.01, and asterisks a P value of ≤ 0.01.\n\n【30】The patients were also grouped according to their combined values for urea reduction ratio and serum albumin concentration . Considering patients with a urea reduction ratio of ≥ 65 percent and a serum albumin concentration of ≥ 4.0 g per deciliter as the reference group, lower serum albumin concentrations were associated with significant increases in the risk of death for all the urea-reduction-ratio groups. The odds ratios increased at lower urea reduction ratios for the patients who had serum albumin concentrations of 3.5 to 3.9 g per deciliter or 3.0 to 3.4 g per deciliter, but not for the patients whose serum albumin concentrations were ≥ 4.0 or <3.0 g per deciliter. Thus, both the urea reduction ratio and serum albumin concentration were important determinants of the odds of death, but factors associated with the serum albumin concentration had a greater effect.\n\n【31】Frequency Distribution and Temporal Trends for Urea Reduction Ratio Values and Serum Albumin Concentrations\n-----------------------------------------------------------------------------------------------------------\n\n【32】Figure 2. Frequency Distribution of the Average Urea Reduction Ratios and Serum Albumin Concentrations in Patients with End-Stage Renal Disease Treated with Hemodialysis from June through September 1990.\n\n【33】The frequency distribution of the mean urea reduction ratio for each patient from June through September 1990 is shown in Figure 2 . The mean urea reduction ratio was 58 ±10 percent. Ten percent of the values were ≥ 70 percent; 24 percent were ≥ 65 percent, and 45 percent were ≥ 60 percent. In contrast, 33 percent of the patients had urea reduction ratios that were below 55 percent, and 16 percent had values below 50 percent. For individual patients, the average urea reduction ratios from June through September 1990 were not significantly different from the values obtained from October 1990 through March 1991 (58 ±10 percent vs. 59 ±9 percent, P>0.05).\n\n【34】The distribution of values for the serum albumin concentrations for each patient from June through September 1990 is shown in Figure 2 . The mean serum albumin concentration was 3.8 ±0.4 g per deciliter. Sixty percent of the serum albumin concentrations were below 4.0 g per deciliter, 13 percent were below 3.5 g per deciliter, 2 percent were below 3.0 g per deciliter, and less than 1 percent were below 2.5 g per deciliter. There was no correlation between the serum albumin concentration and the urea reduction ratio.\n\n【35】Figure 3. Trends in Median Urea Reduction Ratios in Patients with End-Stage Renal Disease Treated with Hemodialysis from October 1990 through March 1992.\n\n【36】For 12,170 patients treated from October through December 1990 (early period) and January through March 1992 (late period), we averaged both the urea reduction ratio and the serum albumin concentration. During the early period, preliminary data concerning the influence of the urea reduction ratio and the serum albumin concentration on the risk of death among patients treated with hemodialysis were disseminated to facility medical directors, and they were urged to respond to these data in their individual facilities. The mean urea reduction ratio increased from 58 ±9 percent during the early period to 62 ±8 percent during the late period (P<0.001). The lower-quartile group of urea reduction ratios increased from 53 to 56 percent . The mean serum albumin concentration during this interval decreased from 3.9 ±0.3 to 3.7 ±0.3 g per deciliter (P<0.001). Each patient's early-period urea reduction ratio and serum albumin concentration were subtracted from the corresponding late-period values. Although the correlation of the change in serum albumin with the change in the urea reduction ratio was direct and statistically significant (r = 0.044, P<0.001), the r  statistic (0.002) indicated a weak association.\n\n【37】We identified a subgroup of 178 patients in whom the urea reduction ratio was ≤ 50 percent during the early period and increased to ≥ 60 percent in the late period. The mean urea reduction ratio in this group increased from 43 ±3 percent in the early period to 67 ±8 percent in the late period, and the mean serum albumin concentration fell from 3.8 ±0.5 to 3.7 ±0.4 g per deciliter (P = 0.01). In contrast, we identified 37 patients with urea reduction ratios of ≥ 65 percent in the early period that fell to ≤ 55 percent during the late period. The mean urea reduction ratios during the early and late periods in this group were 71 ±14 percent and 50 ±7 percent, respectively. The mean serum albumin concentrations in these patients were 3.7 ±0.5 and 3.6 ±0.5 g per deciliter, respectively (P not significant). There was no correlation in the magnitude of the change in serum albumin concentration between the patients in whom the urea reduction ratio increased and the patients in whom it decreased.\n\n【38】Urea Reduction Ratio and Serum Albumin Concentrations in Diabetic and Nondiabetic Patients\n------------------------------------------------------------------------------------------\n\n【39】An adjustment for the serum albumin concentration and urea reduction ratio in the statistical model was associated with a reduction in the odds ratio for death in the diabetic patients. Hence, we evaluated potential differences in the distributions of the serum albumin concentration and urea reduction ratio in patients with and those without diabetes. Women had higher urea reduction ratios than men during treatment (61 ±9 vs. 56 ±9 percent, P<0.001), and nondiabetic patients had higher values than diabetic patients during treatment (59 ±10 vs. 57 ±10 percent, P<0.001). The nondiabetic patients had higher serum albumin concentrations than the diabetic patients (3.9 ±0.4 vs. 3.8 ±0.3 g per deciliter, P<0.001).\n\n【40】Discussion\n----------\n\n【41】Our results support the conclusions of previous studies that inadequate dialysis and poor nutrition are major factors contributing to the excessive mortality of patients treated with hemodialysis in the United States. We used the urea reduction ratio as a measure of the adequacy of dialysis  . When the urea reduction ratio was included in the statistical model, the odds of death were not associated with the duration of dialysis treatments (range, three to four hours) commonly used in these patients. This finding seems to contradict previous studies suggesting that treatment time itself is a critical determinant of survival for patients undergoing hemodialysis  . In these earlier studies, however, no direct measure of solute clearance was reported. The National Cooperative Dialysis Study  also found that the length of dialysis had a significant effect on patient survival,  whereas we did not. Our results suggest that the intensity of treatment should be sufficient to result in a urea reduction ratio of ≥ 60 percent, whereas a urea reduction ratio of >50 percent was implied to be adequate by the National Cooperative Dialysis Study  . The majority of patients currently treated with hemodialysis would not have been eligible to participate in the cooperative study because of age and coexisting conditions such as diabetes mellitus  . Our data suggest that this patient population may require more intensive dialysis to achieve greater clearance.\n\n【42】The increased odds ratio for death with urea reduction ratios below 60 percent and the original frequency distribution of values for the urea reduction ratio suggest that dialysis mortality may decrease with more intensive dialysis. Beginning in late 1990, National Medical Care provided facilities and medical directors with local and national data about the odds ratio for death and the distribution of urea reduction ratios. A subsequent upward trend in urea reduction ratios was probably the result of a revised definition of adequate dialysis -- that is, a target urea reduction ratio of ≥ 60 percent.\n\n【43】Like the authors of previous studies of patients with renal and other diseases,  we found that the serum albumin concentration was the most powerful predictor of death. However, the urea reduction ratio and serum albumin concentration were independent predictors. For the approximately 60 percent of patients at risk because of a low serum albumin concentration (<4.0 g per deciliter), the odds ratios associated with inadequate nutrition (as reflected by low serum albumin concentrations) may be magnified by the presence of suboptimal dialysis. Some investigators suggest that augmentation of urea clearance will improve a patient's nutritional status  . We did not find this to be the case, and the correlation between the adequacy of dialysis and nutritional status (as measured by serum albumin concentration) was weak. Of course, the persistence of a low serum albumin concentration despite an increased urea reduction ratio could have been due to many factors other than malnutrition, including hepatocellular dysfunction, hypervolemia, and exposure to pyrogenic cytokines.\n\n【44】Our analysis confirms the observation that differences in nutrition and the intensity of dialysis explain part of the excess mortality among diabetic patients with end-stage renal disease  . Patients with diabetes have lower serum albumin concentrations and urea reduction ratios than nondiabetic patients. Possible factors contributing to the lower urea reduction ratios in these patients include compromised blood flow during a dialysis treatment as a result of hemodynamic instability, poor flow through the vascular access for hemodialysis, the provision of less intensive dialysis because of inappropriate reliance on low prehemodialysis blood urea nitrogen or serum creatinine values as measures of the adequacy of dialysis, and the prescription of suboptimal therapy because of prejudices that excessive morbidity and mortality are unavoidable in diabetic patients. Women treated with hemodialysis had higher urea reduction ratios and a reduced odds ratio of death. Physicians may have prescribed dialysis treatment independently of a patient's body size, resulting in increased urea reduction ratios in women, who are generally smaller than men.\n\n【45】Because our study was retrospective, a causal relation between the urea reduction ratio, serum albumin concentration, or both and the risk of death should be inferred from these data with caution. However, these results may have practical implications for the care of patients being treated with hemodialysis. Nephrologists may have only limited success in improving nutritional status by augmenting dialysis. Thus, both these components of care must be monitored and independently corrected when they are suboptimal -- a urea reduction ratio below 60 percent and a serum albumin concentration of less than 4.0 g per deciliter. Strategies that improve solute clearance during hemodialysis include using more efficient dialyzers, increasing the time of hemodialysis, and optimizing the blood and dialysate flows during each dialysis treatment. The nutritional status of patients undergoing hemodialysis should be monitored routinely, and when inadequacies are identified, corrective measures must be instituted. These measures include augmenting the patient's caloric intake by providing hyperalimentation during dialysis treatments, correcting dental deficiencies, improving gastrointestinal abnormalities, and supplementing finances for nutritious foods.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "90932e72-5980-4af2-9d28-c0302314322d", "title": "Brief Report: A Case of Severe Ebola Virus Infection Complicated by Gram-Negative Septicemia", "text": "【0】Brief Report: A Case of Severe Ebola Virus Infection Complicated by Gram-Negative Septicemia\nSummary\n-------\n\n【1】Ebola virus disease (EVD) developed in a patient who contracted the disease in Sierra Leone and was airlifted to an isolation facility in Hamburg, Germany, for treatment. During the course of the illness, he had numerous complications, including septicemia, respiratory failure, and encephalopathy. Intensive supportive treatment consisting of high-volume fluid resuscitation (approximately 10 liters per day in the first 72 hours), broad-spectrum antibiotic therapy, and ventilatory support resulted in full recovery without the use of experimental therapies. Discharge was delayed owing to the detection of viral RNA in urine (day 30) and sweat (at the last assessment on day 40) by means of polymerase-chain-reaction (PCR) assay, but the last positive culture was identified in plasma on day 14 and in urine on day 26. This case shows the challenges in the management of EVD and suggests that even severe EVD can be treated effectively with routine intensive care.\n\n【2】Introduction\n------------\n\n【3】Since December 2013, a _Zaire ebolavirus_ (EBOV) epidemic of unprecedented scale has ravaged West Africa, with a focus on Guinea, Sierra Leone, and Liberia.  The current epidemic has led to a public health emergency in the region, exacerbated by high rates of infection among health care personnel. A substantial number of fatal cases are among health care workers.  Several international health care workers have been evacuated to specialized centers in Europe and the United States. The patient transferred to our isolation unit worked for the World Health Organization (WHO) as an epidemiologist in Sierra Leone and was airlifted at the request of the WHO.\n\n【4】History and Findings on Admission\n---------------------------------\n\n【5】The 36-year-old male patient had malaise, headache, myalgias, and arthralgias on day 1 of the illness (August 18, 2014). Fever developed on day 2, and the patient was treated empirically for malaria. On days 2 through 6, he also received empirical antimicrobial therapy with ceftazidime. On day 6, he tested positive for EBOV on real-time reverse-transcriptase–PCR (RT-PCR) assay. Nausea, vomiting, abdominal pain, and nonbloody diarrhea developed on day 7, prompting his admission to a treatment center in Sierra Leone. Single doses of ciprofloxacin and metronidazole were administered on day 8, and supportive therapy with intravenous fluids was initiated and maintained until day 10, when he was transferred to Hamburg.\n\n【6】On the basis of the patient's history, the most likely source of infection was contact with a colleague who had had symptoms of EVD and died 10 days before the onset of symptoms in our patient. The patient and his colleague shared an office for meetings and used the same restroom facilities until 3 days before the colleague died.\n\n【7】On admission to our facility, the patient was clinically stable, with an elevated temperature (38.4°C), but other vital signs were within normal limits. The oxygen saturation was 97% while the patient was breathing ambient air, the blood pressure 110/80 mm Hg, and the heart rate 87 beats per minute. The patient was awake, alert, and fully oriented. Physical examination revealed signs of dehydration and diffuse abdominal tenderness. Rash was absent.\n\n【8】Table 1. Clinical Variables, Fluid Management, and Laboratory Values during the Course of Illness.\n\n【9】The patient's medical history was notable for asymptomatic, chronic hepatitis B virus infection (viral DNA level, as assessed by means of PCR, 11,000 IU per milliliter, with no evidence of liver dysfunction). The results of laboratory tests were consistent with substantial dehydration and hemoconcentration, with a hemoglobin level of 18.0 g per deciliter, a creatinine level of 1.9 mg per deciliter (170 μmol per liter) (normal range, ≤1.1 mg per deciliter \\[100 μmol per liter\\]), and mild hypokalemia (potassium level, 3.4 mmol per liter; normal range, 3.5 to 5.0). Additional abnormal laboratory values included a platelet count of 103×10  per cubic millimeter (normal range, 150 to 450), an aspartate aminotransferase level of 1054 U per liter (normal range, <50), and a D-dimer level of 33 mg per liter (normal range, 0.1 to 0.4) .\n\n【10】Tests for malaria and dengue were negative. Ultrasonography of the abdomen revealed a complete collapse of the inferior vena cava, a paralytic ileus with pronounced edema of the small intestine and large intestinal wall, and distended intestinal loops. With the exception of enlarged mesenteric lymph nodes, all the other organs appeared normal. A full description of the clinical findings and additional laboratory findings are presented in the Supplementary Appendix .\n\n【11】Clinical Course and Management\n------------------------------\n\n【12】Symptomatic Therapy\n-------------------\n\n【13】Treatment of nausea, vomiting, and fever was implemented intravenously immediately after admission, because oral drug intake was not possible. An overview of all the administered drugs with respective timelines and our treatment considerations regarding the administration of experimental therapies is provided in the Supplementary Appendix .\n\n【14】Baseline Fluid and Nutrition Management\n---------------------------------------\n\n【15】Maximal supportive measures were initiated, with a primary goal of restoring and maintaining volume and electrolyte balance. The patient was considered to be at high risk for hypovolemic shock on the basis of a stool output of more than 8000 ml per 24 hours in the first 3 days after the transfer to Hamburg (days 10 to 12) . Nausea and vomiting precluded oral rehydration, and high-volume resuscitation of up to 10 liters per day, with a positive net-volume balance of 30 liters during the first week, was necessary to stabilize cardiocirculatory values. Rehydration was guided by clinical examination and by repeated ultrasonographic examinations of the inferior vena cava. Persistently low potassium levels necessitated continuous intravenous substitution of 8 to 10 mmol of potassium chloride per hour. To meet the demands of volume and electrolyte repletion, a central venous catheter was placed on day 15.\n\n【16】Owing to paralytic ileus and high gastric residual volumes with severe hiccups, enteral nutrition was not tolerated. Attempts to stimulate peristalsis with the use of erythromycin and neostigmine were unsuccessful, prompting the initiation of parenteral nutrition on day 11, including the administration of glutamine at a dose of 0.3 g per kilogram of body weight per day as possible support for mucosal integrity.  After stabilization of the patient's condition, enteral nutrition with a low-fiber standard formula was initiated on day 17.\n\n【17】Clinical Course and Management of Complications\n-----------------------------------------------\n\n【18】The patient remained clinically stable on days 10 through 12. Emesis ceased on day 13, and high-volume diarrhea (>1000 ml) resolved on day 15. Hemoglobin and creatinine levels returned to the normal range by day 12, and the aminotransferase levels gradually declined . However, fever (40.0°C), hypoxemia, tachycardia, shortness of breath, and abdominal pain developed on day 13. Laboratory studies revealed leukocytosis (14.1×10  white cells per cubic millimeter) with a predominance of neutrophils (87%) and an elevated C-reactive protein level (43 mg per liter). These findings were interpreted as suggestive of concomitant secondary peritonitis and sepsis due to the loss of mucosal integrity and bacterial translocation.\n\n【19】Figure 1. Timeline of Plasma Viral RNA Load, Septicemia, and Antimicrobial Therapy in a Patient with Severe Ebola Virus Disease.\n\n【20】The decline in viral copies in plasma (red line) and the development and course of leukocytosis (gray line) are shown. The maximum C-reactive protein levels (in milligrams per liter) are shown in blue above the respective day numbers. The time when the blood culture was performed is marked by an arrow at day 12. The duration of antimicrobial therapy is shown by the gray bars. The dashed gray line represents the upper limit of the normal range for white cells. The dashed red line represents the lower limit of detection of viral RNA in plasma on reverse-transcriptase–polymerase-chain-reaction assay.\n\n【21】Antimicrobial therapy with ceftriaxone was initiated on day 13 and was changed to meropenem and vancomycin on the evening of day 14, when the patient's condition deteriorated further, with an increase in the white-cell count (26.9×10  per cubic millimeter). Blood cultures drawn on day 12 and performed within the UTHCI revealed growth of a gram-negative bacterium resistant to ampicillin, ciprofloxacin, and third-generation cephalosporins but sensitive to meropenem. More advanced tools for full identification of the organism and assessment of speciation were not accessible under the conditions of the UTHCI. An overview of the timeline of sepsis is presented in Figure 1 , showing that new severe systemic symptoms developed while the EBOV RNA load was already declining.\n\n【22】The patient's treatment course was further complicated by the development of small pleural and pericardial effusions, ascites, and increasing intestinal edema, which were probably due to a combination of EBOV endothelial-cell cytotoxicity  and decreasing serum protein concentrations as a consequence of rigorous volume management. On day 15, this condition led to a deficit in organ perfusion complicated by hypoglycemia and lactic acidosis, which was treated with increased volume repletion, sodium bicarbonate, and 40% glucose solution.\n\n【23】A combination of pulmonary atelectasis, volume overload, and encephalopathy with altered mental status resulted in acute respiratory failure on day 18. The respiratory status was further compromised by aspiration of blood from epistaxis in the context of thrombocytopenia. In spite of relative contraindications (gastroparesis and altered mental status), noninvasive ventilation was initiated. After 8 days of intermittent noninvasive ventilation, the patient gradually recovered, and his laboratory values started to normalize. However, the patient had persistent tachycardia (heart rate, 120 to 150 beats per minute) and hypertension (blood pressure, >150/80 to 180/100 mg Hg) with normal electrocardiographic and echocardiographic findings. The tachycardia and hypertension were unresponsive to metoprolol and clonidine but resolved gradually without intervention by day 35.\n\n【24】The patient had severe encephalopathy for 6 days (days 14 to 19) until vigilance slowly improved. However, the encephalopathy was followed by transient delirium with hallucinations (days 20 to 25), which were unresponsive to haloperidol but subsided spontaneously before discharge.\n\n【25】EBOV RNA Load and Serologic Findings\n------------------------------------\n\n【26】Figure 2. Timeline of Viral RNA Load in Plasma, Sweat, and Urine and Antibody Titers in Plasma.\n\n【27】The y axis on the left side of the graph shows the viral RNA load (solid lines). Owing to strong fluctuations in single measurements, line plots for urine and sweat are shown as moving averages over a period of 3 days. The y axis on the right side of the graph shows the antibody titers (dashed lines). The horizontal dashed line indicates the lower limit of detection of viral RNA on reverse-transcriptase–polymerase-chain-reaction assay.\n\n【28】Before transfer, the patient had tested positive for EBOV RNA in blood, as measured by means of a real-time RT-PCR assay, on days 6 and 7 at a local treatment center. From the day of arrival in Hamburg (day 10), the EBOV RNA concentration in plasma was measured daily (RealStar Filovirus Screen RT-PCR Kit 1.0, Altona Diagnostics). The presence of EBOV-specific IgG and IgM antibodies was determined by means of an immunofluorescence assay with the use of EBOV-infected Vero E6 cells as an antigen. The EBOV RNA load decreased starting on day 10 and first became negative on day 17. Anti-EBOV antibody titers steadily increased, with peak titers of  for IgM antibodies and of more than ,000 for IgG antibodies .\n\n【29】After plasma EBOV RNA became negative on day 17, real-time RT-PCR surveillance of sputum, saliva, conjunctival swabs, stool, urine, and sweat (from the axillary, forehead, and inguinal regions) was performed. Saliva, sputum, conjunctival swabs, and stool were already negative on the first day of testing (day 18). However, urine samples remained positive for EBOV RNA until day 30, and RNA extracts from sweat remained positive throughout the observation period until day 40.\n\n【30】In addition, to test for the infectivity of the specimens in cell culture, Vero E6 cells were inoculated with plasma, sweat, and urine (150 μl of inoculum per 25-cm  flask). Cell cultures were incubated for 40 days, and cells were monitored for viral growth by means of immunofluorescence assay. EBOV was isolated on cell culture from plasma samples obtained on days 10 to 14, when EBOV RNA was still detectable in the blood. In addition, viable EBOV was still isolated from urine samples obtained on days 18, 19, 20, 24, and 26, which was up to 9 days after the clearance of EBOV RNA from plasma. At the time of writing (day 63), all cell cultures of clinical specimens (plasma, sweat, and urine) obtained after day 26 of illness were negative for viable EBOV.\n\n【31】Discharge from UTHCI and Infectious Disease Ward\n------------------------------------------------\n\n【32】On day 28, the patient was transferred from the UTHCI to an infectious disease ward with barrier nursing precautions, which are similar to the precautions used in biosafety level 3 laboratories. In addition to negative results on the RT-PCR assay in plasma, the three criteria for the patient's transfer, based on an agreement between the hospital and local and national health authorities, were clinical recovery, continence for stool and urine, and ability to comply with instructions. Discharge from the hospital was delayed until day 40, owing to the prolonged detection of virus RNA in urine and sweat. On agreement with local health authorities, the patient was discharged after all cultures of PCR-positive samples of body fluids had been free of infectious virus particles for 20 days. The patient ultimately recovered, with all laboratory values, including liver-enzyme levels, within the normal range, and he was able to return to his family in Senegal without assistance.\n\n【33】Infection-Control Measures\n--------------------------\n\n【34】Staff members working in the UTHCI were protected by pressurized suits (Astro-Protect, Asatex) that were equipped with ventilators with high-efficiency particulate air filters to provide fresh air supply with a maximum airflow of 160 liters per minute (ProFlow 2 SC, Asatex). All the staff who cared for the patient did so without becoming infected. More details regarding the unit and protective measures are provided in the Supplementary Appendix .\n\n【35】Discussion\n----------\n\n【36】We report a case of severe EVD in a 36-year-old man who had numerous complications but fully recovered with intensive routine treatment (i.e., without any EBOV-specific treatments). The treatment consisted of intensive fluid resuscitation, broad-spectrum antimicrobial therapy, and ventilatory support.\n\n【37】Diarrhea and vomiting have been observed in 66% and 68% of patients, respectively, in the current EVD outbreak, and diarrhea is associated with death.  In the case presented here, these symptoms were associated with concomitant severe enteropathy, including paralytic ileus, large gastric residual volumes, and persistent hiccups. Data from the current outbreak suggest that paralytic ileus is a common finding, with abdominal pain present in 44% of infected persons and hiccups representing a strong predictor of death.  In our patient, paralytic ileus prevented oral rehydration and enteral nutrition and limited the potential choices for orally administered experimental therapies. In line with a recent report, this situation underscores the importance of aggressive volume repletion in patients with severe EVD.  Valuable tools for guiding fluid management were repeated ultrasonographic examinations of the abdomen and laboratory monitoring of electrolyte, pH, and lactate levels.\n\n【38】The patient also had severe gram-negative sepsis, presumably caused by bacterial translocation from the inflamed intestinal tract. Sepsis led to severe illness when the viral load was already decreasing , which suggests that sepsis may contribute substantially to the mortality observed in the current outbreak, specifically with regard to deaths occurring late after disease onset.  The patient was treated initially with ceftazidime in Sierra Leone to prevent septicemia, as recommended by Médecins sans Frontières.  However, bacteremia in this case was related to a multidrug-resistant gram-negative organism, suggesting the importance of monitoring patients for signs of infection by means of laboratory tests (e.g., C-reactive protein). Sepsis may have contributed to the development of severe encephalopathy in our patient and may explain, at least in part, the confusion and coma or unconsciousness that have been observed in 13% and 6% of patients, respectively, in the ongoing EVD outbreak  ; both symptoms have been associated with death.\n\n【39】Although respiratory failure has been described rarely in the current outbreak, it represents a predictor of a negative outcome. In the case presented here, respiratory failure occurred late in the disease course (day 18), probably owing to a combination of pulmonary atelectasis, altered mental status, volume overload, and capillary leakage. A major contributing factor was also likely to be aspiration after epistaxis in the context of thrombocytopenia. Gingival bleeding has been one of the strongest predictors of death in the current EVD outbreak, as reported by the WHO. \n\n【40】In summary, this case shows that severe EVD with serious complications can be treated successfully with general intensive care measures, supporting suggestions by Lamontagne and colleagues  that the initiation of intravenous rehydration and improvement of clinical care and laboratory diagnostics can increase survival considerably, even in the absence of new EBOV-specific therapies. Finally, the opportunity for intensive multisite sampling allowed for detailed insights into the real-time kinetics of EBOV viremia, the development of humoral immunity, and the evolution of viral RNA shedding from body fluids.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5634ce8f-a165-4cdd-9cf0-7206c0b7714f", "title": "Ovarian Toxicity from Sirolimus", "text": "【0】Ovarian Toxicity from Sirolimus\nTo the Editor:\n--------------\n\n【1】Sirolimus prevents rejection of renal allografts by blocking the mammalian target of rapamycin (mTOR), a signaling pathway known to regulate ovarian function.  Sirolimus is being assessed as a treatment for autosomal dominant polycystic kidney disease (ADPKD). Observational data suggest a potential for ovarian toxicity, but this issue has not been evaluated in randomized, controlled trials. \n\n【2】In the Suisse ADPKD study , a randomized, controlled trial conducted at the University Hospital Zurich from March 2006 through March 2010, we reviewed the occurrence of menstrual-cycle disturbances and the appearance of ovarian cysts  in 39 women with ADPKD, an inherited kidney disease that is not known to affect ovarian morphology and function.  The trial was conducted in accordance with the principles of the Declaration of Helsinki, the Good Clinical Practice guidelines of the International Conference on Harmonization, and local regulatory requirements. The local medical ethics committee approved the trial protocol. All patients provided written informed consent. A total of 21 women received sirolimus, and 18 received standard care for 18 months. Abdominal magnetic resonance imaging was performed in each patient every 6 months, and ovarian cysts larger than 2 cm in diameter were identified. Oligoamenorrhea was defined as no menstrual period for 3 months or more, or an interval of more than 35 days between menstrual periods. Analyses of prevalence were carried out with the use of exact logistic regression.\n\n【3】Wyeth (now Pfizer) provided the sirolimus but had no role in the trial design or in the collection, analysis, or interpretation of the data or the writing of the manuscript. The authors had full access to the study data and take responsibility for the integrity of the data, the accuracy of the data analysis, and the fidelity of this report to the trial protocol.\n\n【4】Table 1. Characteristics of the Patients at Randomization and Oligoamenorrhea and Ovarian Cysts over Time.\n\n【5】Among patients who received sirolimus at a dose of 1.5 mg per day, 11 of 21 with ADPKD (52%) reported menstrual-cycle abnormalities, as compared with 3 of 18 in the ADPKD control group (17%)  (odds ratio, 5.3; 95% confidence interval \\[CI\\], 1.0 to 37.0). Five patients in the sirolimus group, as compared with only 1 patient in the control group, reported more than one episode of oligoamenorrhea. Ovarian cysts were detected in 12 of 21 patients in the sirolimus group (57%), as compared with 5 of 18 in the control group (28%) (odds ratio, 3.4; 95% CI, 0.8 to 17.0) . At more than one visit after randomization, ovarian cysts were detected in 6 patients in the sirolimus group, as compared with 2 in the control group. Differences in cycle disturbances were apparent in patients who were not receiving oral contraceptives; these differences were detected in 8 of 11 patients in the sirolimus group and in 2 of 9 in the control group. However, these differences were less apparent in patients receiving oral contraceptives (3 of 10 in the sirolimus group and 1 of 9 in the control group). Differences between the sirolimus and control groups with respect to the presence of ovarian cysts did not appear to depend on the contraceptive method. Ovarian cysts were detected in 7 of 11 patients who used barrier methods in the sirolimus group and in 3 of 9 who used barrier methods in the control group; they were detected in 5 of 10 patients who used oral contraceptives in the sirolimus group and in 2 of 9 who used oral contraceptives in the control group).\n\n【6】Low-dose oral sirolimus appears to increase the risk of menstrual-cycle disturbances and ovarian cysts. Monitoring of sirolimus-associated ovarian toxicity is warranted and might guide clinical practice with the use of mTOR inhibitors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "e47e99b8-a140-406b-ba12-0c7eb155d169", "title": "Maintenance Therapy with Certolizumab Pegol for Crohn's Disease", "text": "【0】Maintenance Therapy with Certolizumab Pegol for Crohn's Disease\nAbstract\n--------\n\n【1】Background\n----------\n\n【2】Certolizumab pegol is a pegylated humanized Fab′ fragment with a high binding affinity for tumor necrosis factor α that does not induce apoptosis of T cells or monocytes.\n\n【3】Methods\n-------\n\n【4】In our randomized, double-blind, placebo-controlled trial, we evaluated the efficacy of certolizumab pegol maintenance therapy in adults with moderate-to-severe Crohn's disease. As induction therapy, 400 mg of certolizumab pegol was administered subcutaneously at weeks 0, 2, and 4. Patients with a clinical response (defined as reduction of at least 100 from the baseline score on the Crohn's Disease Activity Index \\[CDAI\\]) at week 6 were stratified according to their baseline C-reactive protein level and were randomly assigned to receive 400 mg of certolizumab pegol or placebo every 4 weeks through week 24, with follow-up through week 26.\n\n【5】Results\n-------\n\n【6】Among patients with a response to induction therapy at week 6 (428 of 668 \\[64%\\]), the response was maintained through week 26 in 62% of patients with a baseline C-reactive protein level of at least 10 mg per liter (the primary end point) who were receiving certolizumab pegol (vs. 34% of those receiving placebo, P<0.001) and in 63% of patients in the intention-to-treat population who were receiving certolizumab pegol (vs. 36% receiving placebo, P<0.001). Among patients with a response to induction therapy at week 6, remission (defined by a CDAI score of ≤150) at week 26 was achieved in 48% of patients in the certolizumab group and 29% of those in the placebo group (P<0.001). The efficacy of certolizumab pegol was also shown in patients taking and those not taking glucocorticoids or immunosuppressants and in patients who had and those who had not previously taken infliximab. Infectious serious adverse events (including one case of pulmonary tuberculosis) occurred in 3% of patients receiving certolizumab pegol and in less than 1% of patients receiving placebo. Antinuclear antibodies developed in 8% of the patients in the certolizumab group; antibodies against certolizumab pegol developed in 9% of all patients who entered the induction phase.\n\n【7】Conclusions\n-----------\n\n【8】Patients with moderate-to-severe Crohn's disease who had a response to induction therapy with 400 mg of certolizumab pegol were more likely to have a maintained response and a remission at 26 weeks with continued certolizumab pegol treatment than with a switch to placebo. \n\n【9】Introduction\n------------\n\n【10】The proinflammatory cytokine tumor necrosis factor α (TNF-α) is highly expressed in the blood, colonic tissue, and stool of patients with Crohn's disease.  Infliximab and adalimumab are engineered IgG1 monoclonal antibodies that bind to TNF-α (the first represents a chimeric molecule and the latter has been derived from human origin) and are effective in the induction and maintenance of response and remission in patients with Crohn's disease.  The efficacy of infliximab in the treatment of Crohn's disease has been attributed to multiple mechanisms, including reverse signaling through membrane-bound TNF-α and the induction of apoptosis of T cells and monocytes. \n\n【11】Certolizumab pegol, a pegylated humanized Fab′ fragment of an anti–TNF-α monoclonal antibody, has several characteristics that differentiate it from infliximab and adalimumab. In vitro, certolizumab pegol has a higher affinity for TNF-α, is devoid of the Fc portion of the antibody, and does not induce complement activation, antibody-dependent cellular cytotoxicity, or apoptosis.  One study by our group  suggested that induction treatment with certolizumab pegol might be effective for the treatment of moderate-to-severe active Crohn's disease: a 400-mg dose of certolizumab pegol every 4 weeks was effective in patients with a serum C-reactive protein (CRP) level of at least 10 mg per liter; response rates at week 12 were significantly higher than those in the placebo group. In that study, however, we did not prospectively stratify patients according to CRP level; such stratification occurred later in the clinical trial program.\n\n【12】The Pegylated Antibody Fragment Evaluation in Crohn's Disease: Safety and Efficacy 1 (PRECISE 1) trial involved a placebo-controlled induction phase and a 26-week treatment phase in a population of patients with active Crohn's disease; it is reported on elsewhere in this issue of the _Journal_ .  PRECISE 2, which we present in this article, was a 26-week trial involving maintenance and withdrawal therapy with certolizumab pegol in patients with moderate-to-severe Crohn's disease who were selected for having a response to open-label induction therapy.\n\n【13】Methods\n-------\n\n【14】A committee of academic investigators and UCB Pharma scientists (the Study Advisory Board) designed the study. Data were collected by a contract research organization (ICON Clinical Research) and analyzed by the sponsor and external experts. The academic authors vouch for the veracity and completeness of the data and data analyses presented. Both the academic and industry authors wrote the first and subsequent drafts of the manuscript and hold the data, and the decision to publish was made by an academic author.\n\n【15】Patients\n--------\n\n【16】Our multicenter, randomized, double-blind, placebo-controlled trial was conducted worldwide at 147 centers, from February 2004 until May 2005. The protocol was approved by the institutional review board or ethics committee at each center. All patients gave written informed consent.\n\n【17】Adults were eligible if they had a 3-month history of active Crohn's disease, defined as a Crohn's Disease Activity Index (CDAI) score of 220 to 450.  The CDAI is a weighted, composite index of eight items (stool frequency, severity of abdominal pain, degree of general well-being, presence or absence of extraintestinal manifestations or fistula, use or nonuse of antidiarrheal agents, presence or absence of an abdominal mass, hematocrit, and body weight), with scores ranging from 0 to 600 and higher scores indicating more severe disease activity. Permitted concomitant therapies for Crohn's disease were stable doses of 5-aminosalicylates, 30 mg or less of prednisolone per day (or equivalent), azathioprine, 6-mercaptopurine, methotrexate, and antibiotics. Exclusion criteria were the presence of the short-bowel syndrome, ostomy, obstructive symptoms with strictures, abscess, history of tuberculosis, positive chest radiograph, positive tuberculin purified-protein-derivative (PPD) skin test, demyelinating disease, and cancer. (Exclusion criteria did not include a positive PPD skin test in combination with previous vaccination with bacille Calmette–Guérin and a negative chest radiograph.) Patients who had received any certolizumab pegol, who had received an anti-TNF agent or other biologic therapy within 3 months before enrollment, or who had a severe hypersensitivity reaction or no clinical response after initial dosing with an anti-TNF were also excluded. Reasons for discontinuation of any previous anti-TNF treatment were not reported.\n\n【18】Study Design\n------------\n\n【19】Eligible patients received induction therapy, consisting of subcutaneous injections of 400 mg of certolizumab pegol at weeks 0, 2, and 4. A clinical response was defined as a reduction of at least 100 from the baseline score on the CDAI; remission was defined as a CDAI score of 150 points or less.  Patients who had a response to induction therapy at week 6 were randomly assigned to receive 400 mg of certolizumab pegol (certolizumab group) or placebo (placebo group) at weeks 8, 12, 16, 20, and 24 and were followed through week 26. The study was centrally randomized, and group assignment was stratified according to serum CRP level (≥10 mg per liter or <10 mg per liter), concurrent use of glucocorticoids (yes or no), and concurrent use of immunosuppressive agents (yes or no). The randomization code consisted of eight separate lists (one for each stratum) and was generated by an independent contractor. Randomization was centralized by means of an interactive voice-recognition system. Patients and investigators were unaware of the group assignment.\n\n【20】Doses of concomitant medications were kept constant except for those of glucocorticoids, for which the dose could be reduced but no forced tapering was required. Any escalation of dose above baseline levels qualified as a failure of therapy.\n\n【21】Follow-up and Efficacy and Safety Evaluations\n---------------------------------------------\n\n【22】Patients were followed up at weeks 0, 2, 4, 6, 8, 12, 16, 20, 24, and 26. At each visit, diary data were collected, clinical assessments of Crohn's disease were undertaken, adverse events and the use of concomitant medications were recorded, and laboratory samples were collected. Data for determining the CDAI score were collected for 7 days by means of a diary card completed daily by the patient. Safety evaluations included the checking of vital signs, physical examinations, hematologic analysis, serum biochemistry tests, and urinalysis. Antibodies against certolizumab pegol were assayed with the use of previously published methods, with a lower detection limit of 2.4 U per milliliter (an increase by a factor of 2 over the level in a reference population).  CRP measurements were made at a central laboratory (MDS Pharma Services, Central Lab) (normal range, 0 to 4 mg per liter). The health-related quality of life was assessed at weeks 0, 6, 16, and 26 with the use of the Inflammatory Bowel Disease Questionnaire, scores for which range from 32 to 224, with higher scores indicating a better quality of life. \n\n【23】Statistical Analysis\n--------------------\n\n【24】The primary end point was a clinical response at week 26 in patients with a baseline CRP level of at least 10 mg per liter. Secondary end points included response at week 26, remission at week 26 in the intention-to-treat population, and remission in the group with a baseline CRP level of at least 10 mg per liter. If patients received rescue therapy during the study, their treatment was considered to have failed, starting at the time of the administration of the first rescue therapy. The intention-to-treat population included all patients who were randomly assigned to a study group, who received at least one injection of certolizumab pegol or placebo, and who had at least one efficacy evaluation after the first double-blind injection.\n\n【25】Percentages of patients having a response or remission were compared between the two study groups with the use of logistic regression (with adjustment for geographic region, use of glucocorticoids, use of immunosuppressive agents, and baseline CRP level in the intention-to-treat population only). A closed test procedure was used to control for multiple comparisons across secondary end points.  A two-sided significance level of 0.05 was used in all comparisons. Hypothesis testing of major secondary variables was performed only if the primary end point was significant. Baseline characteristics were compared between the two study groups with the use of either the chi-square test or Fisher's exact test for categorical variables or analysis of variance for continuous variables. Safety variables were compared between the two groups with the use of Fisher's exact test. All efficacy analyses were performed according to the intention-to-treat principle. Post hoc analyses were performed to determine the CDAI scores with the last observation carried forward for each visit and to explore the effect of certolizumab pegol as compared with placebo in subpopulations receiving concomitant glucocorticoids, receiving concomitant immunosuppressants, or that previously received infliximab.\n\n【26】We anticipated that 55% of 712 patients receiving induction treatment with certolizumab pegol would have a response at week 6. We calculated that the random assignment of 392 patients who had a response (196 patients with a CRP level ≥10 mg per liter and 196 with a CRP level <10 mg per liter) would result in a statistical power of 80% to detect a difference in the response rate during the maintenance phase, assuming a rate of 45% in the certolizumab group and 25% in the placebo group.\n\n【27】Results\n-------\n\n【28】Patients\n--------\n\n【29】Figure 1. Enrollment, Group Assignments, and Follow-up.\n\n【30】The 668 patients with moderate-to-severe Crohn's disease received induction therapy with certolizumab pegol, 400 mg subcutaneously, at weeks 0, 2, and 4. Of these patients, 428 had a response to induction therapy at week 6 and were randomly assigned to receive maintenance therapy with either certolizumab pegol or placebo and were stratified according to whether their baseline CRP level was at least 10 mg per liter or less than 10 mg per liter. The intention-to-treat population consisted of 425 patients who had a response to the induction therapy with certolizumab pegol and who were not excluded owing to possible unblinding of group assignment, 210 of whom received maintenance placebo and 215 of whom received maintenance certolizumab pegol. Patients could have more than one reason for withdrawal after randomization.Table 1.  Table 1. Demographic and Disease Characteristics of Patients in the Intention-to-Treat Population, According to Type of Maintenance Therapy.\n\n【31】Figure 1 shows the disposition of the study patients. Baseline demographic and disease characteristics were similar in the two study groups. Twenty-four percent (103 of 425) of patients in the intention-to-treat population had previously received infliximab .\n\n【32】Efficacy\n--------\n\n【33】### _Response to Induction at Week 6_\n\n【34】At week 6 (after three 400-mg doses of certolizumab pegol), 64% of patients (428 of 668) had a response and 43% (289 of 668) had a remission. The remaining 36% of patients who did not have a response (240 of 668) were not followed further for efficacy analyses.\n\n【35】### _Primary End Point_\n\n【36】Figure 2. Efficacy of Maintenance Therapy with Certolizumab Pegol in Patients with a Response to Induction Therapy at Week 6.\n\n【37】Panel A shows the percentage of patients with a baseline CRP serum level of at least 10 mg per liter whose response (defined as a reduction of ≥100 from the baseline score on the CDAI) at week 6 was maintained at week 26. Panel B shows the percentage of all patients in the intention-to-treat population who had a response to induction therapy at week 6 who had a response at week 26. Panel C shows the response over time in patients in the intention-to-treat population who had a response to induction therapy at week 6, as well as the response in the subgroup of patients with a CRP level of at least 10 mg per liter. The percentage of patients who had a response at week 26 was greater in the certolizumab group than in the placebo group in both populations (P<0.001). The 95% confidence intervals for the certolizumab group and for the placebo group did not overlap at any time point in the intention-to-treat population and did not overlap at weeks 16 through 26 in the high-CRP subgroup. Panel D shows the percentage of patients with clinical remission (defined as a CDAI score ≤150) at week 26 in the intention-to-treat population and in the subgroup of those with baseline CRP serum levels of at least 10 mg per liter. Panel E shows the median CDAI scores (as calculated with the use of the last-observation-carried-forward method) over time in the intention-to-treat population. The median score was significantly lower in the certolizumab group than in the placebo group at week 16 (P=0.03), week 20 (P=0.02), week 24 (P=0.008), and week 26 (P<0.001).\n\n【38】In total, 213 patients (32% of the 668 patients who entered the induction phase and 50% of the 428 who had a response to induction therapy) had a baseline serum CRP level of at least 10 mg per liter. During the maintenance phase, of these 213 patients, 62% in the certolizumab group (69 of 112) had a response at week 26, as compared with 34% in the placebo group (34 of 101) (P<0.001) .\n\n【39】### _Secondary End Points_\n\n【40】Of the 428 patients in the intention-to-treat population who had a response to induction therapy at week 6, 63% in the certolizumab group (135 of 215) also had a response at week 26, as compared with 36% (76 of 210) in the placebo group (P<0.001) . The difference in response between treatment with certolizumab pegol and treatment with placebo was sustained throughout the maintenance phase, and the magnitude of the difference was similar in the intention-to-treat population and in patients with a baseline CRP serum level of at least 10 mg per liter . The response rate during the maintenance phase of 63% of patients in the intention-to-treat population who had a response to induction therapy at week 6 is equivalent to a combined response rate during the induction and maintenance phases of 40% (with a combined remission rate of 31%) in the 668 patients who entered the induction phase.\n\n【41】Among all patients in the intention-to-treat population who had a response at week 6, remission rates at week 26 were 48% (103 of 215) in the certolizumab group and 29% (60 of 210) in the placebo group (P<0.001) . In the subgroup with baseline CRP serum levels of at least 10 mg per liter, remission rates at week 26 were 42% (47 of 112) in the certolizumab group and 26% (26 of 101) in the placebo group (P=0.01) . Figure 2E depicts the results of an exploratory analysis, showing a reduction of the median CDAI scores in the intention-to-treat population after induction therapy with certolizumab pegol and the gradual rise in disease activity during the maintenance phase in the placebo group, as compared with the certolizumab group.\n\n【42】Fourteen percent of patients in the intention-to-treat population who had a response to induction therapy with certolizumab pegol (58 of 425) had draining fistulas at baseline (28 patients in the certolizumab group and 30 patients in the placebo group). During the study, of the 58 patients, 54% (15 of 28 patients) in the certolizumab group had fistula closure (defined as the absence of drainage on gentle compression at any two consecutive post-baseline visits at least 3 weeks apart), as compared with 43% (13 of 30) in the placebo group.\n\n【43】Among patients who had a response to induction therapy, a significantly greater percentage of patients in the certolizumab group (60% \\[129 of 214 patients\\]) also had a response as measured with the use of the Inflammatory Bowel Disease Questionnaire (an increase in the total score of at least 16 points) at week 26, as compared with those in the placebo group (43% \\[90 of 210 patients\\]) (P<0.001). Mean scores on the questionnaire were 123 at week 0 and 175 at week 6 among patients in the intention-to-treat population; during the maintenance phase, at week 16 mean scores were 169 in the certolizumab group and 158 in the placebo group and at week 26 they were 176 and 168, respectively. Adjusted mean scores were 170 in the certolizumab group and 162 in the placebo group (P=0.008) at week 16 and 171 and 163 (P=0.007), respectively, at week 26.\n\n【44】### _Exploratory Analyses_\n\n【45】In the intention-to-treat population, response rates at week 26 were significantly greater among patients receiving maintenance therapy with certolizumab pegol than among those receiving placebo, both for patients receiving concomitant immunosuppressive agents (61% \\[53 of 87\\] vs. 33% \\[28 of 86\\], P<0.001) and those not receiving concomitant immunosuppressive agents (64% \\[82 of 128\\] vs. 39% \\[48 of 124\\], P<0.001). A significant difference in the response rates at week 26 between the certolizumab group and the placebo group was found for patients who had previously received infliximab (44% \\[23 of 52\\] vs. 25% \\[13 of 51\\], P=0.02) and those who had not previously received infliximab (69% \\[112 of 163\\] vs. 40% \\[63 of 159\\], P<0.001). Neither the response rate nor the remission rate differed significantly between the two groups for patients who were and those who were not receiving concomitant glucocorticoids, immunosuppressive agents, or both.\n\n【46】For rates of response and remission, there were no significant interactions between group assignment and smoking status or body-mass index. A response was maintained at 26 weeks in 56% of current smokers (36 of 64 patients) in the certolizumab group and 34% in the placebo group (26 of 76 patients).\n\n【47】Among patients with a baseline CRP serum level of less than 10 mg per liter, response rates at week 26 were significantly greater in the certolizumab group (64% \\[66 of 103\\]) than in the placebo group (39% \\[42 of 109\\], P<0.001). Remission rates were 54% (56 of 103 patients) in the certolizumab group, as compared with 31% (34 of 109 patients) in the placebo group (P<0.001).\n\n【48】Safety\n------\n\n【49】Table 2. Summary of Adverse Events in the Safety Population.\n\n【50】Serious adverse events occurred in 6% (12 of 216) of patients in the certolizumab group and 7% (14 of 212) of those in the placebo group . Serious infection occurred in 3% (6 of 216) of patients in the certolizumab group and in less than 1% (2 of 212) of patients in the placebo group. The most frequently reported adverse events were similar in the two groups. One patient died of an accidental fentanyl overdose during the open-label phase of this trial. No deaths, solid-organ or hematologic cancers, demyelinating diseases, or lupus occurred during the maintenance phase. Despite the absence of a reaction to the PPD skin test at screening, one patient treated with five doses of certolizumab pegol and concomitant azathioprine had pulmonary tuberculosis, which responded to standard combination therapy with antibiotics. One or more local reactions to injection occurred in 3% (6 of 216) of patients in the certolizumab group and 15% (31 of 212) of patients in the placebo group .\n\n【51】### _Autoimmune Antibodies_\n\n【52】Among patients for whom values were available at both the baseline visit and the final visit, new antinuclear antibodies developed in 8% (16 of 192) of patients receiving certolizumab pegol and in 1% (2 of 178) of patients receiving placebo. New antibodies against double-stranded DNA developed in 1% (2 of 192) of patients in the certolizumab group and in 1% (1 of 178) of patients in the placebo group.\n\n【53】### _Antibodies against Certolizumab Pegol_\n\n【54】Table 3. Summary of Data on Antibodies against Certolizumab Pegol, According to Use of Immunosuppressive Agents.\n\n【55】Of the 668 patients who entered the induction phase, 58 (9%) had detectable antibodies against certolizumab pegol at some point during the study . Three of the four patients in whom antibodies developed during the induction phase were randomly assigned to receive maintenance therapy, one with certolizumab pegol and two with placebo. In the remaining 54 patients, antibodies developed during the maintenance phase. Eighty percent of the antibodies were neutralizing antibodies. Rates of antibody formation were low in patients who received concomitant immunosuppressive agents and in those who received continuous therapy with certolizumab pegol (during the induction and maintenance phases) .\n\n【56】The presence of certolizumab pegol in vivo may have interfered with the antibody assay because it can form complexes with drug antibodies. Therefore, in analyses of data for the certolizumab group during the maintenance phase, we also used a reduced-trough plasma certolizumab pegol level as an indirect measure of the presence of antibodies. The number of patients who were positive for antibodies against certolizumab pegol increased from the 17 detected with the use of the assay to 21. As a result, the number of patients who may have had false negative results on the antibody assay, owing to interference by circulating levels of certolizumab pegol forming complexes with drug antibodies, was low. Of the 17 patients with positive tests for antibodies against certolizumab pegol, 12 (71%) had a response through week 26, as compared with 121 of 196 patients (62%) with negative tests for the antibodies.\n\n【57】Discussion\n----------\n\n【58】The results of our trial show that continued administration of certolizumab pegol subcutaneously every 4 weeks is superior to administration of placebo in the 64% of our patients with moderate-to-severe active Crohn's disease (despite the use of conventional treatments) who had a response to 6 weeks of induction therapy with 400 mg of certolizumab pegol (given at weeks 0, 2, and 4). The combined response rate during the induction and maintenance phases was 40%. However, the open-label study design results in an overestimation of the rate of induction of a response, and response rates to placebo are substantial among patients with Crohn's disease. Contrary to a finding in a previous phase 2 study,  continued treatment with certolizumab pegol was equally effective in patients with high baseline CRP serum levels and those with low levels, and the response rate at week 26 did not depend on the baseline CRP level. Long-term improvement in the health-related quality of life was greater in the certolizumab group than in the placebo group. Antinuclear antibodies developed in 8% of patients treated with certolizumab pegol, an incidence that does not exceed that in healthy persons.  Antibodies to certolizumab pegol developed in 9% of patients.\n\n【59】Since only 14% of patients had draining fistulas and inclusion in the study was based primarily on the overall disease activity, our trial was not powered to examine the efficacy of certolizumab pegol for fistula closure. Additional studies will be required to address this issue.\n\n【60】Our results also show that certolizumab pegol was effective in patients who had previously taken infliximab. The efficacy of maintenance treatment with certolizumab pegol was also seen in patients with and those without concomitant therapy with immunosuppressants. The development of antibodies against certolizumab pegol during the maintenance phase differed modestly between patients receiving and those not receiving concomitant immunosuppressive agents (2% vs. 12% in the certolizumab group and 8% vs. 24% in the placebo group).\n\n【61】Maintenance therapy with certolizumab pegol was associated with a safety profile that was consistent with profiles in previous studies.  Two patients in the placebo group and six in the certolizumab group had serious adverse events consisting of infection, including one case of pulmonary tuberculosis. No solid-organ or hematologic cancers, demyelinating disease, or lupus were reported during the maintenance phase. With regard to the patient who died of a fentanyl overdose, to our knowledge there is no specific interaction between fentanyl and certolizumab pegol. Ongoing follow-up of patients in open-label treatment programs has not resulted in any new safety signals to date. Longer-term exposure of large cohorts of patients will be required to better characterize the adverse events associated with certolizumab pegol.\n\n【62】In conclusion, patients with moderate-to-severe active Crohn's disease who had a response to induction therapy with certolizumab pegol and continued to receive certolizumab pegol as maintenance therapy were more likely to have a maintained response and remission at 26 weeks than were those who switched to placebo.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "95a17371-588d-43a1-bfd4-ff367a1b245c", "title": "Congenital Adrenal Hyperplasia Due to 21-Hydroxylase Deficiency", "text": "【0】Congenital Adrenal Hyperplasia Due to 21-Hydroxylase Deficiency\nCongenital adrenal hyperplasia, a common autosomal recessive disorder, is potentially life-threatening in its classic form and may be asymptomatic or cause female infertility in its nonclassic form. This review focuses on CAH due to 21-hydroxylase deficiency.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b937fa27-c193-4366-8f6f-3f3714bb55a4", "title": "Platelet Survival in Patients with Rheumatic Heart Disease", "text": "【0】Platelet Survival in Patients with Rheumatic Heart Disease\nAbstract\n--------\n\n【1】Platelet studies were carried out in patients with valvular heart disease to determine the correlation of results with thromboembolism. Platelet survival was normal in 11 patients with aortic stenosis, and no thromboembolism occurred. In 55 patients with mitral stenosis, average platelet survival was slightly shortened. In 29, thromboembolism had occurred, and mean platelet survival was abnormal and significantly shorter than in the 26 patients without thromboembolism. Of patients with thromboembolism, 93 per cent had short survival as compared to 31 per cent of those without thromboembolism. Thromboembolism had occurred in 83 per cent of patients with short platelet survival as compared to 10 per cent in those with normal platelet survival. The results demonstrate that short platelet survival differentiates patients with mitral-valve disease and a history of thromboembolism from those without thromboembolism. Such studies done prospectively may identify thromboembolism-prone patients in whom platelet-inhibitor therapy may prove useful.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "a67a0ad5-538e-4245-9ba5-4004b9831f0b", "title": "“Is It Safe?” — The Many Unanswered Questions about Medications and Breast-Feeding", "text": "【0】“Is It Safe?” — The Many Unanswered Questions about Medications and Breast-Feeding\nMore than half of new mothers take one or more medications in the months after delivery. But health care providers often lack the evidence they need to counsel breast-feeding women on medications’ potential effects on lactation, on mothers, or on their babies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b42680da-4758-467b-9b7d-c5a8c6733be2", "title": "Radiologic Contributions to the Investigation and Prosecution of Cases of Fatal Infant Abuse", "text": "【0】Radiologic Contributions to the Investigation and Prosecution of Cases of Fatal Infant Abuse\nAbstract\n\n【1】In 1984 we started a two-year program in Worcester (Mass.) and Boston to provide additional radiologic data for the medical investigation of suspected fatal infant abuse. During that period the investigation of 12 cases of unexplained infant death included the review of complete radiographic skeletal surveys by a pediatric radiologist. Autopsies were supplemented with resection, high-detail radiography, and histologic study of all noncranial sites of suspected osseous injury.\n\n【2】Thirty-four bony injuries were noted, including 12 acute and 16 healing fractures of the long-bone metaphyses and posterior-rib arcs in patterns indicative of infant abuse.\n\n【3】The investigations determined that there were eight cases of abuse, two accidental deaths, and two natural deaths (sudden infant death syndrome). At this writing, the radiologic and osseous histologic studies appear to have influenced the determination of the manner of death in six of the eight cases of abuse and the criminal prosecution in four of the five convictions.\n\n【4】These findings suggest that a thorough postmortem radiologic evaluation followed by selected histologic studies can have an impact on the investigation and prosecution of cases of fatal infant abuse.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "18d52557-1fee-4c21-a818-1f4430e8396b", "title": "Hypokalemic Myopathy Induced by Giardia Lamblia", "text": "【0】Hypokalemic Myopathy Induced by Giardia Lamblia\nTo the Editor:\n--------------\n\n【1】Giardiasis is the leading cause of waterborne outbreaks of diarrhea in the United States,  yet its role as a potential cause of malabsorption in elderly patients may be underestimated  . We report a case of severe hypokalemic myopathy induced by chronic intestinal infection with Giardia lamblia.\n\n【2】Table 1. Laboratory-Test Results.\n\n【3】A 69-year-old man with a one-month history of progressive muscular weakness, diarrhea, and weight loss was hospitalized. Intermittent bouts of diarrhea had begun 10 years before, when G. lamblia cysts were found on a stool examination and metronidazole was given. The patient was a heterosexual industrial manager who had never traveled outside Spain. On physical examination, abnormal findings were limited to the neurologic examination. There was an intense and symmetrical muscular weakness of all four extremities and the neck. Muscle stretch reflexes were absent. Mentation, cranial nerves, and the results of sensory examination were normal. The erythrocyte sedimentation rate was 60 mm per hour. The hematologic findings were normal. Blood-chemical findings on admission and during the hospital course are presented in Table 1 . Tests for urinary myoglobin were positive. An electrocardiogram and an electroencephalogram were normal. Motor- and sensory-nerve conduction studies were normal, except for a moderately decreased amplitude of the compound muscle action potentials. Electromyography revealed fibrillation potentials, positive sharp waves, and low-amplitude, short-duration polyphasic motor-unit action potentials in proximal muscles. Repetitive stimulation at slow rates was normal, but facilitation of 30 percent appeared after 20 seconds of tetanization. Jitter measurement in the extensor digitorum communis muscle after axonal stimulus was normal (18.2 microsec).\n\n【4】These findings were consistent with impairment of muscle excitability and denervation due to muscle necrosis. Stools contained G. lamblia cysts. Treatment with gradual intravenous electrolyte supplementation and oral metronidazole was followed by steady clinical improvement. After three months, the patient had resumed work and gained 15 kg. He has remained asymptomatic for 15 months.\n\n【5】Symptomatic hypokalemia caused by diarrhea is uncommon in adults. Isolated cases of hypokalemic myopathy have been described in salmonella, strongyloides, and yersinia infections  . Giardiasis has been associated with a peripheral neuropathy of obscure origin  . Infestation with G. lamblia should be considered as a potential cause of malabsorption-induced hypokalemia and rhabdomyolysis in elderly patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "15abbbb5-a412-4693-9b93-0392a26a5d58", "title": "Larotrectinib-Enhanced Radioactive Iodine Uptake in Advanced Thyroid Cancer", "text": "【0】Larotrectinib-Enhanced Radioactive Iodine Uptake in Advanced Thyroid Cancer\nTo the Editor:\n--------------\n\n【1】Differentiated thyroid carcinomas may lose their capacity to take up iodine as a consequence of oncogene activation through signaling pathways. Loss of differentiation markers may be the result of epigenetic alterations that can be acquired and can be reversible. With the development of pharmacologic inhibitors that target oncogenic driver mutations or downstream-activated signaling pathways, some agents have become available that may restore the ability of thyroid-cancer cells to take up radioactive iodine (RAI). This effect was first shown with selumetinib, a mitogen-activated protein kinase (MAPK) pathway inhibitor, and later with BRAF inhibitors in the treatment of tumors harboring a _BRAF_ mutation.  Whether it is possible to apply this strategy to oncogenic fusion genes is unknown.\n\n【2】Figure 1. Diagnostic Scans Obtained before and after Larotrectinib Treatment.\n\n【3】Images were obtained with the use of iodine-131 (300 to 370 MBq \\[8 to 10 mCi\\]) before treatment with larotrectinib and 3 weeks after the initiation of treatment. Anterior images of whole-body scans show restored iodine uptake after administration of larotrectinib in almost all lung metastases that had previously shown no uptake . Substantially increased iodine uptake after larotrectinib treatment can also be observed in fused frontal  and axial  chest images.\n\n【4】We report the results of treatment with iodine-131 in a 64-year-old female patient with a 34-year history of papillary thyroid cancer with synchronous lymph-node and lung metastases. After eight RAI treatments (cumulative dose of 52.0 GBq \\[1405.4 mCi\\]), long-term control of the cancer was maintained for 12 years, with stable suppression of the serum thyroglobulin level at close to 15 ng per milliliter and a nonprogressive course of miliary lung lesions. At 12 years after RAI treatment, the patient had a late relapse of symptoms with a high metastatic tumor burden that was treated with pleural drainage and rapid initiation of treatment with lenvatinib, a vascular endothelial growth factor receptor inhibitor. The treatment resulted in a partial response according to the Response Evaluation Criteria in Solid Tumors, version 1.1, but there were high-grade adverse effects despite dose adjustment. A fusion gene ( _EML4-NTRK3_ ) was identified, and treatment with the selective NTRK inhibitor larotrectinib (100 mg administered twice daily) was initiated.  Diagnostic RAI scintigraphy performed during lenvatinib treatment showed no substantial RAI uptake in the lungs, and RAI scintigraphy performed 3 weeks after the initiation of larotrectinib treatment showed restoration of RAI uptake . Larotrectinib induced a partial response at 2 months, and there were minor adverse effects.\n\n【5】There is no preclinical in vitro thyroid model to assess the capacity of NTRK inhibitors to promote redifferentiation.  This case shows that larotrectinib may restore RAI uptake by inhibiting signaling pathways activated by _EML4-NTRK3_ in a manner similar to the action of MAPK pathway inhibitors.  This reactivation of iodine uptake indicates that retreatment with RAI may be considered in patients who are receiving larotrectinib. Because of the response in our patient to larotrectinib alone and the high level of previous radiation exposure, we have not undertaken RAI retreatment in this patient to date.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ea81348b-ba4f-4812-832e-4b3ea1ed245a", "title": "Eculizumab for the Treatment of Dense-Deposit Disease", "text": "【0】Eculizumab for the Treatment of Dense-Deposit Disease\nTo the Editor:\n--------------\n\n【1】Dense-deposit disease (also known as membranoproliferative glomerulonephritis type II) is a rare glomerulopathy characterized by electron-dense deposits in the glomerular basement membrane as well as the glomerular deposition of complement. Within 10 years, the disease progresses to end-stage kidney disease in approximately 50% of patients, and it frequently recurs after kidney transplantation. Dysregulation of the alternative complement pathway, resulting from mutations in the factor H gene or the activation of autoantibodies, plays a pathogenic role in the disease. In animal models, the uncontrolled activation of C5 contributes substantially to renal lesions.  Such studies provide the rationale for the use of eculizumab, a humanized anti-C5 monoclonal antibody, as therapy for dense-deposit disease.\n\n【2】We report the case of a 17-year-old patient with dense-deposit disease, relapsing proteinuria, microhematuria, and low C3 levels . Seven years after disease onset, proteinuria increased, with a urinary protein:creatinine ratio of 2 to 3 (proteinuria, 3.5 to 5.5 g of protein per 24 hours). Focal sclerosis in 40% of glomeruli was confirmed on analysis of a renal biopsy specimen. Renal function and blood pressure were normal, as were levels of factor H and factor B. No mutations in _MCP, CFH,_ or _CFI_ were detected, but there were _CFB_ polymorphisms (C94T/R32W and G95A/R32Q). A test for circulating C3 nephritic factor was positive.\n\n【3】Figure 1. Change in the Patient's Urinary Protein:Creatinine Ratio, the Glomerular Deposition of C3 and C5b-9, and the Thickness of Glomerular Capillary Walls before Treatment and after 6 and 18 Months of Treatment.\n\n【4】Panel A shows the changes in the patient's urinary protein:creatinine ratio (mg/mg), starting 6 weeks before the first eculizumab infusion. The shaded areas represent the time during which the patient was undergoing treatment with eculizumab and the white areas the time before therapy began and the period during which therapy was discontinued. Panel B shows stained specimens from renal biopsies performed before treatment with eculizumab and at 6 and 18 months during the first treatment period. During this period, the clearance of C3 and C5b-9 deposition is nearly complete (C3 is shown in green fluorescence and C5b-9 in red fluorescence; nuclei are stained in blue); histologic improvement is revealed with periodic acid–Schiff staining. The far right column shows the progressive reduction in the thickness of the glomerular capillary walls in response to treatment. Capillary thickness was estimated by observing the width of positive periodic acid–Schiff staining in all peripheral capillary loops of analyzed glomeruli; the number of capillaries sampled at the time of each biopsy is provided.\n\n【5】Eculizumab was administered, and treatment with ramipril and losartan was maintained. After initiation of eculizumab, proteinuria decreased and plasma protein levels increased . Microhematuria decreased to trace levels. Repeat renal biopsies were performed 6 and 18 months after eculizumab was started ; analysis of the specimens showed progressive reduction in mesangial proliferation and a reduction in the thickness of glomerular capillary loops . According to electron microscopy, dense deposits appeared to be reduced at 18 months . Immunofluorescence studies showed progressive reduction of C3 and C5b-9 deposits . However, progression of glomerular sclerosis was noted and tubular atrophy observed.\n\n【6】After 18 months, treatment was interrupted, and proteinuria rapidly increased. Eculizumab therapy was resumed 6 months later, and proteinuria decreased . Treatment is ongoing. At the time of our last observation, the patient had a urinary protein:creatinine ratio of 0.46 (proteinuria, 0.97 g of protein per 24 hours), normal plasma protein levels, no microhematuria, persistently low C3 levels, and normal renal function and blood pressure. Throughout the period of treatment with eculizumab we observed no major side effects.\n\n【7】Current knowledge indicates that different mechanisms lead to the development of the lesions associated with dense-deposit disease.  Our observations show that at least in some patients with this disease, C5 activation participates in the physiopathology of the disease. In such patients, the administration of anti-C5 antibodies may represent a valuable therapeutic option, particularly if it is started early, before significant sclerosis has developed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d57b9d59-b21b-4fa0-945b-f86da66f4df2", "title": "Reentry", "text": "【0】Reentry\nArticle\n-------\n\n【1】There were two intensive care unit (ICU) physician teams and countless nurses and respiratory therapists outside Mr. K.’s room when I arrived on the unit. The attending intensivist filled me in. “He’s maxed out on ECMO and pressors. He has a tension pneumothorax. My fellow is about to do a needle decompression at bedside. We’ve started methylene blue. There’s literally nothing else I can do. He has so many kids.”\n\n【2】We set up a Zoom call with Mr. K.’s family. “I wish I had better news,” I told them. “He’s dying. We’re doing absolutely everything we can. I’m worried time is really short.”\n\n【3】His niece, who was a physician and the family spokesperson, inhaled deeply. “Thank you for all you are doing. Would it be possible for us to pray with him?” They texted me specific verses of the Qur’an and a YouTube video with psalms. I texted John, our chaplain. _John, are you here? I need your help ASAP_ .\n\n【4】John stayed at the bedside for the next 2 hours while Mr. K.’s family prayed for him and said their goodbyes. As Mr. K.’s blood pressure was bottoming out and his heart rate was dropping, a stack of pizzas arrived in the ICU. A token of appreciation from Mr. K.’s family.\n\n【5】From March to June 2020, I led a palliative care team embedded in our hospital’s Covid ICU. We spoke to countless families over the phone and by Zoom calls to tell them their loved ones were critically ill, getting sicker, and eventually, dying. When the prognosis seemed dire, we recommended transitioning to comfort-focused care. And in patients’ final hours and days, we held iPads at their bedsides so that family members around the world could say goodbye.\n\n【6】When the surge abated and our Covid ICUs began to shut down, I couldn’t help but feel an overwhelming sense of relief. _We did it,_ I told myself. _We made it to the other side._ I was wholly unprepared for what came next.\n\n【7】The weeks since Boston temporarily beat back Covid have been more difficult than I ever expected. When I fall asleep, I’m haunted by memories of patients I cared for but never met. I hear their families’ wails, expressions of gratitude and, most often, stunned silence. I can’t remember their names, but I also can’t forget their stories.\n\n【8】I’ve also felt a growing sense of anger and resentment as our hospital has slowly been returning to its “new normal.” My heart rate jumps each time I hear someone talk about Covid’s silver linings or the opportunities we’ve found to reimagine our health system. The message has been clear. We’ve moved on. But Covid continues to live in my muscles. It permeates my marrow. My 5-year-old tells me I’m “not the same Daddy.” How can we speak about transforming our system when we haven’t yet reckoned with how Covid has transformed those of us on the front lines?\n\n【9】Beyond the anger and sadness, though, I’m left with the skeletons of guilt and regret. Fear shrouded the early days of the surge. In the hospital, we worried about crisis standards of care and shortages of personal protective equipment. As palliative care consultants, we resolved to work with the clinicians at the bedside in the ICU to make decisions based on our patients’ prognoses and their goals and values, not bed or ventilator availability.\n\n【10】Our approach was as thoughtful as it could have been, but we didn’t know enough. Along with colleagues from critical care and infectious disease, we tried to extrapolate information about our patients’ prognoses from existing data about similar viral lung infections, prolonged respiratory failure, and experience with Covid in Wuhan, Lombardy, Seattle, and New York. We did our best. We weren’t always right.\n\n【11】Late in the afternoon on the day Mr. K. died, I was back in the ICU. Next to empty pizza boxes on the conference-room table, I sat beside my fellow as she spoke with another patient’s family on the phone. When they told her they didn’t want him to suffer, she said, “I’m really worried he might die from this. I wonder if we should focus our effort on keeping him comfortable for the short time he has left.”\n\n【12】“Thank you,” his sister said. “But let’s give him more time. Keep doing what you’re doing.” The man remained critically ill and near death for another 2 weeks. But then his numbers started to improve. He needed less support from the ventilator. Eventually, he was discharged to rehab. In other words, he lived, despite our intervention. How many families did we advise to transition to comfort-focused care when their loved ones might have pulled through after all?\n\n【13】We experienced the opposite situation, too. I told one mother that her son was getting better and that we were finally becoming cautiously optimistic about how he was doing. He died suddenly a few days later. In palliative care, our most basic intervention aims to help patients and families see what’s coming. In this case, I failed. I worry about the consequences for my patient’s mother as she grieves. \n\n【14】There were also families we kept in a terrible limbo. We didn’t have enough information to make a recommendation, so we waited, only to be faced, weeks later, with the same gut-wrenching decisions about whether to transition to comfort-focused care. Did my ignorance unnecessarily prolong these families’ suffering? If it all happened again right now, would I be better equipped?\n\n【15】Over time, perhaps we’ll uncover the toll Covid-19 has taken on clinicians. The data emerging from China are sobering. In one cohort of health care workers, nearly 70% faced adverse psychological consequences of working on the front lines.  Inundated with reports of new waves of Covid battering our colleagues around the United States, I worry that we’re all at increased risk for anxiety, depression, and post-traumatic stress disorder.\n\n【16】In hospitals where the Covid numbers are down, leaders rightly celebrate their staff’s mobilization and shared sense of purpose in weathering the surge. But their mission is far from accomplished. Laudatory emails and vague referrals to employee assistance programs do little to lessen the pain and isolation many of us are feeling.\n\n【17】It’s impossible for me to refrain from continually adding to the list of what I wish I had done differently. I know I’m being unfair to my past self, since memory is an unreliable narrator. We want the stories we tell ourselves to be clean and linear, so we remember yesterday’s emotions with today’s knowledge. When a situation evolves as quickly as this pandemic, we can rely only on existing evidence, however imperfect, and our best clinical judgment. Staring backward through the black hole of the surge, the path from beginning to end seems clear. In reality, I know we were able to see only a few feet in front of us.\n\n【18】While we try to heal from the collective trauma we experienced in our hospital over the worst 4 months of the local epidemic, I struggle to know where to look. Peering into the future, given what we see on the news, saturates me with dread. And yet it’s too early to look back. Perspective can’t develop in the presence of open wounds.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "346cc628-bc1c-4626-a602-ccc994551022", "title": "Case 21-2006 — A 61-Year-Old Man with Left-Sided Facial Pain", "text": "【0】Case 21-2006 — A 61-Year-Old Man with Left-Sided Facial Pain\nA 61-year-old man was seen in the neurosurgery clinic because of facial pain. The pain had begun eight months earlier and was characterized by sharp paroxysms over the left side of the face that were brought on by the patient's eating or touching his moustache; there was a dull, throbbing pain in the same area. The neurologic examination was normal. A diagnostic and therapeutic procedure was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "9dcda03f-b54a-476a-a520-7f056434c613", "title": "A Study of Occupational Exposure to Antineoplastic Drugs and Fetal Loss in Nurses", "text": "【0】A Study of Occupational Exposure to Antineoplastic Drugs and Fetal Loss in Nurses\nAbstract\n--------\n\n【1】In a case–control study, we examined the relation between fetal loss and occupational exposure to antineoplastic drugs in nurses in 17 Finnish hospitals. The pregnancies studied occurred in 1973 through 1980 and were identified using three national sources: the Central Register of Health Care Personnel, the Hospital Discharge Registry, and policlinic data. Each nurse with fetal loss was matched with three nurses who gave birth. Data on health and exposure were obtained by self-administered, mailed questionnaires; a response rate of 87 per cent was achieved after three mailings. A statistically significant association was observed between fetal loss and occupational exposure to antineoplastic drugs during the first trimester of pregnancy: odds ratio = 2.30 (95 per cent confidence interval, 1.20 to 4.39). Analyses suggested associations between fetal loss and cyclophosphamide, doxorubicin, and vincristine, although the independent effect of each individual drug could not be specifically identified, since many nurses reported handling more than one of these agents. The results of this study, combined with existing data on animals and human beings, suggest that caution be exercised in the handling of these valuable drugs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "57016ee2-3167-47ec-bded-730cba9f4eb0", "title": "Tipping Clonal Hematopoiesis into Transformation", "text": "【0】Tipping Clonal Hematopoiesis into Transformation\nClonal hematopoiesis is a risk factor for acute myeloid leukemia and other hematologic cancers. Analysis of a mouse model shows that progression to cancer is associated with somatic epigenetic changes in addition to mutations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b1ca9b6e-b15d-4e13-bc1f-b32bb06b5091", "title": "Bisphosphonates and Ocular Inflammation", "text": "【0】Bisphosphonates and Ocular Inflammation\nTo the Editor:\n--------------\n\n【1】Bisphosphonates are used to inhibit bone resorption in postmenopausal women and to manage hypercalcemia associated with osteolytic bone cancer, metastases of breast cancer, multiple myeloma, and Paget's disease of bone. Pamidronate disodium can cause uveitis, nonspecific conjunctivitis, episcleritis, or scleritis.  The drug must be discontinued to resolve the scleritis. Other bisphosphonates available in the United States include alendronate sodium (Fosamax, Merck), risedronate sodium (Actonel, Procter & Gamble), zoledronic acid (Zometa, Novartis), etidronate disodium (Didronel, Procter & Gamble), and tiludronate disodium (Skelid, Sanofi Winthrop). Bisphosphonates not approved by the Food and Drug Administration (FDA) include clodronate disodium (Ostac, Roche, and Bonefos, Aventis Pharma), ibandronate (Roche and GlaxoSmithKline), and olpadronate (Gador Pharmaceuticals). The _Physicians' Desk Reference for Ophthalmology_  does not list ocular inflammation as a potential side effect of the bisphosphonates marketed in the United States.\n\n【2】Table 1. Bisphosphonates and Adverse Ocular Effects.\n\n【3】Because of the uniqueness of the reported reactions of the visual system to pamidronate disodium, we reviewed the data on other bisphosphonates available in the United States and abroad .  These data suggest that, in rare instances, this class of medicine can cause serious ocular side effects. The variance in the number of ocular side effects reported for the different bisphosphonates probably reflects the individual popularity of each medication as well as the timing of the FDA's approval of the medication for marketing. A finding of particular importance to clinicians is that no case of unilateral or bilateral scleritis that developed in a person receiving bisphosphonates resolved, regardless of therapy, until the bisphosphonate was discontinued.\n\n【4】Data from spontaneous reporting systems can be suspect; nevertheless, the temporal relation and overall pattern of ocular side effects in patients receiving other bisphosphonates were very similar to those reported for pamidronate. In addition, we are unaware of any reports of scleritis caused by any other class of medication. \n\n【5】We suggest the following guidelines for the care of patients receiving bisphosphonates. First, patients with vision loss or ocular pain should be referred to an ophthalmologist. Second, nonspecific conjunctivitis seldom requires treatment and usually decreases in intensity during subsequent exposure to a bisphosphonate. Third, more than one ocular side effect can occur at the same time; for instance, episcleritis may occur in conjunction with uveitis. In some instances, the drug may need to be discontinued in order for the ocular inflammation to resolve. Finally, for scleritis to resolve, even during full medical therapy, the bisphosphonate must be discontinued.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "82d2b5a1-2c11-40fa-9201-4fc475fdc101", "title": "Reappraisal of Temporary Levodopa Withdrawal (Drug Holiday) in Parkinson's Disease", "text": "【0】Reappraisal of Temporary Levodopa Withdrawal (Drug Holiday) in Parkinson's Disease\nAbstract\n--------\n\n【1】Transient withdrawal of therapy has been advocated as a method of dealing with the complications of long-term use of levodopa in the treatment of Parkinson's disease. We retrospectively examined the effect of a 10-day period of levodopa withdrawal, or \"drug holiday,\" in 28 patients. We then compared the subsequent clinical course of these patients over one year with that of 30 other randomly selected, similar patients with Parkinson's disease. In both groups the disease progressed; there was no difference in disease severity, capacity for daily living activities, or total amounts of dopamine agonists eventually used. For some patients, it was possible to reduce dopamine agonists used immediately after the drug holiday without causing deterioration, but a pulmonary embolus and other complications occurred. Subsequent complications related to long-term dopamine-agonist therapy during the follow-up period were similar in the two groups. This investigation indicates that a drug holiday carries some risk and does not improve the efficacy of levodopa therapy or prevent the problems that occur with long-term administration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "b8bef318-9cdb-419f-aaf3-80e5bfb8a7c1", "title": "Case 33-2018: A 57-Year-Old Man with Confusion, Fever, Malaise, and Weight Loss", "text": "【0】Case 33-2018: A 57-Year-Old Man with Confusion, Fever, Malaise, and Weight Loss\nA 57-year-old man with a history of tobacco smoking presented with confusion, cough, fever, and unintentional weight loss. An evaluation revealed consolidation in the left lower lobe and emboli to the brain, heart, spleen, kidney, and lungs. A diagnostic test was performed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "5ddfa932-c901-433f-b69f-54857b3b4bb1", "title": "Cardiovascular Birth Defects and Antenatal Exposure to Female Sex Hormones", "text": "【0】Cardiovascular Birth Defects and Antenatal Exposure to Female Sex Hormones\nAbstract\n--------\n\n【1】In a cohort of 50,282 pregnancies 19 children with cardiovascular defects were born to 1042 women who received female hormones during early pregnancy (18.2 per 1000). Among 49,240 children not exposed in utero to these agents there were 385 with cardiovascular malformations (7.8 per 1000). Six children with cardiovascular defects were born to a subgroup of 278 women who used oral contraceptives during early pregnancy (21.5 per 1000). After the data were controlled for a wide variety of potentially confounding factors by multivariate methods, the association between in utero exposure to female hormones and cardiovascular birth defects was statistically significant (P<0.05).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "0f58957a-bff5-4ad8-9d9d-35a2e82d4f91", "title": "A Noninvasive Test to Determine Paternity in Pregnancy", "text": "【0】A Noninvasive Test to Determine Paternity in Pregnancy\nTo the Editor:\n--------------\n\n【1】Five percent of women who are raped become pregnant, which results in an estimated 32,000 pregnancies annually in the United States.  In many circumstances, it is unclear whether the pregnancy resulted from the rape or from consensual intercourse. The only options available for prenatal paternity determination are invasive tests, such as the sampling of chorionic villi and amniocentesis, that carry a risk of miscarriage and are not performed before 10 to 15 weeks of gestation. Because 78.9% of terminations of unintended pregnancies are carried out before 10 weeks,  it seems likely that many rape victims terminate pregnancies before testing for paternity. A noninvasive prenatal paternity test based on cell-free fetal DNA present in maternal blood, performed at 8 weeks of gestation or later, could provide a safe option for determining paternity.\n\n【2】Figure 1. The Use of Informative Single-Nucleotide Polymorphisms (SNPs) to Determine Paternity.\n\n【3】An informative SNP is one in which the mother and one of the two potential fathers are homozygous for the same allele, whereas the other potential father is homozygous for the alternative allele. For example, the maternal (M) DNA and paternal 2 (P2) DNA in Panel A are homozygous (AA genotype) and the paternal 1 (P1) DNA is homozygous (GG genotype) at this informative SNP. In the maternal plasma (PL), the fetal DNA has the maternal allele (A allele) and paternal allele (G allele) at the informative SNP site. Paternal 2 does not have a G allele and therefore does not match the fetal DNA signal in maternal plasma. Paternal 2 can thus be excluded as a potential father. In Panels B and C, sequencing gel images of two informative SNPs show that the man with paternal 2 DNA cannot be the father and that paternal 1 is the biologic father of the fetus.\n\n【4】Previous studies of noninvasive prenatal paternity testing have shown that amplification of fetal alleles from maternal blood is suppressed by the presence of cell-free maternal DNA.  Furthermore, fetal DNA in maternal plasma is highly degraded. These limitations can be overcome by first adding a fixative to maternal blood samples to stabilize cell membranes and prevent the release of maternal DNA into the plasma.  By using single-nucleotide polymorphisms to distinguish fetal DNA  from maternal DNA , one can use short amplicons (shorter than 75 bp) to minimize allele dropout (absence of a fetal DNA signal when one should be present).\n\n【5】We collected blood samples from 30 women with pregnancies of 8 to 14 weeks of gestation. Each maternal blood sample was paired with blood from the biologic father and then randomly grouped with 1 of 29 samples from unrelated men. The 3 samples in each group were processed in a blinded manner. We determined paternity correctly for all 30 samples, by comparing the genetic profile of fetal DNA in maternal blood with those of the 2 “paternal” samples (1 genuine, 1 not) . The odds of identifying the correct father for all 30 samples are less than 1 out of 1 billion (P=1.86×10 <sup>−9 </sup> ). Our approach shows that noninvasive prenatal paternity testing can be performed within the first trimester with the use of a maternal blood sample.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "ee0b2215-3de1-4415-b845-b0c97beebbed", "title": "Methicillin-Resistant ", "text": "【0】Methicillin-Resistant \nTo the Editor:\n--------------\n\n【1】Many isolates of community-acquired methicillin-resistant _Staphylococcus aureus_ (MRSA) produce Panton–Valentine leukocidin (PVL), increasing the virulence of the bacteria, which can cause disseminated deep abscesses and necrotizing pneumonia.  We report the transmission of PVL-positive MRSA between a symptomatic woman and both her asymptomatic family and their healthy pet cat.\n\n【2】An otherwise healthy woman presented with recurrent multiple deep abscesses. Swabs from several abscesses and nasal cultures grew MRSA that was resistant to both beta-lactam and fusidic acid antibiotics. Polymerase-chain-reaction assays for the PVL genes _lukS-PV_ and _lukF-PV_ were positive. The genotype of the staphylococcal chromosomal cassette was SCC _mec_ type IV. Nasal, axillary, and inguinal cultures from her husband and their two children yielded MRSA on several occasions. Mupirocin nasal ointment and antiseptic washes were recommended for all family members. Although the patient's husband and children became MRSA-negative, the patient remained MRSA-positive. Therefore, her three apparently healthy cats were screened. Pharyngeal culture from one cat grew MRSA with the same antimicrobial resistance pattern as that of the human isolates. The clonal identity of the isolates from the family and the cats was found by typing of the _spa_ gene repeat region and multilocus sequence typing,  which showed _spa_ \\-type t131 and ST80 in all isolates. This sequence combination does not correspond with that of clone USA300 . \n\n【3】A veterinarian recommended topical decolonization of the MRSA-positive cat with ciprofloxacin and rifampin. Four weeks after the cat's treatment, screening tests of the family were negative for MRSA. Moreover, the patient's deep abscesses completely resolved. Further MRSA screening of the asymptomatic cat was declined by the family.\n\n【4】There is evidence that companion animals, mainly dogs, harbor MRSA,  and interspecies MRSA transmission has been shown in the members of a family and their dog.  This case illustrates that MRSA transmission also occurs between humans and cats. The abscesses in our patient cleared only after antibiotic treatment of the cat. It remains unclear whether the cat was the source of the patient's infection or vice versa, although _spa_ \\-type t131 is extremely rare in humans.  We conclude that pets should be considered as possible household reservoirs of MRSA that can cause infection or reinfection in humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "8af96a4b-d768-4249-afc3-eef6ac74e856", "title": "Treatment of the Bleeding Tendency in Uremia with Cryoprecipitate", "text": "【0】Treatment of the Bleeding Tendency in Uremia with Cryoprecipitate\nAbstract\n--------\n\n【1】We gave cryoprecipitate to six patients with uremia and bleeding times prolonged to more than 15 minutes. After the infusion, all patients had shortened bleeding times; the times of five became normal. In four patients control of major bleeding episodes was attained, and five underwent major surgical or invasive procedures, with good hemostasis. After infusion the time before the nadir of the bleeding time was reached was between one and 12 hours. The bleeding time returned to pretreatment levels by 24 hours after infusion in five patients, and by 36 hours in the other patient. Platelet-aggregation studies before infusion gave normal results in three patients and abnormal results in three. There was no change after infusion. Before infusion, levels of Factor VIII coagulant activity and Factor VIII von Willebrand activity were normal, Factor VIII-Related antigen was increased, and crossed immunoelectrophoresis of Factor VIII-Related antigen was normal.\n\n【2】Our findings suggest that cryoprecipitate can temporarily correct the bleeding tendency in patients with uremia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "3d0b046c-3155-405c-b5ed-52d03844f207", "title": "Acute Pulmonary Edema", "text": "【0】Acute Pulmonary Edema\nA 62-year-old man presents with a three-day history of progressive dyspnea, nonproductive cough, and low-grade fever. His blood pressure is 100/60 mm Hg, his heart rate 110 beats per minute, his temperature 37.9°C, and his oxygen saturation while breathing room air 86 percent. Chest auscultation reveals rales and rhonchi bilaterally. A chest radiograph shows bilateral pulmonary infiltrates consistent with pulmonary edema and borderline enlargement of the cardiac silhouette. How should this patient be evaluated to establish the cause of the acute pulmonary edema and to determine appropriate therapy?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "d23cef2e-d36c-4aa9-8d27-795dbf70a3cb", "title": "Usefulness of Chromosome Examination in the Diagnosis of Malignant Pleural Effusions", "text": "【0】Usefulness of Chromosome Examination in the Diagnosis of Malignant Pleural Effusions\nAbstract\n--------\n\n【1】To determine whether chromosome analysis could facilitate the diagnosis of malignant pleural effusions, we examined chromosomes in effusions from 104 unselected patients. An effusion was regarded as malignant if at least three of 30 metaphase cells were hyperdiploid or contained a marker chromosome. Results were compared with standard cytologic diagnoses. All 22 benign effusions were diagnosed correctly by cytologic examination, but one (acute rheumatoid lung disease) was misclassified as positive by chromosome criteria. Of the 82 malignant effusions, 53 (65 per cent) were diagnosed correctly by cytologic tests, as compared with 58 (71 per cent) by chromosome analysis (P>0.2). Among patients with malignant neoplasms, 13 had leukemia or lymphoma; only four of these (31 per cent) were diagnosed by cytologic tests as compared with 11 (85 per cent) by chromosome analysis (P<0.01). The combination of standard cytologic and chromosome analyses correctly identified 83 per cent of the neoplasms, a result significantly better than that with either technic alone (P<0.01).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
{"seq_id": "2555685a-2074-49f4-93e4-836fa2e05378", "title": "The Climate Crisis and Covid-19 — A Major Threat to the Pandemic Response", "text": "【0】The Climate Crisis and Covid-19 — A Major Threat to the Pandemic Response\nArticle\n-------\n\n【1】Just as an active 2020 Atlantic hurricane season is getting under way, the entire U.S. hurricane coast, from Texas to the Carolinas, is witnessing explosive outbreaks of Covid-19 cases in communities where physical distancing restrictions have been eased. As an early wake-up call, Tropical Storm Cristobal made landfall in Louisiana on June 7, triggering coastal evacuation orders and a federal emergency declaration. Concurrently, temperatures continue to set records throughout the southern United States, while Arizona has been battling multiple historic wildfires that are also requiring communities to evacuate their homes. All this as summer had just begun.\n\n【2】These events suggest that the United States will increasingly face complex, challenging scenarios, given the confluence of our two most pressing global health threats — the rapid emergence of the Covid-19 pandemic and the insidiously evolving climate crisis. Both these crises disproportionately harm the health of vulnerable and economically disadvantaged people, including those affected by structural racism. Understanding the challenges posed by this conjunction is essential if we are to devise effective and equitable strategies to protect and improve health. Attention must be directed toward key pathways through which the climate crisis threatens efforts to contain SARS-CoV-2 transmission and improve Covid-19 outcomes, which include difficulty maintaining physical distancing, exacerbation of coexisting conditions, and disruption of health care services.\n\n【3】The intensity, frequency, and duration of climate-related extreme events — including hurricanes, wildfires, floods, heat waves, and droughts — are increasing, and these events often overlap temporally and geographically,  jeopardizing SARS-CoV-2 infection control. Both the Atlantic hurricane and western wildfire seasons are predicted to be worse than average in 2020. But proven standard disaster mitigation strategies — mass sheltering and population evacuation — increase the risk of viral transmission by moving large groups of people and gathering them close together. For example, evacuation orders were issued for more than 1 million people during Hurricane Florence in 2018. Covid-19 health risks are even greater when weather events are more intense, since widespread catastrophic damage results in mass displacement, which risks introducing the virus into new locales and clustering vulnerable survivors together in temporary accommodations.\n\n【4】No other year in recorded history has been as hot as the years between 2014 and 2019, and 2020 has a high likelihood of being the hottest year ever. Despite the hypothesis that higher temperatures and humidity might reduce SARS-CoV-2 transmission, Covid-19 cases are increasing rapidly throughout warm southern states, confirming expert predictions.  Extreme heat poses additional challenges to Covid-19 mitigation efforts. For example, wearing a face mask, especially an N95, is uncomfortable in high heat and humidity and may exacerbate risks for heat-related illnesses  ; conversely, not wearing a mask increases the likelihood of spreading Covid-19. During heat waves, cooler indoor venues, including designated cooling centers, may become crowded with residents from households lacking air conditioning or facing heat-related electrical blackouts.\n\n【5】Cardiovascular and chronic pulmonary disease — recognized risk factors for severe Covid-19 — are closely linked to climate change, through effects including extreme heat, ground-level ozone, wildfire smoke, and increased pollen counts over longer seasons.  Moreover, fine particulate matter air pollution — linked to combustion of fossil fuels — increases the prevalence of both conditions. Marginalized groups are at higher risk than others for exposure to high levels of air pollution and associated chronic illnesses, as well as for Covid-19–related illness and death. Recent unpublished data have suggested direct associations between long-term exposure to particulate air pollution and risk of Covid-19–associated death. \n\n【6】Climate change also complicates the ability of patients with Covid-19 to gain access to and receive the best available health care services. Heat waves and climate-related disasters may generate a surge of “climate casualties” seeking care in facilities already filled with Covid-19 patients.  Conversely, health care access for these patients may be acutely compromised in the aftermath of climate-driven extreme events, owing to physical damage to facilities, power outages, supply-chain disruptions, and depletion of staff — leading to cascading disruptions of services.\n\n【7】Our responses in the United States to climate change over recent years and to the Covid-19 pandemic over recent months have been inadequate and dangerous, disproportionately harming the most vulnerable communities. Both responses have been characterized by delayed and disjointed government action, denigration of scientific evidence, distortion of truth, withdrawal from critical global alliances, and reliance on antiquated public health infrastructure and fragile health care systems. To effectively manage both crises, we need an integrated response, firmly grounded in science, that values health as a fundamental right for all. As we collectively reimagine an equitable, all-hazards-responsive health infrastructure, we will need to take concrete actions focused on the key intersections between climate change and the Covid-19 pandemic.\n\n【8】In the short term, to minimize the transmission of SARS-CoV-2 during climate-intensified extreme weather events, standard sheltering, evacuation, and related strategies will have to be modified . Long-term actions, with implications for future resiliency, include prioritizing federal and state funding for mitigation plans to prepare for a future of climate-driven intensification of extreme weather and superimposed events, using an approach that takes all hazards into account.\n\n【9】Short-Term Strategies for Managing Climate-Related Extreme Events during the Covid-19 Pandemic.\n-----------------------------------------------------------------------------------------------\n\n【10】*   Extreme events (e.g., hurricanes, wildfires): evacuation and sheltering\n\n【11】*   Communicate clearly to the public that the Covid-19 pandemic does not change the imperative to evacuate, given the substantial risks of remaining in place during extreme climate-driven hazards.\n\n【12】*   Use existing community pandemic-communication channels to disseminate critical information.\n\n【13】*   Increase the number of available shelter sites, with lower occupancy per site, more separated spaces within sites, and more space per shelter resident (e.g., using smaller “noncongregate shelters,” hotels).\n\n【14】*   Use standard shelter-registration information (name, contact phone number) for all persons entering, to facilitate contact tracing in case Covid-19 is diagnosed in persons who used the shelter.\n\n【15】*   Implement shelter protocols for infection control, including daily symptom checks, isolation of symptomatic persons, mandatory wearing of face masks, ample supplies of hand sanitizer, hand-washing stations, and meals provided in disposable containers.\n\n【16】*   Adapt guidance for minimizing Covid-19 viral transmission in mass care settings for use with in-home sheltering — because many evacuees shelter with family and friends.\n\n【17】*   Extreme heat: remaining at home and cooling locations\n\n【18】*   Provide electricity subsidies and extend moratoriums to prevent electricity and water shutoffs for people with pandemic-related unemployment and economic hardships to allow them to remain in their homes.\n\n【19】*   Ensure effective alternatives to minimize heat exposure if designated cooling centers or popular indoor, air-conditioned venues are closed.\n\n【20】*   Ensure that cooling centers follow guidelines similar to best-practice guidelines noted above.\n\n【21】*   Minimize transmission risks by limiting occupancy and providing or requiring masks and hand sanitizer in air-conditioned venues open to the public, such as malls or movie theaters.\n\n【22】*   Use phone text messages, as used for pandemic communication, for heat-health notifications.\n\n【23】During extreme-heat events, interventions are needed to ensure that the people who are most susceptible to both heat-related illness and severe Covid-19 disease can either remain at home safely or have safe cooling options. More sustained approaches include alterations in the built environment (e.g., expanding green space, making more roofs white) and community outreach programs for the most vulnerable.\n\n【24】Ongoing adaptations and transformations in health care delivery, prompted by the Covid-19 pandemic, can also be effectively applied to climate-driven extreme events. In particular, the expansion of telemedicine — in areas where computer or phone service is intact — and the use of community paramedicine services can improve our ability to address medical and psychological needs, and minimize SARS-CoV-2 exposure, for people who cannot readily obtain care. Investments in strengthening our health care infrastructure and delivery systems, such as supply chains, are also essential to ensuring resiliency during pandemic or climate shocks.\n\n【25】Though evidence-based guidance from federal agencies is important and would be welcome, implementation of strategies at the state and local levels requires capacity, coordination, and attention to subnational needs. Given that states were forced to reprioritize budget allocations because of the pandemic, facing the challenges ahead will require coordinated federal policy and dedicated funding.\n\n【26】In recent months, the increasing worldwide attention to the urgency of addressing climate change has been sidelined by the pandemic and the critically needed reckoning on racial inequity. Yet the interconnectedness of these challenges underscores the need for integrated policy initiatives. As emphasized in a letter . opens in new tab from 40 million health professionals to G20 leaders, governments must prioritize investments in health, clean air and water, and a stable climate in stimulus packages for recovering from the Covid-19 pandemic.\n\n【27】Reductions in greenhouse-gas emissions and air pollution that were observed while globally applied lockdown measures were in force to slow the spread of Covid-19 are proving to be temporary. Interventions to create sustained reductions in the use of fossil fuels can reduce the risks for multiple medical conditions — especially in vulnerable communities — by improving air quality and limiting the downstream health harms of the climate crisis.\n\n【28】Until the development and mass deployment of a safe and effective vaccine enables the United States to move past the Covid-19 pandemic, the climate crisis will challenge our pandemic responses; beyond the pandemic, the climate crisis will continue to pose existential risks. It is past time to implement robust and equitable responses to both.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "nejm", "batch_name": "20230925", "version": "version0"}
