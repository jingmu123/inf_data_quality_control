import json
import os
import re
from tqdm import tqdm
from nltk import word_tokenize
import nltk

nltk.download('punkt')
#r'(it:)\d+','\1'
pattern_list_en = [
    [r'\b(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b', ''],
    [
        r'(.*Case\s\d+)|((\bcs\b:.*)|(\bit\b:.*)|(\bsr\b:.*)|(ISBN(:?).*)|(\bde\b:.*)|(\bhe\b:.*))|((-\s*)?Copyright)|(.*copyright.*)|.*IU\.S\..*',
        ''],  # cs:13442112ã€it:1132313ã€ISBN2321311ã€...copyright...ã€...IU.S...
    ['\.(.*?)for detailed recommendations(.*?)\.', ''],
    ['(Return to top)|(Adapted from the FDA Package Insert)|(Loading. Please wait.)', ''],
    ['\(go to Results in Section \d+\)|(\(Recommendation.*?\))', ''],
    ['(\(\d\s*References)?(\(See)?.*\.\.\.\sread more\s*\)\.', ''],  # 1 References See...sread more.
    [r'(\((see|See)\s*(Table|table|Figure|figure|Section)?.*?\))', ''],
    [r'(\()?(Table|table|TABLE|Figure|figure|Section|diagram|Appendix|Box):?\s*(\d+)?(\.?).*?(\))?', ''],
    [r'# (Table|Price|((Re)?(S|s)ource(s?)(:(.*))?)|Notes)', ''],  ## Tableã€# Resource
    [r'.*Fig\.\d+\sPubic.*', ''],
    [r'\bOR\b|(.*\.{{Cite)|(.*further details.*)', ''],  # ORã€...{{Citeã€...further details...
    [r'((\.)?(.*?)(See|(see also)).*)|(\.(.*?)can be found in(.*?)\.)', ''],  # See:...ã€...can be found in...
    [r'.*\b(R|r)eference(s?)\b.*', ''],
    [r'.*Â§.*-\s*-\s*-.*', ''],  # Â§...- - -...
    [r'.*Cavernous\ssinus\saneurysm.*', ''],
    [r'(.*Image Gallery.*)|(\(.*?MRI.*?\))', ''],  # ...Image Gallery...ã€(...MRI...)
    [r'(\(Images.*of.*?\)|(.*M1\s4BT.*)|Contact NICE)', ''],  # (Images...of...)ã€...M1 4BT...ã€Contact NICE
    ['(.*\d{4}.*(ISBN|Retrieved).*)', ''],  # ...1970...ISBN...ã€...1970...Retrieved...
    [r'\(\s*#?((\d+-\s*\d+)|(\d+(,|ï¼Œ)\s*\d+.*)|(\d+))\)', ''],  # ( #(12-44))ã€( #(12.asdf))
    [r'(via:.*)|(el ta(\(\d+.*?\))?)', ''],  # via:...ã€el ta(66english)
    [r'\(\d{4}-\d{4}\s*update.*?et al.\)?', ''],  # (1980-1998 update in et al
    [r'\([a-zA-Z]+\s*.*et\sal.*\d{4}(.*?\))?', ''],  # (today et al 1980 ,1999)
    [r'\..*?(A|a)dditional information.*?\.', ''],

    [r'(\..*?http://.*\.)|\(.*?http://.*?\)|((\()?www\..*((/.*?\))|(g?)))', ''],
    ['\.\.\.', '\.'],
    ['\(DO NOT EDIT\)', ''],
    ['â–¶|â—|Â©|Â®|(.*-\sâ†‘.*)|(:-)|â‘|â€ |Â¶|â•‘|Â§|âˆ§|â„¢', ''],
    ['American Samoa.*', ''],
    ['\(\s*\d+\s*\)', ''],
    # new
    [r'-?.*,\s*\b(19|20)\d{2}\b.*?(\.)?', ''],  # - Copi, Irving. Symbolic Logic.  MacMillan, 1979, fifth edition.
    [r'#?\s*Reference(s?) and notes.*', ''],  # References and notes
    [r'.*(M\.D\.)?,\s*Ph\.\D.', ''],  # Steven C. Campbell, M.D., Ph.D.
    [r'(For more information,)?\s*call\s*\d+-\d+-\d+-\d+.*?\.', ''],
    # For more information, call 1-800-445-3235 or go to .

    # new
    [r'(\(|\[)\n', '\n'],  # å»é™¤ä¸åŒ¹é…æ‹¬å·(
    [r'Taken from.*\+.*?(\.)?,?', ''],  # å»æ‰Taken from Shapiro  .   Taken from Monson  . + Taken from Thomas et a I.,
    ['ğŸ”—|â¤ï¸', ''],
    ['\*$', ''],  # å»é™¤ç»“å°¾çš„*
    ['(-\s*)?(To|For).*click here(\.)?', ''],  # å»é™¤For the WikiDoc page for this topic, click here
    ['Report format', ''],
    ['You can find.*?.', ''],
    ['Return to recommendation(s?)', ''],
    [r'Recommendation(s?)\s+\d+\.\d+\.\d+.*', ''],
    ['For.*details.*?\.', ''],
    ['For related.*?see.*?\.', ''],
    ['\.\.', ''],  # å»é™¤..
    ['(A|a)lso see.*?\.', ''],
    ['refer to', ''],
    [r'-\s^.*', ''],
    # å»é™¤- ^ Lee D.G. Chen T. . "Reduction of Manganate(VI) by Mandelic Acid and Its Significance to Development of a General Mechanism for Oxidation of Organic Compounds by High-Valent Transition Metal Oxides". Journal of the American Chemical Society. 115: 11231â€“11236. doi:10.1021/ja00077a023.
    [r'-?.*\b(19|20)\d{2}\b.*:.*?\.', ''],
    # å»é™¤å‚è€ƒæ–‡çŒ®ï¼š- G. Procter, S. V. Ley, G. H. Castle â€œBarium Manganateâ€  in Encyclopedia of Reagents for Organic Synthesis (Ed: L. Paquette) 2004, J. Wiley & Sons, New York. DOI: 10.1002/047084289.

    # new
    [r'-\s*\n', ''],  # å»æ‰åªæœ‰ â€-  â€æ²¡æœ‰æ–‡å­—çš„è¡Œ
    [r'â†‘.*(\d|\.)', ''],  # å»é™¤æ— æ„ä¹‰ç‰‡æ®µ â†‘ Jump up to: 14.0 14.1 14.2
    [r'(-\s*)?Please.*(Join|Page).*\.', ''],
    # å»é™¤æ— å…³æ–‡æœ¬ Please Join in Editing This Page and Apply to be an Editor-In-Chief for this topic:
    [r'\bnn\b:.*', ''],  # å»é™¤nn:Pulmonalklaff.....
    [r'(,\s*,\s*,)|(,\s*,)|(\.\s*\.)', ','],
    [r'.*((in evidence review)|(evidence review)|(.*details.*evidence)).*?(\.)?', '']
    # å»æ‰evidence reviewÂ E1: management options for mild to moderate acne â€“ network meta-analyses

]


class speicalProces:
    def __init__(self):
        pass

    # åˆ¤æ–­é¦–å­—æ¯éå¤§å†™å­—æ¯
    def is_not_upper(self, char):
        return not char.isupper()

    def step1_duplicate_text(self, content):  # ç²¾å‡†å»é‡
        content1 = content.split("\n")
        for i in range(len(content1)):
            content1[i] = content1[i].lstrip("-").rstrip(".").strip(" ")
        # å»æ‰é‡å¤é¡¹
        content = list(set(content1))
        content.sort(key=content1.index)
        # å»æ‰ä¸Šä¸€è¡Œä»¥ä»‹è¯ç»“å°¾çš„å¤šä½™æ¢è¡Œ
        temple_list = []
        for cur in range(len(content) - 1):
            if ("#" not in content[cur] or "*" not in content[cur]) and (
                    content[cur].endswith("and") or content[cur].endswith("of") or content[cur].endswith(
                "any")) and self.is_not_upper(content[cur + 1][0]):
                temple_list.append(cur + 1)
                content[cur] = content[cur] + " " + content[cur + 1]
        for i in temple_list:
            try:
                content.pop(i)
            except Exception as e:
                pass
        # print(content)
        for i in range(len(content)):
            if content[i] == '' or content[i].startswith("#") or content[i].startswith("*"):
                continue
            else:
                try:
                    if content[i][0].isalpha():
                        content[i] = "- " + content[i]
                except Exception as e:
                    pass
        return "\n".join(content)

    def step2_remove_duplicates(self, text, lang):  # æ¨¡ç³Šå»é‡
        # åˆ†å‰²æ–‡æœ¬æˆå¥å­
        sentences = text.split("\n")

        # å®šä¹‰Jaro-Winklerç›¸ä¼¼åº¦é˜ˆå€¼
        threshold = 0.9

        # åˆå§‹åŒ–å·²è§å¥å­é›†åˆ
        seen_sentences_set = set()

        # åˆå§‹åŒ–å”¯ä¸€å¥å­åˆ—è¡¨
        unique_sentences = []

        # éå†æ‰€æœ‰å¥å­
        for sentence in sentences:
            old_sentence = sentence
            sentence = sentence.strip(" ").strip("\n")  # å»é™¤ç©ºæ ¼å’Œæ¢è¡Œç¬¦
            # æ£€æŸ¥å½“å‰å¥å­æ˜¯å¦ä¸å·²è§å¥å­é›†åˆä¸­çš„ä»»ä½•å¥å­è¶³å¤Ÿç›¸ä¼¼
            is_duplicate = False
            for seen_sentence in seen_sentences_set:
                # è®¡ç®—Jaro-Winklerç›¸ä¼¼åº¦
                jaro_winkler_distance = Levenshtein.jaro_winkler(sentence, seen_sentence)
                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯é‡å¤
                if jaro_winkler_distance > threshold:
                    is_duplicate = True
                    break
            # å¦‚æœä¸æ˜¯é‡å¤çš„ï¼Œåˆ™æ·»åŠ åˆ°å”¯ä¸€å¥å­åˆ—è¡¨å’Œå·²è§å¥å­é›†åˆä¸­
            if not is_duplicate:
                unique_sentences.append(old_sentence)
                seen_sentences_set.add(sentence)

        # é‡æ–°ç»„åˆæ–‡æœ¬
        context = "ã€‚".join(unique_sentences) if lang == "zh" else "\n".join(unique_sentences)
        return context

    def step3_resolve_space(self, sentences):  # å»æ‰æ ‡é¢˜ä¸­å•è¯é—´çš„å¤šä½™ç©ºæ ¼
        words_list = word_tokenize(sentences)
        words_list_new = []
        position = 0
        for word_index in range(0, len(words_list)):
            if words_list[word_index][0].isupper():
                position = word_index
            if self.is_not_upper(words_list[word_index][0]):
                words_list[position] = words_list[position] + words_list[word_index]
        for word in words_list:
            if word[0].isupper():
                words_list_new.append(word)
        return '# ' + ' '.join(words_list_new)

    def step4_repair_parentheses(self, context):  # å»é™¤ä¸åŒ¹é…çš„æ‹¬å·
        stack = []
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œæ–¹ä¾¿å¯¹ç€ç´¢å¼•åˆ é™¤
        list_text = list(context)
        for i, char in enumerate(context):
            if char == '(' or char == "ï¼ˆ" or char == '[' or char == '{':
                stack.append(i)
            elif char == ')' or char == "ï¼‰" or char == ']' or char == '}':
                if not stack:
                    # æ²¡æœ‰å‡ºç°å·¦æ‹¬å·,å³æ‹¬å·ç›´æ¥åˆ é™¤
                    list_text[i] = ''
                else:
                    stack.pop()
        # ä¸ä¸ºç©ºè¯´æ˜æœ‰å¤šä½™æ‹¬å·å­˜åœ¨
        while stack:
            index = stack.pop()
            # å»é™¤çš„æ‹¬å·ç”¨é€—å·ä»£æ›¿
            list_text[index] = ','
        return ''.join(list_text)


def clean_text(context, lang):
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"

    if lang == 'en':
        pattern_list = pattern_list_en

    result = []
    sp = speicalProces()

    for item in context.split(split_token):
        item = item.strip(split_token).strip()
        item = sp.step1_duplicate_text(item)
        item = sp.step2_remove_duplicates(item, "en")
        for pattern_item in pattern_list:
            item = re.sub(pattern_item[0], pattern_item[1], item)
        item = sp.step4_repair_parentheses(item)
        result.append(item)
        # æ•´åˆ
    result_last = []
    for item_1 in result:
        result_1 = []
        item_1 = item_1.split("\n")
        for item_2 in item_1:
            if sum(1 for char in item_2 if char.isupper()) > 2 and item_2.startswith(
                    "#") and "," not in item_2 and "?" not in item_2 and "." not in item_2 and ":" not in item_2:
                item_2 = sp.step3_resolve_space(item_2)
            result_1.append(item_2)
        result_last.append("\n".join(result_1))
    context = split_token.join(result_last)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


fw = open("../../../../full_data/guidelines/guidelines_clean.jsonl", "w", encoding="utf-8")
with open(
        "../../../../full_data/guidelines/guidelines_preformat.jsonl",
        "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        context = item["text"]
        lang = item["lang"]
        context = clean_text(context, lang)
        context = post_process(context)
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        fw.write(item + "\n")

