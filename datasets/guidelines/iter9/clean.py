import json
import os
import re
from tqdm import tqdm
import Levenshtein

pattern_list_en = [
    [r'[^\.\n]*\b(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b[^\n]*', ''],
    [r'([^\.\n]*Case\s\d+)|((\bcs\b:.*)|(\bit\b:\d+)|(\bsr\b:.*)|(ISBN(:?).*)|(\bde\b:.*)|(\bhe\b:))|(\bnl\b:[^\n])|(\bda\b):[^\n]|((-\s*)?Copyright)|([^\.\n]*copyright[^\.\n]*)|[^\.\n]*IU\.S\..*',''],  # cs:13442112ã€it:1132313ã€ISBN2321311ã€...copyright...ã€...IU.S...
    [r'[^\.\n]*(F|f)or\s*detailed\s*(recommendations)?[^\.\n]*', ''],
    [r'[^\(\.\n]*recommendationÂ \d+\.\d+\.\d+\.\d+[^\.\n\)]*',''],
    [r'(Return to top)|(Adapted from the FDA Package Insert)|([^\.\n]*(P|p)lease[^\.\n]*)', ''],
    [r'\(go\s*to\s*(R|r)esults\s*in\s*(S|s)ection(s?)\s*\d+.*?\)', ''],
    ['(\(\d\s*References)?(\(See)?.*\.\.\.\sread more\s*\)\.', ''],  # 1 References See...sread more.
    [r'(\((see|See)\s*(Table|table|Figure|figure|Section)?.*?\))', ''],
    [r'(\()\b(Table(s?)|table(s?)|TABLE|Figure(s?)|figure(s?)|Section|diagram|Appendix|Box|(S|s)ource(s?)|Fig|p)\b:?\.?\s*(\d+)?(\.?).*?(\))', ''],
    [r'[^\.]*\b(\b(T|t)able(s?)\b|TABLE|(F|f)igure(s?)|FIGURE|(S|s)ection(s?)|diagram(s?)|(A|a)ppendix|APPENDIx|Box|Fig)\b[^\n]*', ''],
    [r'(Re)?(S|s)ource(s?):?\s*of\s*evidence',''],
    [r'# (Table|Price|((Re)?(S|s)ource(s?):?\s*)|Notes|((F|f)urther information))', ''],  ## Tableã€# Resource
    [r'[^\.\n]*Fig.*?Pubic[^\.\n]*', ''],
    [r'\bOR\b|(\{\{Cite)', ''],  # ORã€...{{Cite
    [r'[^\.\n]*(see\s*also)[^\.\n]*', ''],  # See:...ã€...can be found in...
    [r'[^\.\n]*\b(R|r)eference(s?)\b[^\.\n]*', ''],
    [r'[^\.\n]*Â§.*-\s*-\s*-[^\.\n]*', ''],  # Â§...- - -...
    [r'[^\.\n]*Cavernous\s*sinus\s*aneurysm[^\.\n]*', ''],
    [r'([^\.\n]*Image Gallery[^\.\n]*)|(\(.*?MRI.*?\))', ''],  # ...Image Gallery...ã€(...MRI...)
    [r'(\(Images.*of.*?\)|(.*M1\s4BT.*)|Contact NICE)', ''],  # (Images...of...)ã€...M1 4BT...ã€Contact NICE
    ['(.*\d{4}.*(ISBN|Retrieved).*)', ''],  # ...1970...ISBN...ã€...1970...Retrieved...
    [r'\(\s*#?((\d+-\s*\d+)|(\d+(,|ï¼Œ)\s*\d+.*)|(\d+))\)', ''],  # ( #(12-44))ã€( #(12.asdf))
    [r'(via:.*)|(el ta(\(\d+.*?\))?)', ''],  # via:...ã€el ta(66english)
    [r'\(\d{4}-\d{4}\s*update.*?et al.\)?', ''],  # (1980-1998 update in et al
    [r'\([a-zA-Z]+\s*.*et\s*al.*\d{4}(.*?\))?', ''],  # (today et al 1980 ,1999)
    [r'[^\.\n]*(A|a)dditional\s*information[^\.\n]*', ''],
    [r'[^\.\n]*(\..*?http://.*\.)|\(.*?http://.*?\)|((\()?www\..*((/.*?\))|(g?)))[^\.\n]*', ''],

    ['\.\.\.', '.'],
    ['\(DO NOT EDIT\)', ''],
    ['â–¶|â—|Â©|Â®|(.*-\sâ†‘.*)|(:-)|â‘|â€ |Â¶|â•‘|Â§|âˆ§|â„¢', ''],
    ['American Samoa.*', ''],
    ['\(\s*\d+\s*\)', ''],

    # new
    [r'#?\s*Reference(s?)\s*and\s*notes.*', ''],  # References and notes
    [r'.*(M\.D\.)?,\s*Ph\.\D.', ''],  # Steven C. Campbell, M.D., Ph.D.
    [r'[^\.\n]*((M|m)ore\s*information)[^\.\n]*', ''],  # For more information, call 1-800-445-3235 or go to .
    # [r'[^\.\n]*following[^\.\n]*',''],

    # new
    [r'Taken\s*from.*\+[^\.\n]*', ''],  # å»æ‰Taken from Shapiro  .   Taken from Monson  . + Taken from Thomas et a I.,
    ['ğŸ”—|â¤ï¸|â¦|â–º|ï‚§|ï®|â¥', ''],
    [r'\*$', ''],  # å»é™¤ç»“å°¾çš„*
    [r'[^\.\n]*(C|c)lick\s*here[^\.\n]*', ''],  # å»é™¤For the WikiDoc page for this topic, click here
    [r'[^\.\n]*Report\s*format[^\.\n]*', ''],
    [r'[^\.\n]*can.*?find[^\.\n]*', ''],
    [r'[^\.\n]*Return\s*to\s*recommendation(s?)[^\.\n]*', ''],
    [r'[^\.\n]*Recommendation(s?)\s*\d+\.\d+\.\d+\s*(to\s*\d+\.\d+\.\d+)?[^\.\n]*', ''],
    [r'[^\.\n]*detail(s?)[^\.\n]*', ''],
    [r'[^\.\n]*(F|f)or\s*related.*?see[^\.\n]*', ''],
    ['\.\.', ''],  # å»é™¤..
    [r'[^\.\n]*(\b(A|a)lso\b\s*\bsee\b)|(\bSee\b)|(\b(S|s)ee\b\s*\balso\b)[^\.\n]*', ''],
    [r'[^\.\n]*(R|r)efer to[^\.\n]*', ''],
    [r'-\s^[^\.\n]*',''], #å»é™¤- ^ Lee D.G. Chen T. . "Reduction of Manganate(VI) by Mandelic Acid and Its Significance to Development of a General Mechanism for Oxidation of Organic Compounds by High-Valent Transition Metal Oxides". Journal of the American Chemical Society. 115: 11231â€“11236. doi:10.1021/ja00077a023.
    [r'-?.*\d{4}.*?(DOI|doi):[^\.\n]*', ''],# å»é™¤å‚è€ƒæ–‡çŒ®ï¼š- G. Procter, S. V. Ley, G. H. Castle â€œBarium Manganateâ€  in Encyclopedia of Reagents for Organic Synthesis (Ed: L. Paquette) 2004, J. Wiley & Sons, New York. DOI: 10.1002/047084289.

    # new
    [r'â†‘.*', ''],  # å»é™¤æ— æ„ä¹‰ç‰‡æ®µ â†‘ Jump up to: 14.0 14.1 14.2
    [r'[^\.]*(P|p)age[^\n]*',''],
    [r'[^\.]*(J|j)oin[^\n]*',''],
    [r'\bnn\b:.*', ''],  # å»é™¤nn:Pulmonalklaff.....
    [r'(,\s*,\s*,)|(,\s*,)|(\.\s*\.)', ','],

    # new
    [r'[^\.\n]*evidence(s?)\s*review(s?)[^\.\n]*', ''],# å»æ‰evidence reviewÂ E1: management options for mild to moderate acne â€“ network meta-analyses
    [r'(".*?"),\s*[A-Za-z]*\s*\d{4}.*\.', ''],# å»é™¤ç±»ä¼¼æ ¼å¼çš„å‚è€ƒæ–‡çŒ®ï¼Œç‰¹å¾æ˜¯æ–‡ç« é¢˜ç›®ç”¨åŒå¼•å·å¼•å‡ºï¼Œå¹¶æœ‰å¹´ä»½ï¼š'Interventional procedure overview of corneal implants for the correction of refractive error', November 2006.
    [r'(\'.*?\'),\s*[A-Za-z]*\s*\d{4}.*\.', ''],
    [r'#[^\.\n]*(Images|(Template:[^\.\n]*)|Acknowledgements)', ''],#å»é™¤ #Images  #Acknowledgements
    [r'[^\.]*\b(APPENDIX|appendix|Appendix|(T|t)able(s?)|TABLE|Figure|diagram|figure|Box|FIG|Fig|fig)\b[^\n\.]*', ''],#å»é™¤å¸¦æœ‰Appendixã€Tableç­‰çš„è¿™å¥è¯ï¼Œå¦‚ï¼šThe current OSHA standards and the NIOSH-recommended exposure limits, as well as pertinent health effects, are listed in Appendix B.

    [r'\nfig\n',''],
    ['{{{indicationType}}} ',''],
    [r'[^\.\n]*(C|c)hapter[^\.\n]*',''],
    [r'[^\.\n]*Editor[^\.\n]*',''],#å»æ‰Assistant Editor-In-Chief: Michelle Lew ç¼–è¾‘äººå
    [r'[^\.\n]*NDC[^\.\n]*',''],
    [r'[^\.\n]*\d+-\d+-\d+[^\.\n]*',''],#å»æ‰NDC 0065-0023-15
    [r'[^\d\(]*pictured[^\)]*',''],
    [r'(\d{1,2}\s*[,\.]\s*)?\d{1,2}\s*\.?$(?:$|[^(\d{4})])',''], #å»é™¤å¥æœ«æ— å…³æ•°å­—å¼•ç”¨ï¼Œå¦‚ï¼š# TEAM BRIEFING 3,4
    [r'[^\(\.\n]*see\s*illustration[^\)\.\n]*',''],
    [r'[^\.\n]*thank[^\.\n]*',''],
    [r'#\s*Disclaimer',''],
    [r'[^\.\n]*WikiDoc[^\.\n]*',''],
    [r'[^\n]*Books[^\n]*',''],
    [r'[^\.\(\n]*[A-Z][a-z]*\s*&[^\n\)]*',''],
    [r'(?:^|[^#])#(?:$|[^#])', '\n\n#'],  # è§£å†³#ä¹‹åæ–‡æœ¬æœªæ¢è¡Œé—®é¢˜ï¼Œå¦‚ï¼šAF and interventional procedures# TEAM BRIEFING
    [r'(\.\s*,)|(\.\.)|(,\s*\.)', '.'],
    [r'^[\.,:\\]*\s*',''],#å»é™¤å¥é¦–å¤šä½™çš„æ ‡ç‚¹
    [r'[^\.\n]*\b(and|AND|or|with|for|on|to|about|as|by|of)\b\s+\.[^\n]*',''],
    [r'\s{3,}',' '],
    [r'\(information\s*available\s*at.*?\)',''],
    [r'\(\s*\)',''],
    [r'^n\s',''],#å»é™¤å¥é¦–çš„n,å¦‚ï¼šn Human rights and gender equality must be placed at the centre of a comprehensive approach to health programming, in particular in relation to sexuality and sexual health.
    [r'#\s*Warnings',''],
    [r'(\bsv\b:[^\n])|(\buk:\b[^\n])',''],
    [r'[^\n]*-\s*-\s*-\s*[^\n]*',''],
    [r'[^\.\n]*\d{4};\s*[A-Za-z]*\s*\d+:?\s*\d+(-|â€“|;)\d+[^\.\n]*',''], #å»é™¤é¡µç ï¼šBioscience and Microflora, 2001; Vol 20: 43-48.
    [r'[^\n]*[A-Z][a-z]+\s*[,\.]?\s*[A-Z]?\s*[,\.]?\s*&\s*[A-Z][a-z]+[^\n]*',''],#å»é™¤å¸¦äººåçš„å‚è€ƒæ–‡çŒ®
    [r'-\s*$',''],
    [r'[^\.\n]*Image\s*courtesy[^\.\n]*',''],
    [r'[^\.\n]*Google\s*Scholar[^\.\n]*',''],
    [r'[^\.\n]*at\s*\.$',''],
    [r'[^\.\n]*Images\s*shown[^\.\n]*',''],
    [r'(\d{4}\s*[A-Z]?([a-z]+)?;\s*)?\d+\s*:\s*\d+\s*(-|â€“)\s*\d+',''],
    [r'[^\n]*((Manufactured for)|(AUTHORS)|(External link)|(Classes and members))[^\n]*',''],
    [r'[^\n]*[A-Z][a-z]+,\s*([A-Z]\.)?\s*[A-Za-z]*\s*[A-Z][a-z]+[^\n]*',''],
    [r'[^\n]*(\bpp\b\.?)?\s*((\d+\s*:)|(\s*\bpp\b\s*))s*\d+-\d+[^\n]*',''],
    [r'[^\n]*(PMID:)|(GeneID)[^\n]*',''],
    [r'[^\.\n]*in\s*the\s*following\s*document[^\.\n]*',''],
    [r'(?<=[^0-9]\.)\s*\d+(?:,\d+)*\s*(?=[A-Z])',''],#å»é™¤å¥å­å‰çš„æ— å…³æ•°å­—å¼•ç”¨ï¼š26 Bedside ultrasound is operator dependent; therefore, the diagnostic accuracy may vary depending on the level of training.
    [r'[^\n]*Recommendations[^\n]*',''],
    [r'[^\.\n]*Links[^\.\n]*','']
        ]

context_pattern=[
    ['see illustration',''],
    ['Pre-delivery 1',''],
    ['Loading',''],
    ['Loadin',''],
    ['# List of contributors:','']

        ]
class speicalProces:
    def __init__(self):
        pass

    # åˆ¤æ–­é¦–å­—æ¯éå¤§å†™å­—æ¯
    def is_not_upper(self, char):
        return not char.isupper()


    def step1_remove_duplicates(self, text, lang):  # æ¨¡ç³Šå»é‡
        # åˆ†å‰²æ–‡æœ¬æˆå¥å­
        sentences = text.split("\n")

        # å®šä¹‰Jaro-Winklerç›¸ä¼¼åº¦é˜ˆå€¼
        threshold = 0.8

        # åˆå§‹åŒ–å·²è§å¥å­é›†åˆ
        seen_sentences_set = set()

        # åˆå§‹åŒ–å”¯ä¸€å¥å­åˆ—è¡¨
        unique_sentences = []

        # éå†æ‰€æœ‰å¥å­
        for sentence in sentences:
            old_sentence = sentence
            sentence = sentence.strip(" ").strip("\n")  # å»é™¤ç©ºæ ¼å’Œæ¢è¡Œç¬¦
            # æ£€æŸ¥å½“å‰å¥å­æ˜¯å¦ä¸å·²è§å¥å­é›†åˆä¸­çš„ä»»ä½•å¥å­è¶³å¤Ÿç›¸ä¼¼
            is_duplicate = False
            for seen_sentence in seen_sentences_set:
                # è®¡ç®—Jaro-Winklerç›¸ä¼¼åº¦
                jaro_winkler_distance = Levenshtein.jaro_winkler(sentence, seen_sentence)
                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯é‡å¤
                if jaro_winkler_distance > threshold:
                    is_duplicate = True
                    break
            # å¦‚æœä¸æ˜¯é‡å¤çš„ï¼Œåˆ™æ·»åŠ åˆ°å”¯ä¸€å¥å­åˆ—è¡¨å’Œå·²è§å¥å­é›†åˆä¸­
            if not is_duplicate:
                unique_sentences.append(old_sentence)
                seen_sentences_set.add(sentence)

        # é‡æ–°ç»„åˆæ–‡æœ¬
        context = "ã€‚".join(unique_sentences) if lang == "zh" else "\n".join(unique_sentences)
        return context

    def step2_removeLinefeed(self, nums):  #å»é™¤å¤šä½™æ¢è¡Œ
        slow, fast = 0, 1
        while fast < len(nums):
            nums[slow] = nums[slow].strip()
            nums[fast] = nums[fast].strip()
            if (nums[slow].endswith("and") or nums[slow].endswith("of") or nums[slow].endswith("any") or nums[
                slow].endswith("a") or nums[slow].endswith("is") or nums[slow].endswith("for") or nums[slow].endswith(
                "the") or nums[slow].endswith("or") or nums[slow].endswith(",")) and self.is_not_upper(nums[fast][0]):
                nums[slow] = nums[slow] + " " + nums[fast]
            elif "#" not in nums[slow] and "*" not in nums[slow] and re.match(r'.*[a-z]$', nums[slow]) and re.match(
                    r'^[a-z]', nums[fast]):
                nums[slow] = nums[slow] + " " + nums[fast]
            else:
                slow = slow + 1
                nums[slow] = nums[fast]
            fast = fast + 1
        return nums[0:slow + 1]

    def step3_is_last_word_preposition(self,sentence): # è¿‡æ»¤ç»“å°¾æ˜¯ä»‹è¯ã€ä¸å®Œæ•´çš„çš„å¥å­
        sentence = re.sub(r'[^\w\s]', '', sentence)  # å»é™¤æ ‡ç‚¹ç¬¦å·
        words = sentence.split()
        prepositions = ["and", "in", "on", "at", "for", "with", "about", "as", "by", "to", "from", "of","AND","or"]  # ä»‹è¯åˆ—è¡¨
        return words[-1] in prepositions

    def step4_repair_parentheses(self, context):  # å»é™¤ä¸åŒ¹é…çš„æ‹¬å·
        stack = []
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œæ–¹ä¾¿å¯¹ç€ç´¢å¼•åˆ é™¤
        list_text = list(context)
        for i, char in enumerate(context):
            if char == '(' or char == "ï¼ˆ" or char == '[' or char == '{':
                stack.append(i)
            elif char == ')' or char == "ï¼‰" or char == ']' or char == '}':
                if not stack:
                    # æ²¡æœ‰å‡ºç°å·¦æ‹¬å·,å³æ‹¬å·ç›´æ¥åˆ é™¤
                    list_text[i] = ''
                else:
                    stack.pop()
        # ä¸ä¸ºç©ºè¯´æ˜æœ‰å¤šä½™æ‹¬å·å­˜åœ¨
        while stack:
            index = stack.pop()
            # å»é™¤çš„æ‹¬å·ç”¨é€—å·ä»£æ›¿
            list_text[index] = ','
        return ''.join(list_text)


def clean_text(context, lang):
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"

    if lang == 'en':
        pattern_list = pattern_list_en

    result = []
    sp = speicalProces()

    for item in context.split(split_token):
        # print(item)
        if "REFERENCES" in item:
            # print(1)
            continue
        if "Additional resources" in item:
            continue
        if "Additional images" in item:
            continue
        if "Additional Links" in item:
            continue
        if "Information for patients" in item:
            continue
        if "Sources of evidence" in item:
            continue
        if "Associated Documents" in item:
            continue
        item = re.sub(r"#\n","# ",item)
        item = item.strip(split_token).strip()
        item = sp.step1_remove_duplicates(item, "en")
        item = item.strip().split("\n")
        item = sp.step2_removeLinefeed(item)

        for i in range(len(item)):

            for pattern_item in pattern_list:
                item[i] = re.sub(pattern_item[0], pattern_item[1], item[i])
                item[i] = sp.step4_repair_parentheses(item[i])
                item[i] = item[i].strip()
            for pattern_item in context_pattern:
                item[i] = re.sub(pattern_item[0], pattern_item[1], item[i])

        for i in range(len(item)-1,-1,-1):
            if item[i].strip().startswith("#") or item[i].strip().startswith("*"):
                item.pop()
            else:
                break
        item = ("\n".join(item)).strip()
        result.append(item)
    context = split_token.join(result)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


fw = open("../../../../full_data/guidelines/guidelines_clean.jsonl", "w", encoding="utf-8")
with open("../../../../full_data/guidelines/guidelines_preformat.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        context = item["text"]
        context = clean_text(context, "en")
        context = post_process(context)
        # print(context)
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        fw.write(item + "\n")

