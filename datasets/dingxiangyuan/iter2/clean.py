import json
import os
import re
from pprint import pprint

from tqdm import tqdm

log = open('../../../../full_data/dingxiangyuan/log.txt', 'w', encoding="utf-8")
# pattern_list_zh = [
#                    [r'\\|`|^',''],# åŒ¹é…æ–‡æœ¬ä¸­å¤šä½™ç¬¦å·\ã€`ã€^
#                    ['å›¾ç‰‡æ¥æºï¼š.*',''],#å›¾ç‰‡æ¥æºï¼švisualdx
#                    [r'<br/><br/>',''],
#                    ['(<sup>)?(\\\\)?(\\[|ï¼»)(\\d+|(\\d+[,.ã€~ï½-]\\s*\\d+.*))(\\\\)?(\\]|ï¼½)(</sup>)?',''],#<sup>[1]</sup>
#                    ['<sup>',''],
#                    ['ã€.*æ—¥æœŸ.*ã€‘',''],#ã€ æ ¸å‡†æ—¥æœŸ ã€‘ã€ ä¿®æ”¹æ—¥æœŸ ã€‘
#                    [r'\d{4}-\d{2}-\d{2}',''],#2008-07-23
#                    [r'ï¼ˆ?å›¾\s*\d+:.*?ï¼‰?','']#å›¾ 2ï¼šåŒ»ç–—å™¨æ¢°ï¼‰
#                      ]
pattern_list_zh = [
    [r'(?:[^ã€‚]*\.\.\. )?ç™»å½•', ''],  # åˆ é™¤ç™»å½•
    [r'\\+\[[^\u4e00-\u9fa5]*?\\+\]', ''],
    [r'\\+[\[\]]', ''],
    [r'\\|`|\^', ''],  # åŒ¹é…æ–‡æœ¬ä¸­å¤šä½™ç¬¦å·\ã€`ã€^   #new å¢åŠ äº†\^
    [r'.*å›¾ç‰‡æ¥æºï¼š.*', ''],  # å›¾ç‰‡æ¥æºï¼švisualdx  #new åœ¨å‰é¢å¢åŠ äº†èŒƒå›´
    [r'<br/>+', '\n'],  # æ›¿æ¢æˆ\n  ç¦ç”¨<br/><br/>7ã€éš¾æ²»
    [r'<sup>.*?</sup>', ''],  # å»é™¤sup
    [r'ã€[^ã€]*(?:æ—¥æœŸ| è¯å“å›¾ç‰‡ )[^ã€‘]*ã€‘', ''],  # ã€ æ ¸å‡†æ—¥æœŸ ã€‘ã€ ä¿®æ”¹æ—¥æœŸ ã€‘
    [r'\d{4}-\d{2}-\d{2}', ''],  # 2008-07-23
    [r' *<\/?sub> *', ''],  # åˆ é™¤sub
    # æ¬çš„ä¸é¦™åŒ»ç”Ÿä»£ç 
    [r'ï¼ˆå¦‚?ä¸Š?ä¸‹?é¢?å·¦?[^ï¼ˆ]*å›¾[ ï¼Œç¤º]*?ç‰‡? ?[\d\w]?\d? ?ï¼‰', ''],#åˆ é™¤å¸¦å›¾çš„å†…å®¹
    [r'å›¾ä¸­ï¼š[\u4e00-\u9fa5ï¼Œ]*', ''],  # ç®€å•æ•°æ®åŒ¹é…
    [r'(?:å…·ä½“)?å¦‚å›¾ï¼š', ''],# ç®€å•æ•°æ®åŒ¹é…
    [r'.*æ›´å¤šå†…å®¹.*?ã€‚', ''],# ç®€å•æ•°æ®åŒ¹é…
    [r'åŠ¨ä½œé“¾æ¥ï¼š[\u4e00-\u9fa5\w\.\d ]*', ''],#å¸¦åŠ¨ä½œé“¾æ¥
    [r'\*\*.*?(?:é«˜èƒ½é¢„è­¦ ?â†“?)\*\*|é«˜èƒ½é¢„è­¦â†“|ğŸ‘‡', ''],#æ— å…³æ–‡æœ¬åˆ é™¤
    [r'(?:https?://|ç½‘ç«™ï¼š|[^\n]*(?:å›¾Â \d\ï¼Œ)?å›¾ç‰‡æ¥æºï¼š?:?\xa0?|å›¾ 1ï¼Œæ¥æºï¼š)(?:www)?[^\n]*|www.g6pd.*org', ''],#åˆ é™¤ç‰¹å®šå†…å®¹ï¼Œéƒ¨åˆ†å¯èƒ½å¤šä½™
    [r'(?:(?:ï¼ˆ?å›¾ ?\d .*)?æ¥æºï¼šè‡ª?|å¤šä½™æ¯›å‘çš„å»é™¤ )UpToDate(?: ?ä¸´åºŠé¡¾é—®ï¼‰?)?', ''],#åˆ é™¤ç‰¹å®šå†…å®¹ï¼Œéƒ¨åˆ†å¯èƒ½å¤šä½™
    [r'.*(?:å›¾ ?\d\.|å›¾æº|å›¾ \dï¼š|å›¾\W\.|çš®è‚¤æ€§ç—…è¯Šæ–­å›¾è°±|å›¾ç‰‡ï¼š).*', ''],#åˆ é™¤æ— å…³æ–‡æœ¬
    [r'\*\*\d\. (?:å¤´éƒ¨æ§åˆ¶|è…¹éƒ¨æ‚¬åŠ|å‚ç›´æ‚¬åŠ)è¯•éªŒï¼Œ?(?:\*\*)?', ''],#åŒ¹é…ç‰¹å®šçš„æ–‡æœ¬
    [r'ï¼ˆ[^ï¼ˆï¼‰äººæµ·ä»¥]*(?:[å‚è¯¦]|ç‚¹å‡»|è§[ä¸‹])[^ï¼ˆï¼‰åŠ ]*ï¼‰', ''],#åˆ é™¤ï¼ˆå‚çœ‹ç¦å¿Œä¸€æ ï¼‰ å¿…é¡»æœ‰ä¸­æ–‡æ‹¬å·
    [r'å…³äº[^ã€‚ï¼ï¼Ÿ]*?è¯¦è§[^ã€‚ï¼ï¼Ÿ]*?å†…å®¹ã€‚?', ''],#åŒ¹é…å•ä¸ªå†…å®¹
    [r'(?:[^ã€‚ï¼Ÿ(?:\n\n)]|(?:å¯ç‚¹å‡»))*è¯¦æƒ…[^ã€‚\n\*]*[\nã€‚]?', ''],#åŒ¹é…æ— å…³æ–‡æœ¬

    [r'\[\d+\]', ''],  # åˆ é™¤æ•°å­—è§’æ ‡ç­‰
    [r'å‚è€ƒèµ„æ–™[\s\S]*', ''],  # ç®€å•æ•°æ®åŒ¹é…
    [r'æ•°æ®æ¥æºï¼š', ''],  # ç®€å•æ•°æ®åŒ¹é…
    ['.*è§ä¸‹å›¾ï¼š', ''],  # ç®€å•æ•°æ®åŒ¹é…
    ['ã€‚[^ã€‚]*tbody[^ã€‚]*ã€‚', ''],  # ç®€å•æ•°æ®åŒ¹é…
    ['\x80-\x9eÂ¢Â®Â¿Â£Â¬Â©Â¨Â¸\xadÂ¶Â¡Â¦Â§]+', ''],  # åˆ é™¤ç‰¹æ®Šç¬¦å·
    [r'\([è¯¦å‚]è§[^\)ï¼‰]*[ï¼‰\)]', '']  # åˆ é™¤å¸¦æ‹¬å·çš„å‚è§è¯¦è§ç­‰å†…å®¹

]
pattern_jiance=[
    [r'.*(?:å›¾ ?\d\.|å›¾æº|å›¾ \dï¼š|å›¾\W\.|çš®è‚¤æ€§ç—…è¯Šæ–­å›¾è°±|å›¾ç‰‡ï¼š).*', ''],

]#æ£€æµ‹è¡¨è¾¾å¼åˆ—è¡¨

class speicalProces:
    def __init__(self):
        pass

    # å»é™¤ ...ç™»å½• é—®é¢˜
    def step1_drop_login(self, content):
        split_token = "\n\n"
        if split_token not in context:
            split_token = "\n"
        pattern = r'å½•ç™»\s*\.\.\.(.*?)[ã€‚ï¼Œ,ã€ï¼š:ï¼›(]'
        text = content.strip(' ').split(split_token)
        for i in range(len(text)):
            text[i] = re.sub(pattern, 'ã€‚', text[i][::-1])[::-1]
        for i in range(len(text)):
            pattern1 = r'[\u4e00-\u9fa5]/[\u4e00-\u9fa5]'  # åˆ é™¤ä¸­æ–‡æ–‡æœ¬ä¸­å¤šä½™çš„/ï¼Œå¦‚åŒ»/é™¢ï¼Œä½†æ˜¯ä¸åˆ é™¤å•ä½é‡Œçš„/ï¼Œå¦‚kg/m2
            sq_list = re.findall(pattern1, text[i])
            for j in sq_list:
                if j in text[i]:
                    text[i] = text[i].replace(j, j.replace('/', ''))
        text = split_token.join(text)
        return text

    # å»é™¤ç©ºæ ¼é—®é¢˜
    def step2_strip(self, context):
        split_token = "\n\n"
        if split_token not in context:
            split_token = "\n"
        new_context = []
        context = context.split(split_token)
        for item in context:
            item = item.lstrip().rstrip()
            context_s = item.split("\n")
            for i in range(len(context_s)):
                context_s[i] = context_s[i].lstrip().rstrip()
            item = "\n".join(context_s)
            new_context.append(item)
        return split_token.join(new_context)


def clean_text(context):
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"

    result = []
    sp = speicalProces()
    # context = sp.step1_drop_login(context)
    for item in context.split(split_token):
        item = item.strip(split_token)
        for pattern_item in pattern_list_zh:
            b = re.findall(pattern_item[0], item)
            for i in b:
                if i != '' and pattern_item in pattern_jiance:
                    log.write(str(i) + '\n')
            item = re.sub(pattern_item[0], pattern_item[1], item)
        result.append(item)
    context = split_token.join(result)

    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # context = re.sub('\n\n-*', "", context)
    context = re.sub(r'\n-{3,}\n', "", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


fw = open("../../../../full_data/dingxiangyuan/dingxiangyuan_clean.jsonl", "w", encoding="utf-8")
with open("../../../../full_data/dingxiangyuan/dingxiangyuan_preformat.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        context = item["text"]
        context = post_process(context)
        context = clean_text(context)
        sp = speicalProces()
        context = post_process(context)

        context = sp.step2_strip(context)
        context = re.sub(r'(\n\nã€[^ã€‘]* *ã€‘)+(\n\nã€[^ã€‘]* *ã€‘)', r'\2', context)
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        fw.write(item + "\n")
