import json
import re
from tqdm import tqdm
import math
import spacy
import random
from nltk.corpus import wordnet
import inflect
inflect = inflect.engine()
import kenlm
from nltk.tokenize import word_tokenize
import fasttext
from collections import defaultdict
model = kenlm.LanguageModel(r"C:\Users\Administrator\Desktop\4k_gram.klm")
fasttext_model = fasttext.load_model('fenlei_model.bin')


import signal
from tqdm import tqdm
from contextlib import contextmanager

# pattern_list = [
#
#
#
#     # å»é™¤å¸¦æœ‰ç½‘å€çš„å¥å­,å…³é”®è¯   wwwã€comã€htmlã€http
#     # todo www,httpæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå³è¾¹ç•Œçš„å‡†ç¡®æ€§éœ€è¦è€ƒè™‘
#     # todo com,htmlæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå·¦è¾¹ç•Œéœ€è¦è€ƒè™‘
#     [r'([^\d][^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',r''],
#
#     [r'(\n\s*[a-zA-Z\.]\s*\n)', r''],
#     [r'([^\n]*Copyright[^\n]*)', r''],
#     [r'(ISBN\s*[A-Z0-9-]*)', r''],
#     # å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
#     [r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ)', r''],
#     # æ— å…³æ–‡æœ¬
#     # æ— å…³å›¾ç‰‡å¼•ç”¨
#     # [r'[Ff]ig\s?\.\s?\d+\s?\.\s?[\w\s-\(\)]+\.(\n{1,2})?', r''],
#     [r'((\()\b([Ff]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|Fig|p)\b:?\.?\s*(\d+)?\.?.*?(\)))',r''],
#     [r'([^\n\.]*\b(Fig[s]?[ure]?[s]?)\b\.?[\s*Â ](\d+)?\.?[^\n]*)', r''],
#     [r'((\([^\(]{0,20})\s[Ff]igures?[^\.\(\)]*.)',r''],
#     [r'([\(\[]\s?([Tt]able|[sS]ee)[^\)\]]*?[\)\]])',r''],
#     [r'[^\n\.]*Fig[s]?(ure)?s?(\s\d\.)?[^\n\.]*\.',r''],
#     # æ•°å­—å¼•ç”¨
#     #1.å¸¦æ‹¬å· book (1ï¼Œ2).
#     [r'(\([\d\s,\.\-â€“]{1,50}\))',r''],
#     #2.ä¸å¸¦æ‹¬å·ä½†æ˜¯æ•°å­—å‰æ˜¯å¥å·. æ•°å­—åæ˜¯æ¢è¡Œæˆ–è€…å¤§å†™å­—æ¯
#     # å¸¦[]ä¸ºguidelineså†™çš„
#     [r'([^\d])(\[\s?\d{1,4}(\s{0,3}[\-â€“â€”,\.]\s{0,3}\d{1,4}){0,20}\s?\])',r'\1'],
#     # [r'([^\d])([1-9]{1,3}(\s{1,3}[\-â€“,\.]\s{1,3}[1-9]{1,3}){1,20})([^\d]?)', r'\1åˆ é™¤2:<u>\2</u>\4'],
#     # # ä¸å¸¦æ‹¬å·
#     [r'([^\d])([1-9][0-9]{1,4}(\s{1,3}[\-â€“,\.]\s{1,3}[1-9][0-9]{1,4}){1,20})([^\d]?)', r''],
#     # ä¸å¸¦æ‹¬å·æ•°å­—æˆ–æ•°å­—åŠ ç‚¹ï¼ˆå®¹æ˜“è¯¯åˆ ï¼‰
#     [r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)', r""],
#     # å‚è€ƒå¼•ç”¨ï¼šæ•°å­—ç±»å‹ï¼ŒåŠ å…¥ç©ºæ ¼åšæ³›å[1,2 - 3, 4, 5]
#     [r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)', r""],
#     # # å‚è€ƒåº”ç”¨çš„å¤æ‚æ ¼å¼ï¼šå­—æ¯(Ia,\nb)
#     [r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]', r""],
#     # ä¸Šä¸€å¥çš„å¥å·\dä¸­é—´æ²¡æœ‰ç‚¹[å¤§å†™]ï¼Œè¿™é‡Œçš„\dåé¢æ²¡æœ‰ç‚¹ä¸æ˜¯åºå·å¯èƒ½æ˜¯ä¸Šä¸€å¥çš„å¼•ç”¨æ•°å­—
#     [r'([^\d][\.,])((\s{1,3}\d+[\s\n]{1,3}){1,5}\.?)(\s?[A-Z])',r'\1\4'],
#     # ä»‹è¯å‰é¢æœ‰æ•°å­—ä¼šæœ‰é—®é¢˜
#     # [r'((\d+[\.,]?){1,5})(\s(and|or|the|:)\s)',r'åˆ é™¤7:<u>\1</u>\3'],
#     # ç»™guidelinesè¡¥å……
#     # [r'([^\d][\.,]\s?)(\d{1,4}(\s{0,3}[\-â€“,\.\s]\s{0,3}[1-9][0-9]{1,4}){0,20})(\n|\s?[A-Z])',r'\1\4'],
#     [r'([^\d][\.,]\s?)(\d{1,4}(\s{0,3}[\-â€“,\.\s]\s{0,3}\d{1,4}){0,20})(\n|\s?[A-Z]|$)',r'\1\4'],
#     # ç»“å°¾å¥å·åé¢ä¸ºæ•°å­—å’Œåºå·åŒºåˆ«å¼€åºå·åé¢è¿˜æœ‰ä¸€ä¸ª.
#     # [r'([^\d]\.)(\s?\d{1,4}(\s{0,3}[\-â€“,\.]\s{0,3}[1-9][0-9]{1,4}){0,20})(\n)',r'\1\4'],
#     [r'(#{1,3})\n',r'\1'],
#  ]
# å¸¦æ ‡ç­¾ç‰ˆ
pattern_list = [
# å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
[
    r'([^\d][^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',
    r'åˆ é™¤1:<u>\1</u>'
],
[
    r'(\n\s*[a-zA-Z\.]\s*\n)',
    r'åˆ é™¤2:<u>\1</u>'
],
[
    r'([^\n]*Copyright[^\n]*)',
    r'åˆ é™¤3:<u>\1</u>'
],
[
    r'(ISBN\s*[A-Z0-9-]*)',
    r'åˆ é™¤4:<u>\1</u>'
],
[
    r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ|ï¿½|âˆ‘|âœ¦)',
    r'åˆ é™¤5:<u>\1</u>'
],
[
    r'(\(\s?\b([Ff]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|[Ff]igs?|p|FIGURE)\b[^\)\n]*\))',
    r''
],

[
    r'((\([^\(]{0,20})\s[Ff]igures?[^\.\(\)]*.)',
    r'åˆ é™¤8:<u>\1</u>'
],
[
    r'([\(\[]\s?([Tt]able|[sS]ee)[^\)\]]*?[\)\]])',
    r'åˆ é™¤9:<u>\1</u>'
],
[
    # r'([^\n\.]*\bFigs?(ure)?s?\b\.?[\s*Â ](\d+)?\.?[^\n]*)',
    r'([^\n\.]*(Figs?(ure)?s?|FIGURE)\s?\.?\s?(\d+\.\d+)?[^\n\.]*\.?)',
    r'åˆ é™¤7:<u>\1</u>'
],
# [
#     r'([^\n\.]*Fig[s]?(ure)?s?(\s\d\.)?[^\n\.]*\.)',
#     r'åˆ é™¤10:<u>\1</u>'
# ],
[
    r'(\([\d\s,\.\-â€“]{1,50}\))',
    r'åˆ é™¤11:<u>\1</u>'
],
[
    r'([^\d])(\[\s?\d{1,4}(\s{0,3}[\-â€“â€”,\.]\s{0,3}\d{1,4}){0,20}\s?\])',
    r'\1åˆ é™¤12:<u>\2</u>'
],
[
    r'([^\d])([1-9][0-9]{1,4}(\s{1,3}[\-â€“,\.]\s{1,3}[1-9][0-9]{1,4}){1,20})([^\d]?)',
    r'\1åˆ é™¤13:<u>\2</u>\4'
],
[
    r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)',
    r'åˆ é™¤14:<u>\1</u>'
],
[
    r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]',
    r'åˆ é™¤15:<u>\1</u>'
],
[
    r'([^\d][\.,])((\s{1,3}\d+[\s\n]{1,3}){1,5}\.?)(\s?[A-Z])',
    r'\1åˆ é™¤16:<u>\2</u>\4'
],
[
    r'([^\d][\.,]\s?)(\d{1,4}(\s{0,3}[\-â€“,\.\s]\s{0,3}\d{1,4}){0,20})(\n|\s?[A-Z]|$)',
    r'\1åˆ é™¤17:<u>\2</u>\4'
],
[
    r'(#{1,3})\n',
    r'åˆ é™¤18:<u>\1</u>'
],
[
    r'(\([^\(\)]{0,100}\set\sal[^\(\)]{0,100}\))',
    r'åˆ é™¤19:<u>\1</u>'
],
# [
#     r'(?<=(\d\.\d))(\n ?\n?)(?=[A-Za-z\(])',
#     r'åˆ é™¤20:<u>\1</u>'
# ]
]

context_pattern = [
    [r'(Â¬\s*)', r''],
    [r'(\(\s*\))', r'']
]

# nlp = spacy.load("en_core_web_trf")
nlp = spacy.load("en_core_web_sm")

class speicalProces:
    def __init__(self):
        pass

    def is_not_upper(self, char):
        return not char.isupper()

    def dict_merge(self, xy_num, set_x, flag=True):

        sorted_keys = sorted(xy_num.keys())
        # åˆå§‹åŒ–
        merged_dict = defaultdict(float)
        current_key_group = []
        current_value_sum = 0
        # éå†æ’åºåçš„ keys å¹¶è¿›è¡Œåˆå¹¶
        for key in sorted_keys:
            if not current_key_group:
                current_key_group.append(key)
                current_value_sum += xy_num[key]
            else:
                if key - current_key_group[-1] <= 18:
                    current_key_group.append(key)
                    current_value_sum += xy_num[key]
                else:
                    # è®¡ç®—å½“å‰ç»„çš„å¹³å‡ key å’Œç´¯åŠ çš„ value
                    avg_key = sum(current_key_group) / len(current_key_group)
                    merged_dict[avg_key] = current_value_sum
                    # é‡ç½®ç»„
                    current_key_group = [key]
                    current_value_sum = xy_num[key]
        # æœ€åä¸€ç»„çš„å¤„ç†
        if current_key_group:
            avg_key = sum(current_key_group) / len(current_key_group)
            merged_dict[avg_key] = current_value_sum

        if flag:  # å·¦ä¾§  æŒ‰ç…§å€¼ï¼ˆvalueï¼‰çš„é™åºå’Œé”®ï¼ˆkeyï¼‰çš„å‡åºæ’åº
            res_left = dict(sorted(merged_dict.items(), key=lambda x: (-x[1], x[0])))
            # print(res_left)
            if len(res_left) >= 2:
                keys = list(res_left.keys())
                values = list(res_left.values())
                if values[0] - values[1] <= 6 and keys[1] < keys[0]:
                    keys[0], keys[1] = keys[1], keys[0]
                    values[0], values[1] = values[1], values[0]
                    res_left = dict(zip(keys, values))
            # first_key = next(iter(res_left.keys()))
            for x in res_left.keys():
                if x < set_x / 3 and res_left[x] >= 4:
                    first_key = x
                    break
                else:
                    first_key = 0
            # print(res_left)
        else:  # å³ä¾§  æŒ‰ç…§å€¼ï¼ˆvalueï¼‰çš„é™åºå’Œé”®ï¼ˆkeyï¼‰çš„é™åºæ’åº
            res_right = dict(sorted(merged_dict.items(), key=lambda x: (x[1], x[0]), reverse=True))
            # print(res_right)
            if len(res_right) >= 2:
                keys = list(res_right.keys())
                values = list(res_right.values())
                if values[0] - values[1] <= 6 and keys[1] > keys[0]:
                    keys[0], keys[1] = keys[1], keys[0]
                    values[0], values[1] = values[1], values[0]
                    res_right = dict(zip(keys, values))
            # first_key = next(iter(res_right.keys()))
            for x in res_right.keys():
                if x > set_x * 2 / 3 and res_right[x] >= 4:
                    first_key = x
                    break
                else:
                    first_key = set_x
            # print(res_right)
        return first_key

    def is_page_foot(self, least_bbox, img_box, first_left, first_right):
        """
        å·¦å³è¾¹è§’ï¼Œå®½åº¦å·®å€¼å°block[2]-block[0.]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­å·¦å³è¾¹ç•Œ
        ä¸Šä¸‹è¾¹è§’ï¼Œé«˜åº¦å·®å€¼å°block[3]-block[1]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­ä¸Šä¸‹è¾¹ç•Œ
        """
        # æ£€æµ‹å³æµ‹è¾¹è§’
        if img_box[2] - least_bbox[0] <= first_right:
            return True
        # æ£€æµ‹ä¸‹é¢è¾¹è§’
        elif img_box[3] - least_bbox[1] <= 60:
            return True
        # æ£€æµ‹å·¦ä¾§è¾¹è§’
        elif least_bbox[2] - img_box[0] <= first_left:
            return True
        # ä¸Šè¾¹è§’ä¼šé‡åˆ°æ ‡é¢˜è¿™ä¸ªé—®é¢˜ï¼Œè¦ä¸è¦è§£å†³ï¼Ÿ
        elif least_bbox[3] - img_box[1] <= 60:
            return True
        else:
            return False

    def step1_drop_Pagefooter(self, item):
        # print(json.dumps(item, ensure_ascii=False))
        """
        1.éå†full_blocksåˆ¤æ–­æ˜¯å¦ä¸ºé¡µè¾¹è§’
        2.åœ¨textä¸­æ‰¾åˆ°å†…å®¹ç»™åˆ æ‰
        """
        raw_info = item['attr']['raw_info']
        img_box = item['attr']['img_box']
        # text = item['text'].split('\n\n')
        # print(len(text))
        # print(len(raw_info))
        # print(text)
        if raw_info != []:
            set_x = img_box[2]
            x1_num = {}
            x2_num = {}
            for raw in raw_info:
                raw_context = raw['raw_context']
                for bb in raw_context:
                    bbox = bb["bbox"]
                    x1, y1, x2, y2 = bbox
                    if x1 not in x1_num:
                        x1_num[x1] = 1
                    else:
                        x1_num[x1] += 1
                    if x2 not in x2_num:
                        x2_num[x2] = 1
                    else:
                        x2_num[x2] += 1
            first_left = int(self.dict_merge(x1_num, set_x))
            first_right = int(self.dict_merge(x2_num, set_x, flag=False))
            # delete_index = []
            # for index,raw in enumerate(raw_info):
            #     full_blocks = raw['full_blocks']
            #     if self.is_page_foot(full_blocks, img_box,first_left,first_right):
            #         delete_index.append(index)
            # for index in delete_index:
            #     text.pop(index)
            # if not text:
            #     item['text'] = ""
            #
            # else:
            #     item['text'] = text
            replace_num = 0
            for index, raw in enumerate(raw_info):
                full_blocks = raw['full_blocks']
                if self.is_page_foot(full_blocks, img_box, first_left, first_right):
                    replace_num += 1
                    # å¦‚æœå¤§å—è¢«åˆ æ‰æ•´æ®µå†…å®¹
                    block_text = raw['block_text'].strip()
                    if not block_text:
                        continue
                    elif len(block_text) <= 3:
                        continue
                    # å¯¹ least_text è¿›è¡Œæ­£åˆ™è½¬ä¹‰
                    escaped_least_text = re.escape(block_text)

                    # æ„å»ºæ­£åˆ™æ¨¡å¼ï¼ŒåŒ¹é…å¯èƒ½çš„å‰åç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œè¿å­—ç¬¦
                    # pattern = re.compile(escaped_least_text + r'[\s\n-]{0,5}')

                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åŒ¹é…çš„æ–‡æœ¬
                    try:
                        item['text'] = re.sub(rf'\b{escaped_least_text}\b', rf'åˆ é™¤è¾¹è§’:{block_text}', item['text'])
                    except re.error as e:
                        print(f"Regex error: {e}")

            if replace_num > 10:
                item['text'] = '(æœ¬é¡µåˆ é™¤)æœ¬é¡µä¸­æœ‰è¶…è¿‡10ä¸ªè¾¹è§’çš„åˆ¤æ–­æ€€ç–‘æ˜¯ç›®å½•é¡µæœ¬é¡µåˆ é™¤' + item['text']
            # print(item['text'])
        return item
    def step2_delete_photopage(self, item):
        raw_info = item['attr']['raw_info']
        img_box = item['attr']['img_box']
        img_area = (img_box[2] - img_box[0]) * (img_box[3] - img_box[1])
        all_block_area = []
        for raw in raw_info:
            full_blocks = raw['full_blocks']
            block_area = (full_blocks[2] - full_blocks[0]) * (full_blocks[3] - full_blocks[1])
            all_block_area.append(block_area)

        if len(raw_info) <= 3 and sum(all_block_area) < img_area * 0.2:
            item['text'] = "(æœ¬é¡µåˆ é™¤)æ­¤é¡µçš„å†…å®¹éƒ¨åˆ†æ‰€å çš„æ¯”ä¾‹å°äº0.2" + item['text']
            # è¿”å›ç©ºçš„text
            # item['text'] = ""
        return item
    def get_score(self, sentence):
        tokenize_text = word_tokenize(sentence)
        final_text = " ".join(tokenize_text)
        if len(tokenize_text) == 0:  # æ£€æŸ¥åˆ†è¯åæ˜¯å¦ä¸ºç©º
            return float('inf')  # æˆ–è€…è¿”å›å…¶ä»–é€‚å½“çš„å€¼ï¼Œå¦‚ 0 æˆ–è€…æŸä¸ªé«˜åˆ†
        score = model.score(final_text, bos=False, eos=False)
        length = len(tokenize_text)
        score = (10 ** (-score / length))
        return score
    def is_merge_ngram(self,text, next_text):
        """
        1.ç»™textæ‰“åˆ†ï¼Œnext_textåˆ†åˆ«æ‰“åˆ†
        2.ç»™marge_textæ‰“åˆ†
        3.åˆ¤å®šmarge_textæ»¡è¶³åˆ†æ•°å°äºtextå’Œnext_textï¼Œä¸”å°äºæŸä¸ªå€¼ è¿™ä¸ªå€¼å¯èƒ½æ˜¯5000ã€3000ã€2000... è¿”å›True
        :return:
        """
        text_score = self.get_score(text)
        next_text_score = self.get_score(next_text)
        merge_text = text + " " + next_text
        merge_text_sorce = self.get_score(merge_text)
        if merge_text_sorce < text_score or merge_text_sorce < next_text_score:
            # print(merge_text)
            return True

    def is_merge_duannei(self, text, next_text):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A']
        if re.search(r'\s\d+\.$', text): #
            return True
        elif re.search(r'^([\d+A-Z][\.,]){1,3}$', text) and next_text.lstrip()[0].isupper():  # åŒ¹é…æ®µè½ä¸­åªæœ‰åºå·ä¸”ä¸‹ä¸€æ®µæ˜¯å¤§å†™å¼€å¤´çš„æƒ…å†µ
            return True
        elif any(text.rstrip().endswith(" " + prep) for prep in preposition_list):  # åŒ¹é…åŒä¸€æ®µä¸­ä»‹è¯ç»“å°¾çš„
            return True
        elif text.rstrip()[-1] in ['-']:
            return True
        elif "#" not in text and "*" not in text and re.search(r'[^\.?!]$', text) and re.match(r'^[a-z]', next_text.lstrip()):
            return True
        elif re.search(r'\([^\)]*$|\[[^\]]*$', text) and re.match(r'^[^\(\[]*[\)\]]',next_text):  # å‰ä¸€è¡Œæœ‰ä¸€ä¸ªæœªå¯¹åº”çš„å·¦æ‹¬å·ï¼Œä¸‹ä¸€è¡Œå‰é¢æœ‰ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„å³æ‹¬å·
            return True
        elif ((text.rstrip()[-1].islower() or text.rstrip()[-1] in [',']) and (next_text.lstrip()[0].isupper() or next_text.lstrip()[0] in ['(',')','"','â€','â€œ']) and "#" not in text and "#" not in next_text and self.is_merge_ngram(text, next_text)):   # å‰ä¸€è¡Œå°å†™æˆ–é€—å·ç»“å°¾ï¼Œä¸‹ä¸€è¡Œå¤§å†™å¼€å¤´ï¼Œä¸”ä¸æ ‡é¢˜æ— å…³ï¼ŒåŠ å…¥ngramåˆ¤æ–­
            return True
        elif re.search(r'^#{1,3}\s?[\d\.]{1,10}$',text) and next_text.lstrip()[0].isupper():
            return True
        return False
    def is_merge_duan(self,stripped_item):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A']
         # æ–°å¢æ¡ä»¶: å¦‚æœ stripped_item ç»“å°¾çš„å•è¯åœ¨ preposition_list ä¸­
        if any(stripped_item.rstrip().endswith(" " + prep) for prep in preposition_list):
            return True
        elif stripped_item and stripped_item[-1] == '-':
            return True
        elif re.search(r'Figs?\.$',stripped_item):
            return True
        elif re.search(r'\([^\)]*$|\[[^\]]*$', stripped_item) :
            return True
        elif len(stripped_item) == 1 and stripped_item in ['.','Â·']:
            return True
        return False

    def step3_more_linefeed(self, context):
        # print(context)
        index = 0
        while index < len(context):
            item = context[index]
            # å°† item æŒ‰ "\n" åˆ†å‰²
            item_sections = re.split(r'\n', item)
            section_index = 0

            while section_index < len(item_sections) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                if self.is_merge_duannei(item_sections[section_index],item_sections[section_index + 1]):
                    item_sections[section_index] += "|åˆ é™¤æ®µå†…æ¢è¡Œ|" + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]
                else:
                    section_index += 1  # åªæœ‰åœ¨ä¸åˆå¹¶æ—¶æ‰è‡ªå¢

            # æ›´æ–° item ä»¥åæ˜ åˆå¹¶çš„æ®µè½
            item = '\n'.join(item_sections)
            context[index] = item
            # åˆå¹¶ä»¥å°å†™å­—æ¯æˆ–ç‰¹å®šæ ‡ç‚¹ç¬¦å·å¼€å¤´çš„æ®µè½
            stripped_item = context[index].strip()
            # print(stripped_item)
            if index >= 0:
                if index > 0 and stripped_item and context[index - 1][-1] not in ['.','!','?'] and (stripped_item[0].islower() or stripped_item[0] in ["(", "[", ")", "]", "."]):
                    """
                    é‡åˆ°å°å†™å¼€å¤´çš„æ®µ 1.ä¸Šä¸€æ®µæ²¡æœ‰#ç›´æ¥è¿ä¸Šå» 2.ä¸Šä¸€æ®µæœ‰#ä½†æ˜¯ä¸æ­¢ä¸€è¡Œï¼Œåˆ‡ä¸Šä¸€æ®µçš„æœ€åä¸€è¡Œå’Œå½“å‰çš„ç¬¬ä¸€è¡Œï¼Œä½¿ç”¨æ¨¡å‹åˆ¤æ–­è¯¥ä¸è¯¥è¿ 3.ä¸Šä¸€æ®µæœ‰#ä½†æ˜¯åªæœ‰ä¸€è¡Œï¼Œå»ä¸Šä¸Šæ®µåˆ‡æœ€åä¸€è¡Œå’Œå½“å‰çš„ç¬¬ä¸€è¡Œï¼Œæ¨¡å‹åˆ¤æ–­è¯¥ä¸è¯¥è¿
                    """
                    # ä¸Šä¸€æ®µä¸èƒ½å‡ºç°#ï¼Œå‡ºç°#è¯æ˜æ˜¯æ ‡é¢˜æ®µ
                    if not re.search(r'#',context[index - 1]):
                        # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                        context[index - 1] = context[index - 1].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-1|" + item.lstrip()
                        # åˆ é™¤å½“å‰ item
                        del context[index]
                        # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue
                    elif len(re.split(r'\n', context[index - 1])) >= 2:
                        # ä¸Šä¸€æ®µæœ‰æ ‡é¢˜ä¹Ÿæœ‰æ­£æ–‡ï¼Œåˆ†å‰²context[index-1]æœ€åä¸€è¡Œï¼Œåˆ†å‰²stripped_itemçš„ç¬¬ä¸€è¡Œ
                        previous_paragraph_lines = re.split(r'\n', context[index - 1])
                        last_line_of_previous = previous_paragraph_lines[-1].strip()
                        first_line_of_current = stripped_item.splitlines()[0].strip()
                        if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                            # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                            context[index - 1] = context[index - 1].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-2|" + item.lstrip()
                            # åˆ é™¤å½“å‰ item
                            del context[index]
                            # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                            index = index - 1
                            continue
                    # elif len(re.split(r'\n', context[index - 1])) >= 2:
                    #     # ä¸Šä¸€æ®µæœ‰æ ‡é¢˜ä¹Ÿæœ‰æ­£æ–‡ï¼Œåˆ†å‰²context[index-1]æœ€åä¸€è¡Œï¼Œåˆ†å‰²stripped_itemçš„ç¬¬ä¸€è¡Œï¼Œå¦‚æœæ ‡é¢˜åœ¨æœ€åä¸€è¡Œçœ‹å€’æ•°ç¬¬äºŒè¡Œ
                    #     previous_paragraph_lines = re.split(r'\n', context[index - 1])
                    #     last_line_of_previous = previous_paragraph_lines[-1].strip()
                    #     first_line_of_current = stripped_item.splitlines()[0].strip()
                    #
                    #     if '#' in last_line_of_previous:  # å¦‚æœæœ€åä¸€è¡Œä¸­å‘ç°æœ‰#
                    #         last_line_of_previous = previous_paragraph_lines[-2].strip()
                    #         if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                    #             # åˆå¹¶åˆ°å€’æ•°ç¬¬äºŒè¡Œå
                    #             previous_paragraph_lines[-2] = previous_paragraph_lines[-2].rstrip() + " " + item.lstrip()
                    #             # ç§»é™¤æœ€åä¸€è¡Œ
                    #             previous_paragraph_lines.pop(-1)
                    #             # é‡æ–°åˆå¹¶ context[index - 1] çš„å†…å®¹
                    #             context[index - 1] = '\n'.join(previous_paragraph_lines)
                    #             # åˆ é™¤å½“å‰ item
                    #             del context[index]
                    #             # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                    #             index = index - 1
                    #             continue
                    #     else:
                    #         if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                    #             # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                    #             context[index - 1] = context[index - 1].rstrip() + " " + item.lstrip()
                    #             # åˆ é™¤å½“å‰ item
                    #             del context[index]
                    #             # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                    #             index = index - 1
                    #             continue
                    elif len(re.split(r'\n',context[index-1])) == 1:
                        if index - 2:
                            previous_paragraph_lines = re.split(r'\n', context[index - 2])
                            last_line_of_previous = previous_paragraph_lines[-1].strip()
                            first_line_of_current = stripped_item.splitlines()[0].strip()
                            if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                                # åˆå¹¶context[index - 2]
                                context[index - 2] = context[index - 2].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-3|" + item.lstrip()
                                # åˆ é™¤å½“å‰ item
                                del context[index]
                                # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                                index = index - 1
                                continue

                elif self.is_merge_duan(stripped_item) and stripped_item is not None:
                    if index + 1 < len(context) and "#" not in context[index + 1] and not re.search(r'^[Ff]ig',context[index + 1].lstrip()):
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-4|" + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue
                # å½“å‰æ®µä¸æ˜¯æ ‡é¢˜æ²¡æœ‰ç»“å°¾æ ‡ç‚¹ï¼Œåˆ‡ç‰‡è¿˜æ˜¯ç”¨æ¨¡å‹åˆ¤æ–­ä¸€ä¸‹
                elif "#" not in stripped_item and re.search(r'[^\.?!:ï¼šâ€"\)]$', stripped_item.strip()):
                    if index + 1 < len(context):
                        """
                        é¦–å…ˆå½“å‰æ®µæ²¡æœ‰#ä¸æ˜¯æ ‡é¢˜
                        å–å½“å‰æ®µçš„æœ€åä¸€è¡Œ æ²¡æœ‰æ ‡ç‚¹ç»“å°¾
                        å–ä¸‹ä¸€æ®µçš„ç¬¬ä¸€è¡Œ ä¹Ÿä¸æ˜¯æ ‡é¢˜è¡Œ æ¨¡å‹åˆ¤æ–­åˆå¹¶æ˜¯å¦åˆé€‚
                        """
                        previous_paragraph_lines = re.split(r'\n', stripped_item)
                        last_line_of_current = previous_paragraph_lines[-1].strip()
                        first_line_of_next = context[index + 1].splitlines()[0].strip()
                        if index + 1 < len(context) and "#" not in context[index + 1] and self.is_merge_ngram(last_line_of_current,first_line_of_next) and not re.search(r'^[Ff]ig',first_line_of_next):
                            # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                            context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-5|" + context[index + 1].lstrip()
                            # åˆ é™¤ä¸‹ä¸€ä¸ª item
                            del context[index + 1]
                            # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                            index = index-1
                            continue

            index += 1
        # print(context)
        return context

    def step4_lack_linefeed(self,context):
        new_context = []
        for item in context:
            # æŸ¥æ‰¾ #â€¢ï¼Œå¹¶åœ¨å…¶å‰ååŠ æ¢è¡Œç¬¦
            # parts = re.split(r'(?<=\s)([#â€¢]{1,3}\s?[A-Z][^#â€¢]*)', item)
            # å®šä¹‰å¤šä¸ªåˆ†éš”ç¬¦å¹¶ç”¨ç«–çº¿è¿æ¥
            splitchar = r'([#â€¢]{1,3}\s)'
            # ä½¿ç”¨è¿æ¥åçš„æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†
            parts = re.split(splitchar, item)
            new_parts = []
            for part in parts:
                if part is None or part.strip() == '':
                    continue
                else:
                    # é‡åˆ°[#â€¢]{1,3}åœ¨å‰é¢åŠ ä¸Šæ¢è¡Œç¬¦è¿›è¡Œæ¢è¡Œ
                    if re.search(splitchar, part) and part.strip() is not None:
                        part = '\n|æ’å…¥æ¢è¡Œ-0|'+part
                        new_parts.append(part.strip())
                    elif re.search(r'(?![Ff]ig)([\.\)\:])(\s+[\da-z]{1,2}\.\s?[A-Z])',part):
                        # part = re.sub(r'(?![Ff]ig)([\.\)\:])(\s[\da-z]{1,2}\.\s?[A-Z])',r'\1\n\2',part)
                        part = re.sub(r'([^([Ff]ig][\.\)\:])(\s+[\da-z]{1,2}\.\s?[A-Z])', r'\1\2\n|æ’å…¥æ¢è¡Œ-1|', part)
                        if part.strip() is not None:
                            new_parts.append(part + '\n')
                    else:
                        new_parts.append(part)

            item = "".join(new_parts)
            # print(item)
            new_context.append(item)
        return new_context

    def get_person_idx(self, item):
        doc = nlp(item)
        person_block = []
        person_num = 0
        # print(doc.ents)
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if len(person_block) == 0:
                    person_block.append([ent.start_char, ent.end_char])
                elif ent.end_char - person_block[-1][-1] > 5:
                    person_block.append([ent.start_char, ent.end_char])
                else:
                    person_block[-1][-1] = ent.end_char
                person_num += 1
        return person_block, person_num

    def step5_rm_cite(self, item):
        # patterns = [
        #     r'\.\s?\b\d{4}\b',  # å¹´ä»½ï¼Œæ¯”å¦‚ . 2010
        #     r'\b\d{4}\b\s?;',  # å¹´ä»½ï¼Œæ¯”å¦‚ 2010;
        #     r'(?:Journal|Proceedings|Conference|Studies|Review|BMJ|JAMA|Pediatrics|Crit Care Med|Nurs Crit Care|Acad Emerg Med|Health Serv Res)',
        #     # æœŸåˆŠæˆ–ä¼šè®®åçš„å…³é”®è¯
        #     r'(?:doi:\s*\S+)',  # DOI
        #     r'(?:vol\.?\s*\d+)',  # å·å·
        #     r'(?:no\.?\s*\d+)',  # æœŸå·
        #     r'(?:pp\.?\s*\d+\s*-\s*\d+)',  # é¡µç èŒƒå›´
        #     r'\d+\s?:\d+\s?[â€“-]\s?\d+',  # é¡µç èŒƒå›´æ ¼å¼
        #     r'\set al\.'  # et al
        # ]
        cite_tag = []
        cite_index = 1 if len(re.findall(r'\[\d+\]', item)) > 0 else 0
        cite_year = 1 if len(
            re.findall(r'\[\d\d\d\d\]', item) or re.findall(r'\.\s?\b\d{4}\b', item) or re.findall(r'\b\d{4}\b\s?[;\.]',
                                                                                                   item)) else 0  # å¹´ä»½ï¼Œæ¯”å¦‚ . 2010 ã€ 2010;
        cite_page = 1 if len(re.findall(r'\d+\s?:\d+\s?[â€“-]\s?\d+', item)) else 0
        cite_page2 = 1 if len(re.findall(r'[\.,]\s?\d+\s?[â€“-]\s?\d+[\.,]', item)) else 0
        cite_J = 1 if len(re.findall(r'\[[Jj]\]', item)) else 0
        cite_doi = 1 if " doi " in item else 0
        cite_etal = 1 if (" et al" in item or 'et\u00A0al' in item or 'etÂ al' in item) else 0
        cite_vol = 1 if (" vol. " in item or " Vol " in item or " Vol. " in item or " vol " in item) else 0
        cite_pp = 1 if (" Pp " in item or " pp " in item or " Pp. " in item or " pp. " in item) else 0
        # cite_page = 1 if len(re.search(r'\.\s?\b\d{4}\b',item)) else 0
        cite_phonenum = 1 if re.search(r" [Pp]hone:|Fax:", item) else 0
        mulu = 1 if re.search(r'[\.\s]{15,}', item) else 0
        cite_tag = [cite_index, cite_year, cite_J, cite_doi, cite_etal, cite_page, cite_vol, cite_phonenum, cite_pp,cite_page2]
        if sum(cite_tag) > 1 and '|' not in item:
            return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(item)
        person_block, person_num = self.get_person_idx(item)
        # è¶…è¿‡5ä¸ªäººå
        person_block_lens = [block_item[1] - block_item[0] for block_item in person_block]
        person_lens = sum(person_block_lens)
        if len(item) > 0 and person_lens / len(item) > 0.5 and len(item) > 100:
            return "å‚è€ƒåˆ é™¤-1:<u>{}</u>".format(item)
        # åªæœ‰ä¸ªåå­—æ•°é‡
        elif person_num > 5 and '|' not in item and len(item) < 200:
            return "å‚è€ƒåˆ é™¤-2:<u>{}</u>".format(item)
        elif sum(cite_tag) > 0 and person_num > 0 and len(item) < 200:
            # print(item)
            return "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(item)
        elif mulu:
            return "ç›®å½•åˆ é™¤:<u>{}</u>".format(item)
        else:
            return item

    def step5_removepage(self, context):
        # context æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ª item æ˜¯ä¸€æ®µå†…å®¹
        context_lens = len(context)
        # ç”¨äºç»Ÿè®¡æœ‰å¤šå°‘ä¸ªæ®µè½ä¸­å‡ºç°äº†äººå
        num = 0
        mulu_num = 0
        new_context = []
        references_started = False
        for item in context:
            # è¿”å›çš„itemæ˜¯å·²ç»è¢«é‡å†™è¿‡çš„item
            if item == "##References":
                references_started = True
            if references_started:
                item = "å‚è€ƒåˆ é™¤-4:<u>{}</u>".format(item)
            else:
                item = self.step5_rm_cite(item)
            # æ–°çš„itemé‡æ–°åŠ å…¥ä¸€ä¸ªæ–°çš„åˆ—è¡¨
            new_context.append(item)
            # åˆ¤æ–­itemæ˜¯å¦è¢«åˆ¤å®šæœªå‚è€ƒæ–‡çŒ®
            if re.search(r'å‚è€ƒåˆ é™¤',item):
                # å¦‚æœå½“å‰æ®µè½ä¸­æœ‰äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾
                num += 1
            elif re.search(r'ç›®å½•åˆ é™¤',item):
                mulu_num += 1
        # print(new_context)
        # å¯¹æ•´é¡µçš„ä¸€ä¸ªåˆ¤æ–­
        if context_lens >= 4 and num >= context_lens * 0.5:
            new_context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µåœ¨è¶…è¿‡ä¸€åŠçš„æ®µè½ä¸­å‘ç°äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾")
            # return []
        elif mulu_num > 0:
            new_context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µå‘ç°ç›®å½•çš„ç‰¹å¾")
            # return []
        # else:
        #     # åˆ é™¤ new_context ä¸­è¢«æ ‡è®°ä¸ºå‚è€ƒåˆ é™¤çš„ item
        #     new_context = [item for item in new_context if not re.search(r'å‚è€ƒåˆ é™¤', item)]
        return new_context

    def step6_ngram_deletenum(self, context):
        # print(context)
        """
        å¾ªç¯ context é‡Œé¢æ¯ä¸ª item, åˆ‡åˆ† item, åˆ‡åˆ†åæ¯ä¸ªæœ€å°å•ä½å°±æ˜¯ä¸€è¡Œå†…å®¹ï¼Œä½¿ç”¨ ngram åˆ¤å®šæ•°å­—
        :param context:
        :return: new_context
        """
        new_context = []

        for item in context:
            item_sections = re.split(r'\n', item)
            new_item_sections = []
            for section in item_sections:
                # print(section)

                section = section.strip()
                if len(section) == 0:
                    new_item_sections.append(section)
                    continue
                else:
                    pattern = r'\d+(\s?[\-â€“\.,]?(to|and)?\s?\d+){0,10}'
                    best_score = self.get_score(section)

                    while True:
                        matches = list(re.finditer(pattern, section))
                        if not matches:
                            break

                        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ•°å­—åŠå…¶ä½ç½®
                        numbers_with_positions = [(match.group(), match.start(), match.end()) for match in matches]

                        # æ ‡è®°æ˜¯å¦æ›´æ–°äº†æ–‡æœ¬
                        updated = False

                        for num, start, end in numbers_with_positions:
                            # print(num,start,end)
                            # å¦‚æœæ˜¯å¼€å¤´çš„æ•°å­—ï¼Œä»–å¯èƒ½æ˜¯åºå·ç›´æ¥è·³è¿‡
                            if start >= 0 and start < 4:
                                continue
                            # ç‰¹æ®Šç¬¦å·åé¢çš„æ•°å­—ä¹Ÿéƒ½æ˜¯åˆç†äº† ä¸ç”¨æ£€æŸ¥ç›´æ¥è·³è¿‡
                            elif start > 0 and (section[start - 1] in ['$','>','<','='] or section[start - 2] in ['$','>','<','=']):
                                continue
                            # ä½¿ç”¨ä½ç½®è¿›è¡Œæ›¿æ¢
                            modified_text = section[:start] + section[end:]
                            modified_score = self.get_score(modified_text)

                            if modified_score < best_score:
                                best_score = modified_score  # æ›´æ–°å½“å‰æœ€ä¼˜åˆ†æ•°
                                section = modified_text  # å°†åˆ†æ•°ä½çš„æ–‡æœ¬é‡æ–°èµ‹ç»™text
                                updated = True
                                break

                        # å¦‚æœæ²¡æœ‰æ›´æ–°æ–‡æœ¬ï¼Œè·³å‡ºå¾ªç¯
                        if not updated:
                            break
                    new_item_sections.append(section)

            new_context.append('\n'.join(new_item_sections))

        return new_context


    def step7_is_shortpage(self,context):
        duanluo_num = len(context)
        short_duanluo_num = 0
        all_duanluo_num = 0
        if duanluo_num <= 3:
            for item in context:
                if len(item.strip()) < 100:
                    short_duanluo_num += 1
                if len(item.strip()) < 300:
                    all_duanluo_num += 1
            if short_duanluo_num > 1 and all_duanluo_num == 3:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº3ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰2æ¡ä»¥ä¸Šåœ¨100ä»¥ä¸‹")
                # return []
        elif duanluo_num <= 5:
            for item in context:
                if len(item.strip()) < 80:
                    short_duanluo_num += 1
            if short_duanluo_num > 3:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº5ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰4æ¡ä»¥ä¸Š80ä»¥ä¸‹")
                # return []
        else:
            # æ®µè½çŸ­
            for item in context:
                if len(item.strip()) < 50:
                    short_duanluo_num+=1
            if short_duanluo_num > duanluo_num * 0.7:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µæœ‰è¶…è¿‡ä¸€åŠçš„æ®µè½é•¿åº¦å°äº50å­—ç¬¦")   # å¦‚æœæœ‰å¾ˆå¤šæ ‡é¢˜æ€ä¹ˆåŠï¼Œä¸€ä¸ªæ ‡é¢˜ä¸€æ®µæ–‡å­—
                # return []
        return context

    def step8_is_mulupage(self,context,page_num):
        """
        ç›®å½•é¡µçš„ç‰¹ç‚¹:
        1.æ–‡ç« å‰å‡ é¡µ
        2.å¯èƒ½å­˜åœ¨å¤šä¸ª\.çš„æƒ…å†µ(å‚è€ƒæ–‡çŒ®å¤„ç†ä¸­å·²ç»æŠŠ\.å¤šçš„é¡µç»™åˆ æ‰äº†)
        3.å¾ªç¯contextï¼Œitemé•¿åº¦çŸ­æ•°é‡å¤š
        :param context: æ–‡æœ¬ä¿¡æ¯
        :param page_num: é¡µç 
        :return: context
        """
        short_item_num = 0
        if page_num <= 10 and len(context) >= 10:
            for item in context:
                if len(item.strip()) <= 200:
                    short_item_num += 1

            if short_item_num > len(context) * 0.7:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µåˆ¤æ–­ä¸ºå‚è€ƒé¡µæˆ–ç›®å½•é¡µ")
                # return []
        text = "\n".join(context)
        label = fasttext_model.predict(text.strip().replace('\n', ''))
        print(label[0][0])
        if label[0][0] in ['__label__mulu', '__label__cankao']:
            context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µè¢«æ¨¡å‹åˆ¤æ–­ä¸ºå‚è€ƒé¡µæˆ–ç›®å½•é¡µ")
        return context

    def step9_complete_sequence_number(self, text):

        state = True
        pattern = r'((?<=\n)(\d{1,2})|^(\d{1,2}))\.[^\n]*\.\n(([^\d][^\d][^\n\d]+\n)+)\n+(\d{1,2})\.[^\d]'

        while state:
            new_text = text
            result = re.search(pattern, text)

            if result:
                # print(text)
                fir_num = result.group(1)
                last_num = result.group(6)

                while int(last_num) == int(fir_num) + 1 or int(last_num) == int(fir_num) or int(last_num) < int(
                        fir_num):
                    start_index = result.start() + len(result.group())
                    new_text = new_text[start_index:]
                    result = re.search(pattern, new_text)
                    if result:
                        new_text = new_text
                        fir_num = result.group(1)
                        last_num = result.group(6)
                    else:
                        state = False
                        break
                if not result:
                    continue

                target_part = result.group(4)
                differ = int(result.group(6)) - int(result.group(1))
                target_list = target_part.split('\n')
                target_list = [x for x in target_list if x]

                for index, part in enumerate(target_list):
                    part = re.escape(part)
                    replace_part = re.sub(r'\\(.)', r'\1', part)
                    text = re.sub(part, str(int(fir_num) + index + 1) + '. ' + 'ç¼ºå°‘åºå·è¡¥å……' + replace_part, text)
                    if index + 2 == differ:
                        return text

            else:
                state = False

        return text


def clean_text(context):
    split_token = "\n\n"


    result = []
    sp = speicalProces()
    """
    ç›®å‰çš„é¡ºåº
    step1:åˆ é™¤é¡µè¾¹è§’
    step2:åˆ é™¤å›¾ç‰‡é¡µ
    step3:è§£å†³å¤šäºæ¢è¡Œ
    step4:è§£å†³ç¼ºå°‘æ¢è¡Œ
    step5:è§£å†³å‚è€ƒæ–‡çŒ®ä»¥åŠå‚è€ƒæ–‡çŒ®é¡µ
    æ­£åˆ™æ›¿æ¢  
    step6:ngramåˆ é™¤æœªæ›¿æ¢çš„æ•°å­—
    step7:åˆ é™¤çŸ­é¡µ
    """
    context = sp.step1_drop_Pagefooter(context)

    context = sp.step2_delete_photopage(context)
    page_num = context["attr"]["page_num"]

    context = post_process(context["text"])
    # contextæ˜¯ä¸€ä¸ªä»¥ä¸¤ä¸ªæ¢è¡Œç¬¦ä¸ºåˆ‡å‰²æ¡ä»¶çš„åˆ—è¡¨
    context = context.split(split_token)
    # å¤šä½™æ¢è¡Œ
    context = sp.step3_more_linefeed(context)
    # print(context)
    # ç¼ºå°‘æ¢è¡Œ
    context = sp.step4_lack_linefeed(context)
    # print(context)
    # åˆ¤å®šå‚è€ƒæ–‡çŒ®
    context = sp.step5_removepage(context)
    # print(context)
    if context and len(context) > 0 and context is not None:
        for item in context:
            item = item.strip(split_token).strip()

            # if "## References" in item:
            #     continue
            for pattern_item in pattern_list:
                item = re.sub(pattern_item[0], pattern_item[1], item)
                item = item.strip()
            for pattern_item in context_pattern:
                item = re.sub(pattern_item[0], pattern_item[1], item)
            result.append(item)
    else:
        result = []

    # print(result)
    # ä½¿ç”¨NLPæ¨¡å‹åˆ¤æ–­å‚è€ƒæ–‡çŒ®
    context = sp.step6_ngram_deletenum(result)
    # print(context)
    # åˆ¤æ–­æ•´é¡µçŸ­è·¯é•¿çŸ­
    context = sp.step7_is_shortpage(context)

    # å‰10é¡µ
    context = sp.step8_is_mulupage(context,page_num)

    context = split_token.join(context)
    context = sp.step9_complete_sequence_number(context)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)

    # context = re.sub("[li][\.,]" , '1.' ,context)
    return context

def process_line(items, sp):
    item = json.loads(items.strip())
    context = clean_text(item,  sp)
    context = post_process(context)
    if len(context) < 100:
        return
    item["text"] = context
    item = json.dumps(item, ensure_ascii=False)
    return item




fw = open("C:/pycharm/orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®/pdf/clean_json/medicalpdf_preformat_en_4.jsonl", "w", encoding="utf-8")
with open("C:/pycharm/orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®/pdf/clean_json/medicalpdf_preformat_en.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        # if item["seq_id"] == "2d19c9ae-66ce-468d-8cd4-58771cd4fe31":
        # print(item)
        context = item
        # lang = item["lang"]

        context = clean_text(context)
        context = post_process(context)
        # print(context)
        if len(context) < 100:
            continue
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        # print(item)
        fw.write(item + "\n")




# æ–‡ä»¶è·¯å¾„
# input_file_path = "C:\\pycharm\\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\\pdf\\clean_json\\guidelines_liangyong_surya_preformat_en.jsonl"
# output_file_path = "C:\\pycharm\\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\\pdf\\clean_json\\guidelines_liangyong_surya_preformat_en_4.jsonl"
#
# # è¯»å–æ‰€æœ‰è®°å½•
# with open(input_file_path, "r", encoding="utf-8") as fs:
#     lines = fs.readlines()
#     # # éšæœºæŠ½å–5000æ¡è®°å½•
#     # sampled_lines = random.sample(lines, 5000)
# # å¤„ç†å¹¶ä¿å­˜æŠ½å–çš„è®°å½•
# with open(output_file_path, "w", encoding="utf-8") as fw:
#     for items in tqdm(lines):
#         item = json.loads(items.strip())
#         # if item["seq_id"] == "f7afd344-b77a-4e73-92f1-65eb9910689a":
#         context = item
#
#         # æ¸…æ´—å’Œå¤„ç†æ–‡æœ¬
#         context = clean_text(context)
#         context = post_process(context)
#
#         if len(context) < 100:
#             continue
#
#         item["text"] = context
#         item = json.dumps(item, ensure_ascii=False)
#         # print(item)
#         fw.write(item + "\n")
