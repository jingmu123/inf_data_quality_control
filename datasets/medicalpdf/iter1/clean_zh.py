# coding:utf-8
# https://github.com/shibing624/pycorrector?tab=readme-ov-file

import json
import re
import time

from tqdm import tqdm
import sys

from inf_data_quality_control.basic_tools import sentence_correct

sys.path.append("../../../")
# from basic_tools import sentence_correct
import threading
# çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—
from queue import Queue
import spacy
import jieba

pattern_list_zh = [
    # fig cite
    [r"([è§å’Œ]å›¾\s?\d+\s?[ã€‚ï¼Ÿï¼\.]?)", r"åˆ é™¤1ï¼š<u>\1</u>"],
    # å‚è€ƒå¼•ç”¨ï¼šæ•°å­—ç±»å‹ï¼ŒåŠ å…¥ç©ºæ ¼åšæ³›åŒ–ï¼š[1,2 - 3, 4, 5]
    [r'(\[\s?(\d{1,3}\s?[-â€“,ï¼Œ\.]+\s?)+\d+\s?\]\s?\*?)', r"åˆ é™¤2ï¼š<u>\1</u>"],
    # å‚è€ƒè¡¥å……ï¼Œä¸€äº›ç¼ºå°‘]çš„ï¼Œä»¥ç¬¦å·ä½œä¸ºå³è¾¹ç•Œ
    [r"(\[\s?\"?\s?(\d{1,3}\s?[-â€“,\.]+\s?)+\d?\s*\*?)([ã€‚\.,ï¼Œ\u4e00-\u9fa5a-zA-Z\(ï¼ˆ]\s?)", r"åˆ é™¤4ï¼š<u>\1</u>\3"],
    [r"([\u4e00-\u9fa5]\s[^\u4e00-\u9fa5\d~ã€‚ï¼Ÿï¼]*?)(\s?\d+\s*?)(\n|ã€‚|\s\.)", r"\1åˆ é™¤5ï¼š<u>\2</u>\3"],
    [r"([\u4e00-\u9fa5])(\s*?\.\s*?\d+\s*?)([\(\u4e00-\u9fa5ã€‚\n])", r"\1åˆ é™¤6ï¼š<u>\2</u>\3"],
    [r'([\u4e00-\u9fa5ã€‹)])(\s?\d+\s?)(ã€‚|\s\.)', r"\1åˆ é™¤7ï¼š<u>\2</u>\3"],

    # # å‚è€ƒåº”ç”¨çš„å¤æ‚æ ¼å¼ï¼šå­—æ¯(Ia,\nb)
    [r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]', r"åˆ é™¤3ï¼š<u>\1</u>"],

    # å¢åŠ æ¢è¡Œï¼šä¸¤ç§æ¦‚å¿µæƒ…å†µï¼Œä¸€ç§æ•°å­—å°æ ‡é¢˜ï¼Œä¸€ç§# æ ‡é¢˜
    [r'^(?!\n)(\d\.(\d{1,2}\.){0,10}\d?)(\s?[^\d])', r'\nå¢åŠ æ¢è¡Œ5:_\1\3_'],
    [r'(\u4e00-\u9fa5\s{3,}|ã€‚|ï¼Ÿ|ï¼)(\d\.(\d{1,2}\.){0,10}\d?)(\s{1,10}[^\da-zA-Z\|%])', r'\1\nå¢åŠ æ¢è¡Œ6:_\2\3_'],
    [r'^(?!\n)([#â€¢]{1,4}\s{,4})', r'\nå¢åŠ æ¢è¡Œ7:_\1_'],
    [r'ã€‚\s+(\d{1,2}\.)', r'\nå¢åŠ æ¢è¡Œ8:_\1_'],
    [r'ã€‚\s?\s*?([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€])', r'\nå¢åŠ æ¢è¡Œ9:_\1_'],
    # -------å¢åŠ æ¢è¡Œï¼š(7)
    [r'([;ï¼›ã€‚ï¼ï¼Œ,]\s?)([\(ï¼ˆ]?\d+[ï¼‰\)]\s?)', r'\1\nå¢åŠ æ¢è¡Œ1:_\2_'],
    [r'([ã€‚ï¼ï¼Ÿ]\s?)(ã€[\u4e00-\u9fa5]*?ã€‘)', r'\1\nå¢åŠ æ¢è¡Œ2:_\2_'],
    [r'(\(\s?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å-]+\s?\))', r'\nå¢åŠ æ¢è¡Œ3:_\1_'],
    [r'([:ï¼š]\s?)(\d+[\.ã€]\s?[\u4e00-\u9fa5])', r'\1\nå¢åŠ æ¢è¡Œ4:_\2_'],

    # å¢åŠ å‚è€ƒé”™è¯¯çš„å‚è€ƒcase:[....]
    [r'(\[[â€¦\.\*\sâ€˜â€™\"]*?\])', r"åˆ é™¤8ï¼š<u>\1</u>"],
    [r'(\[[â€¦\.\*\sâ€˜â€™\"]*?\d+\])', r"åˆ é™¤9ï¼š<u>\1</u>"],
    [r'(\s?\d+\s\[\s?\d+\s?)', r"åˆ é™¤10ï¼š<u>\1</u>"],

    # å›¾å¼•ç”¨1
    [r'(^[ã€‚\n].*?è§å›¾[a-zA-Z\d]+.*?)[ã€‚\n]', r"åˆ é™¤11ï¼š<u>\1</u>"],
    # å›¾å¼•ç”¨2
    [r'([\(ï¼ˆ]\s?å›¾\s?[a-zA-Z\d]\s?[å’Œä¸,ï¼Œã€]?\s?å›¾\s?[a-zA-Z\d]\s?[\)ï¼‰])', r"åˆ é™¤12ï¼š<u>\1</u>"],
    # å›¾å¼•ç”¨3
    [r'(\(å›¾[a-zA-Z\d]+\))', r"åˆ é™¤13ï¼š<u>\1</u>"],
    # å›¾æ³¨1
    [r'(å›¾[a-zA-Z\d]+\s{0,2}[ã€‚\n]?)', r"åˆ é™¤14ï¼š<u>\1</u>"],
    # å›¾æ³¨2
    [r'(Fig[\s\.\d]+[a-zA-Z]*?[ã€‚\n]?)', r"åˆ é™¤15ï¼š<u>\1</u>"],

    [r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|â–²|â­|â—|â—|Û|\s__\s|ï¿¥)', r' ç‰¹æ®Šç¬¦å·åˆ é™¤<u>\1</u>'],
    [r'((ï¼ˆ|\(|ã€ˆ|\[)? ?(å›¾|è¡¨|è§è¡¨|å›°|å‚è§|è‡³å›¾|é™„å›¾) ?(\d+[-|â€“\.\/,\sã€~]+)+(\d+)?([^\n.ã€‚\u4e00-\u9fa5\d]*(ï¼‰|\)|ã€‰|\])|))',r' å›¾ç‰‡ç´¢å¼•åˆ é™¤<u>\1</u>'],
    [r'([\[](\s?\d+[\s,\.\-\]ï¼‰ã€]+)+) ',r' åˆ é™¤è§’æ ‡ä¸€<u>\1</u>'],
    [r'\(\s?[\-â€“]\d+\s?\)',r'åˆ é™¤è§’æ ‡äºŒ<u>\1</u>'],
    [r'(\b([a-zA-Z] ){3,}[a-zA-Z]?\b)',r' åˆ é™¤è‹±æ–‡å™ªå£°<u>\1</u>'],

    # ç‰¹æ®Šcase(èƒŒæ™¯|é¡µçœ‰|é¡µè„š)
    [r'(å°ç‹—(ç§‘ç ”|å…¬å¼€|ç©¶|è¯¾|è®¾|ç¨‹)+)', r"åˆ é™¤16ï¼š<u>\1</u>"],
    [r'((å°ç‹—)?ç§‘ç ”(å…¬å¼€|ç©¶)(è¯¾|è®¾)ï¼Ÿ)', r"åˆ é™¤17ï¼š<u>\1</u>"],
    [r'^[ã€‚\n](.*?å¹´ç¬¬?\d+å·ç¬¬?\d+æœŸ[ã€‚\n])', r"åˆ é™¤18ï¼š<u>\1</u>"],
    [r'((æœ¬æ–‡)?ç¼–è¾‘: .*)', r"åˆ é™¤19ï¼š<u>\1</u>"],
    [r'(^[Aa]dd\s?[sS]?)', r"åˆ é™¤20ï¼š<u>\1</u>"],
    [r'(\.[Aa]dd[sS]\.?$)', r"åˆ é™¤21ï¼š<u>\1</u>"],
    # [r"((Â¶)\s?)",r"åˆ é™¤21:<u>\1</u>"],
    [r"(\s?[Ã—âˆ©â€¡â™‚Â¿â˜Â®âŒ’ï¿½Â¶â™€ã€Œâ€¦]\s?)", r"åˆ é™¤22ï¼š<u>\1</u>"],
    # [r"(\s?[\xAC00-\xD7AF]\s?)",r"åˆ é™¤201:<u>\1</u>"],
    [r"([^\u4e00-\u9fa5ã€‚\n\]]*)(å°[ç‹—è¡—].{0,3}[ç§‘ç ”è‘¡å…¬å¼€è®¾è®¨ç¤¾èŠ±ç©¶æ‰€è¯¾ç³»å¿ƒ,\d\sa-zA-Z]+\s?)([^\u4e00-\u9fa5ã€‚\n]?)",
     r"\1åˆ é™¤23ï¼š<u>\2</u>\3"],
    [r"(çš„?ç§‘ç ”ç©¶*å…¬*å…±*é€š*)", r"åˆ é™¤24ï¼š<u>\1</u>"],
    [r"(\( *\))", r"åˆ é™¤25ï¼š<u>\1</u>"],
    # åˆ é™¤é¡µç 
    [r'(â€¢?\s?\d{1,10}\s?â€¢)', r"åˆ é™¤26ï¼š<u>\1</u>"],

    # åˆ é™¤è‹±æ–‡å¼€å¤´å’Œç»“å°¾çš„
    # [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])",r"åˆ é™¤26:<u>\1</u>\2"],
    # [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$',r"\1åˆ é™¤22:<u>\2</u>"],
    [r"([^\u4e00-\u9fa5ã€‚\n]+ä¸‹è½½.*?)([\sã€‚\n][\u4e00-\u9fa5ã€‚\n])", r"åˆ é™¤27ï¼š<u>\1</u>\2"],

]

context_pattern = [
    [r"[ã€‚ï¼Œ?!,]([ã€‚ï¼Œ?!,])", r"\1"],

]


class speicalProces:
    def __init__(self):
        self.sc = sentence_correct.SentenceCorrector()
        self.special_token = ["Â¶", ",", ".", "-", "|"]
        self.special_mapping = {
            "æœ¨": "æœ¯",
        }
        self.skip_token = ["è«–", "å¦„"]
        # self.en_nlp = spacy.load("zh_core_web_trf")
        self.zh_nlp = spacy.load("zh_core_web_sm")

    def fix_details(self, ditail_list):
        new_detail_list = []
        for detail_item in ditail_list:
            detail_pos_start = detail_item[2]
            detail_pos_end = detail_item[3]
            raw_token = detail_item[0]
            new_token = detail_item[1]
            if raw_token in ["è«–", "å¦„"]:
                continue
            if len(re.findall("\d", raw_token)) > 0:
                continue
            if len(re.findall(r"[^\u4e00-\u9fa5]", raw_token)) > 0:
                continue
            if raw_token in self.special_mapping:
                new_token = self.special_mapping[raw_token]
            new_detail_list.append([raw_token, new_token, detail_pos_start, detail_pos_end])
        return new_detail_list

    def step1_correct_error_token(self, context):
        # å•è¯çº é”™
        rank_length = 480
        context_list = []
        for pos in range(0, len(context), rank_length):
            context_list.append(context[pos:pos + rank_length])
        if len(context_list) == 0:
            return ""
        sc_result = self.sc.process_text(context_list)

        final_correct_list = []
        for item, sc_item in zip(context_list, sc_result):
            # çº é”™å†…å®¹åå¤„ç†
            sc_details = self.fix_details(sc_item["details"])
            # ä¿è¯çº é”™çš„æ•°é‡
            if len(sc_details) == 0 or len(sc_details) > 3:
                final_correct_list.append(item)
            else:
                final_content = ""
                pre_idx = 0
                for detail_item in sc_details:
                    [raw_token, new_token, detail_pos_start, detail_pos_end] = detail_item
                    final_content += "{}{}<u>çº é”™:{}</u>".format(item[pre_idx:detail_pos_start], new_token, raw_token)
                    pre_idx = detail_pos_end
                final_content += item[pre_idx:]
                final_correct_list.append(final_content)
        return "".join(final_correct_list)

    def step2_rm_kongge(self, context):
        context = context.lstrip().rstrip()
        content = context.split(" ")
        first_content = content[0]
        last_content = " ".join(content[1:])
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', last_content)
        # å¤šæ‰§è¡Œä¸€æ¬¡ï¼Œå¼¥è¡¥æ­£åˆ™è¾¹ç•Œé‡å é—®é¢˜ï¼›
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', final_content)
        if len(final_content) == 0:
            return final_content

        merge_piece = first_content + final_content.lstrip()[0]

        split_word = list(jieba.cut(merge_piece, cut_all=False))
        if len(split_word[-1]) > 1:
            return first_content + final_content
        return first_content + " " + final_content

    def step3_rm_noise_dig_alpha(self, context):
        # åˆ é™¤å•ç‹¬çš„é›¶ç¢ï¼ˆå­—æ¯+æ•°å­—ï¼‰
        import numpy as np
        # æœ‰æ±‰å­—
        if len(re.findall(r'[\u4e00-\u9fa5]', context)) > 0:
            return context

        # å¦‚æœæ˜¯åºå·
        if len(re.findall(r'[\da-zA-Z]+\.', context)) == 0:
            return context

        # # æ²¡æœ‰æ•°å­—
        # if len(re.findall(r'\d',context)) == 0:
        #     return context

        if len(context.split(" ")) == 1 or len(context.split(" ")) > 5:
            return context

        if "." in context and len(re.findall(r'[A-Z]', context)) == 0:
            return context

        split_word = [len(word) for word in context.split(" ")]
        mean_word_len = np.mean(split_word)
        if mean_word_len > 3:
            return context
        else:
            # return "æœ¬æ®µåˆ é™¤1:<u>{}</u>".format(context)
            return ""

    def get_person_idx(self, context):
        doc = self.zh_nlp(context)
        person_block = []
        person_num = 0
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if len(person_block) == 0:
                    person_block.append([ent.start_char, ent.end_char])
                elif ent.end_char - person_block[-1][-1] > 5:
                    person_block.append([ent.start_char, ent.end_char])
                else:
                    person_block[-1][-1] = ent.end_char
                person_num += 1
        return person_block, person_num

    def step4_rm_cite(self, context_raw):
        context = str(context_raw)
        cite_index = 1 if len(re.findall(r'\[\d+\]', context)) > 0 else 0
        cite_year = 1 if len(re.findall(r'\d\d\d\d', context)) > 0 else 0
        cite_J = 1 if len(re.findall(r'\[[Jj]\]', context)) > 0 else 0

        cite_doi = 1 if len(re.findall(r'[dD][oO][iI]', context)) > 0 else 0
        cite_cip = 1 if len(re.findall(r'[cC][iI][pP]', context)) > 0 else 0

        cite_etal = 1 if "et al" in context else 0
        za_zhi = 1 if "æ‚å¿—" in context else 0
        cite_vol = 1 if len(re.findall(r'[vV][oO][lL]', context)) > 0 else 0
        cite_jan = 1 if len(re.findall(r'[jJ][aA][nN]', context)) > 0 else 0
        cite_email = 1 if "mail" in context else 0
        cite_com = 1 if "com" in context else 0
        cite_fix = 1 if "ä¿®è®¢" in context else 0
        cite_comm = 1 if "å§”å‘˜ä¼š" in context else 0
        cite_auth = 1 if "ä½œè€…" in context or "ä¸»ç¼–" in context else 0
        cite_zhuanjia = 1 if "ä¸“å®¶" in context else 0
        cite_chuban = 1 if "å‡ºç‰ˆ" in context else 0
        cite_rexian = 1 if "çƒ­çº¿" in context else 0
        cite_ISBN = 1 if len(re.findall(r'[iI][sS][bB][nN]', context)) > 0 else 0

        cite_tag = [cite_index, cite_year, cite_J, cite_doi, cite_cip, cite_etal,
                    za_zhi, cite_vol, cite_jan, cite_email, cite_com,
                    cite_fix, cite_comm, cite_auth, cite_zhuanjia, cite_chuban, cite_rexian, cite_ISBN]
        if sum(cite_tag) >= 3 and len(context) < 500:
            return ""
        if sum(cite_tag) >= 2 and len(context) < 300:
            # return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(context_raw)
            return ""

        # åŸºäºäººååˆ¤å®š
        person_block, person_num = self.get_person_idx(context)
        person_block_lens = [block_item[1] - block_item[0] for block_item in person_block]
        person_lens = sum(person_block_lens)

        if person_lens / len(context) > 0.3:
            # return "å‚è€ƒåˆ é™¤-1:<u>{}</u>".format(context_raw)
            return ""
        if person_num > 5:  # è¶…è¿‡5ä¸ªäººå
            # return "å‚è€ƒåˆ é™¤-2:<u>{}</u>".format(context_raw)
            return ""
        # elif cite_index and person_num > 0 and len(context) < 150:
        #     return "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(context_raw)
        #     # return ""
        else:
            return context_raw

    def is_complete(self, last_sentence, curr_sentence):
        if "|" in last_sentence or "|" in curr_sentence:
            return True
        last_sentence = last_sentence.lstrip(" ").rstrip(" ").strip("\n")
        curr_sentence = curr_sentence.lstrip(" ").lstrip("\n")

        if len(last_sentence) == 0:
            return False
        if last_sentence.strip(" ")[-1] in ["ã€‚", "ï¼Ÿ", "ï¼", ".", "?", "!", ")"]:
            return True
        elif last_sentence.strip(" ")[-1] in ["ã€", "å¯¹", "ä¸", "å’Œ", "åŠ", "å…¶", "ä½†", "ä¸”", "ä¹ƒ", "~", "çš„", "ç”±",
                                              "äº", "è‡´"]:
            return False
        # elif "#" in last_sentence or "#" in curr_sentence:
        elif len(re.findall(r'^#', last_sentence)) > 0 or len(re.findall(r'^#', curr_sentence)) > 0:
            return True
        elif len(re.findall(r'[-ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å](\s\.|ã€)', last_sentence + curr_sentence)) > 0:
            return True
        elif len(re.findall(r'[a-zA-Z\d]$', last_sentence)) > 0 or len(re.findall(r'^[a-zA-Z\d]', curr_sentence)) > 0:
            return False

        try:
            if len(last_sentence) > 6:
                merge_sentence = last_sentence[-5:] + curr_sentence[0]

            else:
                merge_sentence = last_sentence[-2:] + curr_sentence[0]
        except:
            return True
        # print(merge_sentence)
        split_word = list(jieba.cut(merge_sentence, cut_all=False))
        # print(split_word)
        if len(split_word[-1]) > 1:
            return False
        return True

    def rm_lid_piece(self, context):
        context_list = context.split("\n")
        final_list = []
        final_list.append(context_list[0])
        idx = 1
        while idx < len(context_list):
            if self.is_complete(final_list[-1], context_list[idx]):
                final_list.append(context_list[idx])
            else:
                # print("æ¢è¡Œåˆ é™¤1", final_list[-1], "-----------",context_list[idx])
                final_list[-1] = "{} {}".format(final_list[-1], context_list[idx])
            idx += 1
        return "\n".join(final_list)

    def rm_lid_context(self, context):
        context_list = context.split("\n\n")
        final_list = []
        final_list.append(context_list[0])
        idx = 1
        while idx < len(context_list):
            sent_complete = self.is_complete(final_list[-1], context_list[idx])
            if sent_complete:
                final_list.append(context_list[idx])
            else:
                # print("æ¢è¡Œåˆ é™¤2",final_list[-1],context_list[idx])
                final_list[-1] = "{} {}".format(final_list[-1], context_list[idx])
            idx += 1
        return "\n\n".join(final_list)

    def step5_rm_english_noise(self, context):
        find_pattern = [
            [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])", 0],
            [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$', -1]
        ]
        pattern = [
            [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])", r"\2"],
            [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$', r"\1"],
        ]

        for idx, item in enumerate(find_pattern):
            [p_item, p_index] = item
            result = re.findall(p_item, context)
            # print(result)
            if len(result) == 0:
                continue
            elif len(result[0][p_index].strip().split(" ")) == 1:
                continue
            else:
                # print(result[0][p_index].strip().split(" "))
                context = re.sub(pattern[idx][0], pattern[idx][1], context)
        return context

    def step6_rm_short_context(self, context):
        context_list = context.split("\n\n")
        context_lens = [len(item) for item in context_list]
        if context.count("|") >= 3:
            return context
        if len(context_list) >= 5:
            return context
        if max(context_lens) > 100:
            return context
        elif len(context_list) > 2 and max(context_lens) >= 50:
            return context
        else:
            # print(context)
            # print("="*20)
            # return "æ•´é¡µåˆ é™¤\n\n{}".format(context)
            return ""

    def step7_rm_digital(self, context):
        res = re.findall(r"\d", context)
        no_digital = re.sub(r"\d", "", context)
        context_lens = len(list(set(no_digital.split(" "))))

        if len(res) > 8 and (context_lens) < 5:
            # return "åˆ é™¤32:<u>{}</u>".format(context)
            return ""
        return context


def clean_text(context, lang, sp):
    # print("-"*10)
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"
    pattern_list = pattern_list_zh

    # åˆ†è§£å¤„ç†
    result = []

    for idx, item in enumerate(context.split(split_token)):
        if idx == 0:
            item = item.lstrip().rstrip()
            res = re.findall(r"\d+", item)
            if len(res) / len(item) > 0.5:
                # item = "åˆ é™¤0:<u>{}</u>".format(item)
                # result.append(item)
                continue

        item = sp.step4_rm_cite(item)
        # 1.æ­£åˆ™
        for pattern_item in pattern_list:
            item = item.lstrip(" ").rstrip(" ")
            item = re.sub(pattern_item[0], pattern_item[1], item)

        # 2.æ›¿æ¢ å›ºå®šå†…å®¹ï¼ˆéæ³›åŒ–ï¼‰
        for replace_item in context_pattern:
            item = re.sub(replace_item[0], replace_item[1], item)
        item = sp.step5_rm_english_noise(item)
        item = sp.step3_rm_noise_dig_alpha(item)
        item = sp.rm_lid_piece(item)
        item = sp.step2_rm_kongge(item)
        # item = sp.step1_correct_error_token(item)
        item = sp.step7_rm_digital(item)
        result.append(item)

    # æ•´åˆ
    context = split_token.join(result)
    context = sp.rm_lid_context(context)
    context = sp.step6_rm_short_context(context)

    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


def process_line(items, sp):
    try:
        item = json.loads(items.strip())
        context = item["text"]
        # if item["seq_id"] != "87231848-4d52-4de0-ab9c-84ec43bb7acc":
        #     return
        # if need
        lang = item["lang"]

        context = clean_text(context, lang, sp)
        context = post_process(context)
        item["text"] = context
    except:
        print("error")
        exit(0)
    item = json.dumps(item, ensure_ascii=False)
    return item


# save_file = "C:/Users/Administrator/Desktop/333.json"
input_file = "C:/Users/Administrator/Desktop/222.json"
with open(input_file, "r", encoding="utf-8") as file:
    sp = speicalProces()
    items = file.readlines()
    for item in items:
        item = json.loads(item.strip())
        context = item["text"]
        print(context)
        lang = item["lang"]
        context = clean_text(context, lang, sp)
        context = post_process(context)
        item["text"] = context
        print("------------------------------------------------")
        print(context)