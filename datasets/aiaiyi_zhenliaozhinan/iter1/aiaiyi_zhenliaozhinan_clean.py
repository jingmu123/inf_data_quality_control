import json
import os
import re
import random
import wordninja
import jieba
from langdetect import detect
from langdetect import detect_langs
from tqdm import tqdm

url_pattern = r"(https?:\/\/)?(www\.)?([\da-z\.\-@]+)\.([a-z\.]{2,6})([\/\w \.-]+)?\/?"
pattern_list_zh = [
    [r'([^\n\.ã€‚ï¼‰]*[\*\\]*(ç‚¹å‡»ä¸‹è½½|å®Œæ•´ç‰ˆ?ä¸‹è½½|ä¸‹è½½åœ°?å€?ï¼š|ç›¸å…³ä¸“é¢˜é“¾æ¥ï¼š|ç‚¹å‡»æŸ¥çœ‹åŸæ–‡ï¼š|\*ä¸‹è½½)[^\n]+)', r'åˆ é™¤1:<u>\1</u>'],  # ä¸‹è½½é“¾æ¥æç¤º
    [r'(\\?\[[\d\s\-,ï½~ï¼Œ;ï¼›â€”\\]{0,100}\])', r'åˆ é™¤2:<u>\1</u>'],  # \[2\]ã€\[3\]
    [r'([\(ï¼ˆ](æµç¨‹å›¾|[Ff]igure|[Ff]ig\.|è®¡ç®—å™¨|è§è¡¨|è¡¨|è¡¨æ ¼|å›¾|å›¾ç‰‡|å›¾è¡¨|è§å›¾) *\d+([\sï¼Œ,\-â€“\d]{0,20})[\)ï¼‰])', r'åˆ é™¤3:<u>\1</u>'],  # ï¼ˆè§è¡¨1ï¼‰ã€ï¼ˆè¡¨3ï¼‰
    # [r'([^:ï¼š\?ï¼Ÿ,ï¼Œ;ï¼›\./\*ã€‚\-â€”\s\dA-Za-zåˆ†])(\d+([\sï¼Œ,\-â€“\d]{0,20}) *)([,ï¼Œ;ï¼›\.ã€‚][^\dA-Za-z]{2})', r'\1åˆ é™¤4:<u>\2</u>\4'],  # æ ‡ç‚¹å‰çš„æ— å…³æ•°å­—
    [r'(\\*[\(\[ï¼ˆ][^\(\)\[\]ï¼ˆï¼‰]*(\set[\s\xa0]{1,3}al|\d+[:ï¼š] *\w+([\-\.]\d+)?)[^\(\)\[\]ï¼ˆï¼‰]*[\)\]ï¼‰])', r'åˆ é™¤5:<u>\1</u>'],  # å‚è€ƒåˆ é™¤ï¼Œåªåˆ é™¤å¸¦æ‹¬å·çš„å‚è€ƒæ–‡çŒ®
    [r'(\*+ç›¸å…³é˜…è¯»\*+[\w\W]*\*+æŒ‡å—ä¸‹è½½\*+[\w\W]*)', r'åˆ é™¤6:<u>\1</u>'],  # ç›¸å…³é˜…è¯»-æŒ‡å—ä¸‹è½½
    [r'([,ï¼Œ;ï¼›.ã€‚])([\?ï¼Ÿ])', r'\1åˆ é™¤7:<u>\2</u>'],  # 1.å¥å­æ ‡ç‚¹åå¤šä½™æ ‡ç‚¹ï¼Ÿ?
    [r'(\**[Â  ]*(https?:\/\/)?(www\.)?([\da-z\.\-@]+)\.([a-z]{2,6})([\/\w\?= \.-]+)?\/?)', r'åˆ é™¤9:<u>\1</u>'],


    # ï¼ˆè¿™æ¡æ­£åˆ™æœ€å¥½æ”¾æœ€åä¸€æ¡ï¼‰
    # [r'((\\\*)+)', r'åˆ é™¤8:<u>\1</u>'],  # æ­£æ–‡ä¸­çš„\*\*
    [r'((\*){2,})', r'åˆ é™¤10:<u>\1</u>'],  # æ­£æ–‡ä¸­çš„**
]

pattern_list_en = [
    ['\\s*â—\\(See.*', ''],
    ['\\*\\s*urn:lims:.*?â€¢?\\s*No\\s*', ''],#*   urn:lims:b498:s2691415 No
    [' ?View Patient Education', ''],
    ['\\((picture|figure|table)\\s*.*\\)', ''], #(picture1) (figure 2-1xxx)
    [r'[\sâ€¢\\-]{0,5}\((See|see|ESMO|ESC|ASCO)[^\(\)]*(\([^\(\)]*\)[^\(\)]*){0,}\)', ''],#(see table...)    7/25ä¿®æ”¹
    ['\\(show table.*\\)', ''], #(show table...)
    ['(.*)(for|For) additional information(.*)', ''],
    ['(.*)See individual agents(.*)', ''],
    ['(.*)Reference Range(.*)', ''],
    ['(.*)Consumer Information Use and Disclaimer(.*)', ''],
    ['(.*Last Reviewed Date.*)|(SUMMARY AND RECOMMENDATIONS)|SUMMARY|ACKNOWLEDGMENTS|(SOCIETY GUIDELINE LINKS)', ''],#...Last Reviewed Date...ã€SUMMARY AND RECOMMENDATIONSã€SUMMARYã€ACKNOWLEDGMENTSã€SOCIETY GUIDELINE LINKS
    ['\\(å‚è§.*\\)', ''],
    ['(.*)è§(.*)ä¸“é¢˜(.*)', ''],
    # ['(.*\\(ç¬¬\\d+ç‰ˆ\\))|(.*ä¸“å®¶(å…±è¯†|å»ºè®®)(\\(\\d+.*ç‰ˆ\\))?)|(.*(ä¸´æ—¶|é˜²æ§)æŒ‡å—)(ä¸“é¢˜)?|(å­¦ä¼šæŒ‡å—é“¾æ¥ï¼š.*)|(Society guideline links:.*)', ''], #...(ç¬¬1ç‰ˆ)ã€...ä¸“å®¶å…±è¯†ã€...æŒ‡å—ã€å­¦ä¼šæŒ‡å—é“¾æ¥ï¼š...ã€Society guideline links:...
    ['(More on this topic)|(Patient education:.*)', ''],
    ['(ğŸ‘|â–¶|â—|Â©|Â®|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ|ï¿½|âˆ‘|âœ¦|â¤ï¸|â¤)', ''],
    ['(^\\s*(â€“|â€”))|((-|â€“|â€”)\\s*$)', ''], #-patientã€doctor-
    ['\\((\\[?)\\s*#?((\\d+-\\s*\\d+-\\s*\\d+)|(\\d+-\\s*\\d+)|(\\d+(,|ï¼Œ)\\s*\\d+.*)|(\\d+))(\\]?).*?\\)', ''], #1.(23-1-32...) (12,dadada) ([12.åŒ»ç–—æ•°æ®])

    [r'\(\s+Ref\s+\)',''],
    [r'\([^\)\(]{1,50}\d{4};[^\)\(]{1,200}\)',''],

    [r'\([^\(\)]{0,100}algorithm\s[^\(\)]{0,100}\)',''],
    [r'\(\s?[A-Z][^\(\)]{0,20}\s\d{4}[^\(\)]{0,50}\)',''],
    [r'^Contributor Disclosures',''],
    [r'^\s?(Please read the Disclaimer at the end of this page|Links to society and government-sponsored guidelines|Beyond the Basics topics).*',''],
    [r'\([^\(\)]{1,50}(waveform|movie|calculator)[^\(\)]{1,50}\)', ''],
    # 8.06è¡¥å……
    [r'([^\n\.ã€‚ï¼‰]*[\*\\]*(ç‚¹å‡»ä¸‹è½½|å®Œæ•´ç‰ˆ?ä¸‹è½½|ä¸‹è½½åœ°?å€?ï¼š|ç›¸å…³ä¸“é¢˜é“¾æ¥ï¼š|ç‚¹å‡»æŸ¥çœ‹åŸæ–‡ï¼š|\*ä¸‹è½½)[^\n]+)', r'åˆ é™¤1:<u>\1</u>'],  # ä¸‹è½½é“¾æ¥æç¤º
    [r'(\**[Â  ]*(https?:\/\/)?(www\.)?([\da-z\.\-@]+)\.([a-z]{2,6})([\/\w\?=\.-]+)?\/?)', r'åˆ é™¤2:<u>\1</u>'],  # ç½‘å€
    [r'( *[\(ï¼ˆ](\d+([\s,ï¼Œ\-â€“\d]{0,100}))[\)ï¼‰])([,ï¼Œ;ï¼›.ã€‚])', r'åˆ é™¤3:<u>\1</u>\4'],  # å¥æœ«åºå·
    [r'(\\*[\(\[ï¼ˆ][^\(\)\[\]ï¼ˆï¼‰]*(\set[\s\xa0]{1,3}al|\d+[:ï¼š] *\w+([\-\.]\d+)?)[^\(\)\[\]ï¼ˆï¼‰]*[\)\]ï¼‰])', r'åˆ é™¤4:<u>\1</u>'],  # ï¼ˆSmith et al, 2006ï¼‰ã€ï¼ˆSnowden et al 2011ï¼‰
    [r'(([\(ï¼ˆ][^\(\)ï¼ˆï¼‰]{0,50})([fF]igure|NCT|Grade|[pP]icture|FIGURE|PICTURE|[iI]mage|[tT]able) *([^\(\)ï¼ˆï¼‰]{0,50}[\)ï¼‰]))', r'åˆ é™¤5:<u>\1</u>'],  # ( figure 2 ) ( ( figure 2 ), panels A and C)
    [r'([^\d][,ï¼Œ;ï¼›.ã€‚] *)(\d+(([\sï¼Œ,\-â€“]\d+){0,20}) *)([A-Z])', r'\1åˆ é™¤6:<u>\2</u>\5'],  # å¥é¦–8-17ã€8ã€2ï¼Œ3ç­‰
    [r'(\\?\[[\d\s\-,ï½~ï¼Œ;ï¼›â€”\\]{0,100}\])', r'åˆ é™¤7:<u>\1</u>'],  # å¥æœ«\[1, 2\]ã€\[3â€“22\]ã€\[4\]ç­‰

]


class speicalProces:
    def __init__(self):
        pass
    def step1_drop_sentenc(self,content):
        pattern3=r'ã€‚?.*è§.*è¯¦.*?[ã€‚ï¼Œ]'
        pattern1=r'ã€‚?.*é¢˜ä¸“.*è§.*?[ã€‚ï¼Œ]'
        pattern2=r'ã€‚.*è¡¨é™„è§.*?[ã€‚ï¼Œ]'
        pattern4=r'ã€‚(å›¾ç¨‹æµ)?æ–‡(ä¸‹|ä¸Š)è§.*?[ã€‚ï¼Œ]'
        text=content.strip('\n').split("\n")
        for i in range(len(text)):
            text[i] = re.sub(pattern3, 'ã€‚', text[i][::-1])[::-1]
            text[i]=re.sub(pattern1,'ã€‚',text[i][::-1])[::-1]
            text[i] = re.sub(pattern2, 'ã€‚', text[i][::-1])[::-1]
            text[i] = re.sub(pattern4, 'ã€‚', text[i][::-1])[::-1]
        text='\n'.join(text)
        return text

    def step2_endding_filter(self,content):
        if "æ‰“å°" in content or "é‚®ä»¶" in content or "è‡´è°¢" in content or "æ„Ÿè°¢" in content or "å‚è§" in content or "ä¸‹æ–‡" in content or "ä¸Šæ–‡" in content or "æµç¨‹å›¾" in content or "ç½‘ç«™" in content:
            return True
        if "uptodate" in content.lower():
            return True
        if re.search(r'ä¸“é¢˜[^ã€‚]*ç‰ˆæœ¬',content):
            return True
        if len(re.findall(url_pattern, content)) >= 1:
            return True
        return False

    def step3_reference(self, context):
        new_context = []
        references_started = False   #å®šä¹‰ä¸€ä¸ªåˆ é™¤referenceçš„å¼€å…³  åªè¦å‡ºç°å›ºå®šæ ¼å¼çš„è¡¨è¿°å°±å¯¹åé¢çš„å†…å®¹è¿›è¡Œåˆ é™¤
        introduce = 0
        introduce_index = []
        Inc = 0
        Inc_index = []

        guidelines = 0
        guidelines_index = []
        for index, item in enumerate(context):
            if re.search(r'^(References|å‚è€ƒæ–‡çŒ®|è§å‚è€ƒæ–‡çŒ®|è‡´è°¢|REFERENCES|ACKNOWLEDGMENT|The following organizations also provide reliable health information|More on this topic|Topic[^\.]*Version|For country code abbreviations|ACKNOWLEGMENT)', item.strip()):
                references_started = True
            if references_started:
                item = ""

            if re.search(r'^2024Â© UpToDate, Inc', item.strip()):
                Inc += 1
                Inc_index.append(index)
            if re.search(r'ALERT: ', item.strip()):
                Inc -= 1
                Inc_index.append(index)

            # è¦åˆ é™¤ä»Authoråˆ°å¼•è¨€ è®¾å®šäº†ä¸¤ä¸ªæ¡ä»¶åœ¨å¾ªç¯æ—¶åŒæ—¶å‡ºç°Authorå’Œå¼•è¨€ï¼Œè®°ä¸‹indexï¼Œå¯¹ç›¸åº”çš„indexè¿›è¡Œåˆ é™¤
            if re.search(r'^(Author)', item.strip()):
                introduce += 1
                introduce_index.append(index)
            if re.search(r'^(å¼•è¨€|ç®€ä»‹)', item.strip()) or re.search(r'^INTRODUCTION', item.strip()) or re.search(r'^Please read the Disclaimer at the end of this page',item.strip()):
                introduce -= 1
                introduce_index.append(index)

            if re.search(r'^(SOCIETY GUIDELINE LINKS)',item.strip()):
                guidelines += 1
                guidelines_index.append(index)
            if re.search(r'^INFORMATION FOR PATIENT',item.strip()) and guidelines == 0:
                guidelines += 1
                guidelines_index.append(index)
            if re.search(r'^.{,5}(Basics topics|Beyond the Basics topics)', item.strip()):
                guidelines -= 1
                guidelines_index.append(index)

            new_context.append(item)

        if introduce <= 0 and len(introduce_index) >= 2:
            start_index = introduce_index[0]
            end_index = introduce_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''


        if Inc <= 0 and len(Inc_index) >= 2:
            start_index = Inc_index[0]
            end_index = Inc_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''



        if guidelines <= 0 and len(guidelines_index) >= 2:
            start_index = guidelines_index[0]
            end_index = guidelines_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''

        return new_context
    def step4_rm_kongge(self, context):
        context = context.lstrip().rstrip()
        content = context.split(" ")
        first_content = content[0]
        last_content = " ".join(content[1:])
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', last_content)
        # å¤šæ‰§è¡Œä¸€æ¬¡ï¼Œå¼¥è¡¥æ­£åˆ™è¾¹ç•Œé‡å é—®é¢˜ï¼›
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', final_content)
        if len(final_content) == 0:
            return first_content

        merge_piece = first_content + final_content.lstrip()[0]

        split_word = list(jieba.cut(merge_piece, cut_all=False))
        if len(split_word[-1]) > 1:
            return first_content + final_content
        return first_content + " " + final_content

    def step5_sentence_segment(self, context):
        patter = r'([A-Za-z]{15,})'
        if re.search(patter, context):
            word_list = re.findall(patter, context)
            for wordl in word_list:
                # ä½¿ç”¨ wordninja è¿›è¡Œåˆ†è¯
                words = wordninja.split(wordl)
                output_string = " ".join(words)
                words_escape = re.escape(wordl)
                context = re.sub(rf'{words_escape}', output_string, context)
        return context

def clean_text(context, lang):
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"

    if lang == "zh":
        pattern_list = pattern_list_zh
    elif lang == 'en':
        pattern_list = pattern_list_en
    else:
        pattern_list = pattern_list_en+pattern_list_zh

    # åˆ†è§£å¤„ç†
    result = []
    sp = speicalProces()

    # special_processï¼š
    # context = sp.step1_drop_sentenc(context)
    context = context.split(split_token)

    # 7/24uptodata_newä¿®æ”¹
    # context = sp.step3_reference(context)

    for item in context:
        # 1.æ­£åˆ™
        for pattern_item in pattern_list:
            src = pattern_item[0]
            tgt = pattern_item[1]
            # print(pattern_item)
            # print(re.findall(src, item))
            item = re.sub(src, tgt, item)
        if lang == "en":
            item = sp.step5_sentence_segment(item)
        # if "url:" not in item and sp.step2_endding_filter(item):
        #     # print(item)
        #     continue

        result.append(item)
    # for item in result:
    #     print(item)
    # æ•´åˆ
    context = split_token.join(result)

    return context

def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    # å¯¹å¤šæ ‡ç‚¹è¿›è¡Œæ›¿æ¢
    context = re.sub(r'[ã€‚ï¼Œ\.](\s?[ã€‚ï¼Œ\.ï¼š]){1,5}',r'ã€‚',context)
    context = re.sub(r'[,\.](\s+[,\.]){1,5}',r'.',context)
    return context


#è¯»jsonl
fw = open(r"C:\Program Files\lk\projects\pdf\aiaiyi_zhenliaozhinan\aiaiyi_zhenliaozhinan_preformat_en_clean1B.jsonl", "w", encoding="utf-8")
with open(r"C:\Program Files\lk\projects\pdf\aiaiyi_zhenliaozhinan\aiaiyi_zhenliaozhinan_preformat.jsonl", "r", encoding="utf-8") as fs:
    lines = fs.readlines()

    for items in tqdm(lines):
        item = json.loads(items.strip())
        # if item["seq_id"] == "aecebefc-b489-471e-82ee-a9b12fb2ee91":
        context = item["text"]
        # print(context, '\n-------------------')
        lang = item["lang"]
        if lang == 'en':
            context = clean_text(context, lang)
            context = post_process(context)
            # print(context)
            item["text"] = context
            print(item["text"])
            item = json.dumps(item, ensure_ascii=False)
            fw.write(item + "\n")

fw.close()



