# -*- coding: utf-8 -*-
import json
type1={'药理学': 0.331, '内科': 0.299, '生物化学': 0.059, '妇产科': 0.047, '外科': 0.038, '微生物学': 0.032, '皮肤性病科': 0.023, '病理学': 0.023, '生理学': 0.017, '儿科': 0.016, '遗传学': 0.016, '医学伦理学': 0.015, '医学免疫学': 0.011, '麻醉学': 0.009, '眼科': 0.008, '预防医学': 0.008, '全科医学概论': 0.008, '病理生理学': 0.007, '口腔科学': 0.005, '解剖学': 0.005, '急诊医学': 0.004, '流行病学和循证医学': 0.003, '耳鼻咽喉头颈外科学': 0.003, '放射科': 0.003, '营养学': 0.002, '组织学与胚胎学': 0.002, '康复医学': 0.002, '医学心理学': 0.002, '医学统计学': 0.002, '卫生政策与管理': 0.001, '精神病学': 0.001}
type2={'药理学@临床药理学': 0.321, '内科@心血管内科': 0.081, '内科@感染病学': 0.049, '内科@肿瘤学': 0.039, '内科@神经内科': 0.034, '内科@消化内科': 0.032, '内科@呼吸内科': 0.032, '妇产科@妇科': 0.028, '内科@内分泌内科': 0.026, '药理学@分子药理学': 0.025, '医学伦理学@医患沟通': 0.021, '生物化学@分子生物化学': 0.02, '生物化学@有机化学': 0.018, '内科@肾脏内科': 0.018, '病理学@临床病理学': 0.016, '内科@传染病学': 0.014, '妇产科@产科': 0.013, '外科@普通外科': 0.013, '微生物学@医学病毒学': 0.012, '麻醉学@疼痛医学': 0.012, '微生物学@医学真菌学': 0.012, '内科@风湿病学': 0.011, '妇产科@生殖医学': 0.011, '儿科@新生儿科': 0.011, '内科@血液内科': 0.011, '生物化学@临床生物化学': 0.011, '外科@骨科': 0.011, '遗传学@人类遗传学': 0.008, '生物化学@细胞生物化学': 0.008, '微生物学@人体寄生虫学': 0.008, '遗传学@分子遗传学': 0.007, '微生物学@医学细菌学': 0.007, '病理学@系统病理学': 0.007, '外科@神经外科': 0.007, '生理学@人体生理学': 0.007, '生理学@系统生理学': 0.005, '内科@老年病学': 0.004, '外科@烧伤与整形外科': 0.004, '放射科@影像诊断': 0.004, '儿科@儿内科': 0.004, '解剖学@系统解剖学': 0.004, '口腔科学@口腔外科': 0.004, '外科@肝胆外科': 0.003, '眼科': 0.003, '病理学@分子病理学': 0.003, '外科@泌尿外科': 0.003, '精神病学': 0.001, '生理学@细胞生理学': 0.001, '病理学@细胞病理学': 0.001, '皮肤性病科': 0.001}

file="zhonghuayangsheng"
save_file = f"../../../../full_data/{file}/{file}.jsonl"
fw = open(save_file, 'w',encoding="utf-8")

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("../../../basic_tools/tokenizer")
def tokenizer_lens(context):
    ids = tokenizer.encode(context)
    return len(ids)
sum_lens = 0
class_ratio_token= {'level1': {'全科医学概论': '29.654%', '营养学': '86.697%', '内科': '29.212%', '药理学': '20.25%', '医学伦理学': '0.813%', '康复医学': '0.157%', '预防医学': '10.165%', '生物化学': '4.908%', '生理学': '4.556%', '医学免疫学': '1.028%', '病理学': '0.194%', '妇产科': '0.371%', '外科': '0.277%', '眼科': '0.324%', '皮肤性病科': '0.407%', '环境卫生学': '0.318%', '卫生学': '0.1%'}, 'level2': {'内科@老年病学': '2.263%', '药理学@临床药理学': '9.679%', '医学伦理学@医患沟通': '1.091%', '内科@消化内科': '7.419%', '生物化学@临床生物化学': '1.057%', '生理学@人体生理学': '2.549%', '生物化学@有机化学': '1.15%', '内科@肿瘤学': '2.173%', '内科@呼吸内科': '1.712%', '内科@神经内科': '0.689%', '内科@感染病学': '0.775%', '内科@心血管内科': '4.384%', '内科@风湿病学': '1.089%', '内科@内分泌内科': '2.374%', '病理学@分子病理学': '0.194%', '营养学': '0.488%', '外科@肝胆外科': '0.277%', '内科@肾脏内科': '1.693%', '药理学': '0.198%', '妇产科@产科': '0.205%', '生物化学@系统生物化学': '0.15%', '内科@血液内科': '0.565%', '妇产科@妇科': '0.153%'}, 'all_sample_tokens': 113698}
class_ratio_doc= {'level1': {'营养学': '83.0%', '预防医学': '7.5%', '生物化学': '4.75%', '生理学': '4.75%', '药理学': '30.75%', '全科医学概论': '26.5%', '内科': '25.0%', '医学伦理学': '1.5%', '医学免疫学': '0.5%', '妇产科': '0.5%', '康复医学': '0.25%', '病理学': '0.25%', '眼科': '0.25%', '皮肤性病科': '0.25%', '卫生学': '0.25%'}, 'level2': {'内科@消化内科': '7.5%', '内科@心血管内科': '4.0%', '生理学@人体生理学': '2.5%', '内科@肿瘤学': '2.0%', '药理学@临床药理学': '14.0%', '内科@风湿病学': '1.5%', '内科@内分泌内科': '1.5%', '内科@老年病学': '1.25%', '医学伦理学@医患沟通': '1.25%', '生物化学@临床生物化学': '1.25%', '生物化学@有机化学': '1.25%', '内科@感染病学': '1.25%', '内科@呼吸内科': '1.0%', '内科@肾脏内科': '0.75%', '内科@神经内科': '0.5%', '营养学': '0.5%', '内科@血液内科': '0.5%', '病理学@分子病理学': '0.25%', '外科@肝胆外科': '0.25%', '药理学': '0.25%', '妇产科@产科': '0.25%', '生物化学@系统生物化学': '0.25%', '妇产科@妇科': '0.25%'}}

with open(f"../../../../full_data/{file}/{file}_clean.jsonl", "r",encoding="utf-8") as fs:
    for item in fs.readlines():
        item = json.loads(item)
        context = item["text"]
        lens = tokenizer_lens(context)
        sum_lens += lens

with open(f"../../../../full_data/{file}/{file}_clean.jsonl", "r",encoding="utf-8") as fs:
    for item in fs.readlines():
        item = json.loads(item)
        item["tags"] = {
                        "id": item["seq_id"],
                        "clean_iters":"1",
                        "quality_score":"93%",
                        "class_ratio_doc": class_ratio_doc,
                        "class_ratio_tokenize": class_ratio_token,
                        "item_tokens": tokenizer_lens(item["text"]),
                        "dataset_tokens": sum_lens,
                        "bia_class": "百科科普"
        }

        item = json.dumps(item,ensure_ascii=False)
        fw.write(item+"\n")
